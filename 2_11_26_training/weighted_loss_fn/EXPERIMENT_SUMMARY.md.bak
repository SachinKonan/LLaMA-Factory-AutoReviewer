# Weighted Loss Experiment - Quick Reference Summary

## Experiment Configuration (18 Total)

### Design Matrix

```
Balanced (1/2):     Gammas [2, 4, 8] × Variants [accept, reject] = 6 experiments
Imbalanced (1/3):   Gammas [2, 4]    × Variants [accept, reject] = 4 experiments
Imbalanced (1/4):   Gammas [2, 4]    × Variants [accept, reject] = 4 experiments
Imbalanced (1/8):   Gammas [2, 4]    × Variants [accept, reject] = 4 experiments
                                                        TOTAL:      18 experiments
```

### SLURM Array Index Mapping

```
Indices 0-5:   Balanced dataset (1/2)
Indices 6-9:   Proportion 1/3
Indices 10-13: Proportion 1/4
Indices 14-17: Proportion 1/8
```

## Execution Commands

```bash
# Stage 1: Generate datasets (4 datasets with different proportions)
python 2_11_26_training/weighted_loss_fn/scripts/stage1_generate_datasets.py

# Stage 2: Submit all 18 training jobs
sbatch 2_11_26_training/weighted_loss_fn/scripts/stage2_train_models.sbatch

# Stage 3: Submit all 18 inference jobs
sbatch 2_11_26_training/weighted_loss_fn/scripts/stage3_run_inference.sbatch

# Stage 4: Evaluate all experiments
python 2_11_26_training/weighted_loss_fn/scripts/stage4_evaluate.py

# Stage 5: Generate visualizations
python 2_11_26_training/weighted_loss_fn/scripts/stage5_visualize.py
```

## Resource Summary

| Resource | Amount |
|----------|--------|
| Training GPU-hours | 3,456 (18 jobs × 4 GPUs × 48h) |
| Inference GPU-hours | 72 (18 jobs × 1 GPU × 4h) |
| Estimated cost | ~$7,056 |
| Storage | 272GB |
| Wall time | ~2.4 days (parallelized) |

## Key Differences from Original 32-Experiment Design

1. **No gamma=1.0 baseline** - All experiments use weighted loss (gamma ≥ 2)
2. **Differential gamma ranges**:
   - Balanced (1/2): Full range [2, 4, 8]
   - Imbalanced (1/3, 1/4, 1/8): Limited range [2, 4]
3. **44% reduction in experiments** (32 → 18)
4. **44% reduction in cost** (~$12.5K → ~$7.1K)

## Expected Outcomes

### Key Hypotheses

1. **Gamma effect**: Higher gamma → stronger bias toward weighted class
   - Accept variant: Higher predicted acceptance rate
   - Reject variant: Lower predicted acceptance rate

2. **Proportion effect**: More reject-heavy data → lower predicted acceptance rate

3. **Interaction**: Balanced dataset with gamma=8 should show most extreme effects

### Success Criteria

- ✓ Monotonic gamma relationships
- ✓ Accept variant > Reject variant (at same gamma, proportion)
- ✓ Predicted rates span 30-70% range
- ✓ Accuracy > 60% for most configurations

## File Locations

```
2_11_26_training/weighted_loss_fn/
├── experiment_implementation.md    # Full implementation plan
├── EXPERIMENT_SUMMARY.md           # This file
├── data/                           # 4 training datasets
├── configs/generated/              # 18 auto-generated configs
├── scripts/                        # 5 stage scripts
├── logs/                           # SLURM logs (18 train + 18 infer)
├── results/                        # 18 predictions.jsonl files
└── metrics/
    ├── all_metrics.json           # Aggregate metrics
    └── plots/                     # Visualizations
```

## Complete Experiment List

| Index | Proportion | Variant | Gamma | Name                      |
|-------|------------|---------|-------|---------------------------|
| 0     | 1/2        | accept  | 2.0   | accept_gamma2.0_prop1_2   |
| 1     | 1/2        | accept  | 4.0   | accept_gamma4.0_prop1_2   |
| 2     | 1/2        | accept  | 8.0   | accept_gamma8.0_prop1_2   |
| 3     | 1/2        | reject  | 2.0   | reject_gamma2.0_prop1_2   |
| 4     | 1/2        | reject  | 4.0   | reject_gamma4.0_prop1_2   |
| 5     | 1/2        | reject  | 8.0   | reject_gamma8.0_prop1_2   |
| 6     | 1/3        | accept  | 2.0   | accept_gamma2.0_prop1_3   |
| 7     | 1/3        | accept  | 4.0   | accept_gamma4.0_prop1_3   |
| 8     | 1/3        | reject  | 2.0   | reject_gamma2.0_prop1_3   |
| 9     | 1/3        | reject  | 4.0   | reject_gamma4.0_prop1_3   |
| 10    | 1/4        | accept  | 2.0   | accept_gamma2.0_prop1_4   |
| 11    | 1/4        | accept  | 4.0   | accept_gamma4.0_prop1_4   |
| 12    | 1/4        | reject  | 2.0   | reject_gamma2.0_prop1_4   |
| 13    | 1/4        | reject  | 4.0   | reject_gamma4.0_prop1_4   |
| 14    | 1/8        | accept  | 2.0   | accept_gamma2.0_prop1_8   |
| 15    | 1/8        | accept  | 4.0   | accept_gamma4.0_prop1_8   |
| 16    | 1/8        | reject  | 2.0   | reject_gamma2.0_prop1_8   |
| 17    | 1/8        | reject  | 4.0   | reject_gamma4.0_prop1_8   |

## Implementation Checklist

### Code Modifications (src/llamafactory/)

- [ ] `train/trainer_utils.py`: Add `weighted_bce_loss_accept()` and `weighted_bce_loss_reject()`
- [ ] `hparams/finetuning_args.py`: Add `use_weighted_loss`, `weighted_loss_variant`, `weighted_loss_gamma`
- [ ] `train/sft/trainer.py`: Update `__init__` to set `self.compute_loss_func`
- [ ] `data/dataset_info.json`: Register 4 new datasets

### Experiment Files

- [ ] `stage1_generate_datasets.py`: Generate 4 training datasets
- [ ] `stage2_train_models.sbatch`: 18-job array for training
- [ ] `stage3_run_inference.sbatch`: 18-job array for inference
- [ ] `stage4_evaluate.py`: Compute metrics for 18 experiments
- [ ] `stage5_visualize.py`: Generate comparison plots

### Testing

- [ ] Test dataset generation with `--debug` (100 samples)
- [ ] Test single training run (index 0)
- [ ] Test single inference run (index 0)
- [ ] Verify metrics computation
- [ ] Verify plots generation

---

**For full implementation details, see `experiment_implementation.md`**
