==============================================
TextGrad Prompt Optimization
Job ID: 2473614
Date: Sat Jan 17 03:31:07 AM EST 2026
==============================================
Inference Model: /n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/saves/qwen2.5-3b/full/sft_ds3/checkpoint-3750
Gradient Model: Qwen/Qwen3-14B
Main venv: /n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/.venv_vllm_inf
TextGrad venv: /n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments_v2/.venv
==============================================
Starting inference server on port 8000...
Inference server PID: 2909165
Starting gradient server on port 8001...
Gradient server PID: 2909167
Waiting for servers to initialize (180s)...
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:31:18 [api_server.py:1772] vLLM API server version 0.12.0
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:18 [api_server.py:1772] vLLM API server version 0.12.0
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:31:18 [utils.py:253] non-default args: {'port': 8001, 'model': 'Qwen/Qwen3-14B', 'trust_remote_code': True, 'max_model_len': 32768}
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:18 [utils.py:253] non-default args: {'model': '/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/saves/qwen2.5-3b/full/sft_ds3/checkpoint-3750', 'trust_remote_code': True, 'max_model_len': 32768}
[0;36m(APIServer pid=2909166)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=2909164)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:18 [model.py:637] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:18 [model.py:1750] Using max model len 32768
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:31:18 [model.py:637] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:31:18 [model.py:1750] Using max model len 32768
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:18 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:31:18 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:27 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/saves/qwen2.5-3b/full/sft_ds3/checkpoint-3750', speculative_config=None, tokenizer='/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/saves/qwen2.5-3b/full/sft_ds3/checkpoint-3750', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/saves/qwen2.5-3b/full/sft_ds3/checkpoint-3750, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:31:27 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:28 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.32.120:40507 backend=nccl
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:31:28 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.32.120:40545 backend=nccl
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:28 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:31:28 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:28 [gpu_model_runner.py:3467] Starting to load model /n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/saves/qwen2.5-3b/full/sft_ds3/checkpoint-3750...
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:31:28 [gpu_model_runner.py:3467] Starting to load model Qwen/Qwen3-14B...
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:31:29 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:29 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=2909496)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=2909496)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:03,  1.80it/s]
[0;36m(EngineCore_DP0 pid=2909496)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.99it/s]
[0;36m(EngineCore_DP0 pid=2909496)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.84it/s]
[0;36m(EngineCore_DP0 pid=2909496)[0;0m 
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:30 [default_loader.py:308] Loading weights took 1.11 seconds
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:03,  1.65it/s]
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:31 [gpu_model_runner.py:3549] Model loading took 5.7916 GiB memory and 1.512534 seconds
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:01<00:03,  1.63it/s]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:02<00:02,  1.62it/s]
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:37 [backends.py:655] Using cache directory: /n/fs/vision-mix/jl0796/vllm/torch_compile_cache/1c8920a7c1/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:37 [backends.py:715] Dynamo bytecode transform time: 6.63 s
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:42 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.200 s
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:44 [monitor.py:34] torch.compile takes 10.83 s in total
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:44 [gpu_worker.py:359] Available KV cache memory: 32.73 GiB
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:45 [kv_cache_utils.py:1286] GPU KV cache size: 953,376 tokens
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:45 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 29.09x
[0;36m(EngineCore_DP0 pid=2909496)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:02, 18.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:00<00:02, 19.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:00<00:02, 20.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:00<00:01, 20.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:00<00:01, 21.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:00<00:01, 21.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:00<00:01, 22.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:01<00:01, 22.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:01<00:01, 22.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:01<00:00, 22.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:01<00:00, 22.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:01<00:00, 22.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:01<00:00, 22.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:01<00:00, 22.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:01<00:00, 22.74it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:02<00:00, 22.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:02<00:00, 22.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:02<00:00, 22.04it/s]
[0;36m(EngineCore_DP0 pid=2909496)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:01, 23.43it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:01, 24.83it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:01, 25.50it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 25.46it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:00<00:00, 25.89it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:00<00:00, 26.11it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:00<00:00, 26.22it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 26.37it/s]Capturing CUDA graphs (decode, FULL):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:01<00:00, 26.18it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:01<00:00, 26.30it/s]Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:01<00:00, 26.82it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 26.27it/s]
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:49 [gpu_model_runner.py:4466] Graph capturing finished in 4 secs, took 0.53 GiB
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:49 [core.py:254] init engine (profile, create kv cache, warmup model) took 18.23 seconds
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [api_server.py:1520] Supported tasks: ['generate']
[0;36m(APIServer pid=2909164)[0;0m WARNING 01-17 03:31:50 [model.py:1576] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [serving_responses.py:194] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [serving_chat.py:133] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [serving_completion.py:73] Using default completion sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [serving_chat.py:133] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [api_server.py:1847] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO:     Started server process [2909164]
[0;36m(APIServer pid=2909164)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=2909164)[0;0m INFO:     Application startup complete.
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [01:07<01:11, 23.94s/it]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [01:58<01:05, 32.89s/it]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [02:29<00:32, 32.60s/it]
Testing server connectivity...
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59804 - "GET /v1/models HTTP/1.1" 200 OK
  Port 8000: OK
  Port 8001: FAILED
  Checking if process is running...
jl0796   2909166  5.5  0.2 7710232 1161500 ?     Sl   03:31   0:10 /n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/.venv_vllm_inf/bin/python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-14B --port 8001 --gpu-memory-utilization 0.9 --max-model-len 32768 --trust-remote-code
jl0796   2909777  0.0  0.0   6428  2440 ?        S    03:34   0:00 grep -E vllm.*8001
  Server logs (last 50 lines):
==> logs/vllm_gradient_2473614.log <==
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:31:18 [api_server.py:1772] vLLM API server version 0.12.0
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:31:18 [utils.py:253] non-default args: {'port': 8001, 'model': 'Qwen/Qwen3-14B', 'trust_remote_code': True, 'max_model_len': 32768}
[0;36m(APIServer pid=2909166)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:31:18 [model.py:637] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:31:18 [model.py:1750] Using max model len 32768
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:31:18 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:31:27 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:31:28 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.32.120:40545 backend=nccl
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:31:28 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:31:28 [gpu_model_runner.py:3467] Starting to load model Qwen/Qwen3-14B...
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:31:29 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:03,  1.80it/s]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:03,  1.65it/s]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:01<00:03,  1.63it/s]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:02<00:02,  1.62it/s]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [01:07<01:11, 23.94s/it]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [01:58<01:05, 32.89s/it]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [02:29<00:32, 32.60s/it]

==> logs/vllm_inference_2473614.log <==
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:49 [gpu_model_runner.py:4466] Graph capturing finished in 4 secs, took 0.53 GiB
[0;36m(EngineCore_DP0 pid=2909496)[0;0m INFO 01-17 03:31:49 [core.py:254] init engine (profile, create kv cache, warmup model) took 18.23 seconds
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [api_server.py:1520] Supported tasks: ['generate']
[0;36m(APIServer pid=2909164)[0;0m WARNING 01-17 03:31:50 [model.py:1576] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [serving_responses.py:194] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [serving_chat.py:133] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [serving_completion.py:73] Using default completion sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [serving_chat.py:133] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [api_server.py:1847] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:31:50 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=2909164)[0;0m INFO:     Started server process [2909164]
[0;36m(APIServer pid=2909164)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=2909164)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59804 - "GET /v1/models HTTP/1.1" 200 OK

Starting optimization...
==============================================
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:34:12 [chat_utils.py:574] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:34:20 [loggers.py:236] Engine 000: Avg prompt throughput: 9629.5 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.3%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:34:30 [loggers.py:236] Engine 000: Avg prompt throughput: 12202.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:34:40 [loggers.py:236] Engine 000: Avg prompt throughput: 10824.2 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [03:12<00:00, 35.82s/it]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [03:12<00:00, 24.09s/it]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m 
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:34:42 [default_loader.py:308] Loading weights took 192.77 seconds
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:34:43 [gpu_model_runner.py:3549] Model loading took 27.5185 GiB memory and 193.398630 seconds
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:34:50 [loggers.py:236] Engine 000: Avg prompt throughput: 12015.5 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:34:51 [backends.py:655] Using cache directory: /n/fs/vision-mix/jl0796/vllm/torch_compile_cache/d6e300c7a2/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:34:51 [backends.py:715] Dynamo bytecode transform time: 8.51 s
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:35:00 [loggers.py:236] Engine 000: Avg prompt throughput: 10663.8 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.4%
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:35:00 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.611 s
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:35:02 [monitor.py:34] torch.compile takes 17.12 s in total
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:35:03 [gpu_worker.py:359] Available KV cache memory: 10.99 GiB
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:35:03 [kv_cache_utils.py:1286] GPU KV cache size: 72,016 tokens
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:35:03 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 2.20x
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s][0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:06,  7.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:06,  7.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:06,  7.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:06,  7.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:00<00:06,  7.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:00<00:05,  7.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:00<00:05,  7.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:05,  7.83it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:05,  8.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:01<00:04,  8.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:01<00:04,  8.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:01<00:04,  9.06it/s][0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:01<00:04,  9.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:01<00:03,  9.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:01<00:03,  9.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:01<00:03,  9.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:02<00:03, 10.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:02<00:02, 11.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:02<00:02, 11.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:02<00:02, 11.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:02<00:02, 12.12it/s][0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:02<00:01, 12.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:03<00:01, 12.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:03<00:01, 12.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:03<00:01, 12.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:03<00:01, 12.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:03<00:00, 13.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:03<00:00, 13.34it/s][0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:03<00:00, 13.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:04<00:00, 13.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:04<00:00, 13.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:04<00:00, 14.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:04<00:00, 14.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:04<00:00, 11.29it/s]
[0;36m(EngineCore_DP0 pid=2909500)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:02, 11.19it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:02, 11.81it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:02, 12.14it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:02, 12.34it/s][0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:00<00:01, 12.70it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:01, 12.97it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:01, 13.16it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:01<00:01, 13.32it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:01<00:01, 13.60it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:01<00:01, 13.83it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:01<00:00, 14.13it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:01<00:00, 14.38it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:01<00:00, 14.64it/s][0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:35:10 [loggers.py:236] Engine 000: Avg prompt throughput: 11085.4 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.4%
Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:02<00:00, 14.85it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:02<00:00, 15.10it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:02<00:00, 15.61it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:02<00:00, 15.97it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:02<00:00, 14.13it/s]
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:35:11 [gpu_model_runner.py:4466] Graph capturing finished in 8 secs, took 0.65 GiB
[0;36m(EngineCore_DP0 pid=2909500)[0;0m INFO 01-17 03:35:11 [core.py:254] init engine (profile, create kv cache, warmup model) took 28.52 seconds
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [api_server.py:1520] Supported tasks: ['generate']
[0;36m(APIServer pid=2909166)[0;0m WARNING 01-17 03:35:12 [model.py:1576] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [serving_responses.py:194] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [serving_completion.py:73] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [api_server.py:1847] Starting vLLM API server 0 on http://0.0.0.0:8001
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:12 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=2909166)[0;0m INFO:     Started server process [2909166]
[0;36m(APIServer pid=2909166)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:18 [chat_utils.py:574] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:35:20 [loggers.py:236] Engine 000: Avg prompt throughput: 9282.7 tokens/s, Avg generation throughput: 4.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:23 [loggers.py:236] Engine 000: Avg prompt throughput: 47.2 tokens/s, Avg generation throughput: 11.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:35:30 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:35:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:36:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:36:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:36:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:49886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:36:33 [loggers.py:236] Engine 000: Avg prompt throughput: 54.6 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 24.7%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:36:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 24.7%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:49886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:36:50 [loggers.py:236] Engine 000: Avg prompt throughput: 5863.1 tokens/s, Avg generation throughput: 3.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:36:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 24.7%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:37:00 [loggers.py:236] Engine 000: Avg prompt throughput: 12874.3 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:37:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 24.7%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:37:10 [loggers.py:236] Engine 000: Avg prompt throughput: 4567.1 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:37:13 [loggers.py:236] Engine 000: Avg prompt throughput: 51.0 tokens/s, Avg generation throughput: 24.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 35.1%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:39112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:37:20 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:37:23 [loggers.py:236] Engine 000: Avg prompt throughput: 54.6 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 40.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:37:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 40.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:37:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 40.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:37:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 40.5%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:39112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:38:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 40.5%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:38:10 [loggers.py:236] Engine 000: Avg prompt throughput: 8658.5 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:38:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 40.5%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:41160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:38:20 [loggers.py:236] Engine 000: Avg prompt throughput: 13436.7 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:38:23 [loggers.py:236] Engine 000: Avg prompt throughput: 51.1 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 43.6%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:38:30 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:38:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 43.6%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:55674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:38:43 [loggers.py:236] Engine 000: Avg prompt throughput: 54.3 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 45.7%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:38:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 45.7%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:55674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:39:00 [loggers.py:236] Engine 000: Avg prompt throughput: 2593.1 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:39:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 45.7%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:39:10 [loggers.py:236] Engine 000: Avg prompt throughput: 13528.7 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:39:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 45.7%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:39:20 [loggers.py:236] Engine 000: Avg prompt throughput: 7318.2 tokens/s, Avg generation throughput: 4.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:39:23 [loggers.py:236] Engine 000: Avg prompt throughput: 51.2 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 47.2%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:39:30 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:39:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 47.2%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:39:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 47.2%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:45168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:39:53 [loggers.py:236] Engine 000: Avg prompt throughput: 54.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 48.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:40:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 48.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:40:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 48.4%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:45168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:40:20 [loggers.py:236] Engine 000: Avg prompt throughput: 4359.2 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:40:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 48.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:40:30 [loggers.py:236] Engine 000: Avg prompt throughput: 13380.2 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:40:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 48.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:40:40 [loggers.py:236] Engine 000: Avg prompt throughput: 7194.3 tokens/s, Avg generation throughput: 4.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:40:43 [loggers.py:236] Engine 000: Avg prompt throughput: 51.3 tokens/s, Avg generation throughput: 16.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 49.2%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:40:50 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:33526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:40:53 [loggers.py:236] Engine 000: Avg prompt throughput: 53.8 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 50.3%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:41:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 50.3%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:41:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 50.3%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:41:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 50.3%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:41:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 50.3%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:33526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:41:40 [loggers.py:236] Engine 000: Avg prompt throughput: 3321.1 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:41:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.3%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:41:50 [loggers.py:236] Engine 000: Avg prompt throughput: 12705.6 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:41:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.3%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:42:00 [loggers.py:236] Engine 000: Avg prompt throughput: 7172.2 tokens/s, Avg generation throughput: 4.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:42:03 [loggers.py:236] Engine 000: Avg prompt throughput: 51.6 tokens/s, Avg generation throughput: 19.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 50.8%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:42:10 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:42:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 50.8%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:42:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 50.8%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:38864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:42:33 [loggers.py:236] Engine 000: Avg prompt throughput: 53.8 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 51.3%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:42:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 51.3%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:38864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:42:50 [loggers.py:236] Engine 000: Avg prompt throughput: 613.3 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:42:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 51.3%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:43:00 [loggers.py:236] Engine 000: Avg prompt throughput: 12476.5 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:43:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 51.3%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:59664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:43:10 [loggers.py:236] Engine 000: Avg prompt throughput: 10360.8 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:43:13 [loggers.py:236] Engine 000: Avg prompt throughput: 51.4 tokens/s, Avg generation throughput: 10.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 51.6%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:43:20 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:43:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 51.6%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:43:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 51.6%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:52754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:43:43 [loggers.py:236] Engine 000: Avg prompt throughput: 54.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 52.2%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:43:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 52.2%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:44:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 52.2%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:44:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 52.2%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:44:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 52.2%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:52754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:44:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 52.2%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:44:40 [loggers.py:236] Engine 000: Avg prompt throughput: 9636.6 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:44:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 52.2%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:44:50 [loggers.py:236] Engine 000: Avg prompt throughput: 12999.9 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:60600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:44:53 [loggers.py:236] Engine 000: Avg prompt throughput: 51.2 tokens/s, Avg generation throughput: 3.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 52.4%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:45:00 [loggers.py:236] Engine 000: Avg prompt throughput: 1339.5 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:45:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 52.4%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:45:10 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:45:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 52.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:45:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 52.4%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:56004 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:45:33 [loggers.py:236] Engine 000: Avg prompt throughput: 54.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 52.9%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:45:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 52.9%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:45:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 52.9%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:46:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 52.9%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:46:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 52.9%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:56004 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:46:20 [loggers.py:236] Engine 000: Avg prompt throughput: 1917.8 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:46:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 52.9%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:46:30 [loggers.py:236] Engine 000: Avg prompt throughput: 11343.9 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:46:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 52.9%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:49550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:46:40 [loggers.py:236] Engine 000: Avg prompt throughput: 12979.8 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:46:43 [loggers.py:236] Engine 000: Avg prompt throughput: 51.3 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 53.1%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:46:50 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:46:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 53.1%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:47:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 53.1%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:45426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:47:13 [loggers.py:236] Engine 000: Avg prompt throughput: 54.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 55.6%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:45426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:47:20 [loggers.py:236] Engine 000: Avg prompt throughput: 2493.7 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:47:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 55.6%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:47:30 [loggers.py:236] Engine 000: Avg prompt throughput: 13351.1 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:47:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 55.6%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:34282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:47:40 [loggers.py:236] Engine 000: Avg prompt throughput: 8787.7 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:47:43 [loggers.py:236] Engine 000: Avg prompt throughput: 51.4 tokens/s, Avg generation throughput: 13.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 55.6%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:47:50 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:47:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 55.6%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:48:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 55.6%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:48:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 55.6%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:48:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 55.6%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:48:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 55.6%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:54806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:48:43 [loggers.py:236] Engine 000: Avg prompt throughput: 53.8 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 55.7%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:54806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:48:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 55.7%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:49:00 [loggers.py:236] Engine 000: Avg prompt throughput: 10442.7 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:49:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 55.7%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:49:10 [loggers.py:236] Engine 000: Avg prompt throughput: 13442.3 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:49:13 [loggers.py:236] Engine 000: Avg prompt throughput: 51.3 tokens/s, Avg generation throughput: 3.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 55.9%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:49:20 [loggers.py:236] Engine 000: Avg prompt throughput: 1187.2 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:49:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 55.9%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:52820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:49:30 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:49:33 [loggers.py:236] Engine 000: Avg prompt throughput: 54.3 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:49:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:49:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:52820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:50:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:50:10 [loggers.py:236] Engine 000: Avg prompt throughput: 9331.4 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:50:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:50:20 [loggers.py:236] Engine 000: Avg prompt throughput: 12523.3 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:52412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:50:23 [loggers.py:236] Engine 000: Avg prompt throughput: 51.5 tokens/s, Avg generation throughput: 4.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:50:30 [loggers.py:236] Engine 000: Avg prompt throughput: 1422.8 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:50:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:50:40 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:50:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:33278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:50:53 [loggers.py:236] Engine 000: Avg prompt throughput: 53.8 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:51:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:51:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:33278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:51:20 [loggers.py:236] Engine 000: Avg prompt throughput: 7099.9 tokens/s, Avg generation throughput: 4.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:51:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:51:30 [loggers.py:236] Engine 000: Avg prompt throughput: 12650.3 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:51:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:42290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:51:40 [loggers.py:236] Engine 000: Avg prompt throughput: 4091.2 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:51:43 [loggers.py:236] Engine 000: Avg prompt throughput: 51.5 tokens/s, Avg generation throughput: 23.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:51:50 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:51:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:53442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:52:03 [loggers.py:236] Engine 000: Avg prompt throughput: 54.2 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:52:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:53442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:52:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:52:30 [loggers.py:236] Engine 000: Avg prompt throughput: 8818.3 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:52:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:52:40 [loggers.py:236] Engine 000: Avg prompt throughput: 12718.7 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:44208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:52:43 [loggers.py:236] Engine 000: Avg prompt throughput: 51.2 tokens/s, Avg generation throughput: 3.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:52:50 [loggers.py:236] Engine 000: Avg prompt throughput: 1956.5 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:52:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:38156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:53:00 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:53:03 [loggers.py:236] Engine 000: Avg prompt throughput: 53.8 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:53:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:53:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:53:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:38156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:53:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:53:50 [loggers.py:236] Engine 000: Avg prompt throughput: 12950.9 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:53:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:54:00 [loggers.py:236] Engine 000: Avg prompt throughput: 10334.5 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:54:03 [loggers.py:236] Engine 000: Avg prompt throughput: 51.3 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:54:10 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:54:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:54:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 56.0%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:45250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:54:33 [loggers.py:236] Engine 000: Avg prompt throughput: 53.3 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 56.1%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:54:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 56.1%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:54:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 56.1%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:55:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 56.1%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:55:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 56.1%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:45250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:55:20 [loggers.py:236] Engine 000: Avg prompt throughput: 1534.5 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:55:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.1%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:55:30 [loggers.py:236] Engine 000: Avg prompt throughput: 13141.1 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:55:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.1%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:57598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:55:40 [loggers.py:236] Engine 000: Avg prompt throughput: 10254.9 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:55:43 [loggers.py:236] Engine 000: Avg prompt throughput: 51.7 tokens/s, Avg generation throughput: 10.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 56.1%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:55:50 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:55:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 56.1%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:56418 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:56:03 [loggers.py:236] Engine 000: Avg prompt throughput: 53.3 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:56418 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:56:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 23.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:56:20 [loggers.py:236] Engine 000: Avg prompt throughput: 9333.2 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:56:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:56:30 [loggers.py:236] Engine 000: Avg prompt throughput: 12951.5 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:56:40 [loggers.py:236] Engine 000: Avg prompt throughput: 4377.8 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:56:43 [loggers.py:236] Engine 000: Avg prompt throughput: 51.5 tokens/s, Avg generation throughput: 22.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 57.5%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:56:50 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:56:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 57.5%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:57:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 57.5%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:42246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:57:13 [loggers.py:236] Engine 000: Avg prompt throughput: 54.5 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:42246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:57:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:57:30 [loggers.py:236] Engine 000: Avg prompt throughput: 12462.8 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:57:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:57:40 [loggers.py:236] Engine 000: Avg prompt throughput: 10888.4 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:55028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:57:43 [loggers.py:236] Engine 000: Avg prompt throughput: 51.6 tokens/s, Avg generation throughput: 4.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 57.5%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:57:50 [loggers.py:236] Engine 000: Avg prompt throughput: 1763.4 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:57:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 57.5%
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:58:00 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:39254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:58:03 [loggers.py:236] Engine 000: Avg prompt throughput: 53.9 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:58:13 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:58:23 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:58:33 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:58:43 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909166)[0;0m INFO:     127.0.0.1:39254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:58:50 [loggers.py:236] Engine 000: Avg prompt throughput: 1507.9 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:58:53 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:59:00 [loggers.py:236] Engine 000: Avg prompt throughput: 12982.9 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909166)[0;0m INFO 01-17 03:59:03 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 57.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:59:10 [loggers.py:236] Engine 000: Avg prompt throughput: 12398.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:59:20 [loggers.py:236] Engine 000: Avg prompt throughput: 12210.8 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:59:30 [loggers.py:236] Engine 000: Avg prompt throughput: 12370.4 tokens/s, Avg generation throughput: 5.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:59:40 [loggers.py:236] Engine 000: Avg prompt throughput: 10829.9 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO 01-17 03:59:50 [loggers.py:236] Engine 000: Avg prompt throughput: 11905.1 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.4%
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2909164)[0;0m INFO:     127.0.0.1:50600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
============================================================
TextGrad Prompt Optimization v2 (System + Human Prefix)
============================================================

Loading validation dataset: iclr_2020_2025_85_5_10_split6_original_clean_binary_noreviews_v6_validation
Loaded 1252 validation samples

Setting up engines...
  Inference: /n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/saves/qwen2.5-3b/full/sft_ds3/checkpoint-3750 @ port 8000
  Gradient: Qwen/Qwen3-14B @ port 8001

Initial system prompt (75 chars):
  You are an expert academic reviewer tasked with evaluating research papers.

Initial human prefix (189 chars):
  I am giving you a paper. I want to predict its acceptance outcome at ICLR.
 - Your answer will eithe...

Constraints:
  - Keep the system prompt under 500 characters
  - Keep the human prefix under 800 characters
  - The human prefix MUST retain the output format instruction: \boxed{Accept} or \boxed{Reject}
  - The human prefix should clearly state the task is predicting paper acceptance

Starting optimization for 20 epochs...
------------------------------------------------------------
Optimization step failed: Connection error.
Optimization step failed: Connection error.

Epoch 3/20
  Valid rate: 100.0% (16/16)
  Accuracy: 56.2%
  System prompt (260 chars): You are an expert academic reviewer tasked with rigorously evaluating research p...
  Human prefix (214 chars): Predict whether the following paper would be accepted or rejected at ICLR, based...
  *** New best accuracy: 56.2% (valid: 100.0%) ***

Epoch 4/20
  Valid rate: 100.0% (16/16)
  Accuracy: 50.0%
  System prompt (253 chars): You are an AI academic reviewer evaluating a research paper. Assess its original...
  Human prefix (131 chars): Determine if the following paper would be accepted or rejected. Provide your ans...

Epoch 5/20
  Valid rate: 100.0% (16/16)
  Accuracy: 68.8%
  System prompt (266 chars): Act as an AI academic reviewer assessing a research paper's suitability for publ...
  Human prefix (116 chars): Predict whether the following paper is accepted or rejected. Format your answer ...
  *** New best accuracy: 68.8% (valid: 100.0%) ***

Epoch 6/20
  Valid rate: 100.0% (16/16)
  Accuracy: 43.8%
  System prompt (170 chars): Act as an AI academic reviewer to evaluate the research paper's quality, origina...
  Human prefix (106 chars): Predict whether the following paper is accepted or rejected. Answer with \boxed{...

Epoch 7/20
  Valid rate: 100.0% (16/16)
  Accuracy: 68.8%
  System prompt (175 chars): Evaluate the research paper's originality, methodology, and significance as an A...
  Human prefix (103 chars): Determine if the following paper is accepted or rejected. Answer with \boxed{Acc...

Epoch 8/20
  Valid rate: 100.0% (16/16)
  Accuracy: 56.2%
  System prompt (189 chars): You are an academic reviewer tasked with critically evaluating the paper's origi...
  Human prefix (117 chars): Predict whether the following paper is accepted or rejected. Provide your answer...

Epoch 9/20
  Valid rate: 100.0% (16/16)
  Accuracy: 31.2%
  System prompt (151 chars): You are an academic reviewer evaluating a paper's methodology, significance, cla...
  Human prefix (114 chars): Determine if the following paper is accepted or rejected. Provide your answer in...

Epoch 10/20
  Valid rate: 100.0% (16/16)
  Accuracy: 56.2%
  System prompt (153 chars): You are tasked with rigorously evaluating a paper's methodology, significance, c...
  Human prefix (114 chars): Determine if the following paper is accepted or rejected. Provide your answer in...

Epoch 11/20
  Valid rate: 100.0% (16/16)
  Accuracy: 75.0%
  System prompt (159 chars): Act as a rigorous academic reviewer. Evaluate the paper's methodology, significa...
  Human prefix (99 chars): Classify the following paper as accepted or rejected. Answer with \boxed{Accept}...
  *** New best accuracy: 75.0% (valid: 100.0%) ***

Epoch 12/20
  Valid rate: 100.0% (16/16)
  Accuracy: 43.8%
  System prompt (184 chars): Act as a rigorous academic reviewer. Evaluate the paper's methodology, significa...
  Human prefix (134 chars): Predict whether the following paper would be accepted or rejected. Provide your ...

Epoch 13/20
  Valid rate: 100.0% (16/16)
  Accuracy: 43.8%
  System prompt (199 chars): Act as a rigorous academic reviewer. Thoroughly assess the paper's methodology, ...
  Human prefix (92 chars): Classify the paper as accepted or rejected. Use the format \boxed{Accept} or \bo...

Epoch 14/20
  Valid rate: 100.0% (16/16)
  Accuracy: 43.8%
  System prompt (171 chars): Act as a rigorous academic reviewer. Evaluate the paper's originality, methodolo...
  Human prefix (124 chars): Predict whether the paper would be accepted or rejected. Provide your answer in ...

Epoch 15/20
  Valid rate: 100.0% (16/16)
  Accuracy: 62.5%
  System prompt (153 chars): Act as a rigorous academic reviewer, assessing originality, methodology, clarity...
  Human prefix (96 chars): Predict if the paper will be accepted or rejected. Answer with \boxed{Accept} or...

Epoch 16/20
  Valid rate: 100.0% (16/16)
  Accuracy: 56.2%
  System prompt (158 chars): Act as a rigorous academic reviewer. Evaluate originality, methodology, clarity,...
  Human prefix (72 chars): Predict the paper's acceptance status: \boxed{Accept} or \boxed{Reject}....

Epoch 17/20
  Valid rate: 100.0% (16/16)
  Accuracy: 50.0%
  System prompt (201 chars): Act as a rigorous academic reviewer. Thoroughly evaluate originality, methodolog...
  Human prefix (72 chars): Predict the paper's acceptance status: \boxed{Accept} or \boxed{Reject}....

Epoch 18/20
  Valid rate: 100.0% (16/16)
  Accuracy: 31.2%
  System prompt (184 chars): Act as a rigorous academic reviewer. Evaluate originality, methodology, results,...
  Human prefix (136 chars): Task: Predict whether the paper will be accepted or rejected. Please provide you...

Epoch 19/20
  Valid rate: 100.0% (16/16)
  Accuracy: 62.5%
  System prompt (210 chars): Act as a rigorous academic reviewer. Evaluate originality, methodology, results,...
  Human prefix (101 chars): Task: Predict the paper's acceptance status. Provide your answer as \boxed{Accep...

Epoch 20/20
  Valid rate: 100.0% (16/16)
  Accuracy: 68.8%
  System prompt (242 chars): Act as a rigorous academic reviewer. Assess the paper's originality, methodology...
  Human prefix (90 chars): Task: Predict the paper's acceptance status. Answer with \boxed{Accept} or \boxe...

============================================================
Final Evaluation
============================================================

Final metrics (best prompts, 50 samples):
  Valid rate: 100.0% (50/50)
  Accuracy: 66.0%
  Accept F1: 0.485
  Reject F1: 0.746

Best prompts saved to: results/best_prompts_20260117_033411.json
Optimization log saved to: results/optimization_log_20260117_033411.jsonl

============================================================
Best System Prompt:
============================================================
Act as a rigorous academic reviewer. Evaluate the paper's methodology, significance, clarity, and impact. Predict acceptance: \boxed{Accept} or \boxed{Reject}.

============================================================
Best Human Prefix:
============================================================
Classify the following paper as accepted or rejected. Answer with \boxed{Accept} or \boxed{Reject}.
[{"system_prompt": "Act as a rigorous academic reviewer. Evaluate the paper's methodology, significance, clarity, and impact. Predict acceptance: \\boxed{Accept} or \\boxed{Reject}.", "human_prefix": "Classify the following paper as accepted or rejected. Answer with \\boxed{Accept} or \\boxed{Reject}.", "best_valid_rate": 1.0, "best_accuracy": 0.75, "final_valid_rate": 1.0, "final_metrics": {"accuracy": 0.66, "num_valid": 50, "num_total": 50, "accept_f1": 0.48484848484848486, "reject_f1": 0.746268656716418, "num_accept_correct": 8, "num_reject_correct": 25, "num_accept_total": 19, "num_reject_total": 31}, "inference_model": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/saves/qwen2.5-3b/full/sft_ds3/checkpoint-3750", "gradient_model": "Qwen/Qwen3-14B", "num_epochs": 20, "timestamp": "20260117_033411"}, {"accuracy": 0.66, "num_valid": 50, "num_total": 50, "accept_f1": 0.48484848484848486, "reject_f1": 0.746268656716418, "num_accept_correct": 8, "num_reject_correct": 25, "num_accept_total": 19, "num_reject_total": 31}]

==============================================
Cleaning up...
Done!
==============================================
