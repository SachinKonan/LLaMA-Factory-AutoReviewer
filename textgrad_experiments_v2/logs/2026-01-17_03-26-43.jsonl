{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638415.8194196", "msecs": "819.0", "relativeCreated": "12649.917125701904", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# TAFS: Task-aware Activation Function Search for Graph Neural Networks\n\n# Abstract\nSince the inception of Graph Neural Networks (GNNs), extensive research efforts have concentrated on enhancing graph convolution, refining pooling operations, devising robust training strategies, and advancing theoretical foundations. Notably, one critical facet of current GNN research remains conspicuously underexplored\u2014the design of activation functions. Activation functions serve as pivotal components, imbuing GNNs with the essential capacity for non-linearity. Yet, the ubiquitous adoption of Rectified Linear Units (ReLU) persists. In our study, we embark on a mission to craft task-aware activation functions tailored for diverse GNN applications. We introduce TAFS (Task-aware Activation Function Search), an adept and efficient framework for activation function design. TAFS leverages a streamlined parameterization and frames the problem as a bi-level stochastic optimization challenge. To enhance the search for smooth activation functions, we incorporate additional Lipschitz regularization. Our approach automates the discovery of the optimal activation patterns, customizing them to suit any downstream task seamlessly. Crucially, this entire process unfolds end-to-end without imposing significant computational or memory overhead. Comprehensive experimentation underscores the efficacy of our method. We consistently achieve substantial improvements across a spectrum of tasks, including node classification over diverse graph data. Moreover, our approach surpasses state-of-the-art results in the realm of link-level tasks, particularly in biomedical applications.\n\n\n# 1 INTRODUCTION\n\n\nGraph Neural Networks (GNN) have demonstrated their prowess in modeling relationships within graph-structured data, as evidenced by their superior performance in various domains (Kipf & Welling, 2017; Velickovic et al., 2017; Hu et al., 2020; Xu et al., 2019). They have excelled in applications spanning biomedicine (Wu et al., 2023; Jiang et al., 2021), physical simulation (Sanchez-Gonzalez et al., 2020), material design (Reiser et al., 2022), sustainability (Donon et al., 2020), social network (Fan et al., 2019), transportation (Li et al., 2018b), recommendation (Wu et al., 2019), and more. Consequently, GNN models continue to captivate the attention of researchers across diverse scientific communities (Shi et al., 2020; Wang et al., 2022; Seo et al., 2020).\n\n\nDespite the extensive body of literature, we must highlight a significant gap in current research, specifically the design of activation functions, a fundamental component used in nearly every GNN model. While Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) is a prevalent choice for activation, it often falls short, as illustrated in Figure 1. Regrettably, GNN studies have hardly explored alternative activation functions. This oversight is critical, as the activation function plays a pivotal role in introducing non-linearity to GNNs. Without it, GNNs merely perform linear transformations on raw graph features.\n\n\nIn contrast, the Computer Vision community has spent decades exploring a wide array of manually\n\n\n![Figure 1: Activation function makes a big difference in GNN.](images/66c16489c3adf48e54dabed9ea1bf174d62a12679ee3776f2e4cfe1a7cc4106c.jpg)\n\n\ndesigned activation functions such as Sigmoid (LeCun et al., 1998), Tanh, ReLU (Nair & Hinton, 2010), and improved variants of ReLU (He et al., 2015; Clevert et al., 2016; Maas et al., 2013). However, transferring these manually crafted functions to different tasks poses challenges, and customizing new ones is a labor-intensive process. Furthermore, the marginal performance gains from human-designed functions diminish rapidly. To address this, researchers have proposed automated methods to discover tailored activation functions, which have demonstrated notable improvements in other network architectures like Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs) (Ramachandran et al., 2018; Eger et al., 2018; Farzad et al., 2019).\n\n\nHence, our research question is how can we design GNN activation functions to adapt effectively to various graph-based tasks, creating task-aware activation functions?\n\n\nAddressing this question poses two primary challenges. Challenge #1: Existing search algorithms are inefficient. Current activation function search methods suffer from over-parameterization and heavy computation. For example, APL (Agostinelli et al., 2015) introduces additional parameters for each neuron, which usually leads to at least ten times more parameters upon any model, thus significantly increasing the model complexity. Swish (Ramachandran et al., 2018) requires training a full network until convergence for each iteration, making it computationally burdensome. These issues render current search algorithms inefficient and less effective. Challenge #2: Current search methods lack support for non-differentiable objectives. GNN methods have wide applications regarding tasks of different levels (node, link, graph), many of which are evaluated by non-differentiable metrics. In the case of drug interaction prediction, we would like to know a certain positive (synergy) or negative (confliction) interaction exist. In fact, most drug pairs does not have positive or negative interactions. Receiver Operating Characteristic curve (ROC) which is not differentiable, is what we should use instead of accuracy. Similar application cases can be found in hit ratio of recommendation, latency optimization, hardware resources constraint, etc. Supporting these non-differentiable objectives would broaden the applicability of activation function search in diverse GNN tasks.\n\n\nIn this study, we embark on a systematic exploration of GNN activation function search\u2014a first of its kind. We frame this search as a bi-level optimization problem, with the inner level optimizing GNN parameters and the outer level optimizing activation function parameters. We propose an efficient search algorithm that navigates through a compact search space. This space is characterized by universal approximators with additional smoothness constraints, facilitating the rapid discovery of high-quality functions, thereby addressing Challenge #1. Additionally, we tackle Challenge #2 by jointly considering non-differentiable objectives and potential activation function constraints. We incorporate these elements into a stochastic relaxation of the outer level optimization, removing the need to compute gradients for non-differentiable metrics used in GNN tasks. Our algorithm undergoes extensive experimentation across various GNN models, datasets, and objectives, consistently outperforming existing activation functions. By overcoming Challenges #1 and #2, our algorithm achieves task-awareness in GNN activation function design.\n\n\nOur contributions can be summarized in three key points:\n\n- 1. To the best of our knowledge, we are the first to propose activation function search in the context of Graph Neural Networks. Our work serves as a catalyst, drawing attention to this critical aspect of GNN model design and paving the way for future investigations.\n\n- 2. We propose TAFS (Task-aware Activation Function Search), a probabilistic search algorithm capable of efficiently exploring a regularized functional space to discover novel activation functions tailored for diverse downstream tasks.\n\n- 3. Through comprehensive evaluations spanning node and link level tasks, we demonstrate that our algorithm enhances activation function design without requiring extensive manual effort and excels in optimizing non-differentiable objectives. We also conduct ablation studies to examine the searched activation functions, the impact of design choices, and algorithm efficiency.\n\n\n# 2 RELATED WORKS\n\n\nGraph Neural Networks. GNN is a power model in capturing relational information including typically GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2017), GIN (Xu et al., 2019), etc. Mathematically, in the context of GNN with a given network  $\\mathcal{G} = \\{\\mathcal{V},\\mathcal{E}\\}$  containing node set  $\\mathcal{V}$  and edge set  $\\mathcal{E}$ , the problem is formulated below:\n\n\n$$\n\n$$\nz _ {u} ^ {(l + 1)} = \\operatorname {U P D A T E} ^ {(l + 1)} \\left(h _ {u} ^ {(l)}, \\operatorname {A G G R E G A T E} ^ {(l)} \\left(\\left\\{h _ {u} ^ {(l)}, \\forall v \\in \\mathcal {N} (u) \\right\\}\\right)\\right), \\tag {1}\n$$\n\n$$\n\n\nwhere  $h_u^{(l)}$  is the latent representation of node  $u$  at layer  $l$  and  $z_u$  is the pre-activation of node  $u$ . AGGREGATE (abbr. Agg) and UPDATE (abbr. Up) are core modules of GNN, denoting the different message passing operations used across the model for collecting and updating representations. The latent representation (respectively pre-activation) for all nodes constitute  $\\mathbf{H}$  (resp.  $\\mathbf{Z}$ ) and then we have the activation transformation:  $\\mathbf{H}^{(l + 1)} = \\sigma^{(l + 1)}(\\mathbf{Z}^{(l + 1)})$ , where  $\\mathbf{Z}$  is activated by function  $\\sigma$  at the  $(l + 1)$ th layer.\n\n\nNumerous studies have been devoted to ameliorate it through different aspects. For example, GCN (Kipf & Welling, 2017) simplifies graph convolution and derives performant GCN networks. GAT (Velickovic et al., 2017) proposes graph attention as a replacement to model global features. GNN-Pretrain (Hu et al., 2020) studies pretraining strategies at the node level and graph level to make GNN model work for transferable tasks. GIN (Xu et al., 2019) understands the fundamental question of graph expressiveness by discriminating Weisfeiler-Lehman graph isomorphism. GNN Co-training (Li et al., 2018a) connects Laplacian smoothing with graph convolution and studies the problem of oversmoothing. However, almost every GNN model uses ReLU as the activation function (Kipf & Welling, 2017; Velickovic et al., 2017; Xu et al., 2018; Huang et al., 2020; KC et al., 2022; Xu et al., 2019), leaving GNN activation function a missing research piece.\n\n\nActivation Function Design. Since the early application of Sigmoid in Le-Net, activation functions have been considered as an important component until today (Hayou et al., 2019). In 2012, ReLU (Nair & Hinton, 2010) was proposed to train Boltzman Machines and soon extensively adopted in every neural network models. The study of activation function design happens mostly in CNN community, where a couple of milestone works include Swish (Ramachandran et al., 2018; Eger et al., 2018) and APL (Agostinelli et al., 2015). Swish proposes a Reinforcement Learning (RL)-based search algorithm to find appropriate activation functions in a discrete space. APL (Adaptive Piecewise Linear) uses linear hinge functions to approximate target patterns in a differentiable way. Comprehensive surveys of manual designed and parametric activation functions can be found in (Apicella et al., 2021; Dubey et al., 2022). Another notable work related to our research question is GReLU (Zhang et al., 2022), which tries to make GNN activation function adaptive by including graph convolution into the activation function. However, such design is not a typical univariate activation function. As a result, no work yet has proposed novel activation functions designed under the context of GNN.\n\n\n# 3 PROBLEM FORMULATION AND CHALLENGES\n\n\nOur research problem requires to propose a systematic way of designing adaptive activation functions that can be effectively integrated into GNN for downstream applications. Similar to Neural Architecture Search (NAS) (Liu et al., 2019), the activation function design could be modeled into a bi-level optimization problem:\n\n\n$$\n\n$$\n\\min  _ {\\alpha} \\mathcal {M} \\left(w ^ {*} (\\alpha), \\alpha ; \\mathcal {D} _ {\\text {v a l}}\\right) \\quad \\text {s . t .} \\quad w ^ {*} (\\alpha) = \\arg \\min  _ {w} \\mathcal {L} (w, \\alpha ; \\mathcal {D} _ {\\text {t r a i n}}), \\tag {2}\n$$\n\n$$\n\n\nwhere the inner level optimization learns  $w$  weight of GNN and the outer level optimization learns  $\\alpha$  weights of activation function. Both levels may have different objective metrics  $\\mathcal{M}, \\mathcal{L}$  depending on downstream applications.\n\n\nPrevious activation function search methods suffer from low efficiency (Challenge #1) and poor support of non-differentiable metrics (Challenge #2). On one hand, the efficiency bottleneck lies in the search space choice and search strategy design. The search space is crucial for search efficiency and requires careful consideration. The space should be proper both in candidate function number and effectiveness, making it a trade-off between quantity and quality. Then, the search strategy should be able to discover as quickly as possible the most suitable function candidate in the space. On the other hand, diverse GNN applications require that the algorithm is able to tackle any downstream target metric, whether or not differentiable.\n\n\nAll these issues prevent us from using off-the-shelf algorithms for GNN activation function search. To this end, we need a novel parameterization of the search space, that is jointly designed with search strategy to allow efficient search, and we need to deal with differentiable and non-differentiable target metrics at the same time to enable general applications in all kinds of GNN tasks.\n\n\n![Figure 2: Algorithm framework. We replace the activation function with sampled weights from Gaussian distributions. The inner level optimization is the learning of distribution parameter  $\\theta^{(1)},\\ldots ,\\theta^{(L)}$  and the outer level optimization is the learning of GNN weights. Both levels are iterated for faster convergence.](images/82c6262858b1d88a3b812ade0f6dbea69a2a33caa85d9e70103e76290b62aedd.jpg)\n\n\n# 4 THE PROPOSED METHOD\n\n\n# 4.1 ALGORITHM FRAMEWORK\n\n\nAs illustrated in Figure 2 and Table 1, we propose TAFS to solve both challenges in a unified way. We follow the bi-level optimization formulation and search activation function represented by learnable parameters. Specifically, a typical GNN network of L layers can be represented as below:\n\n\n$$\n\n$$\n\\sigma^ {(L)} \\circ \\mathbf {G L} ^ {(L)} \\dots \\sigma^ {(2)} \\circ \\mathbf {G L} ^ {(2)} \\circ \\sigma^ {(1)} \\circ \\mathbf {G L} ^ {(1)} (\\mathbf {X}), \\tag {3}\n$$\n\n$$\n\n\nwhere Graph Layer (GL) denotes all the AGGREGATE (Agg) and UPDATE (Up) operations related to graph,  $\\mathbf{X}$  is the initial graph features. Note that different layers could use different activation functions whereas current GNN models tend to fix the same ReLU for every layer.\n\n\nDenote by  $w_{\\sigma}$  all the parameters of activation functions, i.e.  $\\sigma^{(1)}, \\ldots, \\sigma^{(L)}$ , and denote by  $\\overline{w}_{\\sigma}$  all the parameters of GNN, i.e., parameters of  $\\mathrm{GL}^{(1)}, \\ldots, \\mathrm{GL}^{(L)}$ . We propose a continuous implicit functional space to parameterize  $w_{\\sigma}$ . This search space is expressive yet compact, with smoothness regularization induced by human prior. The parameter update process is stochastic to deal with any downstream objective especially non-differentiable metrics. The search algorithm is bi-level and end-to-end trained. The optimization step of the outer level (learning  $w_{\\sigma}$ ) and the inner level optimization (learning  $\\overline{w}_{\\sigma}$ ) are iterated. In the following parts, we explain in sequence the design of search space, stochastic relaxation and search algorithm.\n\n\n# 4.2 IMPLICIT FUNCTIONAL SEARCH SPACE\n\n\nIn order to facilitate activation function search, we propose a continuous implicit functional space that parameterizes the search space by universal approximators. This implicit functional space could be implemented by Multi-Layer Perception (MLP) to approximate target function. As in Figure 2, activation function parameters is equivalent to the parameters of MLP, denoted by  $w_{\\sigma}$ .\n\n\nIt's worth noting that we employ MLP as a representative example of universal approximators, chosen for its simplicity, while retaining generality. However, it's crucial to emphasize that alternative implementations, such as Gaussian Mixtures or Radial Basis Functions (RBF), are entirely feasible.\n\n\nIn addition, we focus on the smooth functions such that the searched activation functions will not change dramatically if the pre-activation value  $Z$  is slightly perturbed. Smooth functions are bounded by Lipschitz constant  $c$ , i.e.  $|f(x) - f(y)| \\leq c|x - y|$ . As a result, the functional search space is\n\n\nregularized by smoothness constraint  $\\{w_{\\sigma}|w_{\\sigma}\\in R^{|w_{\\sigma}|},c < \\gamma \\}$ , where  $c$  is the Lipschitz constant of the function parameterized by  $w_{\\sigma}$  and  $\\gamma$  is the hard limit on  $c$ . Here, we model the constraint on Lipschitz constant as a regularization term denoted by  $R(w_{\\sigma})$ . Many references design Lipschitz constant as additional soft metric to be trained together with any loss (Hoffman et al., 2019; Weng et al., 2018; Liu et al., 2022). We use Jacobian regularization (Hoffman et al., 2019) without loss of generality  $R(w_{\\sigma}) = ||J(x)||_F = \\{\\Sigma_{i,j}[J_{i,j}(x)]^2\\}$ , where  $J_{i,j}(x) = \\frac{\\partial h_i}{\\partial x_j} (x)$  is the Jacobian matrix.\n\n\nThis design of our functional space encourages the discovery of smooth functions characterized by small Lipschitz constants. Notably, this characteristic aligns with existing manually designed functions, such as ReLU, Tanh, Sigmoid, and Swish, all of which exhibit 1-Lipschitz properties.\n\n\n# 4.3 STOCHASTIC RELAXATION\n\n\nGNN applications are diverse that in many cases, the preferred evaluation metrics may not be differentiable. As mentioned in Table 1 and related works, APL could not deal with non-differentiable metrics but Swish could due to the RL-based search algorithm. However, the search efficiency of Swish is far from satisfactory.\n\n\nWe propose to use stochastic relaxation that re-parameterizes the search space  $(w_{\\sigma})$  with a Gaussian distribution  $p_{\\theta_{\\sigma}}(w_{\\sigma})$ . The Gaussian distribution has its own parameters  $\\theta_{\\sigma}$  and we sample the parameters of activation function  $w_{\\sigma}$  from the probability  $p_{\\theta_{\\sigma}}(w_{\\sigma})$  and optimize the probability parameters  $\\theta_{\\sigma}$  instead of  $w_{\\sigma}$ .\n\n\nFollowing (4), we replace  $\\sigma$  by  $w_{\\sigma}$  to emphasize the parameters on activation function; we replace  $w$  by  $\\overline{w}_{\\sigma}$  to denote the rest parameters of GNN model. Task objective  $\\mathcal{M}$  is jointly integrated into stochastic relaxation with space regularization  $\\mathcal{R}$ . The ultimate problem is formulated as below:\n\n\n$$\n\n$$\n\\theta_ {\\sigma} ^ {*} = \\underset {\\theta_ {\\sigma}} {\\arg \\min } \\left\\{\\mathcal {J} \\left(\\theta_ {\\sigma}\\right) \\equiv \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\theta_ {\\sigma}} \\left(w _ {\\sigma}\\right)} \\left[ \\mathcal {M} \\left(w _ {\\sigma}, \\bar {w _ {\\sigma}} ^ {*}; \\mathcal {D} _ {\\text {v a l}}\\right) + \\eta R \\left(w _ {\\sigma}\\right) \\right] \\right\\}, \\tag {4}\n$$\n\n$$\n\n\n$$\n\n$$\n\\text {s . t .} \\quad \\overline {{w _ {\\sigma}}} ^ {*} = \\underset {\\overline {{w _ {\\sigma}}}} {\\arg \\min } \\mathcal {L} (\\overline {{w _ {\\sigma}}}, w _ {\\sigma}; \\mathcal {D} _ {\\text {t r a i n}}),\n$$\n\n$$\n\n\nwhere  $w_{\\sigma}$  denotes the parameters of activation functions  $\\sigma$  with a regularization term  $R$  weighted by  $\\eta$ ,  $\\overline{w_{\\sigma}}$  represents GNN parameters,  $\\mathcal{L}$  is downstream task criterion of interest,  $\\mathcal{M}$  is upstream task criterion of interest, probably non-differentiable,  $\\theta_{\\sigma}$  represents the re-parameterization of  $w_{\\sigma}$  through Gaussian distribution, then the whole learning problem is optimized in a stochastic way.\n\n\nTo compute the target loss gradient with respect to probability parameters  $\\nabla_{\\theta_{\\sigma}}\\mathcal{J}(\\theta_{\\sigma})$ , we have the following proposition. The proof is given in Appendix A.\n\n\nProposition 1 Let  $w_{\\sigma} \\sim p_{\\theta_{\\sigma}}(w_{\\sigma})$  represent that the weights of activation functions are sampled from  $p_{\\theta_{\\sigma}}$ . We have\n\n\n$$\n\n$$\n\\begin{array}{l} \\begin{array}{l} \\nabla_ {\\theta_ {\\sigma}} \\mathcal {J} (\\theta_ {\\sigma}) = \\nabla_ {\\theta_ {\\sigma}} \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\theta_ {\\sigma}} (w _ {\\sigma})} [ \\mathcal {M} (w _ {\\sigma}, \\bar {w} _ {\\sigma} ^ {*}; \\mathcal {D} _ {v a l}) + \\eta R (w _ {\\sigma}) ] \\\\ - \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\sigma}} \\left[ \\left[ \\mathcal {M} (w _ {\\sigma}, \\bar {w} _ {\\sigma} ^ {*}; \\mathcal {D} _ {v a l}) \\right] \\log p _ {\\sigma} (w _ {\\sigma}) \\right] \\end{array} \\tag {5} \\\\ = \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\theta_ {\\sigma}} (w _ {\\sigma})} \\left[ \\left[ \\mathcal {M} \\left(w _ {\\sigma}, \\overline {{w _ {\\sigma}}} ^ {*}; \\mathcal {D} _ {v a l}\\right) + \\eta R \\left(w _ {\\sigma}\\right) \\right] \\nabla_ {\\theta_ {\\sigma}} \\log p _ {\\theta_ {\\sigma}} \\left(w _ {\\sigma}\\right) \\right] \\\\ \\end{array}\n$$\n\n$$\n\n\nWith the help of stochastic relaxation, the previously needed derivation of  $\\mathcal{M}$  is replaced by a multiplication between forward pass of  $\\mathcal{M}$  and a gradient of probability loss. In practice, this gradient expectation could be further approximated by Monte Carlo samplings, i.e.  $\\nabla_{\\theta_{\\sigma}}\\mathcal{J}(\\theta_{\\sigma})\\approx \\sum_{i = 1}^{K}\\nabla_{\\theta_{\\sigma}}\\log p_{\\theta_{\\sigma}}(w_{\\sigma}^{i})[\\mathcal{M}(w_{\\sigma}^{i},\\overline{w_{\\sigma}}^{*};\\mathcal{D}_{\\mathrm{val}}) + \\eta R(w_{\\sigma}^{i})]$ ,  $K$  is the sample number that we use to approximate the gradient. As a result, the differentiability requirement of  $\\mathcal{M}$  is removed.\n\n\n# 4.4 SEARCH STRATEGY\n\n\nAccording to (4), the learning is divided in two levels. The outer level optimizes probability parameters  $\\theta_{\\sigma}$  on validation dataset with (non-differentiable) metric  $\\mathcal{M}$ . Every time  $K$  number of samples are generated from the probability distribution (such as Gaussian). Each sample is forwarded and calculated according to (Prop. 1), whose average is an approximation of outer level loss gradient. The optimization of outer level parameters  $\\theta_{\\sigma}$  influences directly the value of activation function weights since the weights are sample from the updated probability every time a forward pass is needed. The inner level optimizes GNN parameters  $\\overline{w_{\\sigma}}$  on training dataset with metric  $\\mathcal{L}$ . It is similar to a normal training epoch of any network. The outer and inner levels are interplayed to accelerate convergence.\n\n\n```\n1: Initialize  $\\theta^0 = 1$ , initialize  $\\overline{w}_{\\sigma}$  by Xavier initialization and  $w_{\\sigma}$  randomly sampled from  $p_{\\theta^0}(w_\\sigma)$ .  \n2: for  $m = 0, \\dots, M - 1$  do  \n3: // Outer level optimization  \n4: Freeze GNN paramaters  $\\overline{w}_{\\sigma}$ ;  \n5: for  $k = 0, \\dots, K - 1$  do  \n6: Sample activation functions weights  $w_{\\sigma}^{k}$  from  $p_{\\theta^m}(w_\\sigma)$ ;  \n7: Forward inference of the whole network and accumulate stochastic loss  $\\mathcal{J}(\\theta_{\\sigma})$  as in Prop 1;  \n8: end for  \n9: Obtain  $\\nabla_{\\theta_{\\sigma}} \\mathcal{J}(\\theta_{\\sigma})$  by automatic differentiation and update  $\\theta^m$ ;  \n10: // Inner level optimization  \n11: Sample activation function parameters  $w_{\\sigma}$  from distribution  $p_{\\theta^m}(w_\\sigma)$  and freeze  $w_{\\sigma}$ ;  \n12: Forward inference of the whole network to obtain loss  $\\mathcal{L}$ ;  \n13: Update GNN parameters  $\\overline{w}_{\\sigma}$  by automatic differentiation;  \n14: end for  \n15: Training until convergence and obtain the final model parameter  $\\overline{w}_{\\sigma}^*$  and dist. parameter  $\\theta_{\\sigma}$ ;  \n16: return Final model parameter  $\\overline{w}_{\\sigma}^*$  and distribution parameter  $\\theta_{\\sigma}$ .\n```\n\n\nTable 1: Our proposed TAFS (Task-aware Activation Function Search) enables efficient differentiable search through a flexible and powerful MLP functional space. TAFS supports non-differentiable objective metrics in diverse GNN applications.\n\n\n<table><tr><td rowspan=\"2\">Search Method</td><td colspan=\"2\">Search Efficiency</td><td rowspan=\"2\">Non-Differentiable Metric</td></tr><tr><td>Search Space</td><td>Search Strategy</td></tr><tr><td>Swish</td><td>Discrete template choice</td><td>Reinforcement Learning</td><td>Applicable</td></tr><tr><td>APL</td><td>Explicit piecewise linear</td><td>Differentiable</td><td>Not applicable</td></tr><tr><td>TAFS (ours)</td><td>Continuous implicit MLP</td><td>Differentiable</td><td>Applicable</td></tr></table>\n\n\nThe complete TAFS algorithm is given in Algorithm 1. We also compare in Table 1 our proposed TAFS and literature methods. From the time efficiency perspective, Swish is the slowest in Table 1 because it optimizes a new network until convergence before the learning of RL controller. APL on the other hand, has a number of parameters dependent of base models due to its adaptability per neuron. As a result, TAFS enjoys a compact search space without over-parameterization and has superior efficiency in searching. Empirical results are given in Table 4.\n\n\n# 5 EXPERIMENTS\n\n\nIn this section, we experiment our methods on diverse GNN applications including node classification and link prediction, in order to fully evaluation the methods on differentiable and non-differentiable metrics. Later, we provide detailed analysis on search efficiency and hyperparameter impact. All our experiments are run on single NVIDIA RTX 3090.\n\n\n# 5.1 NODE CLASSIFICATION\n\n\nDatasets. We experiment on diverse graph datasets for node tasks, including Cora and DBLP for paper classification based on reference network, Cornell and Texas for webpage classification from university network, and Chameleon for wikipedia page classification based on hyperlink network. Statistics are in Appx. E. The task metric here is classification accuracy.\n\n\nBaselines. To fairly compare different activation functions, we compare our searchable activation functions with manually designed ones or previously searched function. Some of these activation functions are visualized in Figure 3(a). For each dataset and baseline chosen, we evaluate on two aggregation layers (GCN and GraphSage) and five network connection topologies (stack, residual, dense, jump knowledge, mixhop). Each model has four layers of aggregation layers. The model is trained for 400 epochs.\n\n\nTable 2: Overall node classification improvement of different models on different datasets. Metric is classification accuracy. Avg. Imp. is the improvement of TAFS with respect to the other choices averaged over all the datasets.\n\n\n<table><tr><td colspan=\"2\">Model</td><td>Activation</td><td>Cora</td><td>DBLP</td><td>Cornell</td><td>Texas</td><td>Chameleon</td><td>Avg. Imp.</td></tr><tr><td rowspan=\"40\">GCN</td><td rowspan=\"5\">Stack</td><td>ReLU</td><td>83.06\u00b10.66</td><td>84.63\u00b10.21</td><td>56.76\u00b15.92</td><td>60.54\u00b16.42</td><td>61.60\u00b11.75</td><td>\u2191 2.8%</td></tr><tr><td>Tanh</td><td>84.82\u00b10.51</td><td>85.58\u00b10.15</td><td>56.49\u00b15.19</td><td>57.84\u00b15.01</td><td>61.51\u00b11.88</td><td>\u2191 3.2%</td></tr><tr><td>L-ReLU</td><td>84.57\u00b10.93</td><td>84.50\u00b10.40</td><td>57.38\u00b12.16</td><td>60.54\u00b17.37</td><td>61.95\u00b12.18</td><td>\u2191 2.1%</td></tr><tr><td>Swish</td><td>83.88\u00b10.81</td><td>84.89\u00b10.34</td><td>57.30\u00b13.97</td><td>58.65\u00b15.55</td><td>58.33\u00b11.68</td><td>\u2191 4.1%</td></tr><tr><td>TAFS</td><td>89.08\u00b10.48</td><td>86.24\u00b10.17</td><td>57.37\u00b14.37</td><td>62.11\u00b15.48</td><td>62.31\u00b11.82</td><td>-</td></tr><tr><td rowspan=\"5\">Residual</td><td>ReLU</td><td>85.13\u00b10.95</td><td>84.45\u00b10.34</td><td>57.84\u00b15.43</td><td>57.84\u00b15.95</td><td>66.93\u00b12.17</td><td>\u2191 3.2%</td></tr><tr><td>Tanh</td><td>86.02\u00b10.55</td><td>85.63\u00b10.14</td><td>58.38\u00b14.39</td><td>57.57\u00b15.93</td><td>68.86\u00b11.84</td><td>\u2191 2.0%</td></tr><tr><td>L-ReLU</td><td>86.60\u00b10.72</td><td>84.97\u00b10.33</td><td>55.68\u00b18.30</td><td>57.84\u00b16.75</td><td>67.50\u00b11.48</td><td>\u2191 3.3%</td></tr><tr><td>Swish</td><td>85.86\u00b10.64</td><td>84.67\u00b10.19</td><td>56.22\u00b16.14</td><td>60.54\u00b17.66</td><td>66.29\u00b12.12</td><td>\u2191 2.8%</td></tr><tr><td>TAFS</td><td>88.16\u00b10.58</td><td>86.29\u00b10.18</td><td>58.20\u00b14.80</td><td>60.22\u00b15.51</td><td>70.49\u00b11.64</td><td>-</td></tr><tr><td rowspan=\"5\">JKNet</td><td>ReLU</td><td>86.86\u00b10.71</td><td>84.99\u00b10.25</td><td>76.49\u00b17.36</td><td>77.57\u00b17.36</td><td>58.18\u00b11.63</td><td>\u2191 3.8%</td></tr><tr><td>Tanh</td><td>86.41\u00b10.57</td><td>85.57\u00b10.20</td><td>68.92\u00b16.76</td><td>65.95\u00b19.22</td><td>60.20\u00b12.19</td><td>\u2191 9.1%</td></tr><tr><td>L-ReLU</td><td>87.45\u00b10.51</td><td>85.04\u00b10.15</td><td>74.05\u00b15.57</td><td>76.49\u00b18.80</td><td>57.98\u00b12.36</td><td>\u2191 4.1%</td></tr><tr><td>Swish</td><td>86.34\u00b10.92</td><td>84.95\u00b10.28</td><td>77.03\u00b15.57</td><td>78.11\u00b16.99</td><td>57.00\u00b12.54</td><td>\u2191 4.7%</td></tr><tr><td>TAFS</td><td>88.84\u00b10.56</td><td>87.07\u00b10.22</td><td>81.35\u00b16.40</td><td>81.08\u00b15.01</td><td>60.21\u00b12.04</td><td>-</td></tr><tr><td rowspan=\"5\">Mixhop</td><td>ReLU</td><td>85.31\u00b10.64</td><td>85.10\u00b10.18</td><td>73.78\u00b15.55</td><td>74.05\u00b19.53</td><td>51.64\u00b12.24</td><td>\u2191 2.7%</td></tr><tr><td>Tanh</td><td>85.15\u00b10.67</td><td>85.12\u00b10.30</td><td>72.97\u00b17.55</td><td>76.76\u00b16.86</td><td>50.59\u00b12.60</td><td>\u2191 2.6%</td></tr><tr><td>L-ReLU</td><td>86.38\u00b10.50</td><td>85.01\u00b10.17</td><td>72.43\u00b16.14</td><td>72.70\u00b15.05</td><td>51.36\u00b12.80</td><td>\u2191 3.4%</td></tr><tr><td>Swish</td><td>86.21\u00b11.03</td><td>85.43\u00b10.25</td><td>72.34\u00b18.18</td><td>74.86\u00b16.51</td><td>51.89\u00b12.10</td><td>\u2191 2.5%</td></tr><tr><td>TAFS</td><td>88.77\u00b10.57</td><td>86.18\u00b10.17</td><td>75.14\u00b15.38</td><td>78.43\u00b15.28</td><td>52.17\u00b11.97</td><td>-</td></tr><tr><td rowspan=\"5\">Stack</td><td>ReLU</td><td>83.06\u00b10.66</td><td>83.67\u00b10.41</td><td>58.11\u00b16.19</td><td>70.00\u00b16.78</td><td>47.02\u00b14.20</td><td>\u2191 12.1%</td></tr><tr><td>Tanh</td><td>84.82\u00b10.51</td><td>84.90\u00b10.19</td><td>68.65\u00b16.75</td><td>71.89\u00b17.85</td><td>53.50\u00b11.68</td><td>\u2191 4.3%</td></tr><tr><td>L-ReLU</td><td>84.57\u00b10.65</td><td>84.16\u00b10.23</td><td>62.16\u00b15.92</td><td>68.11\u00b17.23</td><td>49.21\u00b13.02</td><td>\u2191 9.8%</td></tr><tr><td>Swish</td><td>81.53\u00b10.74</td><td>83.62\u00b10.50</td><td>57.03\u00b16.45</td><td>68.65\u00b16.19</td><td>48.42\u00b12.17</td><td>\u2191 13.0%</td></tr><tr><td>TAFS</td><td>87.08\u00b10.48</td><td>85.22\u00b10.30</td><td>72.43\u00b17.23</td><td>74.51\u00b16.92</td><td>58.57\u00b11.20</td><td>-</td></tr><tr><td rowspan=\"5\">Residual</td><td>ReLU</td><td>84.11\u00b10.82</td><td>83.05\u00b10.33</td><td>65.95\u00b16.64</td><td>73.51\u00b16.71</td><td>55.02\u00b12.73</td><td>\u2191 6.1%</td></tr><tr><td>Tanh</td><td>85.62\u00b10.52</td><td>85.22\u00b10.17</td><td>72.43\u00b13.97</td><td>78.11\u00b19.00</td><td>59.17\u00b11.80</td><td>\u2191 0.6%</td></tr><tr><td>L-ReLU</td><td>85.63\u00b10.42</td><td>84.05\u00b10.21</td><td>71.89\u00b13.67</td><td>74.86\u00b15.80</td><td>55.86\u00b11.83</td><td>\u2191 3.1%</td></tr><tr><td>Swish</td><td>84.97\u00b10.79</td><td>84.17\u00b10.43</td><td>71.08\u00b14.02</td><td>75.41\u00b17.09</td><td>54.17\u00b11.44</td><td>\u2191 3.9%</td></tr><tr><td>TAFS</td><td>89.10\u00b10.385</td><td>85.22\u00b10.17</td><td>73.38\u00b13.63</td><td>77.03\u00b16.86</td><td>58.62\u00b12.08</td><td>-</td></tr><tr><td rowspan=\"5\">JKNet</td><td>ReLU</td><td>85.29\u00b10.56</td><td>83.97\u00b10.15</td><td>80.00\u00b16.07</td><td>81.62\u00b15.10</td><td>56.78\u00b11.62</td><td>\u2191 3.0%</td></tr><tr><td>Tanh</td><td>86.01\u00b10.51</td><td>85.25\u00b10.18</td><td>77.03\u00b15.57</td><td>78.92\u00b16.14</td><td>57.68\u00b11.92</td><td>\u2191 3.8%</td></tr><tr><td>L-ReLU</td><td>85.90\u00b10.42</td><td>85.01\u00b10.25</td><td>80.27\u00b16.84</td><td>81.35\u00b14.75</td><td>57.41\u00b12.01</td><td>\u2191 2.5%</td></tr><tr><td>Swish</td><td>85.56\u00b10.61</td><td>84.71\u00b10.22</td><td>77.13\u00b15.30</td><td>81.06\u00b15.43</td><td>55.00\u00b11.93</td><td>\u2191 4.5%</td></tr><tr><td>TAFS</td><td>89.51\u00b10.66</td><td>86.73\u00b10.20</td><td>81.79\u00b15.08</td><td>82.10\u00b15.16</td><td>59.37\u00b11.53</td><td>-</td></tr><tr><td rowspan=\"5\">Mixhop</td><td>ReLU</td><td>84.55\u00b11.07</td><td>84.23\u00b10.21</td><td>75.95\u00b19.59</td><td>81.63\u00b14.80</td><td>54.19\u00b12.15</td><td>\u2191 2.3%</td></tr><tr><td>Tanh</td><td>84.82\u00b10.65</td><td>84.21\u00b10.38</td><td>78.11\u00b16.99</td><td>81.89\u00b15.80</td><td>53.20\u00b11.21</td><td>\u2191 2.0%</td></tr><tr><td>L-ReLU</td><td>84.80\u00b11.10</td><td>84.34\u00b10.28</td><td>76.22\u00b18.36</td><td>77.30\u00b14.71</td><td>53.14\u00b11.72</td><td>\u2191 3.7%</td></tr><tr><td>Swish</td><td>84.18\u00b10.55</td><td>84.69\u00b10.30</td><td>75.95\u00b18.15</td><td>80.27\u00b16.74</td><td>53.20\u00b11.86</td><td>\u2191 3.0%</td></tr><tr><td>TAFS</td><td>87.77\u00b11.40</td><td>85.30\u00b10.24</td><td>77.77\u00b14.39</td><td>83.70\u00b14.05</td><td>55.07\u00b10.57</td><td>-</td></tr></table>\n\n\nResults. We provide in Table 2 the results of node classification tasks. The improvement of TAFS with respect to the other function choices is significant. Note that the improvements are observable across different graph data and GNN models, showing that TAFS is task-aware to graphs in citation, university webpage, wikipedia link graph, etc.\n\n\n# 5.2 MOLECULE AND PROTEIN INTERACTION PREDICTION\n\n\nDatasets. Biomedical graph are one of the most active and effective application areas of GNN. Biomedical GNN has accelerated important studies in protein prediction, molecule generation, gene\n\n\nexpression, etc. We consider the link prediction that is a typical task in molecule and protein interaction prediction. Specifically, we consider Drug-Drug Interaction (DDI), Drug-Target Interaction (DTI), Protein-Protein Interaction (PPI) and Disea-Gene Association (DGA). The statistics of the four datasets are provided in Appx. Table 6.\n\n\nBaselines. We adopt two biomedical graph baselines SkipGNN (Huang et al., 2020) and HOGCN (KC et al., 2022). SkipGNN proposes a general GNN architecture to model molecular interactions and works well on all these biomedical tasks. We use both as base model and experiment TAFS to replace the activation functions. The training hyperparameters are the same as in the original work.\n\n\nTable 3: Drug and protein interaction predictions.\n\n\n<table><tr><td>Task</td><td>Model</td><td>Activation</td><td>ROCAUC</td><td>PRAUC</td></tr><tr><td rowspan=\"6\">Drug-Target Interaction</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.922\u00b10.004</td><td>0.928\u00b10.006</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.933\u00b10.002</td><td>0.934\u00b10.001</td></tr><tr><td>TAFS</td><td>0.952\u00b10.001</td><td>0.954\u00b10.001</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.927\u00b10.001</td><td>0.929\u00b10.001</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.923\u00b10.002</td><td>0.922\u00b10.001</td></tr><tr><td>TAFS</td><td>0.943\u00b10.002</td><td>0.940\u00b10.001</td></tr><tr><td rowspan=\"6\">Drug-Drug Interaction</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.886\u00b10.003</td><td>0.866\u00b10.006</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.890\u00b10.002</td><td>0.874\u00b10.001</td></tr><tr><td>TAFS</td><td>0.911\u00b10.002</td><td>0.898\u00b10.003</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.898\u00b10.002</td><td>0.881\u00b10.003</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.897\u00b10.002</td><td>0.901\u00b10.002</td></tr><tr><td>TAFS</td><td>0.917\u00b10.002</td><td>0.901\u00b10.001</td></tr><tr><td rowspan=\"6\">Protein-Protein Interaction</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.917\u00b10.004</td><td>0.921\u00b10.003</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.920\u00b10.001</td><td>0.922\u00b10.002</td></tr><tr><td>TAFS</td><td>0.927\u00b10.001</td><td>0.937\u00b10.002</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.919\u00b10.001</td><td>0.922\u00b10.002</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.919\u00b10.002</td><td>0.924\u00b10.001</td></tr><tr><td>TAFS</td><td>0.923\u00b10.003</td><td>0.929\u00b10.002</td></tr><tr><td rowspan=\"6\">Disease-Gene Association</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.912\u00b10.004</td><td>0.915\u00b10.003</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.916\u00b10.001</td><td>0.920\u00b10.001</td></tr><tr><td>TAFS</td><td>0.930\u00b10.001</td><td>0.940\u00b10.001</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.927\u00b10.001</td><td>0.934\u00b10.001</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.929\u00b10.002</td><td>0.933\u00b10.001</td></tr><tr><td>TAFS</td><td>0.933\u00b10.001</td><td>0.942\u00b10.002</td></tr></table>\n\n\nResults. We provide in Table 3 the results of four link prediction tasks. Again, both SkipGNN and HOGCN use ReLU by default. With TAFS, SkipGNN and HOGCN has gained significant performance evaluated in ROCAUC and PRAUC, two non-differentiable metrics. Furthermore, when TAFS is integrated with SkipGNN, a model from 2020, it outperforms HOGCN, the state-of-the-art model from 2022. This underscores the significance of activation function search, which has hitherto been overlooked in the GNN community.\n\n\n# 5.3 ABLATION STUDY\n\n\nTo further analyze our proposed TAFS algorithm, we provide additional experiments to illustrate the search results, search efficiency and hyperparameter impact.\n\n\nVisualization of activation function search. We show in Figure 3 the searched activation functions from the literature methods and TAFS. It can be observed that TAFS could find diverse activation functions different than manually design ones or the searched ones by Swish and APL. Moreover, TAFS learns layer-wise activation function, leading to different behaviours of functions in different layers as in Figure 3(b)(c). Deeper layers' activation functions are smoother than shallow layers.\n\n\nSearch efficiency The modeling differences between literature search methods and TAFS are given in Table 1. In this part, we provide more empirical details of the search efficiency comparison in Table 4. TAFS has a significantly smaller consumption of extra memory and shorter running time. This huge efficiency improvement is credited to TAFS' compact MLP functional search space and differentiable search strategy, making TAFS' extra parameters independent of base models, whichever dataset or\n\n\n![(a) Literature search algorithms.](images/af3d2dc16af32136e39f596dae27e88d65ea40aedbff3b999636f48c35dcb38d.jpg)\n\n\n![(b) Results on dataset Cora Figure 3: Literature activation functions and searched activation results. (b)(c) include searched results on different layers across two datasets.](images/b11507cfba3258b75da195ae2cbcfaf11e5b524c873c56d4279762d8fdcc6044.jpg)\n\n\n![(c) Results on dataset DBLP](images/7e13fc6da84ed4db609d8a276c20829e94faac99bcbe6f07484b3c5b685003e0.jpg)\n\n\nGNN model (as long as the model has the same number of activation functions), while APL models each neuron with a piecewise linear unit, leading to over 2000 times more parameters than TAFS.\n\n\nHyperparameters impact. The choice of hyperparameters significantly affects performance. TAFS introduces two sets of hyperparameters: the number of samples (K) and the selection of the MLP architecture. We present their effects in Figure 4. The number of samples (K) in stochastic optimization exhibits a consistent increasing trend, representing a trade-off between accuracy and computational time. Regarding MLP hyperparameters, we analyze their impact on two node tasks, DBLP and Cornell, using nine different configurations: depths ranging from two to four layers and widths spanning from 10 to 1000 neurons. It is evident that a very small MLP (e.g., two layers with 10 neurons) is inadequate for\n\n\nmodeling adaptive activation functions. However, the distinctions between other choices are negligible. Given that deeper and wider MLPs require significantly more parameters, we opt for a two-layer MLP with 100 hidden units in all other experiments.\n\n\nTable 4: Search efficiency comparison.\n\n\n<table><tr><td>Dataset</td><td>Model</td><td>Parameters</td><td>Time(min)</td></tr><tr><td rowspan=\"4\">DBLP</td><td>Base</td><td>420K</td><td>0.15</td></tr><tr><td>Swish</td><td>+340K (+82%)</td><td>350</td></tr><tr><td>APL</td><td>+2400K (+575%)</td><td>4</td></tr><tr><td>TAFS</td><td>+1.3K (+0.3%)</td><td>1</td></tr><tr><td rowspan=\"4\">Chameleon</td><td>Base</td><td>315K</td><td>1.2</td></tr><tr><td>Swish</td><td>+420K (+108%)</td><td>2990</td></tr><tr><td>APL</td><td>+1760K (+558%)</td><td>33</td></tr><tr><td>TAFS</td><td>+1.3K (+0.4%)</td><td>11</td></tr><tr><td rowspan=\"4\">Ogbg-Molhiv</td><td>Base</td><td>27M</td><td>1020</td></tr><tr><td>Swish</td><td>-</td><td>&gt;70 days</td></tr><tr><td>APL</td><td>OOM (+150M)</td><td>-</td></tr><tr><td>TAFS</td><td>+12K</td><td>1380</td></tr></table>\n\n\n![(a) Number of samples in stochastic optimization. Larger K is lawways better but slower.](images/1b4fb305a40747e0fdddae0ea5b9af8243af20e143813ef2b16d61f37abbd836.jpg)\n\n\n![(b) MLP dimension impacts on DBLP dataset. Figure 4: Hyperparameter impact of number of samples in stochastic relaxation and the impact of MLP dimensions. In (b)(c), deeper color means better performance.](images/ec9501441f0a88918d11f9983df6b4dce431e54dd07be173b545a963595dad4d.jpg)\n\n\n![(c) MLP dimension impacts on Cornell dataset.](images/18218eff81e36c61f38c170a3e6f2b484b0ed369de546c8895363ea2e4c9c7b6.jpg)\n\n\n# 6 CONCLUSION\n\n\nIn a word, we achieve a task-aware activation function search in GNN through an expressive and compact representation of search space, stochastic relaxation with reparameterization, which are carefully co-designed with search strategy. Our search space is inclusive and parameter efficient, including appropriate number of high-quality functions. The search strategy is end-to-end trained and every operation of the framework is differentiable. Finally, the stochastic relaxation is capable of dealing with any metric of interest, closing the optimization gap.\n\n\n# REFERENCES\n\n- Forest Agostinelli, Matthew D. Hoffman, Peter J. Sadowski, and Pierre Baldi. Learning activation functions to improve deep neural networks. In International Conference on Learning Representations, ICLR Workshop, 2015.\n\n- Andrea Apicella, Francesco Donnarumma, Francesco Isgro, and Roberto Prevete. A survey on modern trainable activation functions. Neural Networks, 138:14-32, 2021.\n\n- Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). In International Conference on Learning Representations, ICLR, 2016.\n\n- Balthazar Donon, Zhengying Liu, Wenzhuo Liu, Isabelle Guyon, Antoine Marot, and Marc Schoenauer. Deep statistical solvers. In Neural Information Processing Systems, NeurIPS, 2020.\n\n- Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Activation functions in deep learning: A comprehensive survey and benchmark. Neurocomputing, 503:92-108, 2022.\n\n- Steffen Eger, Paul Youssef, and Iryna Gurevych. Is it time to swish? comparing deep learning activation functions across NLP tasks. In Empirical Methods in Natural Language Processing, Brussels, EMNLP, 2018.\n\n- Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In World Wide Web Conference, WWW, pp. 417-426. ACM, 2019.\n\n- Amir Farzad, Hoda Mashayekhi, and Hamid Hassanpour. A comparative performance analysis of different activation functions in LSTM networks for classification. Neural Computing and Applications, 31:2507-2521, 2019.\n\n- Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function on deep neural networks training. In International Conference on Machine Learning, ICML, volume 97, pp. 2672-2680, 2019.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In International Conference on Computer Vision, ICCV, 2015.\n\n- Judy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust learning with jacobian regularization. CoRR, abs/1908.02729, 2019.\n\n- Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In International Conference on Learning Representations, ICLR, 2020.\n\n- Kexin Huang, Cao Xiao, Lucas M Glass, Marinka Zitnik, and Jimeng Sun. Skipgnn: predicting molecular interactions with skip-graph networks. *Scientific reports*, 10(1):1-16, 2020.\n\n- Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang, Chao Shen, Dongsheng Cao, Jian Wu, and Tingjun Hou. Could graph neural networks learn better molecular representation for drug discovery? a comparison study of descriptor-based and graph-based models. Journal of Cheminformatics, 2021.\n\n- K. KC, R. Li, F. Cui, and A. R. Haake. Predicting biomedical interactions with higher-order graph convolutional networks. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(02), mar 2022. ISSN 1557-9964. doi: 10.1109/TCBB.2021.3059415.\n\n- Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulouse, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 1998.\n\n- Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), AAAI, 2018a.\n\n- Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. In International Conference on Learning Representations, ICLR, 2018b.\n\n- Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.\n\n- Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In International Conference on Learning Representations, ICLR, 2019.\n\n- Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, and Or Litany. Learning smooth neural functions via lipschitz regularization. In SIGGRAPH. ACM, 2022.\n\n- Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rectifier nonlinearities improve neural network acoustic models. In ICML. Atlanta, GA, 2013.\n\n- Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Johannes Furnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 807-814. Omnipress, 2010.\n\n- Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. In International Conference on Learning Representations, ICLR, 2018.\n\n- Patrick Reiser, Marlen Neubert, Andr\u00e9 Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer, and Pascal Friederich. Graph neural networks for materials science and chemistry. Communications Materials, 3(1), 2022. doi: 10.1038/s43246-022-00315-6. URL https://doi.org/10.1038/s43246-022-00315-6.\n\n- Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W. Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, ICML, 2020.\n\n- Sungyong Seo, Chuizheng Meng, and Yan Liu. Physics-aware difference graph networks for sparsely-observed dynamics. In International Conference on Learning Representations, ICLR, 2020.\n\n- Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. In International Conference on Learning Representations, ICLR, 2020.\n\n- Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. CoRR, abs/1710.10903, 2017.\n\n- Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. Comenet: Towards complete and efficient message passing for 3d molecular graphs. In NeurIPS, 2022.\n\n- Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. In International Conference on Learning Representations, ICLR, 2018.\n\n- Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-based recommendation with graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, 2019.\n\n- Zhenxing Wu, Jike Wang, Hongyan Du, Dejun Jiang, Yu Kang, Dan Li, Peichen Pan, Yafeng Deng, Dongsheng Cao, Chang-Yu Hsieh, and Tingjun Hou. Chemistry-intuitive explanation of graph neural networks for molecular property prediction with substructure masking. Nature Communications, 2023.\n\n- Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, ICML, volume 80 of Proceedings of Machine Learning Research, pp. 5449-5458. PMLR, 2018.\n\n- Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, ICLR, 2019.\n\n- Yifei Zhang, Hao Zhu, Ziqiao Meng, Piotr Koniusz, and Irwin King. Graph-adaptive rectified linear unit for graph neural networks. In The ACM Web Conference, WWW, 2022.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638416.8744354", "msecs": "874.0", "relativeCreated": "13704.932928085327", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Numerical Pitfalls in Policy Gradient Updates\n\n# Abstract\nNumerical instability, such as gradient explosion, is a fundamental problem in practical deep reinforcement learning (DRL) algorithms. Beyond anecdotal debugging heuristics, there is a lack of systematic understanding of the causes for numerical sensitivity that leads to exploding gradient failures in practice. In this work, we demonstrate that the issue arises from the ill-conditioned density ratio in the surrogate objective that comes from importance sampling, which can take excessively large values during training. Perhaps surprisingly, while various policy optimization methods such as TRPO and PPO prevent excessively large policy updates, their optimization constraints on KL divergence and probability ratio cannot guarantee numerical stability. This also explains why gradient explosion often occurs during DRL training, even with code-level optimizations. To address this issue, we propose the Vanilla Policy Gradient with Clipping algorithm, which replaces the importance sampling ratio with its logarithm. This approach effectively prevents gradient explosion while achieving performance comparable to PPO.\n\n\n# 1 INTRODUCTION\n\n\nDeep reinforcement learning (DRL) has demonstrated its effectiveness in various domains (Mnih et al., 2015; Silver et al., 2017; Vinyals et al., 2019; Ouyang et al., 2022). Despite these successes, the reliability and numerical stability of DRL algorithms remain fundamental limitations that hinder their application in real-world environments. In particular, DRL algorithms are often observed to have numerical instability issues, such as exhibiting the major problem of gradient explosion that destroys the learning progress. Existing work on the brittleness of DRL methods attributes this to either the high variance in actions (Fujita & Maeda, 2018), the gradient estimator (Liu et al., 2020), or the sensitivity to hyperparameters (Henderson et al., 2018).\n\n\nIn this paper, we perform a rigorous analysis of the root cause of numerical instability in policy optimization algorithms such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) (Schulman et al., 2015a; 2017). Gradient explosion is a consequence of overflows caused by computations involving excessively large numbers that exceed the limits of floating-point arithmetic. We show that the long horizons and large value variance are not the core reason for overflow, especially under action clipping as commonly used in standard DRL implementations (Duan et al., 2016; Raffin et al., 2021; Bou et al., 2024). Another popular explanation is that the covariance matrix of the stochastic policy can become singular during training, leading to gradient explosion. However, our experiments in Appendix B show that arithmetic overflows can occur even when the standard deviations are lower-bounded. Instead, our analysis indicates that the root cause is the importance sampling steps in TRPO and PPO (Schulman et al., 2015a; 2017), where the probability ratio can take exponentially large values, especially in the case of Gaussian distributions. Specifically, we analyze the condition number of the probability ratio, and demonstrate that the probability ratio is exponential with respect to a line integral in the policy space, which can lead to arithmetic overflows and cause gradient explosion.\n\n\nIn particular, we demonstrate that optimization constraints proposed in TRPO/PPO, such as limiting the KL-divergence and clipping, do not prevent numerical instability. We show that the probability ratio in importance sampling can grow exponentially fast, ultimately causing arithmetic overflows, even while the KL divergence remains small. This indicates that various policy optimization methods that prevent excessively large policy updates do not prevent numerical problems. Indeed, the probability ratios are not guaranteed to stay small, especially when the policy network's output is\n\n\nfurther clipped in the action space, leading to excessively small values in the probability density functions and large condition numbers in the density ratio.\n\n\nGiven that importance sampling is the root cause of the exploding gradient issue, we propose an algorithm called Vanilla Policy Gradient with Clipping, a modified version of the PPO algorithm in which the importance sampling ratio is replaced with its logarithm. We conduct experiments on continuous-control benchmarks to evaluate its effectiveness. Specifically, we find that this algorithm performs comparably to PPO on small policy networks where no gradient explosion occurs and maintains good performance on large policy networks where PPO encounters numerical issues. Additionally, the influence of other code-level techniques, such as reward scaling and learning rates, is discussed in Appendix B.\n\n\nOverall, our findings suggest that the TRPO/PPO loss is inherently ill-conditioned due to importance sampling and action clipping, which directly contribute to the numerical instability of deep policy gradient methods. This issue should not be viewed merely as a hyperparameter tuning problem but as a fundamental limitation of DRL algorithms that requires more attention.\n\n\n# 2 RELATED WORK\n\n\nDeep policy gradients in practice. It has been reported that implementation details and code-level optimizations fundamentally impact the performance of deep policy gradient algorithms such as TRPO and PPO (Henderson et al., 2018; Engstrom et al., 2020; Andrychowicz et al., 2021). In particular, empirical studies have shown that the most significant discrepancies between theory and implementation arise in gradient estimation, value prediction, and optimization landscapes (Ilyas et al., 2020). These findings were later supported by theoretical studies, which indicated that they stem from the fractal structures present in both value and policy landscapes (Wang et al., 2023). Our work contributes to this line of research by providing an in-depth study on the numerical stability of deep policy gradient methods.\n\n\nExploding gradients in deep learning. Many neural network architectures have been reported to suffer from the gradient explosion issue, including deep multi-layer networks (Bengio et al., 1994; Glorot & Bengio, 2010) and residual neural networks (RNNs) (Pascanu et al., 2013). This problem is closely linked to the depth of the network architecture (Philipp et al., 2017; Schoenholz et al., 2017), which can result in a long chain of Jacobian multiplications. In the context of reinforcement learning (RL), Liu et al. (2018) points out that the product of density ratios over a long horizon can grow exponentially, leading to exploding variance in off-policy estimation. Also, Khorasani et al. (2023) proposes to use second-order information in the form of Hessian vector products to bypass importance sampling weights in policy gradient. In this work, we demonstrate that even a single density ratio can take excessively large values and cause numerical overflow, and propose an algorithm that drops the importance sampling from PPO objective while achieving comparable performance. Wang et al. (2024) investigates gradient explosion in the context of generative adversarial imitation learning (GAIL), suggesting that a large reward function can cause exploding gradients. However, our empirical results in Section B indicate that this does not fully explain gradient explosion in policy gradient algorithms, as scaling down the reward does not completely overcome the issue.\n\n\n# 3 NUMERICAL INSTABILITY CAUSED BY IMPORTANCE SAMPLING\n\n\nTRPO and PPO objective. The original policy gradient estimator has the form\n\n\n$$\n\n$$\n\\nabla \\mathcal {J} (\\theta) = \\mathbb {E} _ {\\left(s _ {t}, a _ {t}\\right) \\sim \\pi_ {\\theta}} \\left[ \\nabla_ {\\theta} \\log \\pi_ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) Q ^ {\\pi} \\left(s _ {t}, a _ {t}\\right) \\right], \\tag {1}\n$$\n\n$$\n\n\nwhere  $Q^{\\pi}$  is the  $Q$ -function of the current policy  $\\pi_{\\theta}$  (Sutton et al., 1999). One difficulty of directly optimizing equation 1 is due to the complex dependency of sampled data on  $\\pi_{\\theta}$  (Schulman et al., 2015a). To address it, TRPO proposes a surrogate objective that incorporates importance sampling:\n\n\n$$\n\n$$\n\\max  _ {\\theta} \\hat {\\mathbb {E}} _ {(s _ {t}, a _ {t}) \\sim \\pi} \\left[ \\frac {\\pi_ {\\theta} (a _ {t} | s _ {t})}{\\pi (a _ {t} | s _ {t})} \\hat {A} ^ {\\pi} (s _ {t}, a _ {t}) \\right], \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\hat{\\mathbb{E}}_{(s_t,a_t)\\sim \\pi}[\\cdot ]$  denotes the estimated expectation over sampled trajectories and  $\\hat{A}^{\\pi}(s_t,a_t)$  the estimated advantage function at  $(s_t,a_t)$  which is usually obtained through Generalized Advantage\n\n\nEstimation (GAE, Schulman et al. (2015b)). Let  $L(\\theta)$  denote the surrogate objective in equation 2. It has been proved that the gradients of equation 1 and equation 2 coincide at  $\\pi = \\pi_{\\theta}$ . Additionally, both TRPO and PPO suggest that the current policy  $\\pi_{\\theta}$  should not deviate too much from the old policy  $\\pi$ , leading to various optimization constraints which will be discussed in Section 5.\n\n\nCondition number and numerical stability. We now briefly introduce the concept of the condition number from numerical analysis, which measures how much the output of an algorithm changes in response to a small change in its input:\n\n\nDefinition 3.1. (Condition number) Given a function  $f$  and an input  $x$ , let  $\\delta x$  be the error in  $x$  and  $\\delta f(x) = f(x + \\delta x) - f(x)$  be the corresponding error resulted in the output. We define the absolute condition number of  $f$  at  $x$  as\n\n\n$$\n\n$$\n\\kappa = \\lim  _ {\\delta \\rightarrow 0} \\sup  _ {| \\delta x | \\leq \\delta} \\frac {| \\delta f (x) |}{| \\delta x |}. \\tag {3}\n$$\n\n$$\n\n\nIn particular, when the function  $f$  is differentiable, it further has\n\n\n$$\n\n$$\n\\kappa = \\| J _ {f} (x) \\|,\n$$\n\n$$\n\n\nwhere  $J_{f}(x)$  is the Jacobian of  $f$  at  $x$ .\n\n\nA detailed study can be found in Trefethen & Bau (1997). The following concept of machine precision defines the threshold of numerical overflows:\n\n\nDefinition 3.2. (Machine precision) Let  $e > 0$  denote the machine precision throughout this paper, any quantity with its absolute value smaller than  $e$  will be considered zero.\n\n\nImportance sampling with clipped actions. Consider the probability ratio  $p_{\\theta}(a|s) = \\frac{\\pi_{\\theta}(a|s)}{\\pi(a|s)}$  in the loss function. This ratio might become ill-conditioned when  $\\pi(a|s)$  is very small. However, it is rarely reported in practice that importance sampling itself causes any numerical issues. This is because, when importance sampling is applied to modify a probabilistic distribution, the probability ratio  $p_{\\theta}(a|s)$  is only evaluated at data points sampled from the distribution  $\\pi(\\cdot|s)$ . This means that the probability of obtaining an action  $a$  where  $\\pi(a|s) \\ll 1$  is also very small, thereby rarely causing numerical instability. For example, let  $a \\sim \\mathcal{N}(\\mu_0(s), \\Sigma_0)$  be  $m$ -dimensional Gaussian random variable with probability density function  $\\pi(a|s)$ , then we have\n\n\n$$\n\n$$\nP (\\pi (a | s) <   e) \\leq 2 \\sqrt {2 \\pi} m \\left(e \\sqrt {\\det \\Sigma_ {0}}\\right) ^ {\\frac {1}{m}}, \\tag {4}\n$$\n\n$$\n\n\nwhere  $e \\ll 1$  is the machine precision.\n\n\nAlthough it is unlikely for  $\\pi(a|s)$  to take extremely small values if  $a$  is directly sampled from  $\\pi$  itself, as shown in equation 4, it is important to note that the actual action space usually has a bounded range due to physical or environmental constraints. Therefore, a common practice in DRL is to clip the sampled actions before feeding them into the simulator. Without loss of generality, we assume that the action space is of the box form  $\\mathcal{A} = [-\\beta, \\beta]^m$  for some positive  $\\beta$ , and write the action-clipping operation as  $\\phi: \\mathbb{R}^m \\to [-\\beta, \\beta]^m$ . Namely, for any  $a = (a_1, \\dots, a_m) \\in \\mathbb{R}^m$ ,  $\\phi(a) = (a_1', \\dots, a_m')$  where  $a_i' = \\max(-\\beta, \\min(a_i, \\beta))$ .\n\n\nNote that the probability ratio is evaluated after action clipping (Raffin et al., 2021). That is,  $p_{\\theta}(\\phi(a)|s)$  is used instead of  $p_{\\theta}(a|s)$  when computing the objective in equation 2. Consequently, the distance between the mean  $\\mu(s)$  and the clipped action  $\\phi(a)$  is no longer guaranteed to be small with high probability as suggested in equation 4, especially when  $\\mu(s)$  lies outside the action space  $A$ :\n\n\nTheorem 3.1. Let  $a \\sim \\mathcal{N}(\\mu(s), \\Sigma)$  be  $m$ -dimensional Gaussian random variable with probability density function  $\\pi(\\cdot|s)$ , and  $\\mathcal{A} = [-\\beta, \\beta]^m$  with  $\\beta > 0$ . Let  $\\phi: \\mathbb{R}^m \\to \\mathcal{A}$  be the action-clipping transformation, then\n\n\n1. Suppose that  $\\mu(s) \\in \\mathcal{A}$ , then for any  $a \\in \\mathbb{R}^m$ ,\n\n\n$$\n\n$$\nP (\\pi (\\phi (a) | s) <   e) \\leq 2 \\sqrt {2 \\pi} m (e \\sqrt {\\det \\Sigma_ {0}}) ^ {\\frac {1}{m}},\n$$\n\n$$\n\n\nwhere  $e$  is the machine precision;\n\n\n2. If  $\\mu(s) \\notin \\mathcal{A}$ , then for any  $a \\in \\mathbb{R}^m$  it has\n\n\n$$\n\n$$\n\\pi (\\phi (a) | s) \\leq \\frac {1}{(2 \\pi) ^ {\\frac {m}{2}} \\sqrt {\\det \\Sigma}} \\exp \\left(- \\frac {d ^ {2}}{2 \\lambda_ {m a x}}\\right),\n$$\n\n$$\n\n\nwhere  $d = \\mathrm{dist}(\\mu (s),\\mathcal{A})$  be the distance between  $\\mu (s)$  and the action space  $\\mathcal{A}$ ,  $\\lambda_{max}$  is the largest eigenvalue of  $\\Sigma$ .\n\n\nTherefore,  $\\pi (\\phi (a)|s)$  may fall below machine precision  $e$  when  $\\mu (s)$  is far from the action space  $\\mathcal{A}$  or when the standard deviations of  $\\pi (\\cdot |s)$  are sufficiently small. In either case,  $\\pi (\\phi (a)|s)$  becomes equal to 0 on the machine, and dividing by it can cause numerical issues.\n\n\nTanh-Gaussian transformation. It is worth mentioning that we can also bound the action by applying an invertible squashing function, such as tanh, to the Gaussian samples (Haarnoja et al., 2018). However, this approach still suffers from the ill-conditioned nature of importance sampling. For instance, consider the modified probability density function in the one-dimensional case, given by\n\n\n$$\n\n$$\n\\pi^ {\\prime} (a | s) = \\pi (u | s) \\left(1 - \\tanh  ^ {2} (u)\\right) ^ {- 1} \\tag {5}\n$$\n\n$$\n\n\nwhere  $u \\sim \\pi$  is a Gaussian random variable and  $a = \\tanh(u)$ . Note that when the mean of  $\\pi$  is too large, it is likely that the random variable  $u$  will take on extreme values, resulting in an excessively small value for the term  $1 - \\tanh^2(u)$  and causing numerical overflow in subsequent steps.\n\n\nIll-conditioned probability ratios. We have seen that the action-clipping transformation in deep policy gradients can significantly affect the numerical stability of importance sampling. However, one might argue that small values in  $\\pi(a|s)$  can be counterbalanced by small values in  $\\pi_{\\theta}(a|s)$  assuming that the current policy  $\\pi_{\\theta}$  is constrained to remain within a neighborhood of  $\\pi$ . This is particularly true in the first optimization step of each epoch, where  $\\pi_{\\theta}$  is reset to  $\\pi$  and all probability ratios are equal to 1. We will show that when action clipping is applied, the condition number of the probability ratio can become very large, causing  $p_{\\theta}(\\phi(a)|s)$  to grow exponentially fast and leading to instability as  $\\pi_{\\theta}$  deviates from  $\\pi$ . Let us again consider a Gaussian policy  $\\pi_{\\theta}(\\cdot|s) = \\mathcal{N}(\\mu(s;\\theta), \\Sigma(\\theta))$ , where  $\\mu(s)$  is typically parameterized by a neural network and  $\\Sigma(\\theta)$  is positive-definite and state-independent. Let  $\\pi(\\cdot|s) = \\mathcal{N}(\\mu(s;\\theta_0), \\Sigma(\\theta_0)))$ , the probability ratio is calculated as\n\n\n$$\n\n$$\n\\begin{array}{l} p _ {\\theta} (\\phi (a) | s) = \\frac {\\pi_ {\\theta} (\\phi (a) | s)}{\\pi (\\phi (a) | s)} \\\\ = \\sqrt {\\frac {\\det \\Sigma (\\theta_ {0})}{\\det \\Sigma (\\theta)}} \\frac {\\exp \\Big (- \\frac {1}{2} (\\phi (a) - \\mu (s ; \\theta)) ^ {T} \\Sigma (\\theta) ^ {- 1} (\\phi (a) - \\mu (s ; \\theta)) \\Big)}{\\exp \\Big (- \\frac {1}{2} (\\phi (a) - \\mu (s ; \\theta_ {0})) ^ {T} \\Sigma (\\theta_ {0}) ^ {- 1} (\\phi (a) - \\mu (s ; \\theta_ {0}))) \\Big)}. \\\\ \\end{array}\n$$\n\n$$\n\n\nTo better illustrate how the action-clipping transformation  $\\phi$  affects the condition number of  $p_{\\theta}$ , we further assume that  $\\Sigma (\\theta)\\equiv constant$ . The general case with parameterized standard deviations is discussed in Appendix C. Then, the gradient of the probability ratio is given by\n\n\n$$\n\n$$\n\\nabla_ {\\theta} p _ {\\theta} (\\phi (a) | s) = p _ {\\theta} (\\phi (a) | s) \\left(\\phi (a) - \\mu (s; \\theta)\\right) ^ {T} \\Sigma (\\theta) ^ {- 1} \\frac {\\partial \\mu (s ; \\theta)}{\\partial \\theta}, \\tag {6}\n$$\n\n$$\n\n\nwhere  $\\frac{\\partial\\mu(s;\\theta)}{\\partial\\theta}$  is the Jacobian of  $\\mu$  with respect to its parameters. Note that equation 6 is equivalent to\n\n\n$$\n\n$$\n\\nabla_ {\\theta} \\log \\Big (p _ {\\theta} (\\phi (a) | s) \\Big) = (\\phi (a) - \\mu (s; \\theta)) ^ {T} \\Sigma (\\theta) ^ {- 1} \\frac {\\partial \\mu (s ; \\theta)}{\\partial \\theta}, \\quad p _ {\\theta_ {0}} (\\phi (a) | s) = 1,\n$$\n\n$$\n\n\nwhich describes the gradient flow of  $p_{\\theta}$  in the policy space. Let  $\\mathcal{C}$  be the trajectory of policy parameters generated by PPO in the policy space, starting at  $\\theta_0$  and ending at  $\\theta_{1}$ . Applying the gradient theorem yields\n\n\n$$\n\n$$\np _ {\\theta_ {1}} (\\phi (a) | s) = \\exp \\left(\\int_ {\\mathcal {C}} (\\phi (a) - \\mu (s; \\theta)) ^ {T} \\Sigma (\\theta) ^ {- 1} \\frac {\\partial \\mu (s ; \\theta)}{\\partial \\theta} \\cdot d \\mathbf {r}\\right) \\tag {7}\n$$\n\n$$\n\n\nwhere the term in the exponent is the line integral along  $\\mathcal{C}$ . Therefore, the condition number  $\\kappa_{\\theta} = \\| \\nabla_{\\theta}p_{\\theta}(\\phi (a)|s)\\|$  can be extremely large as illustrated in Figure 1, implying that the probability ratio may overflow when\n\n- - The standard deviations of  $\\pi$  are small so that the matrix  $\\Sigma (\\theta)^{-1}$  has large eigenvalues;\n\n- - The action is clipped and the mean  $\\mu(s)$  lies outside  $\\mathcal{A}$  so that  $|\\phi(a) - \\mu(s; \\theta)|$  is large (illustrated in Figure 1(c));\n\n- - The policy parameterization  $\\mu (\\cdot ;\\theta)$  is complex (e.g. large policy networks) so that a small change in the parameter can lead to a dramatic difference in the output.\n\n\nSome DRL libraries further transform the standard deviations to prevent low variances. However, gradient explosion still occurs with those code-level techniques, indicating that the ill-conditioned probability ratio is the primary cause of the issue.\n\n\n![(a)](images/80e070eac0dce631c8c57b7069542977cfdf8fe6e6fa78a9d2282a2d8c8aa950.jpg)\n\n\n![(b) Figure 1: (a) When  $\\mu_0$  (the mean of  $\\pi_0$ ) is large, the condition number  $\\kappa_{\\theta}$  of the probability ratio  $p_{\\theta}$  changes exponentially when  $\\mu$  (the mean of  $\\pi$ ) deviates from  $\\mu_0$ ; (b)  $\\kappa_{\\theta}$  can become extremely large when the variance  $\\Sigma$  is small, particularly in cases of large means; (c) The action with the largest norm in each mini-batch grows outside the boundary during PPO training.](images/0309c81b33a71dcb866c9f40639ec3d55517aac930344bf0ca4c4c170bf15fa4.jpg)\n\n\n![(c)](images/1a4cc773ec14b85865c68182f6670597de906ac29e3e22ded4b878126cd4339e.jpg)\n\n\n# 4 EMPIRICAL ANALYSIS OF GRADIENT EXPLOSION\n\n\nIn the previous section, we theoretically demonstrated the ill-conditioned nature of importance sampling in TRPO/PPO methods. In this section, we present experimental evidence to support the theory. The experimental setup follows that described in Appendix A.\n\n\nGradient explosion is always accompanied by excessively large probability ratios. We run the Humanoid-v4 environment on five random seeds, all of which fail due to arithmetic overflow. In Figure 2, we observe that the maximum of probability ratio  $\\max_t \\log_{10}\\left(p_\\theta (a_t|s_t)\\right)$  takes excessively large values across all five random seeds. A widely accepted explanation for gradient explosion is that the standard deviations of the Gaussian policy can become very small, leading to potential singularity issues in the covariance matrix  $\\Sigma$ . While this can indeed cause numerical instability, it is not the only reason for gradient explosion. In Figure 6, we observe that the gradient explodes even before the standard deviations become small.\n\n\nLarge mean causes gradient explosion. We have mentioned that large outputs generated by the policy network are one of the primary causes of gradient explosion. To verify this analysis, we apply an additional transformation  $g(\\cdot)$  to the mean  $\\mu(s)$  to prevent it from leaving the action space  $\\mathcal{A}$ . The new Gaussian policy is defined as  $a \\sim \\mathcal{N}(g(\\mu(s)), \\Sigma(s))$ , where  $g(\\mu(s)) \\in \\mathcal{A}$ . In Table 1, we consider two transformations: the action-clipping function  $g = \\phi$ , defined in Section 3, and the hyperbolic tangent activation function  $g = \\tanh$ . We observe that both transformations effectively prevent the exploding gradient issue across three MuJoCo environments, indicating that having deviated means is indeed one of the main causes of numerical instability. Meanwhile, we also observe that the final returns are significantly lower than the benchmark (Raffin et al., 2021). This may be attributed to the fact that when the mean  $\\mu(s_t)$  goes outside  $\\mathcal{A}$  for some state  $s_t$ , the gradient norm\n\n\n$$\n\n$$\n\\nabla_ {\\theta} g (\\mu (s _ {t}; \\theta)) = \\frac {\\mathrm {d} g (\\mu)}{\\mathrm {d} \\mu} \\bigg | _ {\\mu = \\mu (s _ {t})} \\frac {\\partial \\mu (s _ {t} ; \\theta)}{\\partial \\theta},\n$$\n\n$$\n\n\n![(a)](images/6e586a0a0b9dad307be69807809d17c9b76159a59df20576ae5b0abe1ca28606.jpg)\n\n\n![(b)](images/fbf567908cdc2f0f8098d3a0b76f90126e7440fcba6657c57c14c59a02c002e0.jpg)\n\n\n![(c)](images/7c9d3df1f2e3fe85d813ad2b309b6dc51f5255ce87f855e7b105150b55975bb2.jpg)\n\n\n![(d)](images/5baf450fbdd89610e6212211c80f5148a02368bcbe392ea6839f0aece29e6dd0.jpg)\n\n\n![(e) Figure 2: The maximum of probability ratio after clipping takes exponentially large values when the gradient explodes in five individual runs.](images/adebdbcffc751f4bfeae8641e8a001fbcc2c91135a5bb55d531b5087c2251a8c.jpg)\n\n\n<table><tr><td>ENVIRONMENT</td><td>TRANSFORMATION</td><td>FINAL RETURN</td><td>EXPLOSION RATE</td></tr><tr><td>HOPPER-V4</td><td>CLIPPING</td><td>332 \u00b1 129</td><td>0%</td></tr><tr><td>HOPPER-V4</td><td>tanh</td><td>234 \u00b1 106</td><td>0%</td></tr><tr><td>HOPPER-V4</td><td>NONE</td><td>N/A</td><td>100%</td></tr><tr><td>WALKER2D-V4</td><td>CLIPPING</td><td>692 \u00b1 346</td><td>0%</td></tr><tr><td>WALKER2D-V4</td><td>tanh</td><td>434 \u00b1 60</td><td>0%</td></tr><tr><td>WALKER2D-V4</td><td>NONE</td><td>N/A</td><td>100%</td></tr><tr><td>HUMANOID-V4</td><td>CLIPPING</td><td>586 \u00b1 102</td><td>0%</td></tr><tr><td>HUMANOID-V4</td><td>tanh</td><td>530 \u00b1 72</td><td>0%</td></tr><tr><td>HUMANOID-V4</td><td>NONE</td><td>N/A</td><td>100%</td></tr></table>\n\n\nTable 1: Performance of mean transformation via clipping function or tanh function in three MuJoCo environments. For Hopper-v4 and Walker2d-v4, we directly apply tanh as their action input range is between -1 and 1. For the Humanoid-v4 task, we rescale the mapping to  $g = 0.4 * \\tanh$ , as the action inputs are bounded between -0.4 and 0.4.\n\n\nbecomes exponentially small or even equal to zero, depending on the transformation  $g$ . Notably, we may have  $|\\frac{\\mathrm{d}g(\\mu)}{\\mathrm{d}\\mu}|_2 \\simeq 0$  due to the vanishing gradient property of the transformation  $g$  when  $\\mu(s_t) \\notin \\mathcal{A}$ . As a result, policy gradient algorithms are more likely to converge towards sub-optimal regions in this case.\n\n\n# 5 OPTIMIZATION CONSTRAINTS CANNOT GUARANTEE NUMERICAL STABILITY\n\n\nIn previous sections, we demonstrated that importance sampling can be numerically unstable when the probability ratio is evaluated at transformed sample points. This is particularly problematic in deep policy gradient methods like TRPO and PPO, which incorporate the probability ratio into their surrogate objective functions. This integration is the fundamental reason for the numerical instability of these algorithms. However, it should be noted that both TRPO and PPO also apply certain constraints to the original optimization problem, such as KL divergence and probability ratio clipping. In this section, we will explain why these optimization constraints are insufficient to resolve the numerical issues as they are supposed to.\n\n\nKL divergence. In TRPO algorithm (Schulman et al., 2015a), it proposes to use the KL divergence between the current and old policy as a constraint to prevent large updates during policy improvement. Specifically, the KL divergence of two continuous probability distributions  $P$  and  $Q$  is defined as\n\n\n$$\n\n$$\nD _ {K L} (P \\| Q) = \\int_ {\\Omega} p (x) \\log \\frac {p (x)}{q (x)} \\mathrm {d} x, \\tag {8}\n$$\n\n$$\n\n\nwhere  $p$  and  $q$  are the probability density functions of  $P$  and  $Q$ , respectively. When optimizing the objective equation 2, TRPO applies the following hard constraint to the KL distance between successive policies\n\n\n$$\n\n$$\n\\hat {\\mathbb {E}} _ {s \\sim \\pi} \\left[ D _ {K L} \\left(\\pi_ {\\theta} (\\cdot | s) \\| \\pi (\\cdot | s)\\right) \\right] \\leq \\delta , \\tag {9}\n$$\n\n$$\n\n\nwhere  $\\delta > 0$  is a small quantity called the KL stepsize. Note that for many commonly used distributions, including Gaussian, the KL divergence between  $\\pi_{\\theta}$  and  $\\pi$  can be analytically calculated via a closed-form formula. For instance, consider two multivariate Gaussian policies  $\\pi_{\\theta}(\\cdot |s) \\sim \\mathcal{N}(\\mu(s), \\Sigma_0)$  and  $\\pi(\\cdot |s) \\sim \\mathcal{N}(\\mu_0(s), \\Sigma_0)$  with identical covariance matrices. The KL divergence is\n\n\n$$\n\n$$\nD _ {K L} \\left(\\pi_ {\\theta} (\\cdot | s) \\| \\pi (\\cdot | s)\\right) = \\frac {1}{2} \\left(\\mu (s) - \\mu_ {0} (s)\\right) ^ {T} \\Sigma_ {0} ^ {- 1} \\left(\\mu (s) - \\mu_ {0} (s)\\right). \\tag {10}\n$$\n\n$$\n\n\nWhile the above constraint can effectively prohibit  $\\pi_{\\theta}$  from making large updates steps in the distribution space, it may not be able to prevent the probability ratio  $p_{\\theta}(a|s)$  from taking extremely large values at some state-action pair  $(s,a)$  for two reasons: First, the KL divergence  $D_{KL}(\\pi_{\\theta}(\\cdot |s)\\parallel \\pi (\\cdot |s))$  measures the averaged distance between two distributions over the entire space  $\\mathbb{R}^m$ , while  $p_{\\theta}(a|s)$  is determined at a specific point  $a\\in \\mathbb{R}^m$ , which means that we can find some  $a\\in \\mathbb{R}^m$  such that the probability ratio  $p_{\\theta}(a|s)$  takes a large value even if the KL distance  $D_{KL}(\\pi_{\\theta}(\\cdot |s))$  is small. Second, the absolute value of KL divergence  $|D_{KL}(\\pi_{\\theta}(\\cdot |s)\\parallel \\pi (\\cdot |s))|$  is quadratic with respect to the policy update  $|\\mu (s) - \\mu_0(s)|$ , while the probability ratio  $\\pi_{\\theta}(a|s)$  grows exponentially with  $|\\mu (s) - \\mu_0(s)|$  especially for excessively small  $\\pi (a|s)$ .\n\n\nProbability ratio clipping. While TRPO involves second-order computations that are usually expensive in practice, PPO (Schulman et al., 2017) proposes a clipped surrogate objective\n\n\n$$\n\n$$\nL ^ {C L I P} (\\theta) = \\hat {\\mathbb {E}} _ {(s _ {t}, a _ {t}) \\sim \\pi} \\left[ \\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} (s _ {t}, a _ {t}), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right)\\right) \\right] \\tag {11}\n$$\n\n$$\n\n\nwhere  $\\epsilon \\in (0,1)$  is the clipping parameter. The clipped loss equation 11 is expected to block large updates in the policy that can improve the objective (i.e.,  $\\hat{A}^{\\pi}(s_t,a_t) > 0$ ), and allow them if they make the objective worse ( $\\hat{A}^{\\pi}(s_t,a_t) < 0$ ). This property, however, makes it possible that some ill-conditioned probability ratios are evaluated when computing  $L^{CLIP}(\\theta)$ .\n\n\nIn particular, consider a given state-action pair  $(s,a)$  and the estimated advantage  $\\hat{A}^{\\pi}(s,a)$ :\n\n\n- If  $\\hat{A}^{\\pi}(s,a)\\geq 0$  , it has\n\n\n$$\n\n$$\n\\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} (s, a), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} (s, a)\\right) = (1 + \\epsilon) \\hat {A} ^ {\\pi} (s, a),\n$$\n\n$$\n\n\nwhenever  $p_{\\theta} \\geq 1 + \\epsilon$ . Thus, it automatically blocks extremely large values in  $p_{\\theta}$  when evaluating  $L^{CLIP}(\\theta)$ ;\n\n\n- If  $\\hat{A}^{\\pi}(s,a) < 0$ , it is possible that the machine directly evaluates large probability ratios, since\n\n\n$$\n\n$$\n\\begin{array}{l} \\text {I n t e g r a n d i n e q u a t i o n 1 1} = \\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} (s, a), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} (s, a)\\right) \\\\ = \\hat {A} ^ {\\pi} (s, a) \\max  \\left(p _ {\\theta}, \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right)\\right) \\\\ = \\hat {A} ^ {\\pi} (s, a) \\max  \\left(p _ {\\theta}, 1 - \\epsilon\\right) \\\\ = p _ {\\theta} \\hat {A} ^ {\\pi} (s, a) \\\\ \\end{array}\n$$\n\n$$\n\n\nwhen  $p_{\\theta} \\geq 1 - \\epsilon$ , which then may lead to numerical overflows.\n\n\nIn practice, the expectation in equation 11 is estimated by the average over sampled trajectories\n\n\n$$\n\n$$\nL ^ {C L I P} (\\theta) \\simeq \\frac {1}{T} \\sum_ {t = 0} ^ {T - 1} \\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right)\\right).\n$$\n\n$$\n\n\nTherefore, numerical errors may occur if there exists any pair  $(s_t, a_t)$  with negative advantage  $\\hat{A}^{\\pi}(s_t, a_t) < 0$  and large probability ratio  $p_{\\theta}(a_t | s_t) \\gg 1$ .\n\n\nEmpirical results. We perform experiments to validate aforementioned theoretical analysis. In Figure 3 (a), we see that the KL divergence  $D_{KL}(\\pi_{\\theta}(\\cdot |s) \\parallel \\pi (\\cdot |s))$  remains small even when the probability ratio  $p_{\\theta}(a|s)$  takes very large values, indicating that  $D_{KL}(\\pi_{\\theta}(\\cdot |s) \\parallel \\pi (\\cdot |s))$  cannot ensure numerical stability. We also test the following KL-penalized objective\n\n\n$$\n\n$$\nL ^ {K L P E N} (\\theta) = \\hat {\\mathbb {E}} _ {t} \\left[ p _ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right) - b D _ {K L} \\left(\\pi_ {\\theta} (\\cdot \\mid s _ {t}) \\| \\pi (\\cdot \\mid s _ {t})\\right) \\right] \\tag {12}\n$$\n\n$$\n\n\nwhich is also proposed in Schulman et al. (2017). In Figure 3 (b), we can observe that optimizing equation 12 still fails to prevent the probability ratio growing exponentially. While probability ratio clipping has proven insufficient to overcome numerical instability, it is worth noting that the PPO algorithm becomes even more unstable without clipping, as shown in Figure 3 (c) where the gradient explodes within 4 steps. When plotting  $\\max_t\\log_{10}\\left(p_\\theta (a_t|s_t)\\right)$ , it is calculated after clipping. For example, a large  $p_\\theta (a_t|s_t)$  with positive advantage  $\\hat{A}^{\\pi}(s_t,a_t)$  is clipped and equal to  $1 + \\epsilon$ .\n\n\n![(a)](images/11471b6d0fad3487752b972cc65877b4b0ef8018fe8c68e7bfebf09eb7c48397.jpg)\n\n\n![(b) Figure 3: The logarithm of KL divergence  $\\log_{10}\\left(D_{KL}(\\pi_{\\theta}(\\cdot |s)\\parallel \\pi (\\cdot |s))\\right)$  and the maximum of probability ratio  $\\max_t\\log_{10}\\left(p_\\theta (a_t|s_t)\\right)$  at each optimization step is plotted with different optimization constraints, where environment is MuJoCo Humanoid-v4. In each plot, we apply (a) probability ratio clipping; (b) KL-penalty; (c) no constraints.](images/82524e3bd5e898124a4561cc6f1647abfacfbb1fe725f5673d8caad7f0b45a1f.jpg)\n\n\n![(c)](images/81539e00d6b66443c13f8c6da010e300f757475bd52664bc667f30ebea601d94.jpg)\n\n\n# 6 BYPASSING IMPORTANCE SAMPLING IN PPO\n\n\nIn previous sections, we demonstrated that the TRPO/PPO objective is potentially ill-conditioned and that the built-in optimization constraints cannot effectively alleviate its numerical instability. In this section, we examine the numerical stability of the vanilla policy gradient algorithm and propose a new algorithm based on it.\n\n\nNumerical stability of vanilla policy gradient. We have discussed why importance sampling in TRPO/PPO is the primary cause of gradient explosion. It is also worth stepping back to revisit the vanilla policy gradient, whose objective does not involve any density ratio. In this case, we optimize the following objective\n\n\n$$\n\n$$\nL ^ {V A N I L L A} (\\theta) = \\hat {\\mathbb {E}} _ {\\left(s _ {t}, a _ {t}\\right) \\sim \\pi_ {\\theta}} \\left[ \\log \\pi_ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right) \\right], \\tag {13}\n$$\n\n$$\n\n\nwhere we replace  $Q^{\\pi}(s_t, a_t)$  with  $\\hat{A}^{\\pi}(s_t, a_t)$  to reduce the variance. The logarithm of the Gaussian probability density function is\n\n\n<table><tr><td>ENVIRONMENT</td><td>FINAL RETURN</td><td>EXPLOSION RATE</td></tr><tr><td>HOPPER-V4</td><td>536 \u00b1 254</td><td>0%</td></tr><tr><td>WALKER2D-V4</td><td>195 \u00b1 135</td><td>0%</td></tr><tr><td>HUMANOID-V4</td><td>355 \u00b1 191</td><td>0%</td></tr></table>\n\n\nTable 2: Empirical results of vanilla policy gradient in three MuJoCo environments. Despite its poor performance, vanilla policy gradient does not suffer from the exploding gradient issue. Hyperparameters are specified in Appendix A.\n\n\n$$\n\n$$\n\\log \\pi_ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) = - \\frac {1}{2} \\sum_ {i = 1} ^ {m} \\frac {\\left(a _ {t} - \\mu\\right) _ {i} ^ {2}}{\\sigma_ {i} ^ {2}} - \\frac {m}{2} \\log (2 \\pi) - \\sum_ {i = 1} ^ {m} \\log \\sigma_ {i}, \\tag {14}\n$$\n\n$$\n\n\nwhich is less likely to take extreme values where  $(a_{t} - \\mu)_{i}$  denotes the  $i$ -th element in  $(a_{t} - \\mu)$  and  $\\Sigma = \\mathrm{diag}(\\sigma_1^2,\\dots,\\sigma_m^2)$ . While the only problematic thing is that  $\\Sigma$  might have small eigenvalues, some DRL libraries employ a lower bound to the standard deviation of the policy to avoid singularity issues as shown in Table 5. Furthermore, the numerical stability of vanilla policy gradient is not very sensitive to the dimension of the action space: in equation 14, the magnitude of  $|\\log \\pi_{\\theta}(a_t|s_t)|$  is roughly linear to the dimension  $m$ . Similarly, the magnitude of the integrand  $\\mathcal{L}$  in equation 17 is also approximately linear to the dimension of  $\\mathcal{A}$ , implying that the probability ratio  $p_{\\theta}$  is approximately exponential to the dimension of  $\\mathcal{A}$  and thus much easier to explode as  $m$  increases.\n\n\nIn Table 2, we observe that the vanilla policy gradient does not exhibit any numerical instability when the standard deviations are lower-bounded, further supporting the claim that importance sampling is the primary cause of gradient explosion in deep policy gradient methods. It should be noted that while extremely small standard deviations can still affect numerical stability, this issue can be effectively mitigated by setting a lower bound on the standard deviations, as is done in PPO (though it is not entirely effective).\n\n\nVanilla Policy Gradient with Clipping. Based on the previous analysis, the vanilla policy gradient algorithm is more stable than TRPO/PPO. Motivated by this observation, we propose an objective that combines the strengths of both approaches. This modification results in the following objective:\n\n\n$$\n\n$$\nL ^ {C P G} (\\theta) = \\hat {\\mathbb {E}} _ {\\left(s _ {t}, a _ {t}\\right) \\sim \\pi} \\left[ \\min  \\left(\\log p _ {\\theta} \\hat {A} _ {t} ^ {\\pi}, \\operatorname {c l i p} \\left(\\log p _ {\\theta}, \\log (1 - \\epsilon), \\log (1 + \\epsilon)\\right) \\hat {A} _ {t} ^ {\\pi}\\right)\\right) \\tag {15}\n$$\n\n$$\n\n\nwhere  $\\log p_{\\theta} = \\log \\pi_{\\theta} - \\log \\pi$ . Notably, the new objective can also be interpreted as a logarithmic variant of the PPO loss in equation 11. As shown in Figure 5 and Table 2, this method achieves performance comparable to PPO and outperforms the vanilla policy gradient. Also, when there is no exploding gradient issue, our algorithm can achieve similar performance to PPO. To examine the performance in the general case, we use smaller policy networks with width 64 which are less affected by numerical issues based on our analysis in Section 3, and run both algorithms in three benchmarks. As shown in Figure 4, both achieve similar performance across all three environments, indicating that our algorithm serves as a numerically robust alternative to PPO.\n\n\n![(a) Hopper-v4](images/21ca62d2c9ca779e8122f1ddea3aacf9d1b8c513193f498927b375332d66233e.jpg)\n\n\n![(b) Walker2d-v4 Figure 4: For smaller policy networks that are less affected by gradient explosion, the proposed algorithm achieves performance comparable to PPO on three MuJoCo benchmarks.](images/be65ccf84f4c529bcdbccda3361e9b7ec3d02c740c77b6d1a464bf473d37e5f3.jpg)\n\n\n![(c) Humanoid-v4](images/9d28cdc1d2a8bb33a1e10bf9add76a5aaecff99fca1d40d441b06634b3485de1.jpg)\n\n\n![(a) Hopper-v4](images/d8a4a9efc920bce77427c71b9dfad3a453f9e3aa418c9b71399d2242921753a5.jpg)\n\n\n![(b) Walker2d-v4 Figure 5: For larger policy networks, where PPO completely fails due to gradient explosion, our algorithm avoids numerical issues and continues to produce reasonable results.](images/086c6077f5f1bfb37dcd8997c151d319fc5bcd33f58557aaa06ff698ddd1456a.jpg)\n\n\n![(c) Humanoid-v4](images/8809f256f79cd3b66f7b5d10d68356cff5782c114d6cc72ac3ed996063f584d0.jpg)\n\n\n# 7 CONCLUDING REMARKS\n\n\nIn this work, we demonstrate that ill-conditioned density ratios in importance sampling are the fundamental cause of numerical instability in deep policy gradient methods. While this issue cannot be effectively addressed through optimization constraints alone, code-level techniques are necessary to fully eliminate or mitigate its effects. To address this issue, we find that directly dropping the importance sampling term yields the best performance, and we therefore advocate for further investigation in this direction in future studies.\n\n\nAccording to our analysis, larger neural networks make bigger updates in each optimization step, resulting in larger density ratios in importance sampling and thus increasing the risk of gradient explosion. Therefore, we question whether deep policy gradient methods are numerically stable and robust when optimizing complex policy networks. This is particularly important for the scalability of deep policy gradient methods. Although the experiments in this work focus on MuJoCo continuous-control environments, the limitations discussed are inherent to the algorithms and may apply to other scenarios as well. We believe that a better understanding of these issues can undoubtedly enhance the effectiveness of deep reinforcement learning in real-world applications.\n\n\n# REFERENCES\n\n- A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98): 1-76, 2021.\n\n- M. Andrychowicz, A. Raichuk, P. Stanczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski, S. Gelly, and O. Bachem. What matters for on-policy deep actor-critic methods? A large-scale study. In *ICLR*, 2021.\n\n- Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, 1994.\n\n- A. Bou, M. Bettini, S. Dittert, V. Kumar, S. Sodhani, X. Yang, G. De Fabritiis, and V. Moens. TorchRL: A data-driven decision-making library for pytorch. In ICLR, 2024.\n\n- Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In ICML, 2016.\n\n- L. Engstrom, A. Ilyas, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph, and A. Madry. Implementation matters in deep RL: A case study on PPO and TRPO. In ICLR, 2020.\n\n- Y. Fujita and S. Maeda. Clipped action policy gradient. In ICML, 2018.\n\n- X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, pp. 249-256, 2010.\n\n- T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel, and S. Levine. Soft actor-critic algorithms and applications. In NeurIPS, 2018.\n\n- P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning that matters. In AAAI, pp. 3207-3214, 2018.\n\n- A. Ilyas, L. Engstrom, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph, and A. Madry. A closer look at deep policy gradients. In *ICLR*, 2020.\n\n- S. Khorasani, S. Salehkaleybar, N. Kiyavash, N. He, and M. Grossglauer. Efficiently escaping saddle points for non-convex policy optimization. arXiv preprint arXiv:2311.08914, 2023.\n\n- D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\n- T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n\n- Q. Liu, L. Li, Z. Tang, and D. Zhou. Breaking the curse of horizon: Infinite-horizon off-policy estimation. In NeurIPS, 2018.\n\n- Y. Liu, K. Zhang, Tamer Ba,sar, and W. Yin. An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods. In NeurIPS, pp. 7624-7636, 2020.\n\n- E.N.Lorenz.The Essence of Chaos.University of Washington Press,1995.\n\n- V. Mnih, . Kavukcuoglu, D. Silver, A. A Rusu, J. Veness, M. G Bellemare, A. Graves, M. Riedmiller, A. K Fidjeland, G. Ostrovski, and et al. Human-level control through deep reinforcement learning. Nature, 518:529-533, 2015.\n\n- L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In NeurIPS, pp. 27730-27744, 2022.\n\n- R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In ICML, pp. 1310-1318, 2013.\n\n- G. Philipp, D. Song, and J. G. Carbonell. The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions. arXiv preprint arXiv:1712.05577, 2017.\n\n- A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann. Stable-Baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22 (268):1-8, 2021.\n\n- S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. In ICLR, 2017.\n\n- J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In ICML, pp. 1889-1897, 2015a.\n\n- J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.\n\n- J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\n- D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In ICML, pp. 387-395, 2014.\n\n- D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. Driessche, T. Graepel, and D. Hassabis. Mastering the game of Go without human knowledge. Nature, 550:354-359, 2017.\n\n- R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, pp. 1057-1063, 1999.\n\n- L.N.Trefethen and D.Bau.Numerical Linear Algebra.SIAM,1997.\n\n- O. Vinyals, I. Babuschkin, W.M. Czarnecki, and et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575:350-354, 2019.\n\n- T. Wang, S. Herbert, and S. Gao. Fractal landscapes in policy optimization. In NeurIPS, pp. 4277-4294, 2023.\n\n- W. Wang, Y. Zhu, Y. Zhou, C. Shen, J. Tang, Z. Xu, Y. Peng, and Y. Zhang. Exploring gradient explosion in generative adversarial imitation learning: A probabilistic perspective. In AAAI, 2024.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638417.916871", "msecs": "916.0", "relativeCreated": "14747.368574142456", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Improving Neural Language Generation with Spectrum Control\n\n# Abstract\nRecent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an anisotropic cone in the vector space, which is called the representation degeneration problem. In this paper, we propose a novel spectrum control approach to address this degeneration problem. The core idea of our method is to directly guide the spectra training of the output embedding matrix with a slow-decaying singular value prior distribution through a reparameterization framework. We show that our proposed method encourages isotropy of the learned word representations while maintains the modeling power of these contextual neural models. We further provide a theoretical analysis and insight on the benefit of modeling singular value distribution. We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model, and various Transformer-based models for machine translation, on common benchmark datasets for these tasks.\n\n\n# 1 INTRODUCTION\n\n\nNeural language generation (NLG) is an important task with many practical applications, such as automatic speech recognition (Graves et al., 2013; Toshniwal et al., 2018), text generation (Bowman et al., 2016; Radford et al., 2019; Keskar et al., 2019), machine translation (Bahdanau et al., 2015; Vaswani et al., 2017) and dialog systems (Gao et al., 2019a; Tang et al., 2019). Most NLG models utilize a complex encoding model to map a given context into a hidden state vector, and then predict the next word distribution by multiplying the encoded vector with the output embedding layer, followed by a softmax layer. In the past few years, it has witnessed a significant progress in NLG by improving the encoding model, from the recurrent neural network (RNN) (Bahdanau et al., 2015; Jozefowicz et al., 2016; Merity et al., 2018a) based models to the current Transformer-based models (Vaswani et al., 2017; Devlin et al., 2019; Dai et al., 2019; Radford et al., 2019). However, embeddings in the softmax output layer have been shown not capable enough to model the conditional probability (Yang et al., 2018).\n\n\nRecently, Gao et al. (2019b) pointed out another limitation of the output embeddings: the representation degeneration problem. They showed that the singular value distribution of the output embedding matrix tends to decay very fast, and the embedding space is squeezed into a narrow cone (as shown in Figure 1(a) and 1(c) in 2-D plots). Such anisotropic shape (Ethayarajh, 2019) is very different from what one would expect from an expressive word embedding space (Arora et al., 2016a; Mu & Viswanath, 2018). Therefore, several efforts (Gao et al., 2019b; Wang et al., 2019a) have been made to address the degeneration problem.\n\n\nUnlike previous approaches that applied implicit regularization to singular values of the output embedding matrix, we propose a novel spectrum control (SC) approach to explicitly control the singular value distribution: we first reparameterize the output embedding matrix  $\\mathbf{W}$  by its singular value decomposition (SVD):  $\\mathbf{W} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ , where  $\\mathbf{U},\\mathbf{V}$  are column orthonormal matrices, and  $\\boldsymbol{\\Sigma}$  is a diagonal matrix of singular values. Then we guide the training of  $\\boldsymbol{\\Sigma}$  by a predefined slow-decaying prior distribution, such as a polynomial decay distribution, or an exponential decay distribution. At the end of training, the distribution of singular values of the embedding matrix gets close to the prior distribution. Our spectrum control approach alleviates the representation degeneration problem by encouraging the diversity of word representations and improving isotropic property of these representations (see Figure 2), even on top of the powerful Transformer-based models.\n\n\n![(a) Transformer-XL](images/ac64bc4b862a767a1e12e3ad5746e66c95ffe8652ed3d69a7f21501bf8762236.jpg)\n\n\n![(b) Singular Values Figure 1: Projected word embeddings $^{1}$  and singular value distributions. (a) and (c): 2-D visualization of word embedding matrices of Transformer-XL for language modeling and Transformer for machine translation; (b) and (d): Normalized singular value distributions of embedding matrices.](images/054f52f50c2204308cc2c44275b5dee1b043c433a0593b2ed58961c49eb9568c.jpg)\n\n\n![(c) Transformer](images/4c00db17561ecfe9b8b65479b68cdec2c81f73634844c320f958ca51e18b6f5f.jpg)\n\n\n![(d) Singular Values](images/3434a9626338ddec8ced32870afed94c7432f7e4160af3ae12fffb7a3862d74a.jpg)\n\n\n![(a) Transformer-XL+SC](images/03ccd35f568b9f17f48bfdee7f08254a9eb1104fe9950abe8f641f5a0982581e.jpg)\n\n\n![(b) Singular Values Figure 2: Projected word embeddings and singular value distributions using our spectrum control method. (a) and (c): 2-D visualization of word embedding matrices of Transformer-XL for language modeling and Transformer for machine translation; (b) and (d): Normalized singular value distributions of embedding matrices.](images/1e1b6053028e7dd8f15fe323190ea4e0a0845bb141a5794ae5073a94b5959b17.jpg)\n\n\n![(c) Transformer+SC](images/b915bb7d43461a73390a90e5c0c8d7d92e47eb906abfc74f049f5fdc332891a7.jpg)\n\n\n![(d) Singular Values](images/eba354725e722319aaf51957ed7361487e9e5d0fb18968870672d78337a59a6d.jpg)\n\n\nWe further present a theoretical analysis to justify our method. The results suggest that it is beneficial to directly guide the singular value distribution of the embedding matrix throughout the training process and control the decay rate of these singular values. We demonstrate the effectiveness of our training framework with extensive experimental results on two tasks: language modeling and machine translation. Our spectrum control method outperforms the latest state-of-the-art TransformerXL model on WikiText-103 dataset for language modeling; and obtains close to 1.5 BLEU improvement on IWSLT 2014 German-English translation task compared to the Transformer baseline model.\n\n\n# 2 RELATED WORK\n\n\nIn this paper, we mainly focus on the output embedding matrix that is used in the softmax output layer for language generation tasks. This embedding matrix is also used as input word embeddings, which is known as weight tying trick. The weight tying not only reduces the number of parameters but also enjoys theoretical benefits (Inan et al., 2017). Thus the weight tying has been successfully applied to many state-of-the-art models for language modeling and machine translation (Merit et al., 2018a; Vaswani et al., 2017; Yang et al., 2018).\n\n\nHowever, there are certain issues with the softmax output layer. Yang et al. (2018) first identified a problem called \"softmax bottleneck\" that the softmax output layer does not have enough capacity to model natural language due to its connection with the rank bottleneck in matrix factorization. A simple and effective method called Mixture of Softmaxes (MoS) was proposed to deal with this issue. Follow-up work in Kanai et al. (2018); Ganea et al. (2019) tried to replace softmax with alternative activation functions. Kanai et al. (2018) proposed to use sigsoftmax, which is composed of a multiplication of an exponential function and sigmoid function. Ganea et al. (2019) proposed a Linear-Monotonic-Softmax (LMS) model that generalized the approach in Kanai et al. (2018) by learning parametric point-wise increasing functions to optimally distort the logits before feeding them into the softmax layer. Pappas & Henderson (2019) instead resorted to a powerful deep residual nonlinear output mapping while using a single softmax function without modifying its dimensionality or rank.\n\n\nAnother line of work focuses on increasing the expressive power of the output embedding matrix by adding a cosine similarity regularization term (Gao et al., 2019b), or adversarial noise (Wang et al., 2019a). Gao et al. (2019b) analyzed the representation degeneration problem that the out-\n\n\nput embeddings tend to degenerate and be distributed into a narrow cone. A regularization term based on the summation of pairwise cosine similarity among all words was proposed to increase the representation power of word embeddings. Wang et al. (2019a) pointed out the computation of regularization term in (Gao et al., 2019b) depends on the size of the vocabulary and hence is costly. Instead, they proposed a simple yet highly effective adversarial training method that adds adversarial noises to the output embedding layer when training the models. They proved in theory that this adversarial training increases the distances between two different words, and thus encourages the diversity of the embedding vectors. Our work follows this line of thought, but takes a different approach: inspired by the spectrum control that encourages slow singular value decay in Generative Adversarial Network (GAN) training (Jiang et al., 2019), we propose a novel method that directly guides the singular values by a slow-decaying prior distribution during the model training process. We show that the anisotropic behavior of the contextualized word representations from powerful Transformer-based models (Ethayarajh, 2019) are alleviated by our spectrum control approach.\n\n\n# 3 PROBLEM SETUP\n\n\nIn this section, we briefly introduce the neural models for language generation, and illustrate the singular value decay phenomena of existing neural language models. We first introduce some notations used in the rest of the paper.\n\n\nNotation: For a  $d$ -dimensional vector  $\\mathbf{x} \\in \\mathbb{R}^d$ , we use  $\\| \\mathbf{x} \\|_q = (\\sum_{i=1}^d |x_i|^q)^{1/q}$ , where  $0 < q < \\infty$  to denote its  $\\ell_q$ -norm, and  $\\| \\mathbf{x} \\|_\\infty = \\max_i |x_i|$  to be its infinity norm. For a matrix  $\\mathbf{A} \\in \\mathbb{R}^{d_1 \\times d_2}$ , let  $\\mathbf{A}_{i*}$  be the  $i$ -th row of  $\\mathbf{A}$ , and we use  $\\| \\mathbf{A} \\|_2$ ,  $\\| \\mathbf{A} \\|_F$ ,  $\\| \\mathbf{A} \\|_1$  to denote its spectral norm, Frobenius norm, and matrix 1-norm. Given two sequences  $\\{a_n\\}$  and  $\\{b_n\\}$ , if there exists a constant  $0 < C < \\infty$  such that  $a_n \\leq C b_n$ , we write  $a_n = O(b_n)$ , and we use  $\\widetilde{O}(\\cdot)$  to hide the logarithmic factors.\n\n\n# 3.1 NEURAL LANGUAGE GENERATION\n\n\nWe first briefly review the softmax output layer typically used in neural language generation models. We define the joint probability of a given length- $n$  sentence of words (tokens)  $\\mathbf{s}_n = (\\mathbf{y}_1, \\dots, \\mathbf{y}_n)$  as the following product of conditional probabilities\n\n\n$$\n\n$$\n\\mathbb {P} \\left(\\mathbf {s} _ {n}\\right) = \\prod_ {t = 1} ^ {n} \\mathbb {P} \\left(\\mathbf {y} _ {t} \\mid \\mathbf {c} _ {t}\\right), \\tag {3.1}\n$$\n\n$$\n\n\nwhere  $\\mathbf{y}_t\\in \\mathcal{V}$  is the  $t$  -th word in sentence  $\\mathbf{s}_n$ , and  $\\mathcal{V}$  represents the word vocabulary,  $\\mathbf{c}_t = \\mathbf{y}_{1:t - 1} = (\\mathbf{y}_1,\\dots ,\\mathbf{y}_{t - 1})$  is referred to as the context of word  $\\mathbf{y}_t$ . In addition, the context  $\\mathbf{c}_t$  is usually modeled by a fixed size vector  $\\mathbf{h}_t\\in \\mathbb{R}^d$ , which is referred to as the hidden state, using some neural networks such as LSTM (Hochreiter & Schmidhuber, 1997) and Transformer (Vaswani et al., 2017). Then, the probability distribution of the output word  $\\mathbf{y}_t$  given the context  $\\mathbf{c}_t$ , i.e.,  $\\mathbb{P}(\\mathbf{y}_t|\\mathbf{c}_t)$  in (3.1), is parameterized as the following softmax function:\n\n\n$$\n\n$$\n\\mathbb {P} \\left(\\mathbf {Y} _ {t} = \\mathbf {y} _ {t} \\mid \\mathbf {c} _ {t}\\right) = \\mathbb {P} \\left(\\mathbf {Y} _ {t} = \\mathbf {y} _ {t} \\mid \\mathbf {h} _ {t}\\right) = \\frac {\\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\mathbf {W} _ {\\mathcal {I} (\\mathbf {y} _ {t}) *}\\right)}{\\sum_ {i = 1} ^ {N} \\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\mathbf {W} _ {i *}\\right)}, \\tag {3.2}\n$$\n\n$$\n\n\nwhere  $\\mathbf{W} \\in \\mathbb{R}^{N \\times d}$  is the weight matrix and is usually tied with the input word embedding matrix (Press & Wolf, 2017; Inan et al., 2017),  $N = |\\mathcal{V}|$  is the vocabulary size,  $d$  is the embedding dimension,  $\\mathcal{I}(\\mathbf{y}_t)$  represents the index of word  $\\mathbf{y}_t$  in vocabulary  $\\mathcal{V}$ . In the following discussion, we call output weight matrix  $\\mathbf{W}$  as the word embedding matrix of the neural language model since it is tied with the input word embedding matrix.\n\n\nIn this paper we focus on the output layer of the neural model for language generation, i.e., the softmax layer in (3.2), as in (Yang et al., 2018; Kanai et al., 2018; Ganea et al., 2019; Gao et al., 2019b). We will examine the singular value distribution of  $\\mathbf{W}$  and propose a different approach to increase the expressive power of the word embedding.\n\n\n# 3.2 FAST SINGULAR VALUE DECAY\n\n\nAs we mentioned before, the singular values of  $\\mathbf{W}$  tend to drop very fast and it may interact with the cone-shaped embedding space. More specifically, Figure 1(b) and 1(d) illustrate the distributions of the normalized singular values of the Transformer-XL based language model $^2$  (Dai et al., 2019)\n\n\nand the Transformer-based machine translation model (Vaswani et al., 2017) trained on WikiText-103 (Merit et al., 2018a) and IWSLT 2014 De-En (Cettolo et al., 2014) datasets, respectively. The plots show a fast singular value decay phenomenon, i.e., there is a huge drop between the first and remaining singular values. Such a phenomenon has also been observed in some previous work (Gao et al., 2019b; Wang et al., 2019a).\n\n\nFigure 1(a) and 1(c) present the distributions of the projected word embeddings of the aforementioned two models. We can see from the plots that the projected word embeddings are distributed into some narrow cone shapes, which implies an anisotropic property of the learned word representations, i.e., the embedding vectors are not uniformly distributed in the space. The detailed analysis in Ethayarajh (2019) also confirms that contextualized word embeddings learnt from ELMo (Peters et al., 2018) (LSTM-based model), BERT and GPT-2 (Devlin et al., 2019; Radford et al., 2019) (Transformer-based models) indeed tend to be anisotropic.\n\n\nThese contextualized word embeddings have shown great success on many NLP tasks. However, static word embeddings, such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) have been shown (Arora et al., 2016b; Mu & Viswanath, 2018) to be isotropic with great expressive power. Hence it may be also beneficial to increase the expressive power of the contextualized word embedding by increasing its isotropy. This motivates us to alleviate the fast singular value decay phenomenon to increase the isotropy of the learned word representations.\n\n\n# 4 PROPOSED METHOD\n\n\nTo alleviate the fast singular value decay phenomenon, we propose to guide the singular value distribution of the contextualized word embedding throughout the training. As a result, we can achieve a trade-off between modeling contextual information, that tends to make word representations anisotropic, and the expressive power of word representations, that tends to be isotropic.\n\n\n# 4.1 SVD REPARAMETERIZATION\n\n\nWe propose to apply singular value decomposition (SVD) based reparameterization to the embedding matrix  $\\mathbf{W}$ , i.e.,  $\\mathbf{W} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ , where  $\\mathbf{U} \\in \\mathbb{R}^{N \\times d}$ ,  $\\mathbf{V} \\in \\mathbb{R}^{d \\times d}$  are column orthonormal matrices, and  $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}$  is a diagonal matrix with  $\\Sigma_{kk} = \\sigma_k$  being the  $k$ -th largest singular value of  $\\mathbf{W}$ . Note that SVD reparameterization is standard and has been widely used in the literature such as model compression (Chen et al., 2018), training DNNs (Zhang et al., 2018), and analyzing word embeddings (Arora et al., 2016c). Given the SVD reparameterization, we can control the singular values of the embedding matrix  $\\mathbf{W}$  by constraining the matrix  $\\mathbf{E}$ , and the conditional distribution in (3.2) can be rewritten as follows:\n\n\n$$\n\n$$\n\\mathbb {P} \\left(\\mathbf {Y} _ {t} = \\mathbf {y} _ {t} \\mid \\mathbf {y} _ {<   t}\\right) = \\frac {\\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\left(\\mathbf {U} \\boldsymbol {\\Sigma} \\mathbf {V} ^ {\\top}\\right) _ {\\mathcal {I} \\left(\\mathbf {y} _ {t}\\right) *}\\right)}{\\sum_ {i = 1} ^ {N} \\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\left(\\mathbf {U} \\boldsymbol {\\Sigma} \\mathbf {V} ^ {\\top}\\right) _ {i *}\\right)} \\text {s u b j e c t t o} \\mathbf {U} ^ {\\top} \\mathbf {U} = \\mathbf {I}, \\mathbf {V} ^ {\\top} \\mathbf {V} = \\mathbf {I}, \\boldsymbol {\\Sigma} \\in \\mathcal {P}, \\tag {4.1}\n$$\n\n$$\n\n\nwhere  $\\mathcal{P}$  is a feasible set to represent the singular value distribution of the embedding matrix  $\\mathbf{W}$ .\n\n\nTo ensure the orthogonal constraints in (4.1), we propose to use the orthogonal regularization for  $\\mathbf{U},\\mathbf{V}$  during the training process. In particular, we use the following regularization, which is a linear combination of Frobenius norm and spectral norm errors:  $\\lambda_1\\| \\mathbf{U}^\\top \\mathbf{U} - \\mathbf{I}\\| _F^2 +\\lambda_2\\| \\mathbf{V}^\\top \\mathbf{V} - \\mathbf{I}\\| _F^2 +\\lambda_3\\| \\mathbf{U}^\\top \\mathbf{U} - \\mathbf{I}\\| _2^2 +\\lambda_4\\| \\mathbf{V}^\\top \\mathbf{V} - \\mathbf{I}\\| _2^2$  where  $\\{\\lambda_i\\}_{i = 1}^4$  are positive regularization parameters.\n\n\n# 4.2 SPECTRUM CONTROL\n\n\nRecall the SVD of  $\\mathbf{W} = \\mathbf{U}\\pmb{\\Sigma}\\mathbf{V}^{\\top}$ , and we consider the following two types of the singular value distribution for  $\\pmb{\\Sigma}$ , which is inspired by the eigenvalue distribution of kernel methods (Wei et al., 2017; Pacchiano et al., 2019):\n\n- - Exponential decay: we say the singular values  $\\{\\sigma_k\\}_{k=1}^d$  of  $\\mathbf{W}$  satisfy the exponential decay if  $\\mathbf{W} \\in \\mathcal{P}_e(\\gamma) = \\{\\mathbf{W} \\in \\mathbb{R}^{N \\times d} \\mid \\sigma_k \\leq c_1 \\exp(-c_2 k^\\gamma), k = 1, \\dots, d, \\gamma > 0, c_1, c_2 > 0$  are universal constants\\}.\n\n- - Polynomial decay: we say the singular values  $\\{\\sigma_k\\}_{k=1}^d$  of  $\\mathbf{W}$  satisfy the polynomial decay if  $\\mathbf{W} \\in \\mathcal{P}_p(\\gamma) = \\{\\mathbf{W} \\in \\mathbb{R}^{N \\times d} \\mid \\sigma_k \\leq c_1 k^{-\\gamma}, k = 1, \\dots, d, \\gamma > 0, c_1 > 0$  is a universal constant\\}.\n\n\nFor  $\\mathcal{P}_e(\\gamma)$  and  $\\mathcal{P}_p(\\gamma)$ , the parameter  $\\gamma$  controls the rate of singular value decay: the larger  $\\gamma$  is, the faster singular value decay will be. To ensure the learned word embedding matrix  $\\mathbf{W}$  have the desired singular value distributions, we propose to add the following regularizations to\n\n\nour training objective:  $\\mathcal{R}_e(\\Sigma) = \\lambda_e\\sum_{k = 1}^d\\left(\\sigma_k - c_1\\exp (-c_2k^\\gamma)\\right)^2$  for exponential decay and  $\\mathcal{R}_p(\\Sigma) = \\lambda_p\\sum_{k = 1}^d\\left(\\sigma_k - c_1k^{-\\gamma}\\right)^2$  for polynomial decay, where  $\\lambda_{e},\\lambda_{p}$  are positive regularization parameters. Although the concept of \"spectrum control\" was previously used in Jiang et al. (2019) to improve the training of GANs, our spectrum control method is trying to solve a totally different problem, i.e., neural language generation, and its motivation is coming from a very different perspective, i.e., the representation degeneration of the word representations. In addition, our method of controlling the singular value with prior distributions is significantly different from the penalty function used in their method. Finally, our method is essential to improve the performance of the neural language generation, while the penalty function proposed in Jiang et al. (2019) can deteriorate the training of neural language models, as we illustrated in Appendix C.\n\n\n# 4.3 THEORETICAL ANALYSIS\n\n\nIn this subsection, we show some theoretical insights on why our proposed method can improve the performance of the NLG model. In particular, we follow the similar setup as considered in (Gao et al., 2019b), i.e., focusing on the optimization of the embedding matrix  $\\mathbf{W} \\in \\mathbb{R}^{N \\times d}$  and assume all the other parameters are fixed and well-optimized. In practice, we use our method to train the models from scratch. Therefore, according to the output layer in (3.2), we consider the following empirical risk minimization problem: given a training dataset  $S = \\{(\\mathbf{h}_i, y_i)\\}_{i=1}^n$  with each example drawn i.i.d. from some unknown but fixed distribution  $\\mathcal{D}$ , and  $\\mathbf{h}_i \\in \\mathbb{R}^d$  as a hidden state,  $y_i \\in \\{1, \\dots, N\\}$  as its associated label, our goal is to minimize the training loss as follows:\n\n\n$$\n\n$$\n\\min  _ {\\mathbf {W} \\in \\mathbb {R} ^ {N \\times d}} L _ {S} (\\mathbf {W}) = \\frac {1}{n} \\sum_ {i = 1} ^ {n} \\ell \\left(\\mathbf {h} _ {i} ^ {\\top} \\mathbf {W}, y _ {i}\\right), \\tag {4.2}\n$$\n\n$$\n\n\nwhere  $\\ell(\\mathbf{h}_i^\\top \\mathbf{W}, y_i)$  is the cross-entropy loss with respect to  $\\mathbf{h}_i^\\top \\mathbf{W}$  and  $y_i$ . The cross-entropy loss defined above is widely used to train NLG models, and is also used to compute the perplexity of the trained model, which is the benchmark criterion to evaluate the performance of language models.\n\n\nIn addition, we define the expected loss as follows  $L_{\\mathcal{D}}(\\mathbf{W}) = \\mathbb{E}[\\ell (\\mathbf{h}^{\\top}\\mathbf{W},y)]$ , where the expectation is taken over the distribution  $\\mathcal{D}$  of the training data.\n\n\nLet  $\\widehat{\\mathbf{W}} = \\arg \\min_{\\mathbf{W} \\in \\mathcal{P}(\\gamma)} L_S(\\mathbf{W})$ , where  $\\mathcal{P}(\\gamma) = \\mathcal{P}_e(\\gamma)$  for exponential decay and  $\\mathcal{P}(\\gamma) = \\mathcal{P}_p(\\gamma)$  for polynomial decay. We assume the right singular vector matrix satisfies  $\\| \\mathbf{V} \\|_1 \\leq V$  for all  $\\mathbf{W} \\in \\mathcal{P}(\\gamma)$ . Now, we are ready to provide the main theory of our method (The proof can be found in Appendix A).\n\n\nTheorem 4.1. Under previously stated conditions, suppose that  $|\\ell(\\cdot)| \\leq B$ ,  $\\ell$  is  $G$ -Lipschitz continuous, and  $\\| \\mathbf{h}_i \\|_{\\infty} \\leq H$  for all  $i = 1, \\ldots, n$ . If we choose  $\\gamma > 1/2$ , then with probability at least  $1 - \\delta$ , we have\n\n\n$$\n\n$$\nL _ {\\mathcal {D}} (\\widehat {\\mathbf {W}}) \\leq \\min  _ {\\mathbf {W} \\in \\mathcal {P} (\\gamma)} L _ {S} (\\mathbf {W}) + \\frac {C _ {1} A N \\left(\\sqrt {\\sum_ {j = 1} ^ {m - 1} \\sigma_ {j} ^ {2}} + \\sqrt {m ^ {1 - 2 \\gamma} / (2 \\gamma - 1)}\\right) + C _ {2} B \\sqrt {\\log (1 / \\delta)}}{\\sqrt {n}}, \\tag {4.3}\n$$\n\n$$\n\n\nwhere  $C_1, C_2$  are absolute constants,  $m \\in [2, d]$ ,  $A = GVH\\sqrt{\\log d}$ .\n\n\nRemark 4.2. According to Theorem 4.1, the expected loss of the learned embedding matrix  $\\widehat{\\mathbf{W}}$  consists of two terms. The first term represents the training loss, the second term is the generalization error gap. Specifically, the smaller the  $\\gamma$ , the larger the feasible set  $\\mathcal{P}(\\gamma)$ , thus the smaller the training loss  $\\min_{\\mathbf{W} \\in \\mathcal{P}(\\gamma)} L_S(\\mathbf{W})$ . On the other hand, the larger the  $\\gamma$ , the faster the singular value decays, and the smaller the generalization error gap. Therefore, our generalization error bound demonstrates an appealing property of our proposed method: by directly controlling the singular value distribution of the learned word embedding, we are able to achieve a trade-off between the training loss and generalization error.\n\n\nRemark 4.3. For the generalization error bound in (4.3), penalizing the largest singular value could reduce this upper bound. It validates the method proposed in Gao et al. (2019b), which implicitly penalizes the largest singular value (c.f. Section 5 in Gao et al. (2019b)). Compared with their method, the error term  $\\widetilde{O}\\left(N\\sqrt{m^{1 - 2\\gamma} / (2\\gamma - 1)} /\\sqrt{n}\\right)$  suggests that by explicitly manipulating the singular value distribution, our method has a better control of the tail sum of the singular values.\n\n\n# 5 EXPERIMENTS\n\n\nWe demonstrate the effectiveness of our proposed spectrum control algorithm on two tasks: language modeling and machine translation. We compare our results with the state-of-the-art models. In our experiments, we try both exponential and polynomial singular value decays, and present the one with the better result. The performances of exponential decay and polynomial decay are very close (See Appendix E). In practice we found that for large scale dataset its better to use polynomial decay and for small scale dataset its better to use exponential decay.\n\n\n# 5.1 LANGUAGE MODELING\n\n\nDatasets We consider two benchmark datasets for language modeling: WikiText-2 and WikiText-103, which consist of pre-processed Wikipedia articles and were introduced by Merity et al. (2018a). WikiText-2 is a small dataset with around 2 million words and 30K vocabulary size, while WikiText-103 is a significantly large dataset with around 103 million words and 260K vocabulary size.\n\n\nModel Configuration On the small WikiText-2 dataset, we implement our method based on the state-of-the-art AWD-LSTM model (Merit et al., 2018a). It is a 3-layer LSTM model with 1150 dimensional hidden states and 400 dimensional embeddings. We also follow the same regularization and optimization procedures introduced in (Merit et al., 2018a). The implementation of our method is based on the open-source code<sup>3</sup> for AWD-LSTM. On the large WikiText-103 dataset, we implement our method based on the state-of-the-art Transformer-XL based models (Dai et al., 2019). We follow the same settings reported in (Dai et al., 2019), and our implementation is based on the official code<sup>4</sup> for Transformer-XL. To evaluate the performance of our method more thoroughly, we consider two Transformer-XL models with different number of layers. The first is the standard Transformer-XL model with 16 layers used in (Dai et al., 2019). For the second, we consider a smaller Transformer-XL model with just 4 layers and other configurations unchanged.\n\n\nParameters For the parameters  $\\{\\lambda_i\\}_{i=1}^4$  of the orthogonal regularizations, we tune them by grid search over  $\\{0.01, 0.1, 1, 10\\}$ . For the parameters  $\\lambda_e, \\lambda_p$  of the spectrum control, we tune them over the grid  $\\{0.1, 1, 10, 100\\}$ . We try different singular value distributions, and the best distributions for AWD-LSTM and Transformer-XL are exponential and polynomial, respectively.\n\n\nResults of the LSTM Model on WikiText-2 We first present the results of language modeling on the small dataset WikiText-2 using LSTM models. In Table 1 we compare the validation/test perplexity<sup>5</sup> of the baseline AWD-LSTM model (Merit et al., 2018a), the cosine similarity regularization (MLE-CosReg) model (Gao et al., 2019b), and the models trained using our method under three different settings (Merit et al., 2018a): without finetune, with finetune and with further continuous cache pointer. Compared with the baselines, our method achieves  $2.3 / 2.9 / 2.3$  test perplexity reduction under all three settings; compared with the MLE-CosReg method, our method achieves  $1.5 / 1.2 / 0.3$  test perplexity reduction under all three settings.\n\n\nResults of the Transformer-XL Model on WikiText-103 We next show the results of language modeling on the large dataset WikiText-103 using Transformer-XL models. Table 2 compares the validation/test perplexity of Transformer-XL based models (Dai et al., 2019) and the models trained by our method on WikiText-103 dataset. The results demonstrate that our method consistently improves upon the small Transformer-XL model (0.9 test perplexity reduction) and the standard Transformer-XL model (0.8 test perplexity reduction).\n\n\nAnalysis We study the output embedding matrix of Transformer-XL trained on WikiText-103 using our method. In particular, we want to evaluate the isotropy of the learned word representations. We consider the partition function  $Z(\\mathbf{a}) = \\sum_{i=1}^{N} \\exp(\\langle \\mathbf{w}_i, \\mathbf{a} \\rangle)$  introduced in (Arora et al., 2016b), where  $\\mathbf{w}_i$  is the  $i$ -th row of the embedding matrix  $\\mathbf{W} \\in \\mathbb{R}^{N \\times d}$  and  $\\mathbf{a} \\in S^{d-1}$  is a unit vector. According to Lemma 2.1 in (Arora et al., 2016b), if the word representation vectors are isotropic,  $Z(\\mathbf{a})$  is close to some constant with high probability for all unit vectors. Thus to empirically measure\n\n\nTable 1: Comparison of different methods in terms of perplexity on WikiText-2 dataset for the task of language modeling.\n\n\n<table><tr><td>Method</td><td>Parameters</td><td>Validation</td><td>Test</td></tr><tr><td colspan=\"4\">Existing results</td></tr><tr><td>Variational LSTM (Inan et al., 2017)</td><td>51M</td><td>91.5</td><td>87.0</td></tr><tr><td>2-layer skip connection LSTM (Mandt et al., 2017)</td><td>24M</td><td>69.1</td><td>65.9</td></tr><tr><td colspan=\"4\">w/o finetune</td></tr><tr><td>AWD-LSTM (Merit et al., 2018a)</td><td>33M</td><td>69.1</td><td>66.0</td></tr><tr><td>MLE-CosReg (Gao et al., 2019b)</td><td>33M</td><td>68.2</td><td>65.2</td></tr><tr><td>Ours</td><td>33M</td><td>66.3</td><td>63.7</td></tr><tr><td colspan=\"4\">+ finetune</td></tr><tr><td>AWD-LSTM (Merit et al., 2018a)</td><td>33M</td><td>68.6</td><td>65.8</td></tr><tr><td>MLE-CosReg (Gao et al., 2019b)</td><td>33M</td><td>67.1</td><td>64.1</td></tr><tr><td>Ours</td><td>33M</td><td>65.3</td><td>62.9</td></tr><tr><td colspan=\"4\">+ continuous cache pointer</td></tr><tr><td>AWD-LSTM (Merit et al., 2018a)</td><td>33M</td><td>53.8</td><td>52.0</td></tr><tr><td>MLE-CosReg (Gao et al., 2019b)</td><td>33M</td><td>51.7</td><td>50.0</td></tr><tr><td>Ours</td><td>33M</td><td>51.1</td><td>49.7</td></tr></table>\n\n\nTable 2: Comparison of different methods in terms of perplexity on WikiText-103 dataset for the task of language modeling.\n\n\n<table><tr><td>Method</td><td>Parameters</td><td>Validation</td><td>Test</td></tr><tr><td colspan=\"4\">Existing results</td></tr><tr><td>4 layer QRNN (Merit et al., 2018b)</td><td>151M</td><td>32.0</td><td>33.0</td></tr><tr><td>Hebbian + Cache (Rae et al., 2018)</td><td>-</td><td>29.7</td><td>29.9</td></tr><tr><td>Small Transformer-XL (Dai et al., 2019)</td><td>120M</td><td>29.6</td><td>30.4</td></tr><tr><td>Ours</td><td>120M</td><td>29.0</td><td>29.5</td></tr><tr><td>Standard Transformer-XL (Dai et al., 2019)</td><td>151M</td><td>23.1</td><td>24.0</td></tr><tr><td>Ours</td><td>151M</td><td>22.9</td><td>23.2</td></tr></table>\n\n\nthe isotropy of the learned word representations, we consider two criteria based on  $Z(\\mathbf{a})$ :\n\n\n$$\n\n$$\nI _ {1} (\\mathbf {W}) = \\frac {\\operatorname* {m i n} _ {\\mathbf {a} \\in \\mathcal {E}} Z (\\mathbf {a})}{\\operatorname* {m a x} _ {\\mathbf {a} \\in \\mathcal {E}} Z (\\mathbf {a})} \\quad \\text {a n d} \\quad I _ {2} (\\mathbf {W}) = \\sqrt {\\frac {\\sum_ {\\mathbf {a} \\in \\mathcal {E}} (Z (\\mathbf {a}) - \\bar {Z} (\\mathbf {a})) ^ {2}}{| \\mathcal {E} | \\bar {Z} (\\mathbf {a}) ^ {2}}},\n$$\n\n$$\n\n\nwhere  $\\mathcal{E}$  is the set of eigenvectors of  $\\mathbf{W}^{\\top}\\mathbf{W}$ , as suggested by Mu & Viswanath (2018). We also propose to check sampled standard deviation measure  $I_{2}(\\mathbf{W})$  (normalized by its average, i.e.,  $\\bar{Z}(\\mathbf{a})$ ). We have  $I_{1}(\\mathbf{W}) \\in [0,1]$  and  $I_{2}(\\mathbf{W}) \\geq 0$ . Larger  $I_{1}(\\mathbf{W})$  and smaller  $I_{2}(\\mathbf{W})$  indicate more isotropic for word embeddings. We uniformly sample 40K words from the vocabulary (around 260K) of WikiText-103 to compute these two criteria. The left half of Table 3 summarizes the values of  $I_{1}(\\mathbf{W})$  and  $I_{2}(\\mathbf{W})$ , which are averaged over 10 runs, for the baseline method and our method. We can see from the results that our method significantly improves the isotropy of the learned word representations in terms of both criteria.\n\n\nTable 3: Comparison of different methods in terms of isotropy for the tasks of language modeling and machine translation. (For perfect isotropy,  $I_{1}(\\mathbf{W}) = 1$ ,  $I_{2}(\\mathbf{W}) = 0$ .)\n\n\n<table><tr><td colspan=\"3\">Language Modeling</td><td colspan=\"3\">Machine Translation</td></tr><tr><td>Method</td><td>I1(W)</td><td>I2(W)</td><td>Method</td><td>I1(W)</td><td>I2(W)</td></tr><tr><td>Standard Transformer-XL</td><td>0.24</td><td>0.037</td><td>Transformer-Base</td><td>0.31</td><td>0.031</td></tr><tr><td>Ours</td><td>0.63</td><td>0.022</td><td>Ours</td><td>0.88</td><td>0.005</td></tr></table>\n\n\nTable 4: Comparison of different methods in terms of BLEU scores on the task of De\u2192En machine translation, trained on IWSLT 2014 dataset.\n\n\n<table><tr><td rowspan=\"2\">IWSLT 2014 De\u2192En</td><td colspan=\"4\">Method</td></tr><tr><td>Adversarial (Wang et al., 2019a)</td><td>Dual-learning (Wang et al., 2019b)</td><td>Transformer-Base (Wang et al., 2019b)</td><td>Ours</td></tr><tr><td>BLEU</td><td>35.18</td><td>35.44</td><td>34.01</td><td>35.50</td></tr></table>\n\n\n# 5.2 MACHINE TRANSLATION\n\n\nWe also apply our spectrum control method to machine translation tasks. Given a source sentence  $s$ , the decoder of an neural machine translation (NMT) model is to predict the next word in the target sentence  $t$  and the previous decoded words in  $t$ . In the following we use the state-of-the-art Transformer-based NMT model as our baseline.\n\n\nDatasets We compare various NMT models on the IWSLT 2014 German\u2192English (De-En) dataset (Cettolo et al., 2014). We follow the same setup as in (Gehring et al., 2017). More specifically, we have 160K sentence pairs as the training data, 7K sentence pairs as the validation data, and we combine tst2010, tst2011, tst2012, dev2010 and dev2012 datasets to form our test data.\n\n\nModel Configuration We implement our method on top of Transformer model (Vaswani et al., 2017). In particular, we consider the Transformer-Base architecture (Vaswani et al., 2017), which has a 6-layer encoder and 6-layer decoder with 512 dimensional hidden states and embeddings, except that we choose the dimension of the inner feed-forward layer as 1024 instead of 2048 and the number of attention heads is set to be 4 rather than 8. Our implementation is based on the open-sourced code provided by Ott et al. (2018). We follow the same procedures as in the language modeling task to choose the regularization parameters. In addition, the best results for this task are from the models where the singular values are controlled by exponential distribution.\n\n\nResults Comparisons of different methods in terms of BLEU scores are summarized in Table 4. Compared with the baseline models, our method improves the BLEU score from 34.01 to 35.50 on the German\u2192English task, close to 1.5 gain on BLEU score; our method is also better than 35.18 reported in (Wang et al., 2019a), and 35.44 reported in (Wang et al., 2019b).\n\n\nAnalysis We also study the learned word embedding matrix of the Transformer trained on IWSLT 2014 De-En using our method in terms of isotropy. The values of  $I_{1}(\\mathbf{W})$  and  $I_{2}(\\mathbf{W})$  are reported in the right half of Table 3, we compute these two values based on all the tokens. It shows that the isotropy of the learned word representations using our method increases significantly in terms of these criteria, from very anisotropic to nearly isotropic. We also demonstrate the projected word embedding matrices and the singular value distributions of different methods in Figure 3. The plots show that the learned word representations from our method are distributed isotropically in the space, which is in contrast to the narrow cone distribution from the baseline method.\n\n\n![(a) Transformer for NMT](images/a3a3e55cd643e2fae8cd9afe73fdf5428e123204a82941a8f82c90127deb9751.jpg)\n\n\n![(b) Transformer for NMT+SC](images/cb27d7792c80b28b706465fbd93e32f3a9c044bdbe576634467d8646fafb3202.jpg)\n\n\n![(c) Singular Value Decay Figure 3: (a) Word embedding for the vanilla Transformer, which has a narrow cone distribution; (b) Word embedding for Transformer using spectrum control, which has a uniform distribution; (c) Normalized singular value for different methods, which shows a slow decay of our method.](images/818c0e945de1b87307fd1f72b37ccc2013c067d4c85430209bc7cff16049e95d.jpg)\n\n\n# 6 CONCLUSIONS AND FUTURE WORK\n\n\nIn this paper, we tackle the degeneration problem that occurs at the output embeddings in the softmax layer used in neural language generation models. We develop a novel spectrum control method to explicitly guide the spectra training of the output embedding matrix with some slow-decaying singular value prior distributions. Our proposed method is shown to alleviate the degeneration problem and improve isotropy of the learned contextualized word representations. Thorough experimental results demonstrate the advantage of our method over the state-of-the-art neural models for language model and machine translation. Since our work is orthogonal to Wang et al. (2019a), it would be interesting to combine the adversarial softmax training with our spectrum control method, and investigate its performance. For the future work, we would also like to investigate how the frequency-agnostic word representations in (Gong et al., 2018) relate to the single value distribution of the output embedding matrix.\n\n\n# REFERENCES\n\n- Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. NeurIPS, 2019.\n\n- Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach to pmi-based word embeddings. TACL, 4:385-399, 2016a.\n\n- Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach to pmi-based word embeddings. TACL, 4:385-399, 2016b.\n\n- Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. 2016c.\n\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICML, 2015.\n\n- Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. JMLR, 2002.\n\n- Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. In Proceedings of the Twentieth Conference on Computational Natural Language Learning, 2016.\n\n- Mauro Cettolo, Jan Niehues, Sebastian St\u00fcker, Luisa Bentivogli, and Marcello Federico. Report on the 11th iwslt evaluation campaign, iwslt 2014. In SLT, 2014.\n\n- Patrick Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-Jui Hsieh. Groupreduce: Block-wise low-rank approximation for neural language model shrinking. In Advances in Neural Information Processing Systems, pp. 10988-10998, 2018.\n\n- Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. ACL, 2019.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In *NAACL*, 2019.\n\n- Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. EMNLP, 2019.\n\n- Octavian-Eugen Ganea, Sylvain Gelly, Gary B\u00e9cigneul, and Aliaksei Severyn. Breaking the softmax bottleneck via learnable monotonic pointwise non-linearities. ICML, 2019.\n\n- Jianfeng Gao, Michel Galley, Lihong Li, et al. Neural approaches to conversational ai. Foundations and Trends\u00ae in Information Retrieval, 13(2-3):127-298, 2019a.\n\n- Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tieyan Liu. Representation degeneration problem in training natural language generation models. In ICLR, 2019b.\n\n- Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In ICML, pp. 1243-1252, 2017.\n\n- Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Frage: frequency-agnostic word representation. In NIPS, 2018.\n\n- A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, 2013.\n\n- Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.\n\n- Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. In ICLR, 2017.\n\n- Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo Zhao. On computation and generalization of generative adversarial networks under spectrum control. In ICLR, 2019.\n\n- Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n\n- Sekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka, and Shuichi Adachi. Sigsoftmax: Reanalysis of the softmax bottleneck. In NIPS, 2018.\n\n- Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional transformer language model for controllable generation. arXiv:1909.05858, 2019.\n\n- Percy Liang. Cs229t/stat231: Statistical learning theory (winter 2016), 2014.\n\n- Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian inference. JMLR, 2017.\n\n- Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. In ICLR, 2018a.\n\n- Stephen Merity, Nitish Shirish Keskar, and Richard Socher. An analysis of neural language modeling at multiple scales. arXiv preprint arXiv:1803.08240, 2018b.\n\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\n- Jiaqi Mu and Pramod Viswanath. All-but-the-top: Simple and effective postprocessing for word representations. In ICLR, 2018.\n\n- Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation, 2018.\n\n- Aldo Pacchiano, Niladri S Chatterji, and Peter L Bartlett. Online learning with kernel losses. ICML, 2019.\n\n- Nikolaos Pappas and James Henderson. Deep residual output layers for neural language generation. In ICML, 2019.\n\n- Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In EMNLP, 2014.\n\n- Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In *NAACL*, 2018.\n\n- Ofir Press and Lior Wolf. Using the output embedding to improve language models. In EACL, 2017.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\n\n- Jack Rae, Chris Dyer, Peter Dayan, and Timothy Lillicrap. Fast parametric learning with activation memorization. In ICML, 2018.\n\n- Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric P. Xing, and Zhiting Hu. Target-guided open-domain conversation. In ACL, 2019.\n\n- Shubham Toshniwal, Anjuli Kannan, Chung-Cheng Chiu, Yonghui Wu, Tara Sainath, and Karen Livescu. A comparison of techniques for language model integration in encoder-decoder speech recognition. In IEEE Workshop on Spoken Language Technology, 2018.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n\n- Dilin Wang, Chengyue Gong, and Qiang Liu. Improving neural language modeling via adversarial training. In ICML, 2019a.\n\n- Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multiagent dual learning. In ICLR, 2019b.\n\n- Yuting Wei, Fanny Yang, and Martin J Wainwright. Early stopping for kernel boosting algorithms: A general analysis with localized complexities. In NIPS, 2017.\n\n- Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. Breaking the softmax bottleneck: A high-rank RNN language model. In *ICLR*, 2018.\n\n- Jiong Zhang, Qi Lei, and Inderjit Dhillon. Stabilizing gradients for deep neural networks via efficient svd parameterization. In International Conference on Machine Learning, pp. 5801-5809, 2018.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638419.0634718", "msecs": "63.0", "relativeCreated": "15893.969297409058", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Multi-Label Knowledge Distillation\n\n# Abstract\nExisting knowledge distillation methods typically work by enforcing the consistency of output logits or intermediate feature maps between the teacher network and student network. Unfortunately, these methods can hardly be extended to the multi-label learning scenario. Because each instance is associated with multiple semantic labels, neither the prediction logits nor the feature maps obtained from the whole example can accurately transfer knowledge for each label. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by label decoupling with the one-versus-all reduction strategy; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the proposed method can avoid knowledge counteraction among labels, and achieve superior performance against diverse comparing methods.\n\n\n# 1 INTRODUCTION\n\n\nDespite the remarkable success in training deep neural networks (DNNs) (Krizhevsky et al., 2012), it is hard to deploy these large neural networks on lightweight terminals, e.g., mobile phones, under the constraint of computational resource or requirement of short inference time. To mitigate this issue, knowledge distillation (Hinton et al., 2015) aims to improve the performance of a small network (also known as the student) by requiring the knowledge from a large network (also known as the teacher) to guide the training of the student network. Existing knowledge distillation methods can be roughly divided into two categories: logits-based methods and feature-based methods. The former minimizes the difference between logits of teacher model and student model (Hinton et al., 2015; Zhao et al., 2022), while the latter distills knowledge from feature maps of intermediate layers (Park et al., 2019; Tian et al., 2019; Chen et al., 2021).\n\n\nTypical knowledge distillation methods focus on the multi-class classification task, where each instance is associated with only one class label. However, in many real-world scenarios, an instance inherently contains complex semantics and can be simultaneously assigned with multiple class labels. For example, an image of street scene may be annotated with labels building, car and person. To learn the complex object-label mapping, there is always necessity of training large models to obtain desirable performance in multi-label classification. Unfortunately, due to computational resource constraints, it cannot be allowed to adopt large neural networks in many practical applications, leading to noticeable decrease in model performance (Gou et al., 2021). To alleviate the performance degradation, it is necessary to design specific knowledge distillation methods for multi-label learning. We formalize such a problem as a new learning framework called Multi-Label Knowledge Distillation (MLKD).\n\n\nAlthough knowledge distillation has been proven to be effective for improving the performance of the student network in single-label classification, it is still a challenging problem to directly extend existing KD methods to solve MLKD problems. Specifically, logits-based methods often obtain the predicted probabilities based on the softmax function, which is unavailable for MLKD, since the sum of predicted probabilities may not equal to one in Multi-Label Learning (MLL). Feature-based methods often perform knowledge distillation based on feature maps of the whole image with multiple semantics, which makes the model focus on the major objects while neglect the minor objects. This would lead the model to obtain sub-optimal even undesirable distillation performance. Figure 1 provides empirical validations for these observations.\n\n\n![(a) Models with the same architecture](images/f19190e708504d97ed3128099f7c12e9c7ec22ab86ab96476cbbcf643736c555.jpg)\n\n\n![(b) Models with different architectures Figure 1: Comparison results between our proposed L2D method and conventional KD methods on MS-COCO. We compare our method with the following three baselines: 1) Vanilla: student trained without distillation; 2) Softmax (Hinton et al., 2015): a representative logits-based method by measuring KL divergence on softmax score after logits; 3) ReviewKD (Chen et al., 2021): a feature-based method that achieves sota performance. The red dashed lines mark the performance of teachers. Conventional KD methods achieve unfavorable performance similar to the student, while our method significantly outperforms these methods and achieves comparable performance to the teacher. Details about the implementation of this experiment can be found in Appendix A.](images/8eea145e1dc78c1c6a262d374b1f7762f2d7f4f3908017810b858a491913b47f.jpg)\n\n\nIn this paper, to perform multi-label knowledge distillation, we propose a new method consisting of multi-label logits distillation and label-wise embedding distillation (L2D for short). Specifically, to exploit informative semantic knowledge compressed in the logits, L2D employs the one-versus-all reduction strategy to obtain a series of binary classification problems and perform logits distillation for each one. To enhance the distinctiveness of learned feature representations, L2D encourages the student model to maintain a consistent structure of intra-class and intra-instance (inter-class) label-wise embeddings with the teacher model. By leveraging the structural information of the teacher model, these two structural consistencies respectively enhance the compactness of intra-class embeddings and dispersion of inter-class embeddings for the student model.\n\n\nOur main contributions can be summarized as follows:\n\n- - A general learning framework called MLKD is proposed. To our best knowledge, the framework is the first study specially designed for knowledge distillation in the multi-label learning scenario.\n\n- - A new approach called L2D is proposed. It performs multi-label logits distillation and label-wise embedding distillation simultaneously. The former provides informative semantic knowledge while the latter encourages the student model to learn more distinctive feature representations.\n\n- - Extensive experimental results on benchmark datasets demonstrate the effectiveness of our proposed method.\n\n\n# 2 RELATED WORK\n\n\nThe concept of knowledge distillation (KD) proposed by Hinton et al. (2015) defines a learning framework that transfers knowledge from a large teacher network to a small student network. Existing works can be roughly divided into two groups, i.e., logits-based methods and feature-based methods. Logits-based methods mainly focus on designing effective distillation losses to distill knowledge from logits and softmax scores after logits. DML (Zhang et al., 2018) introduces a mutual learning method to train both teachers and students simultaneously. TAKD (Mirzadeh et al., 2020) proposes a new architecture called \"teacher assistant\", which is an intermediate-sized network bridging the gap between teachers and students. Besides, a recent study (Zhao et al., 2022) proposes a novel logits-based method to reformulate the classical KD loss into two parts and achieves the state-of-the-art performance by adjusting weights for these two parts. Some other methods focus on distilling knowledge from intermediate feature layers. FitNet (Romero et al., 2014) is the first approach to distill knowledge from intermediate features by measuring the distance between feature maps. Attention transfer (Zagoruyko & Komodakis, 2016a) achieves better performance than FitNet\n\n\nby distilling knowledge from the attention maps. PKT (Passalis & Tefas, 2018) measures the KL divergence between features by treating them as probability distributions. RKD (Park et al., 2019) utilizes the relations among instances to guide the training process of the student model. CRD (Tian et al., 2019) incorporates contrastive learning into knowledge distillation. ReviewKD (Chen et al., 2021) proposes a review mechanism which uses multiple layers in the teacher to supervise one layer in the student. ITRD (Miles et al., 2021) aims to maximize the correlation and mutual information between the students' and teachers' representations.\n\n\nMulti-label learning has increasingly attracted a lot of interest recently. Existing solutions for solving MLL problems can be categorized into three directions. The first type attempts to design novel loss functions for tackling the intrinsic positive-negative imbalance issue in multi-label classification tasks. For example, ASL (Ridnik et al., 2021a) uses different weights to re-weight positive and negative examples for the balanced training. The second type focuses on modeling the label correlations, which provides prior knowledge for multi-label classification. Among them, MLGCN (Chen et al., 2019b) is a representative method that employs a graph convolutional network to model correlation matrix. CADM (Chen et al., 2019a) constructs a similar graph based on class-aware maps. To handle the multiple semantic objects contained in an image, the last type of methods aims to locate areas of interest related to semantic labels by using attention techniques. Among them, C-Tran (Lanchantin et al., 2021) first utilizes the transformer to retrieve embeddings from visual features for each label. Query2Label (Liu et al., 2021a) uses several stacked transformer encoders to identify interesting areas. ML-Decoder (Ridnik et al., 2021b) simplifies transformer encoders in Query2Label. ADDS (Xu et al., 2022b) introduces encoders from CLIP (Radford et al., 2021) in order to get better textual and visual embedding inputs to the classification head. In addition, ADDS adds a multi-head cross-attention layer and a skipping connection from the query input to the query output based on ML-Decoder.\n\n\nSeveral previous studies have applied KD techniques to improve the performance of MLL. For example, Liu et al. (2018) and Xu et al. (2022a) simply minimized mean squared error (MSE) loss between teacher logits and student logits. Song et al. (2021) designed a partial softmax function by combining a positive label with all other negative labels. Then, the conventional KD loss can be computed for each positive label. The main difference between our work and previous works is that we aim to improve the distillation performance instead of the performance of MLL. This can be reflected in experiments in Section 4 and Appendix B, where the proposed method mainly compares with KD methods while the previous method mainly compares with MLL methods.\n\n\n# 3 THE PROPOSED APPROACH\n\n\nLet  $\\pmb{x} \\in \\mathcal{X}$  be an instance and  $\\pmb{y} \\in \\mathcal{Y}$  be its corresponding label vector, where  $\\mathcal{X} \\subset \\mathbb{R}^d$  is the input space with  $d$  dimensions and  $\\mathcal{Y} \\subset \\{0,1\\}^q$  is the target space with  $q$  class labels. We further use  $y_j$  to denote the  $j$ -th component of  $\\pmb{y}$ . For a given instance  $\\pmb{x}$ ,  $y_j = 1$  indicates the  $j$ -th label is relevant to the instance;  $y_j = 0$ , otherwise. In multi-label learning, each instance may be assigned with more than one label, which means  $\\sum_{j=1}^{q} \\mathbb{I}(y_j = 1) \\geq 1$ , where  $\\mathbb{I}(\\cdot)$  is the indicator function. We also denote by  $[q]$  the integer set  $\\{1,2,\\dots,q\\}$ .\n\n\nIn this paper, we use a classification model consisting of three components, i.e., a visual backbone  $f$ , which extracts a feature map  $f(\\pmb{x})$  for the input  $\\pmb{x}$ , a label-wise embedding encoder  $g$  (Lanchantin et al., 2021; Liu et al., 2021a), which produces a label-wise embedding  $e_k = g_k(f(\\pmb{x}))$  with respect to the  $k$ -th class based on the feature map  $f(\\pmb{x})$ , and a multi-label classifier  $h$ , which predicts multi-label probabilities  $\\hat{\\pmb{y}} = [\\sigma(h_1(e_1)), \\sigma(h_2(e_2)), \\dots, \\sigma(h_q(e_q))]$ , where  $\\sigma(\\cdot)$  denotes the sigmoid function. It is noteworthy that the used model is very general, which can be built by equipping commonly used backbones, e.g., ResNet (He et al., 2016), with a label-wise embedding encoder  $g$ . For the notations mentioned above, we use the superscripts  $\\mathcal{T}$  (or  $S$ ) to denote the teacher (or student) model. For example, we use  $e_k^T$  and  $e_k^S$  to denote the label-wise embeddings for teacher and student models.\n\n\nIn multi-label learning, a popular method is to employ the one-versus-all reduction strategy to transform the original task into multiple binary problems. Among the various loss functions, the most commonly used one is the binary cross entropy (BCE) loss. Specifically, given a batch of examples\n\n\n![Figure 2: An illustration of the L2D framework. The framework simultaneously performs multi-label logits distillation and label-wise embedding distillation to improve the performance of the student model.](images/3aac4bb2349429772ac122d178545b82a5989e70f1e03e86fe96473b48c9979a.jpg)\n\n\n$\\{(\\pmb{x}_i,\\pmb{y}_i)\\}_{i = 1}^b$  and the predicted probabilities  $\\hat{\\pmb{y}}$ , the BCE loss can be defined as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {B C E}} = - \\frac {1}{b} \\sum_ {i = 1} ^ {b} \\sum_ {k = 1} ^ {q} y _ {i k} \\log \\left(\\hat {y} _ {i k}\\right) + (1 - y _ {i k}) \\log \\left(1 - \\hat {y} _ {i k}\\right). \\tag {1}\n$$\n\n$$\n\n\nFigure 2 illustrates the distillation process of the proposed L2D framework. For a batch of training examples, we feed them into the teacher/student model to obtain the label-wise embeddings and predicted probabilities. In order to train the student model, besides the BCE loss, we design the following two distillation losses: 1) multi-label logits distillation loss  $\\mathcal{L}_{\\mathrm{MLD}}$  to exploit informative semantic knowledge compressed in the logits, 2) label-wise embedding distillation loss  $\\mathcal{L}_{\\mathrm{LED}}$  to leverage structural information for enhancing the distinctiveness of learned feature representations. The overall objective function can be presented as\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {L 2 D}} = \\mathcal {L} _ {\\mathrm {B C E}} + \\lambda_ {\\mathrm {M L D}} \\mathcal {L} _ {\\mathrm {M L D}} + \\lambda_ {\\mathrm {L E D}} \\mathcal {L} _ {\\mathrm {L E D}}, \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\lambda_{\\mathrm{MLD}}$  and  $\\lambda_{\\mathrm{LED}}$  are two balancing parameters.\n\n\n# 3.1 MULTI-LABEL LOGITS DISTILLATION\n\n\nTraditional logits-based distillation normally minimizes the Kullback-Leibler (KL) divergence between the predicted probabilities, i.e., the logits after the softmax function, of teacher and student model. However, the method cannot be directly applied to the MLL scenario, since it depends on a basic assumption that the predicted probabilities of all classes should sum to one, which hardly holds for MLL examples.\n\n\nTo mitigate this issue, inspired by the idea of one-versus-all reduction, we propose a multi-label logits distillation (MLD) loss, which decomposes the original multi-label task into multiple binary classification problems and minimizes the divergence between the binary predicted probabilities of two models. Formally, the MLD loss can be formulated as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {M L D}} = \\frac {1}{b} \\sum_ {i = 1} ^ {b} \\sum_ {k = 1} ^ {q} \\mathcal {D} \\left(\\left[ \\hat {y} _ {i k} ^ {\\mathcal {T}}, 1 - \\hat {y} _ {i k} ^ {\\mathcal {T}} \\right] \\mid \\mid \\left[ \\hat {y} _ {i k} ^ {\\mathcal {S}}, 1 - \\hat {y} _ {i k} ^ {\\mathcal {S}} \\right]\\right), \\tag {3}\n$$\n\n$$\n\n\nwhere  $[\\cdot, \\cdot]$  is an operator used to concatenate two scalars into a vector, and  $\\mathcal{D}$  is a divergence function. The most common choice is the KL divergence  $\\mathcal{D}_{\\mathrm{KL}}(P\\|Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\left(\\frac{P(x)}{Q(x)}\\right)$ , where  $P$  and  $Q$  are two different probability distributions. The MLD loss aims to improve the performance of student model by sufficiently exploiting informative knowledge from logits.\n\n\n# 3.2 LABEL-WISE EMBEDDING DISTILLATION\n\n\nThe MLD loss performs distillation on the predicted probabilities that can be regarded as a high-level representation, i.e., the final outputs of model. The knowledge distilled from the teacher model\n\n\n![Figure 3: An illustration of class/instance-aware label-wise embedding distillation. Class-aware label-wise embedding distillation (CD) captures structural relations among intra-class label-wise embeddings from different examples, while instance-aware label-wise embedding distillation (ID) explores structural relations among intra-instance (inter-class) label-wise embeddings.](images/fc91b8a5684cb5236c7dad483996d6e68e9365330e16f8de842aedff6512faab.jpg)\n\n\nby only using the MLD loss would be insufficient to train a student model with desirable performance due to the limited information carried by the logits. To further strengthen the effectiveness of distillation, we design the label-wise embedding distillation (LED) loss, which aims to explore the structural knowledge from label-wise embeddings. The main idea is to capture two types of structural relations among label-wise embeddings: 1) class-aware label-wise embedding distillation (CD) loss  $\\mathcal{L}_{\\mathrm{LED - CD}}$ , which captures the structural relation between any two intra-class label-wise embeddings from different examples; 2) instance-aware label-wise embedding distillation (ID) loss  $\\mathcal{L}_{\\mathrm{LED - ID}}$ , which models the structural relation between any two inter-class label-wise embeddings from the same example. In the following content, we introduce these two distillation losses in detail.\n\n\n# 3.2.1 CLASS-AWARE LABEL-WISE EMBEDDING DISTILLATION\n\n\nClass-aware label-wise embedding distillation aims to improve the distillation performance by exploiting the structural relations among intra-class label-wise embeddings. Generally, the same semantic objects from two different images often differ from each other by their individual characteristics, such as two cars with different colors and diverse styles (see the left side in Figure 3). Since our goal is to distinguish between car and other semantic classes instead of identifying different cars, these distinctiveness would be confusing information for the corresponding classification task. Due to the powerful learning capacity, the large model is able to capture the highly abstract semantic representations for each class label by neglecting the useless individual information. From the perspective of learned feature representations, as shown in the left side of Figure 3, the teacher model tends to obtain a more compact structure of intra-class label-wise embedding, which often leads to better classification performance. By transferring the structural knowledge from the teacher model to the student model, CD encourages the student model to enhance the compactness of intra-class label-wise embeddings, which can improve its classification performance.\n\n\nFor a batch of examples, let  $\\{e_{ik}^{\\mathcal{T}}\\}_{i=1}^{b}$  and  $\\{e_{ik}^{\\mathcal{S}}\\}_{i=1}^{b}$  respectively denote the intra-class label-wise embeddings with respect to class  $k \\in [q]$  generated by the teacher and student models. Then, we can capture the structural relation between any two intra-class label-wise embeddings  $e_{ik}^{\\mathcal{T}}$  and  $e_{jk}^{\\mathcal{T}}$\n\n\nby measuring their distance in the embedding space:\n\n\n$$\n\n$$\n\\phi_ {\\mathrm {C D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {j k} ^ {\\mathcal {T}}\\right) = \\left\\{ \\begin{array}{l l} \\| \\boldsymbol {e} _ {i k} ^ {\\mathcal {T}} - \\boldsymbol {e} _ {j k} ^ {\\mathcal {T}} \\| _ {2} & y _ {i k} = 1, y _ {j k} = 1, \\\\ 0 & \\text {o t h e r w i s e .} \\end{array} \\right. \\tag {4}\n$$\n\n$$\n\n\nIt is worth to note that we only consider the structural relation between any two valid label-wise embeddings, i.e., the embeddings with respect to positive labels. Similar to Eq.(4), for any two intra-class label-wise embeddings  $e_{ik}^{S}$  and  $e_{jk}^{S}$ , we can obtain the structural relation  $\\phi_{\\mathrm{CD}}(e_{ik}^{S}, e_{jk}^{S})$  for the student model.\n\n\nBy enforcing the teacher and student structural relations to maintain the consistency for each pair of intra-class label-wise embeddings, we can achieve the class-aware structural consistency as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {L E D - C D}} = \\sum_ {k = 1} ^ {q} \\sum_ {i, j \\in [ b ]} \\ell \\left(\\phi_ {\\mathrm {C D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {j k} ^ {\\mathcal {T}}\\right), \\phi_ {\\mathrm {C D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {S}}, \\boldsymbol {e} _ {j k} ^ {\\mathcal {S}}\\right)\\right), \\tag {5}\n$$\n\n$$\n\n\nwhere  $\\ell$  is a function to measure the consistency between the teacher and student structural relations. In experiments, we use the following Huber loss function as a measurement:\n\n\n$$\n\n$$\n\\ell (a, b) = \\left\\{ \\begin{array}{l l} \\frac {1}{2} (a - b) ^ {2} & | a - b | \\leq 1, \\\\ | a - b | - \\frac {1}{2} & \\text {o t h e r w i s e .} \\end{array} \\right. \\tag {6}\n$$\n\n$$\n\n\nwhere  $a$  and  $b$  are two different structural relations.\n\n\n# 3.2.2INSTANCE-AWARE LABEL-WISE EMBEDDING DISTILLATION\n\n\nInstance-aware label-wise embedding distillation (ID) aims to improve the distillation performance by exploring the structural relations among inter-class label-wise embeddings from the same image. Generally, one can hardly distinguish between two different semantic objects occurring in an image due to the high similarities they share. For example, see the right side of Figure 3, for an image annotated with sky, sea and beach, one can hardly distinguish between the semantic objects sky and sea, since they share the same color and similar texture. A feasible solution is to exploit other useful information, such as their spatial relation, i.e., sky is always above sea. Due to the powerful learning capacity, the large model is able to distinguish between the similar semantic objects by exploiting such implicit supervised information. From the perspective of learned feature representations, as shown in the right side of 3, the teacher model tends to learn a dispersed structure of inter-class label-wise embedding, which is beneficial for improving its discrimination ability. By distilling the structural knowledge from the teacher model, ID enforces the student model to enhance the dispersion of inter-class label-wise embeddings, which can improve its discrimination ability.\n\n\nFor a given instance  $\\boldsymbol{x}_i$ , let  $\\{e_{ik}^{\\mathcal{T}}\\}_{k=1}^{q}$  and  $\\{e_{ik}^S\\}_{k=1}^{q}$  respectively denote the label-wise embeddings generated by teacher and student models. Then, we can capture the structural relation between any two inter-class label-wise embeddings  $e_{ik}^{\\mathcal{T}}$  and  $e_{il}^{\\mathcal{T}}$  by measuring their distance in the embedding space:\n\n\n$$\n\n$$\n\\phi_ {\\mathrm {I D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {i l} ^ {\\mathcal {T}}\\right) = \\left\\{ \\begin{array}{l l} \\| \\boldsymbol {e} _ {i k} ^ {\\mathcal {T}} - \\boldsymbol {e} _ {i l} ^ {\\mathcal {T}} \\| _ {2} & y _ {i k} = 1, y _ {i l} = 1, \\\\ 0 & \\text {o t h e r w i s e .} \\end{array} \\right. \\tag {7}\n$$\n\n$$\n\n\nNote that in Eq.(7), we only consider the structural relation between any two valid label-wise embeddings, i.e., the embedding with respect to positive labels. Similar to Eq.(7), for any two inter-class label-wise embeddings  $e_{ik}^S$  and  $e_{il}^S$ , we can obtain the structural relation  $\\phi_{\\mathrm{ID}}(e_{ik}^S, e_{il}^S)$  for the student model.\n\n\nBy encouraging the teacher and student model to maintain the consistent structure of intra-instance label-wise embeddings, we can minimize the following  $\\mathcal{L}_{\\mathrm{LED - ID}}$  loss to achieve the instance-aware structural consistency where  $\\ell (.)$  is Huber loss as defined in Eq.(6):\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {L E D - I D}} = \\sum_ {i = 1} ^ {b} \\sum_ {k, l \\in [ q ]} \\ell \\left(\\phi_ {\\mathrm {I D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {i l} ^ {\\mathcal {T}}\\right), \\phi_ {\\mathrm {I D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {S}}, \\boldsymbol {e} _ {i l} ^ {\\mathcal {S}}\\right)\\right). \\tag {8}\n$$\n\n$$\n\n\nFinally, the overall objective function of L2D (Eq.(2)) can be re-written as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {L 2 D}} = \\mathcal {L} _ {\\mathrm {B C E}} + \\lambda_ {\\mathrm {M L D}} \\mathcal {L} _ {\\mathrm {M L D}} + \\lambda_ {\\mathrm {L E D - C D}} \\mathcal {L} _ {\\mathrm {L E D - C D}} + \\lambda_ {\\mathrm {L E D - I D}} \\mathcal {L} _ {\\mathrm {L E D - I D}}, \\tag {9}\n$$\n\n$$\n\n\nwhere  $\\lambda_{\\mathrm{MLD}}$ ,  $\\lambda_{\\mathrm{LED - CD}}$  and  $\\lambda_{\\mathrm{LED - ID}}$  are all balancing parameters.\n\n\nTable 1: Results on MS-COCO where teacher and student models are in the same architectures.\n\n\n<table><tr><td>Teacher</td><td colspan=\"3\">RepVGG-A2</td><td colspan=\"3\">ResNet-101</td><td colspan=\"3\">WRN-101</td><td colspan=\"3\">Swin-S</td></tr><tr><td>Student</td><td colspan=\"3\">RepVGG-A0</td><td colspan=\"3\">ResNet-34</td><td colspan=\"3\">WRN-50</td><td colspan=\"3\">Swin-T</td></tr><tr><td>Metrics</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td></tr><tr><td>Teacher</td><td>72.71</td><td>74.11</td><td>68.63</td><td>73.62</td><td>73.89</td><td>68.61</td><td>74.70</td><td>75.56</td><td>70.73</td><td>81.70</td><td>80.48</td><td>77.12</td></tr><tr><td>Student</td><td>70.02</td><td>72.49</td><td>66.77</td><td>70.31</td><td>72.49</td><td>66.82</td><td>74.45</td><td>75.43</td><td>70.61</td><td>79.59</td><td>79.18</td><td>75.42</td></tr><tr><td>RKD</td><td>70.08</td><td>72.39</td><td>66.73</td><td>70.13</td><td>72.44</td><td>66.78</td><td>74.70</td><td>75.71</td><td>70.84</td><td>79.63</td><td>79.19</td><td>75.57</td></tr><tr><td>PKT</td><td>70.11</td><td>72.47</td><td>66.80</td><td>70.43</td><td>72.64</td><td>66.68</td><td>74.54</td><td>75.47</td><td>70.58</td><td>79.64</td><td>79.09</td><td>75.39</td></tr><tr><td>ReviewKD</td><td>70.00</td><td>72.35</td><td>66.82</td><td>70.39</td><td>72.62</td><td>66.76</td><td>74.03</td><td>75.29</td><td>70.36</td><td>79.81</td><td>79.18</td><td>75.55</td></tr><tr><td>MSE</td><td>70.26</td><td>72.54</td><td>66.99</td><td>70.54</td><td>72.75</td><td>66.85</td><td>74.53</td><td>75.60</td><td>70.71</td><td>79.67</td><td>79.20</td><td>75.52</td></tr><tr><td>PS</td><td>70.65</td><td>72.89</td><td>67.60</td><td>70.86</td><td>72.66</td><td>67.12</td><td>75.12</td><td>76.05</td><td>71.63</td><td>79.96</td><td>79.64</td><td>76.20</td></tr><tr><td>MLD</td><td>70.74</td><td>72.81</td><td>67.46</td><td>70.68</td><td>72.69</td><td>67.19</td><td>74.92</td><td>75.75</td><td>71.21</td><td>80.11</td><td>79.68</td><td>76.44</td></tr><tr><td>L2D</td><td>72.81</td><td>74.59</td><td>69.49</td><td>72.87</td><td>74.45</td><td>69.43</td><td>76.61</td><td>77.08</td><td>72.79</td><td>81.59</td><td>81.03</td><td>77.86</td></tr></table>\n\n\nTable 2: Results on MS-COCO where teacher and student models are in the different architectures.\n\n\n<table><tr><td>Teacher</td><td colspan=\"3\">ResNet-101</td><td colspan=\"3\">Swin-T</td><td colspan=\"3\">ResNet-101</td><td colspan=\"3\">Swin-T</td></tr><tr><td>Student</td><td colspan=\"3\">RepVGG-A0</td><td colspan=\"3\">ResNet-34</td><td colspan=\"3\">MobileNet v2</td><td colspan=\"3\">MobileNet v2</td></tr><tr><td>Metrics</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td></tr><tr><td>Teacher</td><td>73.62</td><td>73.89</td><td>68.61</td><td>79.43</td><td>78.77</td><td>75.07</td><td>73.62</td><td>73.89</td><td>68.61</td><td>79.43</td><td>78.77</td><td>75.07</td></tr><tr><td>Student</td><td>70.02</td><td>72.49</td><td>66.77</td><td>70.31</td><td>72.49</td><td>66.82</td><td>71.85</td><td>73.59</td><td>68.26</td><td>71.85</td><td>73.59</td><td>68.26</td></tr><tr><td>RKD</td><td>70.08</td><td>72.35</td><td>66.72</td><td>70.00</td><td>72.34</td><td>66.64</td><td>71.76</td><td>73.68</td><td>68.40</td><td>71.74</td><td>73.68</td><td>68.37</td></tr><tr><td>PKT</td><td>69.99</td><td>72.35</td><td>66.56</td><td>70.26</td><td>72.39</td><td>66.82</td><td>71.88</td><td>73.60</td><td>68.35</td><td>71.84</td><td>73.76</td><td>68.37</td></tr><tr><td>ReviewKD</td><td>70.00</td><td>72.33</td><td>66.62</td><td>70.29</td><td>72.39</td><td>66.58</td><td>71.92</td><td>73.73</td><td>68.48</td><td>71.73</td><td>73.71</td><td>68.36</td></tr><tr><td>MSE</td><td>70.07</td><td>72.50</td><td>66.85</td><td>70.33</td><td>72.57</td><td>66.72</td><td>71.91</td><td>73.68</td><td>68.28</td><td>71.80</td><td>73.74</td><td>68.38</td></tr><tr><td>PS</td><td>70.30</td><td>72.61</td><td>67.10</td><td>70.94</td><td>72.93</td><td>67.57</td><td>72.11</td><td>73.89</td><td>68.42</td><td>72.42</td><td>74.14</td><td>68.94</td></tr><tr><td>MLD</td><td>70.48</td><td>72.77</td><td>67.10</td><td>71.14</td><td>72.99</td><td>67.63</td><td>72.17</td><td>73.84</td><td>68.52</td><td>72.35</td><td>74.10</td><td>68.91</td></tr><tr><td>L2D</td><td>72.14</td><td>74.08</td><td>68.78</td><td>73.42</td><td>74.97</td><td>70.20</td><td>73.24</td><td>74.85</td><td>69.72</td><td>74.21</td><td>75.72</td><td>70.87</td></tr></table>\n\n\n# 4 EXPERIMENTS\n\n\nDatasets. We perform experiments on three benchmark datasets Pascal VOC2007 (Everingham et al., 2015) (VOC for short), MS-COCO2014 (Lin et al., 2014) (MS-COCO for short) and NUS-WIDE (Chua et al., 2009). VOC contains 5,011 images in the train-val set, and 4,952 images in the test set. It covers 20 common objects, with an average of 1.6 labels per image. MS-COCO contains 82,081 training images and 40,137 test images. It covers 80 common objects, with an average of 2.9 labels per image. NUS-WIDE contains 161,789 training images and 107,859 test images. It covers 81 visual concepts, with an average of 1.9 labels per image.\n\n\nMetrics. Following existing works (Zhang & Zhou, 2013; Liu et al., 2021a; Sun et al., 2022), we adopt the mean average precision (mAP) over all classes, overall F1-score (OF1) and average per-class F1-score (CF1) to evaluate the performance. We choose OF1 and CF1, since they consider both recall and precision and thus are more comprehensive.\n\n\nMethods. To validate the proposed method, we compare it with the following KD methods: RKD (Park et al., 2019), which captures the relations among instances to guide the training of the student model; PKT (Passalis & Tefas, 2018), which measures KL divergence between features by treating them as probability distributions; ReviewKD (Chen et al., 2021), which transfers knowledge across different stages instead of just focusing on features in the same levels; MSE (Xu et al., 2022a), which minimizes the MSE loss between logits of teacher and student model; PS (Song et al., 2021), which minimizes KL divergence of logits after a partial softmax function.\n\n\nImplementation Details. We use the models pretrained on ImageNet (Deng et al., 2009) as the backbones. We resize the resolution of all images to  $224 \\times 224$  and set the batch size as 64. For each training image, we adopt a weak augmentation consisting of random horizontal flipping and a strong augmentation consisting of Cutout (Devries & Taylor, 2017) and RandAugment Cubuk et al. (2020). We use the Adam optimization method (Kingma & Ba, 2015) to train the model for 80 epochs. The one-cycle policy is used with a maximal learning rate of 0.0001 and the weight decay (Loshchilov & Hutter, 2018) of 0.0001. For all experiments, we set  $\\lambda_{\\mathrm{MLD}} = 10$ ,  $\\lambda_{\\mathrm{LED-CD}} = 100$ , and  $\\lambda_{\\mathrm{LED-ID}} = 1000$ . Parameter sensitivity analysis in Appendix D shows that the performance of\n\n\n![Figure 4: Comparison results of the comparing methods on VOC in terms of AP and mAP  $(\\%)$  where the backbones of teacher and student model are respectively ResNet-50 and ResNet-18.](images/815ee5fde3e1930a3dc20789856e9d2a4070e1872c8094a7427757a554e80b2a.jpg)\n\n\nL2D are insensitive to all of our balancing parameters. For the comparing methods, we set their parameters as suggested in the original papers. Especially, for all feature-based methods, we just deploy them on the feature maps which is output from the visual backbone  $f$ . All the experiments are conducted on GeForce RTX 2080 GPUs. More details about the used models and implementation of the label-wise embedding encoder are attached in Appendix A.\n\n\n# 4.1 COMPARISON RESULTS\n\n\nTable 1 and Table 2 report comparison results on MS-COCO with the same and different architectures of student and teacher models. From Table 1, it can be observed that: 1) Conventional feature-based distillation methods only achieve minor improvements in performance when compared with the student model (without distillation). This indicates these methods do not work in multi-label scenarios due to their disability to capture multiple semantics occurred in feature maps. 2) MLD can outperform conventional feature-based distillation methods in most cases, which indicates by performing one-versus-all reduction, the logits-based distillation can be adapted into multi-label knowledge distillation. 3) The proposed L2D significantly outperforms all other methods and achieves comparable performance with the teacher model, which convincingly validates the effectiveness of the proposed label-wise embedding distillation. From Table 2, we can see: 1) Compared with the same architecture, the performance gap between teacher and student model is larger for different architectures, which indicates the corresponding distillation task is harder. 2) Our method significantly outperforms all the comparing methods by a significant margin in all cases. 3) L2D achieves the best performance in all cases and significantly outperforms MLD. These results provide a strong empirical validation for the effectiveness of the proposed method.\n\n\nFigure 4 illustrates the performance of the proposed methods and other comparing methods on VOC in terms of AP and mAP. It is noteworthy that the performance is ranked in descending according to the performance of student model. From the figure, it can be observed that: 1) Our proposed L2D achieves the best performance and significantly outperforms the comparing methods in terms of mAP. 2) L2D consistently achieves superior performance to the comparing methods in most classes and the performance gap is large especially for classes that the student model achieves poor performance. This observation discloses that L2D improves the performance of hard classes by enhancing the distinctiveness of feature representations. These experimental results demonstrate the practical usefulness of the proposed method. More results on VOC and NUS-WIDE can be found in Appendix B and Appendix C.\n\n\n# 4.2 ABLATION STUDY\n\n\nIn this section, to further analyze how the proposed method improves distillation performance, Table 3 reports results of ablation studies on MS-COCO (teacher: ResNet-101, student: ResNet-34) and VOC (teacher: ResNet-50, student: ResNet-18). The first line in Table 3 reports the performance of student model without knowledge distillation. It can be observed that the distillation performance is improved slightly by only conducting multi-label logits distillation. Compared with MLD, label-wise embeddings distillation achieves the major improvement for the proposed method. It can\n\n\nTable 3: Ablation studies on MS-COCO and VOC.\n\n\n<table><tr><td colspan=\"3\"></td><td colspan=\"3\">MS-COCO</td><td colspan=\"3\">Pascal VOC 2007</td></tr><tr><td>MLD</td><td>CD</td><td>ID</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td></tr><tr><td></td><td></td><td></td><td>70.31</td><td>72.49</td><td>66.82</td><td>84.01</td><td>83.60</td><td>79.42</td></tr><tr><td>\u2713</td><td></td><td></td><td>70.68</td><td>72.69</td><td>67.19</td><td>84.48</td><td>84.07</td><td>80.29</td></tr><tr><td>\u2713</td><td>\u2713</td><td></td><td>71.91</td><td>73.74</td><td>68.76</td><td>85.65</td><td>85.64</td><td>81.94</td></tr><tr><td>\u2713</td><td></td><td>\u2713</td><td>71.79</td><td>73.62</td><td>68.43</td><td>84.59</td><td>84.19</td><td>80.24</td></tr><tr><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>72.87</td><td>74.45</td><td>69.43</td><td>85.71</td><td>85.70</td><td>82.11</td></tr></table>\n\n\n![(a) Vanilla](images/d1f0114043661588281e56adabc3a7f07e5c07ecd238fe740dd2877829ba3029.jpg)\n\n\n![(b) PKT Figure 5: The differences between correlation matrices of student and teacher predicted probabilities on MS-COCO. The student distilled by our method shows significant matching between student's and teacher's correlations compared to others.](images/6270e730612b0212bd711e9f655031c90ed12f996ed053b587e1b58e4052ef8e.jpg)\n\n\n![(c)MLD](images/7810745b4c3ccc7a22fd8ee4f3532aeb7c16bf2e7fb0a55c4a65ebed7ec91a5c.jpg)\n\n\n![(d) L2D](images/f332ad196cfd7cef24131b81982c14cc1022b94d9004b8bdd121d8b918934cd7.jpg)\n\n\nbe observed that by performing CD and ID, the mAP performance achieves  $1.6\\%$  and  $1.48\\%$  increments, respectively. Finally, we also examine the combination of these techniques. By incorporating these components together, the fusing method achieves the best performance and significantly outperforms each other method. These results demonstrate that all of three components are of great importance to the performance of the proposed L2D.\n\n\n# 4.3 DISTILLING INTER-CLASS CORRELATIONS\n\n\nAs discussed in the previous works (Hinton et al., 2015), the conventional supervised learning losses, e.g., BCE loss, often neglects the correlations among class predictions. However, the label correlation is a foundational element for multi-label classification. Distillation losses utilize \"soft targets\", which can effectively distill such correlation information from the teacher model, leading to a desirable distillation performance. To validate whether the correlations can be captured by L2D effectively, Figure 5 illustrates the differences between correlation matrices of student and teacher predicted probabilities on MS-COCO. From the figure, it can be observed that: 1) Without knowledge distillation, the teacher and student correlations are very different, which indicates teacher model tends to capture a more precise correlation than student model and thus achieves better performance. 2) The representative comparing method PKT shows a large difference, which discloses that the conventional KD methods are ineffective to capture the correlations in multi-label scenarios. 3) MLD shows a reduced difference, which indicates that the rich informativeness of logits is beneficial for capturing precise correlations. 4) L2D shows significant matching between the teacher and student correlations. By enhancing the distinctiveness of label-wise embeddings, L2D can obtain more correct predicted probabilities, leading to a more precise correlation estimation.\n\n\n# 5 CONCLUSION\n\n\nThe paper studies the problem of multi-label knowledge distillation. In the proposed method, the multi-label logits distillation explores the informative semantic knowledge compressed in the teacher logits to obtain more semantic supervision. Furthermore, the label-wise embedding distillation exploits the structural knowledge from label-wise embeddings to learn more distinctive feature representations. Experimental results on benchmark datasets validate the effectiveness of the proposed method. In future, we plan to improve the performance of MLKD by exploiting other abundant structural information.\n\n\n# REFERENCES\n\n- Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge review. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5008-5017, 2021.\n\n- Zhao-Min Chen, Xiu-Shen Wei, Xin Jin, and Yanwen Guo. Multi-label image recognition with joint class-aware map disentangling and label correlation embedding. In Proceedings of the IEEE International Conference on Multimedia and Expo, pp. 622-627, 2019a.\n\n- Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen Guo. Multi-label image recognition with graph convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5177-5186, 2019b.\n\n- Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yan-Tao Zheng. Nus-wide: A real-world web image database from national university ofsingapore. In Proceedings of ACM Conference on Image and Video Retrieval, 2009.\n\n- Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In Advances in Neural Information Processing Systems, 2020.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2009.\n\n- Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.\n\n- Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pp. 13733-13742, 2021.\n\n- Mark Everingham, SM Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The Pascal visual object classes challenge: A retrospective. International Journal of Computer Vision, pp. 98-136, 2015.\n\n- Fran\u00e7ois-Guillaume Fernandez. Torchcam: class activation explorer. https://github.com/frgfmd/torch-cam, 2020.\n\n- Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. International Journal of Computer Vision, pp. 1789-1819, 2021.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.\n\n- Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\n- Peng-Tao Jiang, Chang-Bin Zhang, Qibin Hou, Ming-Ming Cheng, and Yunchao Wei. Layercam: Exploring hierarchical class activation maps for localization. IEEE Transactions on Image Processing, pp. 5875-5888, 2021.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.\n\n- Jack Lanchantin, Tianlu Wang, Vicente Ordonez, and Yanjun Qi. General multi-label image classification with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16478-16488, 2021.\n\n- Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr D\u00f3lar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision, pp. 740-755, 2014.\n\n- Shilong Liu, Lei Zhang, Xiao Yang, Hang Su, and Jun Zhu. Query2label: A simple transformer way to multi-label classification. arXiv preprint arXiv:2107.10834, 2021a.\n\n- Yongcheng Liu, Lu Sheng, Jing Shao, Junjie Yan, Shiming Xiang, and Chunhong Pan. Multi-label image classification via knowledge distillation from weakly-supervised detection. In Proceedings of the 26th ACM International Conference on Multimedia, pp. 700-708, 2018.\n\n- Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012-10022, 2021b.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.\n\n- Roy Miles, Adri\u00e1n L\u00f3pez Rodr\u00edguez, and Krystian Mikolajczyk. Information theoretic representation distillation. arXiv preprint arXiv:2112.00459, 2021.\n\n- Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 5191-5198, 2020.\n\n- Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3967-3976, 2019.\n\n- Nikolaos Passalis and Anastasios Tefas. Learning deep representations with probabilistic knowledge transfer. In Proceedings of the European Conference on Computer Vision, pp. 268-284, 2018.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning, pp. 8748-8763, 2021.\n\n- Tal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi Zelnik-Manor. Asymmetric loss for multi-label classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 82-91, 2021a.\n\n- Tal Ridnik, Gilad Sharir, Avi Ben-Cohen, Emanuel Ben-Baruch, and Asaf Noy. MI-decoder: Scalable and versatile classification head. arXiv preprint arXiv:2111.12933, 2021b.\n\n- Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.\n\n- Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. *Mobilenetv2: Inverted residuals and linear bottlenecks*. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 4510-4520, 2018.\n\n- Liangchen Song, Jialian Wu, Ming Yang, Qian Zhang, Yuan Li, and Junsong Yuan. Handling difficult labels for multi-label image classification via uncertainty distillation. In Proceedings of the 29th ACM International Conference on Multimedia, pp. 2410-2419, 2021.\n\n- Feng Sun, Ming-Kun Xie, and Sheng-Jun Huang. A deep model for partial multi-label image classification with curriculum based disambiguation. arXiv preprint arXiv:2207.02410, 2022.\n\n- Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In International Conference on Learning Representations, 2019.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.\n\n- Jiazhi Xu, Sheng Huang, Fengtao Zhou, Luwen Huangfu, Daniel Zeng, and Bo Liu. Boosting multi-label image classification with complementary parallel self-distillation. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, 2022a.\n\n- Shichao Xu, Yikang Li, Jenhao Hsiao, Chiuman Ho, and Zhu Qi. A dual modality approach for (zero-shot) multi-label classification. arXiv preprint arXiv:2208.09562, 2022b.\n\n- Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016a.\n\n- Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British Machine Vision Conference, 2016b.\n\n- Min-Ling Zhang and Zhi-Hua Zhou. A review on multi-label learning algorithms. IEEE Transactions on Knowledge and Data Engineering, pp. 1819-1837, 2013.\n\n- Ying Zhang, Tao Xiang, Timothy M Hospedales, and Hutchuan Lu. Deep mutual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4320-4328, 2018.\n\n- Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. Decoupled knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11953-11962, 2022.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638420.859551", "msecs": "859.0", "relativeCreated": "17690.048456192017", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Learning Graph Invariance by Harnessing Spuriosity\n\n# Abstract\nRecently, graph invariant learning has become the _de facto_ approach to tackle the Out-of-Distribution (OOD) generalization failure in graph representation learning. They generically follow the framework of invariant risk minimization to capture the invariance of graph data from different environments. Despite some success, it remains unclear to what extent existing approaches have captured invariant features for OOD generalization on graphs. In this work, we find that representative OOD methods such as IRM and VRex, and their variants on graph invariant learning may have captured a limited set of invariant features. To tackle this challenge, we propose $\\texttt{LIRS}$, a novel learning framework designed to **L**earn graph **I**nvariance by **R**emoving **S**purious features. Different from most existing approaches that _directly_ learn the invariant features, $\\texttt{LIRS}$ takes an _indirect_ approach by first learning the spurious features and then removing them from the ERM-learned features, which contains both spurious and invariant features. We demonstrate that learning the invariant graph features in an _indirect_ way can learn a more comprehensive set of invariant features. Moreover, our proposed method outperforms the second-best method by as much as 25.50% across all competitive baseline methods, highlighting its effectiveness in learning graph invariant features.\n\n\n# 1 INTRODUCTION\n\n\nGraph representation learning with graph neural networks (GNNs) (Kipf & Welling, 2017; Xu et al., 2019; Velickovi\u0107 et al., 2017) has proven to be highly successful in tasks involving relational information (Qiu et al., 2018; Wu et al., 2022b; Yu et al., 2018; Zhang et al., 2022c). Despite their achievements, GNNs often assume that the training and test data share the same distribution. This assumption often does not hold in real-world applications (Hu et al., 2020; Huang et al., 2021; Ji et al., 2022; Koh et al., 2021) due to changes in the underlying data generation process, leading to distribution shifts. Such phenomenon can severely degrade the performance of GNN models, which presents a critical challenge for their deployment in practical scenarios (DeGrave et al., 2020).\n\n\nTo address the OOD generalization challenge, graph invariant learning has become the de facto approach. Inspired by the success of invariance principle (Arjovsky et al., 2020; Kreuzer et al., 2021), graph invariant learning aims to identify invariant subgraphs that are causally related with the targets (Yang et al., 2022; Li et al., 2022b; Liu et al., 2022; Zhuang et al., 2023; Chen et al., 2022; Li et al., 2022a). Typically, they follow the framework of Invariant Risk Minimization (IRM) (Arjovsky et al., 2020) or Variance-Risk Minimization (VRex) (Krueger et al., 2021), which aim to capture invariant features by learning an equipredictive classifier across different environments. Despite some success, many graph invariant learning methods only perform similarly compared to the traditional Empirical Risk Minimization (ERM) (Vapnik, 1995) in graph OOD benchmarks (Gui et al., 2022), it raises a critical yet overlooked question:\n\n\nTo what extent do the graph invariant learning algorithms capture the graph invariance?\n\n\nIn this work, we address this question by investigating two representative OOD algorithms, namely IRMv1 (Arjovsky et al., 2020) and VRex (Kreuzer et al., 2021), in graph-level OOD classification scenarios. These algorithms are widely used, and many graph invariance learning algorithms are based on their variants or extensions (Li et al., 2022b; Liu et al., 2022; Zhuang et al., 2023; Wu et al., 2022c). Surprisingly, we find that IRM, VRex and their variants on graph invariant learning\n\n\nmay only learn a subset of invariant features (Sec. 3). In contrast, the learning paradigm that learns invariant features indirectly by removing spurious features from the ERM-learned features may\n\n\nrender learning a more comprehensive set of invariant features (Figure 1 and Sec. 3). Motivated by this observation, we propose a novel learning framework LIRS: Learning graph Invariance by Removing Spuriosity. Different from most existing approaches that directly learn the invariant features, LIRS adopts the indirect learning paradigm. Specifically, LIRS first leverages the biased info-max principle (Def. 3) to effectively learn graph spuriosity (Theorem 4.1), then employees class-conditioned cross entropy loss to effectively learn graph invariance (Theorem 4.2). Extensive experiments on synthetic datasets demonstrate that LIRS is able to learn more invariant features compared to state-of-the-art graph invariant learning methods that adopt the direct invariant learning paradigm. Furthermore, LIRS shows superior OOD performance on real-world datasets with various types of distribution shifts, highlighting its effectiveness in learning graph invariant features. Our contributions can be summarized as follows:\n\n\n![Figure 1: Illustration of the two graph invariant learning paradigms. Compared with the invariant learning methods  $\\mathcal{A}_{Inv}$  that adopt the direct learning paradigm, our proposed method first adopts a spuriosity learner  $\\mathcal{A}_{Spu}$  to learn spurious features, then learns invariant features by removing the spurious features using the feature disentanglement module  $\\mathcal{A}_{Disent}$ .](images/20bd0e2cd67019623f594f852f2e9acd438e2ced314646b25e099ac300624c64.jpg)\n\n- - We reveal that learning invariant features indirectly i.e., by first learning and then removing spurious features, can be more effective in capturing invariant features than existing (graph) invariant learning algorithms that learn invariant features directly.\n\n- - We propose LIRS, a novel framework that adopts the indirect learning paradigm for learning invariant features, which consists of: a) The biased infomax principle, a contrastive learning algorithm that provably learns graph spuriosity; b) class-conditioned cross-entropy loss, a learning objective that effectively learns graph invariance by removing spurious features from ERM-learned features.\n\n- - Extensive experiments demonstrate that LIRS outperforms second-best baseline methods by up to  $25.50\\%$  across 17 competitive baselines on both synthetic and real-world datasets with various distribution shifts.\n\n\n# 2 PRELIMINARY\n\n\nNotations. An undirected graph  $G$  with  $n$  nodes and  $m$  edges is denoted by  $G := \\{\\mathcal{V}, \\mathcal{E}\\}$ , where  $\\mathcal{V}$  is the node set and  $\\mathcal{E}$  is the edge set.  $G$  is also represented by the adjacency matrix  $\\mathbf{A}$  and the node feature matrix  $\\mathbf{X} \\in \\mathbb{R}^{n \\times D}$  with  $D$  feature dimensions. We use  $G_{c}$  and  $G_{s}$  to denote invariant and spurious subgraphs for graph  $G$  respectively, and  $\\mathbf{h}_{c}$  and  $\\mathbf{h}_{s}$  are invariant and spurious features in the latent space.  $\\widehat{\\mathbf{h}}_{i}$  and  $\\widehat{\\mathbf{h}}_{G}$  denote the estimated representations for node  $v_{i}$  and graph  $G$  respectively. Finally, we use  $[K] := \\{1, 2, \\dots, K\\}$  to denote an index set,  $w$  to denote a scalar value,  $\\mathbf{w}$  to denote a vector,  $\\mathbf{W}$  to denote a matrix,  $W$  to denote a random variable, and  $\\mathcal{W}$  to denote a set. A more complete set of notations is presented in Appendix A.\n\n\nProblem Definition. We focus on OOD generalization in graph classification in hidden environments. Given a set of graph datasets  $\\mathcal{D} = \\{\\mathcal{G}^e\\}_{e\\in \\mathcal{E}_{\\mathrm{tr}}\\subseteq \\mathcal{E}_{\\mathrm{all}}}$ , a GNN model  $f$ , denoted as  $\\rho \\circ h$ , comprises an encoder  $h:\\mathbb{G}\\to \\mathbb{R}^{F}$  that learns a representation  $\\mathbf{h}_G$  for each graph  $G$ , followed by a downstream classifier  $\\rho :\\mathbb{R}^{F}\\to \\mathbb{Y}$  to predict the label  $\\widehat{Y}_G = \\rho (\\widehat{\\mathbf{h}}_G)$ . The objective of OOD generalization on graphs is to learn an optimal GNN model  $f^{*}(\\cdot):\\mathbb{G}\\rightarrow \\mathbb{Y}$  using data from training environments  $\\mathcal{D}_{\\mathrm{tr}} = \\{\\mathcal{G}^e\\}_{e\\in \\mathcal{E}_{\\mathrm{tr}}}$  that effectively generalizes across all (unseen) environments:  $f^{*}(\\cdot) = \\arg \\min_{f}\\sup_{e\\in \\mathcal{E}_{\\mathrm{all}}}\\mathcal{R}(f|e)$ , where  $\\mathcal{R}(f|e) = \\mathbb{E}_{G,Y}^{e}[l(f(G),Y)]$  is the risk of the predictor  $f$  on the environment  $e$ , and  $l(\\cdot ,\\cdot):\\mathbb{Y}\\times \\mathbb{Y}\\to \\mathbb{R}_{+}$  denotes a loss function.\n\n\nAssumption 1. (Stable and predictive subgraphs) Given a graph  $G \\in \\mathcal{D}$ , there exists a set of stable (causal) substructure patterns  $\\mathcal{G}_c$  for every class label  $Y = y$ , satisfying: a)  $\\forall e, e' \\in \\mathcal{E}_{tr}, G_c \\in$\n\n\n$\\mathcal{G}_c, \\mathbb{P}^e(Y \\mid G_c) = \\mathbb{P}^{e'}(Y \\mid G_c)$ ; and b) The target  $Y$  can be expressed as  $Y = f^*(G_c) + \\epsilon$ , where  $\\epsilon \\perp G$  represents random noise, and  $\\perp$  indicates statistical independence.\n\n\nAssumption 1 posits that one or more substructure patterns in  $\\mathcal{G}_c$  are not only stably associated with the target label  $Y$  across different environments but also possess sufficient predictive power to accurately determine  $Y$ . This assumption is well-aligned with real-world scenarios. For instance, in the GOODHIV dataset (Gui et al., 2022; Hu et al., 2020; Wu et al., 2018), a molecule's ability to inhibit HIV may depend on the presence of several functional groups interacting with various parts of the virus. Moreover, recent study also provides empirical evidence for graph applications, suggesting that multiple substructures remain stable and predictive of the targets (see Appendix E.2 in Bui et al. (2024)), thereby supporting the validity of Assumption 1. Therefore, when the OOD algorithms are able to learn a broader set of invariant substructures (features), they will generalize more effectively across different environments.\n\n\n# 3 WHY LEARNING INVARIANT FEATURES INDIRECTLY?\n\n\nIn this section, we motivate our study by addressing the question: Why might learning invariant features indirectly be more effective than learning them directly? We begin by presenting an intuitive hypothesis related to our research question, followed by an empirical study on synthetic datasets to support this hypothesis. Consider a dataset  $\\mathcal{D}$  where each sample  $G^{(i)}$  com\n\n\n![Figure 2: Comparison of the distribution  $\\mathbb{P}_{\\Delta_{prob}}$  across different OOD algorithms.](images/a4bc8b56c142669b20aeb036887a2d3a63ed256ab156d39bd1704e38c1c09ec1.jpg)\n\n\n![](images/8ada362b52dede972e473ab00104cae1f0532f0706e48035dcb9d80f94679fc8.jpg)\n\n\nprises invariant features  $\\{\\mathbf{h}_{c,j}^{(i)}\\}_{j = 1}^{n_c}$  and spurious features  $\\{\\mathbf{h}_{s,j}^{(i)}\\}_{j = 1}^{n_s}$ , with  $n_c$  and  $n_s$  denoting the number of invariant and spurious features respectively. Any subset of the invariant features  $\\{\\mathbf{h}_{c,j}^{(i)}\\}_{j = 1}^{n_c}$  maintains stable relationships with  $Y$ , consequently OOD objectives that aim to directly learn invariant features (e.g., IRMv1 and VREx) may only capture a subset of these invariant features if a subset of invariant features can already achieve accurate predictions in the training set. In contrast, an OOD objective that seeks to remove the spurious features  $\\{\\mathbf{h}_{s,j}^{(i)}\\}_{j = 1}^{n_s}$  from the ERM-learned features may result in learning a more complete set of invariant features from  $\\{\\mathbf{h}_{c,j}^{(i)}\\}_{j = 1}^{n_c}$ , thus facilitating OOD generalization in unseen environments.\n\n\nEmpirical Study. To validate our hypothesis that learning invariant features by removing spurious features may facilitate learning more invariant features, we perform experiments using a variant of SPMotif datasets (Wu et al., 2022c; Ying et al., 2019). Specifically, for each sample three invariant subgraphs are attached to the base spurious subgraph in the training and validation sets. In the test dataset, we perform model inference and record the estimated probability for correctly predicted samples, denoted as  $\\hat{y}_j^{(i)}$ . Subsequently, we randomly removed two invariant subgraphs from these samples and compute the new estimated probability score, denoted as  $\\tilde{y}_j^{(i)}$ . We then fit the distribution  $\\mathbb{P}_{\\Delta_{prob}}$  using Kernel Density Estimation (KDE) (Terrell & Scott, 1992), based on the changes in probability  $\\Delta_{prob} \\coloneqq \\hat{y}_j^{(i)} - \\tilde{y}_j^{(i)}, \\forall i,j$ , as illustrated in Figure 2. If the encoder  $h(\\cdot)$  can indeed learn all invariant features in the test set, then removing two invariant subgraphs should not change  $\\hat{y}_j^{(i)}$ . Therefore, the closer  $\\mathbb{P}_{\\Delta_{prob}}$  is to 0, the more it indicates that the encoder has learned the full set of invariant features. In addition to the density function of  $\\Delta_{prob}$ , we also calculate the Wasserstein distance of each algorithm to a null distribution, where the probability mass is entirely centered at 0. As demonstrated in Figure 2, IRMv1 (Arjovsky et al., 2020) and VRex (Krueger et al., 2021) exhibit significant changes in  $\\Delta_{prob}$ , implying that for some samples, IRMv1 and VRex learn only a subset of the invariant patterns. GIL (Li et al., 2022b) and GREA (Liu et al., 2022) are two variants of VRex and IRM that learn invariant features directly by performing environment inference and environment augmentation respectively. As shown in Figure 2, the Wasserstein distances for GIL (Li et al., 2022b) and GREA (Liu et al., 2022) are also greater than that of our proposed\n\n\nmethod. This highlights the advantages of the learning paradigm that indirectly learns invariant features by first learning and then removing spuriosity. However, one may raise concerns that while\n\n\nlearning invariant features by first removing spurious features may indeed lead to capturing a more complete set of invariant features for the correctly predicted samples, it may also introduce spurious patterns if the spurious features are not effectively removed, which could render such a learning paradigm suboptimal in practice. To address this concern, we report test accuracy under varying strengths of spurious correlations, following previous experiments. As the strength of the spurious correlation increases, it becomes more challenging to eliminate the spurious features. From Figure 3, we have three observations: a) Our method consistently achieves superior OOD performance across different correlation strengths; b) Our method is less sensitive to the increased spurious correlations compared to other representative OOD methods, which is evident by the flatter curve in Figure 3; c) The higher test accuracy, combined with the distribution of\n\n\nprobability changes from the previous experiment, suggests that our method enables a larger number of test samples to learn a more complete set of invariant features. Based on these observations and analysis, we conclude that indirectly learning invariant features is more effective than learning them directly.\n\n\n![Figure 3: Test accuracy under varying spurious correlation strengths of the variant of SPMtoif datasets.](images/9e168d893f9aee8ed0839b19a8ec58acc698df5b966b09eb71bf26d4e9cff799.jpg)\n\n\n# 4 METHODOLOGY\n\n\nMotivated by the observations in Sec. 3, we now introduce our proposed LIRS framework, which learns invariant features on graphs indirectly.\n\n\n# 4.1 LEARNING GRAPH SPURIOSITY VIA BIASED INFOMAX\n\n\nThe first step of learning invariant features in the LIRS framework is to effectively learn graph spuriosity. In this section, we first propose a variant of the infomax principle (Hjelm et al., 2019; Velickovic et al., 2019; Linsker, 1988), namely biased infomax, which encourages spuriosity learning by suppressing invariant signals. We then present a more practical version of biased infomax, suitable for the real-world datasets. We first formally define spuriosity learning in below:\n\n\nDefinition 1. (Spuriosity Learning) Assuming each data instance  $G^{(i)}$  in a dataset  $\\mathcal{D}$  is composed of two parts  $\\mathbf{h}_c^{(i)}$  and  $\\mathbf{h}_s^{(i)}$ , an algorithm  $\\mathcal{A}$  is said to be a spuriosity learner if  $\\mathbf{h}_s^{(i)} = \\mathcal{A}(G^{(i)})$ ,  $\\forall i$ , i.e.,  $\\mathcal{A}$  learns only the spurious features for all instances in  $\\mathcal{D}$ .\n\n\nOne such algorithm to encourage spuriosity learning is the global-local infomax principle (Hjelm et al., 2019; Velickovi\u0107 et al., 2019; Linsker, 1988), which maximizes the mutual information (MI) between global representation and local node representations, as defined in the following:\n\n\nDefinition 2. (The Infomax Principle) The infomax principle optimizes the following optimization objective in Eqn. 1 w.r.t the GNN encoder  $h_{\\theta}(\\cdot)$ .\n\n\n$$\n\n$$\n\\max  _ {\\theta} \\mathbb {E} _ {G \\sim \\mathcal {G}} \\frac {1}{| G |} \\sum_ {v _ {i} \\in G} I (\\widehat {\\mathbf {h}} _ {i}, \\widehat {\\mathbf {h}} _ {G}), \\text {s . t .} \\widehat {\\mathbf {h}} _ {i} = h _ {\\theta} (G), \\widehat {\\mathbf {h}} _ {G} = R E A D O U T (\\widehat {\\mathbf {h}} _ {i}). \\tag {1}\n$$\n\n$$\n\n\nThe encoder  $h_{\\theta}(\\cdot)$  will primarily learn spurious features (Yao et al., 2024). However, it will still be constrained by  $|G_{c}|$ , the size of the invariant subgraph. As  $|G_{c}|$  increases, the learned global representation  $\\widehat{\\mathbf{h}}_G$  will increasingly capture invariant representations (Yao et al., 2024). To mitigate this constraint and further facilitate spuriosity learning, we propose the biased infomax principle.\n\n\nDefinition 3. (The Biased Infomax Principle) The biased infomax principle optimizes the following optimization objective in Eqn. 2 w.r.t the GNN encoder  $h_{\\theta}(\\cdot)$ .\n\n\n$$\n\n$$\n\\max  _ {\\theta} \\mathbb {E} _ {G \\sim \\mathcal {G}} \\frac {1}{| G |} \\left(\\sum_ {v _ {i} \\in G _ {s}} I (\\widehat {\\mathbf {h}} _ {i}, \\widehat {\\mathbf {h}} _ {G}) - \\sum_ {v _ {i} \\in G _ {c}} I (\\widehat {\\mathbf {h}} _ {i}, \\widehat {\\mathbf {h}} _ {G})\\right), \\text {s . t .} \\widehat {\\mathbf {h}} _ {i} = h _ {\\theta} (G), \\widehat {\\mathbf {h}} _ {G} = \\operatorname {R E A D O U T} (\\widehat {\\mathbf {h}} _ {i}). \\tag {2}\n$$\n\n$$\n\n\nBy maximizing MI across nodes  $v_{i} \\in G_{s}$  and minimizing MI across nodes  $v_{i} \\in G_{c}$ , the biased infomax principle yields an encoder  $h_{\\theta^{*}}(\\cdot)$  that encourages the learning of spurious patterns while suppressing invariant ones. Formally, we can demonstrate that the biased infomax principle achieves spuriosity learning.\n\n\nTheorem 4.1. Given that the invariant subgraph  $G_{c}$  contains invariant patterns that are causally related to the target labels, and  $G_{s}$  contains only spurious patterns, the biased infomax principle achieves spuriosity learning, i.e., the encoder  $h_{\\theta^{*}}(\\cdot)$  learns solely the spurious features for each data sample  $G$  in  $\\mathcal{D}$ .\n\n\nTo achieve invariant learning by removing spuriosity, it is crucial to effectively learn the spurious features first. Theorem 4.1 demonstrates that the biased infomax principle can learn graph spuriosity more effectively than vanilla infomax, as defined in Def. 2. However, in real-world scenarios,  $G_{c}$  is often unobservable, which hinders the practicality of the biased infomax principle. To make the proposed algorithm more practical, we propose an instance-level adaptive biased infomax, which incorporates a GNN explainer to identify the important nodes first, and then performs instance-wise counterfactual inference to determine whether the selected nodes should be biased or not.\n\n\nInstance-level adaptive biased infomax. At first glance, the biased infomax principle appears impractical due to the necessity of knowing  $G_{c}$  and  $G_{s}$ . To address this limitation, we employ an approximation algorithm to identify  $\\widehat{G}_c$ , which serves as an estimate for  $G_{c}$ . Subsequently, we treat nodes  $v_{i}\\in \\widehat{G}_{c}$  to realize the biased infomax principle. In practice, we utilize GSAT (Miao et al., 2022), a GNN explainer  $e(\\cdot)$  that is robust under distribution shifts, to identify  $\\widehat{G}_c$  and  $\\widehat{G}_s = G\\backslash \\widehat{G}_c$ . Nevertheless, the approximation error may inadvertently bias the algorithm by amplifying invariant signals and suppressing spurious ones, as demonstrated in the following proposition.\n\n\nProposition 1. Given an error rate  $p\\%$  in the approximation algorithm for  $G_{c}$ , i.e.,  $\\widehat{G}_c$  contains  $1 - p\\%$  nodes from  $G_{c}$  and  $p\\%$  nodes from  $G_{s}$ , let the learning objectives for the biased infomax with the ground-truth subgraphs  $G_{c}$  and  $G_{s}$  and with the approximated subgraphs  $\\widehat{G}_{c}$  and  $\\widehat{G}_{s}$  be denoted as  $\\mathcal{L}(\\theta^{*};\\mathcal{D})$  and  $\\mathcal{L}(\\theta^{\\prime};\\mathcal{D})$  respectively. The difference between  $\\mathcal{L}(\\theta^{*};\\mathcal{D})$  and  $\\mathcal{L}(\\theta^{\\prime};\\mathcal{D})$  can be expressed as:\n\n\n$$\n\n$$\n\\mathbb {E} _ {G \\sim \\mathcal {G}} \\left[ \\frac {2}{| G |} \\left(\\sum_ {v _ {i} \\in p G _ {s}} I (\\widehat {\\mathbf {h}} _ {i}; \\widehat {\\mathbf {h}} _ {G}) - \\sum_ {v _ {i} \\in p G _ {c}} I (\\widehat {\\mathbf {h}} _ {i}; \\widehat {\\mathbf {h}} _ {G})\\right) \\right].\n$$\n\n$$\n\n\nHere  $pG$  denotes  $p\\%$  nodes in graph  $G$ . When  $\\widehat{\\mathbf{h}}_G$  learns primarily spurious features,  $I(\\widehat{\\mathbf{h}}_i; \\widehat{\\mathbf{h}}_G) > I(\\widehat{\\mathbf{h}}_j; \\widehat{\\mathbf{h}}_G), \\forall v_i \\in G_s, v_j \\in G_c$ , then Prop. 1 implies that as  $p\\%$  increases, the gap between the ground-truth learning objective and the learning objective with approximation will also increase, thus leading to inaccurate graph representations. To reduce the approximation error and mitigate this issue, we conduct counterfactual inference for a graph  $G$  by evaluating the changes in predicted probability with and without  $\\widehat{G}_c$  derived from the GNN explainer. Assuming  $\\widehat{G}_c = e(G, Y)$  closely approximates the ground-truth  $G_c$ , the removal of  $\\widehat{G}_c$  should result in a significant change in the predicted probability score. Under this condition, for graph  $G$ , we bias  $v_i \\in \\widehat{G}_c$ . Otherwise, we employ vanilla infomax without the bias operation. Specifically, we use a pre-defined threshold  $\\tau$ . For each graph data instance  $G^{(i)}$  with target label  $j$ , let  $\\widehat{y}_j^{(i)}$  and  $\\tilde{y}_j^{(i)}$  denote the predicted probability scores before and after the removal of  $\\widehat{G}_c$ , respectively. We then bias the infomax for graph  $G^{(i)}$  if  $\\widehat{y}_j^{(i)} - \\tilde{y}_j^{(i)} > \\tau$  using Eqn. 2; otherwise, we use Eqn. 1 to learn the graph representation.\n\n\n# 4.2 LEARNING GRAPH INVARIANCE VIA CLASS-CONDITIONED CROSS-ENTROPY LOSS\n\n\nClass-conditioned cross-entropy loss. So far we have obtained the spurious graph representations  $\\mathbf{h}_s$  for every instance  $G$  in the dataset. Next we consider how to learn graph invariance by re\n\n\nmoving the spurious features from ERM-learned representations. To solve this challenge, previous study (Yao et al., 2024) takes spurious features  $\\mathbf{h}_s$  and target label  $Y$  as inputs to train a linear classifier using cross-entropy loss to obtain the logits, which capture spurious correlation for each sample  $G$ , and use these (spurious) logits for subsequent spurious feature removing. Although the input only contains spurious features, we argue that: The logits generated may be inaccurate to capture the spurious correlation strengths due to the overlapping of spurious patterns across different classes. To see this, we propose Prop. 2 to demonstrate this issue:\n\n\nProposition 2. Given a linear regression model with parameters  $\\{\\theta_1, \\theta_2\\}$  and spurious features  $\\{x_1, x_2\\}$ , the correlation strength for feature  $x_1$  is  $p$  and for  $x_2$  is  $1 - p$  when  $Y = 0$ . Similarly, the correlation strength for feature  $x_1$  is  $q$  and for  $x_2$  is  $1 - q$  when  $Y = 1$ . Assuming the spurious features  $x_1$  and  $x_2$  can each take values in  $\\{0, 1\\}$ , we obtain the following parameter estimates using Mean Squared Error (MSE) loss:  $\\theta_1 = \\frac{q}{p + q}, \\theta_2 = \\frac{1 - q}{2 - p - q}$ .\n\n\nProp. 2 demonstrates the issue brought by the interference of overlapping spurious features across different classes. For instance, when  $p = q$ ,  $\\theta_{1} = \\theta_{2} = \\frac{1}{2}$ , the generated logits fail to distinguish different spurious patterns. To mitigate this issue, we propose class-conditioned cross-entropy loss. Specifically, we first perform clustering within each class based on the spurious features  $\\{\\mathbf{h}_s^{(i)}\\}_{i=1}^n$ . The resulted clusters will reflect different spurious patterns and environments accurately given the spurious features learned by the biased infomax objective. However, employing hard labels can result in a loss of information regarding the relative positions of samples within a cluster. To address this, we refit a linear classifier using the cluster labels as targets. This approach ensures that samples near the cluster boundaries exhibit higher entropy in their spurious logits, thereby preserving the information about their position within the cluster. Given the estimated spurious logits  $\\mathbf{s}^{(i)} \\in \\mathbb{R}^K$  with  $K$  clusters in each class, we propose the following learning objective for spurious feature removing:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {i n v}} := \\mathbb {E} _ {y \\sim \\mathcal {Y}} \\mathbb {E} _ {G ^ {(i)} | Y = y} - \\sum_ {j = 1} ^ {K} w ^ {(i)} \\mathbf {s} _ {j} ^ {(i)} \\log \\left(\\sigma \\left(\\widehat {\\mathbf {s}} _ {j} ^ {(i)}\\right)\\right), s. t., \\widehat {\\mathbf {s}} ^ {(i)} = \\rho^ {\\prime} \\left(\\widehat {\\mathbf {h}} _ {G} ^ {(i)}\\right) \\in \\mathbb {R} ^ {K}. \\tag {3}\n$$\n\n$$\n\n\nHere,  $\\sigma(\\cdot)$  denotes the softmax function,  $\\rho'(\\cdot): \\mathbb{R}^F \\to \\mathbb{R}^C$  represents the clustering classification head, which takes  $\\widehat{\\mathbf{h}}_G$  from the GNN encoder  $h(\\cdot)$  as input. The reweighting coefficient for each sample  $G^{(i)}$  is denoted as  $w^{(i)}$ , which adjusts the weight for samples from the majority group by reducing it and increases the weight for samples from the minority group. The generalized cross-entropy (GCE) method (Zhang & Sabuncu, 2018) is employed to calculate  $w^{(i)}$ , i.e.,  $w^{(i)} = \\frac{1 - (\\mathbf{s}_j^{(i)})^\\gamma}{\\gamma}$ , where  $j$  is the ground-truth clustering label for sample  $G^{(i)}$ , and  $\\gamma$  is a hyperparameter. Finally, the loss objective for feature disentanglement is:\n\n\n$$\n\n$$\n\\mathcal {L} = \\mathcal {L} _ {G T} + \\lambda \\mathcal {L} _ {i n v}, \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\mathcal{L}_{GT}$  denotes the ERM loss.  $\\mathcal{L}_{inv}$ , serving as a regularization term, will guide the learning process to learn invariant features by removing spurious features from the ERM-learned features. Formally, we present the following theorem.\n\n\nTheorem 4.2. There exists a suitable  $\\gamma$  and clustering number  $K$ , such that minimizing the loss objective  $\\mathcal{L} = \\mathcal{L}_{GT} + \\lambda \\mathcal{L}_{inv}$  will lead to the optimal encoder  $h^{*}(\\cdot)$  which elicits invariant features for any graph  $G$ , i.e.,  $\\widehat{\\mathbf{h}}_G = h^* (G) = \\mathbf{h}_c$ .\n\n\nThe proof is included in Appendix D. In the proof of Theorem 4.2, we show that  $\\mathcal{L}_{inv}$  will guide the ERM-learned features to focus on invariant features by removing or unlearning the spurious ones. More concretely, given ERM learns both invariant and spurious features (Kirichenko et al., 2023; Chen et al., 2023b), i.e.,  $\\widehat{\\mathbf{h}}_G = \\kappa (\\mathbf{h}_c,\\mathbf{h}_s)$ , where  $\\kappa (\\cdot)$  is a functional mapping to graph representation  $\\widehat{\\mathbf{h}}_G$  given a set of invariant features  $\\mathbf{h}_c$  and a set of spurious features  $\\mathbf{h}_s$ ,  $\\mathcal{L}_{inv}$  regularizes the learning process to optimize  $\\kappa^{*}(\\cdot)$  such that  $\\widehat{\\mathbf{h}}_G$  contains only  $\\mathbf{h}_c$ . While previous OOD methods achieve a similar goal (to learn only invariant features), our approach differs by first learning the spurious features and then leveraging them to identify the causal features from the ERM-learned features, a process we refer to as spurious feature removing or unlearning.\n\n\nTable 1: Performance on synthetic and real-world datasets. Numbers in bold indicate the best performance, while the underlined numbers indicate the second best performance.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GOOD-Motif</td><td colspan=\"2\">GOOD-HIV</td><td colspan=\"2\">OGBG-Molbace</td><td colspan=\"2\">OGBG-Molbbp</td></tr><tr><td>base</td><td>size</td><td>scaffold</td><td>size</td><td>scaffold</td><td>size</td><td>scaffold</td><td>size</td></tr><tr><td>ERM</td><td>68.66\u00b14.25</td><td>51.74\u00b12.88</td><td>69.58\u00b12.51</td><td>59.94\u00b12.37</td><td>75.11\u00b13.03</td><td>83.60\u00b13.47</td><td>68.10\u00b11.68</td><td>78.29\u00b13.76</td></tr><tr><td>IRM</td><td>70.65\u00b14.17</td><td>51.41\u00b13.78</td><td>67.97\u00b11.84</td><td>59.00\u00b12.92</td><td>75.47\u00b12.22</td><td>83.12\u00b12.58</td><td>67.22\u00b11.15</td><td>77.56\u00b12.48</td></tr><tr><td>GroupDRO</td><td>68.24\u00b18.92</td><td>51.95\u00b15.86</td><td>70.64\u00b12.57</td><td>58.98\u00b12.16</td><td>-</td><td>-</td><td>66.47\u00b12.39</td><td>79.27\u00b12.43</td></tr><tr><td>VREx</td><td>71.47\u00b16.69</td><td>52.67\u00b15.54</td><td>70.77\u00b12.84</td><td>58.53\u00b12.88</td><td>72.81\u00b14.29</td><td>82.55\u00b12.51</td><td>68.74\u00b11.03</td><td>78.76\u00b12.37</td></tr><tr><td>RSC</td><td>46.12\u00b13.76</td><td>51.70\u00b15.47</td><td>69.16\u00b13.23</td><td>61.17\u00b10.74</td><td>74.59\u00b13.65</td><td>84.34\u00b12.65</td><td>69.01\u00b12.84</td><td>78.07\u00b13.89</td></tr><tr><td>DiverseModel</td><td>54.24\u00b18.22</td><td>41.01\u00b11.98</td><td>69.17\u00b13.62</td><td>61.59\u00b12.23</td><td>73.48\u00b13.56</td><td>79.40\u00b11.70</td><td>68.04\u00b13.27</td><td>77.62\u00b11.90</td></tr><tr><td>DropEdge</td><td>45.08\u00b14.46</td><td>45.63\u00b14.61</td><td>70.78\u00b11.38</td><td>58.53\u00b11.26</td><td>70.81\u00b12.12</td><td>76.39\u00b12.29</td><td>66.49\u00b11.55</td><td>78.32\u00b13.44</td></tr><tr><td>FLAG</td><td>61.12\u00b15.39</td><td>51.66\u00b14.14</td><td>68.45\u00b12.30</td><td>60.59\u00b12.95</td><td>80.37\u00b11.58</td><td>84.72\u00b10.88</td><td>67.69\u00b12.36</td><td>79.26\u00b12.26</td></tr><tr><td>LiSA</td><td>54.59\u00b14.81</td><td>53.46\u00b13.41</td><td>70.38\u00b11.45</td><td>52.36\u00b13.73</td><td>78.05\u00b15.01</td><td>83.92\u00b12.52</td><td>68.11\u00b10.52</td><td>78.62\u00b13.74</td></tr><tr><td>DIR</td><td>62.07\u00b18.75</td><td>52.27\u00b14.56</td><td>68.07\u00b12.29</td><td>58.08\u00b12.31</td><td>75.49\u00b12.80</td><td>77.42\u00b17.43</td><td>66.86\u00b12.25</td><td>76.40\u00b14.43</td></tr><tr><td>DisC</td><td>51.08\u00b13.08</td><td>50.39\u00b11.15</td><td>68.07\u00b11.75</td><td>58.76\u00b10.91</td><td>57.78\u00b13.60</td><td>71.13\u00b18.86</td><td>67.12\u00b12.11</td><td>56.59\u00b110.09</td></tr><tr><td>CAL</td><td>65.63\u00b14.29</td><td>51.18\u00b15.60</td><td>67.37\u00b13.61</td><td>57.95\u00b12.24</td><td>76.29\u00b11.60</td><td>79.68\u00b14.06</td><td>68.06\u00b12.60</td><td>79.50\u00b14.81</td></tr><tr><td>GREA</td><td>56.74\u00b19.23</td><td>54.13\u00b110.02</td><td>67.79\u00b12.56</td><td>60.71\u00b12.20</td><td>77.16\u00b11.37</td><td>83.15\u00b19.07</td><td>69.72\u00b11.66</td><td>77.34\u00b13.52</td></tr><tr><td>GSAT</td><td>62.80\u00b111.41</td><td>53.20\u00b18.35</td><td>68.66\u00b11.35</td><td>58.06\u00b11.98</td><td>72.32\u00b15.66</td><td>82.45\u00b12.73</td><td>66.78\u00b11.45</td><td>75.63\u00b13.83</td></tr><tr><td>CIGA</td><td>66.43\u00b111.31</td><td>49.14\u00b18.34</td><td>69.40\u00b12.39</td><td>59.55\u00b12.56</td><td>76.44\u00b11.72</td><td>83.95\u00b12.75</td><td>64.92\u00b12.09</td><td>65.98\u00b13.31</td></tr><tr><td>AIA</td><td>73.64\u00b15.15</td><td>55.85\u00b17.98</td><td>71.15\u00b11.81</td><td>61.64\u00b13.37</td><td>79.42\u00b12.01</td><td>85.11\u00b10.74</td><td>70.79\u00b11.53</td><td>81.03\u00b15.15</td></tr><tr><td>OOD-GCL</td><td>56.46\u00b14.61</td><td>60.23\u00b18.49</td><td>70.85\u00b12.07</td><td>58.48\u00b12.94</td><td>75.96\u00b12.21</td><td>85.34\u00b11.77</td><td>67.28\u00b13.09</td><td>78.11\u00b13.32</td></tr><tr><td>EQuAD</td><td>67.11\u00b110.11</td><td>59.72\u00b13.69</td><td>72.24\u00b10.64</td><td>64.19\u00b10.56</td><td>79.15\u00b12.32</td><td>86.41\u00b15.63</td><td>70.22\u00b12.36</td><td>80.82\u00b15.28</td></tr><tr><td>LIRS</td><td>75.51\u00b12.19</td><td>74.95\u00b17.69</td><td>72.82\u00b11.61</td><td>66.64\u00b11.44</td><td>81.91\u00b11.98</td><td>88.77\u00b11.64</td><td>71.04\u00b10.76</td><td>82.19\u00b11.57</td></tr></table>\n\n\n# 5 EXPERIMENTS\n\n\nIn this section, we perform empirical study on both synthetic and real-world datasets. More details about the datasets and experiment setup are included in Appendix H.\n\n\n# 5.1 EXPERIMENTAL SETUP\n\n\nDatasets. We adopt GOOMotif and GOODHIV datasets (Gui et al., 2022), OGBG-Molbace and OGBG-Molbbp datasets (Hu et al., 2020; Wu et al., 2018) to comprehensively evaluate the OOD generalization performance of our proposed framework. For GOOMotif datasets, we adopt base shift and size shift, for OBGG datasets, in addition to scaffold shift, we also create size shift, following previous studies (Gui et al., 2022; Sui et al., 2023). More details on the datasets used in our work are included in Appendix H.\n\n\nBaselines. Besides ERM (Vapnik, 1995), we compare our method against three lines of OOD baselines: (1) OOD algorithms on Euclidean data, including IRM (Arjovsky et al., 2020), VREx (Krueger et al., 2021), and GroupDRO (Sagawa et al., 2019); (2) Diverse feature learning methods, including RSC (Huang et al., 2020), and DiverseModel (Teney et al., 2022). (3) graph-specific OOD and data augmentation algorithms without requiring environment labels, including DIR (Wu et al., 2022c), GSAT (Miao et al., 2022), GREA (Liu et al., 2022), DisC (Fan et al., 2022), CIGA (Chen et al., 2022), AIA (Sui et al., 2023), OOD-GCL (Li et al., 2024a), EQuAD (Yao et al., 2024), DropEdge (Rong et al., 2019), FLAG (Kong et al., 2022), and LiSA (Yu et al., 2023). Details of the baselines and their implementation are provided in Appendix H.\n\n\nEvaluation. We report the ROC-AUC score for GOOD-HIV, OGBG-Molbbbp, and OGBG-Molbace datasets, where the tasks are binary classification. For GOOD-Motif and SPMotif datasets, we use accuracy as the evaluation metric. We run experiments 4 times with different random seeds, select models based on the validation performance, and report the mean and standard deviations on the test set.\n\n\n# 5.2 EXPERIMENTAL RESULTS\n\n\nIn this section, we report the main results on both synthetic and real-world datasets.\n\n\nSynthetic datasets. For the GOOD-Motif datasets, LIRS demonstrates its superiority by outperforming the second-best method by  $12.51\\%$  and  $25.50\\%$  in the base and size split respectively. This substantial margin highlights the effectiveness of LIRS in capturing domain-invariant features across different types of distribution shifts. Although EQuAD adopts a similar learning paradigm\n\n\nby attempting to remove spurious features, LIRS outperforms it by  $25.50\\%$  in the size split of the GOOD-Motif datasets. The underlying reason for this performance gap is that in the training set of GOOD-Motif, the spurious subgraph  $G_{s}$  occupies a significant portion of the graph. The infomax principle adopted in EQuAD can inadvertently affect the learning of spurious features and hinder the subsequent invariant learning process. In contrast, LIRS adopts biased infomax, which facilitates the suppression of invariant signals, thereby allowing it to learn spurious features more effectively. OOD-GCL (Li et al., 2024a) also utilizes contrastive learning to learn invariant features, however, it aims to learn invariant features without labeled data, which is inherently more challenging. As illustrated in Table 1, LIRS outperforms OOD-GCL across all the datasets, likely due to that LIRS has access to labeled data during training, while OOD-GCL uses labeled data only to fine-tune the linear layer. We also compare LIRS with RSC Huang et al. (2020) and DiverseModel Teney et al. (2022), which aim to learn diverse features via ERM. LIRS can be viewed as a diverse feature learning method, as it seeks to learn more invariant features by removing spurious ones. The key distinction between LIRS and these two methods lies in that: while RSC and DiverseModel propose different regularization terms to ERM to capture diverse features, LIRS focuses on removing spurious features from ERM-learned features. As illustrated in Table 1, although RSC and DiverseModel perform well on real-world datasets, they achieve suboptimal results on the GOOD-Motif datasets. This discrepancy may arise from the fact that these methods attempt to capture diverse features, while in the GOOD-Motif datasets, only one invariant subgraph is causally related to the target. As a result, these methods may mistakenly capture non-generalizable patterns in  $G_{s}$ . In contrast, LIRS leverages biased infomax to effectively learn spurious patterns and subsequently identify the invariant subgraph by unlearning the spurious patterns. Lastly, LIRS significantly outperforms other OOD methods that aim to directly learn invariant features, showcasing the efficacy of the proposed learning paradigm.\n\n\nReal-world datasets. In real-world datasets, which present more complex and realistic distribution shifts, many graph OOD algorithms exhibit instability, occasionally underperforming ERM. In contrast, LIRS adopts an indirect learning paradigm, which may enable the learning of more invariant features compared to previous graph invariant learning methods, leading to more effective OOD generalization. Similarly to the results on synthetic datasets, LIRS also demonstrates superior OOD performance under both scaffold shift and size shift compared to other methods. EQuAD, which adopts a similar learning paradigm as LIRS, shows a comparable trend, with a more significant advantage under the size shift relative to other methods. Furthermore, the improvements of LIRS over EQuAD across various datasets highlights the effectiveness of the biased infomax principle and the intra-class cross-entropy loss for learning spurious features and invariant features respectively. Compared to other baselines that learns invariant features directly, LIRS outperforms the best method (AIA) by  $4.62\\%$  in size shift and  $1.95\\%$  in scaffold shift respectively.\n\n\n# 5.3 IN-DEPTH ANALYSIS\n\n\nCan LIRS learn more invariant features? To further validate the quality of invariant features learned by LIRS, we curated a dataset derived from the SPMotif dataset. Specifically, we constructed a binary classification dataset where the motifs House and Crane corresponds to label 0, and Diamond and Cycle corresponds to label 1. During the construction of each class's samples, we attached both invariant subgraphs to the base subgraph with  $50\\%$  chance, while in the remaining  $50\\%$ , we randomly attached one invariant subgraph to the base spurious subgraph. For the test set, we randomly attached a single invariant subgraph to the base subgraph. Similar to the SPMotif dataset, the base spurious subgraph was correlated with the target la\n\n\nTable 2: Performance on the SPMotif-binary datasets under varying spurious correlations.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">SPMotif-binary</td></tr><tr><td>b=0.40</td><td>b=0.60</td><td>b=0.90</td></tr><tr><td>ERM</td><td>74.93\u00b13.94</td><td>72.78\u00b13.15</td><td>63.78\u00b14.18</td></tr><tr><td>IRM</td><td>76.01\u00b14.12</td><td>70.85\u00b14.73</td><td>66.55\u00b14.80</td></tr><tr><td>VREx</td><td>79.03\u00b11.02</td><td>73.78\u00b11.75</td><td>65.27\u00b16.78</td></tr><tr><td>GIL</td><td>77.15\u00b13.18</td><td>73.85\u00b12.76</td><td>68.90\u00b17.28</td></tr><tr><td>GREA</td><td>79.65\u00b16.36</td><td>73.01\u00b17.99</td><td>69.85\u00b10.35</td></tr><tr><td>CIGA</td><td>76.93\u00b13.94</td><td>71.70\u00b11.55</td><td>66.80\u00b15.35</td></tr><tr><td>AIA</td><td>78.46\u00b13.19</td><td>71.83\u00b10.69</td><td>64.37\u00b14.14</td></tr><tr><td>EQuAD</td><td>80.82\u00b10.65</td><td>74.20\u00b14.10</td><td>69.79\u00b17.81</td></tr><tr><td>LIRS</td><td>82.17\u00b10.91</td><td>75.32\u00b11.65</td><td>71.29\u00b12.12</td></tr></table>\n\n\nbels, maintaining an equal correlation strength with the labels in the test set. In addition to evaluating LIRS against traditional methods such as IRM, VRex and their variants, we also incorporated strong graph-specific OOD baselines, including CIGA, AIA, and EQuAD for comparisons. The ex\n\n\nExperimental results in Table 2 demonstrate that LIRS consistently outperforms other baselines across varying spurious correlation strengths, indicating its superior ability to learn more invariant features. Similarly, EQuAD, adopting a similar learning paradigm, also achieves competitive performance in SPMotif-binary datasets. This demonstrates that learning invariant features indirectly can lead to learning a more comprehensive set of invariant features and achieving better OOD generalization ability.\n\n\n![(a) Ablation study on biased infomax and intra-class cross-entropy loss. Figure 4: Ablation and hyperparameter sensitivity studies for LIRS.](images/52c4afbb88b891862759bcd6da85f0b71a7dad421c9d229e0a3bd4571a991717.jpg)\n\n\n![(b) Hyperparameter sensitivity analysis on GOODHIV datasets.](images/6380874ca18b68e126048cd9d1e76f038db57e853e70256eadb6f1e8b7d4bbda.jpg)\n\n\nAblation study. We conducted an ablation study on biased infomax and class-conditioned (intra-class) cross-entropy loss to evaluate their effectiveness in LIRS. As illustrated in Figure 4a, replacing biased infomax with vanilla infomax to generate spurious features leads to performance degradation across all datasets. This decline is primarily due to the enhanced capability of biased infomax in learning spurious features, which is crucial for subsequent steps for learning graph invariance. Furthermore, replacing class-conditioned cross-entropy loss with standard cross-entropy loss also led to negative effects, which is mainly due to the interference of spurious patterns across different classes, as discussed in Sec. 4. One typical empirical support arises from the Motif-size dataset, where each spurious pattern is correlated with different target labels with nearly equal strength. As a result, even though biased infomax can generate accurate spurious features, the logits generated using the standard cross-entropy loss are not sufficiently precise, which aligns with Prop. 2. This limitation significantly constrains the test performance, as illustrated in Figure 4a. In contrast, the class-conditioned cross-entropy loss is able to generate more accurate spurious logits, thus enhancing the OOD performance.\n\n\nHyperparameter Sensitivity. We investigate the impact of hyperparameters in LIRS, including the reweighting coefficient  $\\gamma$ , the penalty weights  $\\lambda$ , and hyperparameters related to biased infomax. Specifically, we analyze the effect of the epoch  $E$  at which embeddings are derived from biased infomax and the threshold used to determine whether a sample should be considered biased or not. As illustrated in Figure 4b, for both the GOODHIV-Sca and GOODHIV-Size datasets, LIRS demonstrates stable performance, highlighting its robustness to variations in hyperparameter settings. This consistency suggests that LIRS is agnostic to hyperparameter tuning, which further emphasizes its applicability in real-world scenarios.\n\n\nHow do different size split methods affect the OOD performance? For the real-world datasets, the size split was performed in descending order, where the graphs in the training set have a larger number of nodes compared\n\n\nto those in the test set. To evaluate the impact of varying graph sizes on generalization ability of various OOD methods, we also performed size splits based on ascending order for the OGBG-\n\n\nTable 3: Performance on OGBG datasets using a different size split procedure, where the graphs in the training set have a smaller number of nodes compared to those in the test set.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">OGBG Datasets</td></tr><tr><td>OGBG-Molbace</td><td>OGBG-Molbbbp</td></tr><tr><td>ERM</td><td>76.31\u00b10.58</td><td>87.31\u00b12.37</td></tr><tr><td>IRM</td><td>79.44\u00b13.05</td><td>88.77\u00b12.45</td></tr><tr><td>VREx</td><td>76.38\u00b11.75</td><td>84.20\u00b12.80</td></tr><tr><td>GREA</td><td>77.46\u00b15.57</td><td>86.18\u00b12.54</td></tr><tr><td>GSAT</td><td>72.29\u00b14.45</td><td>87.46\u00b12.67</td></tr><tr><td>CIGA</td><td>77.89\u00b13.68</td><td>87.94\u00b10.86</td></tr><tr><td>AIA</td><td>78.62\u00b12.88</td><td>89.18\u00b11.77</td></tr><tr><td>EQuAD</td><td>82.34\u00b13.42</td><td>88.65\u00b13.83</td></tr><tr><td>LIRS</td><td>84.62\u00b10.84</td><td>90.24\u00b14.14</td></tr></table>\n\n\nMolbace and OGBG-Molbbb datasets. The results of these experiments are presented in Table 3. As shown, LIRS continues to outperform all competitive baselines under this alternative size split. Specifically, LIRS achieves the highest ROC-AUC scores on both OGBG-Molbace and OGBG-Molbbb datasets. This highlights the superiority of LIRS in generalizing to larger graphs when trained on smaller ones, further confirming its potential for real-world applications.\n\n\n# 6 RELATED WORK\n\n\nLearning invariant features. OOD generalization is a critical challenge in machine learning, where models trained on a specific data distribution often fail to generalize well to unseen distributions. Recently invariance learning has been proposed to tackle this issue, which builds upon the theory of causality (Peters et al., 2016; Pearl, 2009) to learn causally-related representation that remain stable across different environments (Arjovsky et al., 2020; Parascandolo et al., 2020; Mahajan et al., 2021; Wald et al., 2021; Ahuja et al., 2020; 2021). Inspired from IRM (Arjovsky et al., 2020), several invariant learning methods on graphs are proposed (Yang et al., 2022; Li et al., 2022b; Fan et al., 2022; Liu et al., 2022; Wu et al., 2022c; Chen et al., 2022; 2023a; Gui et al., 2023; Sui et al., 2023; Li et al., 2024b) to learn graph-level representations that are robust to distribution shifts. Most of these methods aim to learn invariant features directly by training an equipredictive classifier. Recent study (Yao et al., 2024) has proposed a new learning paradigm to learn graph invariance indirectly by learning spurious features first, then disentangle them from the ERM-learned representations. In this work, we adopt this learning paradigm for learning graph invariance.\n\n\nSelf-supervised learning induces spuriosity. Self-supervised learning (SSL) has emerged as a potent paradigm for learning representations from unlabeled datasets. The fundamental concept involves devising pretext tasks that maximize similarity between two augmented views of the same data point, often utilizing the InfoNCE loss (Oord et al., 2018). This approach has been widely applied in domains such as images (Chen et al., 2020a;b) and graphs (Velickovi\u0107 et al., 2019; Sun et al., 2019; Zhu et al., 2020; You et al., 2020). Recent work have shown that SSL may tend to learn spurious features in both image domain (Hamidieh et al., 2024; Meehan et al., 2023) and graph domain (Yao et al., 2024). Hamidieh et al. (2024) attempted to enhance SSL model performance by addressing this issue and promoting the learning of invariant features. In contrast, our goal in this work is to improve SSL model ability to capture spuriosity, thereby facilitating better feature disentanglement for invariant learning.\n\n\nLearning diverse features. Recent studies have shown that ERM tends to encourage models to learn the simplest predictive features (Hermann & Lampinen, 2020; Kalimeris et al., 2019; Neyshabur et al., 2014; Pezeshki et al., 2021). This simplicity bias causes the models to rely on simple (spurious) but non-causal features, ignoring more complex patterns that might be equally predictive. To address this challenge, Huang et al. (2020) employs a self-challenging mechanism to force the model to learn diverse patterns by discarding dominant features, while Teney et al. (2022) constructs diverse features by training a collection of classifiers with diversity regularization. Additionally, Zhang et al. (2022a); Chen et al. (2023b) adopt DRO to iteratively explore new features. While previous works aim to encourage models to learn a diverse set of predictive patterns, our method takes a different approach. Specifically, we aim to remove or unlearn spurious features from ERM-learned features, thereby allowing the model to capture more invariant features.\n\n\n# 7 CONCLUSIONS\n\n\nIn this study, we identified a critical limitation in IRM, VRex and their variants on graph invariant learning, i.e., these methods may only capture a subset of the invariant features, thereby limiting their OOD generalization performance. To address this issue, we investigate the effectiveness of learning invariant features indirectly by first learning and then removing spurious features. Our theoretical and empirical analyses demonstrate that this approach facilitates the learning of a more comprehensive set of invariant features than traditional (graph) invariant learning methods, thereby improving generalization to unseen environments. We then propose LIRS that adopts this learning paradigm, which consists of: a) The biased infomax principle, and b) The class-conditioned cross-entropy loss, which elicit effective spuriosity learning and invariant learning respectively, aiming to learn more invariant features. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of our proposed method over existing state-of-the-art OOD algorithms.\n\n\n# REFERENCES\n\n- Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. In International Conference on Machine Learning, pp. 145-155. PMLR, 2020.\n\n- Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. Advances in Neural Information Processing Systems, 34:3438-3450, 2021.\n\n- Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization, 2020.\n\n- Steve Azzolin, Antonio Longa, Pietro Barbiero, Pietro Li\u00f2, and Andrea Passerini. Global explainability of gnns via logic combination of learned concepts. arXiv preprint arXiv:2210.07147, 2022.\n\n- Guy W Bemis and Mark A Murcko. The properties of known drugs. 1. molecular frameworks. Journal of medicinal chemistry, 39(15):2887-2893, 1996.\n\n- Ngoc Bui, Hieu Trung Nguyen, Viet Anh Nguyen, and Rex Ying. Explaining graph neural networks via structure-aware interaction index. arXiv preprint arXiv:2405.14352, 2024.\n\n- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. PMLR, 2020a.\n\n- Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.\n\n- Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, MA Kaili, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-distribution generalization on graphs. Advances in Neural Information Processing Systems, 35:22131-22148, 2022.\n\n- Yongqiang Chen, Yatao Bian, Kaiwen Zhou, Binghui Xie, Bo Han, and James Cheng. Does invariant graph learning via environment augmentation learn invariance? In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openreview.net/forum?id=EqpR9Vtt13.\n\n- Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, and James Cheng. Understanding and improving feature learning for out-of-distribution generalization. Advances in Neural Information Processing Systems, 36, 2023b.\n\n- Alex J. DeGrave, Joseph D. Janizek, and Su-In Lee. Ai for radiographic covid-19 detection selects shortcuts over signal. medRxiv, 2020. URL https://api(semanticscholar.org/ CorpusID:221653623.\n\n- Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. Advances in neural information processing systems, 32, 2019.\n\n- Cian Eastwood, Shashank Singh, Andrei L Nicolicioiu, Marin Vlastelica Pogan\u010di\u0107, Julius von K\u00fcgelgen, and Bernhard Sch\u00f6lkopf. Spuriosity didn't kill the classifier: Using invariant predictions to harness spurious features. Advances in Neural Information Processing Systems, 36, 2023.\n\n- Shaohua Fan, Xiao Wang, Yanhu Mo, Chuan Shi, and Jian Tang. Debiasing graph neural networks via learning disentangled causal substructure. Advances in Neural Information Processing Systems, 35:24934-24946, 2022.\n\n- Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric, 2019.\n\n- Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of machine learning research, 17(59):1-35, 2016.\n\n- Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=8hHg-zs_p-h.\n\n- Shurui Gui, Meng Liu, Xiner Li, Youzhi Luo, and Shuiwang Ji. Joint learning of label and environment causal independence for graph out-of-distribution generalization. Advances in Neural Information Processing Systems, 36, 2023.\n\n- Kimia Hamidieh, Haoran Zhang, Swami Sankaranarayanan, and Marzyeh Ghassemi. Views can be deceiving: Improved SSL through feature space augmentation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=mutJBk3ILg.\n\n- Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In International conference on machine learning, pp. 4116-4126. PMLR, 2020.\n\n- Katherine Hermann and Andrew Lampinen. What shapes feature representations? exploring datasets, architectures, and training. Advances in Neural Information Processing Systems, 33: 9995-10006, 2020.\n\n- R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization, 2019.\n\n- Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give robust classifiers? In International Conference on Machine Learning, pp. 2029-2037. PMLR, 2018.\n\n- Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118-22133, 2020.\n\n- Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. arXiv preprint arXiv:2102.09548, 2021.\n\n- Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In Computer vision-ECCV 2020: 16th European conference, Glasgow, UK, August 23-28, 2020, proceedings, part II 16, pp. 124-140. Springer, 2020.\n\n- Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Long-Kai Huang, Tingyang Xu, Yu Rong, Lanqing Li, Jie Ren, Ding Xue, et al. Drugood: Out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discovery-a focus on affinity prediction problems with noise annotations. arXiv preprint arXiv:2201.09637, 2022.\n\n- Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in neural information processing systems, 32, 2019.\n\n- Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:18661-18673, 2020.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\n- Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=SJU4ayYgl.\n\n- Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations, 2023.\n\n- Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal-subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International conference on machine learning, pp. 5637-5664. PMLR, 2021.\n\n- Kezhi Kong, Guohao Li, Muong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. Robust optimization as data augmentation for large-scale graphs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 60-69, 2022.\n\n- Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L\u00e9tourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. Advances in Neural Information Processing Systems, 34:21618-21629, 2021.\n\n- David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In International Conference on Machine Learning, pp. 5815-5826. PMLR, 2021.\n\n- Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Ood-gnn: Out-of-distribution generalized graph neural network. IEEE Transactions on Knowledge and Data Engineering, 2022a.\n\n- Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning invariant graph representations for out-of-distribution generalization. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022b. URL https://openreview.net/forum?id=acKK8MQe2xc.\n\n- Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Invariant node representation learning under distribution shifts with multiple latent environments. ACM Transactions on Information Systems, 42(1):1-30, 2023a.\n\n- Haoyang Li, Xin Wang, Zeyang Zhang, Haibo Chen, Ziwei Zhang, and Wenwu Zhu. Disentangled graph self-supervised learning for out-of-distribution generalization. In *Forty-first International Conference on Machine Learning*, 2024a.\n\n- Xiner Li, Shurui Gui, Youzhi Luo, and Shuiwang Ji. Graph structure and feature extrapolation for out-of-distribution generalization. arXiv preprint arXiv:2306.08076, 2023b.\n\n- Xiner Li, Shurui Gui, Youzhi Luo, and Shuiwang Ji. Graph structure extrapolation for out-of-distribution generalization. In *Forty-first International Conference on Machine Learning*, 2024b. URL https://openreview.net/forum?id=Xgrey8uQhr.\n\n- Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European conference on computer vision (ECCV), pp. 624-639, 2018.\n\n- R. Linsker. Self-organization in a perceptual network. Computer, 21(3):105-117, 1988. doi: 10. 1109/2.36.\n\n- Gang Liu, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Graph rationalization with environment-based augmentations. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '22. ACM, August 2022. doi: 10.1145/3534678.3539347. URL http://dx.doi.org/10.1145/3534678.3539347.\n\n- Yang Liu, Xiang Ao, Fuli Feng, Yunshan Ma, Kuan Li, Tat-Seng Chua, and Qing He. Flood: A flexible invariant learning framework for out-of-distribution generalization on graphs. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1548-1558, 2023.\n\n- Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Parameterized explainer for graph neural network. Advances in neural information processing systems, 33:19620-19631, 2020.\n\n- Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In International conference on machine learning, pp. 7313-7324. PMLR, 2021.\n\n- Casey Meehan, Florian Bordes, Pascal Vincent, Kamalika Chaudhuri, and Chuan Guo. Do ssl models have d\u00e9j\u00e0 vu? a case of unintended memorization in self-supervised learning. Advances in Neural Information Processing Systems, 36, 2023.\n\n- Siqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. In International Conference on Machine Learning, pp. 15524-15543. PMLR, 2022.\n\n- Hongseok Namkoong and John C. Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS'16, pp. 2216-2224, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819.\n\n- Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.\n\n- Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n\n- Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, and Bernhard Scholkopf. Learning explanations that are hard to vary. arXiv preprint arXiv:2009.00329, 2020.\n\n- Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.\n\n- Judea Pearl. Causality. Cambridge University Press, Cambridge, UK, 2 edition, 2009. ISBN 978-0-521-89560-6. doi: 10.1017/CBO9780511803161.\n\n- F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.\n\n- Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(5):947-1012, 2016.\n\n- Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. Advances in Neural Information Processing Systems, 34:1256-1272, 2021.\n\n- Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainability methods for graph convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10772-10781, 2019.\n\n- Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. Deepinf: Social influence prediction with deep learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery amp; Data Mining, KDD '18. ACM, July 2018. doi: 10.1145/3219819.3220077. URL http://dx.doi.org/10.1145/3219819.3220077.\n\n- Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Droppedge: Towards deep graph convolutional networks on node classification. arXiv preprint arXiv:1907.10903, 2019.\n\n- Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.\n\n- Thomas Schnake, Oliver Eberle, Jonas Lederer, Shinichi Nakajima, Kristof T Sch\u00fctt, Klaus-Robert M\u00fcller, and Gr\u00e9goire Montavon. Higher-order explanations of graph neural networks via relevant walks. IEEE transactions on pattern analysis and machine intelligence, 44(11):7581-7596, 2021.\n\n- Yongduo Sui, Qitian Wu, Jiancan Wu, Qing Cui, Longfei Li, Jun Zhou, Xiang Wang, and Xiangnan He. Unleashing the power of graph data augmentation on covariate distribution shift. Advances in Neural Information Processing Systems, 36, 2023.\n\n- Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation, 2016.\n\n- Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. arXiv preprint arXiv:1908.01000, 2019.\n\n- Damien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton Van den Hengel. Evading the simplicity bias: Training a diverse set of models discovers solutions with superior ood generalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16761-16772, 2022.\n\n- George R Terrell and David W Scott. Variable kernel density estimation. The Annals of Statistics, pp. 1236-1265, 1992.\n\n- Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle, 2015.\n\n- Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\n\n- Vladimir N. Vapnik. The nature of statistical learning theory. Springer-Verlag New York, Inc., 1995. ISBN 0-387-94559-8.\n\n- Petar Velickovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\n- Petar Velickovi\u0107, William Fedus, William L Hamilton, Pietro Li\u00f2, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.\n\n- Petar Velickovi\u0107, William Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rklz9iAcKQ.\n\n- Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain generalization. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=XWYJ25-yTRS.\n\n- Xiaoqi Wang and Han-Wei Shen. Gnninterpreter: A probabilistic generative model-level explanation for graph neural networks. arXiv preprint arXiv:2209.07924, 2022.\n\n- Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. Handling distribution shifts on graphs: An invariance perspective, 2022a.\n\n- Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in recommender systems: A survey. ACM Comput. Surv., 55(5), dec 2022b. ISSN 0360-0300. doi: 10.1145/3535101. URL https://doi.org/10.1145/3535101.\n\n- Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. In International Conference on Learning Representations, 2022c. URL https://openreview.net/forum?id=hGXij5rfiHw.\n\n- Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Genisses, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. Moleculenet: A benchmark for molecular machine learning, 2018.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638422.2455928", "msecs": "245.0", "relativeCreated": "19076.090335845947", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws\n\n# Abstract\nThe composition of pretraining data is a key determinant of foundation models' performance, but there is no standard guideline for allocating a limited computational budget across different data sources. Most current approaches either rely on extensive experiments with smaller models or dynamic data adjustments that also require proxy models, both of which significantly increase the workflow complexity and computational overhead. In this paper, we introduce Adaptive Data Optimization (ADO), an algorithm that optimizes data distributions in an online fashion, concurrent with model training. Unlike existing techniques, ADO does not require external knowledge, proxy models, or modifications to the model update. Instead, ADO uses per-domain scaling laws to estimate the learning potential of each domain during training and adjusts the data mixture accordingly, making it more scalable and easier to integrate. Experiments demonstrate that ADO can achieve comparable or better performance than prior methods while maintaining computational efficiency across different computation scales, offering a practical solution for dynamically adjusting data distribution without sacrificing flexibility or increasing costs. Beyond its practical benefits, ADO also provides a new perspective on data collection strategies via scaling laws.\n\n\n# 1 INTRODUCTION\n\n\nFoundation models, large neural networks pre-trained on vast amounts of data, are the backbone for a wide range of today's machine learning workload (Bommasani et al., 2021). It is well established that the composition of the pretraining data plays a crucial role in these models' final performance (Gadre et al., 2024); however, there are currently no standard guidelines for selecting good pretraining data. Most existing datasets are filtered and curated snapshots of the web data (Gao et al., 2020; Penedo et al., 2024), often categorized into distinct heterogeneous domains (e.g., CommonCrawl and Github). Even after filtering, a challenge remains: deciding how to allocate computational resources across these different domains. Given the ever increasing cost of pretraining large foundation models (Sharir et al., 2020), optimizing the data composition is a potential avenue for improving performance without increasing computational cost.\n\n\nThere are two popular approaches to adjusting the data distribution. A straightforward approach to this problem is to experiment with smaller models, test different data policies, and then apply the best-performing policy to the larger model (Ye et al., 2024; Ge et al., 2024). However, this strategy has several drawbacks. First, as the number of domains increases, the cost of training on all possible policies can grow linearly or even exponentially (if the level of discretization remains the same), making even small-scale experiments costly. Second, the optimal data policy for a smaller model does not necessarily generalize to larger models (Ye et al., 2024; Albalak et al., 2023). Another approach involves dynamically adjusting the data policy during training based on various statistics (Xie et al., 2024; Chen et al., 2024; Qin et al., 2024; Fan et al., 2023). However, these methods often require training a smaller proxy model to guide the data policy, whose cost is still non-negligible.\n\n\nA significant limitation of almost all existing methods (with the exception of Albalak et al. (2023)) is their requirement for additional computational steps beyond standard training. This extra complexity entails substantial modifications to the training pipeline, making it challenging to integrate these methods when other components are varied (e.g., the architecture, tokenizer, or optimizer). The consequence of this usage barrier is that these methods have not been tested widely, making them less\n\n\n![Figure 1: ADO is a cheap online technique for adjusting the data distribution while training large models. In contrast to prior methods, ADO tailors its data distribution to the model as it is being trained, and does not require training smaller proxy models in advance.](images/9697384cb73119500dd1ce99a205ab4ad357ee227801b9967a8dda180e28b851.jpg)\n\n\n![Multi-staged data selection with proxy models Adaptive Data Optimization (ADO)](images/9c989d2e48a57b7462591bc2ea0bf1644160cd19b1802b08a81a57fe0c12afcc.jpg)\n\n\npopular for those training a new model. To enable broader adoption of data mixture optimization, we argue that it must be implemented in an online fashion, concurrent with model training, without disrupting the standard training process, which means that the data mixture must adapt itself based on the feedback from the model. This is closely related to curriculum learning (Bengio et al., 2009), a training strategy where models are progressively exposed to domains in a curated order.\n\n\nAt a fundamental level, this work develops and empirically investigates questions about the existence of good pretraining curricula and the feasibility of cheaply identifying them. We first demonstrate in controlled experiments that good curricula can generally be found with more computation and that it is hard to accurately predict larger models' performance with only a few small models. This leads to our main contribution, Adaptive Data Optimization (ADO): an algorithm for adaptively adjusting the data mixture online during training (Figure 1). In experiments on language models up to 1.3B parameters trained on the Pile (Gao et al., 2020), we find that ADO improves performance across a variety of common benchmarks and improves validation loss on SlimPajama (Soboleva et al., 2023) and FineWeb (Penedo et al., 2023), both of which are considered to be higher-quality datasets. Most importantly, ADO achieves these without requiring significant additional computation (less than  $0.4\\%$  wallclock time for 1.3B), proxy models, or extensive modification to the training pipelines.\n\n\n# 2 MOTIVATIONS\n\n\nThe drawbacks of small proxy models. At first glance, transferring data selection strategies from smaller to larger models seems plausible, with positive results in specific cases, such as in Mindermann et al. (2022) and Xia et al. (2024b). However, as shown in Figure 2, using a small number of models to predict the behavior of larger models can exhibit high variance on the Pythia model family (Biderman et al., 2023), indicating that they cannot reliably forecast larger models' performance. At a per-domain level (Figure 7), the variation becomes even more pronounced. While this does not fully rule out the possibility of transfer, it raises doubts about whether small models can reliably select data mixtures for larger models. Another issue is that the efficacy of distributions generated by proxy models is highly sensitive to factors like the tokenizer and other hyperparameters (Albalak et al., 2023). Our findings also confirm this brittleness (Section 4). This means that the optimized distributions from one experiment might not transfer, and they need to be retrained for a new model. This is undesirable because proxy models add non-trivial overhead \u2014 Ge et al. (2024)\n\n\nestimated that a single round of DoReMi proxy training requires 760 GPU hours on 8 Nvidia A100 GPUs, and several proxy training rounds are needed before training the full model.\n\n\nThe existence of good curricula. Curriculum learning (Bengio et al., 2009) is a conceptually appealing idea that has been regularly studied over the years, but these methods have not been widely\n\n\n![Figure 2: Extrapolating larger models' loss using scaling laws up to a threshold model size, indicated by color. Blue dots correspond to different models. Extrapolations become more accurate with more models.](images/0083870cabf99e4a3f0bdcc0ec6148faadaa488f30861ef6c96b61f101729e4a.jpg)\n\n\n![Figure 3: The average learning curves of SGD and the meta-optimized curricula for a logistic regression (Left) and an MLP (Middle). The meta-optimized curricula consistently outperform SGD. Further, the meta loss for curricula is still decreasing after 1000 steps (Right). The first two figures are averaged over 50 runs and the shaded region corresponds to 1 standard deviation.](images/5d32c8b54d184ada09e4785f7ed86414ff312fba0a725c31b5d3435c348b718d.jpg)\n\n\n![](images/ceb85badf6f0996317ed829beb993b51b646dba869bc0370d5284bbf763ee42e.jpg)\n\n\n![](images/0266dccf9d95578c9841e3671e42df4ffa39289eabaeb0aaf9929caefa948c04.jpg)\n\n\nadopted in deep learning outside of a few relatively niche areas (Graves et al., 2017; Florensa et al., 2017; Jesson et al., 2018; Tang et al., 2018). To the best of our knowledge, there is no consensus on why curriculum learning has not had wide success in deep learning. Some hypothesize that deep learning is different from human learning or that overparameterization makes curriculum less effective (Wu et al., 2021; Mannelli et al., 2024). On the other hand, some experimental evidence suggests that deep neural networks implicitly learn functions of increasing complexity from easy to hard (Kalimeris et al., 2019; Baldock et al., 2021), loosely resembling some aspects of human learning (Lawrence, 1952; Baker & Osgood, 1954; Skinner, 1958). For overparameterization, it is not clear if modern foundation model pretraining is in the overparameterized regime since dataset size generally scales with the number of model parameters (Hoffmann et al., 2022).\n\n\nTo motivate our later work, we propose an alternative hypothesis: good curricula exist but they are computationally difficult to find. Let's start with an instructive experiment of finding good curricula with more computation. Suppose we have a fixed dataset of size  $N$  and want to train SGD at a minibatch size of 1 for 1 epoch (i.e., each data point is used once, which is common for language model pretraining). We are interested in finding a data ordering that is better than a random shuffle when training a model initialized at random. Brute force search is intractable even for a small dataset. Unlike most existing curricula that rely on some prior notions of difficulty, we will find a good curriculum with only training data by optimizing over the space of permutations with meta learning. Due to space constraints, we defer the details of this meta learning procedure to Appendix B.\n\n\nWe apply this procedure to a logistic regression problem as well as a multi-layer perceptron where the goal is to imitate a teacher. We randomly sample the initialization  $\\theta_0$ , ground truth  $\\theta^{\\star}$  and the data  $\\mathcal{Z} = \\{(x_i, f(x_i; \\theta^{\\star}))\\}_{i=1}^{N}$ . For a fixed set of data points, we run the meta optimization for some steps (200 for logistic regression and 1000 for MLP) and evaluate the performance of learned ordering on a new initialization. We repeat this experiment 50 times and report the results in Figure 3. As can be seen from the results, the performance of meta-optimized data ordering is significantly better than that of random order even though during evaluation the models are initialized at random. In a concurrent work, Gu et al. (2024) conducted a similar experiment but their meta objective uses additional validation data and optimizes a distribution over all training data points rather than SGD.\n\n\nThis shows that given a dataset, it is possible to find an ordering that greatly outperforms random ordering without committing to any prior notion of difficulty. This procedure does not leverage additional data, but it requires significantly more computational resources than random ordering, so it is neither practical nor scalable. Moreover, given our limited understanding of the landscape of curriculum optimization, it is highly unlikely that we are identifying the globally optimal curriculum. As shown in Figure 3 (right), even this meta-optimization has not fully converged after 1000 steps. Nonetheless, it demonstrates definitively the existence of a good curriculum, which raises the question of how we can find a better curriculum in a computationally efficient manner<sup>1</sup>.\n\n\n# 3 ADAPTIVE DATA OPTIMIZATION\n\n\nWe demonstrated above a general, albeit costly, strategy for finding good curricula but it is too expensive to be practically useful. We believe that an ideal online data selection strategy for pretraining foundation models should incur a minimal computational cost.\n\n\nFurthermore, computational efficiency alone is not sufficient. Perhaps an equally important consideration is that such methods should avoid explicit dependence on any particular specification of downstream tasks. The fundamental premise of a foundation model is to serve as the basis for all reasonable downstream tasks; explicitly relying on a fixed set of downstream tasks to pick pretraining data distribution risks unintentionally overfitting to the downstream tasks (Goodhart, 1985). Indeed, the evaluation of current language models has become increasingly difficult as performance on existing benchmarks becomes saturated. While evaluating downstream tasks is undoubtedly important because they are representative of the models' main use cases, we believe that it is valuable to define an objective for online data selection that is agnostic to the downstream tasks.\n\n\nPrevious works like DoReMi (Xie et al., 2024), and DoGE (Fan et al., 2023) share this task-agnostic philosophy of data selection. However, these methods require training proxy models which makes them inconvenient and expensive to use. Lifting the requirement for a proxy model would make data selection methods more accessible and practical.\n\n\nThe goal of this work is to advocate for data selection methods that satisfy the following desiderata:\n\n\n# ADAPTIVE DATA OPTIMIZATION\n\n- (i) Does not leverage extra information such as additional data or existing models.\n\n- (ii) Does not depend on multi-staged training with different proxy models.\n\n- (iii) Does not require significant computational resources and can be run online.\n\n\nBased on these criteria, we introduce our method of online data selection, Adaptive Data Optimization (ADO). In the following sections, we present the algorithmic components of ADO. First, we discuss how we fit a scaling law for each domain, allowing the data mixture to account for variations in the natural diversity across different domains. Next, we describe how the data mixture is dynamically adjusted based on these scaling laws during model training. Each domain is characterized by two key quantities: 1. the domain's learning potential, indicating the potential improvement from further optimization in that domain (Section 3.1), and 2. a credit assignment score that quantifies the domain's contribution to the reduction of training loss (Section 3.2). To address potential noise from simultaneously updating the data mixture and the model, we employ several time-averaging techniques to smooth these quantities (Section 3.3).\n\n\n# 3.1 CONSTRUCTION NEURAL SCALING LAWS FOR EACH DOMAIN\n\n\nTo achieve our desiderata, it is crucial to be able to predict the evolution of our model's training online with cheap computational routines that do not scale with the size of the model. To accomplish this, we use neural scaling laws (Kaplan et al., 2020) to extrapolate the loss as a function of the amount of training data observed so far, which we denote  $n$ . It is important to highlight that this scaling law is for predicting the future training loss within a single training run. In contrast, typical neural scaling laws (Hoffmann et al., 2022) use the results of completed training runs to extrapolate the loss of future training runs, typically with larger models or datasets.\n\n\nFor simplicity and interpretability, we use a standard power law:  $\\widehat{\\mathcal{L}}(n; \\alpha, \\beta, \\varepsilon) = \\varepsilon + \\beta n^{-\\alpha}$ , where  $\\alpha, \\beta, \\varepsilon$  are the parameters to be fitted, and  $\\widehat{\\mathcal{L}}$  is a surrogate loss used to model the behavior of the true loss. The parameter  $\\varepsilon$  can be seen as the irreducible loss or the \"entropy\" of the data. Data with high diversity (e.g., CommonCrawl) would naturally have a higher irreducible loss and data that are more predictable (e.g., code and math) would tend to have a lower irreducible loss. The parameter  $\\alpha$  measures how quickly the loss decreases with more training data. Although such scaling laws may be inaccurate at extrapolating training loss far into the future (e.g., due to the learning rate schedule),\n\n\nwe can cheaply refit the parameters on the fly (Appendix A.1). Since the parameters from each fit are only used for a short period before being updated and the learning curves usually do not change abruptly, our scaling law need only be locally accurate.\n\n\nInstead of fitting a single scaling law for the overall loss (Kaplan et al., 2020; Hoffmann et al., 2022), we will fit a separate scaling law for the loss on each domain, each of which will guide ADO to adjust the weights of each domain.\n\n\nDefinition 1. A domain scaling law is a training sample scaling law,  $\\widehat{\\mathcal{L}}_k(n) = \\widehat{\\mathcal{L}}(n; \\alpha_k, \\beta_k, \\varepsilon_k) = \\varepsilon_k + \\beta_k n^{-\\alpha_k}$ , that predicts the  $k^{th}$  domain's training loss after training on  $n$  samples.  $\\varepsilon_k, \\beta_k, \\alpha_k$  are functions of the architecture, training algorithms, and the overall data distribution.\n\n\nThe domain scaling law serves as a tractable middle ground between modeling the overall loss and modeling how every domain interacts with each other. Modeling the full dependencies faithfully may be challenging while training online because we cannot modify the data distribution drastically. As the first step towards this goal, our method will only model how each domain interacts with itself and leave more complex modeling to future works. Nonetheless, this scaling law reveals important information about each domain. For example,  $\\varepsilon_{k}$  corresponds to the estimated irreducible loss of a domain, which could be useful for various applications (Xia et al., 2024a). More importantly, we can interpret the derivative of the loss with respect to the number of samples as the following:\n\n\n$$\n\n$$\n\\frac {d \\widehat {\\mathcal {L}} _ {k} (n)}{d n} = \\frac {- \\alpha_ {k} \\beta_ {k} n ^ {- \\alpha_ {k}}}{n} = - \\frac {1}{n} \\begin{array}{c c} \\alpha_ {k} & (\\widehat {\\mathcal {L}} _ {k} (n) - \\varepsilon_ {k}) \\\\ \\text {L e a n i n g s p e e d} & \\text {R e d u c i b l e l o s s} \\end{array} . \\tag {1}\n$$\n\n$$\n\n\nThe quantity  $\\widehat{\\mathcal{L}}_k(n) - \\varepsilon_k$  informs us about how far away a particular domain is from the estimated minimum loss, which coincides with the notion of population-level reducible loss (Mindermann et al., 2022), and  $\\alpha_{k}$  indicates how fast the loss of a particular domain is changing with new data. This derivative informs us about how much loss decrease we can expect per data point locally. At a high level, the cross entropy loss has an information theoretic interpretation, so this quantity can be understood as a form of information density, or, the amount of information gain per unit of computation. Intuitively, prioritizing data with high information density is a likely better use of computation and could lead to richer representation as it implies more information is \"stored in\" the model weights. We leave the formalization of this statement to future works. It may be useful to contrast this quantity with Albalak et al. (2023, ODM) which only prioritizes domains with a high loss  $\\mathcal{L}_k(n)$  without considering the irreducible loss. This objective could lead to the underrepresentation of domains with inherently lower entropy such as code or math<sup>3</sup>.\n\n\n# 3.2 THE CONTRIBUTION OF THE DATA FROM EACH DOMAIN\n\n\nIn contrast to prior works that train multiple models to find optimal data policies offline (Ye et al., 2024; Ge et al., 2024; Liu et al., 2024), our online approach adaptively prioritizes selecting data so that the model learns quickly, as predicted by scaling laws that we fit on the fly. It consists of a data policy  $\\pi(t) \\in \\Delta^K$  that specifies a sampling distribution over the  $K$  domains. An intuitive approach would be to prioritize sampling domains where the model will learn quickly: e.g., we could sample from domain  $k$  in proportion to  $-\\frac{d\\widehat{\\mathcal{L}}_k}{dn}$  (Equation 1).\n\n\nHowever, independent domain scaling laws do not account for how samples from one domain help learning on a different domain. Consider a thought experiment where the loss  $\\mathcal{L}_k$  is decreasing rapidly, but we have sampled very little data from domain  $k$ . Intuitively, data from domain  $k$  should not get much credit for this loss decrease, which can instead be attributed to transfer learning from other domains. This calls for some form of credit assignment based on whether a domain has actually been sampled recently.\n\n\nDefinition 2. A credit assignment score,  $\\lambda_{k}(t)$ , is a real positive-valued function that indicates how much data from the  $k^{th}$  domain contributed to recent changes in the loss  $\\mathcal{L}_k$ .\n\n\nIdeally, we would like to understand how each domain affects every other domain. Unfortunately, this interaction might be highly complex and not local (i.e., the best solution locally at a given time\n\n\n```\n1: Input: prior  $\\mu \\in \\Delta^K$ , update interval  $t_{\\mathrm{update}}$ , warmup duration  $t_{\\mathrm{warmup}}$ ,  $\\gamma_1, \\gamma_2, s, \\delta_{\\mathrm{min}}$   \n2: Initialize  $h \\gets \\mu$ ,  $\\bar{\\pi} \\gets \\mu$ ,  $\\{\\mathrm{loss\\_k} \\gets []\\}_{k \\in [K]}$   \n3: Train with  $\\mu$  for  $t_{\\mathrm{warmup}}$  steps to initialize each loss_k  \n4: Initialize domain  $k$ 's scaling law with loss_k, for  $k \\in [K]$   \n5: for  $t = 0 \\to T$  do  \n6:  $\\rho(t) \\gets$  compute the preference distribution according to Equation 3  \n7:  $\\pi(t) \\gets$  clip  $(\\gamma_2 \\rho(t) + (1 - \\gamma_2) \\bar{\\pi}(t - 1), \\delta_{\\mathrm{min}})$   \n8:  $\\{\\ell_k\\}_{k \\in [K]} \\gets$  train the model according to  $\\pi$  and add domain  $k$  loss to loss_k  \n9:  $h(t) \\gets \\gamma_1 \\pi(t) + (1 - \\gamma_1) h(t - 1)$   \n10:  $\\bar{\\pi}(t) \\gets \\frac{1}{t + 1} \\rho(t) + (1 - \\frac{1}{t + 1}) \\bar{\\pi}(t - 1)$   \n11: if  $t \\mod t_{\\mathrm{update}} = 0$  then  \n12: Update domain  $k$  scaling law with loss_k, for  $k \\in [K]$\n```\n\n\nstep may not be optimal in the long run). As mentioned earlier, we focus on modeling the domain's contribution to itself for now. A reasonable assumption is that the contribution a domain makes to its own learning progress is positively correlated with what proportion of the recent data came from this domain. In other words, we assume that if a domain was useful for itself in recent history, it will likely continue to be useful. Based on this assumption, we keep a history,  $h(t)$ , of the exponential moving average (EMA) of the recent data policy (with a coefficient of  $\\gamma_{1} < 1$ ) and apply a power transformation with a smoothing parameter  $s < 1$  to smooth the distribution since the assignment can be inaccurate:\n\n\n$$\n\n$$\nh _ {k} (t) = \\gamma_ {1} \\pi_ {k} (t) + (1 - \\gamma_ {1}) h _ {k} (t - 1), \\quad \\lambda_ {k} (t) \\propto h _ {k} (t) ^ {s}. \\tag {2}\n$$\n\n$$\n\n\nHere a smaller  $\\gamma_{1}$  puts more weighting on policy history in the EMA, and decreasing  $s$  leads to stronger smoothing of the credit assignment across tasks. We find that  $\\gamma_{1} = 0.1$  and  $s = 0.5$  work well consistently for all scales we tested. We emphasize that this design choice is a simple heuristic that can be easily computed and other choices are possible.\n\n\n# 3.3 CONSTRUCTING THE DATA POLICY UPDATE\n\n\nTo define our data policy, we combine the two principles we have seen thus far: prioritize sampling from a domain  $k$  if the model is decreasing its loss  $\\mathcal{L}_k$  quickly, but only if that decrease can be attributed to data from domain  $k$ . Additionally, it is common to have a natural prior distribution  $\\mu \\in \\Delta^K$  over the domains (e.g., number of tokens in each domain), which we may also incorporate:\n\n\nDefinition 3. Given the learning speed forecast by a scaling law,  $\\frac{\\partial}{\\partial n}\\widehat{\\mathcal{L}}_k(n)$ , credit assignment score  $\\lambda_{k}(t)$ , and a prior domain weight  $\\mu_{k}$ , the preference distribution is\n\n\n$$\n\n$$\n\\rho_ {k} (t) \\propto - \\mu_ {k} \\frac {\\partial}{\\partial n} \\widehat {\\mathcal {L}} _ {k} (n) \\lambda_ {k} (t) = \\frac {1}{n} \\mu_ {k} \\lambda_ {k} (t) \\alpha_ {k} (\\widehat {\\mathcal {L}} _ {k} (n) - \\varepsilon_ {k}). \\tag {3}\n$$\n\n$$\n\n\nNote that the preference distribution is not a perfect indication of the best possible distribution to learn from because we have no access to the true gradient of the data policy. As shown in the definition of the credit assignment score, we have to resort to various approximations.\n\n\nTemporal average. This local estimate of a data policy may not be optimal globally. Both DoReMi (Xie et al., 2024) and DoGE (Fan et al., 2023) found superior performance by averaging the proxy model's training distribution over all time steps, to obtain a stationary distribution for the actual training run. Since we need to select a data policy online, we instead take inspiration from Defazio et al. (2024) and use an online analogy to post-hoc averaging:\n\n\n$$\n\n$$\n\\bar {\\pi} _ {k} (t) = \\frac {1}{t + 1} \\rho_ {k} (t) + \\left(1 - \\frac {1}{t + 1}\\right) \\bar {\\pi} _ {k} (t - 1), \\tag {4}\n$$\n\n$$\n\n\n$$\n\n$$\n\\pi_ {k} (t) = \\gamma_ {2} \\rho_ {k} (t) + (1 - \\gamma_ {2}) \\bar {\\pi} _ {k} (t - 1). \\tag {5}\n$$\n\n$$\n\n\nHere  $\\bar{\\pi}_k(t)$  is a temporal moving average of the preference policy at every step and  $\\pi_k(t)$  is a linear combination of the moving average and the chosen policy according to which the data are sampled\n\n\nTable 1: Zero-shot performance of different methods across different downstream evaluations. Highlighted cells indicate top-performing methods (deeper color indicates better performance).\n\n\n<table><tr><td></td><td>HellaSwag</td><td>WinoGrande</td><td>PIQA</td><td>ARC-E</td><td>SciQ</td><td>LogiQA2</td><td>LAMBADA</td><td>Average</td></tr><tr><td>124M-Pile</td><td>0.279</td><td>0.515</td><td>0.615</td><td>0.435</td><td>0.747</td><td>0.228</td><td>0.357</td><td>0.454</td></tr><tr><td>124M-DoReMi</td><td>0.279</td><td>0.520</td><td>0.609</td><td>0.429</td><td>0.741</td><td>0.237</td><td>0.368</td><td>0.455</td></tr><tr><td>124M-ODM</td><td>0.285</td><td>0.514</td><td>0.603</td><td>0.453</td><td>0.764</td><td>0.230</td><td>0.374</td><td>0.461</td></tr><tr><td>124M-Balanced</td><td>0.280</td><td>0.511</td><td>0.610</td><td>0.447</td><td>0.762</td><td>0.227</td><td>0.362</td><td>0.457</td></tr><tr><td>124M-Natural</td><td>0.290</td><td>0.503</td><td>0.624</td><td>0.435</td><td>0.755</td><td>0.234</td><td>0.401</td><td>0.463</td></tr><tr><td>124M-ADO</td><td>0.290</td><td>0.520</td><td>0.635</td><td>0.456</td><td>0.771</td><td>0.244</td><td>0.371</td><td>0.470</td></tr><tr><td>1.3B-DoReMi</td><td>0.416</td><td>0.582</td><td>0.707</td><td>0.609</td><td>0.870</td><td>0.228</td><td>0.615</td><td>0.575</td></tr><tr><td>1.3B-Balanced</td><td>0.382</td><td>0.546</td><td>0.689</td><td>0.580</td><td>0.862</td><td>0.225</td><td>0.613</td><td>0.557</td></tr><tr><td>1.3B-Natural</td><td>0.424</td><td>0.584</td><td>0.718</td><td>0.627</td><td>0.886</td><td>0.232</td><td>0.624</td><td>0.585</td></tr><tr><td>1.3B-ADO</td><td>0.442</td><td>0.590</td><td>0.730</td><td>0.625</td><td>0.875</td><td>0.228</td><td>0.638</td><td>0.590</td></tr></table>\n\n\nfrom, which combines both the current preference distribution and the time-averaged distribution. Using the coefficient of  $\\frac{1}{t}$  ensures that all the preference distributions from all time steps contribute equally to the final average, similar to the effect of post-hoc averaging. Intuitively, a larger value of  $\\gamma_{2}$  makes the algorithm more responsive to the current loss dynamics, while a smaller value makes it smoother and less responsive. We find that  $\\gamma_{2} = 0.1$  works well consistently. Additionally, we enforce the smallest domain probability to be no smaller than  $\\delta_{\\mathrm{min}} = 0.01$  to avoid domains being unsampled (Appendix C). Algorithm 1 shows the pseudocode for the entire algorithm.\n\n\n# 4 EXPERIMENTS\n\n\nEvaluation. We conduct all our experiments on the Pile dataset (Gao et al., 2020) with decoder-only transformer language models (Vaswani et al., 2017) of varying sizes. We consider two types of metrics: 1. validation loss on the Pile, an unweighted version of the pile validation set where each domain receives equal probability, SlimPajama (Soboleva et al., 2023), and a 1 billion token subset of FineWeb (Penedo et al., 2024), 2. zero-shot downstream performance on 6 common-sense reasoning domains from the language model evaluation harness (Gao et al., 2024): HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019), PIQA (Bisk et al., 2019), ARC-E (Clark et al., 2018), SciQ (Welbl et al., 2017), LogiQA2 (Liu et al., 2023) and LAMBADA (Paperno et al., 2016).\n\n\nMethods. We conduct experiments at 2 computation scales: 124 million parameters (124M) and 1.3 billion parameters (1 . 3B). For 124M models, we use a batch size of 256 with a context length of 1024 trained for 60,000 steps (approximately 15 billion tokens), and for 1 . 3B, we use a batch size of 2048 with a context length of 1024 trained for 60000 steps (approximately 125 billion tokens). Both scales use the natural distribution for  $\\mu$  and the same hyperparameters for ADO. Training details can be found in Appendix A.\n\n\nComparisons. For comparison, we use the original Pile weights (Gao et al., 2020, Pile) and DoReMi weights (Xie et al., 2024, DoReMi) $^{4}$  as the baselines. We also consider ODM (Albalak et al., 2023), another online method. In addition, we introduce a simple baseline that is surprisingly missing from the literature: weighing each domain by the number of tokens in it. This mixture naturally takes into account the tokenizer being used and training on this policy essentially corresponds to training on the natural distribution of the dataset (i.e., corresponds to empirical risk minimization where train, validation, and test sets all follow the same distribution defined by the tokenizer). We estimate the tokens per domain by randomly sampling 1000 documents from each domain to estimate each domain's average tokens per document, and then multiply it by the total number of documents (Appendix A). We will refer to this policy as Natural. We also use a second baseline, Balanced, which considers the unweighted mixture of all the domains.\n\n\nObservations. The results for downstream performance are shown in Table 3. We will highlight some key observations here. Observation 1: at  $124\\mathrm{M}$  scale, ADO achieves the highest average accuracy; it also outperforms all the baselines on 6 out of 7 downstream tasks and is competitive on\n\n\n![Figure 4: Perplexity of 1.3B models on different domains, trained on the Pile using either DoReMi, Natural, or ADO. On the left side of the red line, we have the validation perplexity on the Pile validation set, the unweighted Pile validation set, the SlimPajama validation set, and a random subset of FineWeb. On the right side of the red line, we show the validation perplexity of each domain of the Pile.](images/7d3b145e813659831860c0bba6a2dc7d26bcb9691f2f5bd1e1080370093e56e6.jpg)\n\n\nLAMBADA. At 1.3B scale, ADO also achieves the highest average accuracy and outperforms the baselines on 4 out of 7 downstream tasks. Observation 2: interestingly, Natural turns out to be a very competitive baseline. It achieves the second-best average downstream performance at both scales and performs well on individual domains, too. To the best of our knowledge, very few works on data selection have benchmarked against using the empirical distribution of tokens given by the dataset and tokenizer. As such, we recommend future works on data selection to compare against this simple baseline. Balanced also turns out to be a quite competitive baseline at 124M scale likely because smaller domains are not extensively repeated given the low total number of tokens processed at this scale. This observation is in line with the findings of Goyal et al. (2024) which suggest that good data policies are different for different compute scales.\n\n\nConventionally, validation loss is considered a good indicator of the models' performance if they are trained on the same data, but it is less clear whether it remains a good indicator when the data policy is dynamically changing throughout the training. Empirically (Figure 4), we found that it is difficult to outperform Natural on the Pile validation loss (at least within the same training setup and without spending much more computation), even though we do outperform this mixture on the downstream tasks as shown above. Observation 3: for validation loss, ADO slightly underperforms Natural on the Pile validation set, but on the validation set of SlimPajama and a subset of FineWeb, ADO outperforms Natural. Since SlimPajama and FineWeb are heavily filtered to be \"higher quality\" data compared to the Pile, we speculate that ADO may be able to implicitly select data aligned with some notion of high quality throughout the training process (e.g., by prioritizing learnability). Nonetheless, since it can be challenging to quantify or precisely define \"high quality\" data, we will leave this investigation to future work. We include more analysis in Appendix D.1.\n\n\n# 4.1 ANALYSIS\n\n\nMixture over the training. A natural question about data selection methods is what does the distribution look like. We now show that the curricula learned by ADO differ for models of different scales in Figure 5. We observe that CommonCrawl (File-CC) receives the most probability mass in both 124M and 1.3B. This observation is consistent with some previous works that put most of the probability mass on CommonCrawl (Xie et al., 2024). OpenText2 also receives a higher proportion compared to the empirical mixture. Github receives a higher probability in 124M initially but eventually decays whereas in 1.3B it first decreases rapidly and gradually increases\n\n\n![](images/76cba97914be8647da4b3061246500ea27c1360fe8415b5718079811e21a9c5e.jpg)\n\n\n![](images/73924ae91cd1a32fd8918e283d5e8ba80802561b08eda33f2f8d83b06c3c33d0.jpg)\n\n\n![Figure 5: The sampling distribution produced by our data policy during training on The Pile. For ease of visualization, we only highlight the top 6 largest domains. ADO produces qualitatively different strategies at different model scales and adaptively changes its weightings over time.](images/65bbf58386697c109a20e08de3bf087dc52fd269fa03a77bb3091b29125c9df6.jpg)\n\n\n![Figure 6: The forecasts from our scaling laws (fit online throughout training) versus the actual training loss on different domains in the Pile. The forecasts shown were fit at 10,000, 30,000, and 50,000 steps (time of fit is shown in dashed lines). We observe that the scaling laws: (1) become more accurate over the course of training, and (2) can be surprisingly accurate at forecasting the final loss even very early into training on some domains, such as Pile-CC. Forecasts from all domains are shown in the Appendix, Figure 8.](images/c79642bea79e475d4c4b0f14a8633c2c9c9c3e23e7fa2cab13ff5bbccc618ef1.jpg)\n\n\ntowards the end of the training. This is consistent with the intuition that code data naturally have much lower entropy so it is \"easier\" to learn in some sense. ADO does not account for the fact that code could be desirable for general capabilities such as reasoning (Ma et al., 2024) since it is agnostic to the downstream tasks. This issue can be partially resolved by assigning a higher prior to code but further research on data selection targeting reasoning is likely needed.\n\n\nAccuracy of the scaling law throughout the training. Unlike most applications of scaling laws, we refit the scaling laws on the fly as the models train and the data mixtures are dynamically changing. This introduces new challenges because the forecasts from scaling laws fit early on can be inaccurate for steps far in the future. In Figure 6, we show the predictions of the scaling laws for 3 domains at various points during training (all domains are shown in the Appendix, Figure 8). It can be seen that while the predictions of our scaling laws eventually deviate from the true learning curves, they are accurate locally for much of the training and thus can act as a learning signal for the data policy. More specifically, we can see that the scaling laws consistently overestimate the final loss for all domains, likely due to the presence of learning rate schedules. Empirically, when the learning rate decays towards the end of the training, the training loss tends to decrease more rapidly, until a certain point when the learning rate is too small (H\u00e4gele et al., 2024). We believe that incorporating recent techniques for incorporating the effect of learning rate schedules into the scaling laws (Tissue et al., 2024) could further improve ADO.\n\n\n# 5 RELATED WORKS\n\n\nData curation and selection. For current large language models, compute often poses a greater constraint than data availability, making data selection crucial (Albalak et al., 2024). A widely used approach is data filtering (Soboleva et al., 2023; Penedo et al., 2023; 2024), where undesirable data points are removed based on heuristics like perplexity, repetition (Lee et al., 2021), or semantic similarity (Abbas et al., 2023). This filtering process is foundational for constructing most large-scale datasets today. After filtering, data is often categorized into subsets or domains (e.g., code, books), and deciding how much data to use from each domain becomes an important step.\n\n\nTwo primary strategies for data selection are prevalent: one focuses on deciding whether individual data points should be included based on various criteria (Mindermann et al., 2022; Engstrom et al., 2024), and the other uses all available data but samples from different domains with varying probabilities (Xie et al., 2024; Fan et al., 2023; Albalak et al., 2023). While data selection aims to enhance training efficiency, these methods may introduce considerable computational overhead (Xie et al., 2024; Chen et al., 2024; Fan et al., 2023). Moreover, Kaddour et al. (2023) show that under the same computational budget, these methods often fail to surpass standard training, and Wang et al. (2024) proves that the data selection's efficacy depends on the user's utility function.\n\n\nAnother line of work focuses on selecting pre-training data that aligns more closely with downstream tasks (Kang et al., 2024). Data selection has also been explored for computer vision. For example, Evans et al. (2023) use a small reference model to select data for CLIP, while others propose pruning batches based on diversity criteria (Qin et al., 2024; Hong et al., 2024) to improve training efficiency.\n\n\nNeural scaling laws. Studies have found that various quantities of interest for large pretrained models (e.g., validation loss) can be reliably predicted with simple statistics such as the model size, dataset size, or the amount of computation (Kaplan et al., 2020; Hoffmann et al., 2022). These findings have been central to the design of training protocols for large language models where trial and error are expensive. More recent works have studied the relationship between data repetition and the performance of the models (Hernandez et al., 2022; Muennighoff et al., 2024; Goyal et al., 2024). Data curation, in particular pruning, has also been shown to have significant effects to achieve better scaling laws (Sorscher et al., 2022). Studying the loss curve via scaling law (Hutter, 2021) is relatively less well-explored because the loss curves do not follow a power law exactly due to the learning rate schedule; however, we found that a power law is still a decent model for learning curves with cosine decay for language models. Recently, Tissue et al. (2024) showed that learning rate schedules can be incorporated into scaling laws to make even more accurate predictions, though we do not explore this direction in this work.\n\n\n# 6 CONCLUSION\n\n\nDynamic data selection has the potential to improve the pretraining efficiency of foundation models, but most existing approaches incur additional computation costs. We introduce ADO, a cheap online data selection method that dynamically adjusts data distribution over different domains based on domain scaling laws. The scaling laws forecast the model's loss on different data domains and automatically adjust the training data distribution based on each domain's learning potential. ADO does not require a proxy model so it naturally takes into account the architecture, the optimizer, and other hyperparameters of the training. In our experiments, with a single set of hyperparameters, ADO performs comparably to or better than existing methods on scales from 124M to 1.3B, with much less additional computation. Our implementation only added 20 minutes of additional wall-clock time to a 3.5-day training run ( $\\sim 0.4\\%$ ), and can be optimized further. The relative cost would decrease more at larger scales since ADO's cost does not scale with the model size. Overall, we believe ADO represents an important step towards accessible and automated online data selection.\n\n\nLimitations and future directions. Given further computational resources, it would be interesting to scale ADO up to larger models (such as 8B models in DoReMi (Xie et al., 2024)) and datasets and study its behaviors more thoroughly. On the technical side, we believe there are several promising directions: 1. how to create better and more fine-grained domains, 2. how to design more realistic scaling laws that can model inter-domain interactions and learning rate schedules, 3. whether insights of ADO can be applied to other training settings such as continued pretraining or fine-tuning, and 4. why do these domain scaling laws arise from naturally occurring data distributions.\n\n\n# REFERENCES\n\n- Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic dedduplication. arXiv preprint arXiv:2303.09540, 2023.\n\n- Ryan Prescott Adams and Richard S Zemel. Ranking via sinkhorn propagation. arXiv preprint arXiv:1106.1925, 2011.\n\n- Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for language model pre-training. In *R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models*, 2023.\n\n- Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024.\n\n- Robert A Baker and Stanley W Osgood. Discrimination transfer along a pitch continuum. Journal of Experimental Psychology, 48(4):241, 1954.\n\n- Robert Baldock, Hartmut Maennel, and Behnam Neyshabur. Deep learning through the lens of example difficulty. Advances in Neural Information Processing Systems, 34:10876-10889, 2021.\n\n- Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41-48, 2009.\n\n- Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. PMLR, 2023.\n\n- Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence, 2019. URL https://api-semanticscholar.org/CorpusID:208290939.\n\n- Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022.\n\n- Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258.\n\n- James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n\n- Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher R\u00e9. Skill-it! a data-driven skills framework for understanding and training language models. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. URL https://api_semanticscholar.org/ CorpusID:3922816.\n\n- DeepMind, Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci, Jonathan Godwin, Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza Merzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Laurent Sartran, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, Milo\u0161 Stanojevic, Wojciech Stokowiec, Luyu Wang, Guangyao Zhou, and Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL http://github.com/google-deepmind.\n\n- Aaron Defazio, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, Ashok Cutkosky, et al. The road less scheduled. arXiv preprint arXiv:2405.15682, 2024.\n\n- Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels. arXiv preprint arXiv:2401.12926, 2024.\n\n- Talfan Evans, Shreya Pathak, Hamza Merzic, Jonathan Schwarz, Ryutaro Tanno, and Olivier J Henaff. Bad students make great teachers: Active learning accelerates large-scale visual understanding. arXiv preprint arXiv:2312.05328, 2023.\n\n- Simin Fan, Matteo Pagliardini, and Martin Jaggi. Doge: Domain reweighting with generalization estimation. ArXiv, abs/2310.15393, 2023. URL https://apisemantic scholar.org/ CorpusID:264439382.\n\n- Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and P. Abbeel. Reverse curriculum generation for reinforcement learning. ArXiv, abs/1707.05300, 2017. URL https://api-semanticscholar.org/CorpusID:19181872.\n\n- Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\n- Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602.\n\n- Ce Ge, Zhijian Ma, Daoyuan Chen, Yaliang Li, and Bolin Ding. Data mixing made efficient: A bivariate scaling law for language model pretraining. arXiv preprint arXiv:2405.14908, 2024.\n\n- Charles Albert Eric Goodhart. Monetary control\u2014the British experience. In Monetary Conditions for Economic Recovery, pp. 59-84. Springer, 1985.\n\n- Sachin Goyal, Pratyush Maini, Zachary C Lipton, Aditi Raghunathan, and J Zico Kolter. Scaling laws for data filtering-data curation cannot be compute agnostic. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22702-22711, 2024.\n\n- Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. In international conference on machine learning, pp. 1311-1320. Pmlr, 2017.\n\n- Yuxian Gu, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, and Furu Wei. Towards optimal learning of language models. arXiv preprint arXiv:2402.17759, 2024.\n\n- Alexander Hagele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro Von Werra, and Martin Jaggi. Scaling laws and compute-optimal training beyond fixed training durations. arXiv preprint arXiv:2405.18392, 2024.\n\n- Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487, 2022.\n\n- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL https://api-semanticscholar.org/CorpusID:247778764.\n\n- Feng Hong, Yueming Lyu, Jiangchao Yao, Ya Zhang, Ivor W Tsang, and Yanfeng Wang. Diversified batch selection for training acceleration. arXiv preprint arXiv:2406.04872, 2024.\n\n- Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021.\n\n- Andrew Jesson, Nicolas Guizard, Sina Hamidi Ghalehjegh, Damien Goblot, Florian Soudan, and Nicolas Chapados. CASED: curriculum adaptive sampling for extreme data imbalance. CoRR, abs/1807.10819, 2018. URL http://arxiv.org/abs/1807.10819.\n\n- Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt J. Kusner. No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models, July 2023. URL http://arxiv.org/abs/2307.06440. arXiv:2307.06440 [cs].\n\n- Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in neural information processing systems, 32, 2019.\n\n- Feiyang Kang, Hoang Anh Just, Yifan Sun, Himanshu Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit Kumar Sahu, and Ruoxi Jia. Get more for less: Principled data selection for warming up fine-tuning in llms. In ICLR, 2024. URL https://openreview.net/forum?id=QmYNBVukex.\n\n- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\n- Patrick Kidger and Cristian Garcia. Equinox: neural networks in JAX via callable PyTrees and filtered transformations. Differentiable Programming workshop at Neural Information Processing Systems 2021, 2021.\n\n- Douglas H Lawrence. The transfer of a discrimination along a continuum. Journal of Comparative and Physiological Psychology, 45(6):511, 1952.\n\n- Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021.\n\n- Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue Zhang. Logiqa 2.0\u2014an improved dataset for logical reasoning in natural language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:2947-2962, 2023. URL https://api-semanticscholar.org/CorpusID:259515154.\n\n- Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024.\n\n- I Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\n- Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. At which training stage does code data help LLMs reasoning? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=KIPJKST4gw.\n\n- Stefano Sarao Mannelli, Yaraslau Ivashinka, Andrew Saxe, and Luca Saglietti. Tilting the odds at the lottery: the interplay of overparameterisation and curricula in neural networks. arXiv preprint arXiv:2406.01589, 2024.\n\n- Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning latent permutations with gumbel-sinkhorn networks. arXiv preprint arXiv:1802.08665, 2018.\n\n- S\u00f6ren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H\u00f6ltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In International Conference on Machine Learning, pp. 15630-15649. PMLR, 2022.\n\n- Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and R. Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. ArXiv, abs/1606.06031, 2016. URL https://api-semanticscholar.org/CorpusID:2381275.\n\n- Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refined web dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\n\n- Guilherme Penedo, Hynek Kydl\u00ed\u010dek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fine web datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557.\n\n- Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, xu Zhao Pan, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, and Yang You. Infobatch: Lossless training speed up by unbiased dynamic data pruning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=C61sk5LsK6.\n\n- Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. An adversarial winograd schema challenge at scale. 2019. URL https://apisemantic scholar.org/ CorpusID:199370376.\n\n- Or Sharir, Barak Peleg, and Yoav Shoham. The cost of training NLP models: A concise overview. CoRR, abs/2004.08900, 2020. URL https://arxiv.org/abs/2004.08900.\n\n- Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\n\n- Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. The annals of mathematical statistics, 35(2):876-879, 1964.\n\n- Burrhus F Skinner. Reinforcement today. American Psychologist, 13(3):94, 1958.\n\n- Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B.\n\n- Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523-19536, 2022.\n\n- Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. corr abs/2104.09864 (2021). arXiv preprint arXiv:2104.09864, 2021.\n\n- Yuxing Tang, Xiaosong Wang, Adam P. Harrison, Le Lu, Jing Xiao, and Ronald M. Summers. Attention-guided curriculum learning for weakly supervised classification and localization of thoracic diseases on chest radiographs. ArXiv, abs/1807.07532, 2018. URL https://api.sementicscholar.org/CorpusID:49882848.\n\n- Howe Tissue, Venus Wang, and Lu Wang. Scaling law with learning rate annealing, 2024. URL https://arxiv.org/abs/2408.11029.\n\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Jasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\n- Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. URL https://apisemantic scholar.org/CorpusID:13756489.\n\n- Jiachen T Wang, Tianji Yang, James Zou, Yongchan Kwon, and Ruoxi Jia. Rethinking data shapley for data selection tasks: Misleads and merits. arXiv preprint arXiv:2405.03875, 2024.\n\n- Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. ArXiv, abs/1707.06209, 2017. URL https://api_semanticscholar.org/CorpusID:1553193.\n\n- Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. When do curricula work? In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=tW4QEInpni.\n\n- Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared LLaMA: Accelerating language model pre-training via structured pruning. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=09iOdaeOzp.\n\n- Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. LESS: Selecting influential data for targeted instruction tuning. In *Forty-first International Conference on Machine Learning*, 2024b. URL https://openreview.net/forum?id=PG5fvV50maR.\n\n- Sang Michael Xie and Stefano Ermon. Reparameterizable subset sampling via continuous relaxations, 2021. URL https://arxiv.org/abs/1901.10517.\n\n- Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. arXiv preprint arXiv:2403.16952, 2024.\n\n- Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Annual Meeting of the Association for Computational Linguistics, 2019. URL https://api-semanticscholar.org/CorpusID:159041722.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638423.2508745", "msecs": "250.0", "relativeCreated": "20081.372022628784", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations\n\n# Abstract\nReconstructing visual images from fMRI data presents a challenging task, particularly when dealing with limited data and compute availability. This work introduces a novel approach to fMRI-based visual image reconstruction using a subject-agnostic common representation space. We show that subjects' brain signals naturally align in this common space during training, without the need for explicit alignment. This is leveraged to demonstrate that aligning subject-specific adapters to a reference subject is significantly more efficient than traditional end-to-end training methods. Our approach excels in low-data scenarios, where training the adapter with limited data achieves faster and better performance. We also introduce a novel method to select the most representative subset of images for a new subject, allowing for fine-tuning with 40\\% less data while maintaining performance. These advancements make fMRI data collection more efficient and practical, reducing the burden on subjects and improving the generalization of fMRI reconstruction models.\n\n\n# 1 INTRODUCTION\n\n\nOver the past several decades, the use of machine learning techniques has enabled decoding and/or reconstruction of information represented in neural activation patterns measured in awake, behaving humans. Because the neural representations supporting behavior likely entail patterns of activity across large swaths of tissue, these approaches are thought to offer a key step towards characterizing and quantifying the computational principles brains use to support cognition (Naselaris et al., 2011).\n\n\nEarly approaches, including self-supervised learning frameworks (Beliy et al., 2019; Gaziv et al., 2022; Mozafari et al., 2020; Shen et al., 2019; Seeliger et al., 2018; Ren et al., 2021) struggled to generate semantically accurate image reconstructions, often resulting in images with limited resemblance to the original stimuli. Furthermore, these methods required large amounts of data for training, and their generalization to a wider population of subjects was severely limited. Significant advancements were achieved by incorporating multimodal data, such as text (Lin et al., 2022), and later by leveraging the generative power of latent diffusion models (LDMs) (Takagi & Nishimoto, 2023; Scotti et al., 2023; Lu et al., 2023; Xia et al., 2024). These models introduced novel mapping techniques to project fMRI signals into the input space of LDMs, yielding higher-quality natural images. However, despite the improved fidelity, the issue of generalization to new participants persisted. In response, Scotti et al. (2024) proposed projecting fMRI data into a shared representation space, demonstrating significant improvements in both performance and generalizability.\n\n\nBeyond being able to use decoding and reconstruction approaches to provide evidence of the existence of information represented in a particular brain activation pattern, an additional goal is to compare decoding or reconstruction performance across experimental manipulations, such as cued visual attention (Serences & Boynton, 2007; Kamitani & Tong, 2005; Scolari et al., 2012; Sprague & Serences, 2013; Itthipuripat et al., 2019; Sprague et al., 2018) and visual working memory (Serences et al., 2009; Harrison & Tong, 2009; Ester et al., 2013; Sprague et al., 2014; Christophel et al., 2012; Li et al., 2021) However, for these approaches to be useful for a cognitive neuroscience lab, the lab would need to acquire sufficient data to train a decoding or reconstruction model for each individual participant (often  $>20$  per study to attain statistical power). The state-of-the-art methods described above use extremely massive datasets with dozens of hours of fMRI data per participant, which is not feasible in almost all cases. Thus, there is a clear need to maximize the efficiency and generalizability\n\n\nof image reconstruction methods such that a model for a new participant can be trained using as little data as possible.\n\n\nIn this work, we introduce an efficient and generalizable approach to fMRI-based visual reconstruction that leverages a subject-agnostic common representation space. Unlike previous approaches that perform end-to-end training of fMRI signals to mapped contextual embeddings, we propose aligning brain activity from new subjects to a pre-trained reference subject in a shared representation space using Adapter Alignment (AA) training. Throughout this work, we refer to lightweight subject-specific modules designed to map input fMRI signals to a shared representation space as \"adapters\"(Liu et al., 2024). These modules are implemented as single-layer neural networks. In their linear form, adapters consist of a single linear transformation layer, while in the non-linear form, they include a linear layer followed by a non-linear activation function such as GELU (Hendrycks & Gimpel, 2023). Using AA training, we significantly reduce the amount of fMRI data required to achieve accurate reconstructions and improve the reconstruction quality. Our method achieves better performance to existing state-of-the-art models, but with far fewer training hours, making it feasible for cognitive neuroscience labs with limited data collection resources. We build on this approach by also proposing an adaptive image selection strategy, allowing for optimal fine-tuning of new subjects with minimal data, further improving the practicality and accessibility of fMRI-based image reconstruction.\n\n\nOur contributions to this work are threefold:\n\n- - We provide strong evidence for the existence of a common visual representation space across subjects, where brain activity from different individuals can be aligned in a shared space. By extracting and analyzing embeddings from a pre-trained model, we show that even without explicit alignment, subject-specific fMRI signals, when mapped to a common space display similar behavior and representation structures. Furthermore, we demonstrate that applying a simple orthogonal transformation improves the alignment of embeddings between subjects, and using non-linear adapters with explicit mapping leads to almost perfect alignment across subjects in the common space. (Section 2)\n\n- - We introduce Adapter Alignment (AA), a novel approach to facilitate efficient subject-specific alignment in fMRI-based visual reconstruction. Our training pipeline first pre-trains on a reference subject, then aligns new subjects' brain data using lightweight adapters that map them to the shared common space. This process, which is more efficient than traditional end-to-end training, enables faster convergence and better performance, particularly in low-data scenarios. By incorporating MSE losses at the adapter level for common images, we show that our method not only reduces training time but also improves generalization and reconstruction quality. (Section 3)\n\n- - We develop a greedy image selection algorithm that identifies the most representative subset of images for training new subjects. By efficiently selecting fMRI signals that capture the most informative aspects of the common space, this strategy significantly reduces the amount of data required for fine-tuning without sacrificing performance (from a baseline of 250 images to 150 images). This makes our method more practical for real-world applications, where acquiring large fMRI datasets is often prohibitively expensive. (Section 4)\n\n\n# 2 A COMMON VISUAL REPRESENTATION SPACE\n\n\nCentral to our approach is the hypothesis that each individual's brain (or at least the tissue involved in processing static visual images) can be considered an instantiation of a 'common' brain, with activity patterns varying along similar dimension in a common shared representational space. Aspects of this hypothesis are already strongly supported: most human brains have localized tissue responsive to specific types of stimuli, like faces and scenes. Additionally, several continuous dimensions of high-level visual selectivity have been identified across the ventral temporal cortex (Huth et al., 2012), including dimensions like animate/inanimate, large/small (based on real-world size) (Long et al., 2018), and spiky/round (Bao et al., 2020). Retinotopic organization is another example of a shared representational space, as each individual brain instantiates a unique anatomical mapping from retinal coordinates to cortical representations, and adaptive models can be estimated to transform activation patterns from individual brains to this common shared retinotopic coordinate system (Sprague & Serences, 2013; Wandell & Winawer, 2015). We analyze the similarity of representation spaces\n\n\n<table><tr><td>Dimension 1</td><td>Animals (giraffes, zebras, and elephants), nature, and outdoor scenes</td></tr><tr><td>Dimension 2</td><td>Cats and food</td></tr><tr><td>Dimension 3</td><td>Kite flying, airplanes, and flying objects</td></tr><tr><td>Dimension 4</td><td>Surfing, water sports, beach, and ocean scenes</td></tr><tr><td>Dimension 5</td><td>Airplanes, flying objects, outdoor activities and events, transportation scenes</td></tr></table>\n\n\nTable 1: Concept analysis of images along principal dimensions.\n\n\nfrom different perspectives of common concepts, visual analysis, and eigenvectors in the following subsections. Finally, we reflect on the different ways of aligning the common representations.\n\n\n# 2.1 EMERGENCE OF COMMON CONCEPTS\n\n\nThe existence of a semantic common representation space is suggested from the emergence of semantic categories in this space. For this, we extracted the most significant left singular vectors from the ridge regression weight matrix. This matrix was extracted from the pre-trained MindEye2 (Scotti et al., 2024) model which was trained using the 1024-dimensional common space. Subsequently, we projected the 1024-dimensional embeddings of each image onto these vectors and sampled from the two extremes (set 1 and set 3) and the middle (set 2). In order to obtain an objective classification, we presented these sets of images to GPT-4o (Achiam et al., 2023) and asked it to find the semantic categories that are well represented in set 1, somewhat represented in set 2 and not represented in set 3. Results for the first five singular vectors are shown in Table 1. As is evident, images in the principal dimensions of the representation space have a common theme.\n\n\nPrevious studies, such as those by Huth et al. (2012), have indicated that animal categories, including humans, are represented distinctly from non-animal categories within the brain. In our study, we observed that the common representational space also preserves these semantic properties. As demonstrated in Figure 1, after projecting the fMRI signals of Subject 2 into the common space and onto the first two singular vectors, we can see the animal vs. non-animal distinction, consistent with previous hypotheses. Extending this analysis across additional dimensions, we observed similar separations between other hypothesized semantic categories of \"Social interaction,\" \"man-made vs nature,\" and \"biological vs non-biological.\"\n\n\n![Figure 1: Visualization of theAnimate/Inanimate semantic category on the two principal singular vectors. Each point (embedding) is colored based on whether its corresponding image contains 'objects' that fall into theAnimate category (red), Inanimate category (blue) or both categories (green).](images/aceb003e4bbbb907ac8ac8753fba27d87d056a52418260fb80892ff654a4dcee.jpg)\n\n\nWe also analyze the images in the representation space visually. We present our results and analysis in the Appendix Section D.\n\n\n# 2.2 SIMILARITY IN REPRESENTATION STRUCTURE ACROSS SUBJECTS\n\n\nExisting works, such as MindEye2 (Scotti et al., 2024), employ a simple linear adapter network to project subject-specific fMRI signals into a common representation space. A key indication of the existence of such a shared space is the structural similarity of embeddings across subjects. To explore this, we first use a pre-trained MindEye2 model to extract common-space embeddings for the shared images (the test set in MindEye2) across all subjects. Initial cosine similarity between the embeddings from different subjects is low. However, after applying an orthogonal transformation using Procrustes analysis (Sch\u00f6nemann, 1966), the cosine similarity improves, suggesting that the embeddings can be structurally aligned. Despite this improvement, the alignment is not perfect, likely due to the limitations of linear transformations.\n\n\n![Figure 2: Cosine Similarity of shared image embeddings in common space. (a) The alignment is weak before an aligning orthogonal transformation is applied. (b) The alignment improves after an orthogonal transformation. For each row in this heatmap, the embedding spaces of subjects are transformed orthogonally to align with the corresponding row's subject. (c) Near perfect alignment with nonlinear adapters and explicit alignment. We only consider subjects 1,2,5 and 7 in this plot as we worked with 4 subjects only when training our pipeline.](images/fbfa5b3b456d0a58e1f258d0b4a107407b4d58aebc6911a4adf44fa7bf9b5c06.jpg)\n\n\n![](images/2aa41a62aa36f445bd016bce3b6d5b52973989cf17f4f3f61bfeb25b88d404eb.jpg)\n\n\n![](images/5dd64f82c2ee5f4f979c99cd98bfea0f58569d09e09d9e0e2568119890c1fdcf.jpg)\n\n\nAs a preliminary experiment, we pre-trained a single subject model with a non-linear adapter and explicitly mapped subsequent subjects to this reference subject using subject-specific non-linear adapters. This approach results in near-perfect alignment in the common space. Although existing methods do not explicitly align subject-specific representations, we observed significant semantic and structural similarities across subjects' representations. Furthermore, it is possible to map one subject's representations to another's using non-linear adapters. While this alignment in the common space is nearly perfect, it does not directly translate to better reconstruction performance.\n\n\nThis evidence supports the core hypothesis of our work: it is possible to construct an aligned, subject-agnostic common space. In Section 3, we empirically test this hypothesis by mapping a new subject to a reference subject during training and evaluating its reconstruction performance. Additional analysis is provided in Appendix C.\n\n\n# 3 FMRI RECONSTRUCTION\n\n\nBuilding a common brain representation requires constructing an efficient fMRI reconstruction pipeline. Our approach is guided by two primary objectives: 1) achieving competitive reconstruction performance and 2) prioritizing efficiency over marginal performance gains. We hypothesize that it is possible to pre-train a single reference brain and align subsequent subjects to this reference using subject-specific adapters, significantly reducing the computational cost while maintaining high reconstruction quality.\n\n\n# 3.1 DATASET\n\n\nWe use the Natural Scenes Dataset (NSD) (Allen et al., 2022) but apply a different split compared to the traditional setup. Typically, common images across subjects are assigned to the test set, while the\n\n\ntraining set consists of unique images viewed by each subject. However, for effective Common Space Alignment, it is critical to have a one-to-one mapping of images across subjects. To achieve this, we incorporate the common images into the training set and swap an equal amount of training images to the test sets. This means that every subject now has a unique randomly sampled set of images on which they are tested. We present the performance of the original training method and the proposed technique on the new split to ensure a fair comparison across subjects and alignment techniques.\n\n\nOur initial experiments revealed that models performed slightly worse on the unique image test set than on the common image test set, suggesting that the unique image test set presents a more challenging benchmark. Similar to prior work, we focus on subjects 1, 2, 5, and 7, as they have complete 40-hour fMRI datasets available in NSD, enabling better reconstruction performance. These subjects provide robust data for training and evaluation, allowing us to validate our alignment and reconstruction methods comprehensively.\n\n\n# 3.2 ADAPTER ALIGNMENT\n\n\n![Figure 3: Adapter Alignment Procedure. The Reference Subject is trained end-to-end first. The common space embeddings of the shared images from the reference subject are used as ground truth to train the adapters of subsequent subjects.](images/67929ce7549af003d6115c3b4ffe0f12751cbb0bf3a91632f8698599708534a6.jpg)\n\n\nCurrent approaches for training multi-subject pipelines with a common representation space typically follow an end-to-end strategy, where fMRI inputs are processed by the model and the loss is computed at the final layer to minimize the discrepancy between the ground-truth and predicted embeddings. In this setup, the common space emerges organically during training without explicit constraints. However, this approach has two key drawbacks: (1) it requires extended training to form a satisfactory common space and achieve convergence, and (2) the subject-specific embeddings in the common space are not strongly aligned, making it harder for the residual MLP to map multiple subject inputs to a unified output.\n\n\nTo address these challenges, we propose Adapter Alignment (AA) training. In AA, the pipeline is first pre-trained end-to-end on a single reference subject, chosen based on reconstruction performance and data availability. Among NSD subjects, Subject 1 consistently exhibited the best reconstruction performance, making it the ideal candidate for constructing the reference space. After pre-training, the output embeddings for the subject-specific adapter, particularly those corresponding to the shared images across subjects, are extracted.\n\n\nWhen fine-tuning a new subject, we adopt a two-step process. First, we train the new subject's adapter using the shared images, with the objective of aligning its output to the embeddings of Subject 1 for the same images. The adapter is allowed to overfit on these shared images until the loss is minimized. In the second step, we unfreeze the entire pipeline and resume training, applying the usual losses at the final output. Additionally, we introduce an MSE loss at the adapter level specifically for the common images, which allows the adapter to restructure itself to account for subject-specific discrepancies and adapt to the unique images encountered by the new subject. We call this process of overfitting the adapter and letting it fine-tune end to end \"AAMax\".\n\n\nThis training method offers improvements in both high-data and low-data scenarios. When all training data is available, AAMax converges faster while achieving performance similar to traditional end-to-end training. However, in limited data settings, AAMax proves significantly more efficient, yielding better reconstructions and higher accuracy with fewer training samples.\n\n\n# 3.3 RECONSTRUCTION PIPELINE\n\n\n![Figure 4: fMRI Reconstruction Pipeline. The high level pipeline maps data to CLIP ViT-L-14 when using Versatile Diffusion and OpenCLIP ViT/bigG-14 when using Stable Diffusion XL. The Adapter Alignment Procedure is explained in more detail in Section 3.2](images/25b3519a15c4de70a40254ad25e705ba28462f8b8f54c25d5cefb160b7f730fd.jpg)\n\n\nOur reconstruction pipeline follows a similar structure to previous works, dividing the reconstruction process into two components: a high-level semantic pipeline that maps to CLIP embeddings and a low-level visual pipeline that maps to visual autoencoder embeddings. Both pipelines receive the same fMRI data as input but are trained separately. The outputs from both pipelines are then integrated into a pre-trained diffusion model to generate the final reconstruction.\n\n\nFlattened fMRI voxels from the nsdgeneral region are fed into a subject-specific adapter, consisting of a single linear layer with a GELU non-linearity. This step maps the fMRI data into a subject-agnostic shared space. The shared space representation is then processed by a Residual Multi-Layer Perceptron (MLP). For mappings into the CLIP (Radford et al., 2021) embedding space, we further incorporate a pre-trained diffusion prior based on DALL-E 2 (Ramesh et al., 2022) to bridge the modality gap between fMRI data and CLIP embeddings.\n\n\nOur high-level pipeline is based on the architecture used in MindEye1 (Scotti et al., 2023) and MindEye2 (Scotti et al., 2024), where the fMRI data is mapped to CLIP image embeddings of the corresponding source images. The pretraining phase leverages a combination of diffusion prior loss and CLIP loss. During fine-tuning, we add an additional MSE loss for the common images to further refine the mappings. The overall loss function in this stage is:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {n e w}} = \\left\\{ \\begin{array}{l l} \\lambda_ {1} \\mathcal {L} _ {\\mathrm {P r i o r}} + \\lambda_ {2} \\mathcal {L} _ {\\mathrm {C L I P}}, & \\text {f o r n o n - c o m m o n i m a g e s} \\\\ \\lambda_ {1} \\mathcal {L} _ {\\mathrm {P r i o r}} + \\lambda_ {2} \\mathcal {L} _ {\\mathrm {C L I P}} + \\lambda_ {3} \\mathcal {L} _ {\\mathrm {M S E}} (\\mathbf {z} _ {\\mathrm {n e w}}, \\mathbf {z} _ {\\mathrm {r e f}}), & \\text {f o r c o m m o n i m a g e s} \\end{array} \\right.\n$$\n\n$$\n\n\nwhere  $\\mathbf{z}_{\\mathrm{adapter, new}}$  and  $\\mathbf{z}_{\\mathrm{adapter, ref}}$  are the adapter-level embeddings for the new and reference subjects, respectively, on the shared common images. We present a breakdown of the high level pipeline loss functions in Appendix H.\n\n\nThe low-level pipeline also follows a similar structure to the high-level one but maps the fMRI data to latent embeddings derived from a custom autoencoder. This autoencoder is trained on downscaled images from the NSD dataset (64x64x3) and reduces them to a latent space of size 1024. The low-level pipeline learns to map to this latent space, and passing these embeddings through a pre-trained decoder produces blurry reconstructions. The loss function for this stage is:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {n e w}} = \\left\\{ \\begin{array}{l l} \\mathcal {L} _ {\\text {M S E}} (\\mathbf {z} _ {\\text {f M R I , n e w}}, \\mathbf {z} _ {\\text {A E}}), & \\text {f o r n o n - c o m m o n i m a g e s} \\\\ \\mathcal {L} _ {\\text {M S E}} (\\mathbf {z} _ {\\text {f M R I , n e w}}, \\mathbf {z} _ {\\text {A E}}) + \\mathcal {L} _ {\\text {M S E}} (\\mathbf {z} _ {\\text {a d a p t e r , n e w}}, \\mathbf {z} _ {\\text {a d a p t e r , r e f}}), & \\text {f o r c o m m o n i m a g e s} \\end{array} \\right.\n$$\n\n$$\n\n\nwhere  $\\mathbf{z}_{\\mathrm{fMRI, new}}$  is the output embedding for the new subject,  $\\mathbf{z}_{\\mathrm{AE}}$  is the AE latent space representation of the image and  $\\mathbf{z}_{\\mathrm{adapter, new}}$  and  $\\mathbf{z}_{\\mathrm{adapter, ref}}$  are the adapter-level embeddings for the new and reference subjects, respectively. We present a breakdown of the low level pipeline loss functions in Appendix I.\n\n\nFor the final step, the mapped CLIP embedding is used as a conditioning input to the pre-trained diffusion prior model. The low-level blurry reconstructions serve as an initialization for the diffusion model, and the final, refined reconstructions are then produced. For more architectural details, we refer readers to the MindEye1 (Scotti et al., 2023) and MindEye2 (Scotti et al., 2024)papers.\n\n\n# 3.4 TESTS ON COMPLETE DATA\n\n\nWe first validate our training method using the full dataset. Subject 1 is used as the reference subject, while fine-tuning is performed on Subjects 2, 5, and 7. The results are presented in Table 2. We use the same set of metrics as Scotti et al. (2024) to evaluate our reconstructions. Pixel Correlation, SSIM and 2-way percent correct for the 2nd and 5th layer of AlexNet are considered low level visual metrics. 2-way percent correct for Inception(Szegedy et al., 2016), CLIP, EfficientNet(Tan, 2019) and SwAV(Caron et al., 2020) score are considered high level metrics. When trained to completion, both methods\u2014end-to-end training and adapter alignment\u2014show similar overall performance. However, the MSE loss at the adapter level is significantly lower (by an order of magnitude as shown in Appendix C.2) when using adapter alignment. This allows for more precise post-hoc analysis after extracting embeddings from the common space.\n\n\nAdditionally, we observe that adapter alignment yields better results than regular end-to-end training when the model is trained for fewer epochs, with faster convergence. Both approaches eventually reach similar performance levels, as shown in Figure 5.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">Low-Level</td><td colspan=\"4\">High-Level</td></tr><tr><td>PixCorr \u2191</td><td>SSIM \u2191</td><td>Alex(2) \u2191</td><td>Alex(5) \u2191</td><td>Incep \u2191</td><td>CLIP \u2191</td><td>Eff \u2193</td><td>SwAV \u2193</td></tr><tr><td>Subj1 Pretraining</td><td>0.345</td><td>0.346</td><td>92.80%</td><td>96.88%</td><td>94.40%</td><td>90.02%</td><td>0.692</td><td>0.399</td></tr><tr><td>Normal FineTune Avg</td><td>0.258</td><td>0.339</td><td>89.64%</td><td>95.05%</td><td>92.63%</td><td>89.64%</td><td>0.717</td><td>0.427</td></tr><tr><td>AAMax FineTune Avg</td><td>0.259</td><td>0.337</td><td>89.20%</td><td>95.09%</td><td>92.68%</td><td>89.49%</td><td>0.713</td><td>0.423</td></tr><tr><td>Subj2 FineTune Normal</td><td>0.294</td><td>0.345</td><td>91.42%</td><td>95.92%</td><td>93.00%</td><td>88.68%</td><td>0.721</td><td>0.425</td></tr><tr><td>Subj2 FineTune AAMax</td><td>0.295</td><td>0.340</td><td>91.52%</td><td>96.20%</td><td>92.96%</td><td>89.87%</td><td>0.707</td><td>0.409</td></tr><tr><td>Subj5 FineTune Normal</td><td>0.236</td><td>0.335</td><td>89.45%</td><td>95.32%</td><td>93.98%</td><td>91.63%</td><td>0.701</td><td>0.419</td></tr><tr><td>Subj5 FineTune AAMax</td><td>0.240</td><td>0.332</td><td>88.60%</td><td>95.16%</td><td>94.04%</td><td>90.71%</td><td>0.698</td><td>0.413</td></tr><tr><td>Subj7 FineTune Normal</td><td>0.244</td><td>0.339</td><td>88.06%</td><td>93.91%</td><td>90.92%</td><td>88.61%</td><td>0.731</td><td>0.438</td></tr><tr><td>Subj7 FineTune AAMax</td><td>0.242</td><td>0.340</td><td>87.48%</td><td>93.91%</td><td>91.05%</td><td>87.89%</td><td>0.734</td><td>0.448</td></tr></table>\n\n\nTable 2: Quantitative results on training the reconstruction models on all 40 hours of data. Both methods perform similarly across all metrics.\n\n\n# 3.5 TESTS ON LIMITED DATA\n\n\nA key objective of multi-subject pipelines is to achieve satisfactory models of new subjects with limited data. Acquiring fMRI data is costly, and reducing the data requirements for modeling a subject's brain makes fMRI reconstruction more accessible. MindEye2 demonstrated that fine-tuning a new subject with as little as 1 hour of data is possible, though the resulting reconstructions leave room for improvement.\n\n\n![Figure 5: Comparing finetuning reconstruction performance over time with all the data. From left to right: (a) High level metrics over time for Subject 1. Both methods perform similarly but the normal method takes about 20 epochs to match AAmax's numbers (b) Example reconstruction over time. The top row is the normal pipeline and the bottom row is the Adapter alignment pipeline.](images/90d94af57a79add0e1e469d8c07e585d9c2a6a8c50f67c82d6e320ee10d1493b.jpg)\n\n\nIn our experiments, we evaluate the fine-tuning of a new subject in three scenarios: 1 hour, 2 hours, and 4 hours of training data. The NSD dataset provides 40 hours of data per subject, all of which is used for the full data tests in Section 3.4. For this experiment, we designate Subject 5 as the reference subject and fine-tune Subject 1 using the shared images. We compare the performance of normal end-to-end training and Adapter Alignment (AA), with results presented in Table 3.\n\n\nAdapter Alignment consistently outperforms the end-to-end approach, showing significant improvements even after just one epoch of training. The performance gap widens as the amount of fine-tuning data is reduced. Due to the lightweight nature of Adapter Alignment, which involves training on just 250 images using a single layer, the time required to align the adapter is minimal compared to full end-to-end training. In fact, even after 160 epochs of normal training, the model's performance fails to match the Epoch 1 results of Adapter Alignment. Figure 6 illustrates the reconstruction performance across epochs for both methods.\n\n\n![Figure 6: Comparing finetuning reconstruction performance over time with limited data. From left to right: (a) Finetuning Subject 1 with 250 images or 1 hour of data (b) Finetuning Subject 1 with 500 images or 2 hours of data (c) Finetuning Subject 1 with 1000 images or 3 hours of data. We observe that adapter alignment (AAMax) not only outperforms normal end-to-end training, but the performance at Epoch 1 is better than the normal method's performance at Epoch 160.](images/cac86a5a9787d88217707f484de2bacaed4234f1d0bd83105c80194b3c1daef7.jpg)\n\n\n![](images/6cf19ed9819f1168942a5a0ef9a90c26cee26284aa0d91abfb0228502d83ea25.jpg)\n\n\n![](images/c7718cf32635aac70c5984e796f5c7478ae5cf015315a018f4ae570bff91c35f.jpg)\n\n\n# 4 BEST CANDIDATE IMAGE SELECTION\n\n\nHere, we present a selection algorithm that significantly reduces the amount of data required to train a model for a new subject. Later, we carry out an empirical analysis of the method.\n\n\n# 4.1 GREEDY ALGORITHM\n\n\nIn the context of fMRI-based visual reconstruction, we introduce a greedy algorithm that selects the most representative subset of images from the common space. We begin with an embedding of the common set of images into a representation space and choose the first  $d$  dimensions corresponding to the principal singular vectors. For each dimension  $j$ , we partition the range into  $B_{j}$  bins that is determined by the ratio of singular value  $\\lambda_{j}$  to the largest singular value  $\\lambda_{1}$ :\n\n\n$$\n\n$$\nB _ {j} = \\left\\lfloor w \\cdot \\frac {\\lambda_ {j}}{\\lambda_ {1}} \\right\\rfloor , \\quad j = 1, 2, \\dots , d\n$$\n\n$$\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">Low-Level</td><td colspan=\"4\">High-Level</td></tr><tr><td>PixCorr \u2191</td><td>SSIM \u2191</td><td>Alex(2) \u2191</td><td>Alex(5) \u2191</td><td>Incep \u2191</td><td>CLIP \u2191</td><td>Eff \u2193</td><td>SwAV \u2193</td></tr><tr><td>Subj1 FT NS 250 Normal E1</td><td>0.031</td><td>0.234</td><td>58.31%</td><td>62.56%</td><td>58.51%</td><td>58.71%</td><td>0.947</td><td>0.611</td></tr><tr><td>Subj1 FT NS 250 AAMAX E1</td><td>0.095</td><td>0.254</td><td>77.20%</td><td>85.98%</td><td>78.80%</td><td>79.39%</td><td>0.833</td><td>0.503</td></tr><tr><td>Subj1 FT NS 250 Normal E160</td><td>0.094</td><td>0.239</td><td>71.54%</td><td>80.10%</td><td>73.11%</td><td>74.02%</td><td>0.875</td><td>0.537</td></tr><tr><td>Subj1 FT NS 250 AAMAX E160</td><td>0.105</td><td>0.259</td><td>76.77%</td><td>86.20%</td><td>80.08%</td><td>80.14%</td><td>0.828</td><td>0.496</td></tr><tr><td>Subj1 FT NS 500 Normal E1</td><td>0.049</td><td>0.242</td><td>62.41%</td><td>68.06%</td><td>62.64%</td><td>64.80%</td><td>0.925</td><td>0.583</td></tr><tr><td>Subj1 FT NS 500 AAMAX E1</td><td>0.112</td><td>0.275</td><td>79.46%</td><td>88.29%</td><td>83.28%</td><td>82.61%</td><td>0.803</td><td>0.485</td></tr><tr><td>Subj1 FT NS 500 Normal E160</td><td>0.110</td><td>0.270</td><td>75.32%</td><td>84.62%</td><td>76.66%</td><td>78.55%</td><td>0.845</td><td>0.516</td></tr><tr><td>Subj1 FT NS 500 AAMAX E160</td><td>0.114</td><td>0.285</td><td>80.00%</td><td>88.50%</td><td>83.19%</td><td>83.05%</td><td>0.799</td><td>0.480</td></tr><tr><td>Subj1 FT NS 1000 Normal E1</td><td>0.079</td><td>0.241</td><td>65.84%</td><td>73.04%</td><td>64.19%</td><td>68.10%</td><td>0.914</td><td>0.574</td></tr><tr><td>Subj1 FT NS 1000 AAMAX E1</td><td>0.129</td><td>0.265</td><td>80.83%</td><td>90.93%</td><td>86.66%</td><td>86.17%</td><td>0.767</td><td>0.460</td></tr><tr><td>Subj1 FT NS 1000 Normal E160</td><td>0.124</td><td>0.253</td><td>77.86%</td><td>89.18%</td><td>83.10%</td><td>83.26%</td><td>0.801</td><td>0.480</td></tr><tr><td>Subj1 FT NS 1000 AAMAX E160</td><td>0.127</td><td>0.273</td><td>81.73%</td><td>91.29%</td><td>86.23%</td><td>86.26%</td><td>0.763</td><td>0.453</td></tr></table>\n\n\nTable 3: Quantitative results on fine tuning a new subject with limited data. FT=Fine tuning. The first section shows the result of fine tuning with 250 images after 1 epoch and after 160 epochs using normal end-to-end training vs AAMax. The second section compares performance on 500 images and the third section compares performance on 1000 images. In all cases AAMax, significantly outperforms normal training. The greyed out low level metrics are not optimized for in these experiments.\n\n\nwhere  $w$  is a predefined scaling parameter. Dimensions with larger singular values are partitioned into more bins, reflecting their greater variance and importance in capturing meaningful data features.\n\n\nEach image (and its fMRI) maps to exactly one bin in each dimension. Our goal is to find the smallest subset of images that covers all bins in each dimension. This subset is critical in selecting the most representative images for a new subject, ensuring that the selected images capture the key activity patterns that generalize across subjects. The problem as stated is NP-hard. However, it is submodular and a greedy heuristic such as given below achieves an  $(1 - 1 / e)$  approximation ratio.\n\n\nGiven a subset  $S$  of images, let  $Gap(S, j)$  be the number of empty bins in dimension  $j$ , and let  $Gap(S) = \\Sigma_{j} Gap(S, j)$  be the total number of images across all dimensions. At every step, the greedy algorithm chooses image  $i \\notin S$  that minimizes  $Gap$  when added to  $S$ .\n\n\n# 4.2 RESULTS\n\n\nWe achieve better results on high-level metrics compared to state-of-the-art methods while reducing the required data by  $40\\%$ . Figure 7 (a) illustrates how Inception and CLIP distances fluctuate as a function of the amount of training data. Figure 7 (b) presents the model's performance when 150 images are selected for the training set, while varying the number of principal eigenvectors onto which the embeddings are projected. A notable improvement is observed when the number of eigenvectors reaches 40, likely due to the nature of the binning process, as the number of bins created along the eigenvectors provides better coverage. Further analysis of this behavior presents an interesting direction for future work. Our method highlights the critical role of strategic data selection in achieving superior performance. For additional details on the experimental performance, please refer to Tables 4 and 5 in the Appendix Section A.1.\n\n\n# 5 RELATED WORK\n\n\nRecent works have achieved impressive results by mapping fMRI data to latent diffusion model (LDM) spaces (Takagi & Nishimoto, 2023; Scotti et al., 2023; Lu et al., 2023; Xia et al., 2024), while simultaneously integrating multiple modalities. Despite this progress, these methods have not been thoroughly tested for their generalization performance across a larger population of subjects. In other words, constructing a model for a completely new subject still requires retraining the model from scratch to ensure optimal performance.\n\n\n![Figure 7: Comparison of the performance of the image selection strategy. Both figures display the Inception and CLIP distance metrics. (a) Performance with varying training set sizes, where the red and black lines (labeled as Inception ME2 and CLIP ME2) at the 250-point mark indicate the model's performance without a data selection strategy. (b) Performance with varying numbers of principal eigenvectors.](images/32e9efa66924c8c8f80e5cbf62a93339df7ebb8ae9ae230f5f90637d24d2cc03.jpg)\n\n\n![](images/1b2a391b6e0692c0aafecf19ee4de43fdcc826f8ecaca5bee44bfcee48fa7980.jpg)\n\n\nApproaches like Ferrante et al. (2024) have demonstrated cross-subject brain decoding using limited data samples by aligning multiple subjects. Although the authors, like us, leveraged visual stimuli commonly used for multiple subjects, the fidelity of their reconstructed images lags behind current advancements. Scotti et al. (2024), the foundation of our method, produced significant results in image reconstruction under limited data conditions but did not fully exploit the properties of the common representation space. In contrast, our novel approach enables faster convergence and improved performance in a limited data setting by effectively leveraging the structure of the common space.\n\n\n# 6 CONCLUSIONS & DISCUSSION\n\n\nIn this work, we present compelling evidence for the existence of a shared visual representation space across subjects, allowing brain activity from different individuals to be aligned in a common space. Building on this foundation, we introduce Adapter Alignment (AA), a novel and efficient approach for subject-specific alignment in fMRI-based visual reconstruction. Our method, which is more efficient than traditional end-to-end training pipelines, achieves faster convergence and superior performance, particularly in low-data scenarios. Additionally, we propose a greedy image selection algorithm that identifies the most representative subset of images for training new subjects. This approach reduces the amount of data required for fine-tuning by up to  $40\\%$  without sacrificing performance, addressing the challenge of minimizing data requirements in fMRI-based image reconstruction.\n\n\nBy leveraging the structure of the common representation space, we demonstrate the feasibility of producing high-quality visual reconstructions with a significantly reduced amount of subject-specific data. Future work could explore the integration of non-linear adaptors to enhance concept extraction, potentially improving the performance and flexibility of the model. Another promising direction is to extend our methodology to different pipelines, ensuring its generalizability across various architectures, thereby making the approach architecture-agnostic. Furthermore, validating the results using different datasets (Contier et al., 2021) would offer valuable insights into the robustness of the method across diverse data settings.\n\n\n# REFERENCES\n\n- Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\n- Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. A massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, 25(1):116-126, 2022.\n\n- Pinglei Bao, Liang She, Mason McGill, and Doris Y Tsao. A map of object space in primate inferotemporal cortex. Nature, 583(7814):103-108, 2020.\n\n- Roman Belyi, Guy Gaziv, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal Irani. From voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912-9924, 2020.\n\n- Thomas B Christophel, Martin N Hebart, and John-Dylan Haynes. Decoding the contents of visual short-term memory from human visual and parietal cortex. Journal of Neuroscience, 32(38): 12983-12989, 2012.\n\n- Oliver Contier, Martin N Hebart, Adam H Dickter, Lina Teichmann, Alexis Kidder, Anna Corriveau, Charles Zheng, Maryam Vaziri-Pashkam, and Charles Baker. Things-fmri/meg: A large-scale multimodal neuroimaging dataset of responses to natural object images. Journal of Vision, 21(9): 2633-2633, 2021.\n\n- Edward F Ester, David E Anderson, John T Serences, and Edward Awh. A neural measure of precision in visual working memory. Journal of cognitive neuroscience, 25(5):754-761, 2013.\n\n- Matteo Ferrante, Tommaso Boccato, Furkan Ozcelik, Rufin VanRullen, and Nicola Toschi. Through their eyes: multi-subject brain decoding with simple alignment techniques. Imaging Neuroscience, 2:1-21, 2024.\n\n- Marshall L Fisher, George L Nemhauser, and Laurence A Wolsey. An analysis of approximations for maximizing submodular set functions-II. Springer, 1978.\n\n- Guy Gaziv, Roman Belyi, Niv Granot, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal Irani. Self-supervised natural image reconstruction and large-scale semantic classification from brain activity. NeuroImage, 254:119121, 2022.\n\n- Stephenie A Harrison and Frank Tong. Decoding reveals the contents of visual working memory in early visual areas. Nature, 458(7238):632-635, 2009.\n\n- Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023. URL https://arxiv.org/abs/1606.08415.\n\n- Alexander G Huth, Shinji Nishimoto, An T Vu, and Jack L Gallant. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. Neuron, 76(6):1210-1224, 2012.\n\n- Sirawaj Itthipuripat, Thomas C Sprague, and John T Serences. Functional mri and eeg index complementary attentional modulations. Journal of Neuroscience, 39(31):6162-6179, 2019.\n\n- Yukiyasu Kamitani and Frank Tong. Decoding the visual and subjective contents of the human brain. Nature neuroscience, 8(5):679-685, 2005.\n\n- Hsin-Hung Li, Thomas C Sprague, Aspen H Yoo, Wei Ji Ma, and Clayton E Curtis. Joint representation of working memory and uncertainty in human cortex. Neuron, 109(22):3699-3712, 2021.\n\n- Sikun Lin, Thomas Sprague, and Ambuj K Singh. Mind reader: Reconstructing complex images from brain activities. Advances in Neural Information Processing Systems, 35:29624-29636, 2022.\n\n- Yulong Liu, Yongqiang Ma, Guibo Zhu, Haodong Jing, and Nanning Zheng. See through their minds: Learning transferable neural representation from cross-subject fmri, 2024. URL https://arxiv.org/abs/2403.06361.\n\n- Bria Long, Chen-Ping Yu, and Talia Konkle. Mid-level visual features underlie the high-level categorical organization of the ventral stream. Proceedings of the National Academy of Sciences, 115(38):E9015-E9024, 2018.\n\n- Yizhuo Lu, Changde Du, Qiongyi Zhou, Dianpeng Wang, and Huiguang He. Minddiffuser: Controlled image reconstruction from human brain activity with semantic and structural diffusion. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 5899-5908, 2023.\n\n- Milad Mozafari, Leila Reddy, and Rufin VanRullen. Reconstructing natural scenes from fmri patterns using bigbigan. In 2020 International joint conference on neural networks (IJCNN), pp. 1-8. IEEE, 2020.\n\n- Thomas Naselaris, Kendrick N Kay, Shinji Nishimoto, and Jack L Gallant. Encoding and decoding in fmri. Neuroimage, 56(2):400-410, 2011.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021.\n\n- Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n\n- Ziqi Ren, Jie Li, Xuetong Xue, Xin Li, Fan Yang, Zhicheng Jiao, and Xinbo Gao. Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning. NeuroImage, 228:117602, 2021.\n\n- Peter H. Schonemann. A generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):1-10, 1966.\n\n- Alexander Schrijver et al. Combinatorial optimization: polyhedra and efficiency, volume 24. Springer, 2003.\n\n- Miranda Scolari, Anna Byers, and John T Serences. Optimal deployment of attentional gain during fine discriminations. Journal of Neuroscience, 32(22):7723-7733, 2012.\n\n- Paul Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Aidan Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth Norman, et al. Reconstructing the mind's eye: fmri-to-image with contrastive learning and diffusion priors. Advances in Neural Information Processing Systems, 37, 2023.\n\n- Paul S Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth A Norman, et al. Mindeye2: Shared-subject models enable fmri-to-image with 1 hour of data. arXiv preprint arXiv:2403.11207, 2024.\n\n- Katja Seeliger, Umut G\u00fc\u00e7l\u00fc, Luca Ambrogioni, Yagmur G\u00fc\u00e7l\u00fct\u00fcrk, and Marcel AJ Van Gerven. Generative adversarial networks for reconstructing natural images from brain activity. NeuroImage, 181:775-785, 2018.\n\n- John T Serences and Geoffrey M Boynton. Feature-based attentional modulations in the absence of direct visual stimulation. Neuron, 55(2):301-312, 2007.\n\n- John T Serences, Edward F Ester, Edward K Vogel, and Edward Awh. Stimulus-specific delay activity in human primary visual cortex. Psychological science, 20(2):207-214, 2009.\n\n- Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu Horikawa, and Yukiyasu Kamitani. End-to-end deep image reconstruction from human brain activity. Frontiers in computational neuroscience, 13:432276, 2019.\n\n- Thomas C Sprague and John T Serences. Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices. Nature neuroscience, 16(12):1879-1887, 2013.\n\n- Thomas C Sprague, Edward F Ester, and John T Serences. Reconstructions of information in visual spatial working memory degrade with memory load. Current Biology, 24(18):2174-2180, 2014.\n\n- Thomas C Sprague, Sirawaj Itthipuripat, Vy A Vo, and John T Serences. Dissociable signatures of visual salience and behavioral relevance across attentional priority maps in human cortex. Journal of neurophysiology, 119(6):2153-2165, 2018.\n\n- Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826, 2016.\n\n- Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from human brain activity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14453-14463, 2023.\n\n- Mingxing Tan. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.\n\n- Brian A Wandell and Jonathan Winawer. Computational neuroimaging and population receptive fields. Trends in cognitive sciences, 19(6):349-357, 2015.\n\n- Weihao Xia, Raoul de Charette, Cengiz Oztireli, and Jing-Hao Xue. Dream: Visual decoding from reversing human visual system. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 8226-8235, 2024.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638424.1613479", "msecs": "161.0", "relativeCreated": "20991.84536933899", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Language-Informed Visual Concept Learning\n\n# Abstract\nOur understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training.\n\n\n# 1 INTRODUCTION\n\n\nIn order to make sense of the myriad visual entities in the world, humans develop an abstracted generative model of them and organize the underlying sources of variation into visual concepts, such as different colors or different types of objects. Designing systems that can recognize visual concepts within images as humans do has been a longstanding goal in the fields of computer vision and artificial intelligence (Russakovsky et al., 2015; Krizhevsky et al., 2012; Girshick et al., 2014).\n\n\nTo facilitate efficient reasoning and communication of these concepts, humans created symbolic depictions that have evolved into natural language. Such natural language grounding of visual data has been instrumental in the recent proliferation of powerful large vision-language models that are capable of semantically identifying objects in images (Radford et al., 2021; Kirillov et al., 2023) or generating photo-realistic images from arbitrary text prompts (Ramesh et al., 2021; Rombach et al., 2022; Sahara et al., 2022; Yu et al., 2022). While different concept axes can be easily specified by words, such as category and style, it is much less intuitive to delineate the subtleties of low-level visual nuances along each axis using language, such as one particular style of a painting.\n\n\nIn this work, our goal is to distill from large pre-trained vision-language models a function that extracts visual concepts along a set of language-specified concept axes from images. As illustrated in Figure 1, once these concepts are extracted, we can recompose them across different image instances at inference time to produce new images with novel concept combinations. To learn this function, rather than collecting a large-scale dataset of human annotations for each specific visual concept, we design a language-informed visual concept representation, and simply distill from a pre-trained Text-to-Image (T2I) generation model. There are three fundamental properties we seek in this visual concept representation.\n\n\nFirst, unlike T2I generation, which relies on generic words as visual concept descriptors, we would like to capture fine-grained visual nuances using continuous concept embeddings. One common technique is to invert the text-to-image generation process by optimizing an embedding with the objective of reproducing a given input image using a pre-trained T2I model, often referred to as Textual Inversion (Gal et al., 2022). However, most existing Textual Inversion methods (Gal et al.,\n\n\n![Figure 1: Language-Informed Visual Concept Learning. Our goal is to learn a visual concept representation grounded on a set of language-informed concept axes, e.g., category, color, and material, by simply distilling from pre-trained text-to-image generation models without manual annotations. After training, the concept encoders extract disentangled axis-specific embeddings from an image, which can be remixed to generate new images with novel concept compositions.](images/6cbedad65b640ee4643ec354e3272add961f72badef713adc46c3340efa57a0f.jpg)\n\n\n![Figure 2: Learned Disentangled Concept Embeddings Improve Compositionality. Left: Vanilla text-to-image model may fail to adhere to text prompts of uncommon combinations of concepts even with prompt engineering, e.g. \"red banana\". Right: With the same backbone T2I generator, our learned disentangled concept embeddings greatly enhance concept compositionality.](images/6265050443a8dd5d6436db8a5a10b895d5a05dfd528dd484535bac88388debcf.jpg)\n\n\n2022) optimize embeddings for individual image instances independently, overlooking the shared nature of visual concepts across instances. For instance, the concept of \"red\" is shared between a \"red apple\" and a \"red dress\". Moreover, the concepts of \"red\" and \"yellow\" also are instances of the property of color.\n\n\nHence, the second desired property of the visual concept representation is to preserve such common concept structures among various visual instances. Instead of optimizing on individual image instances independently, we design a set of concept encoders, where each encoder learns to encode the visual characteristics of an input image pertaining to one concept axis specified by language. This ensures that the inverted concept embeddings can be shared across different instances and remixed to generate new images.\n\n\nThe third crucial aspect of this representation is to ascertain that different concept axes are disentangled, allowing for changes to be made specifically on single concept axis without modifying other axes. To do so, we reuse the disentangled nature of linguistic concepts and ground the predictions to a set of discrete text anchors in the concept embeddings space, which can be obtained by querying a pre-trained generic Visual Question Answering (VQA) model, e.g., BLIP-2 (Li et al., 2023b). This soft anchoring constraint significantly improves the disentanglement of concept embeddings across different axes while still retaining sufficient leeway to capture nuanced visual variations that BLIP-2 struggles to discern, e.g., the style of an art piece in Figure 6.\n\n\nPutting these ideas together, we design a generic framework for learning disentangled and compositional visual concepts grounded to linguistic structures by exploiting pre-trained text-to-image generation and visual question answering models. We show that these concept encoders can be trained purely on synthetic images generated by a pre-trained T2I model, and extract concept embeddings from real images at test time, which capture the fine-grained visual nuances.\n\n\nOur contributions can be summarized as follows:\n\n- 1. We propose a generic framework for learning language-informed visual concepts by simply distilling pretrained vision-language models.\n\n- 2. At inference time, the trained concept encoders extract concept embeddings from a test image, which can be remixed to generate images with novel compositions of concepts.\n\n- 3. Using a light-weight test-time finetuning procedure, these encoders can also be quickly adapted to extract novel concepts unseen during training.\n\n\n4. Experiments show that this visual concept representation achieves better disentanglement and compositionality, compared to text-based prompting baselines, as shown in Figures 2 and 6.\n\n\n# 2 RELATED WORK\n\n\n# 2.1 VISUAL CONCEPT LEARNING\n\n\nDesigning learning-based systems to discover various visual concepts in natural images has been a long-standing goal in machine perception and intelligence. Early attempts typically rely on extensive semantic annotations done by humans, such as object classification (Barnard et al., 2003; Fei-Fei et al., 2006; Fergus et al., 2005), which were later epitomized by the effort of ImageNet (Russakovsky et al., 2015). Visual concepts are intrinsically linked to concepts in language, and such end-to-end supervised learning paradigms can be seen as learning a direct mapping between visual concepts and discrete linguistic concepts. Other approaches attempt to better exploit this inherent structure in language by constructing a structured representation of visual concepts such as scene graphs (Zhong et al., 2021) and symbolic programs (Mao et al., 2019; Han et al., 2019).\n\n\nMore recently, the success of natural language modeling (Devlin et al., 2018; Brown et al., 2020; Raffel et al., 2020) has paved the way for grounding visual concepts to open vocabularies, unlike category labels or fixed symbolic programs, by training large Vision-Language Models (VLMs) on massive image captioning datasets (Schuhmann et al., 2022). This has powered recent Text-to-Image (T2I) generation models to turn linguistic concepts from free-form text prompts into photo-realistic images (Rombach et al., 2022; Sahara et al., 2022). These T2I models have been leveraged by Personalization methods for extracting individual visual concepts from one or a few images. This is done by either by optimizing token embeddings (Gal et al., 2022; Vinker et al., 2023; Avrahami et al., 2023; Chefer et al., 2023b; Liu et al., 2023), finetuning the backbone denoiser (Ruiz et al., 2023), or training additional encoders for amortized optimization (Gal et al., 2023; Arar et al., 2023; Li et al., 2023a). We also distill visual concepts from a pre-trained T2I model, but unlike existing works, we train encoders to adhere to a set of language-specified concept axes, preserving the disentangled and compositional nature of language. Ranasinghe & Ryoo (2023) also explores language-defined concepts but focuses on video action recognition tasks while we focus on image generation.\n\n\nA separate line of work focuses on unsupervised visual concept disentanglement without explicitly leveraging language, typically by simply imposing information constraints in the latent space of a generative model, like VAEs and GANs (Higgins et al., 2017; Chen et al., 2016; Hsu et al., 2023). Here, we are interested in learning visual concepts that are explicitly grounded to language.\n\n\n# 2.2 CONTROLLABLE IMAGE GENERATION\n\n\nThe success of GAN-based image generation (Goodfellow et al., 2014; Brock et al., 2018; Karras et al., 2019) has spawned a series of works that discover controllable directions in the GAN latent space (Voynov & Babenko, 2020; Harkonen et al., 2020). More recently, the advancements of diffusion-based T2I models have unlocked new possibilities for controllable image generation, where photo-realistic images can be generated from free-form text prompts. Recent works proposed to further improve the alignment of image samples and input text conditions by manipulating attention maps within T2I models (Chefer et al., 2023a; Epstein et al., 2023). Another form of controllable image generation is compositional generation. Liu et al. (2022) proposes to improve the quality of T2I diffusion models for composing multiple pre-given concepts, specified via text prompts, by modifying the inference procedure. In this work, instead of assuming that concepts are given and are in a text format, we tackle the task of identifying disentangled concepts which can be used for composition.\n\n\nImage generation can also be controlled with image analogies (\u0160ubtrova et al., 2023; Hertzmann et al., 2001), a form of visual prompting. These works typically do not explicitly extract visual concepts from inputs unlike ours. In this work, we amalgamate both visual prompts and text queries, employing them as the editing interface.\n\n\n# 3 METHOD\n\n\nFig. 3 gives an overview of our proposed learning framework. Our goal in this work is to extract visual concepts from images along a number of concept axes specified by language, such as\n\n\n![Figure 3: Training Pipeline. During training, an input image is processed by a set of concept encoders that predict concept embeddings specific to given concept axes. These embeddings are trained to (1) retain information in order to reproduce visual inputs via a pre-trained Text-to-Image model given an axis-informed text template, and (2) ensure disentanglement across different axes by anchoring to text embeddings obtained from a pre-trained Visual Question Answering model.](images/b89b484cc861cafea48ff730bbca55210c3cfe07e46b038f971fe09bd2439a2b.jpg)\n\n\ncategory, color, and material, so as to enable the flexible composition of concepts into high-quality image generations.\n\n\nTo achieve this, we train a set of visual concept encoders by distilling concept guidance from pretrained vision-language models. Specifically, the encoders are trained to extract concept embeddings from an image in order to fulfill two objectives. First, they should be recomposed to explain the input image through a pretrained text-to-image (T2I) generation model, given a concept-axis-informed text prompt. Second, these visual concept embeddings should be anchored to the corresponding text embeddings obtained from a pre-trained visual question answering (VQA) model, further exploiting the disentangled nature of linguistic concepts for better disentanglement of visual concepts.\n\n\n# 3.1 VISUAL CONCEPT ENCODING BY INVERTING TEXT-TO-IMAGE GENERATION\n\n\nOur understanding of the visual world is centered around various concept axes, to which we have often assigned words due to their significance in communication and reasoning. This vision-language grounding has fueled recent explosion of text-to-image generation models (Rombach et al., 2022; Sahara et al., 2022; Ramesh et al., 2022), allowing them to generate photo-realistic images with various combinations of concepts defined by words.\n\n\nHere, we are interested in the reverse direction of text-to-image generation, where the goal is to extract language-grounded visual concepts present in natural images. Specifically, given  $K$  concept axes of interest defined by language, we would like to learn  $K$  concept encoders  $\\{f_k(\\cdot)\\}_{k=1}^K$ , each of which extracts a concept representation  $\\mathbf{e}_k = f_k(\\mathbf{x})$  along a concept axis from an input image  $\\mathbf{x}$ .\n\n\nIn order to train these concept encoders  $\\{f_k(\\cdot)\\}$ , instead of relying on extensive human labeling, we opt to exploit the vision-language grounding embedded within large pre-trained T2I generation models. Using the technique of Textual Inversion (Gal et al., 2022), one can optimize a token embedding  $<\\star>$  to capture a visual entity in a given image, through the objective of regenerating the image with the T2I model from a text template, such as \"a photo of  $<\\star>$ \". Here, we adopt a similar objective, but instead of inverting a specific embedding capturing the overall \"identity\" of an individual image instance, we would like to predict embeddings  $\\mathbf{e}_k$  that are grounded to a number of meaningful concept axes, using an axis-informed text template, such as \"a photo of  $<\\mathbf{e}_1>$  with  $<\\mathbf{e}_2>$  color and  $<\\mathbf{e}_3>$  material\". This allows the extracted concept embeddings to be shared across different images, encapsulating the common visual characteristics pertinent to one concept axis.\n\n\nSpecifically, given an image  $\\mathbf{x}$ , the concept encoders  $\\{f_k(\\cdot)\\}$  extract a set of concept embeddings  $\\{\\mathbf{e}_k\\in \\mathbb{R}^D\\}$ , which have the same dimension  $D$  as the text embeddings so that they can be directly inserted into the text embeddings of the axis-informed text template. To simplify the notations, let  $f_{\\gamma}(\\cdot)$  denote the function that takes in the image and produces the final sequence of embeddings\n\n\nof the template and the predicted concept embeddings, and  $\\gamma$  be the parameters of all the encoders which will be optimized during training. Let  $\\mathbf{c}_{\\theta}$  be the part of the T2I model's text encoder that takes in a sequence of text embeddings and outputs a conditioning vector for the T2I model's denoising network  $\\hat{\\epsilon}_{\\theta}$ , where  $\\theta$  denotes network parameters. We use DeepFloyd (StabilityAI; Sahara et al., 2022) as the backbone T2I model, which utilizes a pre-trained T5 model (Raffel et al., 2020) as the text encoder, and keep the parameters  $\\theta$  frozen in all experiments. To train the encoders, we reuse the training objective for the backbone diffusion model:\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {r e c o n}} (\\mathbf {x}; \\gamma) = \\mathbb {E} _ {\\boldsymbol {\\epsilon} \\sim \\mathcal {N} (\\mathbf {0}, I), t \\sim \\mathcal {U} ([ 0, 1 ])} \\left[ \\| \\hat {\\epsilon} _ {\\theta} (\\mathbf {x}, t, \\mathbf {c} _ {\\theta} (f _ {\\gamma} (\\mathbf {x}))) - \\boldsymbol {\\epsilon} \\| _ {2} ^ {2} \\right], \\tag {1}\n$$\n\n$$\n\n\nwhere the noise  $\\epsilon$  is sampled from a standardmultivariate Gaussian distribution and the timestep  $t$  is sampled from a uniform distribution in [0, 1]. Minimizing  $\\mathcal{L}^{\\mathrm{recon}}$  amounts to finding concept embeddings within the space of the pre-trained T2I model that can best reproduce the input image  $\\mathbf{x}$ , resembling a \"reconstruction\" objective.\n\n\nCompared to per-instance token optimization in vanilla Textual Inversion, the advantages of training these concept encoders are two-fold. First, the concept embedding space is naturally shared across different image instances, encapsulating the common understanding of the corresponding concept axes. Second, it makes training more efficient by amortizing the optimization across all instances, and more crucially, it allows for test-time inference in a feed-forward pass.\n\n\n# 3.2 CONCEPT DISENTANGLEMENT USING TEXT ANCHORS\n\n\nThe objective of  $\\mathcal{L}^{\\mathrm{recon}}$  ensures that the extracted concept embeddings can sufficiently reconstruct the concept of a given image through a pre-trained text-to-image generation model. However, with this loss alone, there is little guarantee that each embedding encodes only the information pertinent to a particular concept axis. In practice, we found that this baseline results in poor disentanglement of different concept axes when remixing the concept embeddings to generate new images, potentially due to the imprecise vision-language grounding in the pre-trained T2I model. For instance, as shown in Figure 8, the extracted category embedding  $<\\mathbf{e}_1>$  for \"red berries\" cannot be remixed with various color embeddings  $<\\mathbf{e}_2>$  e.g., \"orange\", as  $<\\mathbf{e}_1>$  is highly entangled with the concept of a \"red\" color due to the bias in natural images.\n\n\nTo encourage better disentanglement of different concept axes, we further incorporate a sparse set of text anchors into the concept embedding space. Along each concept axis like color, we have often named some prominent modes, such as \"red\" or \"yellow\", and these text labels entail clearly disentangled concepts. Therefore, we would like to reuse this disentangled nature of linguistic concepts to improve the disentanglement of visual concepts. To this end, we make use of the text predictions from a pre-trained Visual Question Answering (VQA) model, BLIP-2 (Li et al., 2023b), as pseudo ground-truth anchors for the concept embeddings.\n\n\nSpecifically, for each training image  $\\mathbf{x}$  and for each concept axis of interest (e.g., color) indexed by  $k$ , we query the BLIP-2 model  $\\Psi$  with the image  $\\mathbf{x}$  and a question  $q_{k}$  in natural language that is specific to this concept axis, e.g., \"what is the color of the object in the image\". Denote the answer from BLIP-2, also in the form of natural language, as  $\\Psi(\\mathbf{x}, q_{k})$ . We encode this answer with the pre-trained text encoder  $c_{\\theta}$  to obtain a text embedding  $\\tilde{\\mathbf{e}}_{k} = c_{\\theta}(\\Psi(\\mathbf{x}, q_{k}))$ . The prediction of our concept encoders  $f_{k,\\gamma}$  is encouraged to stay close to this anchor text embedding:\n\n\n$$\n\n$$\n\\mathcal {L} _ {k} ^ {\\text {a n c h o r}} (\\mathbf {x}; \\gamma) = \\| f _ {k, \\gamma} (\\mathbf {x}) - \\tilde {\\mathbf {e}} _ {k} \\| _ {2} ^ {2}, \\quad \\text {w h e r e} \\quad \\tilde {\\mathbf {e}} _ {k} = c _ {\\theta} \\left(\\Psi (\\mathbf {x}, q _ {k})\\right). \\tag {2}\n$$\n\n$$\n\n\nIt is crucial to highlight that we use these BLIP-2 predictions only as anchors by assigning a small weight to this anchor loss  $\\mathcal{L}_k^{\\mathrm{anchor}}$  during training. Otherwise, the embeddings predicted by the concept encoders could easily collapse to a set of discrete text embeddings and fail to capture the visual nuances in images.\n\n\n# 3.3 TRAINING AND INFERENCE\n\n\nTraining. Given a collection of training images  $\\mathcal{D}$  containing various combinations of concepts along each axis, the final objective to train the concept encoders consists of the two parts:\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {t o t a l}} (\\gamma) = \\mathbb {E} _ {\\mathbf {x} \\sim \\mathcal {D}} \\left[ \\mathcal {L} ^ {\\text {r e c o n}} (\\mathbf {x}; \\gamma) + \\sum_ {k = 1} ^ {K} \\lambda_ {k} \\mathcal {L} _ {k} ^ {\\text {a n c h o r}} (\\mathbf {x}; \\gamma) \\right]. \\tag {3}\n$$\n\n$$\n\n\nInference. At inference time, given a new test image, the concept encoders extract embeddings  $\\{\\mathbf{e}_k\\}$  capturing its characteristics along each concept axis of interest. These embeddings can be remixed across different images, or be replaced by embeddings converted from explicit words, to produce images with new compositions of visual concepts through the backbone T2I generator.\n\n\nGeneralization to Unseen Concepts via Test-Time Finetuning. While the encoders can precisely extract an axis-specific concept that has been seen during training from a new test image, they tend to be less robust to concepts unseen at training. However, with a lightweight test-time optimization procedure, where we use only the reconstruction objective  $\\mathcal{L}^{\\mathrm{recon}}$  to update the parameters for all encoders,  $\\gamma$ , these encoders can generalize to novel concepts unseen during training. Note that  $\\mathcal{L}^{\\mathrm{anchor}}$  is omitted here in order to capture the visual nuances without over-committing to the coarse text anchors. After training, the encoders have learned to generate outputs within a relatively narrow region of the embedding space, which allows the model to adapt to the test images shown in Figure 5 within around 600 iterations while maintaining disentanglement and compositional capability.\n\n\n# 4 EXPERIMENTS\n\n\n# 4.1 EXPERIMENT SETUP\n\n\nTraining Data Generation. We train the concept encoders only using synthetic images generated by DeepFloyd from 5 different domains, including fruits, figurines, furniture, art, and clothing. More details of our dataset can be found in A.2. For each dataset, we consider 2-3 concept axes, such as category, color, material, style, and season. For example, considering category and color for the fruits dataset, we generate training images by prompting DeepFloyd with text prompts describing varying combinations of categories and colors, e.g. \"a photo of an apple which is red in color\". Note that these text prompts are used only for data generation and not for training, as they may not be reliable (Figure 2). On average, we obtain 669 training images for each dataset.\n\n\nImplementation Details. Inspired by Gal et al. (2023), we leverage a pre-trained CLIP ViT/L-14 model for image encoding (Radford et al., 2021; Dosovitskiy et al., 2021), which was trained with a contrastive objective aligning image and text features, and hence well-suited for our task. We extract image features from CLIP ViT and train  $K$  separate concept encoders  $f_{k}$  on top of the features, which share the same architecture but maintains separate weights. Specifically, we take the [CLS] tokens from each CLIP layer and process each token with a distinct linear layer. This is different from Gal et al. (2023), which extracts [CLS] tokens from every other layer and uses a single shared linear layer for all token features. The transformed features are then aggregated with average pooling followed by a LeakyReLU (Xu et al., 2015) activation, and passed into another linear layer that produces the final predicted concept embeddings.\n\n\nTo ground the concept embeddings to the concept axes, we adapt the text templates from CLIP (Radford et al., 2021), which were originally used to assemble captions with class categories from ImageNet (Russakovsky et al., 2015). For training, we use AdamW (Loshchilov & Hutter, 2017) optimizer with learning rate 0.02, and randomly flip the images horizontally. For test-time finetuning, we use the AdamW optimizer with learning rate 0.001. We set  $\\lambda_{k} = 0.0001$  (Equation (3)) for the category axis and  $\\lambda = 0.001$  for others. We use IF-I-XL from DeepFloyd as the backbone model, with training resolution  $64 \\times 64$ . Training on one dataset takes approximately 12 hours on one NVIDIA GeForce RTX 3090 GPU. Generated images are upsampled  $256 \\times 256$  using IF-II-L for visualization purpose.\n\n\n# 4.2 QUALITATIVE RESULTS\n\n\nVisual Concept Extraction, Decomposition and Extrapolation. Once trained, the concept encoders can extract disentangled concept embeddings specific to each concept axis from different test images, which can be recomposed to generate new images with various concept compositions. As shown in Figure 4, across various datasets, our method is able to recompose axis-specific visual concepts from different images and consistently generate new images depicting the recomposed concepts. More examples can be found in Appendix A.1, where images generated from individual decomposed concept embeddings are also presented.\n\n\nThis disentangled concept representation also allows us to extrapolate along a particular concept axis for visual exploration, as shown in Figure 7. For instance, we can ask BLIP-2 \"what is the style\n\n\n![(c) Furniture](images/9ca0e6d02b392d03e980ae8e71551e4b0df2fbd1553f75e8a3e9ddd236c3c193.jpg)\n\n\n![Figure 4: Concept Decomposition. At test time, our model extracts visual concepts along various axes from different images and recompose them to generate new images. We show decomposition results across different pairs of concept axes in 3 datasets: (a) Fruits, (b) Paintings, and (c) Furniture. Figure 5: Generalization to Unseen Concepts via Finetuning. After test-time fine-tuning on a single test-time image, encoders can adapt to novel concept. Visual details from the input images are preserved as can be seen from images visualizing embedding predictions. Importantly, these embeddings do not overfit to the input images and maintain a good disentanglement, such that they can be freely recomposed to create new concepts. More results can be found in Figures 9 to 13. More real-world results can be found in Figures 20 to 24.](images/85d5816954f586fcaa9669d8c3122a783a6ffb288a6c17a0a841c707cb4048aa.jpg)\n\n\nof the painting?\" in an image, and prompt GPT-4 (OpenAI, 2023) to name a few alternatives. We can then recompose the text embeddings of these alternative styles with our concept embeddings and generate images to visualize these variants. Representing concepts as continuous embeddings further enables concept interpolations. Details and results are shown in Appendix A.6.\n\n\nGeneralization to Unseen Concepts via Test-Time Finetuning. Although the encoders have only seen a limited range of concepts during training due to the small size of the training dataset, it can be quickly adapted to unseen concepts with the lightweight test-time finetuning procedure in Section 3.3, as shown in Figure 5. For instance, after 600 finetuning iterations, the model can extract the specific style of the dog painting unseen at training and compose it with the content from other images. It can also capture nuanced colors e.g. yellow-ish-orange and transfer them to other objects.\n\n\n![Figure 6: Baselines Comparisons. Compared the text-based image editing methods, our method achieves significantly better compositionality due to the disentangled concept representation, and captures fine-grained color variations, which the baseline struggles to encode with language.](images/186927e07b61069a975658bfbcb53a7f42b9171212357179fb24ca686d9ddadd.jpg)\n\n\n<table><tr><td rowspan=\"3\"></td><td colspan=\"6\">CLIP-Score \u2191</td><td colspan=\"2\">Human Evaluation \u2191</td></tr><tr><td colspan=\"3\">Edit Category</td><td colspan=\"3\">Edit Color</td><td>Edit Cat.</td><td>Edit Clr.</td></tr><tr><td>Cat.&amp;Clr.</td><td>Cat.</td><td>Clr.</td><td>Cat.&amp;Clr.</td><td>Cat.</td><td>Clr.</td><td>Score</td><td>Score</td></tr><tr><td>Null-text Inversion</td><td>0.258</td><td>0.249</td><td>0.249</td><td>0.259</td><td>0.265</td><td>0.223</td><td>0.287</td><td>0.316</td></tr><tr><td>InstructPix2Pix</td><td>0.267</td><td>0.277</td><td>0.226</td><td>0.270</td><td>0.245</td><td>0.268</td><td>0.233</td><td>0.648</td></tr><tr><td>Baseline w/ BLIP-2</td><td>0.313</td><td>0.294</td><td>0.248</td><td>0.287</td><td>0.271</td><td>0.237</td><td>0.448</td><td>0.379</td></tr><tr><td>Ours</td><td>0.308</td><td>0.297</td><td>0.238</td><td>0.302</td><td>0.287</td><td>0.236</td><td>0.968</td><td>0.840</td></tr><tr><td>w/o Lkanchor</td><td>0.268</td><td>0.276</td><td>0.219</td><td>0.263</td><td>0.257</td><td>0.236</td><td>-</td><td>-</td></tr><tr><td>w/o Encoder &amp; Lkanchor</td><td>0.288</td><td>0.300</td><td>0.214</td><td>0.242</td><td>0.213</td><td>0.265</td><td>-</td><td>-</td></tr></table>\n\n\nTable 1: Quantitative Comparisons on Visual Concept Editing. Compared to existing image editing baselines (Brooks et al., 2023; Mokady et al., 2022), our method achieves better overall CLIP score when editing either axis, and is particularly effective at retaining category-related concepts as reflected in human evaluation. 'Cat' denotes Category and 'Crl' denotes Color.\n\n\n# 4.3 COMPARISON WITH PRIOR WORKS\n\n\nQualitative Comparisons While this task of image generation with disentangled visual concepts is new, we identified prior work that is capable of text-based image editing and generation, and establish a side-by-side comparison on the task of visual concept editing. Specifically, given an image  $\\mathbf{x}$ , for example, of a teal-colored apple in Figure 6, the task is to generate a new image  $\\hat{\\mathbf{x}}_{\\mathbf{e}_k \\to \\mathbf{e}_k'}$  with one concept axis  $k$  (e.g. category) modified by a text prompt, from  $\\langle \\mathbf{e}_k \\rangle$  to  $\\langle \\mathbf{e}_k' \\rangle$ , while preserving other axes  $\\{i | i \\neq k\\}$  from the input.\n\n\nWe compare to two existing methods. Null-text Inversion (Mokady et al., 2022) performs concept editing by first inverting the input image to token embeddings of Stable-Diffusion (Saharia et al., 2022) and then applying Prompt-to-Prompt (Hertz et al., 2022), which modifies cross attention maps for editing, a process that leads to pixel-aligned editing results. InstructPix2Pix (Brooks et al., 2023) is a conditional diffusion model for text-based image editing. Since it is trained on data curated with Prompt-to-Prompt for supervision, it also tends to generate pixel-aligned results. We also design a naive baseline for this task, where we query BLIP-2 for a text description of the attributes to be retained from the test image, and combine the answer with a target concept to generate the final recomposed image. As shown in Figure 6, our method achieves better decomposition results, whereas baseline methods fail to disentangle the desired axis-specific concepts from input images, and therefore struggle to faithfully reproduce the desired concepts from the image and the text.\n\n\nQuantitative Comparisons. We also conduct quantitative evaluations on the task of text-based visual concept editing and compare with prior work. Specifically, we record the ground-truth text\n\n\n![Figure 7: Visual Concept Extrapolation. Given an input image, we can extrapolate along a concept axis by querying BLIP-2 and GPT-4 to name a few alternatives to the concept in the input. Our model can then generate new variants of the input for visual exploration, by mixing them with the extracted concept embeddings.](images/156d041247697f43fbfafd963eacdff341bf090a710ea3b3147443cebac4467d.jpg)\n\n\n![Figure 8: Ablations. Both the text anchor loss and the encoder design reduce overfitting to input images and improve concept disentanglement. Removing these components significantly deteriorates the visual concept editing results. Additionally, we observe that the BLIP-2 baseline has difficulties transferring unnatural colors.](images/09e741c32b44cf2ae1e99561bd91ca98f1f15a6ac745de310932013bcab3c389.jpg)\n\n\nprompts  $y$  that we used to generate each training image  $\\mathbf{x}$ , and manually filter out the inconsistent pairs. For each concept axis  $k$ , we randomly select a target label  $\\langle \\mathbf{e}_k' \\rangle$  from a set of candidates that differ from the one in the original prompt  $\\langle \\mathbf{e}_k \\rangle$ . We then measure the alignment score between the modified image  $\\hat{\\mathbf{x}}_{\\mathbf{e}_k \\rightarrow \\mathbf{e}_k'}$  with the modified text prompt  $y_{\\mathbf{e}_k \\rightarrow \\mathbf{e}_k'}$  using CLIP (Radford et al., 2021), following Gal et al. (2022). We also break down the score to each individual axis, by computing the alignment of the image to a prompt specifying only one axis, e.g. \"a photo of  $\\langle \\mathbf{e}_k' \\rangle$ \". All results are summarized in Table 1. Our method captures the characteristics of category particularly well and outperforms others in changing category or preserving category while changing color, which highlights its ability to extract disentangled concepts allowing for flexible compositions. InstructPix2Pix is more effective in changing color, but tends to do poorly in preserving the category, as seen in Figure 6. More details on our quantitative results can be found in A.4.\n\n\nWe further conduct a human evaluation. Each participant is presented the results from all three methods in a random order together with the editing instruction, and asked to rank them considering both realism and faithfulness to the instruction (see Appendix A.3 for details). We aggregate the responses from 20 people and report the average average score normalized to 0-1 in Table 1. More details of our setup can be found in A.3.\n\n\n# 4.4 ABLATIONS\n\n\nWe conduct an ablation study to understand the effects of the proposed encoder (as opposed per instance optimization) and the text anchoring loss  $\\mathcal{L}_k^{\\mathrm{anchor}}$ . We use the same evaluation dataset and the CLIP-alignment metric as described in Section 4.3. As shown in Fig. 8 and Table 1, removing  $\\mathcal{L}_k^{\\mathrm{anchor}}$  and the encoders deteriorates disentanglement of the different concept axes due to severe overfitting, resulting in poor recomposition results. More results are included in the Appendix.\n\n\n# 5 CONCLUSION\n\n\nIn this paper, we have presented a framework for learning language-informed visual concepts from images, by simply distilling from pre-trained vision-language models. After training, the concept encoders extract disentangled concept embeddings along various concept axes specified by language, which can be remixed or edited to generate images with novel concept compositions. We conduct thorough evaluations both quantitatively and qualitatively, and demonstrate that our approach yields superior results in visual concept editing compared to prior work.\n\n\n# REFERENCES\n\n- Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H Bermano. Domain-agnostic tuning-encoder for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06925, 2023.\n\n- Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from a single image. SIGGRAPH Asia, 2023.\n\n- Kobus Barnard, Pinar Duygulu, David Forsyth, Nando de Freitas, David M. Blei, and Michael I. Jordan. Matching words and pictures. JMLR, 2003.\n\n- Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\n\n- Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023.\n\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020.\n\n- Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models, 2023a.\n\n- Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Michal Irani, Inbar Mosseri, and Lior Wolf. The hidden language of diffusion models. arXiv preprint arXiv:2306.00966, 2023b.\n\n- Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-gan: Interpretable representation learning by information maximizing generative adversarial nets. NeurIPS, 29, 2016.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.\n\n- Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. NeurIPS, 2023.\n\n- Li Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE TPAMI, 2006.\n\n- R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. Learning object categories from google's image search. In ICCV, 2005.\n\n- Bruce Fraser, Fred Bunting, and Chris Murphy. Real world color management, 2004.\n\n- Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.\n\n- Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Trans. Graph., 2023.\n\n- Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.\n\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014.\n\n- Chi Han, Jiayuan Mao, Chuang Gan, Josh Tenenbaum, and Jiajun Wu. Visual concept-metaconcept learning. In NeurIPS, 2019.\n\n- Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering interpretable gan controls. NeurIPS, 2020.\n\n- Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.\n\n- Aaron Hertzmann, Charles E Jacobs, Nuria Oliver, Brian Curless, and David H Salesin. Image analogies. In SIGGRAPH, 2001.\n\n- Irina Higgins, Loic Matthew, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017.\n\n- Kyle Hsu, Will Dorrell, James C. R. Whittington, Jiajun Wu, and Chelsea Finn. Disentanglement via latent quantization. In NeurIPS, 2023.\n\n- Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019.\n\n- Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.\n\n- Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023a.\n\n- Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023b.\n\n- Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. In ECCV, 2022.\n\n- Nan Liu, Yilun Du, Shuang Li, Joshua B Tenenbaum, and Antonio Torralba. Unsupervised compositional concepts discovery with text-to-image generative models. ICCV, 2023.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\n- Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In ICLR, 2019.\n\n- Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.\n\n- OpenAI. Gpt-4 technical report, 2023.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.\n\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n\n- Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\n- Kanchana Ranasinghe and Michael Ryoo. Language-based action concept spaces improve video self-supervised learning. NeurIPS, 2023.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pp. 10684-10695, 2022.\n\n- Nataniel Ruiz, Yanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023.\n\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015.\n\n- Donald G Saari. Selecting a voting method: the case for the borda count. Constitutional Political Economy, 34(3):357-366, 2023.\n\n- Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022.\n\n- Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022.\n\n- StabilityAI. DeepFloyd IF. https://github.com/deep-floyd/IF.\n\n- Ad\u00e9la \u0160ubrtov\u00e1, Michal Luk\u00e1\u010d, Jan \u010cech, David Futschik, Eli Shechtman, and Daniel Sykora. Diffusion image analogies. In SIGGRAPH, pp. 1-10, 2023.\n\n- Yael Vinker, Andrey Voynov, Daniel Cohen-Or, and Ariel Shamir. Concept decomposition for visual exploration and inspiration. arXiv preprint arXiv:2305.18203, 2023.\n\n- Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan latent space. In ICML, 2020.\n\n- Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.\n\n- Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\n\n- Yiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu, and Yin Li. Learning to generate scene graph from natural language supervision. In ICCV, 2021.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638425.433781", "msecs": "433.0", "relativeCreated": "22264.278411865234", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph\n\n# Abstract\nLarge Language Models (LLMs) excel in code generation yet struggle with modern AI software engineering tasks. Unlike traditional function-level or file-level coding tasks, AI software engineering requires not only basic coding proficiency but also advanced skills in managing and interacting with code repositories. However, existing methods often overlook the need for repository-level code understanding, which is crucial for accurately grasping the broader context and developing effective solutions. On this basis, we present RepoGraph, a plug-in module that manages a repository-level structure for modern AI software engineering solutions. RepoGraph offers the desired guidance and serves as a repository-wide navigation for AI software engineers. We evaluate RepoGraph on the SWE-bench by plugging it into four different methods of two lines of approaches, where RepoGraph substantially boosts the performance of all systems, leading to a new state-of-the-art among open-source frameworks. Our analyses also demonstrate the extensibility and flexibility of RepoGraph by testing on another repo-level coding benchmark, CrossCodeEval.\n\n\n# 1 INTRODUCTION\n\n\nRecent advancements in large language models (LLMs) have showcased their powerful capabilities across various natural language processing tasks (OpenAI, 2023; Anil et al., 2023; Dubey et al., 2024), and now, coding-specific LLMs are emerging to tackle complex software engineering challenges (Hou et al., 2023; Fan et al., 2023), such as Code-Llama (Rozi\u00e8re et al., 2023) and StarCoder (Li et al., 2023a). These coding-specific LLMs are capable of assisting users with various software engineering tasks, even achieving human-level performance in many function-level coding tasks, such as program synthesis (Chen et al., 2021; Austin et al., 2021), code annotation (Yao et al., 2019), bug fixing (Tufano et al., 2019), and code translation (Rozi\u00e8re et al., 2020).\n\n\nReal-world software engineering often extends beyond single function or self-contained code files. Applications are typically built as repositories containing multiple interdependent files, modules, and libraries (Bairi et al., 2024). These complex structures require a holistic understanding of the entire codebase to perform tasks such as code completion (Shrivastava et al., 2023; Ding et al., 2023), feature addition (Liang et al., 2024), or issue resolving (Jimenez et al., 2024). Recent benchmarks like SWE-Bench (Jimenez et al., 2024) have been proposed to evaluate LLMs on real-world GitHub issues. It requires LLMs to modify the repository to resolve the issue, either by fixing a bug or introducing a new feature. This task is particularly challenging because it requires navigating complex code bases, understanding intricate dependencies between code files, and ensuring that changes integrate seamlessly without introducing new issues, which highlights the difficulties in scaling from function-level to repository-level understanding, as expounded in Figure 1.\n\n\nA key step in addressing repository-level tasks is to understand the structure of a repository and identify related code. To achieve this, retrieval-augmented generation (RAG) and its variants (Zhang et al., 2023; Phan et al., 2024; Wu et al., 2024) have been leveraged, in a procedural manner, to retrieve relevant code files across the repository first, providing context for LLMs for further edition.\n\n\n(a) Function-level Coding Problem\n\n\nInput text: Write a python function to find the first repeated character in a given string.\n\n\n![](images/9883c20b91c9601cb769ab3107450abf2647de1e7bf46fa8a1a880fdccf8bb42.jpg)\n\n\n1. def first_repeated_char(str1):\n\n- 2. for index,c in enumerate(str1):\n\n- 3. if str1[:index+1].count(c) > 1:return c\n\n- 4. return \"None\"\n\n\n(b) Repository-level Coding Problem\n\n\nIssue: modeling's \"separability_matrix\" does not compute separability correctly for CompoundModels...\n\n\n![Figure 1: The illustration of (a) a function-level coding problem from HumanEval (Chen et al., 2021) and (b) a repository-level coding problem from SWE-Bench (Jimenez et al., 2024).](images/4211fe98f1c4cec12d3fb9abb19390e3a7b311d09f6bab3ef01c17d82545fe3a.jpg)\n\n\n(i) Understand intricate dependencies\n\n\n![](images/f1d49d72c356a5a629a423aa534583b49eaa79e7dd4dc8dd5b51e970f43c3a9e.jpg)\n\n\nGenerated patch:\n\n- diff --git a/astropy/modeling/separable.py\n\n- --- a/astropy/modeling/separable.py\n\n- $+ + +$  b/astropy/modeling/separable.py @@-242,7  $+ 2 4 2 , 7 \\text{\u5df2}$  def cstack(left, right):...\n\n\n![](images/e71c4face282d548b2b6dd6d7f72227649ae3e018e4af3e9fb3eedb3a11ef9b3.jpg)\n\n\nUnit test:\n\n\n<table><tr><td>test_coord_matrix</td><td>X</td></tr><tr><td>test_cdot</td><td>\u2713</td></tr><tr><td>test_arith_opera</td><td>X</td></tr></table>\n\n\n(iii) Resolve without introducing new issues\n\n\nHowever, indexing at file-level can only identify semantically similar but not genuinely related code snippets. Instead of using RAG, recent approaches like Agentless (Xia et al., 2024) construct a skeletal format for each file, and directly prompt LLMs to identify relevant files and code lines. However, this method still treats code repositories as flat documents (Zhang et al., 2024), which suffers from limitations of repository structure such as the intricate inter-dependencies cross files. An alternative approach is to design agent frameworks (Yang et al., 2024; Wang et al., 2024), which enables LLMs to interact with repositories using actions. While LLM agents can freely determine the next action based on current observations, without the grasp of global repository structures, they tend to focus narrowly on specific files, resulting in local optimums. Addressing these limitations requires going beyond semantic matching and developing techniques that enable a deeper understanding of the codebase structure. This will allow LLMs to leverage fine-grained context across multiple files and function calls, facilitating more informed, repository-wide decision-making for coding tasks.\n\n\nMotivated by this, we propose REPOGRAPH, a plug-in module designed to help LLMs-based AI programmers leverage the code structure of an entire repository. REPOGRAPH is a graph structure and operates at the line level, offering a more fine-grained approach compared to previous file-level browsing methods. Each node in the graph represents a line of code, and edges represent the dependencies of code definitions and references. REPOGRAPH is constructed via code line parsing and encodes the structured representation of the entire repository. Sub-graph retrieval algorithms are then used to extract ego-graphs, which represent the relationships of a central node (in our case, specific keywords). These ego-graphs can be smoothly integrated with both procedural and agent frameworks, offering key clues that provide a more comprehensive context for LLMs to solve real-world software engineering problems.\n\n\nTo assess REPOGRAPH's effectiveness and versatility as a plug-in module, we integrate it with four existing software engineering frameworks and evaluate its performance using SWE-bench, a recent benchmark for AI software engineering. Experiment results show that REPOGRAPH boosts the success rate of existing methods for both agent and procedural frameworks by achieving an average relative improvement of  $32.8\\%$ . We also test REPOGRAPH on CrossCodeEval to verify its transferability to general coding tasks that require repository-level code understanding. Additionally, we systematically analyze different sub-graph retrieval algorithms and integration methods. Together with error analyses, we hope to shed light on future works targeting modern AI software engineering.\n\n\n# 2 RELATED WORKS\n\n\n# 2.1 LLM-BASED METHODS FOR AI SOFTWARE ENGINEERING\n\n\nRecently, there has been a significant increase in research focused on AI-driven software engineering, which can be broadly categorized into two primary approaches: (i) LLM agent-based frameworks and (ii) SWE-featured procedural frameworks. While this field has advanced rapidly, with most methods being released as proprietary solutions for industry applications (Cognition, 2024), our related work section will concentrate specifically on open-source frameworks.\n\n\nLLM agent-based framework equips large language models (LLMs) with a set of predefined tools, allowing agents to iteratively and autonomously perform actions, observe feedback, and plan future steps (Yang et al., 2024; Zhang et al., 2024; Wang et al., 2024; Cognition, 2024). While the exact\n\n\nset of tools may vary across different agent frameworks, they typically include capabilities such as opening, writing, or creating files, searching for code lines, running tests, and executing shell commands. To solve a problem, agent-based approaches involve multiple actions, with each subsequent turn depending on the actions taken in previous ones and the feedback received from the environment. For example, SWE-agent (Yang et al., 2024) facilitates interactions with the execution environment by designing a special agent-computer interface (ACI). There are various actions, including \"search and navigation\", \"file viewer and editor\", and \"context management\". Another work, AutoCodeRover (Zhang et al., 2024), further offers fine-grained searching methods for LLM agents in better contexts without an execution process. Specifically, it supports class and function-level code search. OpenDevin (Wang et al., 2024), initiated after Devin (Cognition, 2024), is a community-driven platform that integrates widely used agent systems and benchmarks. The action space design of OpenDevin is highly flexible, requiring LLM agents to generate code on the fly.\n\n\nSWE-featured procedural frameworks typically follow a two-step Localize/Search-Edit approach, as seen in existing literature (Zhang et al., 2023; Wu et al., 2024; Liang et al., 2024; Xia et al., 2024). The localize step focuses on identifying relevant code snippets, while the edit step involves completing or revising the code. Some works introduce additional steps to further enhance performance, such as the Search-Expand-Edit approach (Phan et al., 2024). Retrieval (Lewis et al., 2020) is a popular technique used for localization, allowing models to search for relevant code snippets from large repositories by treating issue descriptions as queries and code snippets as indexed data. Some approaches use a sliding window to ensure completeness (Zhang et al., 2023). Besides, Agentless (Xia et al., 2024) is a recently developed method that uses LLMs to directly identify relevant elements for editing within code repositories. It first recursively traverses the repository structure to generate a format that aligns files and folders vertically, with indices for subdirectories. This structure and the issue description are then input into the LLM, which performs a hierarchical search to identify the top-ranked suspicious files requiring further inspection or modification.\n\n\n# 2.2 REPOSITORY-LEVEL CODING CAPABILITY\n\n\nThe evaluation of coding capabilities in AI systems has traditionally focused on function-level or line-level assessments (Lu et al., 2021; Chen et al., 2021; Austin et al., 2021), where individual code snippets or isolated functions are the primary units of analysis. Unlike previous studies, SWE-bench (Jimenez et al., 2024) highlights the trend of repository-level coding, driven by recent advances of coding-specific LLMs (Guo et al., 2024; Li et al., 2023b). It reflects the growing user demand to understand and contribute to entire projects rather than isolated functions (Ouyang et al., 2023), as well as\n\n\nsolving real-world problems in an end-to-end and automatic manner.\n\n\nTable 1: Comparison between our approach REPOGRAPH and existing methods for representing the repository on various aspects. *RepoUnderstander (Ma et al., 2024) and CodexGraph (Liu et al., 2024) are concurrent works to ours.\n\n\n<table><tr><td>Model</td><td>Line-level</td><td>File-level</td><td>Repo-level</td></tr><tr><td>DraCo</td><td>X</td><td>\u2713</td><td>X</td></tr><tr><td>Aider</td><td>\u2713</td><td>X</td><td>X</td></tr><tr><td>RepoUnderstander*</td><td>X</td><td>\u2713</td><td>\u2713</td></tr><tr><td>CodexGraph*</td><td>X</td><td>\u2713</td><td>\u2713</td></tr><tr><td>REPOGRAPH</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></table>\n\n\nLong before the LLM era, the software engineering community has been studying the navigation of code repositories, including using eye-tracking data (Busjahn et al., 2015) and exploring interconnections (Begel et al., 2010). Then pre-trained code LLMs incorporate repository-level information such as file dependencies. But tasks at the repository level often involve more intricate call relationships within their context. Recent works like RepoCoder (Zhang et al., 2023) and RepoFuse (Liang et al., 2024) have started integrating RAG modules to harness additional information from repositories. Building on this, subsequent research has focused on embedding repository-level context into their methodologies. For instance, DraCo (Cheng et al., 2024) introduces importing relationships between files, CodePlan maintains the code changes by LLMs with incremental dependency analysis, while Aider (Gauthier, 2024) employs PageRank (Page, 1999) to identify the most significant contextual elements. RepoUnderstander (Ma et al., 2024) and CodexGraph (Liu et al., 2024) model code files as a knowledge graph. Despite similarities in representation, methods vary in how they retrieve information from these structures and utilize it for downstream tasks. Table 1 summarizes the differences between these methods and REPOGRAPH. REPOGRAPH surpasses previous approaches by effectively integrating context at the line, file, and repository levels.\n\n\n![(a) RepoGraph Construction](images/2d58b3de9c36ef24e6e7b44b1bb321b3caa41bb05fa7cd8ef79b2a34d51bf7ac.jpg)\n\n\n![(c) Integration with Agent Frameworks Figure 2: An in-depth illustration of (a) the construction, (b) the integration with procedural frameworks, and (c) the integration with agent frameworks of REPOGRAPH. Given a code repository, we first utilize AST to construct  $\\mathcal{G} = \\{\\mathcal{V},\\mathcal{E}\\}$ , where  $\\mathcal{G}$  consists of \"reference\" and \"definition\" node,  $\\mathcal{E}$  includes \"invoke\" and \"contain\" relations (files and code lines shown in corresponding color). The constructed REPOGRAPH are then used in procedural frameworks by adding sub-retrieval results into each step, and agent frameworks by adding graph retrieval as an additional action \"search_repograph\". A simplified version can be found in Figure 10.](images/126123f3edba1131af28c26f023bf724a182b63f285c8a33a79fc30aa0bf46e2.jpg)\n\n\n# 3 REPOGRAPH\n\n\nThis section introduces REPOGRAPH, a novel plug-in module that can be seamlessly integrated into existing research workflows for both agent-based and procedural frameworks. The primary goal of REPOGRAPH is to provide a structured way to analyze and interact with complex codebases, enabling detailed tracing of code dependencies, execution flow, and structural relationships across the repository. In the following sections, we will provide a detailed description of REPOGRAPH's construction, its underlying representation, and its utility across various scenarios. The overall architecture is depicted in Figure 2, highlighting its key components and operational flow.\n\n\n# 3.1 CONSTRUCTION\n\n\nGiven a repository-level coding task, the first step is to carefully examine the repository structure so that the necessary information can be collected. The input for REPOGRAPH construction is a repository, i.e., a collection of its folders and files, while the output is a structured graph, where each node is a code line, and each edge represents the dependencies in between. REPOGRAPH enables tracing back to the root cause of the current issue and gathering dependent code context to help solve the problem. The construction process of REPOGRAPH could be divided into three key steps.\n\n\nStep 1: Code line parsing. We first traverse the entire repository using a top-down approach to identify all code files as candidates for next-step parsing. This is accomplished by filtering based on file extensions, retaining only those with relevant code file suffixes (e.g., .py) while excluding other file types (e.g., .git or requirements.txt), which might be noisy and irrelevant for coding tasks. For each code file, we utilize tree-sitter  ${}^{2}$  to parse the code,leveraging its Abstract Syntax Tree (AST) framework. The AST provides a tree-based representation of the abstract syntactic structure of the source code, enabling the identification of key elements such as functions, classes, variables, types, and other definitions. While recognizing these definitions is crucial, tracing their usage and references throughout the code is equally important. Tree-sitter facilitates this by capturing the definitions and tracking where they are utilized or referenced within the codebase. For example, in figure 2, we not only identify definitions like class Model and its inherent methods but also references like self._validate_input.units(). After processing each line of code with a tree-sitter, we selectively retain lines that involve function calls and dependency relations, discarding extraneous information. Our focus is primarily on the functions and classes, as these represent the core structural components of\n\n\nthe code. By concentrating on these elements and their interrelationships, REPOGRAPH optimizes the analysis process by excluding less significant details, such as individual variables, which tend to be redundant and less relevant for further processing.\n\n\nStep 2: Project-dependent relation filtering. After the previous parsing step, we obtain code lines with calling and dependency relations. However, not all relations are useful for fixing issues. Specifically, many default and built-in function/class calls could distract from the project-related ones. Therefore, we additionally introduce a filtering process that excludes the repository-independent relations. Two types of such relations exist: (i) global relation refers to Python standard and built-in functions and classes. (ii) local relation are introduced by third-party libraries, which are specific to the current code file. For global relations, we maintain a comprehensive list of methods from standard and built-in libraries, excluding any identified relations from this list. The list is empirically constructed by gathering methods of the builtins library and default methods such as \"list\" and \"tuple\". For example, in figure 2, line inputs = len(input) is excluded since \"len\" is a default method. For local relations, we parse import statements in the code to identify third-party methods that are included, and exclude them accordingly. A detailed illustration of this step can be found in Figure 11.\n\n\nStep 3: Graph organization. At this stage, we construct REPOGRAPH using code lines as the fundamental building units. The graph can be represented as  $\\mathcal{G} = \\{\\mathcal{V},\\mathcal{E}\\}$ , where  $\\mathcal{V}$  represents the set of nodes, with each node corresponding to a line of code, and  $\\mathcal{E}$  represents the set of edges, capturing the relationships between these code lines. Each node in  $\\mathcal{V}$  contains attributes to represent its meta-information, such as line_number, file_name, directory, etc. Additionally, we classify each code line as either a \"definition\" (def) or a \"reference\" (ref) to a particular module. A \"def\" node corresponds to the line where a function, class, or variable is initially defined, while a \"ref\" node indicates a code line where this entity is referenced or invoked elsewhere in the code. Similar to soft links to \"def\" nodes, \"ref\" nodes also represent other variations of invoking methods. For example, in Figure 2, the class definition \"class Model\" would be a \"def\" node, while any subsequent usages of Model would be \"ref\" nodes. Each \"def\" node may have multiple \"ref\" nodes associated with it, as a single function or class can be referenced in various places throughout the code. We define two types of edges:  $\\mathcal{E}_{\\text{invoke}}$  and  $\\mathcal{E}_{\\text{contain}}$ . The triple  $(\\mathcal{V}_1,\\mathcal{E}_{\\text{contain}},\\mathcal{V}_2)$  denotes that  $\\mathcal{V}_1$  (e.g., a function definition) contains another module  $\\mathcal{V}_2$  (e.g., an internal function or class). The edge  $\\mathcal{E}_{\\text{contain}}$  typically connects a \"def\" node to its internal components. In contrast,  $\\mathcal{E}_{\\text{invoke}}$  represents an invocation relationship, usually connecting a \"def\" node to a \"ref\" node, where the reference node includes a dependency on the definition.\n\n\n# 3.2 UTILITY\n\n\nThe constructed REPOGRAPH serves as a structured representation of the current repository and facilitates better-related information collection and aggregation. For information collection based on REPOGRAPH, specifically, we use one search term each time for subgraph retrieval. Search terms are the key functions or classes that are determined by current states. For example, \"separability_matrix\" is the initial search term in Figure 2(c). We retrieve the  $k$ -hop ego-graphs (Hu et al., 2024) with the search term in the centric. The ego-graph is crucial for solving the problem because it focuses on the immediate relationships (Jin et al., 2024) around the search term, capturing the relevant dependencies and interactions within the repository, which is key to understanding the functional context. Additionally, the retrieved content explicitly contains information at both the method and line levels and implicitly expresses the grouping at the file level.\n\n\nThis process is abstracted via search_repograph() as illustrated in the middle of Figure 2. The retrieved  $k$ -hop ego-graph will be flattened for further processing. We also tried other variants for integration later in Section 5.2 and their performance in Table 4. We narrate how REPOGRAPH could be plugged in with existing representative research lines in the following.\n\n\nIntegration with procedural framework. In a procedural framework, LLMs are usually prompted in \"localization\" and \"editing\" stages with the given repository context and issue description. In this case, we use search_repograph() before both stages, leveraging our REPOGRAPH to assist in making more informed decisions at each step. For example, in Figure 2, we first include the subgraph of \"separability_matrix\" for localization, and then use the localized result \"Model\" to search in the\n\n\nedition stage. To implement the strategy, we flatten the context of retrieved ego-graphs and append it as part of the prompt. As a result, the LLM generation is conditioned on both retrieved ego-graphs and the context provided by baseline methods, helping the model preserve nuances.\n\n\nIntegration with agent framework. A significant difference in existing agent frameworks is the action space design, as expounded in Section 2. To leverage the power of REPOGRAPH, we put search_repograph() as an additional action in the action space. The agent decides when and where to use this action. The search term is also determined by the agent accordingly. The returned subgraph will be flattened and used as an observation for the next state.\n\n\n# 4 EXPERIMENTS\n\n\n# 4.1 SETUP\n\n\nWe evaluated REPOGRAPH as a plug-in component, i.e., integrated into existing baseline models of the two aforementioned research lines to assess its performance. We use the same baseline settings and configurations when incorporating REPOGRAPH to ensure a fair comparison.\n\n\nDataset. We test REPOGRAPH in SWE-bench-Lite<sup>3</sup>. Each problem in the dataset requires submitting a patch to solve the underlying issue described in the input issue description. The goal is to generate a patch that accurately revises the relevant portions of the codebase within the repository, ensuring that all test scripts included in the dataset are successfully executed.\n\n\nBaselines. We integrate REPOGRAPH with representative methods from both aforementioned research lines. (i) For procedural frameworks, we evaluate the widely used traditional method, RAG (Lewis et al., 2020), as well as Agentless (Xia et al., 2024), an open-source state-of-the-art approach in this direction. For RAG, we follow its initial setting and use BM25 for file-level retrieval. After that, we append the context of REPOGRAPH after the code files as part of the prompt. Agentless first performs a hierarchical localization in terms of \"file-class/function- edits\" and then conducts repair based on localization. The context of REPOGRAPH is inserted in every step of Agentless. (ii) For agent frameworks, we consider SWE-agent (Yang et al., 2024) and AutoCodeRover (Zhang et al., 2024). For both frameworks, we add an additional action \"search_repograph\" for the LLM agent as described in Section 3. All the choices in the two research lines incorporate GPT-4 and GPT-4o based methods to ensure the generalization. Detailed implementations and prompts used can be found in Appendix A.\n\n\nEvaluation metrics. We evaluate all methods across two key dimensions: Accuracy and Average Cost. (i) For accuracy, we report the resolve rate and patch application rate. The resolve rate represents the percentage of issues successfully resolved across all data points. An issue is considered resolved if the submitted patch passes all test scripts. To assess the patch application rate, we attempt to apply the generated patches to the repository using the patch program, counting only successful applications toward this metric. (ii) To evaluate cost efficiency, we report two metrics: average cost and average tokens, which refer to the inference cost and the number of input/output tokens used when querying the LLMs, respectively.\n\n\nConfigurations. We use the same GPT version as in the baselines in the experiments. We used GPT-4o (2024-05-13) and GPT-4-Turbo (gpt-4-1106-preview) from OpenAI for evaluation and analyses in our experiments. All evaluation processes are performed in a containerized Docker environment  $^{4}$ , ensuring stability and reproducibility, made possible through contributions from the open-source community. For plug-ins with procedural frameworks, it usually takes around 2-3 hours to finish. For agent frameworks, the inference time is larger, up to around 10 hours.\n\n\n# 4.2 EXPERIMENT RESULTS\n\n\nTable 2 presents the main evaluation results of all baseline methods and the corresponding performance with REPOGRAPH (+REPOGRAPH) as a plug-in in the SWE-bench-Lite test\n\n\nTable 2: Results of REPOGRAPH with open-source baselines in two research lines, including procedural and agent frameworks. Numbers of accuracy-related metrics are directly taken from the leaderboard, while the cost-related ones are computed from the corresponding trajectories<sup>6</sup>.\n\n\n<table><tr><td>Methods</td><td>LLM</td><td>resolve</td><td>Accuracy # samples</td><td>patch apply</td><td>Avg. Cost $ cost</td><td># tokens</td></tr><tr><td colspan=\"7\">Procedural frameworks</td></tr><tr><td>RAG</td><td>GPT-4</td><td>2.67</td><td>8</td><td>29.33</td><td>$0.13</td><td>11,736</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>5.33\u21912.66</td><td>16\u21918</td><td>47.67\u219118.34</td><td>$0.17</td><td>15,439</td></tr><tr><td>Agentless</td><td>GPT-4o</td><td>27.33</td><td>82</td><td>97.33</td><td>$0.34</td><td>42,376</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>29.67\u21912.34</td><td>89\u21917</td><td>98.00\u21910.67</td><td>$0.39</td><td>47,323</td></tr><tr><td colspan=\"7\">Agent frameworks</td></tr><tr><td>AutoCodeRover</td><td>GPT-4</td><td>19.00</td><td>57</td><td>83.00</td><td>$0.45</td><td>38,663</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>21.33\u21912.33</td><td>64\u21917</td><td>86.67\u21913.67</td><td>$0.58</td><td>45,112</td></tr><tr><td>SWE-agent</td><td>GPT-4o</td><td>18.33</td><td>55</td><td>87.00</td><td>$2.53</td><td>498,346</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>20.33\u21912.00</td><td>61\u21916</td><td>90.33\u21913.33</td><td>$2.69</td><td>518,792</td></tr></table>\n\n\nset. We also report the number of correct samples for each method. The performance increase is marked by  $\\uparrow$ num. We also tested other (open-source) LLMs using Claude-3.5-Sonnet (claude-3-5-sonnet-20240620) (Anthropic, 2024) from Anthropic in Appendix C. Based on the results, we have the following key observations:\n\n- (i) REPOGRAPH brings consistent performance gain for all combinations of frameworks and LLM model bases. Specifically, REPOGRAPH achieves an absolute improvement of  $+2.66$  and  $+2.34$  in terms of the resolve rate for RAG and Agentless, respectively, which is  $99.63\\%$  and  $8.56\\%$  of relative improvement. The notable improvement demonstrates the effectiveness of our REPOGRAPH in adapting to various scenarios by inducing relevant code context and performing precise code editorings. Additionally, our best performance so far by plugging in Agentless, 29.67, achieves the state-of-the-art performance on the benchmark among all open-source methods.\n\n- (ii) Performance gain brought by REPOGRAPH is slightly larger on procedural frameworks than agent ones. With procedural frameworks, REPOGRAPH correctly fixes more issues than agent ones. This could be due to two primary reasons. Firstly, we observed that mature procedural frameworks tend to achieve better baseline performance than agent-based frameworks on SWE-bench. The initial definitive nature of procedural frameworks, with their well-defined running flow and structure, allows them to leverage plug-ins more effectively. Another reason is that this deterministic behavior reduces the complexity that arises from dynamic decision-making, a key characteristic of agent-based systems, thereby enabling a smoother integration of performance improvements.\n\n- (iii) Performance gain brought by REPOGRAPH does not rely on more costs. We also compute and report each method's average cost and token consumption. By introducing REPOGRAPH, we manage to reduce the costs associated with managing the entire repository while achieving comparable or even superior performance. As shown in Table 2, the additional token cost introduced by RepoGraph is justified given the significant performance improvement it enables, particularly when compared to the substantially higher costs incurred by previous methods like SWE-agent. This indicates REPOGRAPH's performance gains are not mainly due to increased token usage.\n\n- (iv) Average costs are generally larger on agent frameworks with REPOGRAPH. This phenomenon is especially obvious with SWE-agent, as it allows the agent to freely determine the next action based on the current observation. We also found that the integration with agent frameworks usually leads to larger cost increases, as exemplified by  $+0.13$  and  $+0.18$  with AutoCodeRover and SWE-agent, respectively. The reason lies in the large exploration space in agent frameworks. The agents might call the search_repograph() action many times, which leads to the explosion of prompt contexts. We encourage users to be mindful of cost and to adopt a more granular approach to cost control when integrating REPOGRAPH into the agent framework in the future.\n\n\n# 5 ANALYSIS\n\n\nThis section presents a detailed analysis to demonstrate that the additional context provided by REPOGRAPH is beneficial for the task. We begin by analyzing localization accuracy in comparison to the gold-standard patch. Next, we explore various REPOGRAPH configurations, focusing on how the additional context can be effectively integrated into the existing system. Finally, we perform an in-depth error analysis, highlighting aspects where REPOGRAPH can be further improved. For more analyses including resolve rate in various aspects and action distributions of agent frameworks, please refer to Appendices C.\n\n\n# 5.1 LOCALIZATION COVERAGE\n\n\nA crucial step in issue resolution is accurately identifying the correct locations within the code that require modification. Proper localization is essential, as it forms the foundation for generating an effective and accurate patch. Without this step, the quality of the fix may be compromised, leading to incomplete or incorrect solutions. In three granularity, we compute the percentage of problems where the edit locations match the ground truth patch. Namely, file-level, function-level, and line-level. We report that a patch con\n\n\nTable 3: Percentage of problems for accurate edition localizations with respect to file, function, and line levels. All the numbers are computed from the corresponding generated patches.\n\n\n<table><tr><td>Methods</td><td>LLM</td><td>file</td><td>function</td><td>line</td></tr><tr><td>RAG</td><td>GPT-4</td><td>47.3</td><td>23.3</td><td>12.7</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>51.7\u21914.4</td><td>25.3\u21912.0</td><td>14.3\u21911.6</td></tr><tr><td>Agentless</td><td>GPT-4o</td><td>68.7</td><td>51.0</td><td>34.3</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>74.3\u21915.6</td><td>54.0\u21913.0</td><td>36.7\u21912.4</td></tr><tr><td colspan=\"5\">Agent frameworks</td></tr><tr><td>AutoCodeRover</td><td>GPT-4</td><td>62.3</td><td>42.3</td><td>29.0</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>69.0\u21914.7</td><td>45.7\u21913.4</td><td>31.7\u21912.7</td></tr><tr><td>SWE-agent</td><td>GPT-4o</td><td>61.7</td><td>46.3</td><td>32.3</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>67.3\u21915.6</td><td>49.3\u21913.0</td><td>35.0\u21912.7</td></tr></table>\n\n\ntains the correct location if it edits a superset of all locations in the ground truth patch.\n\n\nTable 3 presents the results of our analysis. We observed that integrating REPOGRAPH with all baseline methods significantly improves file-level accuracy, whereas the enhancement of accuracy in line-level is comparatively modest. This result aligns with our expectations, as file-level localization is the most coarse-grained, making it inherently easier to improve. In contrast, line-level localization, being the most fine-grained, poses a greater challenge due to its need for more precise identification of code segments. Additionally, we found that although line-level accuracy improvements are more pronounced for agent frameworks, their overall resolve rate is lower than that of procedural frameworks, as shown in Table 2. This discrepancy can be attributed to the fact that localization, while necessary for generating a final patch, is insufficient. The success of the final revision still heavily relies on the underlying capabilities of LLMs. Agent frameworks, designed to operate in a trial-and-error fashion, are particularly susceptible to error accumulation. As these frameworks iteratively refine their patches, small inaccuracies in earlier localization steps can propagate and magnify throughout the process, ultimately reducing the overall resolve rate. Procedural frameworks, on the other hand, follow a more structured and deterministic approach to localization and patch generation. They typically localize and fix issues in a single, more direct step, which can help mitigate the compounding of errors.\n\n\n# 5.2 INVESTIGATION OF REPOGRAPH VARIANTS\n\n\nIn this section, we investigate the efficacy of various combinations of sub-graph retrieval and integration techniques as outlined in Section 3. We explore two sub-graph retrieval variants and two integration methods. Specifically, for sub-graph retrieval, we index  $k$ -hop ego-graphs where  $k$  is set to 1 and 2. We limit our exploration to  $k$  values up to 2 due to the extensive context required for integration and the potential introduction of noise or irrelevant nodes for  $k \\geq 3$ . For the integration methods, we employ two distinct approaches: (i) directly flattening the textual sub-graph by explicitly detailing the relationships between the search term and its neighboring nodes, and (ii) leveraging an LLM first to summarize the sub-graph in terms of the core modules and salient dependencies, before proceeding with further processing. Detailed implementations of these variants and prompts used can be found in Appendix B.1.\n\n\nTable 4: The number of nodes, edges, and tokens of REPOGRAPH and its variants. For different retrieval and integration variants, we report the average number on the test set. \"summ.\" refers to the summarized version by LLMs of the retrieved ego-graph.\n\n\n<table><tr><td>Metrics</td><td>REPOGRAPH</td><td>1-hop + flatten</td><td>1-hop + summ.</td><td>2-hop + flatten</td><td>2-hop + summ.</td></tr><tr><td># Nodes</td><td>1419.3</td><td>11.6</td><td>11.6</td><td>54.5</td><td>54.5</td></tr><tr><td># Edges</td><td>26392.1</td><td>37.1</td><td>37.1</td><td>89.9</td><td>89.9</td></tr><tr><td># tokens</td><td>-</td><td>2310.7</td><td>717.5</td><td>10505.3</td><td>1229.2</td></tr><tr><td>resolve rate</td><td>-</td><td>29.67</td><td>28.33</td><td>26.00</td><td>28.67</td></tr></table>\n\n\nWe begin by presenting some statistics for REPOGRAPH and its various configurations. Performance evaluations are conducted on Agentless with REPOGRAPH integrated as a plug-in module. Table 4 reports the number of nodes and edges within the (sub)-graphs. Notably, the average number of nodes and edges for REPOGRAPH across the SWE-bench dataset is quite substantial, featuring over 1,000 nodes and 25,000 edges. This highlights the comprehensive nature of the constructed structure. For the different variants, when  $k = 1$ , the information within REPOGRAPH is concentrated around the search term, resulting in an average of 11.6 nodes and 37.1 edges. As  $k$  increases to 2, the retrieved ego-graph expands exponentially, reaching an average of 54.5 nodes and 89.9 edges. Moreover, directly flattening the retrieved ego-graph often significantly increases the token count, frequently reaching several thousand tokens. However, utilizing an LLM as an additional summarizer greatly reduces the token count, typically around a thousand.\n\n\nTable 4 presents the resolve rates for four variants of our method. Notably, while the 2-hop variant incorporates additional information, directly flattening this information results in the poorest performance, with a resolve rate of  $26.00\\%$ , even lower than the original baseline. In contrast, incorporating summarization via the LLM significantly alleviates context length constraints and enhances information organization, thereby improving performance. For the 1-hop variant, however, we observed that adding summarization actually degrades performance, reducing the resolve rate to  $28.33\\%$ . We hypothesize that this occurs because the flat 1-hop ego-graph already contains comprehensive information that fits within the LLM's context window; thus, summarization may introduce inevitable information loss.\n\n\n# 5.3 TRANSFERIBILITY TEST\n\n\nTo demonstrate the representational power of REPOGRAPH for repositories and its transferability to tasks requiring an understanding of repository structures, we conducted experiments using the CrossCodeEval benchmark (Ding et al., 2023). CrossCodeEval is a multilingual code completion benchmark that is derived from real-world repositories. It is designed to emphasize numerous cross-file dependencies. We focus on problems using Python as the evaluation programming lan\n\n\nTable 5: Results on the subset of CrossCodeEval with GPT-4o and Deepseek-Coder-V2-LiteInstruct as the backbone LLMs.\n\n\n<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">Code Match</td><td colspan=\"2\">Identifier Match</td></tr><tr><td>EM</td><td>ES</td><td>EM</td><td>F1</td></tr><tr><td>Deepseek-Coder</td><td>10.2</td><td>57.3</td><td>16.6</td><td>49.1</td></tr><tr><td>+REPOGRAPH</td><td>19.7</td><td>67.8</td><td>29.3</td><td>58.9</td></tr><tr><td>GPT-4o</td><td>10.5</td><td>59.6</td><td>16.8</td><td>47.9</td></tr><tr><td>+REPOGRAPH</td><td>28.7</td><td>68.9</td><td>36.0</td><td>61.3</td></tr></table>\n\n\nguage, leading to 2,665 samples. For evaluation metrics, we follow the settings in the original paper and measure performance with code match and identifier match metrics, assessing accuracy with exact match (EM), edit similarity (ES), and F1 scores. In our experiments, the search terms are determined to be the function within which the current line is to be completed.\n\n\nTable 5 demonstrates the results on CrossCodeEval using GPT-4o and Deepseek-Coder as the backbone language model. The original LLMs struggles with repository-level tasks, evidenced by an EM score of only 10.8 for code matching and 16.7 for identifier matching. These results indicate significant limitations in handling code structure and variable usage in a broader repository context. However, with the integration of the REPOGRAPH method, there is a substantial improvement across all metrics with both LLMs. Particularly, we find that the improvement brought by REPOGRAPH is more pronounced when integrated with GPT-4o. A potential reason is that GPT-4o is better at inter\n\n\n![Figure 3: Venn diagram of REPOGRAPH and baselines on (a) procedural framework and (b) agent framework on SWE-Bench-Lite. We also plot the error distribution of failing cases against counterparts, e.g., detailed error distribution of 12 cases REPOGRAPH succeeds while Agentless fails.](images/8593889f7523e5c7cb665d49567ce8b44e18922f502c7e8a49cd0937fbf90ccd.jpg)\n\n\npreting the additional context information provided for better code reasoning. These improvements suggest that incorporating repository-level knowledge, as facilitated by REPOGRAPH, greatly enhances the model's ability to understand and generate more contextually accurate and semantically consistent code.\n\n\n# 5.4 ERROR ANALYSIS\n\n\nWe want to compare REPOGRAPH with the corresponding baselines to see the distribution of resolved cases and analyze the error reasons for unresolved cases. We plot a Venn diagram for representative methods in both procedural and agent frameworks in Figure 3, respectively. We manually examined all the unique error cases and defined three error categories. (i) Incorrect localization refers to the failure in accurately identifying code snippets, (ii) contextual misalignment happens when the generated patch fails to align with the broader context of the codebase, and (iii) regressive fix introduces new issues in resolving the original issues. More examples are in Appendix D.\n\n\nWe found that the improvement in agent frameworks is more complementary than procedural frameworks, with larger uniquely resolved cases of 22 compared with 12 in procedural frameworks. Together, they make even larger performance of  $31.33\\%$  and  $22.33\\%$ . The reason could also be attributed to the determinism of procedural frameworks. As a plug-in module, REPOGRAPH tends to make modifications on existing deterministic processes, resulting in larger overlaps in resolved issues compared with baselines. Agent frameworks, on the other hand, have quite different action distributions with REPOGRAPH as a plug-in (please refer to Appendix A.2). Therefore, the uniquely resolved cases are more compared with procedural frameworks. For error distributions, contextual misalignment is the most prevalent error type, followed by incorrect localization and regressive fixes for all methods, suggesting that while localization is often correct, the applied solutions may regress or fail to integrate contextually. The phenomenon also echoes with conclusion obtained in Section 5.1. This is intuitive as all the existing methods focus on providing a more comprehensive and desired context for LLMs to solve the task, which fundamentally depends on the power of backbone LLMs. We also found that when integrated with REPOGRAPH, the proportion of all three error types decreases (specifically for \"incorrect localization\"), indicating that REPOGRAPH is specifically useful in aggregating the related contexts of the current to-be-fixed issue.\n\n\n# 6 CONCLUSION AND DISCUSSION\n\n\nThis paper introduces REPOGRAPH, a plug-in module for modern AI software engineering. REPOGRAPH operates at the code-line level and offers desired navigation through code bases by incorporating and aggregating information at the line level, file level, and repository level through subgraph retrieval of ego-graphs. Extensive experiments on real-world SWE tasks show that REPOGRAPH significantly boosts the performance of both procedural and agent frameworks. In addition, REPOGRAPH is proved to be smoothly and effectively transferred to other tasks that require repository-level understanding. Future work could further investigate the generalizability of REPOGRAPH across different programming languages, frameworks, and developer environments. Real-time execution and feedback could also be explored to further enhance REPOGRAPH towards a dynamic version for better AI software engineering abilities.\n\n\n# REFERENCES\n\n- Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pittler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan First, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv.2312.11805.\n\n- Anthropic. Introducing claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. Accessed: 2024-10-01.\n\n- Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n\n- Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675-698, 2024.\n\n- Andrew Begel, Yit Phang Khoo, and Thomas Zimmermann. Codebook: discovering and exploiting relationships in software repositories. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1, ICSE '10, pp. 125-134, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781605587196. doi: 10.1145/1806799.1806821. URL https://doi.org/10.1145/1806799.1806821.\n\n- Teresa Busjahn, Roman Bednarik, Andrew Begel, Martha E. Crosby, James H. Paterson, Carsten Schulte, Bonita Sharif, and Sascha Tamm. Eye movements in code reading: relaxing the linear order. In Andrea De Lucia, Christian Bird, and Rocco Oliveto (eds.), Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension, ICPC 2015, Florence/Firenze, Italy, May 16-24, 2015, pp. 255-265. IEEE Computer Society, 2015. doi: 10.1109/ICPC.2015.36. URL https://doi.org/10.1109/ICPC.2015.36.\n\n- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fiotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.\n\n- Wei Cheng, Yuhan Wu, and Wei Hu. Dataflow-guided retrieval augmentation for repository-level code completion. In ACL, 2024.\n\n- Cognition. Devin, 2024. URL https://www.cognition.ai/blog/introducing-devin.\n\n- Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang. Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=wgDcbBMSfh.\n\n- Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aur\u00e9lien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi\u00e8re, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gr\u00e9goire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Ko-revaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783.\n\n- Angela Fan, Belize Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M Zhang. Large language models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FOSE), pp. 31-53. IEEE, 2023.\n\n- Paul Gauthier. Aider is ai pair programming in your terminal. https://aider chat/, 2024.\n\n- Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming - the rise of code intelligence, 2024. URL https://arxiv.org/abs/2401.14196.\n\n- Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: A systematic literature review. arXiv preprint arXiv:2308.10620, 2023.\n\n- Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao. GRAG: graph retrieval-augmented generation. CoRR, abs/2405.16506, 2024. doi: 10.48550/ARXIV.2405.16506. URL https://doi.org/10.48550/arXiv.2405.16506.\n\n- Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66.\n\n- Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, and Jiawei Han. Graph chain-of-thought: Augmenting large language models by reasoning on graphs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 163-184, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024-findings-acl.11. URL https://aclanthology.org/2024.findings-acl.11.\n\n- Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 9459-9474, 2020.\n\n- Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee,\n\n- Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason T. Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! Trans. Mach. Learn. Res., 2023, 2023a. URL https://openreview.net/forum?id=KoFOg41haE.\n\n- Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023b.\n\n- Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng Di, Hongwei Chen, Chengpeng Wang, Gang Fan, et al. Repofuse: Repository-level code completion with fused dual context. arXiv preprint arXiv:2402.14323, 2024.\n\n- Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Wenmeng Zhou, Fei Wang, and Michael Shieh. Codexgraph: Bridging large language models and code repositories via code graph databases. arXiv preprint arXiv:2408.03910, 2024.\n\n- Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code understanding and generation. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/ hash/c16a5320fa475530d9583c34fd356ef5-Abstraction-round1.html.\n\n- Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. How to understand whole software repository? arXiv preprint arXiv:2406.01422, 2024.\n\n- OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.\n\n- Siru Ouyang, Shuohang Wang, Yang Liu, Ming Zhong, Yizhu Jiao, Dan Iter, Reid Pryzant, Chenguang Zhu, Heng Ji, and Jiawei Han. The shifted and the overlooked: A task-oriented investigation of user-GPT interactions. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 2375-2393, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.146. URL https://aclanthology.org/2023.emnlp-main.146.\n\n- L Page. The page-rank citation ranking: Bringing order to the web, 1999.\n\n- Huy N Phan, Hoang N Phan, Tien N Nguyen, and Nghi DQ Bui. Repohyper: Better context retrieval is all you need for repository-level code completion. arXiv preprint arXiv:2403.06095, 2024.\n\n- Baptiste Rozi\u00e8re, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised translation of programming languages. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/black/ed23fbf18c2cd35f8c7f8de44f85c08d-AAbstract.html.\n\n- Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and\n\n- Gabriel Synnaeve. Code llama: Open foundation models for code. CoRR, abs/2308.12950, 2023. doi: 10.48550/ARXIV.2308.12950. URL https://doi.org/10.48550/arXiv.2308.12950.\n\n- Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. Repository-level prompt generation for large language models of code. In International Conference on Machine Learning, pp. 31693-31715. PMLR, 2023.\n\n- Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. An empirical study on learning bug-fixing patches in the wild via neural machine translation. ACM Transactions on Software Engineering and Methodology (TOSEM), 28(4):1-29, 2019.\n\n- Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024.\n\n- Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, and Xiaofei Ma. Repoformer: Selective retrieval for repository-level code completion. arXiv preprint arXiv:2403.10059, 2024.\n\n- Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024.\n\n- John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024.\n\n- Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. Coacor: Code annotation for code retrieval with reinforcement learning. In The world wide web conference, pp. 2203-2214, 2019.\n\n- Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repeditor: Repository-level code completion through iterative retrieval and generation. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=q09vTY1Cqh.\n\n- Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement, 2024.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638427.0264428", "msecs": "26.0", "relativeCreated": "23856.940269470215", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# ZipIt! Merging Models from Different Tasks without Training\n\n# Abstract\nTypical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then averages them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce \"ZipIt!\", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to allow for merging features within each model by defining a general \"zip\" operation. Second, we add support for partially zipping the models up until a specified layer, naturally creating a multi-head model. We find that these two changes combined account for 20-60% improvement over prior work, making it more feasible to merge models trained on disjoint tasks without retraining.\n\n\n# 1 INTRODUCTION\n\n\nEver since AlexNet (Krizhevsky et al., 2017) popularized deep learning in computer vision, the field has thrived under the reign of massive models with an ever increasing number of parameters. Many vision problems once considered difficult or impossible are now benchmark tasks: classification with tens of thousands of classes (Deng et al., 2009; Zhou et al., 2017; Gemmeke et al., 2017), fast instance segmentation (He et al., 2017; Bolya et al., 2019), realistic image generation (Karras et al., 2018; Ho et al., 2020; Rombach et al., 2022), and more.\n\n\nThere are an abundance of independent, carefully tuned models out there for many tasks. However, if we want to expand an existing model's capabilities, we run into many potential issues. If we try training the model on an additional task, we face catastrophic forgetting (Kirkpatrick et al., 2017; Li & Hoiem, 2017; De Lange et al., 2021). If we evaluate the same model on different data without adaptation, we often find it doesn't generalize to out of domain samples (Blanchard et al., 2011; Muandet et al., 2013; Wang et al., 2022). We can try so called \"intervention\" strategies (Wang et al., 2022; De Lange et al., 2021) to mitigate these effects, but these often require further training which can be expensive (Dosovitskiy et al., 2020; Zhai et al., 2022; Dehghani et al., 2023). Instead, it would be nice if we could expand a model's capacity to solve new tasks by simply \"zipping\" it with other models trained on those tasks without additional training.\n\n\nCombining multiple models into one has recently started to gain traction in the vision community. Model Soups (Wortsman et al., 2022a) can add multiple models finetuned from the same pretrained initialization to improve accuracy and robustness. Git Re-Basin (Ainsworth et al., 2022) generalizes further to models trained on the same data but with different initializations, though with a significant accuracy drop. REPAIR (Jordan et al., 2022) improves on Git Re-Basin by adding new parameters and adjusting model batch norms where applicable. However, all of these methods only combine models trained on the same task. In this paper, we take this line of work to a logical extreme: merging differently initialized models trained on completely separate tasks (see Fig. 1ab). We show that this is an incredibly difficult problem for prior work and employ two simple strategies to make it feasible.\n\n\n![Figure 1: Setting and ZipIt! (a) Prior work merges differently initialized models from the same dataset with the same label sets: e.g., merging two models both trained to classify dog breeds. (b) Our setting expands this to merging models from different datasets with different label sets: e.g., merging a model that classifies dog breeds with one that classifies bird species. (c) ZipIt! merges these models without retraining by identifying shared features.](images/6e7a6073a90b331ae60be0db208311215cf0b4c1495e1345205151b5a84237ac.jpg)\n\n\n![](images/f47160305d6d399085e0d2019bdb58d4549ce0632ac6011aca456837e3d9780f.jpg)\n\n\n![](images/84d3f8e963bef0f726c3823d5cf2aa41f2c25da9c5dadea3cb66554433037972.jpg)\n\n\nFirst, we note that prior work focuses on permuting one model to the other when merging them. This creates a 1-1 mapping between the two models, inherently assuming that most features across them are correlated. Since this isn't necessarily the case for models trained on different tasks, we cannot rely on permutation alone. Instead, we generalize model merging to support \"zipping\" any combination of correlated features within and across each model. We find that on some tasks, this alone improves accuracy by up to  $20\\%$  vs. permutation-based approaches. Moreover, we prove that merging within models can yield a better result in the theoretical setting of Entezari et al. (2021).\n\n\nSecond, existing methods merge the entire network. While this might work for extremely similar models trained in the same setting, the features of models trained on disjoint tasks become less correlated over the course of the network (Kornblith et al., 2019). To solve this, we introduce partial zipping, where we only \"zip\" up to a specified layer. Afterward, we feed the merged model's outputs to the remaining unmerged layers of the original networks, creating a multi-head model. Depending on task difficulty, this can improve accuracy by over  $15\\%$  while still keeping most layers merged.\n\n\nIncorporating both of these strategies, we introduce ZipIt! (Fig. 1c), a general method for \"zipping\" any number of models trained on different tasks into a single multitask model without retraining. By deriving a general graph-based algorithm for merging and unmerging (Sec. 4), we can zip models of the same architecture together, merge features within each model, and partially zip them to create a multi-task model. We validate our approach by merging models trained on entirely disjoint sets of CIFAR (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009) categories, as well as merging several models trained on completely independent datasets into one, significantly outperforming prior work (Sec. 5). Finally, we ablate and analyze our method's capabilities on these scenarios (Sec. 6).\n\n\n# 2 RELATED WORK\n\n\nModel merging combines the weights of two or more models into a one. Our work differs from prior work in that we adapt mode connectivity techniques to target models trained on disjoint tasks (Fig. 1).\n\n\nMerging Finetuned Models. If two models are finetuned from the same pretrained checkpoint, they often lie in the same error basin (Neyshabur et al., 2020). Several works (Huang et al., 2017; Izmailov et al., 2018; Von Oswald et al., 2020; Wortzman et al., 2022b; Ilharco et al., 2022b; Don-Yehiya et al., 2023) have exploited this property to average together the weights of a model at different stages of training. Tarvainen & Valpola (2017); Cai et al. (2021); Grill et al. (2020); Caron et al. (2021); Baevski et al. (2022) use an \"exponential moving average\" of training checkpoints as a teacher for self-supervised learning. Other works merge models initialized from the same pretrained base, but that were finetuned independently, either by simply averaging their weights (McMahan et al., 2017; Wortzman et al., 2022a; Choshen et al., 2022; Ram\u00e9 et al., 2022), permuting one model to the other (Ashmore & Gashler, 2015; Yurochkin et al., 2019; Wang et al., 2020), combining meaningful weight regions (Ilharco et al., 2022a; Gueta et al., 2023; Yadav et al., 2023; Sung et al., 2023), or maximizing an objective (Matena & Raffel, 2021). Our setting differs, as we do not assume the same initialization.\n\n\nMerging Differently Initialized Models. Merging models with different initializations is a much more challenging problem. Works in this space often rely on mode connectivity (Freeman & Bruna, 2016; Garipov et al., 2018; Draxler et al., 2018; Frankle et al., 2020), attempting to interpolate between models along a low loss path (e.g., Tatro et al. (2020); Singh & Jaggi (2020); Liu et al. (2022)). Most recent work follows the intuition, later formalized by Entezari et al. (2021), that\n\n\n![Figure 2: Task Loss Landscapes for models in Tab. 1b. Model A and Model B lie in low loss basins for their own tasks, but not for the other task. Thus, any interpolation between Model A and a permuted Model B (e.g., Git Re-basin) lies outside the minima for both tasks and thus performs poorly. In contrast, ZipIt! improves the merge by finding a model that lies in a low loss basin for both.](images/6b1b9d301e6c25aa303a00d56e5748de1b45d6f70552d2505002eb9e741e4121.jpg)\n\n\n![](images/491cce3b1f925a5cef5b4f1c0ae5be6ed78c26db5a2c330f9a56fbd09ab26557.jpg)\n\n\n![](images/04a96a7630097237363b4d28764689540ca9ba488e3d72fffd669b6459f40124.jpg)\n\n\nmodels permuted to the same loss basin can be merged by averaging their weights. Most notably, Git Re-Basin (Ainsworth et al., 2022) permutes models by comparing the similarity between their weights. REPAIR (Jordan et al., 2022) improves the accuracy of Git Re-Basin by instead computing the correlation between their intermediate layer feature activations, and adding several batch norms to the network. Pe\u00f1a et al. (2022) find permutations using global rather than local optimization, though they don't support skip connections. Some of these works (e.g., Singh & Jaggi (2020); Ainsworth et al. (2022)) evaluate on on a setting where each model sees varying numbers of instances per class. And Pe\u00f1a et al. (2022) evaluates on a continual learning setting with disjoint categories, but their method requires training optimization. Similarly, He et al. (2018) merges models of different tasks, but requires jointly finetuning after each layer merge. As far as we are aware, we present the first general method to successfully merge models trained on disjoint tasks without additional training.\n\n\n# 3 BACKGROUND AND MOTIVATION\n\n\nModel merging stems from mode connectivity (Garipov et al., 2018), where it is conjectured that models trained with SGD on the same dataset lying in the same loss basin (i.e., region or mode of low loss) can be combined into a single model that's just as performant as the original models. If these models can be combined well by linearly interpolating their weights, they are said to be linearly mode connected (LMC) (Garipov et al., 2018; Entezari et al., 2021). Our work is similar to finding LMC, but across different datasets with disjoint label sets (i.e., separate tasks as in De Lange et al. (2021)).\n\n\nConsider a model  $\\mathcal{L}$  as a collection of layers  $L_{i} \\in \\mathcal{L}$ , each of which may have some parameters (e.g.,  $W_{i}, b_{i}$  for a linear layer). If  $\\mathcal{L}^A$  and  $\\mathcal{L}^B$  are finetuned from the same checkpoint, several works (e.g., Izmailov et al. (2018); Wortsman et al. (2022a)) find merging them is as easy as linearly interpolating their weights (i.e., they are LMC). E.g., if  $L_{i}$  is a linear layer, the new weight matrix  $W_{i}^{*}$  is simply\n\n\n$$\n\n$$\nW _ {i} ^ {*} = \\gamma W _ {i} ^ {A} + (1 - \\gamma) W _ {i} ^ {B} \\tag {1}\n$$\n\n$$\n\n\nwith an interpolation constant  $\\gamma \\in [0,1]$ , usually set to  $^{1 / 2}$ . However, if  $\\mathcal{L}^A$  and  $\\mathcal{L}^B$  were not finetuned from the same checkpoint, they often do not lie in the same mode (Entezari et al., 2021; Ainsworth et al., 2022) and cannot be interpolated. Indeed, Eq. 1 typically results in random accuracy.\n\n\nTo fix this, Entezari et al. (2021) conjecture that large enough models are likely LMC modulo permutation. This is because (1) many neural networks can be permuted internally without affecting their outputs and (2) permutation can move two models into the same basin, allowing much of the lost accuracy to be recovered. More concretely, let  $P_{i}$  be a permutation matrix that permutes outputs of layer  $L_{i}^{B}$  to the space of  $L_{i}^{A}$ . Then for each layer, permutation works apply\n\n\n$$\n\n$$\nW _ {i} ^ {*} = \\gamma W _ {i} ^ {A} + (1 - \\gamma) P _ {i} W _ {i} ^ {B} P _ {i - 1} ^ {T} \\tag {2}\n$$\n\n$$\n\n\nNote that here we permute the output space of  $W_{i}^{B}$ , but we also need to permute its input space to undo the permutation from the previous layer (hence the use of  $P_{i - 1}^{T}$ ).\n\n\nProblems with Permutation. Eq. 2 relies on the likelihood of model B lying in the same basin as model A after permutation being high. However, this is far less likely when the models are trained\n\n\non different tasks, as each model optimizes for basins containing distinct task-specific information. In this case the optimal permutation of model B to model A still lies in a strong basin on task B but doesn't lie in a basin on task A, as shown in Figure 2. This causes the interpolated model to perform worse than either the two original models. Thus, we explore alternative merging methods.\n\n\n# 4 ZIPIT!\n\n\nIn this work, we treat model merging as combining the checkpoints (i.e., collection of weights) of multiple models into a single checkpoint that can perform all the tasks of its constituents. We do this by merging the layers of the models together. For instance, suppose  $L_{i} \\in \\mathcal{L}$  is a linear layer with parameters  $W_{i} \\in \\mathbb{R}^{n_{i} \\times m_{i}}$ ,  $b_{i} \\in \\mathbb{R}^{n_{i}}$  with input features  $x \\in \\mathbb{R}^{m_{i}}$  and outputs features  $f_{i} \\in \\mathbb{R}^{n_{i}}$ :\n\n\n$$\n\n$$\nf _ {i} = L _ {i} (x) = W _ {i} x + b _ {i} \\tag {3}\n$$\n\n$$\n\n\nOur goal is to take  $L_{i}^{A} \\in \\mathcal{L}^{A}$  from model A and  $L_{i}^{B} \\in \\mathcal{L}^{B}$  from model B and merge them into a layer  $L_{i}^{*}$  that combines their feature spaces such that information from both  $f_{i}^{A}$  and  $f_{i}^{B}$  is retained in  $f_{i}^{*}$ . We accomplish this by merging each layer of one model with the corresponding layer in the other, both merging features in one across both layers or within the same layer. This is in contrast to permutation-based merging method, which only combine features across layers.\n\n\nWhy should we merge within? Features of models trained on different tasks may be dissimilar, as the models solve different problems. Forcibly combining these dissimilar features can yield merges that don't perform well on either original task (Fig 2). Instead, those features may be more compatible with others within the same model, which would better retain performance when combined.\n\n\nIn fact, we can prove that methods which allow merges within each model (as well as across both) perform equal to or better than those which only merge across models (e.g., permutation-reliant approaches) in a limited but prevalent setting. Specifically, we obtain a tighter bound over Theorem 3.1 from Entezari et al. (2021) when redundancy exists within a model and is leveraged. Both Theorem 3.1 and our Theorem 1 (see Appendix G for formalization and proof) bound the degree to which the loss of a merged two-layer model with  $d$ -input dimensions and  $h$ -intermediate dimensions increases compared to the losses of the original models. Theorem 3.1 bounds this increase to  $\\tilde{O}(h^{-1/(2d+4)})$ . However, if features within a model are redundant, then we reduce the bound to\n\n\n$$\n\n$$\n\\text {L o s s I n c r e a s e o f M e r g e d M o d e l} \\leq \\left\\{ \\begin{array}{l l} \\tilde {O} \\left(\\left(\\frac {h}{1 - 2 \\Gamma}\\right) ^ {- \\frac {1}{2 d + 4}}\\right) & \\Gamma <   0. 5 \\\\ 0 & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {4}\n$$\n\n$$\n\n\nwith  $\\Gamma \\in [0,1]$  measuring what portion of features are redundant. This bound is  $2^{d + 4}\\sqrt{1 - 2\\Gamma} \\leq 1$  times that of Theorem 3.1 when  $\\Gamma < 0.5$  (equal only when  $\\Gamma = 0$ ) and explicitly zero when  $\\Gamma \\geq 0.5$ .\n\n\nHow do we merge features? In prior work, each of the merged features  $f_{i}^{*}$  is the result of combining one feature from  $f_{i}^{A}$  and one from  $f_{i}^{B}$ . However in our case, we can also merge features by combining two from just  $f_{i}^{A}$  or two from just  $f_{i}^{B}$ . To account for this, we concatenate  $f_{i}^{A}$  and  $f_{i}^{B}$  into a single feature vector:  $f_{i}^{A}\\|f_{i}^{B}\\in\\mathbb{R}^{2n_{i}}$ . Then, like prior work (e.g. Li et al. (2015); Jordan et al. (2022)), we define feature similarity as the pairwise correlation between between neuron activations over a small set of images (without labels). However, unlike those works, we compute correlations between every activation in the full concatenated space  $f_{i}^{A}\\|f_{i}^{B}$ . Our approach thus measures the similarity of every feature  $f_{i}^{A}$  and  $f_{i}^{B}$  to all features in both models, rather than solely between  $f_{i}^{A}$  and  $f_{i}^{B}$ .\n\n\nNext, if two features are well correlated, we can average them without losing much information. Thus, we can construct  $f_{i}^{*}$  by finding  $n_i$  pairs of similar features in  $f_{i}^{A} \\| f_{i}^{B}$  and averaging them together. By default, we do this greedily: i.e., iteratively match the features with the highest correlation without replacement; though we explore extensions to this in Sec. 4.3 and test other methods in Tab. 4. Then we can use these matches to construct  $f_{i}^{*}$ . Formally, we define a \"merge matrix\"  $M_{i} \\in \\mathbb{R}^{n_{i} \\times 2n_{i}}$  s.t.\n\n\n$$\n\n$$\nf _ {i} ^ {*} = M _ {i} \\left(f _ {i} ^ {A} \\| f _ {i} ^ {B}\\right) \\tag {5}\n$$\n\n$$\n\n\n$M_{i}$  averages the matched features, with each match corresponding to one output feature in  $f_{i}^{*}$ . For instance, if  $u$ th match is between indices  $s, t \\in \\{1, \\dots, 2n_{i}\\}$  of  $f_{i}^{A} \\| f_{i}^{B}$ , then the  $u$ th row of  $M_{i}$  would be  $1/2$  at columns  $s$  and  $t$  and  $0$  elsewhere. This results in  $f_{i}^{*}[u] = \\frac{1}{2}(f_{i}^{A} \\| f_{i}^{B})[s] + \\frac{1}{2}(f_{i}^{A} \\| f_{i}^{B})[t]$ .\n\n\n![The Zip Operation (a) Two Disjoint Layers](images/db0e3414e86afd5bf89943298f95523230c31eacdb4e14557c1fd8a039d96b9a.jpg)\n\n\n![](images/29a35c847f61bf6c02e33e5aa9edac28d3a7cfdc22abce2d8ff9763ae61ab8f9.jpg)\n\n\n![(c) Merge and Unmerge Figure 3: ZipIt! merges models layer-wise by exploiting redundancy in their features. (a) Output features  $f^A$  and  $f^B$  from two disjoint layers are (b) paired with other features based on the similarity of their activations. (c) We produce a merge matrix  $M$  to combine the pairs into a single shared output feature space, and a corresponding unmerge matrix  $U$  that undoes this operation. (d) We then propagate  $U$  up the network to align the next layer's input space, and simultaneously receive the previous layer's  $U$  to align our input space. (e) We apply Eq. 8 to \"zip\" the layers together using the  $M$  for the output and  $U$  for the input, producing a single layer (f). We then repeat (a) on the next layer.](images/07b291970f89cf53e1105c4e713ed9a3e3f7c747d646d30a14c1debf6372ce72.jpg)\n\n\n![(d) Propagate M and U](images/18b1097526d090664980cff0c57f742f5eb573d5a04ec61f6cd42b40b9cf6bf1.jpg)\n\n\n![(e) Apply M and U to Weights](images/d1f94863a899cda90bc182ec1f7be82e9a6add699beecb317dc0a5fc7c0a49c6.jpg)\n\n\n![(f) Result: One Zipped Layer](images/b003cb8c8136ad8dd0742c08fb5f322abb51185059da82b397c962c3ef515271.jpg)\n\n\nThus, applying  $M_{i}$  has the effect of interpolating with  $\\gamma = 1 / 2$  but is more general (e.g., allows for merging more than 2 models at once, see Sec. 4.3).\n\n\nWhat about the next layer? After merging features in one layer, we now have the problem that the next layers,  $L_{i+1}^{A}, L_{i+1}^{B}$ , are incompatible with  $f_{i}^{*}$ . Instead, we need to undo the merge operation before passing the features to the next layer. Thus, we define an \"unmerge\" matrix  $U_{i} \\in \\mathbb{R}^{2n_{i} \\times n_{i}}$  s.t.\n\n\n$$\n\n$$\nU _ {i} f _ {i} ^ {*} \\approx f _ {i} ^ {A} \\| f _ {i} ^ {B} \\tag {6}\n$$\n\n$$\n\n\n$U_{i}$  is the pseudoinverse of  $M_{i}$  and in the case of the matching from earlier is simply  $2M_{i}^{T}$ . Note that strict equality is unlikely here. Like in prior work, merging models is a lossy operation.\n\n\nWe split this unmerge matrix in half along its rows into  $U_{i}^{A}, U_{i}^{B} \\in \\mathbb{R}^{n_{i} \\times n_{i}}$  that act individually to produce  $f_{i}^{A}$  and  $f_{i}^{B}$ . With this, we can evaluate the next layers using the merged features:\n\n\n$$\n\n$$\nf _ {i + 1} ^ {A} \\approx L _ {i + 1} ^ {A} \\left(U _ {i} ^ {A} f _ {i} ^ {*}\\right) \\quad f _ {i + 1} ^ {B} \\approx L _ {i + 1} ^ {B} \\left(U _ {i} ^ {B} f _ {i} ^ {*}\\right) \\tag {7}\n$$\n\n$$\n\n\n# 4.1 THE \"ZIP\" OPERATION\n\n\nWe now have all the necessary pieces, and can derive a general operation to merge  $L_{i}^{A}$  and  $L_{i}^{B}$  at an arbitrary point in the network (Fig. 3). First, we compute  $M_{i}$  and  $U_{i}$  by matching features between  $f_{i}^{A}$  and  $f_{i}^{B}$ . We then pass  $U_{i}$  to the next layer and receive  $U_{i-1}$  from the previous layer. Using  $M_{i}$  and  $U_{i-1}$ , we \"fuse\" the merge and unmerge operations into the layer's parameters. For a linear layer:\n\n\n$$\n\n$$\nW _ {i} ^ {*} = M _ {i} ^ {A} W _ {i} ^ {A} U _ {i - 1} ^ {A} + M _ {i} ^ {B} W _ {i} ^ {B} U _ {i - 1} ^ {B} \\tag {8}\n$$\n\n$$\n\n\nwhere  $M_i^A$  and  $M_i^B$  are  $M_i$  split along its columns.  $b_i^*$  has the same equation but without unmerging.\n\n\nNote the similarity between Eq. 8 and Eq. 2. This isn't a coincidence: if we only allowed merging across models and not within models, our \"zip\" operation would be identical to Git Re-Basin's permute-then-interpolate approach. Thus, Eq. 8 can be thought of as a generalization of prior work.\n\n\n# 4.2 ZIP PROPAGATION\n\n\nHowever, most modern neural networks are not simply collections of linear layers stacked on top of each other. In practice, we cannot combine merge and unmerge matrices into every layer of the network, as a local zip (Eq. 8) expects the layer to have a weight matrix\u2014i.e., the layer has to have separate input and output spaces so that we can unmerge the input space and merge the output space. Other layers (e.g., BatchNorm, ReLU) don't have such a weight matrix.\n\n\n![Figure 4: Zip Propagation. We propagate  $M_{i}$  backward until we hit a layer with weights, merging merging element-wise layers (e.g., BatchNorm) along the way.](images/923810c928ff8b8b0525cdf727993a0ba94df427adbddba39c7ef71e7df6d55e.jpg)\n\n\n![](images/14a289c2d198501c220c735b7cf0cc6cbf8da424a0032968902e674897243bb6.jpg)\n\n\n![](images/53500d042eb1f88fdf720a81f1cce7c12b125e19b16ff7be8da55090199dbf3b.jpg)\n\n\n![](images/1043733cc3faed0660df8e79ece58f75251ca6ae83ebbeefdeecc8ce6eabf31e.jpg)\n\n\n![(a) Stop zipping (b) Unmerge Figure 5: Partial Zip. (a) If we stop zipping early and (b) apply the latest U from the zip propagation to the inputs of the first unmerged layer in each model, (c) we get a multi-head model with a head for each task.](images/95eee3f2f24e55c211388c8c3ff2283c7bd8f08e6b62d02dc600561691d8657d.jpg)\n\n\n![](images/272fe708035c93c549911d362aadf7a2a7940f647384d915c946895b4d0a12e4.jpg)\n\n\n![(c) Result is Multi-Head](images/50bfab26a0b5e0cae33761c04a8f47717ca3301ca8a637b5ded64cf9b14a59f3.jpg)\n\n\nThus, we \"propagate\"  $M_{i}$  and  $U_{i}$  through these layers. For instance, in Fig. 4, we show a common stack of layers found in a typical ConvNet. Following Jordan et al. (2022), we compute  $M_{i}$  and  $U_{i}$  using the activations of the network (i.e., after each ReLU). We can't fuse  $M_{i}$  with the ReLU layer, as it doesn't have any parameters. Similarly, we can merge the parameters of the preceding BatchNorm layer (i.e., in the same way as bias). But it doesn't have a weight matrix, so we also can't fuse  $M_{i}$  into it. Only once we've reached the Conv layer can we fuse  $M_{i}$  and  $U_{i}$  into it using Eq. 8 (in this case, treating each kernel element as independent).\n\n\nSimilar care needs to be taken with skip connections, as every layer that takes input from or outputs to a skip connection shares the same feature space. However, this too can be dealt with during propagation\u2014we just need to propagate  $M_{i}$  backward and  $U_{i}$  forward to each layer connected by the same skip connection. In general, we can define propagation rules to handle many different types of network modules (see Appendix C).\n\n\n# 4.3 EXTENSIONS\n\n\nPartial Zip. We don't always want to zip every layer of the two networks, especially if their output spaces are incompatible, or if doing so would lose too much accuracy. Instead, we can perform a partial zip. That is, we zip most of the layers together, but leave the later ones unzipped (Fig. 5).\n\n\nImplementing this operation is simple in our framework: zip as normal until the specified layer  $i$ , then the remaining unzipped layers will receive  $U_{i}$  through zip propagation. If we apply  $U_{i}^{A}$  to  $L_{i+1}^{A}$  and  $U_{i}^{B}$  to  $L_{i+1}^{B}$ , the remaining unzipped layers will form \"heads\" that expect merged features as input. We can then ensemble the heads or choose one to evaluate at runtime.\n\n\nRepeated Matching  $(\\alpha)$ . In some cases, we'd like to merge more than two models together. To do this, we allow \"repeated matches\". That is, when two features are matched in our greedy algorithm, they are removed and replaced with the resulting merged feature. To ensure that one feature doesn't get merged endlessly, we set the correlations of the new feature to be the minimum of the old features' similarities weighted by  $\\alpha \\in (0,1]$ . We find a small value of  $\\alpha$  typically works best.\n\n\nSame-model Budget  $(\\beta)$ . To demonstrate the effectiveness of same-model merges, we introduce a \"budget\" parameter  $\\beta \\in [0,1]$  that denotes what percent of total merged features can come from models merging within themselves, with each model receiving an equal portion of this budget. Note that a budget of 0 results in Eq. 2, as in that case no features can be merged within models.\n\n\n# 5 RESULTS\n\n\nThere is no standard benchmark to evaluate merging approaches on models from distinct tasks, so we construct our own. We evaluate our approach in two different settings. (1) A versatile test-bed: disjoint category splits of the same dataset (i.e., same dataset and different label sets). (2) A very challenging setting: completely different datasets and tasks (i.e., different datasets and label sets).\n\n\nExperimental Details. For each experiment where we sample multiple disjoint splits of categories, we hold one split out for hyperparameter search and report mean and standard deviation on the rest. For experiments with models trained on different datasets, we subsample the validation set into a validation and test set to use for the same purpose. To compute correlations, we use a portion of the\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">FLOPs (G)</td><td colspan=\"4\">Accuracies (%)</td></tr><tr><td>Joint</td><td>Task A</td><td>Task B</td><td>Avg</td></tr><tr><td>Model A</td><td>0.68</td><td>48.2\u00b11.0</td><td>97.0\u00b10.6</td><td>45.1\u00b18.6</td><td>71.0\u00b14.4</td></tr><tr><td>Model B</td><td>0.68</td><td>48.4\u00b13.8</td><td>49.1\u00b19.3</td><td>96.1\u00b11.1</td><td>72.6\u00b14.9</td></tr><tr><td>W. Avg (Eq. 1)</td><td>0.68</td><td>43.0\u00b11.6</td><td>54.1\u00b11.4</td><td>67.5\u00b11.2</td><td>60.8\u00b14.5</td></tr><tr><td>Git Re-Basin\u2021</td><td>0.68</td><td>46.2\u00b10.8</td><td>76.8\u00b18.9</td><td>82.7\u00b15.1</td><td>79.8\u00b16.5</td></tr><tr><td>Permute (Eq. 2)</td><td>0.68</td><td>58.4\u00b16.8</td><td>86.6\u00b12.1</td><td>87.4\u00b11.1</td><td>87.4\u00b11.4</td></tr><tr><td>ZipIt&#x27;20/20</td><td>0.68</td><td>79.1\u00b11.1</td><td>92.9\u00b11.1</td><td>91.2\u00b11.4</td><td>92.1\u00b11.0</td></tr><tr><td>Ensemble</td><td>1.37</td><td>87.4\u00b12.6</td><td>97.0\u00b10.6</td><td>96.1\u00b11.1</td><td>96.6\u00b10.4</td></tr><tr><td>ZipIt&#x27;13/20</td><td>0.91</td><td>83.8\u00b13.1</td><td>95.1\u00b10.7</td><td>94.1\u00b11.5</td><td>94.6\u00b10.6</td></tr></table>\n\n\n(a) CIFAR-10  $(5 + 5)$ . ResNet-20  $(4 \\times$  width).\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">FLOPs (G)</td><td colspan=\"4\">Accuracies (%)</td></tr><tr><td>Joint</td><td>Task A</td><td>Task B</td><td>Avg</td></tr><tr><td>Model A</td><td>2.72</td><td>41.6\u00b10.3</td><td>82.9\u00b10.7</td><td>24.8\u00b10.4</td><td>53.9\u00b10.5</td></tr><tr><td>Model B</td><td>2.72</td><td>41.6\u00b10.2</td><td>25.1\u00b11.2</td><td>82.8\u00b10.2</td><td>54.0\u00b10.6</td></tr><tr><td>W. Avg (Eq. 1)</td><td>2.72</td><td>17.0\u00b11.7</td><td>23.8\u00b16.9</td><td>24.8\u00b15.9</td><td>24.3\u00b11.9</td></tr><tr><td>Git Re-Basin\u2021</td><td>2.72</td><td>40.9\u00b10.2</td><td>57.3\u00b11.5</td><td>56.7\u00b10.7</td><td>57.0\u00b10.8</td></tr><tr><td>Permute (Eq. 2)</td><td>2.72</td><td>42.8\u00b10.7</td><td>61.6\u00b11.4</td><td>60.5\u00b10.5</td><td>61.0\u00b10.8</td></tr><tr><td>Ziplt&#x27;20/20</td><td>2.72</td><td>54.9\u00b10.8</td><td>68.2\u00b10.8</td><td>67.9\u00b10.6</td><td>68.0\u00b10.4</td></tr><tr><td>Ensemble</td><td>5.45</td><td>73.5\u00b10.4</td><td>82.9\u00b10.7</td><td>82.8\u00b10.2</td><td>82.8\u00b10.4</td></tr><tr><td>Ziplt&#x27;13/20</td><td>3.63</td><td>70.2\u00b10.4</td><td>80.3\u00b10.8</td><td>80.1\u00b10.7</td><td>80.2\u00b10.6</td></tr></table>\n\n\n(b) CIFAR-100  $(50 + 50)$ . ResNet-20  $(8 \\times$  width).\n\n\nTable 1: CIFAR Results. ZipIt! vs. baselines on combining a model trained on half the classes (Task A) with one trained on the other half (Task B) without extra training. We report both joint (10/100-way) and per-task (5/50-way) accuracy. ZipIt! significantly outperforms its baseline and closes in on the upper bound (ensemble accuracy).  $\\ddagger$  refers to Ainsworth et al. (2022).\n\n\ntraining set for each dataset as in Li et al. (2015) (see Appendix B). For a fair comparison, we reset the batch norms for all methods (including the original models) using the training data (following the recommendation in Jordan et al. (2022)). For our method,  $\\text{ZipIt!}_{\\mathrm{n/m}}$  indicates that  $n$  out of the  $m$  layers in the network have been zipped (Sec. 4.3). Note, all our models have different initializations.\n\n\nEvaluation. For the setting with disjoint class splits of the same dataset, we evaluate performance in two ways: joint accuracy and per task accuracy. For joint accuracy, we evaluate each model over all classes in the combined dataset. For per task accuracy, we compute the accuracy of each task individually (i.e., supposing we had task labels at runtime) and then report the average. The former is similar to a continual learning setting where we want to augment the knowledge of the model, while the latter is akin to a multi-task setting where we know which task we're using at test time. For the scenario where we merge models trained on different datasets, we use the per task accuracy metric, as the label spaces are not comparable.\n\n\nBaselines. In addition to the default Weight Matching version of Git Re-Basin (Ainsworth et al., 2022), we compare to two baselines: Weight Averaging (Eq. 1) and Permute (Eq. 2) with  $\\gamma = 1/2$  using our framework (i.e., we set  $M_i$  and  $U_i$  such that Eq. 8 is equivalent). For Permute, we use linear sum assignment to find optimal permutations (following Li et al. (2015)). Note that our Permute is a strong baseline we create using our framework and is more accurate than Git Re-Basin in our settings. It's also similar to REPAIR (Jordan et al., 2022), but without adding extra parameters to the model. Finally, with perfect merging, the merged model's outputs would be identical to the originals. Thus we include Ensemble as an upper bound (executing and concatenating the results of both models).\n\n\n# 5.1 CIFAR-10 AND CIFAR-100\n\n\nWe train 5 pairs of ResNet-20 (He et al., 2016) from scratch with different initializations on disjoint halves of the CIFAR-10 and CIFAR-100 classes (Krizhevsky et al., 2009). While ZipIt! supports \"partial zipping\" to merge models with different outputs (in this case, disjoint label sets), prior methods without retraining do not. To make a fair comparison, we train these CIFAR models with a CLIP-style loss (Radford et al., 2021) using CLIP text encodings of the class names as targets. This way, both models output into the same CLIP-space regardless of the category. Note, this means the models are capable of some amount of zero-shot classification on the tasks they were not trained on.\n\n\nCIFAR-10 (5+5). In Tab. 1a, we merge models trained on disjoint 5 class subsets of CIFAR-10 using ResNet-20 with a  $4 \\times$  width multiplier (denoted as ResNet-20\u00d74). In joint classification (i.e., 10-way), Git Re-Basin is unable to perform better than using either of the original models alone, while our Permute baseline performs slightly better. In stark contrast, our ZipIt! performs a staggering  $32.9\\%$  better than Git Re-Basin and  $20.7\\%$  better than our baseline. If allow the last stage of the network to remain unzipped (i.e., zip up to 13 layers), our method obtains  $83.8\\%$ , which is only  $3.6\\%$  behind an ensemble of model A and model B (which is practically the upper bound for this setting). We also achieve similar results when merging VGG11 models in this setting (Appendix D).\n\n\nCIFAR-100 (50+50). We find similar results on disjoint 50 class splits of CIFAR-100 in Tab. 1b, this time using an  $8 \\times$  width multiplier instead. Like with CIFAR-10, Git Re-Basin fails to outperform even the unmerged models themselves in joint classification (i.e., 100-way), and this time Permute is only  $1.2\\%$  ahead. ZipIt! again significantly outperforms prior work with  $+14\\%$  accuracy over Git Re-Basin for all layers zipped, and a substantial  $+29.2\\%$  if zipping 13/20 layers. At this accuracy,\n\n\nZipIt!13/20 is again only  $3.3\\%$  behind the ensemble for joint accuracy and  $2.6\\%$  behind for average per task accuracy, landing itself in an entirely different performance tier compared to prior work.\n\n\n# 5.2 IMAGENET-1K (200+200)\n\n\nTo test our method on the much harder setting of large-scale data, we train 5 differently initialized ResNet-50 models with cross entropy loss on disjoint 200 class subsets of ImageNet-1k (Deng et al., 2009). To compare to prior work that doesn't support partial zipping, we initialize the models with capacity for all 1k classes, but only train each on their subset.\n\n\nIn Tab. 2 we show results on exhaustively merging pairs from the 5 models. To compute joint (i.e., 400-way) accuracy, we softmax over each task's classes individually (like in Ahn et al. (2021)), and take the argmax over the combined\n\n\n400 class vector. On this extremely difficult task, Git Re-Basin only obtains  $3.1\\%$  for joint accuracy (with random accuracy being  $0.25\\%$ ). Both the Permute baseline and ZipIt! with all layers zipped perform better, but with each at  $8.6\\%$ , are still clearly lacking. Note that we find the same-model merging budget  $\\beta$  to not matter for this set of models (see Fig. 6), which suggests that there's not a lot of redundant information within each model in this setting. Thus, ZipIt! chooses to merge mostly across models instead, performing similarly to the permute baseline. We find this same trend in CIFAR with smaller models (see Fig. 6), and may be an artifact of model capacity. The story changes when we increase the capacity of the merged model by partial zipping:  $\\mathrm{ZipIt!}_{10/50}$  reaches close to upper bound ensemble accuracy on this extremely difficult task, while saving on FLOPs.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">FLOPs (G)</td><td colspan=\"4\">Accuracies (%)</td></tr><tr><td>Joint</td><td>Task A</td><td>Task B</td><td>Avg</td></tr><tr><td>Model A</td><td>4.11</td><td>37.2\u00b12.0</td><td>74.3\u00b14.0</td><td>0.5\u00b10.1</td><td>37.4\u00b12.0</td></tr><tr><td>Model B</td><td>4.11</td><td>35.3\u00b11.6</td><td>0.5\u00b10.1</td><td>70.5\u00b13.2</td><td>35.5\u00b11.6</td></tr><tr><td>W. Avg (Eq.1)</td><td>4.11</td><td>0.3\u00b10.1</td><td>0.6\u00b10.1</td><td>0.7\u00b10.1</td><td>0.6\u00b10.1</td></tr><tr><td>Git Re-Basin\u2020</td><td>4.11</td><td>3.1\u00b11.2</td><td>5.3\u00b12.6</td><td>5.7\u00b12.4</td><td>5.5\u00b11.7</td></tr><tr><td>Permute (Eq. 2)</td><td>4.11</td><td>8.6\u00b15.8</td><td>10.1\u00b14.4</td><td>15.3\u00b111.1</td><td>12.7\u00b17.7</td></tr><tr><td>ZipIt!50/50</td><td>4.11</td><td>8.6\u00b14.7</td><td>12.4\u00b15.9</td><td>14.7\u00b17.8</td><td>13.5\u00b16.6</td></tr><tr><td>Ensemble</td><td>8.22</td><td>63.3\u00b14.9</td><td>74.3\u00b14.0</td><td>70.5\u00b13.2</td><td>72.4\u00b12.5</td></tr><tr><td>ZipIt!22/50</td><td>6.39</td><td>55.8\u00b14.1</td><td>65.9\u00b12.5</td><td>64.1\u00b13.0</td><td>65.0\u00b12.3</td></tr><tr><td>ZipIt!10/50</td><td>7.43</td><td>60.9\u00b14.1</td><td>70.7\u00b13.0</td><td>69.0\u00b12.9</td><td>69.9\u00b11.9</td></tr></table>\n\n\n# 5.3 MULTI-DATASET MERGING\n\n\nWe now take our model merging framework one step further by merging differently initialized models trained on completely separate datasets and tasks. We present two settings: merging multiple classification datasets and merging semantic segmentation with image classification.\n\n\nImage Classification Datasets. Merging ResNet-50 models trained on: Stanford Dogs (Khosla et al., 2011), Oxford Pets (Parkhi et al., 2012), CUB200 (Welinder et al., 2010), and NABirds (Van Horn et al., 2015). In Tab. 3, we show the average per task accuracy from exhaustively merging each pair and the much more difficult setting of merging all four at once. We report the accuracy of our baselines by applying them up until the last layer, but we can't compare to prior work as they don't support this setting. As in all our previous experiment we merge without retraining.\n\n\nFor pairs of models, ZipIt! slightly outperforms our permute baseline across all tasks and performs similarly when merging all 4 models at once. However, if we add capacity to the merged model through partial zipping, we perform up to  $33\\%$\n\n\nbetter on merging pairs and  $50\\%$  better on merging all four models than the permute baseline. Partial zipping is a significant factor to obtain strong performance, especially with more than 2 models.\n\n\nTable 2: ImageNet-1k (200+200) Results. Merging ResNet-50 models trained from scratch on disjoint 200 category subsets (Task A and B) of ImageNet-1k. Prior work performs poorly, but ZipIt! makes this task feasible.  ${}^{ \\ddagger  }$  Ainsworth et al. (2022).\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">FLOPs (G)</td><td colspan=\"5\">Per-Task Accuracies (%)</td></tr><tr><td>SD</td><td>OP</td><td>CUB</td><td>NAB</td><td>Avg</td></tr><tr><td colspan=\"7\">Merging Pairs</td></tr><tr><td>W. Avg (Eq. 1)</td><td>4.11</td><td>12.9</td><td>18.2</td><td>13.9</td><td>0.2</td><td>11.3</td></tr><tr><td>Permute (Eq. 2)</td><td>4.11</td><td>46.2</td><td>47.6</td><td>35.6</td><td>13.5</td><td>35.7</td></tr><tr><td>ZipIt!49/50</td><td>4.11</td><td>46.9</td><td>50.7</td><td>38.0</td><td>12.7</td><td>37.1</td></tr><tr><td>Ensemble</td><td>8.22</td><td>72.7</td><td>81.1</td><td>71.0</td><td>77.2</td><td>75.5</td></tr><tr><td>ZipIt!22/50</td><td>6.39</td><td>62.6</td><td>71.2</td><td>62.8</td><td>53.0</td><td>62.4</td></tr><tr><td>ZipIt!10/50</td><td>7.42</td><td>66.5</td><td>75.8</td><td>65.6</td><td>66.8</td><td>68.7</td></tr><tr><td colspan=\"7\">Merging All 4</td></tr><tr><td>W. Avg (Eq. 1)</td><td>4.12</td><td>0.8</td><td>3.0</td><td>0.6</td><td>0.3</td><td>1.2</td></tr><tr><td>Permute (Eq. 2)</td><td>4.12</td><td>15.7</td><td>26.1</td><td>14.0</td><td>5.3</td><td>15.3</td></tr><tr><td>ZipIt!49/50</td><td>4.12</td><td>21.1</td><td>33.3</td><td>8.6</td><td>3.9</td><td>16.8</td></tr><tr><td>Ensemble</td><td>16.4</td><td>72.7</td><td>81.2</td><td>71.0</td><td>77.2</td><td>75.5</td></tr><tr><td>ZipIt!22/50</td><td>11.0</td><td>50.2</td><td>55.9</td><td>44.0</td><td>32.0</td><td>45.5</td></tr><tr><td>ZipIt!10/50</td><td>14.1</td><td>63.5</td><td>70.8</td><td>63.7</td><td>63.1</td><td>65.3</td></tr></table>\n\n\nTable 3: Multi-Dataset Results. Merging ResNet-50 models trained on completely different datasets: Stanford Dogs (SD), Oxford Pets (OP), CUB200 (CUB), and NABirds (NAB). We report average per-task accuracy over merging model pairs, and all four.\n\n\nMultiple Output Modalities. In Appendix F, we combine across modalities by merging the ResNet-50 backbone of a DeeplabV3 (Chen et al., 2017) segmentation model with an ImageNet-1k classification model. The resulting combined model can perform both semantic segmentation and image classification. Even with half of layers merged, ZipIt! retains good performance on both tasks.\n\n\n![(a) Ablation: Varying  $\\beta$](images/75ed538048be079e8872350e230041163fbe61f22c7797600a0eb65421f3fc11.jpg)\n\n\n![Figure 6: Varying  $\\beta$  and Model Scale. Left: We find when the model has enough capacity for the task, a high budget (Sec. 4.3) improves performance. Right: ZipIt! makes effective use of extra model capacity to quickly reach the ensemble on CIFAR-100 (50+50) when we increase the width of ResNet-20 models. In contrast, our baselines only slightly benefit from the extra scale.](images/cc00d41500bd1efc41b879a07dc05932ef9df0c58dcbc02d39b923b6b2e72944.jpg)\n\n\n![(b) Ablation: Model Scale](images/f103d1d45e90c5d5e82deeb4a0d07e77b5e38b3517cd0143b776855bcc2be31a.jpg)\n\n\n# 6 ANALYSIS\n\n\nMerging within Models. A critical piece of ZipIt! compared to prior work is the ability to merge within models, not just across models. In Sec. 4.3, we introduce a budget parameter  $\\beta$  to limit the number of same-model merges, and here use CIFAR-100 (50+50) and ImageNet-1k (200+200) to illustrate its effectiveness (Fig. 6a). On CIFAR, same-model merges are very important, with the optimal budget being above 0.8, meaning  $80\\%$  of merges are allowed to be within the same model. This is not the case, however, on ImageNet, where the difficulty of the task means there likely are much fewer redundant features within each model.\n\n\nModel Scale. In Fig. 6b, we test the effect of model scale directly by evaluating joint accuracy on our CIFAR-100 (50+50) setting with ResNet-20 models of increasing width. Here, we explicitly see that when the width of the models are too small for the task (e.g.,  $< 4 \\times$ ), ZipIt! and the Permute baseline perform identically (though both much better than Git Re-Basin). However, when the scale increases, ZipIt! trends toward the ensemble upper bound of  $75\\%$ , while both the Permute baseline and Git Re-Basin plateau at around  $45\\%$ . This corroborates Eq. 4 and indicates our method uses the extra model capacity effectively, much better than prior work.\n\n\nMatching Algorithm. In Tab. 4, we compare matching algorithms used to compute  $M_{i}$  in Eq. 8. Using either the identity (weight averaging) or a permutation (as in prior work) underperforms on CIFAR-10 (5+5) joint 10-way classification. In contrast, we obtain up to  $21.2\\%$  higher accuracy if we allow both permutations and merging within models. However, doing this optimally is difficult, as the standard linear sum assignment algorithm assumes bipartite matches. We could use a optimal graph-based solver (e.g., Hagberg et al. (2008)) instead, but doing so is prohibitively slow (11 minutes to transform a ResNet-20\u00d74 model). Thus, we find matches greedily by repeatedly taking the most corre\n\n\nlated pair of features without replacement. This performs almost as well, and is multiple orders of magnitude faster. If we allow repeated matches (Sec. 4.3), we obtain a slightly better result. Like Bolya et al. (2023), we find that matching is better for merging features than clustering (K-Means).\n\n\n<table><tr><td>Algorithm</td><td>A\u2194A/B\u2194B?</td><td>Acc</td><td>Time</td></tr><tr><td>Identity (Eq. 1)</td><td>X</td><td>43.0\u00b13.1</td><td>1.8 ms</td></tr><tr><td>Permute (Eq. 2)</td><td>X</td><td>58.4\u00b11.3</td><td>28 ms</td></tr><tr><td>K-Means</td><td>\u2713</td><td>29.1\u00b15.5</td><td>19 sec</td></tr><tr><td></td><td>Zip (Eq. 8)</td><td></td><td></td></tr><tr><td>Optimal Match</td><td>\u2713</td><td>79.6\u00b11.7</td><td>11 min</td></tr><tr><td>Greedy Match</td><td>\u2713</td><td>79.0\u00b11.8</td><td>1.1 sec</td></tr><tr><td>Greedy, \u03b1=0.1</td><td>\u2713</td><td>79.1\u00b12.1</td><td>1.2 sec</td></tr></table>\n\n\nTable 4: Comparing Matching Algorithms to use for  $M_{i}$  on CIFAR-10  $(5 + 5)$  joint 10-way accuracy. Permuting  $\\mathrm{B}\\rightarrow \\mathrm{A}$  as in prior work (Eq. 2) performs poorly. We significantly improve by merging features within each model (Eq. 8). Our greedy approach is nearly as accurate as the optimal algorithm while being two orders of magnitude faster.\n\n\n# 7 CONCLUSION\n\n\nIn this paper, we tackle the extremely difficult task of merging models trained on completely disjoint tasks without additional training. We find that prior work underperforms in this setting and posit that they neither fully (1) exploit model similarities nor (2) account for model dissimilarities. We introduce ZipIt!, a general framework for merging models that addresses these issues, and show it to significantly outperform prior work across several difficult settings, comprehensively analyzing each.\n\n- Reproducibility Statement. To ensure reproducibility, we will release code for our algorithm, experiments, and baselines. We also include algorithm details in Section 4 and further in Appendix C, experimental details in Section 5 and Appendix B, and a proof of our Theorem 1 in Appendix G.\n\n- Acknowledgements. This work was supported in part by funding from NSF CAREER #2144194, ARL, Google, and NSF GRFP. All views and conclusions expressed in this work are those of the authors and not a reflection of these sources.\n\n\n# REFERENCES\n\n- Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax for incremental learning. In ICCV, 2021.\n\n- Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. arXiv:2209.04836, 2022.\n\n- Stephen Ashmore and Michael Gashler. A method for finding similarity between multi-layer perceptrons by forward bipartite alignment. In IJCNN, 2015.\n\n- Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. In ICML, 2022.\n\n- Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011.\n\n- Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In ICCV, 2019.\n\n- Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. ICLR, 2023.\n\n- Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes, Zhuowen Tu, and Stefano Soatto. Exponential moving average normalization for self-supervised and semi-supervised learning. In CVPR, 2021.\n\n- Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.\n\n- Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\n\n- Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. Fusing finetuned models for better pretraining. arXiv:2204.03044, 2022.\n\n- Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. TPAMI, 2021.\n\n- Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. arXiv:2302.05442, 2023.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\n\n- Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, and Leshem Choshen. ColD fusion: Collaborative descent for distributed multitask finetuning. Association for Computational Linguistics, 2023.\n\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv:2010.11929, 2020.\n\n- Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In ICML, 2018.\n\n- Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. arXiv:2110.06296, 2021.\n\n- M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303-338, June 2010.\n\n- Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In ICML, 2020.\n\n- C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. arXiv:1611.01540, 2016.\n\n- Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. NeurIPS, 2018.\n\n- Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In ICASSP, 2017.\n\n- Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. NeurIPS, 2020.\n\n- Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. Knowledge is a region in weight space for fine-tuned language models. arXiv:2302.04863, 2023.\n\n- Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. Exploring network structure, dynamics, and function using networkx. In Proceedings of the 7th Python in Science Conference, 2008.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016.\n\n- Kaiming He, Georgia Gkioxari, Piotr D\u00f3lar, and Ross Girshick. Mask r-cnn. In ICCV, 2017.\n\n- Xiaoxi He, Zimu Zhou, and Lothar Thiele. Multi-task zipping via layer-wise neuron sharing. NeurIPS, 2018.\n\n- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. *NeurIPS*, 2020.\n\n- Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. arXiv:1704.00109, 2017.\n\n- Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv:2212.04089, 2022a.\n\n- Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. NeurIPS, 2022b.\n\n- Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. UAI, 2018.\n\n- Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur. Repair: Renormalizing permuted activations for interpolation repair. arXiv:2211.08403, 2022.\n\n- Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2018.\n\n- Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for fine-grained image categorization: Stanford dogs. In CVPR Workshop on Fine-Grained Visual Categorization (FGVC), 2011.\n\n- James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 2017.\n\n- Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In ICML, 2019.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 2017.\n\n- Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent Learning: Do different neural networks learn the same representations? arXiv:1511.07543, 2015.\n\n- Zhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 2017.\n\n- Chang Liu, Chenfei Lou, Runzhong Wang, Alan Yuhan Xi, Li Shen, and Junchi Yan. Deep neural network fusion via graph matching with applications to model ensemble and federated learning. In ICML, 2022.\n\n- Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. arXiv:2111.09832, 2021.\n\n- Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics. PMLR, 2017.\n\n- Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant feature representation. In ICML, 2013.\n\n- Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? NeurIPS, 2020.\n\n- Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012.\n\n- Fidel A Guerrero Pena, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric Granger, and Marco Pedersoli. Re-basin via implicit sinkhorn differentiation. arXiv:2212.12042, 2022.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.\n\n- Alexandre Ram\u00e9, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, L\u00e9on Bottou, and David Lopez-Paz. Recycling diverse models for out-of-distribution generalization. arXiv:2212.10445, 2022.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n\n- Berfin Simsek, Fran\u00e7ois Ged, Arthur Jacot, Francesco Spadaro, Clement Hongler, Wulfram Gerstner, and Johann Brea. Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances. In Proceedings of the 38th International Conference on Machine Learning. PMLR, 2021.\n\n- Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. NeurIPS, 2020.\n\n- Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, and Lijuan Wang. An empirical study of multimodal model merging, 2023.\n\n- Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. NeurIPS, 2017.\n\n- Norman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, and Rongjie Lai. Optimizing mode connectivity via neuron alignment. NeurIPS, 2020.\n\n- Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In CVPR, 2015.\n\n- Johannes Von Oswald, Seijin Kobayashi, Joao Sacramento, Alexander Meulemans, Christian Henning, and Benjamin F Grewe. Neural networks with late-phase weights. arXiv:2007.12927, 2020.\n\n- Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. arXiv:2002.06440, 2020.\n\n- Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. TKDE, 2022.\n\n- P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.\n\n- Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In ICML, 2022a.\n\n- Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In CVPR, 2022b.\n\n- Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Resolving interference when merging models. arXiv:2306.01708, 2023.\n\n- Masanori Yamada, Tomoya Yamashita, Shin'ya Yamaguchi, and Daiki Chijiwa. Revisiting permutation symmetry for merging models between different datasets, 2023.\n\n- Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In ICML, 2019.\n\n- Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In CVPR, 2022.\n\n- Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638428.1842997", "msecs": "184.0", "relativeCreated": "25014.79721069336", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Inductive Linguistic Reasoning with Large Language Models\n\n# Abstract\nEvaluating large language models (LLMs) on their linguistic reasoning capabilities is an important task to understand the gaps in their skills that may surface during large-scale adoption. In this work, we investigate the abilities of such models to perform abstract multilingual reasoning through the lens of linguistic puzzles on extremely low-resource languages. As these translation tasks involve inductive and deductive reasoning from reference instances, we examine whether diverse auxiliary demonstrations can be automatically induced from seed exemplars, through analogical prompting. We employ a two-stage procedure, first generating analogical exemplars with a language model, and then applying them in-context along with provided target language exemplars. We explore various combinations of language models as analogical generators and reasoning agents, testing different model sizes and specialized multilingual LLMs. Our results on the modeLing dataset show that analogical prompting is effective in eliciting models' knowledge of language grammar similarities, boosting the performance of GPT-4o by as much as 8.1\\% and Llama-3.1-405B by 5.9\\% over chain-of-thought approaches. These gains are realized with self-generated analogical demonstrations as well as those generated by weaker multilingual models. We also report several findings about interesting phenomena which drive linguistic reasoning performance, suggesting that such puzzles are a valuable benchmark for new reasoning methods.\n\n\n# 1 INTRODUCTION\n\n\nAs the capabilities of large language models (LLMs) continue to grow, it is necessary to develop ways of testing the boundaries of their ability to reason over a wide range of languages. In particular, adapting language models to low-resource languages is challenging due to a lack of high-quality annotated data in the target language for supervised fine-tuning. This has led to zero-shot and few-shot transfer learning approaches being more commonly employed (Zoph et al., 2016; Nguyen & Chiang, 2017; Lin et al., 2019). However, given the emergence of the in-context learning phenomenon in LLMs, we hypothesize that this behavior can be used to enable few-shot generalization to new languages at inference time.\n\n\nIn this work, we explore the task of linguistic reasoning, using linguistics puzzles akin to the International Linguistics Olympiad (IOL). Notably, in these puzzles, the target language is often extremely low-resource or functionally extinct (Bean et al., 2024). While prior work has largely examined the effect of vanilla in-context learning with English-target and target-English exemplars, chain-of-thought prompting, and traditional neural machine translation methods (Chi et al., 2024; \u015eahin et al., 2020), we believe that generating auxiliary exemplars which supplement the target language demonstrations can guide the model to more effectively learn grammar similarities over a language family. As such, we introduce an approach based on analogical prompting (Yasunaga et al., 2024), which uses strong language models to self-generate exemplars of relevant problems given the test instance and performs in-context learning conditioned on those demonstrations. In our setting, the knowledge retrieval-like nature of analogical prompting allows us to test models' parametric understanding of language families, performing inference with both the provided and induced demonstrations.\n\n\nWe evaluate our approach on the modeLing (Chi et al., 2024) dataset, consisting of unseen IOL-style problems. We find that strong models such as GPT-4o and Llama-3.1-405B-Instruct can identify the language family, similar languages within said family, generate exemplars in those similar\n\n\n![Figure 1: An illustration of our 2-stage analogical prompting approach, translating a phrase in Montenegrin to English. While prior works would solely provide exemplars translating between the source language and English and perform in-context learning, our method seeks diverse exemplars. Model  $M_{1}$  first identifies the language family (Slavic) and higher-resource languages in the family which the model has knowledge of (Croatian), then produces exemplars in those languages. Finally, both the original and generated set of exemplars are passed with the test puzzle to model  $M_{2}$  to perform the translation.  $M_{1} = M_{2}$  yields the self-generated analogical reasoning setting.](images/631acd980f3520aa3eca145aea5e5a331a5358622aac1bf7a29675297520d3dc.jpg)\n\n\nlanguages, and apply them to solve the test puzzle. Furthermore, while weak models do not benefit significantly from using strong model-generated exemplars, strong models improve from using exemplars produced by weaker yet specialized multilingual models (e.g. Aya-35B). Our findings show that the ability of the model to deduce and apply rules, following inductive learning from the exemplars, largely influences performance; where there is still much to be desired relative to the reasoning of human experts on this task. From our results, we suggest that the linguistic reasoning task presents a fertile ground for research on new language model reasoning methods, to uncover how the skills which drive logical thinking may be imbued to models.\n\n\n# 2 ANALOGICAL PROMPTING FOR LINGUISTIC REASONING\n\n\nAnalogical prompting (Yasunaga et al., 2024) avoids the need for annotated exemplars by relying on a strong model to generate exemplars which are related to the test instance, but are sufficiently diverse relative to one another and the test sample. Our approach of applying analogical prompts follows the human system 2 thinking framework of slow, deliberate reasoning (Kahneman, 2011). In chain-of-thought prompting for these puzzles, the model performs in-context learning with the given exemplars, learning the rules governing the language by induction, including the meaning of particular words, and using deduction to apply these rules to the test sample. This approach is supported by prior works demonstrating the ability of LMs to learn rules and attempt to apply them (Qiu et al., 2024; Zhu et al., 2024). Furthermore, we do not have access to complete grounding sources of human-written rules governing these low-resource languages, so we must rely on the LM to identify and generate these rules itself. However, as we expect the model to have little to no prior knowledge about the target language<sup>1</sup>, we seek to leverage other languages with similar grammar structure which the model has learned in order to guide the language model's reasoning process.\n\n\nWe use language families as a taxonomically-grounded means of identifying similar languages as the target. The generated exemplars provide a source of reasoning support to the model, enabling it to perform inductive reasoning first in a cross-lingual manner over the diverse exemplars, and then deduce from its shared understanding. For instance, in Figure 1, given the test instance and the provided exemplars in Montenegrin, we leverage the model to (i.) identify the family of Montenegrin (Slavic Language Family), (ii.) select a few languages in the Slavic Language Family, and (iii.) generate example puzzles with their solutions in those selected languages, e.g. Croatian. Then, the provided exemplars and the generated cross-lingual demonstrations are provided to the model, to solve the given test puzzle.\n\n\nIn line with the view of analogical prompting as a knowledge retrieval procedure in accordance with the LM's pre-training distribution, we desire for the model to produce exemplars from languages it has learned, while probing its' understanding of language families. In (Qiu et al., 2024), models have been demonstrated to improve with more familiar exemplars (based on inclusion in the pretraining data). Furthermore, their work suggested that noisy demonstrations hurt performance; as the provided examples from the unseen target language could be considered as \"noisy\", we hypothesize that the generated exemplars can help to compensate.\n\n\nExemplar Correctness. While one would ideally prefer to have a validator which, given a set of rules for a language, can determine if they are being appropriately applied for each of the analogical exemplars, this is very challenging at scale. In the context of Linguistics Olympiad problems, only a small fraction of the population who are experts in such tasks (equivalent to achieving a high score on these contests) would be able to reliably annotate solution rationales for these extremely low-resource translation puzzles. Furthermore, the notion of correctness is ambiguous \u2013 we rely on exact match relative to an annotated \"correct\" response, but it is unclear if there could be more than one \"correct\" response which is context-specific, or if partial credit assignment could be possible. Given the models' lack of zero-shot knowledge of these languages (else, there would likely be leakage), we also cannot reliably use another language model as a validator. As a result, we leverage all generated exemplars by the model for inference, and assume each problem has one correct solution.\n\n\n# 2.1 LINGUISTICS PUZZLES\n\n\nAs noted before, the focus of this work is on linguistics puzzles \u2013 in particular, translation problems from English to a low-resource language and vice versa. Such problems are also referred to in the literature as Rosetta Stone puzzles, and constitute one of the most frequent types of problems that appear in Linguistics Olympiad competitions (\u015eahin et al., 2020; Chi et al., 2024; Bean et al., 2024). These problems typically consist of a test phrase in language A along with 5-10 exemplars<sup>2</sup> of translation from language A to language B and vice versa, and the task is to translate the given phrase into language B. We include an example of such a problem below, from Chi et al. (2024).\n\n\n# EXAMPLE TRANSLATIONS FROM ENGLISH TO RAPA NUI\n\n\nEnglish: We see you.  $\\rightarrow$  Rapa Nui: tike'a t\u0101tou koe  \nEnglish: I hear you.  $\\rightarrow$  Rapa Nui: aro'a au koe  \nEnglish: I see you.  $\\rightarrow$  Rapa Nui: tike'a au koe\n\n\nEnglish: We hear you.  $\\rightarrow$  Rapa Nui: aro'a tatou koe English: We bite the bone.  $\\rightarrow$  Rapa Nui: au tatou ivi\n\n\nEnglish: We hit the bird.  $\\rightarrow$  Rapa Nui: pu'a t\u0101tou manu\n\n\n# TRANSLATE TEST PHRASE\n\n\nEnglish: The bird bites you.  $\\rightarrow$  Rapa Nui: au manu koe\n\n\n# 3 METHODS\n\n\nWe explore a number of sampling methods across various language models to assess their performance on reasoning over unknown languages.\n\n\n# 3.1 BASELINES\n\n\nWe include the following methods as baselines for robust comparison to our method, reflecting prior work examined in linguistic reasoning (Chi et al., 2024). We explore their results in Section 4.1.\n\n\nZero-Shot Prompting. Given the low-resource nature of the languages that we examine, we expect zero-shot performance to be poor, or even zero, on the exact match metric. However, we include this setting for two reasons: (1.) a model which gets multiple questions correct for a given language with zero-shot prompting may be an indication of leakage, and (2.) this serves as a robust check on any additional metrics examined aside from exact match.\n\n\nFew-Shot Prompting / In-Context Learning. As in the Linguistic Olympiad competitions, demonstrations of translation to and from the low-resource language are provided to the model, with the intention for inductive reasoning to guide the model towards identifying the set of grammar rules the language follows.\n\n\nFew-Shot Chain-of-Thought Reasoning. Given the efficacy of chain-of-thought prompting (Wei et al., 2024; Kojima et al., 2022), we extend the few-shot evaluation setting by including prompts for the model to \"think step-by-step\" (Kojima et al., 2022; Yang et al., 2024a). We also include a chain-of-thought rationale exemplar for English-Spanish translation from (Chi et al., 2024), to demonstrate how step-by-step reasoning rationales should be produced, and are denoted in Section 4 as \"w/ rationale\".\n\n\n# 3.2 ANALOGICAL PROMPTING VARIATIONS\n\n\nWe describe the various analogical prompting methods explored in the experiments; their results are in Section 4.2.\n\n\nAnalogical Prompting on Language Families. As noted in Section 2, we seek to use language families as a means to identify similar, auxiliary languages whose exemplars can boost the model's cross-lingual understanding. In a similar environment to the Linguistics Olympiad competition, where one does not have access to any external resources, we test the model on its latent understanding of language families and regional associations to generate further exemplars and puzzles in another language within the same language subgroup as the target language. For a target language  $L$ , we prompt the model to identify a few other languages (denote this list  $L_{Aux}$ ) in the same family as  $L$ ; then, for each language in  $L_{Aux}$ , generate a puzzle translating from  $l$  to English, and a puzzle in the reverse direction. Then, we apply these exemplars along with the given ones for  $L$  in a new instruction to the model. We term this 2-stage analogical reasoning.\n\n\nSeparating the two stages of analogical prompting (generation and application) yields an opportunity to explore how different combinations of models for this approach might perform. While the above entails using the same model for both steps, we look to contrast the strength of the models used, to attempt to boost the performance of both frontier and small models.\n\n\nInference-time Exemplar Distillation. In our work, inference-time distillation refers to generating analogical exemplars with a strong model (e.g. GPT-4o) and applying them to a weak model (e.g. models with roughly 7-8B parameters). Our hypothesis driving this setting is: can higher quality exemplars produced by strong models enable better deductive abilities with weak models?\n\n\nWeak-to-Strong Cross-Linguual Analogies. Specialized multilingual models such as the Aya-23 models hold promise for our linguistic reasoning analysis, as they have been fine-tuned for instruction-following across a wide range of languages. We propose using such models for generating analogical demonstrations, as they may have a stronger understanding of language families and can produce diverse exemplars, which we believe strong models may be able to deduce from.\n\n\n# 3.3 EXPERIMENTAL SETUP\n\n\nDatasets. We primarily evaluate our approaches on the modeLing dataset (Chi et al., 2024). This dataset consists of problems written by the authors and hence uninvolved in prior Linguistics\n\n\nOlympiads. This benchmark was released in 2024, and we rely on its recency to be more assured that leakage is not a factor driving performance. We note that all problems examined are purely text-based; while there exist linguistics puzzles that require deduction from images, filling in diagrams, etc., the benchmark we evaluate on does not include such problems. This suggests that future work could study the performance of multimodal models on these problem types. We also evaluate on the LINGOLY dataset (Bean et al., 2024), which features 1,133 problems and expands beyond \"Rosetta Stone\" translation problems to include grammatical pattern-based translation, matching translation pairs, text purely in an unknown language, identifying errors in machine translation, and longer text in multiple languages. The results are included in Appendix D.\n\n\nModels. We evaluate with the following models:\n\n- - OpenAI models: GPT-4o, GPT-4, and GPT-3.5-turbo\n\n- - Open-weight models: Llama 3.1 8B-Instruct, Llama 3.1 70B-Instruct, Llama 3.1 405B-Instruct (Dubey et al., 2024), Mixtral 8x7B-Instruct-v0.1 (Jiang et al., 2024), and Mixtral 8x22B-Instruct-v0.1\n\n- - Multilingual Instruction-tuned Models: Aya-23 8B and Aya-23 35B (henceforth referred to as Aya-8B and Aya-35B) (Aryabumi et al., 2024)\n\n\nOpenAI models are inferred with the OpenAI API, while the open-weight and multilingual instruction-tuned models are queried with the Together AI API and Apple MLX, respectively.\n\n\n# 4 RESULTS\n\n\nWe report exact match (EM) scores for all experiments performed. ChrF2 Popovic (2015), a character n-gram F-score measure, and corpus-level BLEU scores (Papineni et al., 2002) are recorded in Appendix A. We do not treat these as primary metrics as BLEU ignores word ordering nuances amidst short responses in machine translation, which is integral to measuring correctness in the puzzles we explore (Callison-Burch et al., 2006; Chi et al., 2024), and we find the ChrF scores to be noisy relative to EM scores. Smaller models with weaker instruction-following capabilities often failed to produce their output in the exact desired format specified in the prompts. To ensure that reliable exact match scores are reported while some responses may have parsing issues relative to the expected format, the authors of this work manually examined each response to confirm whether the output generated contains the target response. To enforce standardization across our evaluation procedure, this was performed for all experiments; this was not applicable for stronger models whose responses exactly followed the desired output format.\n\n\n# 4.1 CHAIN-OF-THOUGHT LINGUISTIC REASONING\n\n\nThe results of baseline methods are in Table 1. The prompts for all experiments are included in Appendix F, and all experiments are averaged over 3 runs. For the \"CoT with rationale experiment\", we take the best of using 512 and 4096 max tokens (see Appendix B). For the \"few shot\" results, we take the best out of two different prompt settings, ablated on in Appendix C.\n\n\nOur strongest baseline result is achieved with Llama-3.1-405B-Instruct producing CoT rationales, at  $65.81\\%$ ; in fact, this model produces the best results across all 4 baseline settings. GPT-4o remains in the high 50s, but does not exceed  $60\\%$  on any single run. Among smaller models, Llama-3.1-8B-Instruct performs comparably to Aya-35B and Mixtral-8x7B-Instruct, outperforming it on some baselines, which may be attributable to a stronger and more recent base model. We also observe that GPT-4o and Llama-3.1-405B-Instruct do indeed solve a few puzzles (2 and 4 samples, respectively) in the zero-shot setting. Given the former was released before the modeLing dataset, and the latter was released just shortly after, we do not believe this to be a sign of leakage; furthermore, each correct question was from a different language.\n\n\nWe report a few key observations below:\n\n\nStrong models produce rationales without being instructed to. We find that strong models such as GPT-4o and GPT-4 produce chain-of-thought stepwise rationales for responses, even in the zero-shot and few-shot settings, without including a chain-of-thought prompt or including rationales in\n\n\nTable 1: Baseline experimental results using chain-of-thought methods, reporting exact match. The models have been split into three groups, corresponding to the models noted in Section 3.3. All results reported are average of 3 runs at a temperature of 0.3, to address sampling variance.\n\n\n<table><tr><td>Model</td><td>Zero-Shot</td><td>Few-Shot w/o CoT</td><td>Few-Shot w/ CoT</td><td>Few-Shot CoT w/ Rationales</td></tr><tr><td>GPT 3.5-Turbo</td><td>0%</td><td>25.74%</td><td>26.10%</td><td>38.6%</td></tr><tr><td>GPT-4</td><td>0%</td><td>56.25%</td><td>45.22%</td><td>45.59%</td></tr><tr><td>GPT-4o</td><td>1.10%</td><td>59.19%</td><td>58.82%</td><td>55.88%</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>0%</td><td>22.79%</td><td>16.91%</td><td>23.16%</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>0%</td><td>45.22%</td><td>44.49%</td><td>42.28%</td></tr><tr><td>Llama-3.1-405B-Instruct</td><td>1.47%</td><td>61.76%</td><td>59.19%</td><td>65.81%</td></tr><tr><td>Mixtral-8x7B-Instruct</td><td>0%</td><td>22.43%</td><td>22.06%</td><td>16.18%</td></tr><tr><td>Mixtral 8x22B-Instruct</td><td>0%</td><td>45.59%</td><td>43.38%</td><td>39.71%</td></tr><tr><td>Aya-23-8B</td><td>0%</td><td>9.93%</td><td>7.35%</td><td>5.88%</td></tr><tr><td>Aya-23-35B</td><td>0%</td><td>23.53%</td><td>20.59%</td><td>14.34%</td></tr></table>\n\n\nthe exemplars. This is a key reason why the few-shot without chain-of-thought setting performs the highest for both models. Furthermore, when prompted with rationale-inducing exemplars (see Appendix F), these strong models produce rule libraries from the exemplars, akin to (Zhu et al., 2024), leading to very lengthy responses; some models such as Llama-3.1 70B fall into loops of repeating the same rule many times. This necessitates the use of a higher number of max tokens to be generated, to ensure that the final answer is indeed outputted, although this makes human verification of response correctness harder due to their length; we report ablations on this in Appendix B.\n\n\nCertain models perform uncertainty-based refusal. Some models, such as Mixtral-8x7B-Instruct and Mixtral-8x22B-Instruct respond to test instances by stating an inability to perform the desired task. This behavior especially appears in CoT with rationale exemplars; interestingly, this occurs after the implicit induction stage has been performed. For instance, with Mixtral-8x22B-Instruct, the model enumerates a set of word-level translations between the target language and English, respectively, then upon recognizing ambiguity in one of the word-level translations, it claims that solving the problem is impossible without additional information. By contrast, models such as GPT-4o instead output multiple candidate answers when it is not entirely certain. We include qualitative examples of this behavior in Appendix E. This appears to reinforce the findings of the Qiu et al. (2024) in that models are unable to reliably apply their inductively learned rules.\n\n\nOur analogical reasoning method introduces an inference-time approach to boost deductive reasoning, by deliberately using their learned multilingual knowledge to guide puzzle solving.\n\n\n# 4.2 TWO-STAGE ANALOGICAL REASONING\n\n\nTo critically explore the evaluation settings introduced in Section 3.2, we select 2 frontier models \u2013 GPT-4o and Llama-3.1-405B-Instruct \u2013 which were the strongest performers in our baselines. We select 2 weaker models \u2013 Aya-35B and Llama-3.1-8B-Instruct \u2013 for the inference-time distillation and weak-to-strong prompting experiments. These models performed comparably to one another in the baselines, and allow us to contrast multilingual specialization against a generalist model with multilingual support. The experiments with Llama-3.1-8B-Instruct are included in Appendix I.\n\n\nWe also establish an upper bound on the performance we can attain with our approach, by a psuedo-open-book method with oracle language families. That is, for each language in the evaluation set, rather than prompting the model to implicitly infer the language family and other languages which are a member of it, we abstract away the former by providing the language family in the prompt. We suggest that a human expert with strong cross-lingual reasoning abilities would be able to deduce such relationships with similar languages, so providing language family labels eliminates one uncertainty source in the model's generations. The results of this are included in Figure 2b.\n\n\nAnalogical reasoning boosts frontier models. We find that 2-stage analogical reasoning pushes the boundaries of the performance of frontier models, relative to their best baseline results. Solely considering the self-generation setting (where the same model both generates analogical exemplars\n\n\nTable 2: Pairwise results with our 2-stage analogical prompting method. The left column denotes the model generating the analogical exemplars, and the top row denotes the model applying both the generated and provided exemplars to answer the test puzzle. Note that these results address the self-generated analogical reasoning, inference-time distillation, and weak-to-strong prompting settings posed in Section 3.2.\n\n\n<table><tr><td>Deducer\nGenerator</td><td>GPT-4o</td><td>Llama-3.1-405B-Instruct</td><td>Aya-23-35B</td></tr><tr><td>GPT-4o</td><td>66.91%</td><td>71.69%</td><td>21.32%</td></tr><tr><td>Llama-3.1-405B-Instruct</td><td>67.28%</td><td>67.65%</td><td>20.22%</td></tr><tr><td>Aya-23-35B</td><td>65.44%</td><td>71.32%</td><td>15.44%</td></tr></table>\n\n\nand applies them), GPT-4o improves  $7.2\\%$  ( $59.19\\% \\rightarrow 66.91\\%$ ), and Llama-3.1-405B-Instruct improves  $1.8\\%$  ( $65.81\\% \\rightarrow 67.65\\%$ ). We subsequently observe even stronger gains for both models as the deducer, when selecting different models as the analogical exemplar generator. In the first stage, both of these frontier models correctly identify the language family at a fairly high rate (see Appendix H), select a few languages from said family, and generate analogical puzzles for those auxiliary languages, as intended. Then, in the second stage, the model considers the tokens in the test phrase, and analyzes how each is to be translated to the target language, and combines them together in the appropriate order following the syntactical patterns observed from the given exemplars. Thus, it appears that the model uses the analogical exemplars to better induce the mappings of words in the target language to words in the source language, which it then applies to the target phrase.\n\n\nWeak analogical \"supervision\" improves performance. We find that generating the analogical exemplars with Aya-35B and applying them to the test sample with Llama-3.1-405B-Instruct yields  $71.32\\%$ , averaged over 3 runs; a  $5.5\\%$  improvement over the best baseline for Llama-3.1-405B-Instruct  $(65.81\\% \\rightarrow 71.32\\%)$ . We similarly find that leveraging Aya-35B-generated exemplars and applying them with GPT-4o yields a  $6.2\\%$  improvement over the best GPT-4o baseline setting  $(59.19\\% \\rightarrow 65.44\\%)$ . In the case of Llama-3.1-405B-Instruct, using Aya-generated exemplars outperforms using self-generated exemplars, by  $3.7\\%$ $(67.65\\% \\rightarrow 71.32\\%)$ . Our findings suggest that when equipped with the right tools (analogical demonstrations) from effective multilingual reasoners, strong deducers can thrive.\n\n\nThis claim is further reinforced by the inference-time distillation results: smaller models such as Aya-35B do not benefit from the analogical exemplars, regardless of the analogical generator. At the same time, using the GPT-4o exemplars applied by Llama-3.1-405B-Instruct yields  $71.69\\%$ , our strongest result across all evaluation settings. Moreover, the reverse direction (Llama-3.1-405B-Instruct exemplars applied by GPT-4o) yields an  $8.09\\%$  improvement over the best GPT-4o baseline result. From these findings, we conclude that analogical exemplars generated by good multilingual reasoners do not \"unlock\" deductive reasoning abilities for models without them (Aya-35B); however, for strong baseline reasoners (Llama-3.1-405B, GPT-4o), better exemplars help performance. $^{3}$\n\n\nFrontier models understand language families. We compare model performance with and without oracle language families, finding that prompting models to infer the language family is superior. We observe that frontier models such as GPT-4o and Llama-3.1-405B, as well as specialized multilingual models like Aya-35B, have a strong parametric knowledge of language families, and do not need to rely on language family labels to identify similar languages. Furthermore, the model performing retrieval of the language family helps it to identify a few languages within the family that will help it, bootstrapping from the provided exemplars, whereas providing the language family often leads to the model listing many languages in the family and attempting to produce exemplars for all of them. We hypothesize that this is a source of noise; demonstrations beyond a certain number yield diminishing returns in performance. That is, the oracle language families setting stimulates inductive cross-lingual reasoning, but makes deductive reasoning more challenging due to having\n\n\n![(a) Best baseline vs. best analogical](images/48245bc3b4d3c14218c341f144d4ec1edb2d635b531c78a09df87e2dbe38caf8.jpg)\n\n\n![(b) Inferred families vs. oracle families Figure 2: Figure (a) contains a comparison of the best baseline (in Table 1) with the best 2-stage analogical reasoning result (in Table 2), for our two frontier models as the deducer. We find analogical to improve GPT-4o by  $8.1\\%$  and Llama-3.1-405B-Instruct by  $5.9\\%$ . Figure (b) compares self-generated analogical reasoning methods, with prompt-determined language families (\"inferred families\") and human-annotated language family labels (\"oracle families\").](images/40673e39e356c374031350dd339d76203c7ff32b0726b4f620fc6b512f46cf29.jpg)\n\n\nmany exemplars. Specific examples of this behavior are included in Appendix E, and the language families table is in Appendix G. We also include further discussion on the language families identified by GPT-4o and Llama-3.1-405B-Instruct in the inferred families setting, in Appendix H, and find that they achieve a high correctness rate relative to the oracle labels.\n\n\nLanguage Isolates and Proxy Languages. Language isolates would appear to pose particular difficulty to our models, as by definition, they do not belong to any well-defined language family. As a result, we rely solely on the models' ability to trace grammatical correspondences based on the languages it has seen in pre-training, even for our experiments with oracle language family labels. While in the baseline experiments, our models often believed that the target language is imaginary, prompting for language families leads models to note that the language is isolate. They then attempt to either follow syntactic or morphological patterns to induce a new fictitious language which is similar to the target, or select learned geographically-proximate languages. For the language of Bangime, spoken in Mali, the model either retrieves languages from families in the same geographical region, such as Dogon, or creates a new language (e.g. \"Xangime\") for which it generates analogical exemplars (see Appendix E). Analyzing at the instance level, this improves the correctness on the Bangime puzzles from  $27.8\\%$  to  $50\\%$  for GPT-4o in the self-generated setting.\n\n\nIn summary, our results suggest that the ability of the model to deduce by leveraging the given and analogically-generated exemplars is the key performance driver. This is lent credence by the efficacy of weak-to-strong prompting (i.e. relying on the exemplars of Aya-35b), while the performance of inference-time distillation remains roughly similar. Thus, we posit that the \"strength\" of a linguistic reasoning agent can be decomposed along two lenses, corresponding to our two stages: (1.) generating analogical exemplars by language identification and multilingual reasoning, and (2.) deducing from hypotheses in complex evaluation settings.\n\n\n# 5 RELATED WORK\n\n\n# 5.1 LARGE LANGUAGE MODEL REASONING.\n\n\nFew-shot Chain-of-Thought Reasoning. In-context learning has emerged as an exciting phenomenon in language models, enabling them to learn from few-shot demonstrations at inference-time to generalize to various tasks (Brown et al., 2020; Wei et al., 2022). At the same time, the chain-of-thought (CoT) reasoning method guides language models to think systematically through a problem, in a step-by-step manner (Wei et al., 2024; Kojima et al., 2022). In particular, applying chain-of-thought prompting (zero-shot or few-shot) with the goal to induce rationales yields explanations of why each step was performed, along with performance and faithfulness improvements (Nye et al., 2021; Lampinen et al., 2022). Various similar approaches (Yao et al., 2023; Wang et al., 2023; Besta et al., 2024) have been proposed to sample more diverse generations from models, leveraging test-time compute to improve reasoning performance; we believe such methods make for interesting lines of future research for the linguistic reasoning task.\n\n\nInductive Reasoning in LLMs. Inductive and deductive reasoning skills in language models have often been studied in the context of logical or abstract reasoning problems. Much of this prior work on inductive reasoning with language models studies evaluation settings with more clearly defined rules to be inductively learned and then applied; these works suggest gaps relative to the human intelligence in performing both inductive and deductive reasoning (Xu et al., 2024; Gendron et al., 2024; Yang et al., 2024c). In particular, Yang et al. (2024c) notes the need for more challenging tasks in inductive reasoning to better assess the boundaries of LM capabilities, such as hypothesis generation and pattern induction. Works such as Tang et al. (2023) demonstrate that models struggle to create rules by induction when the semantics of the exemplars do not follow in a commonsense manner \u2013 in our work, generating analogical exemplars similar to the models pre-training data may steer the model towards a relative \"commonsense\" representation of the rules underlying the exemplars.\n\n\nSeveral works dive into the realm of hypothesis search, determining the ability of LMs to pose hypotheses about the problem (e.g. rules which exemplars follow) before seeking to deductively apply them (Zhu et al., 2024; Qiu et al., 2024; Wang et al., 2024). Zhu et al. (2024) propose hypotheses-to-theories (HtT), which learns a rule library from an induction stage, and then applies it by a deduction stage; this multi-stage method is similar to our analogical approach, although we still perform both induction and deduction together after analogical generation. Furthermore, their rule library depends on verification \u2013 this is not possible in the linguistic reasoning task due to the lack of a reliable feedback source to judge responses, aside from expert humans. As discussed earlier in our work, Qiu et al. (2024) demonstrates that models can propose rules well, but cannot consistently apply them. Wang et al. (2024) proposes Hypothesis Search, a method which proposes hypotheses, implements a subset of them as Python programs, and applies them to training samples to verify their correctness.\n\n\nExemplar Generation and Automated Reasoning. Analogical prompting (Yasunaga et al., 2024) has been demonstrated to be an effective inference-time method to produce diverse, task-conditioned exemplars, improving in-context learning. As noted above, this effectively serves as a knowledge retrieval method which retrieves exemplars similar to (or directly from) the pre-training distribution which the model has seen; RECITE (Sun et al., 2023) similarly retrieves passages directly from the model's memory. Methods such as SG-ICL (Kim et al., 2022) and Auto-ICL (Yang et al., 2024b) also self-generate in-context exemplars in a similar manner as analogical prompting.\n\n\n# 5.2 MULTILINGUAL REASONING.\n\n\nMultilingual reasoning in LMs for low-resource languages poses a unique challenge, as the pretraining corpora and supervised fine-tuning datasets for many models are largely concentrated on a few high-resource languages. XLT (Huang et al., 2023) introduces a prompt template which translates problems in other languages to English and solves the problem with chain-of-thought in English. Qin et al. (2023) aligns each step in the chain-of-thought between the source language and English explanations, then solves the problem given this alignment; they also apply self-consistency with cross-lingual alignments with a set of pre-specified target languages. Li et al. (2024) trains on code data with multilingual comments, while using multilingual code prompts at inference time with symbolic function API calls as a structured way to solve the reasoning problem.\n\n\nLinguistic Reasoning Benchmarks. The PuzzLing Machines dataset (Sahin et al., 2020) first introduced a set of Linguistics Olympiad problems to study the ability of language models to learn from a small amount of data; they apply RoBERTa-based neural machine translation methods, but demonstrate a vast gap (attaining less than  $4\\%$  exact match performance). With concerns of potential leakage given the vast web scraping performed in procuring pre-training tokens for language model training, modeLing (Chi et al., 2024) introduced a new set of hand-written Linguistics Olympiad problems, demonstrating the performance of current models with CoT methods. The LINGOLY (Bean et al., 2024) dataset presents problems from the UK Linguistics Olympiad competition, and studies zero-shot and few-shot performance of current models categorized by question type.\n\n\n# 6 DISCUSSION\n\n\nWe propose applying analogical prompting as a test of inductive reasoning from diverse exemplars for challenging linguistic puzzles. Our results encouragingly suggest that despite frontier models struggling with deductive reasoning, in line with the findings of (Qiu et al., 2024), they can indeed\n\n\nfollow grammar rule similarities to generate analogical demonstrations and attempt to apply them adeptly. This yields improved performance in self-generated analogical prompting with GPT-4o and Llama-3.1-405B, as well as weak-to-strong prompting for those models employing Aya-35B-generated demonstrations. We also show that Llama-3.1-405B-Instruct is the best model for linguistic reasoning at present, becoming the first model to achieve over  $70\\%$  on the modeLing benchmark by way of our 2-stage analogical reasoning approach. This could be attributed to the vast scale of multilingual pre-training data (15T tokens), as well as multilingual supervised fine-tuning and dialogue data seen in adaptation. The multilingual prowess of the Llama-3.1 models is evidenced by its strong performance on the MGSM and multilingual MMLU benchmarks (Dubey et al., 2024).\n\n\nFurthermore, the ability of smaller and specialized multilingual models (Aya) to generate coherent analogical exemplars, which improve frontier models over their own self-generated exemplars, is promising towards developing widely-available multilingual reasoners. We find that the improvements observed can be attributed to the auxiliary exemplars generated, which are in turn due to the model's understanding of language families and grammar rules from the pre-training data or its multilingual adaptation. The errors made by current models due to an inability to apply diverse and complex exemplars suggest that the linguistic reasoning task is an exciting and challenging evaluation setting for LM reasoning at large. That is, seeking to emulate human reasoning, where deduction involves a clear application of recognized patterns, provides a ripe space for future work.\n\n\nThe interesting phenomenon examined with language isolates also provides a glimpse of model capabilities to follow grammatical similarities, rather than relying on knowledge retrieval of language families. That is, the multilingual language understanding abilities of frontier models expand beyond typological knowledge, going so far as to create proxy fictitious languages which enable it to solve the problem correctly. We suggest that future efforts in multilingual adaptation be placed in identifying techniques to guide languages models to support typologically unique languages.\n\n\nResearch at the intersection of machine translation and reasoning in the development of the latest foundation models is important from a societal perspective. With large language models being adopted widely, the need for multilingual capabilities and rapid adaptation grows, and our work proposes an effective method by which this can be performed at test-time. Notably, we have demonstrated evidence that models follow language similarities \u2013 given the massive number of languages and dialects present worldwide, this could help guide humans to learn dying languages, thus keeping their tradition alive, while doing so in a scalable manner. We hope that these findings can inspire future models releases to include evaluation on challenging multilingual tasks such as these puzzles, and research on reasoning can explore the multilingual setting further in depth.\n\n\n**Limitations** We note that the reliance on exact match scoring as our primary signal of performance is not ideal, as it is a binary indicator. We have sought to examine other metrics which correspond to \"partial credit\" such as ChrF2 and BLEU; however, there are flaws in these methods as well. A stronger human understanding of the rules which these extremely low-resource languages follow could guide us to better metrics, especially capturing semantic meaning and word ordering inversions, where appropriate. For instance, some languages might retain the same meaning while inverting the word order \u2013 exact match is sensitive to this, and while ChrF2 and BLEU are not, we should only be insensitive to ordering for languages which follow this property. We also recognized that the IOL 2024 problems could not be used as a benchmark with our method, as they require multimodality \u2013 our method only analyzes unimodal text problems. Another limitation of our work is that we do not have a reliable means of verifying the correctness of analogical exemplars, nor contrasting the quality of exemplars generated across models to determine the best analogical generator model. An expert annotator who could identify where a mistake was made in the model's reasoning process also would have been helpful to yield further insights into the fallacies of current models' linguistic reasoning. Nonetheless, our most effective deducer models are able to leverage exemplars generated by models of various sizes for improved linguistic reasoning.\n\n\nReproducibility Statement. We include all prompts used for generating our baseline experimental results, and all analogical prompting methods, in Appendix F. We have also broken down the two stages of our analogical reasoning method for clarity on how the method should be applied with two separator models (e.g. weak-to-strong prompting, inference-time exemplar distillation). We evaluate our work on the modeLing dataset, which is publicly available. We have included details of the platforms through which the models we evaluate have been queried (OpenAI API, TogetherAI API, Apple MLX), along with the list of models studied. Lastly, we will release our code publicly.\n\n\n# REFERENCES\n\n- Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet \u00dcst\u00fcn, and Sara Hooker. Aya 23: Open weight releases to further multilingual progress, 2024. URL https://arxiv.org/abs/2405.15032.\n\n- Andrew M. Bean, Simi Hellsten, Harry Mayne, Jabez Magomere, Ethan A. Chi, Ryan Chi, Scott A. Hale, and Hannah Rose Kirk. Lingoly: A benchmark of olympiad-level linguistic reasoning puzzles in low-resource and extinct languages, 2024. URL https://arxiv.org/abs/2406.06196.\n\n- Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefer. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17682-17690, March 2024. ISSN 2159-5399. doi: 10.1609/aaai.v38i16.29720. URL http://dx.doi.org/10.1609/aaaai.v38i16.29720.\n\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.\n\n- Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Diana McCarthy and Shuly Wintner (eds.), 11th Conference of the European Chapter of the Association for Computational Linguistics, pp. 249-256, Trento, Italy, April 2006. Association for Computational Linguistics. URL https://aclanthology.org/E06-1032.\n\n- Nathan Chi, Teodor Malchev, Riley Kong, Ryan Chi, Lucas Huang, Ethan Chi, R. McCoy, and Dragomir Radev. ModeLing: A novel dataset for testing linguistic reasoning in language models. In Michael Hahn, Alexey Sorokin, Ritesh Kumar, Andreas Shcherbakov, Yulia Otmakhova, Jinrui Yang, Oleg Serikov, Priya Rani, Edoardo M. Ponti, Saliha Murado\u011flu, Rena Gao, Ryan Coterell, and Ekaterina Vylomova (eds.), Proceedings of the 6th Workshop on Research in Computational Linguistic Typology and Multilingual NLP, pp. 113-119, St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.sigtyp-1.14.\n\n- Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\n- Gael Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. Large language models are not strong abstract reasoners, 2024. URL https://arxiv.org/abs/2305.19555.\n\n- Haoyang Huang, Tianyi Tang, Dongdong Zhang, Xin Zhao, Ting Song, Yan Xia, and Furu Wei. Not all languages are created equal in LLMs: Improving multilingual capability by cross-lingual-thought prompting. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 12365\u201312394, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023-findings-emnlp.826. URL https://aclanthology.org/2023-findings-emnlp.826.\n\n- Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of experts, 2024. URL https://arxiv.org/abs/2401.04088.\n\n- Daniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, New York, 2011. ISBN 9780374275631 0374275637. URL https://www.amazon.de/ Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=wl_it_dp_o_pdT1_nS_nC?ie  $\\equiv$  UTF8&colid  $=$  151193SNGKJT9&coliid  $=$  I3OCESLZCVDFL7.\n\n- Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang goo Lee. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator, 2022. URL https://arxiv.org/abs/2206.08082.\n\n- Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 22199-22213. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf.\n\n- Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from explanations in context? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 537-563, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022-findings-emnlp.38. URL https://aclanthology.org/2022-findings-emnlp.38.\n\n- Bryan Li, Tamer Alkhouli, Daniele Bonadiman, Nikolaos Pappas, and Saab Mansour. Eliciting better multilingual structured reasoning from LLMs through code. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5154\u20135169, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.281. URL https://aclanthology.org/2024.acl-long.281.\n\n- Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios Anastasopoulos, Patrick Littell, and Graham Neubig. Choosing transfer languages for cross-lingual learning. In Anna Korhonen, David Traum, and Lluis M\u00e1rquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3125\u20133135, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1301. URL https://aclanthology.org/P19-1301.\n\n- Toan Q. Nguyen and David Chiang. Transfer learning across low-resource, related languages for neural machine translation. In Greg Kondrak and Taro Watanabe (eds.), Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 296-301, Taipei, Taiwan, November 2017. Asian Federation of Natural Language Processing. URL https://aclanthology.org/I17-2050.\n\n- Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114.\n\n- Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin (eds.), Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.\n\n- Maja Popovic. chrF: character n-gram F-score for automatic MT evaluation. In Ondrej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina (eds.), Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 392\u2013395, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https://aclanthology.org/W15-3049.\n\n- Matt Post. A call for clarity in reporting BLEU scores. In Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9veol, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186-191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https://aclanthology.org/W18-6319.\n\n- Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 2695-2709, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.163. URL https://aclanthology.org/2023.emnlp-main.163.\n\n- Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=bNt7oajl2a.\n\n- G\u00f6zde G\u00fcl \u015eahin, Yova Kementchedjhieva, Phillip Rust, and Iryna Gurevych. PuzzLing Machines: A Challenge on Learning From Small Data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 1241-1254, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.115. URL https://aclanthology.org/2020.acl-main.115.\n\n- Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=-cqvvvb-NkI.\n\n- Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. Large language models are in-context semantic reasoners rather than symbolic reasoners, 2023. URL https://arxiv.org/abs/2305.14825.\n\n- Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman. Hypothesis search: Inductive reasoning with language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=G7UtIGQmjm.\n\n- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. URL https://arxiv.org/abs/2203.11171.\n\n- Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. URL https://arxiv.org/abs/2206.07682.\n\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088.\n\n- Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. Are large language models really good logical reasoners? a comprehensive evaluation and beyond, 2024. URL https://arxiv.org/abs/2306.09841.\n\n- Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers, 2024a. URL https://arxiv.org/abs/2309.03409.\n\n- Jinghan Yang, Shuming Ma, and Furu Wei. Auto-icl: In-context learning without human supervision, 2024b. URL https://arxiv.org/abs/2311.09263.\n\n- Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. Language models as inductive reasoners. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 209-225, St. Julian's, Malta, March 2024c. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-long.13.\n\n- Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 11809-11822. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf.\n\n- Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. Large language models as analogical reasoners. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=AgDICX1h50.\n\n- Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. Large language models can learn rules, 2024. URL https://arxiv.org/abs/2310.07064.\n\n- Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning for low-resource neural machine translation, 2016. URL https://arxiv.org/abs/1604.02201.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638429.156965", "msecs": "156.0", "relativeCreated": "25987.462520599365", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability\n\n# Abstract\nPrecipitation nowcasting, a short-term (up to six hours) rain prediction, is arguably one of the most demanding weather forecasting tasks. To achieve accurate predictions, a forecasting model should consider miscellaneous meteorological and geographical data sources. Currently available datasets provide information only about precipitation intensity, vertically integrated liquid (VIL), or maximum reflectivity on the vertical section. Such single-level or aggregated data lacks description of the reflectivity change in vertical dimension, simplifying or distorting the corresponding models. To fill this gap, we introduce an additional dimension of the precipitation measurements in the RuDar dataset that incorporates 3D radar echo observations. Measurements are collected from 30 weather radars located mostly in the European part of Russia, covering multiple climate zones. Radar product updates every 10 minutes with a 2 km spatial resolution. The measurements include precipitation intensity (mm/h) at an altitude of 600 m, reflectivity (dBZ) and radial velocity (m/s) at 10 altitude levels from 1 km to 10 km with 1 km step. We also add the orography information as it affects the intensity and distribution of precipitation. The dataset includes over 50 000 timestamps over a two-year period from 2019 to 2021, totalling in roughly 100 GB of data. We evaluate several baselines, including optical flow and neural network models, for precipitation nowcasting on the proposed data. We also evaluate the uncertainty quantification for the ensemble scenario and show that the corresponding estimates do correlate with the ensemble errors on different sections of data. We believe that RuDar dataset will become a reliable benchmark for precipitation nowcasting models and also will be used in other machine learning tasks, e.g., in data shift studying, anomaly detection, or uncertainty estimation. Both dataset and code for data processing and model preparation are publicly available.\n\n\n# 1 INTRODUCTION\n\n\nPrecipitation nowcasting is the task of forecasting a rainfall situation (precipitation location and strength) for a short period of time, usually up to six hours. Due to climate change the frequency and magnitude of extreme weather events, e.g. sudden downpours, increase, and the techniques for forecasting such events are needed. Precipitation nowcasting can provide information about such events with a high spatiotemporal resolution. Such kind of weather forecasting plays an essential role in resource planning in the agricultural industry, aviation, sailing, etc. as well as in daily life.\n\n\nIncorrect precipitation forecasting could have a negative impact on human life activity, and data with diverse meteorological and geographical characteristics are needed for improving precipitation nowcasting models. The different benchmark dataset usage could improve the quality of precipitation nowcasting models to minimize the risk of forecasting error.\n\n\nPreviously published benchmarks Holleman (2007); Shi et al. (2017); Ansari et al. (2018); Ramsauer et al. (2018); Franch et al. (2020); Veillette et al. (2020) provide data collected with one or several\n\n\n![Figure 1: The geographical area covered by the proposed weather radar dataset (light areas). The covered area has a variety of geographic and climatic characteristics. The color indicates the height above sea level.](images/6b7a11ab3bcee6fb552fe032af459bc0a020f72b692afc4bead480af66c7a554.jpg)\n\n\n![](images/d32d32fc7354fd0c0181ce79e635e681ff299e6043a1604598286919cdb9bef2.jpg)\n\n\nweather radars. Some of those datasets only contain information about precipitation intensity, others provide vertically integrated liquid value (VIL) or maximum reflectivity on the vertical section.\n\n\nHowever, a single measurement type is often not enough for extreme weather events forecasting. Thereby, we propose a RuDar dataset that contains several measurement products: reflectivity (dBZ) and radial velocity  $(\\mathrm{m / s})$  on ten altitude levels from  $1\\mathrm{km}$  to  $10\\mathrm{km}$  with  $1\\mathrm{km}$  step and intensity  $(\\mathrm{mm / h})$  on a  $600\\mathrm{m}$  altitude level. Each measurement was carried out with a  $2\\mathrm{km}$  spatial resolution and a 10 minute temporal resolution. The dataset from 30 dual-pol Doppler weather radars were collected and processed at the Radar Center of the Central Aerological Observatory (CAO) of the Russian Federal Service for Hydrometeorology and Environmental Monitoring (ROSGIDROMET) and is used by our team within the conditions of commercial contract. For each radar, we additionally provide information about the surrounding orography Becker et al. (2009). The radars are located mostly in the European part of Russia as shown in Figure 1, therefore, a wide range of geographical and climatic conditions is considered. The proposed dataset includes more than 50 000 timestamps over a two year period from 2019 to 2021, allowing to investigate the effect of seasonality on rainfall forecast.\n\n\nWe illustrate the applicability of our dataset to the nowcasting task by benchmarking the current state-of-the-art optical flow approach Ayzel et al. (2019) and neural network models Shi et al. (2015); Veillette et al. (2020); Ravuri et al. (2021) on it. Experiments show a seasonal dependence having effect on the algorithm performance due to different precipitation intensity rates and differences between adjacent timestamps in different months.\n\n\nThe main paper contributions are (i) published weather radar dataset with different geographical and climatic conditions (provided under the CC BY NC SA 4.0 license) together with the accompanying exploratory data analysis, (ii) evaluations of common simple precipitation nowcasting models and its extension to support additional data, (iii) uncertainty estimation and its connection to the error for the nowcasting ensemble case, and (iv) accompanying source code for data processing and experiments.\n\n\nThe structure of the paper is as follows: Section 2 covers previously published datasets for the precipitation nowcasting task, Section 3 describes the proposed dataset, Section 4 introduces evaluated nowcasting benchmarks, Section 5 explores the uncertainty estimation scenario for the ensemble of models, and Section 6 concludes the paper.\n\n\n# 2 RELATED WORK\n\n\nDoppler weather radar is the most effective tool for detecting precipitation. The radar measures reflectivity of radio waves from precipitation drops, which can then be converted into precipitation intensity using Z-R relation Marshall & Palmer (1948). Standard ways of obtaining a single reflectivity measure from the different heights is either taking measurements from only the lower level (base reflectivity), or aggregating these measurements by the maximum value (composite reflectivity). In addition, a Doppler radar can detect movement towards or away from itself, which allows measuring the speed of precipitation movement along or against the direction of the radar. The latter type of measurement is called radial velocity.\n\n\nIn the public domain, one can find quite a variety of weather radar datasets collected and maintained by international agencies. We summarized the information about some of them in Table 1, where we\n\n\nprovided a comparison by spatial, temporal and pixel resolution, time periods, geographic coverage and number of radars from which measurements were taken.\n\n\nThe first three data sources in Table 1 are more large databases than ready-made ML benchmarks:\n\n- - NEXRAD Ansari et al. (2018) by US National Oceanic and Atmospheric Service (NOAA), which has been collected since 1994 and contains reflectivity data, radial velocity, and derivative products.\n\n- - MRMS Zhang et al. (2016) by NOAA National Severe Storms Laboratory combines the previous source together with various sources to provide severe weather, transportation, and precipitation products with 2-minute updates. This dataset is used in a number of nowcasting studies S\u00f8nderby et al. (2020), Klocek et al. (2021).\n\n- - Radarnet Fairman et al. (2017) by UK Met Office, which has been collected since 1970 and contains composite precipitation data derived from reflectivity data.\n\n\nThe ready-to-go datasets, suitable for exploration by ML practitioners and researchers include:\n\n- - HKO-7 Shi et al. (2017) by Hong Kong Observatory (HKO) contains a single-height reflectivity level from a single radar at the center of Hong Kong for a six years period.\n\n- - KNMI Overeem & Imhoff (2020) by the Royal Netherlands Meteorological Institute (KNMI) contains ten-year single-height composite reflectivity data from two radars.\n\n- - TAASRAD19 Franch et al. (2020) by Meteotrentino contains nine-year aggregated reflectivity data from a single radar located in the Italian Alps. It is interesting due to geographical specifics and a large amount of extreme phenomena, such as snowstorms, hails, downpours, etc.\n\n- - SEVIR Veillette et al. (2020) by NOAA and the Geostationary Environmental Satellite System (GOES) contains US radar and satellite data for a two year-period, sampled either randomly or on an event basis. It is focused on the detection of storm events.\n\n- - RADOLAN Ramsauer et al. (2018) by the German Weather Service contains three-year reflectivity and precipitation data collected by 18 radars in Germany.\n\n\nOur dataset was collected by 30 radars of the Central Aerological Observatory (CAO) and contains two years of observations (spring 2019 - spring 2021). Measurements were carried out mainly in the European part of Russia, but some areas of the Siberian and Far Eastern regions were also captured.\n\n\nThe main advantages of our dataset relative to the abovementioned are as follows:\n\n- 1. A large area is covered with different climatic conditions - from the extreme north (Arkhangelsk,  $64.62^{\\circ}N$ $40.51^{\\circ}E$ ) to the southern regions (Krasnodar,  $45.04^{\\circ}N$ $39.15^{\\circ}E$ ).\n\n- 2. Dataset introduces continuous measurements over a year period, which allows taking into account the influence of seasonal dependence on strength and distribution of precipitation.\n\n- 3. The data presents several radar products at once: reflectivity and radial velocity at 10 altitude levels  $(1 - 10\\mathrm{km})$  and precipitation  $(600\\mathrm{m})$ . We do not specifically aggregate the reflectivity and radial velocity data, as we believe that it can be useful for forecasting to know how moisture and its velocity are distributed depending on the height. There are situations when precipitation has not yet appeared at low levels, but at the same time, at high levels, there is information about future precipitation (see an example in Figure 6 in Supp.Mat.).\n\n- 4. In addition to the radar data itself, we provide information about the orography in the area around each radar. These data allow one to investigate the influence of the surrounding orography in predicting precipitation.\n\n\n# 3 DATASET DESCRIPTION AND PROCESSING\n\n\nThe proposed RuDar dataset contains measurements from 30 weather radars mainly located in the European part of Russia (Figure 1). Radars scan the area around with a radius of 250 kilometers.\n\n\nTable 1: Comparative table of various international open sources of radar data. The data were compared by spatio-temporal (km per pixel / minutes) and pixel resolution, as well as by time periods, geographical coverage and the number of radars involved. The main products contained in the datasets are base and composite reflectivity, as well as derived products: precipitation, maximum reflectivity and vertically integrated liquid (VIL). If there is more than one radar, all except NEXRAD and RuDar combine measurements from different radars into one frame.\n\n\n(*) For the 2020.03\u20132021.02 period, we publish every fifth day to be used for validation and testing.  \n(**) Feature codes: (1) precipitation rate, (2) radial velocity, (3) reflectivity by altitude layers, (4) integrated or one layer reflectivity, (5) satellites\n\n\n<table><tr><td>Dataset</td><td>Time periods</td><td>Spatio-temporal resolution</td><td>Pixel resolution per frame</td><td>Geography</td><td>No. of radars</td><td>Features</td><td>Ready to use</td></tr><tr><td>NEXRAD Ansari et al. (2018)</td><td>1994\u2013</td><td>1 km / 5 min</td><td>460\u00d7460</td><td>United States</td><td>160</td><td>1, 2, 3, 4</td><td>-</td></tr><tr><td>MRMS Zhang et al. (2016)</td><td>2014\u2013</td><td>1 km / 2 min</td><td>3500\u00d77000</td><td>United States</td><td>160</td><td>1, 3, 4</td><td>-</td></tr><tr><td>Radarnet Fairman et al. (2017)</td><td>1970\u2013</td><td>1 km / 5 min</td><td>1536\u00d71280</td><td>United Kingdom</td><td>15</td><td>1, 3</td><td>-</td></tr><tr><td>HKO-7 Shi et al. (2017)</td><td>2009\u20132015</td><td>1.06 km / 6 min</td><td>480\u00d7480</td><td>Hong-Kong</td><td>1</td><td>4</td><td>+</td></tr><tr><td>KNMI Overeem &amp; Imhoff (2020)</td><td>2008\u20132018</td><td>1 km / 5 min</td><td>400\u00d7400</td><td>Netherlands</td><td>2</td><td>4</td><td>+</td></tr><tr><td>TAASRAD19 Franch et al. (2020)</td><td>2010\u20132019</td><td>0.5 km / 5 min</td><td>480\u00d7480</td><td>Italian Alps</td><td>1</td><td>4</td><td>+</td></tr><tr><td>SEVIR Veillette et al. (2020)</td><td>2017\u20132019</td><td>1 km / 5 min</td><td>384\u00d7384</td><td>United States</td><td>160</td><td>4, 5</td><td>+</td></tr><tr><td>RADOLAN Ramsauer et al. (2018)</td><td>2014.12\u20132017.11</td><td>1 km / 5 min</td><td>900\u00d7900</td><td>Germany</td><td>18</td><td>1, 4</td><td>+</td></tr><tr><td>RuDar (Ours)</td><td>2019.03\u20132021.02*</td><td>2 km / 10 min</td><td>252\u00d7252</td><td>Russia</td><td>30</td><td>1, 2, 3</td><td>+</td></tr></table>\n\n\nMeasurements come from radars every 10 minutes and have a spatial resolution of  $2 \\times 2$  kilometers. The dataset covers two years from 2019 to 2021 and has over 50 000 unique timestamps.\n\n\nEach data sample is a three-dimensional tensor that contains the result of a 10 minute scan of the atmosphere with a single radar. The center of the frame corresponds to the location of the radar in the scanned area. Tensors have 21 channels with a spatial resolution of  $252 \\times 252$  pixels. The first channel contains information about precipitation intensity (mm/h) on a  $600\\mathrm{m}$  altitude level. The channels 2-11 contain reflectivity measurements (dBZ) performed at 10 altitude levels from  $1\\mathrm{km}$  to  $10\\mathrm{km}$  with  $1\\mathrm{km}$  step, and the channels 12-21 contain 10 radial velocity (m/s) measurements from the same  $1 - 10\\mathrm{km}$  altitudes. We also provide data on orography Becker et al. (2009) and latitude-longitude coordinates of the territory surrounding the area of the radar measurements. Coordinates were initially set on a kilometer grid with a step of 2 kilometers with zero at the radar location.\n\n\nThe data example is partially shown in Figure 2a. Please refer to the Section D in Supplementary for the comprehensive documentation<sup>2</sup>.\n\n\n# 3.1 PRECIPIATION INTENSITY\n\n\nPrecipitation intensity is measured at an altitude of 600 meters in mm/h and is represented by a single channel in the data. It is not a direct measurement: the precipitation rate is calculated from reflectivity values with Marshal-Palmer type of Z-R relationship Marshall & Palmer (1948). In addition to mm/h values, two special values are presented in data:  $-2\\in 6$  value marks areas where measurements are not available,  $-1\\in 6$  value marks areas where no precipitation events were detected. Both values can be reduced to  $0\\mathrm{mm / h}$ , however, when feeding a radar frame to the model, it is better to mask  $-2\\in 6$  values so that the model can distinguish the absence of precipitation from the blind zone of the radar.\n\n\nFigure 2d shows the distribution of precipitation by seasons. The data for each season is taken from the entire period 2019-2021. As can be seen in Figure 2d, the intensity has a seasonal dependence: the highest values of precipitation amount are reached in summer and the lowest in winter.\n\n\nWe investigated the distribution of precipitation intensity difference between two adjacent radar images. As Figure 2e shows, in the winter season the precipitation rate difference between two adjacent radar images is much lower than in the summer season. This means that in winter precipitation changes much less in the short-term interval than in summer, and therefore it is more difficult to solve the problem of nowcasting in warmer seasons.\n\n\nPeaks which are visible in Figure 2e are the result of specificity of the combination of radar-side filtration algorithms and Marshall-Palmer type of Z-R relationship.\n\n\n![(a)](images/9b019653c88aedfdbe7bacb10fa13d7823ae78b484546b301878fbd9ecf78d23.jpg)\n\n\n![](images/3486f0e198b84c80380ff99289f74bd14821c375782aafb4f6b5b213b73b4dfa.jpg)\n\n\n![(c)](images/5ebc53b64e0c891478909b3555467b551a789aef25589281138221548d2c26c9.jpg)\n\n\n![Figure 2: Featuring RuDar dataset. (a) The sky above the Moscow Region, Vnukovo Radar; axes ticks are given in kilometers. Ten levels of the precipitation intensity are shown (for each height level from  $1\\mathrm{km}$  to  $10\\mathrm{km}$ ), together with the orography (magnified for visualization purposes) on the bottom. The lined cuts on the left part of the plot demonstrate a specific case of urban obstruction in the radar vision: skyscrapers are not shown in the orography map yet they affect the scan result. (b) Tensor representation for a single radar at the given timestamp: each tensor has a shape of (252, 252, 21). Note that orography information is stored separately to avoid duplication. (c) Natural obstructions near Mineralnye Vody city: the mountain chain on the south blocks the radar, reducing the receptive field down to the area denoted by the white dotted line. (d) Seasonal dependence of the precipitation intensity distribution (mm/h per pixel). During the winter season events with high precipitation rates are much less represented than in the summer season. We clipped high precipitation rates ( $>140\\mathrm{mm/h}$ ). (e) The distribution of precipitation intensity difference between two adjacent radar images (mm/h per pixel) in RuDar dataset. In the winter season, precipitation events change slower than in the summer season. The periodical peaks are the result of a peculiarity of the combination of radar-side filtration algorithms and Marshall-Palmer type of Z-R relationship. Best viewed in color.](images/d2934a1d463d295dfe3e2859113d7f160bd1f4ab76986fddf40e17961fd467e8.jpg)\n\n\n![](images/9ea25e5b2e4e517987a3062f06cf5818c6a11e0965866b90b6b3925511bb3703.jpg)\n\n\n# 3.2 RADAR REFLECTIVITY\n\n\nReflectivity values represent direct radar measurements in dBZ units. The radar measures the amount of energy reflected from droplets distributed in the atmosphere, and this amount of energy turns out to be proportional to the amount of moisture in the air. All reflectivity measurements were carried out at heights of  $1 - 10\\mathrm{km}$  with  $1\\mathrm{km}$  step. We provide reflectivity data at several altitude levels, since we believe that not only information about precipitation changes in time, but also in space, is important for nowcasting, see Fig. 6 in Supp. Mat. for an example.\n\n\nTo demonstrate the geospatial diversity, we compare the mean reflectivity for two radars: Arkhangelsk (in the north) and Krasnodar (in the south) in Figure 3. It can be seen that both seasonal and altitudinal dependencies are different for these radars, which shows the wide range of weather conditions and data variability.\n\n\n# 3.3 RADIAL VELOCITY\n\n\nThe radial velocity is the wind velocity projected onto the ray starting in the radar. Negative values correspond to the movement towards the radar, and positive values correspond to movement away from the radar. Due to the peculiarities of this type of measurement, the values are available only at points where the radar has registered moisture droplets. The data is measured in m/s and provided\n\n\n![Figure 3: Comparison of two different regions: Krasnodar (on the south) and Arkhangelsk (on the north) in terms of mean reflectivity (left) and mean radial velocity (right) on different height levels. Only non-zero values are taken into account. Regions demonstrate different patterns in the altitude intensity and radial velocity distribution as well as the seasonal changes.](images/c2b85ab3f45c626550491af9d844518a1151ba94bb544d6d2517cd9a5379902d.jpg)\n\n\n![](images/6d8daf18a6f880a9e8c0b2e308697602233e380d75eca29dc8e115ce07296cb9.jpg)\n\n\nfor ten altitude levels from  $1\\mathrm{km}$  to  $10\\mathrm{km}$  with  $1\\mathrm{km}$  step similar to reflectivity data. According to Figure 3, radial velocity values are seasonally dependent.\n\n\n# 3.4 GEOGRAPHICAL INFORMATION\n\n\nIn addition to atmosphere scans, the RuDar dataset contains orography and geographical coordinates associated with each radar. Geographical coordinates (latitudes and longitudes) are provided for each point where measurements are presented. Orography is a two-dimensional tensor in which positive values correspond to elevations above sea level and negative values correspond to elevations below sea level. We resampled an available information Becker et al. (2009) to the used coordinate grid. An example of orography is shown in Figure 2c where the case of natural radar vision obstruction is demonstrated. The radar's visibility area in the figure is greatly reduced by high mountains located in the immediate vicinity of Mineralnye Vody. Note that the orography measurements do not include the urban areas, and Figure 2a shows that skyscrapers in Moscow may also pose a problem.\n\n\n# 4 NOWCASTING BASELINES\n\n\n# 4.1 PROBLEM STATEMENT\n\n\nThe nowcasting problem can be formulated in terms of sequence prediction task (precisely video prediction task) - the goal is to predict a sequence of future observations from an input sequence of historical measurements. More formally, let us have an input sequence of measurements  $X = (X_{t-M+1}, \\ldots, X_t)$  of length  $M$  for time step  $t$ , and a sequence of future measurements  $Y = (Y_{t+1}, \\ldots, Y_{t+K})$  of length  $K$ . Then our task is to construct a model  $f$  that predicts  $Y$  by  $X$ :\n\n\n$$\n\n$$\nY _ {t + 1}, Y _ {t + 2}, \\dots , Y _ {t + K} = f \\left(X _ {t - M + 1}, \\dots , X _ {t - 1}, X _ {t}\\right). \\tag {1}\n$$\n\n$$\n\n\nEach element  $X_{i}$  is a tensor of shape  $H \\times W \\times C_{input}$ , where  $H$  and  $W$  are the height and width of one frame, and  $C_{input}$  is the number of measurements used for prediction. An element  $Y_{i}$  of the output sequence is a tensor of shape  $H \\times W \\times C_{output}$ , where  $C_{output}$  is the number of predicted measurements. The numbers  $C_{input}$  and  $C_{output}$  may differ, since in the input sequence one can possibly use all the available measurements (intensity, reflectivity, radial velocity, orography), and the output sequence will contain only the target values. In our case, the target is future precipitation intensity, which implies  $C_{output} = 1$ .\n\n\nThe quality of the model can be measured by how well it predicts the strength of precipitation directly, as well as how well it copes with detecting precipitation. In the first case, we use pixelwise mean squared error (MSE) to measure quality, in the second we use  $F_{1}$ -score, see details of calculation in\n\n\nSection I of Supplementary Material. In meteorological articles Espeholt et al. (2021); Ravuri et al. (2021); Bouget et al. (2021), metrics such as Probability of Detection (POD), Success Ratio (SUCR), Critical Success Index (CSI) are sometimes used, but they are actually the equivalents of recall, precision and intersection over union (IOU), - standard metrics in machine learning. We chose  $F_{1}$  because it correlates with IOU and depends on both precision and recall.\n\n\n# 4.2 EXPERIMENTAL SETUP\n\n\n# 4.2.1 DATA SETUP\n\n\nWe are focusing on a year scale setup, which uses 2019 data for training, and every fifth day of 2020-2021 data for validation and testing.\n\n\nIn our experiments, we use 4 input tensors with historical observations to predict 12 future intensity frames ( $M = 4$ ,  $K = 12$ ,  $C_{output} = 1$ ). The spatial size of the input and output tensors is  $H = W = 252$ . We clip precipitation intensity values to  $50~\\mathrm{mm / h}$ , since high intensity rates are rare in the given geographical area. As we use an MSE loss to train our models, we use a binary mask for the special value  $-2 \\in 6$  to prevent penalizing our models for errors outside the radar visibility range.\n\n\nWhen only intensity is used as an input observation,  $C_{input} = 1$ . Using reflectivity levels increases  $C_{input}$  to 11. All values are clipped in the interval between  $-32$  dBZ and  $65$  dBZ, and then reduced to the interval from 0 to 1 by subtracting  $-32$  dBZ and dividing by  $65 - (-32) = 87$  dBZ. The intensity values for this setup are divided by  $50\\mathrm{mm / h}$  (for both input and target frames) to be also in the unit range (0, 1). Radial velocity also adds 10 input channels ( $C_{input} = 11$ ). All radial velocity values are clipped between  $-63\\mathrm{m / s}$  and  $63\\mathrm{m / s}$  and then divided by  $63\\mathrm{m / s}$  to be in the interval  $(-1, 1)$ . Orography adds one additional channel ( $C_{input} = 2$ ). Negative values of heights are reduced to zero, and positive values are divided by the maximum height equal to  $5336\\mathrm{m}$ .\n\n\nFor both radial velocity and orography, the intensity preprocessing is the same as in the absence of additional features.\n\n\n# 4.2.2 MODELS AND SETUPS\n\n\nWe represented  $f(\\cdot)$  with a persistent model as a weak baseline and with a state-of-the-art optical flow approach Ayzel et al. (2019) as a strong baseline. Also we trained UNet-like Veillette et al. (2020) and ConvLSTM-like Shi et al. (2015) neural network models (including Extended ConvLSTM architecture shown in Fig.4b) and tested a pretrained GAN-like model Ravuri et al. (2021). Each of the above models (excepting the pretrained GAN model) was trained on a single Tesla A100 80GB for approximately 70 hours. Details about model architectures are provided in Section E of Supplementary Materials.\n\n\nAlso we evaluated Earthformer Gao et al. (2022) on the proposed dataset using official implementation. Other strong neural network baselines S\u00f8nderby et al. (2020); Klocek et al. (2021); Espeholt et al. (2021) will be considered in future research as they require additional input data that is not presented in the proposed dataset. Satellite information is used in S\u00f8nderby et al. (2020), numerical weather forecast is used in Klocek et al. (2021), and Espeholt et al. (2021) requires both.\n\n\n# 4.3 RESULTS\n\n\nWe provide the results on  $F_{1}$  and  $MSE$  for each of the above models, as well as the results of Extended ConvLSTM using each of the additional features. We evaluate the baselines on seasonal test splits of the RuDar dataset. The results are shown in Table 2.\n\n\nAll baselines work better than Persistent, and among baselines without additional features, Extended ConvLSTM shows the best result in all seasons except summer: there it is inferior to Optical Flow in  $F_{1}$ .\n\n\nAdding reflectivity shows the best result for all metrics and seasons, except for  $F_{1}$  in summer (where it is inferior to Optical Flow) and in autumn (where it is inferior to Extended ConvLSTM with orography). Adding radial speed improves both  $MSE$  and  $F_{1}$  and adding orography improves  $F_{1}$  relative to Extended ConvLSTM without additional features.\n\n\nTable 2: Metrics of baseline models calculated separately for each season. The first value in the cell is  $MSE$ , the second value is  $F_{1}$ . Metrics are average across prediction horizons. Red highlights the maximum in the entire column, blue highlights the maximum among models without additional features.\n\n\n<table><tr><td></td><td>Spring 2020</td><td>Summer 2020</td><td>Autumn 2020</td><td>Winter 2020-2021</td></tr><tr><td>Persistent</td><td>0.2691 / 0.5177</td><td>0.6281 / 0.4975</td><td>0.0579 / 0.5233</td><td>0.0260 / 0.6425</td></tr><tr><td>Optical Flow</td><td>0.1936 / 0.6297</td><td>0.4768 / 0.6435</td><td>0.0384 / 0.6351</td><td>0.0190 / 0.6884</td></tr><tr><td>GAN</td><td>0.1885 / 0.6368</td><td>0.5820 / 0.6140</td><td>0.0352 / 0.6507</td><td>0.0218 / 0.6580</td></tr><tr><td>U-Net</td><td>0.1420 / 0.5649</td><td>0.3215 / 0.5406</td><td>0.0312 / 0.5730</td><td>0.0160 / 0.6232</td></tr><tr><td>ConvLSTM</td><td>0.1217 / 0.6222</td><td>0.3244 / 0.5580</td><td>0.0242 / 0.6650</td><td>0.0156 / 0.6799</td></tr><tr><td>Extended ConvLSTM</td><td>0.1192 / 0.6436</td><td>0.3193 / 0.5877</td><td>0.0233 / 0.6814</td><td>0.0150 / 0.6947</td></tr><tr><td>Extended ConvLSTM + reflectivity</td><td>0.1133 / 0.6597</td><td>0.3080 / 0.6170</td><td>0.0219 / 0.6936</td><td>0.0140 / 0.7014</td></tr><tr><td>Extended ConvLSTM + radial velocity</td><td>0.1170 / 0.6516</td><td>0.3153 / 0.5970</td><td>0.0228 / 0.6885</td><td>0.0144 / 0.7038</td></tr><tr><td>Extended ConvLSTM + orography</td><td>0.1193 / 0.6526</td><td>0.3197 / 0.5968</td><td>0.0234 / 0.6876</td><td>0.0148 / 0.7016</td></tr><tr><td>Earthformer</td><td>0.0124 / NA</td><td>0.0153 / NA</td><td>0.0047 / NA</td><td>0.0048 / NA</td></tr></table>\n\n\n![](images/9b975d14807bf789ca0c22282d655937c76750cb1835c552c8b7fba8f622c081.jpg)\n\n\n![](images/798e72c213a50109713fae9f8d01e9cbea2fe4c074f2769ad13e50e7f2280480.jpg)\n\n\n![Figure 4: (Left) Algorithm performance (left column:  $F_{1}$ , right column: MSE) as a function of a prediction horizon on the spring season of the test set. Top row: baseline algorithms, bottom row: Extended ConvLSTM with the various additional data. While optical flow may outperform other models on the initial time steps, NN-based and trained baselines easily outperform baselines. Usage of the radar reflectivity improves the performance of the ConvLSTM model. (Right) An extension of the architecture from Shi et al. (2015). The first ConvLSTM layer receives the output of a convolutional encoder as input, and the output of the last ConvLSTM layer is upsampled to the final prediction with a convolutional decoder. To output the next frame, the predictor uses the outputs of the last ConvLSTM layer from the previous step.](images/5a26ecde1fcf3943043f623642b88b80559d4db4f25fb822c4dfca64ef21b3a1.jpg)\n\n\n![](images/91639229c0a577590ed803505e8ed6a79303da312ba46cae8f54c98f30fc8f0c.jpg)\n\n\n![](images/5a55689205526b516be921d61af12155f78d142e91285d65c312219527c895e4.jpg)\n\n\nWe also show the relationship between algorithm performance and forecast horizon for the spring season in Figure 4a.\n\n\n# 5 UNCERTAINTY ESTIMATION FOR THE ENSEMBLE CASE\n\n\nWeather forecasting task, given its properties such as noisy data, probabilistic predictions, and general weather variability, may organically benefit from the uncertainty quantification. Uncertainty estimates (UEs) are helpful in a number of cases, including \"failing to predict\" (when the model understands that its prediction is not reliable enough, and gets the results from the basic robust model), active learning, data shift reaction, as well as the error analysis and model selection.\n\n\nWe focus on the last scenario and analyze the relationship between the error and UE for the ensemble case. Given an ensemble of five ConvLSTM models, trained on the same data but different seeds, we treat the mean and standard deviation of predictions as the output and UE, correspondingly. An ensemble error and UE on the test set are correlated with a large Spearman coefficient  $(>0.8)$ ; this relationship holds even when both metrics are averaged over the various dimensions: time, location, and forecast horizon, see Fig. 7, 8 in Section H of Supp.Mat. We also report that we did not find a relationship between the UE and orography (Pearson  $|r| < 0.1$ ).\n\n\nThis analysis suggests that UE metrics can be used to indicate large errors, which, in turn, can be used to deliver better predictions or uncertainty of the forecast, as well as assist the model selection and improvement.\n\n\n# 6 DISCUSSION\n\n\n# 6.1 SUMMARY\n\n\nIn this paper, we propose a weather radar dataset with a wide variety of geographical and climatic conditions and show in contrast to previously published works that a precipitation nowcasting task can be a seasonal dependent problem. This encourages the usage of the seasonal models and separate models for precipitation rate forecasting and precipitation events existence.\n\n\nWe also evaluated an ensemble of nowcasting models, analyzing the uncertainty estimates and showing its deep relation to the ensemble error on various scales.\n\n\nWe believe that our proposed dataset will become a reliable benchmark for precipitation nowcasting models. The code for data preprocessing and model preparation is publicly available  $^{3}$ . This dataset is provided under the CC BY NC SA 4.0 license. The data is maintained on the cloud service, and is maintained by authors team $^{4}$ .\n\n\n# 6.2 LIMITATIONS\n\n\nWe would like to point out a few limitations of our work:\n\n- - Dataset size. We included the two-year period only in order for the dataset to be comprehensible and processible by the community, as its unpacked size already exceeds 100 GB. This dataset is not intended for a study of long-term climatic changes.\n\n- - Data shift. Like any other piece of real-world data, our dataset may contain several peculiarities. In the text, we discuss the seasonal dependence and geographical shifts, however, it should be noted that the measurements within the sample are also not simultaneous as they were obtained by combining the measurements from the narrow beam that scans the atmosphere.\n\n- - Baselines. We provide the readers with simple baselines supporting the dataset. However, we plan to benchmark a number of state-of-the-art neural network-based baselines S\u00f8nderby et al. (2020); Espeholt et al. (2021); Klocek et al. (2021), in future studies.\n\n\n# 6.3 POSSIBLE PROSPECTS\n\n\nA two-year dataset may be applicable not for the nowcasting task only but in a number of contemporary ML problems, including:\n\n\nRare event detection. Various storms and rare weather conditions are of special interest both to the researchers and end-users. Some works even emphasize this area of research Veillette et al. (2020). We expect various anomaly detection algorithms to be of great use here.\n\n\nData shift and uncertainty estimation. The variety of both geographical and temporal conditions makes this dataset a good candidate for the modeling of distribution shift scenarios (see, e.g. Malinin et al. (2021)), where test data may naturally vary from the training data dramatically. In this case, various uncertainty estimation approaches, like in Gronquist et al. (2019) may be helpful.\n\n\nActive learning. One of the most important scenarios for the day-to-day forecasting systems is active learning since continuous data flow allows for the production model to learn from its own mistakes, correcting previous predictions. However, processing and retraining on huge amounts of data poses a challenge, and one may use smarter ways of data sampling (of both past archives and daily chunks of data) in order to reduce the data processing and model training times.\n\n\n# REFERENCES\n\n- Steve Ansari, Stephen Del Greco, Edward Kearns, Otis Brown, Scott Wilkins, Mohan Ramamurthy, Jeff Weber, Ryan May, Jed Sundwall, Jeff Layton, Ariel Gold, Adam Pasch, and Valliappa Lakshmanan. Unlocking the Potential of NEXRAD Data through NOAA's Big Data Partnership. Bulletin of the American Meteorological Society, 99(1):189 - 204, 2018. doi: 10.1175/BAMS-D-16-0021.1. URL https://journals.ametsoc.org/view/journals/bams/99/1/bams-d-16-0021.1.xml.\n\n- G. Ayzel, M. Heistermann, and T. Wintersmith. Optical flow models as an open benchmark for radar-based precipitation nowcasting (rainymotion v0.1). Geoscientific Model Development, 12 (4):1387-1402, 2019. doi: 10.5194/gmd-12-1387-2019. URL https://gmd.copernicus.org/articles/12/1387/2019/.\n\n- J. J. Becker, D. T. Sandwell, W. H. F. Smith, J. Braud, B. Binder, J. Depner, D. Fabre, J. Factor, S. Ingalls, S-H. Kim, R. Ladner, K. Marks, S. Nelson, A. Pharaoh, R. Trimmer, J. Von Rosenberg, G. Wallace, and P. Weatherall. Global bathymetry and elevation data at 30 arc seconds resolution: SRTM30_plus. Marine Geodesy, 32(4):355-371, 2009. doi: 10.1080/01490410903297766. URL https://doi.org/10.1080/01490410903297766.\n\n- Vincent Bouget, Dominique B\u00e9r\u00e9ziat, Julien Brajard, Anastase Charantonis, and Arthur Filoche. Fusion of rain radar images and wind forecasts in a deep learning model applied to rain nowcasting. Remote Sensing, 13(2), 2021. ISSN 2072-4292. doi: 10.3390/rs13020246. URL https://www.mdpi.com/2072-4292/13/2/246.\n\n- Lasse Espeholt, Shreya Agrawal, Casper S\u00f8nderby, Manoj Kumar, Jonathan Heek, Carla Bromberg, Cenk Gazen, Jason Hickey, Aaron Bell, and Nal Kalchbrenner. Skillful Twelve Hour Precipitation Forecasts using Large Context Neural Networks. arXiv preprint arXiv:2111.07470, 2021.\n\n- Jonathan G Fairman, David M Schultz, Daniel J Kirshbaum, Suzanne L Gray, and Andrew I Barrett. Climatology of Size, Shape, and Intensity of Precipitation Features over Great Britain and Ireland. Journal of Hydrometeorology, 18(6):1595-1615, 2017. doi: 10.1175/JHM-D-16-0222.1. URL https://journals.ametsoc.org/view/journals/hydr/18/6/jhm-d-16-0222{\\_}1.xml.\n\n- Gabriele Franch, Valerio Maggio, Luca Coviello, Marta Pendesini, Giuseppe Jurman, and Cesare Furlanello. TAASRAD19, a high-resolution weather radar reflectivity dataset for precipitation nowcasting. Scientific Data, 7(1):1-13, 2020.\n\n- Zhihan Gao, Xingjian Shi, Hao Wang, Yi Zhu, Bernie Wang, Mu Li, and Dit-Yan Yeung. Earthformer: Exploring space-time transformers for earth system forecasting. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=1zZstLVGVGW.\n\n- Peter Gronquist, Tal Ben-Nun, Nikoli Dryden, Peter Dueben, Luca Lavarini, Shigang Li, and Torsten Hoefler. Predicting weather uncertainty with deep convnets. arXiv preprint arXiv:1911.00630, 2019.\n\n- Iwan Holleman. Bias adjustment and long-term verification of radar-based precipitation estimates. Meteorological Applications, 14(2):195-203, 2007. doi: https://doi.org/10.1002/met.22. URL https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/met.22.\n\n- Sylwester Klocek, Haiyu Dong, Matthew Dixon, Panashe Kanengoni, Najeeb Kazmi, Pete Luferenko, Zhongjian Lv, Shikhar Sharma, Jonathan Weyn, and Siqi Xiang. MS-nowcasting: Operational Precipitation Nowcasting with Convolutional LSTMs at Microsoft Weather. arXiv preprint arXiv:2111.09954, 2021.\n\n- Andrey Malinin, Neil Band, German Chesnokov, Yarin Gal, Mark JF Gales, Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal Raina, et al. Shifts: A dataset of real distributional shift across multiple large-scale tasks. arXiv preprint arXiv:2107.07455, 2021.\n\n- J. S. Marshall and W. Mc K. Palmer. The distribution of raindrops with size. Journal of Atmospheric Sciences, 5(4):165 - 166, 1948. doi: 10.1175/1520-0469(1948)005<0165:TDORWS>2.0.CO;2. URL https://journals.ametsoc.org/view/journals/atsc/5/4/1520-0469_1948_005_0165_tdorws_2_0_co_2.xml.\n\n- Aart Overeem and Ruben Imhoff. *Archived 5-min rainfall accumulations from a radar dataset for the Netherlands*, Mar 2020. URL https://data.4tu.nl/articles/dataset/Archived_5-min_rainfall_accumulations_from_a_radar_dataset_for_the_Netherlands/12675278/1.\n\n- Thomas Ramsauer, Thomas Wei\u00df, and Philip Marzahn. Comparison of the GPM IMERG final precipitation product to RADOLAN weather radar data over the topographically and climatically diverse germany. Remote Sensing, 10(12), 2018. ISSN 2072-4292. doi: 10.3390/rs10122029. URL https://www.mdpi.com/2072-4292/10/12/2029.\n\n- Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Piotr Mirowski, Megan Fitzsimons, Maria Athanassiadou, Sheleem Kashem, Sam Madge, et al. Skilful precipitation nowcasting using deep generative models of radar. Nature, 597(7878):672-677, 2021.\n\n- Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. Springer, 2015.\n\n- Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun Woo. Convolutional LSTM network: a machine learning approach for precipitation nowcasting. In Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1, pp. 802-810, 2015.\n\n- Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun WOO. Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/a6db4ed04f1621a119799fd3d7545d3d-Paper.pdf.\n\n- Mennatullah Siam, Sepehr Valipour, Martin Jagersand, and Nilanjan Ray. Convolutional gated recurrent networks for video segmentation. In 2017 IEEE International Conference on Image Processing (ICIP), pp. 3090-3094, 2017. doi: 10.1109/ICIP.2017.8296851.\n\n- Casper Kaae S\u00f8nderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, and Nal Kalchbrenner. Metnet: A neural weather model for precipitation forecasting. arXiv preprint arXiv:2003.12140, 2020.\n\n- Mark Veillette, Siddharth Samsi, and Chris Mattioli. SEVIR: A storm event imagery dataset for deep learning applications in radar and satellite meteorology. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 22009-22019. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/fa78a16157fed00d7a80515818432169-Paper.pdf.\n\n- Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and Philip S Yu. PredRNN++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5123-5132. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/wang18b.html.\n\n- Jian Zhang, Kenneth Howard, Carrie Langston, Brian Kaney, Youcun Qi, Lin Tang, Heather Grams, Yadong Wang, Stephen Cocks, Steven Martinaitis, et al. Multi-Radar Multi-Sensor (MRMS) quantitative precipitation estimation: Initial operating capabilities. Bulletin of the American Meteorological Society, 97(4):621-638, 2016.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638430.9258657", "msecs": "925.0", "relativeCreated": "27756.36315345764", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Test Time Learning for Time Series Forecasting\n\n# Abstract\nWe propose the use of Test-Time Training (TTT) modules in a cascade architecture to enhance performance in long-term time series forecasting. Through extensive experiments on standard benchmark datasets, we demonstrate that TTT modules consistently outperform state-of-the-art models, including Mamba-based TimeMachine, particularly in scenarios involving extended sequence and prediction lengths. Our results show significant improvements, especially on larger datasets such as Electricity, Traffic, and Weather, underscoring the effectiveness of TTT in capturing long-range dependencies. Additionally, we explore various convolutional architectures within the TTT framework, showing that convolutional blocks as hidden layer architectures can achieve competitive results.\n\n\n# 1 INTRODUCTION\n\n\nLong Time Series Forecasting (LTSF) is a crucial task in various fields, including energy Doe & Smith (2023), industry Doe & Smith (2024), defense Bakdash et al. (2017), and atmospheric sciences Lim & Zohren (2020). LTSF uses a historical sequence of observations, known as the look-back window, to predict future values through a learned or mathematically induced model. However, the stochastic nature of real-world events makes LTSF challenging. Deep learning models, including time series forecasting, have been widely adopted in engineering and scientific fields. Early approaches employed Recurrent Neural Networks (RNNs) to capture long-range dependencies in sequential data like time series. However, recurrent architectures like RNNs have limited memory retention, are difficult to parallelize, and have constrained expressive capacity. Transformers (Vaswani et al., 2017), with ability to efficiently process sequential data in parallel and capture contextual information, have significantly improved performance on time series prediction task (Wen et al., 2023; Liu et al., 2022b; Ni et al., 2024; Chen et al., 2024). Yet, due to the quadratic complexity of attention mechanisms with respect to the context window (or look-back window in LTSF), Transformers are limited in their ability to capture very long dependencies.\n\n\nIn recent years, State Space Models (SSMs) such as Mamba (Gu & Dao, 2024), a gated linear RNN variant, have revitalized the use of RNNs for LTSF. These models efficiently capture much longer dependencies while reducing computational costs and enhancing expressive power and memory retention. A new class of Linear RNNs, known as Test Time Training (TTT) modules (Sun et al., 2024), has emerged. These modules use expressive hidden states and provide theoretical guarantees for capturing long-range dependencies, positioning them as one of the most promising architectures for LTSF and due to their weight adaptation during test time are very effective on non-stationary data. We provide more motivation on TTT for non-stationary data in Appendix A.\n\n\n# KEY INSIGHTS AND RESULTS\n\n\nThrough our experiments, several key insights emerged regarding the performance of TTT modules when compared to existing SOTA models:\n\n- - Superior Performance with Longer Sequence and Prediction Lengths: TTT modules consistently outperformed the SOTA TimeMachine model, particularly as sequence and prediction lengths increased. Architectures such as Conv Stack 5 demonstrated their ability to capture long-range dependencies more effectively than Mamba-based models, resulting in noticeable improvements in Mean Squared Error (MSE) and Mean Absolute Error (MAE) across various benchmark datasets.\n\n- - **Strong Improvement on Larger Datasets:** On larger datasets, such as Electricity, Traffic, and Weather, the TTT-based models excelled, showing superior performance compared to both Transformer- and Mamba-based models. These results underscore the ability of TTT to handle larger temporal windows and more complex data, making it especially effective in high-dimensional, multivariate datasets.\n\n- - Hidden Layer Architectures: The ablation studies revealed that while convolutional architectures added to the TTT modules provided some improvements, Conv Stack 5 consistently delivered the best results among the convolutional variants. However, simpler architectures like Conv 3 often performed comparably, showing that increased architectural complexity did not always lead to significantly better performance. Very complex architectures like the modern convolutional block from Donghao & Xue (2024) showed competitive performance when used as TTT hidden layer architectures compared to the simpler single architectures proposed, hinting on the potential of more complex architectures in capturing more long term dependencies.\n\n- - Adaptability to Long-Term Predictions: The TTT-based models excelled in long-term prediction tasks, especially for really high prediction lengths like 2880. TTT-based models also excelled on increased sequence lengths as high as 5760 which is the maximum sequence length allowed by the benchmark datasets. This verified the theoretically expected superiority of TTT based models relative to the mamba/transformer based SOTA models.\n\n\n# MOTIVATION AND CONTRIBUTIONS\n\n\nIn this work, we explore the potential of TTT modules in Long-Term Series Forecasting (LTSF) by integrating them into novel model configurations to surpass the current state-of-the-art (SOTA) models. Our key contributions are as follows:\n\n- - We propose a new model architecture utilizing quadruple TTT modules, inspired by the TimeMachine model (Ahamed & Cheng, 2024), which currently holds SOTA performance. By replacing the Mamba modules with TTT modules, our model effectively captures longer dependencies and predicts larger sequences.\n\n- - We evaluate the model on benchmark datasets, exploring the original look-back window and prediction lengths to identify the limitations of the SOTA architecture. We demonstrate that the SOTA model achieves its performance primarily by constraining look-back windows and prediction lengths, thereby not fully leveraging the potential of LTSF.\n\n- - We extend our evaluations to significantly larger sequence and prediction lengths, showing that our TTT-based model consistently outperforms the SOTA model using Mamba modules, particularly in scenarios involving extended look-back windows and long-range predictions.\n\n- - We conduct an ablation study to assess the performance of various hidden layer architectures within our model. By testing six different convolutional configurations, one of which being ModernTCN by Donghao & Xue (2024), we quantify their impact on model performance and provide insights into how they compare with the SOTA model.\n\n\n# 2 RELATED WORK\n\n\nTransformers for LTSF Several Transformer-based models have advanced long-term time series forecasting (LTSF), with notable examples like iTransformer (Liu et al., 2024) and PatchTST\n\n\n(Nie et al., 2023). iTransformer employs multimodal interactive attention to capture both temporal and inter-modal dependencies, suitable for multivariate time series, though it incurs high computational costs when multimodal data interactions are minimal. PatchTST, inspired by Vision Transformers (Dosovitskiy et al., 2021), splits input sequences into patches to capture dependencies effectively, but its performance hinges on selecting the appropriate patch size and may reduce model interpretability. Other influential models include Informer (Zhou et al., 2021), which uses sparse self-attention to reduce complexity but may overlook finer details in multivariate data; Autoformer (Wu et al., 2022), which excels in periodic data but struggles with non-periodic patterns; Pyraformer (Liu et al., 2022b), which captures multi-scale dependencies through a hierarchical structure but at the cost of increased computational requirements; and Fedformer (Zhou et al., 2022), which combines time- and frequency-domain representations for efficiency but may underperform on noisy time series. While each model advances LTSF in unique ways, they also introduce specific trade-offs and limitations.\n\n\nState Space Models for LTSF S4 models (Gu et al., 2022a;b; Gupta et al., 2023) are efficient sequence models for long-term time series forecasting (LTSF), leveraging linear complexity through four key components:  $\\Delta$  (discretization step size),  $A$  (state update matrix),  $B$  (input matrix), and  $C$  (output matrix). They operate in linear recurrence for autoregressive inference and global convolution for parallel training, efficiently transforming recurrences into convolutions. However, S4 struggles with time-invariance issues, limiting selective memory. Mamba (Gu & Dao, 2024) addresses this by making  $B$ ,  $C$ , and  $\\Delta$  dynamic, creating adaptable parameters that improve noise filtering and maintain Transformer-level performance with linear complexity. SIMBA (Patro & Agneeswaran, 2024) enhances S4 by integrating block-sparse attention, blending state space and attention to efficiently capture long-range dependencies while reducing computational overhead, ideal for large-scale, noisy data. TimeMachine (Ahamed & Cheng, 2024) builds on these advances by employing a quadruple Mamba setup, managing both channel mixing and independence while avoiding Transformers' quadratic complexity through multi-scale context generation, thereby maintaining high performance in long-term forecasting tasks.\n\n\nLinear RNNs for LTSF RWKV-TS (Hou & Yu, 2024) is a novel linear RNN architecture designed for time series tasks, achieving O(L) time and memory complexity with improved long-range information capture, making it more efficient and scalable compared to traditional RNNs like LSTM and GRU. Orvieto et al. (2023) introduced the Linear Recurrent Unit (LRU), an RNN block matching the performance of S4 models on long-range reasoning tasks while maintaining computational efficiency. TTT (Sun et al., 2024) layers take a novel approach by treating the hidden state as a trainable model, learning during both training and test time with dynamically updated weights. This allows TTT to capture long-term relationships more effectively through real-time updates, providing an efficient, parallelizable alternative to self-attention with linear complexity. TTT's adaptability and efficiency make it a strong candidate for processing longer contexts, addressing the scalability challenges of RNNs and outperforming Transformer-based architectures in this regard.\n\n\nMLPs and CNNs for LTSF Recent advancements in long-term time series forecasting (LTSF) have introduced efficient architectures that avoid the complexity of attention mechanisms and recurrence. TSMixer (Chen et al., 2023b), an MLP-based model, achieves competitive performance by separating temporal and feature interactions through time- and channel-mixing, enabling linear scaling with sequence length. However, MLPs may struggle with long-range dependencies and require careful hyperparameter tuning, especially for smaller datasets. Convolutional neural networks (CNNs) have also proven effective for LTSF, particularly in capturing local temporal patterns. ModernTCN (Donghao & Xue, 2024) improves temporal convolution networks (TCNs) using dilated convolutions and a hierarchical structure to efficiently capture both short- and long-range dependencies, making it well-suited for multi-scale time series data.\n\n\nBuilding on these developments, we improve the original TimeMachine model by replacing its Mamba blocks with Test-Time Training (TTT) blocks to enhance long-context prediction capabilities. We also explore CNN configurations, such as convolutional stacks, to enrich local temporal feature extraction. This hybrid approach combines the efficiency of MLPs, the local pattern recognition of CNNs, and the global context modeling of TTT, leading to a more robust architecture for LTSF tasks that balances both short- and long-term forecasting needs.\n\n\n# 3 MODEL ARCHITECTURE\n\n\nThe task of Time Series Forecasting can be defined as follows: Given a multivariate time series dataset with a window of past observations (look-back window)  $L$ :  $(\\mathbf{x}_1, \\ldots, \\mathbf{x}_L)$ , where each  $\\mathbf{x}t$  is a vector of dimension  $M$  (the number of channels at time  $t$ ), the goal is to predict the next  $T$  future values  $(\\mathbf{x}_{L+1}, \\ldots, \\mathbf{x}_{L+T})$ .\n\n\nThe TimeMachine (Ahamed & Cheng, 2024) architecture, which we used as the backbone, is designed to capture long-term dependencies in multivariate time series data, offering linear scalability and a small memory footprint. It integrates four Mamba (Gu & Dao, 2024) modules as sequence modeling blocks to selectively memorize or forget historical data, and employs two levels of downsampling to generate contextual cues at both high and low resolutions.\n\n\nHowever, Mamba's approach still relies on fixed-size hidden states to compress historical information over time, often leading to the model forgetting earlier information in long sequences. TTT (Sun et al., 2024) uses dynamically updated weights (in the form of matrices inside linear or MLP layers) to compress and store historical data. This dynamic adjustment during test time allows TTT to better capture long-term relationships by continuously incorporating new information. Its Hidden State Updating Rule is defined as:\n\n\n$$\n\n$$\nW _ {t} = W _ {t - 1} - \\eta \\nabla \\ell (W _ {t - 1}; x _ {t}) = W _ {t - 1} - \\eta \\nabla \\| f (\\tilde {x} _ {t}; W) - x _ {t} \\| ^ {2}\n$$\n\n$$\n\n\nWe incorporated TTT into the TimeMachine model, replacing the original Mamba block. We evaluated our approach with various setups, including different backbones and TTT layer configurations. Additionally, we introduced convolutional layers before the sequence modeling block and conducted experiments with different context lengths and prediction lengths. We provide mathematical foundations as to why TTT is able to perform test-time adaptation without catastrophic forgetting and how the module adapts to distribution shifts in Appendix A. In the same Appendix we quantify the computational overhead introduced by test-time updates and provide empirical validation, published by the authors who proposed TTT in Sun et al. (2020), on how it performs on real corrupted data and provide some intuition on the parameter initialization in TTT as discussed in the same reference.\n\n\nOur goal is to improve upon the performance of the state-of-the-art (SOTA) models in LTSF using the latest advancements in sequential modeling. Specifically, we integrate Test-Time Training (TTT) modules into our model for two key reasons, TTT is theoretically proven to have an extremely long context window, being a form of linear RNN (Orvieto et al., 2023), capable of capturing long-range dependencies efficiently. Secondly, the expressive hidden states of TTT allow the model to capture a diverse set of features without being constrained by the architecture, including the depth of the hidden layers, their size, or the types of blocks used.\n\n\n# 3.1 GENERAL ARCHITECTURE\n\n\nOur model architecture builds upon the TimeMachine model (Ahamed & Cheng, 2024), introducing key modifications, as shown in Figure 1a, 1b and 1c. Specifically, we replace the Mamba modules in TimeMachine with TTT (Test-Time Training) modules (Sun et al., 2024), which retain compatibility since both are linear RNNs (Orvieto et al., 2023). However, TTT offers superior long-range dependency modeling due to its adaptive nature and theoretically infinite context window. A detailed visualization of the TTT block and the different proposed architectures can be found in Appendix D\n\n\nOur model features a two-level hierarchical architecture that captures both high-resolution and low-resolution context, as illustrated in Figure 1a. To adapt to the specific characteristics of the dataset, the architecture handles two scenarios\u2014Channel Mixing and Channel Independence\u2014illustrated in Figure 1b and 1c respectively. A more detailed and mathematical description of the normalization and prediction procedures can be found in Appendix D. We provide a computational complexity analysis of the TTT, Transformer, Mamba and ModernTCN modules in Appendix F and we also provide a computational complexity analysis for our model, TimeMachine, iTransformer, PatchTST, TSMixer and ModernTCN in Appendix G. We also included a mathematical comparison between the Mamba and TTT modules in Appendix B as well as theoretical comparison between the TTT module and models handling noise or temporal regularization in Appendix C.\n\n\n![(a) TimeMachine incorporating TTT-Blocks](images/2803ba6504911df49205efad04e246c5be49517aae973ab5a9064c1d7fc38fb5.jpg)\n\n\n![(b) Channel Mixing Mode](images/de31e3200758d072f5599ab7b1d2132912e301c346df5e7ab0078eb3c99d0836.jpg)\n\n\n![(c) Channel Independence Mode Figure 1: Our model architecture. (a) We replace the four Mamba Block in TimeMachine with four TTT(Test-Time Training) Block. (b) There are two modes of TimeMachine, the channel mixing mode for capturing strong between-channel correlations, and the channel independence mode for modeling within-channel dynamics. Recent works such as PatchTST (Nie et al., 2023) and TiDE (Das et al., 2024) have shown channel independence can achieve SOTA accuracy. For the channel independence scenario, the inputs are first transposed, and then we integrate two linear layers  $(1 \\times 16$  and  $16 \\times 1)$  to provide the TTT Block with a sufficiently large hidden size.](images/70339a4d854407c29e9bfa85c6eb9383adfea74fd276c2191e572a0700e76555.jpg)\n\n\n# 3.2 HIERARCHICAL EMBEDDING\n\n\nThe input sequence  $BML$  (Batch, Channel, Length) is first passed through Reversible Instance Normalization (Kim et al., 2021) (RevIN), which stabilizes the model by normalizing the input data and helps mitigate distribution shifts. This operation is essential for improving generalization across datasets.\n\n\nAfter normalization, the sequence passes through two linear embedding layers. Linear E1 and Linear E2 are used to map the input sequence into two embedding levels: higher resolution and lower resolution. The embedding operations  $E_1:\\mathbb{R}^{M\\times L}\\to \\mathbb{R}^{M\\times n_1}$  and  $E_{2}:\\mathbb{R}^{M\\times n_{1}}\\to \\mathbb{R}^{M\\times n_{2}}$  are achieved through MLP.  $n_1$  and  $n_2$  are configurations that take values from  $\\{512,256,128,64,32\\}$ , satisfying  $n_1 > n_2$ . Dropout layers are applied after each embedding layer to prevent overfitting, especially for long-term time series data. As shown in Figure 1a.\n\n\nWe provide more intuition on the effectiveness of hierarchical modeling in Appendix E.\n\n\n# 3.3 TWO LEVEL CONTEXTUAL CUE MODELING\n\n\nAt each of the two embedding levels, a contextual cues modeling block processes the output from the Dropout layer following E1 and E2. This hierarchical architecture captures both fine-grained and broad temporal patterns, leading to improved forecasting accuracy for long-term time series data.\n\n\nIn Level 1, High-Resolution Contextual Cues Modeling is responsible for modeling high-resolution contextual cues. TTT Block 3 and TTT Block 4 process the input tensor, focusing on capturing fine-grained temporal dependencies. The TTT Block3 operates directly on the input, and transposition may be applied before TTT Block4 if necessary. The outputs are summed, then concatenated with the Level 2 output. There is no residual connection summing in Level 1 modeling.\n\n\nIn Level 2, Low-Resolution Contextual Cues Modeling handles broader temporal patterns, functioning similarly to Level 1. TTT Block 1 and TTT Block 2 process the input tensor to capture low-resolution temporal cues and add them together. A linear projection layer (P-1) is then applied to maps the output (with dimension  $\\mathrm{RM}\\times \\mathrm{n}2$ ) to a higher dimension  $\\mathrm{RM}\\times \\mathrm{n}1$ , preparing it for concate\n\n\nnation. Additionally, the Level 1 and Level 2 Residual Connections ensure that information from previous layers is effectively preserved and passed on.\n\n\n# 3.4 FINAL PREDICTION\n\n\nAfter processing both high-resolution and low-resolution cues, the model concatenates the outputs from both levels. A final linear projection layer (P-2) is then applied to generate the output predictions. The output is subsequently passed through RevIN Denormalization, which reverses the initial normalization and maps the output back to its original scale for interpretability. For more detailed explanations and mathematical descriptions refer to Appendix D.\n\n\n# 3.5 CHANNEL MIXING AND INDEPENDENCE MODES\n\n\nThe Channel Mixing Mode (Figure 1a and 1b) processes all channels of a multivariate time series together, allowing the model to capture potential correlations between different channels and understand their interactions over longer time. Figure 1a illustrates an example of the channel mixing case, but there is also a channel independence case corresponding to Figure 1a, which we have not shown here. Figures 1b and 1c demonstrate the channel mixing and independence modes of the Level 1 High-Resolution Contextual Cues Modeling part with TTT Block 3 and TTT Block 4. Similar versions of the two channel modes for Level 2 Low-Resolution Contextual Cues Modeling are quite similar to those in Level 1, which we have also omitted here.\n\n\nThe Channel Independence Mode (Figure 1c) treats each channel of a multivariate time series as an independent sequence, enabling the model to analyze individual time series more accurately. This mode focuses on learning patterns within each channel without considering potential correlations between them.\n\n\nThe main difference between these two modes is that the Channel Independence Mode always uses transposition before and after one of the TTT blocks (in Figure 1c, it's TTT Block 4). This allows the block to capture contextual cues from local perspectives, while the other block focuses on modeling the global context. However, in the Channel Mixing Mode, both TTT Block 3 and TTT Block 4 model the global context.\n\n\nThe hidden size value for TTT Blocks in global context modeling is set to  $n_1$  since the input shape is  $BMn_1$  for Channel Mixing and  $(B \\times M)1n_1$  for Channel Independence. To make the TTT Block compatible with the local context modeling scenario\u2014where the input becomes  $(B \\times M)n_1 \\gets \\mathrm{Transpose}((B \\times M)1n_1)$  after transposition\u2014we add two linear layers: one for upsampling to  $(B \\times M)n_116$  and another for downsampling back. In this case, the hidden size of TTT Block 4 is set to 16.\n\n\n# 4 EXPERIMENTS AND EVALUATION\n\n\n# 4.1 ORIGINAL EXPERIMENTAL SETUP\n\n\nWe evaluate our model on seven benchmark datasets that are commonly used for LTSF, namely: Traffic, Weather, Electricity, ETTh1, ETTh2, ETTm1, and ETTm2 from Wu et al. (2022) and Zhou et al. (2021). Among these, the Traffic and Electricity datasets are significantly larger, with 862 and 321 channels, respectively, and each containing tens of thousands of temporal points. Table 6 summarizes the dataset details in Appendix I.\n\n\nFor all experiments, we adopted the same setup as in Liu et al. (2024), fixing the look-back window  $L = 96$  and testing four different prediction lengths  $T = 96, 192, 336, 720$ . We compared our TimeMachine-TTT model against 12 state-of-the-art (SOTA) models, including TimeMachine (Ahamed & Cheng, 2024), iTransformer (Liu et al., 2024), PatchTST (Nie et al., 2023), DLinear (Zeng et al., 2022), RLinear (Li et al., 2023), Autoformer (Wu et al., 2022), Crossformer Zhang & Yan (2023), TiDE (Das et al., 2024), Scinet (Liu et al., 2022a), TimesNet (Wu et al., 2023), FEDformer (Zhou et al., 2022), and Stationary (Liu et al., 2023b). All experiments were conducted with both MLP and Linear architectures using the original Mamba backbone, and we confirmed the results from the TimeMachine paper. We include calculations on the resource utilization of the model in Appendix G and quantify the impact of test-time updates on memory and latency in Appendix A.\n\n\nTable 1: Results in MSE and MAE (the lower the better) for the long-term forecasting task (averaged over 5 runs). We compare extensively with baselines under different prediction lengths,  $T = \\{96, 192, 336, 720\\}$  following the setting of iTransformer (Liu et al., 2023a). The length of the input sequence  $(L)$  is set to 96 for all baselines. TTT (ours) is our TTT block with the Conv Stack 5 architecture. The best results are in bold and the second best are underlined.\n\n\n<table><tr><td colspan=\"2\">Methods\u2192</td><td colspan=\"2\">TTT(ours)</td><td colspan=\"2\">TimeMachine</td><td colspan=\"2\">iTransformer</td><td colspan=\"2\">RLinear</td><td colspan=\"2\">PatchTST</td><td colspan=\"2\">Crossformer</td><td colspan=\"2\">TiDE</td><td colspan=\"2\">TimesNet</td><td colspan=\"2\">DLinear</td><td colspan=\"2\">SCIFNet</td><td colspan=\"2\">FEDformer</td><td colspan=\"2\">Stationary</td></tr><tr><td>D</td><td>T</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.165</td><td>0.214</td><td>0.164</td><td>0.208</td><td>0.174</td><td>0.214</td><td>0.192</td><td>0.232</td><td>0.177</td><td>0.218</td><td>0.158</td><td>0.230</td><td>0.202</td><td>0.261</td><td>0.172</td><td>0.220</td><td>0.196</td><td>0.255</td><td>0.221</td><td>0.306</td><td>0.217</td><td>0.296</td><td>0.173</td><td>0.223</td></tr><tr><td>192</td><td>0.225</td><td>0.263</td><td>0.211</td><td>0.250</td><td>0.221</td><td>0.254</td><td>0.240</td><td>0.271</td><td>0.225</td><td>0.259</td><td>0.206</td><td>0.277</td><td>0.242</td><td>0.298</td><td>0.219</td><td>0.261</td><td>0.237</td><td>0.296</td><td>0.261</td><td>0.340</td><td>0.276</td><td>0.336</td><td>0.245</td><td>0.285</td></tr><tr><td>336</td><td>0.246</td><td>0.275</td><td>0.256</td><td>0.290</td><td>0.278</td><td>0.296</td><td>0.292</td><td>0.307</td><td>0.278</td><td>0.297</td><td>0.272</td><td>0.335</td><td>0.287</td><td>0.335</td><td>0.280</td><td>0.306</td><td>0.283</td><td>0.335</td><td>0.309</td><td>0.378</td><td>0.339</td><td>0.380</td><td>0.321</td><td>0.338</td></tr><tr><td>720</td><td>0.339</td><td>0.343</td><td>0.342</td><td>0.343</td><td>0.358</td><td>0.349</td><td>0.364</td><td>0.353</td><td>0.354</td><td>0.348</td><td>0.398</td><td>0.418</td><td>0.351</td><td>0.386</td><td>0.365</td><td>0.359</td><td>0.345</td><td>0.381</td><td>0.377</td><td>0.427</td><td>0.403</td><td>0.428</td><td>0.414</td><td>0.410</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.397</td><td>0.268</td><td>0.397</td><td>0.268</td><td>0.395</td><td>0.268</td><td>0.649</td><td>0.389</td><td>0.544</td><td>0.359</td><td>0.522</td><td>0.290</td><td>0.805</td><td>0.493</td><td>0.593</td><td>0.321</td><td>0.650</td><td>0.396</td><td>0.788</td><td>0.499</td><td>0.587</td><td>0.366</td><td>0.612</td><td>0.338</td></tr><tr><td>192</td><td>0.434</td><td>0.287</td><td>0.417</td><td>0.274</td><td>0.417</td><td>0.276</td><td>0.601</td><td>0.366</td><td>0.540</td><td>0.354</td><td>0.530</td><td>0.293</td><td>0.756</td><td>0.474</td><td>0.617</td><td>0.336</td><td>0.598</td><td>0.370</td><td>0.789</td><td>0.505</td><td>0.604</td><td>0.373</td><td>0.613</td><td>0.340</td></tr><tr><td>336</td><td>0.430</td><td>0.283</td><td>0.433</td><td>0.281</td><td>0.433</td><td>0.283</td><td>0.609</td><td>0.369</td><td>0.551</td><td>0.358</td><td>0.558</td><td>0.305</td><td>0.762</td><td>0.477</td><td>0.629</td><td>0.336</td><td>0.605</td><td>0.373</td><td>0.797</td><td>0.508</td><td>0.621</td><td>0.383</td><td>0.618</td><td>0.328</td></tr><tr><td>720</td><td>0.456</td><td>0.286</td><td>0.467</td><td>0.300</td><td>0.467</td><td>0.302</td><td>0.647</td><td>0.387</td><td>0.586</td><td>0.375</td><td>0.589</td><td>0.328</td><td>0.719</td><td>0.449</td><td>0.640</td><td>0.350</td><td>0.645</td><td>0.394</td><td>0.841</td><td>0.523</td><td>0.626</td><td>0.382</td><td>0.653</td><td>0.355</td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.135</td><td>0.230</td><td>0.142</td><td>0.236</td><td>0.148</td><td>0.240</td><td>0.201</td><td>0.281</td><td>0.195</td><td>0.285</td><td>0.219</td><td>0.314</td><td>0.237</td><td>0.329</td><td>0.168</td><td>0.272</td><td>0.197</td><td>0.282</td><td>0.247</td><td>0.345</td><td>0.193</td><td>0.308</td><td>0.169</td><td>0.273</td></tr><tr><td>192</td><td>0.153</td><td>0.254</td><td>0.158</td><td>0.250</td><td>0.162</td><td>0.253</td><td>0.201</td><td>0.283</td><td>0.199</td><td>0.289</td><td>0.231</td><td>0.322</td><td>0.236</td><td>0.330</td><td>0.184</td><td>0.289</td><td>0.196</td><td>0.285</td><td>0.257</td><td>0.355</td><td>0.201</td><td>0.315</td><td>0.182</td><td>0.286</td></tr><tr><td>336</td><td>0.166</td><td>0.255</td><td>0.172</td><td>0.268</td><td>0.178</td><td>0.269</td><td>0.215</td><td>0.298</td><td>0.215</td><td>0.305</td><td>0.246</td><td>0.337</td><td>0.249</td><td>0.344</td><td>0.198</td><td>0.300</td><td>0.209</td><td>0.301</td><td>0.269</td><td>0.369</td><td>0.214</td><td>0.329</td><td>0.200</td><td>0.304</td></tr><tr><td>720</td><td>0.199</td><td>0.285</td><td>0.207</td><td>0.298</td><td>0.225</td><td>0.317</td><td>0.257</td><td>0.331</td><td>0.256</td><td>0.337</td><td>0.280</td><td>0.363</td><td>0.284</td><td>0.373</td><td>0.220</td><td>0.320</td><td>0.245</td><td>0.333</td><td>0.299</td><td>0.390</td><td>0.246</td><td>0.355</td><td>0.222</td><td>0.321</td></tr><tr><td rowspan=\"4\">ETTh1</td><td>96</td><td>0.352</td><td>0.375</td><td>0.364</td><td>0.387</td><td>0.386</td><td>0.405</td><td>0.386</td><td>0.395</td><td>0.414</td><td>0.419</td><td>0.423</td><td>0.448</td><td>0.479</td><td>0.464</td><td>0.384</td><td>0.402</td><td>0.386</td><td>0.400</td><td>0.654</td><td>0.599</td><td>0.376</td><td>0.419</td><td>0.513</td><td>0.491</td></tr><tr><td>192</td><td>0.412</td><td>0.418</td><td>0.415</td><td>0.416</td><td>0.441</td><td>0.436</td><td>0.437</td><td>0.424</td><td>0.460</td><td>0.445</td><td>0.471</td><td>0.474</td><td>0.525</td><td>0.492</td><td>0.436</td><td>0.429</td><td>0.437</td><td>0.432</td><td>0.719</td><td>0.631</td><td>0.420</td><td>0.448</td><td>0.534</td><td>0.504</td></tr><tr><td>336</td><td>0.479</td><td>0.446</td><td>0.429</td><td>0.421</td><td>0.487</td><td>0.458</td><td>0.479</td><td>0.446</td><td>0.501</td><td>0.466</td><td>0.570</td><td>0.546</td><td>0.565</td><td>0.515</td><td>0.491</td><td>0.469</td><td>0.481</td><td>0.459</td><td>0.778</td><td>0.659</td><td>0.459</td><td>0.465</td><td>0.588</td><td>0.535</td></tr><tr><td>720</td><td>0.478</td><td>0.454</td><td>0.458</td><td>0.453</td><td>0.503</td><td>0.491</td><td>0.481</td><td>0.470</td><td>0.500</td><td>0.488</td><td>0.653</td><td>0.621</td><td>0.594</td><td>0.558</td><td>0.521</td><td>0.500</td><td>0.519</td><td>0.516</td><td>0.836</td><td>0.699</td><td>0.506</td><td>0.507</td><td>0.643</td><td>0.616</td></tr><tr><td rowspan=\"4\">ETTh2</td><td>96</td><td>0.274</td><td>0.328</td><td>0.275</td><td>0.334</td><td>0.297</td><td>0.349</td><td>0.288</td><td>0.338</td><td>0.302</td><td>0.348</td><td>0.745</td><td>0.584</td><td>0.400</td><td>0.440</td><td>0.340</td><td>0.374</td><td>0.333</td><td>0.387</td><td>0.707</td><td>0.621</td><td>0.358</td><td>0.397</td><td>0.476</td><td>0.458</td></tr><tr><td>192</td><td>0.373</td><td>0.379</td><td>0.349</td><td>0.381</td><td>0.380</td><td>0.400</td><td>0.374</td><td>0.390</td><td>0.388</td><td>0.400</td><td>0.877</td><td>0.656</td><td>0.528</td><td>0.509</td><td>0.402</td><td>0.414</td><td>0.477</td><td>0.476</td><td>0.860</td><td>0.689</td><td>0.429</td><td>0.439</td><td>0.512</td><td>0.493</td></tr><tr><td>336</td><td>0.403</td><td>0.408</td><td>0.340</td><td>0.381</td><td>0.428</td><td>0.432</td><td>0.415</td><td>0.426</td><td>0.433</td><td>0.426</td><td>0.433</td><td>1.043</td><td>0.731</td><td>0.643</td><td>0.571</td><td>0.452</td><td>0.452</td><td>0.541</td><td>1.000</td><td>0.744</td><td>0.496</td><td>0.487</td><td>0.496</td><td>0.552</td></tr><tr><td>720</td><td>0.448</td><td>0.434</td><td>0.411</td><td>0.433</td><td>0.427</td><td>0.445</td><td>0.420</td><td>0.440</td><td>0.431</td><td>0.446</td><td>1.104</td><td>0.763</td><td>0.874</td><td>0.679</td><td>0.462</td><td>0.468</td><td>0.831</td><td>0.657</td><td>1.249</td><td>0.838</td><td>0.463</td><td>0.474</td><td>0.562</td><td>0.560</td></tr><tr><td rowspan=\"4\">ETTh1</td><td>96</td><td>0.309</td><td>0.348</td><td>0.317</td><td>0.355</td><td>0.334</td><td>0.368</td><td>0.355</td><td>0.376</td><td>0.329</td><td>0.367</td><td>0.404</td><td>0.426</td><td>0.364</td><td>0.387</td><td>0.338</td><td>0.375</td><td>0.345</td><td>0.372</td><td>0.418</td><td>0.438</td><td>0.379</td><td>0.419</td><td>0.386</td><td>0.398</td></tr><tr><td>192</td><td>0.371</td><td>0.389</td><td>0.357</td><td>0.378</td><td>0.377</td><td>0.391</td><td>0.391</td><td>0.392</td><td>0.367</td><td>0.385</td><td>0.450</td><td>0.451</td><td>0.398</td><td>0.404</td><td>0.374</td><td>0.387</td><td>0.380</td><td>0.389</td><td>0.439</td><td>0.450</td><td>0.426</td><td>0.441</td><td>0.459</td><td>0.444</td></tr><tr><td>336</td><td>0.381</td><td>0.401</td><td>0.379</td><td>0.399</td><td>0.426</td><td>0.420</td><td>0.424</td><td>0.415</td><td>0.399</td><td>0.410</td><td>0.532</td><td>0.515</td><td>0.428</td><td>0.425</td><td>0.410</td><td>0.411</td><td>0.413</td><td>0.413</td><td>0.490</td><td>0.485</td><td>0.445</td><td>0.459</td><td>0.464</td><td></td></tr><tr><td>720</td><td>0.433</td><td>0.423</td><td>0.445</td><td>0.436</td><td>0.491</td><td>0.459</td><td>0.487</td><td>0.450</td><td>0.454</td><td>0.439</td><td>0.666</td><td>0.589</td><td>0.487</td><td>0.461</td><td>0.478</td><td>0.450</td><td>0.474</td><td>0.453</td><td>0.595</td><td>0.550</td><td>0.543</td><td>0.490</td><td>0.585</td><td>0.516</td></tr><tr><td rowspan=\"4\">ETTh2</td><td>96</td><td>0.180</td><td>0.253</td><td>0.175</td><td>0.256</td><td>0.180</td><td>0.264</td><td>0.182</td><td>0.265</td><td>0.175</td><td>0.259</td><td>0.287</td><td>0.366</td><td>0.207</td><td>0.305</td><td>0.187</td><td>0.267</td><td>0.193</td><td>0.292</td><td>0.286</td><td>0.377</td><td>0.203</td><td>0.287</td><td>0.292</td><td>0.287</td></tr><tr><td>192</td><td>0.242</td><td>0.301</td><td>0.239</td><td>0.299</td><td>0.250</td><td>0.309</td><td>0.246</td><td>0.304</td><td>0.241</td><td>0.302</td><td>0.414</td><td>0.492</td><td>0.290</td><td>0.364</td><td>0.249</td><td>0.309</td><td>0.284</td><td>0.362</td><td>0.399</td><td>0.445</td><td>0.269</td><td>0.328</td><td>0.289</td><td>0.339</td></tr><tr><td>336</td><td>0.302</td><td>0.341</td><td>0.287</td><td>0.332</td><td>0.311</td><td>0.348</td><td>0.307</td><td>0.342</td><td>0.305</td><td>0.343</td><td>0.597</td><td>0.542</td><td>0.377</td><td>0.422</td><td>0.321</td><td>0.351</td><td>0.369</td><td>0.427</td><td>0.637</td><td>0.591</td><td>0.325</td><td>0.366</td><td>0.334</td><td>0.361</td></tr><tr><td>720</td><td>0.364</td><td>0.384</td><td>0.371</td><td>0.385</td><td>0.412</td><td>0.417</td><td>0.407</td><td>0.398</td><td>0.402</td><td>0.400</td><td>1.730</td><td>1.042</td><td>0.558</td><td>0.524</td><td>0.408</td><td>0.403</td><td>0.554</td><td>0.522</td><td>0.960</td><td>0.735</td><td>0.421</td><td>0.415</td><td>0.417</td><td>0.413</td></tr></table>\n\n\n# 4.2 QUANTITATIVE RESULTS\n\n\nAcross all seven benchmark datasets, our TimeMachine-TTT model consistently demonstrated superior performance compared to SOTA models. In the Weather dataset, TTT achieved leading performance at longer horizons (336 and 720), with MSEs of 0.246 and 0.339, respectively, outperforming TimeMachine, which recorded MSEs of 0.256 and 0.342. The Traffic dataset, with its high number of channels (862), also saw TTT outperform TimeMachine and iTransformer at both medium (336-step MSE of 0.430 vs. 0.433) and long horizons (720-step MSE of 0.464 vs. 0.467), highlighting the model's ability to handle multivariate time series data.\n\n\nIn the Electricity dataset, TTT showed dominant results across all horizons, achieving an MSE of 0.135, 0.153, 0.166 and 0.199 at horizons 96, 192, 336, and 720 respectively, outperforming TimeMachine and PatchTST. For ETTh1, TTT was highly competitive, with strong short-term results (MSE of 0.352 at horizon 96) and continued dominance at medium-term horizons like 336, with an MSE of 0.412. For ETTh2, TTT beat TimeMachine on horizon 96 (MSE of 0.274), TTT also closed the gap at longer horizons (MSE of 0.448 at horizon 720 compared to 0.411 for TimeMachine).\n\n\nFor the ETTm1 dataset, TTT outperformed TimeMachine at nearly every horizon, recording an MSE of 0.309, 0.381 and 0.431 at horizon 96, 336 and 720 respectively, confirming its effectiveness for long-term forecasting. Similarly, in ETTm2, TTT remained highly competitive at longer horizons, with a lead over TimeMachine at horizon 720 (MSE of 0.362 vs. 0.371). The radar plot in Figure 2 shows the comparison between TTT (ours) and TimeMachine for both MSE and MAE on all datasets.\n\n\n# 5 PREDICTION LENGTH ANALYSIS AND ABLATION STUDY\n\n\n# 5.1 EXPERIMENTAL SETUP WITH ENHANCED ARCHITECTURES\n\n\nTo assess the impact of enhancing the model architecture, we conducted experiments by adding hidden layer architectures before the sequence modeling block in each of the four TTT blocks. The goal was to improve performance by enriching feature extraction through local temporal context. As shown in Figure 3 in Appendix D.\n\n\n![Figure 2: Average MSE and MAE comparison of our model and SOTA baselines with  $L = 720$ . The circle center represents the maximum possible error. Closer to the boundary indicates better performance.](images/0d916bb3a1b959772571d39cd5ba1cadbb5ecffd6d30f330dd1d9c83324d456b.jpg)\n\n\nWe tested the following configurations: (1) Conv 3: 1D Convolution with kernel size 3, (2) Conv 5: 1D Convolution with kernel size 5, (3) Conv Stack 3: two 1D Convolutions with kernel size 3 in cascade, (4) Conv Stack 5: two 1D Convolutions with kernel sizes 5 and 3 in cascade, (5) Inception: an Inception Block combining 1D Convolutions with kernel sizes 5 and 3, followed by concatenation and reduction to the original size and (6) ModernTCN: A modern convolutional block proposed in Donghao & Xue (2024) that uses depthwise and pointwise convolutions with residual connections similar to the structure of the transformer block. For the simpler architectures kernel sizes were limited to 5 to avoid oversmoothing, and original data dimensions were preserved to ensure consistency with the TTT architecture. For ModernTCN we reduced the internal dimensions to 16 (down from the suggested 64) and did not use multiscale due to the exponential increase in GPU memory required which slowed down the training process and did not allow the model to fit in a single A100 GPU. We kept the rest of the parameters of ModernTCN the same as in the original paper.\n\n\nAblation Study Findings Our findings reveal that the introduction of additional hidden layer architectures, including convolutional layers, had varying degrees of impact on performance across different horizons. The best-performing setup was the Conv Stack 5 architecture, which achieved the lowest MSE and MAE at the 96 time horizon, with values of 0.261 and 0.289, respectively, outperforming the TimeMachine model at this horizon. At longer horizons, such as 336 and 720, Conv Stack 5 continued to show competitive performance, with a narrow gap between it and the TimeMachine model. For example, at the 720 horizon, Conv Stack 5 showed an MAE of 0.373, while TimeMachine had an MAE of 0.378.\n\n\nHowever, other architectures, such as Conv 3 and Conv 5, provided only marginal improvements over the baseline TTT architectures (Linear and MLP). While they performed better than Linear and MLP, they did not consistently outperform more complex setups like Conv Stack 3 and 5 across all horizons. This suggests that hidden layer expressiveness can enhance model performance.\n\n\nModernTCN showed competitive results across multiple datasets (see Appendix I), such as ETTh2, where it achieved an MSE of 0.285 at horizon 96, outperforming Conv 3 and Conv 5. However, as with other deep convolutional layers, ModernTCN's increased complexity also led to slower training times compared to simpler setups like Conv 3 and it failed to match Conv Stack 5's performance.\n\n\n# 5.2 EXPERIMENTAL SETUP WITH INCREASED PREDICTION & SEQUENCE LENGTHS\n\n\nFor the second part of our experiments, we extended the sequence and prediction lengths beyond the parameters tested in previous studies. We used the same baseline architectures (MLP and Linear) with the Mamba backbone as in the original TimeMachine paper, but this time also included the best-performing 1D Convolution architecture with kernel size 3.\n\n\nThe purpose of these experiments was to test the model's capacity to handle much longer sequence lengths while maintaining high prediction accuracy. We tested the following sequence and prediction lengths, with  $L = 2880$  and 5760, far exceeding the original length of  $L = 96$ :\n\n\n<table><tr><td>Seq Length</td><td>2880</td><td>2880</td><td>2880</td><td>2880</td><td>5760</td><td>5760</td><td>5760</td><td>5760</td><td>720</td><td>720</td><td>720</td><td>720</td></tr><tr><td>Pred Length</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td></tr></table>\n\n\nTable 2: Testing parameters for sequence and prediction lengths.\n\n\n# 5.3 RESULTS AND STATISTICAL COMPARISONS FOR PROPOSED ARCHITECTURES\n\n\nThe proposed architectures\u2014TTT Linear, TTT MLP, Conv Stack 3, Conv Stack 5, Conv 3, Conv 5, Inception, and TTT with ModernTCN\u2014exhibit varying performance across prediction horizons. TTT Linear performs well at shorter horizons (MSE 0.268, MAE 0.298 at horizon 96) but declines at longer horizons (MSE 0.357 at horizon 336). TTT MLP follows a similar trend with slightly worse performance. Conv 3 and Conv 5 outperform Linear and MLP at shorter horizons (MSE 0.269, MAE 0.297 at horizon 96) but lag behind Conv Stack models at longer horizons. TTT with ModernTCN shows promising results at shorter horizons, such as MSE 0.389, MAE 0.402 on ETTh1, and MSE 0.285, MAE 0.340 on ETTh2 at horizon 96. Although results for Traffic and Electricity datasets are pending, preliminary findings indicate TTT with ModernTCN is competitive, particularly for short-term dependencies (see Table 7 in Appendix I). Conv Stack 5 performs best at shorter horizons (MSE 0.261, MAE 0.289 at horizon 96). Inception provides stable performance across horizons, closely following Conv Stack 3 (MSE 0.361 at horizon 336). At horizon 720, Conv 5 shows a marginal improvement over Conv 3, with an MSE of 0.400 compared to 0.406. The Conv Stack 5 architecture demonstrates the best overall performance among all convolutional models.\n\n\n# 5.4 RESULTS AND STATISTICAL COMPARISONS FOR INCREASED PREDICTION AND SEQUENCE LENGTHS\n\n\nBoth shorter and longer sequence lengths affect model performance differently. Shorter sequence lengths (e.g., 2880) provide better accuracy for shorter prediction horizons, with the TTT model achieving an MSE of 0.332 and MAE of 0.356 at horizon 192, outperforming TimeMachine. Longer sequence lengths (e.g., 5760) result in higher errors, particularly for shorter horizons, but TTT remains more resilient, showing improved performance over TimeMachine. For shorter prediction lengths (96 and 192), TTT consistently yields lower MSE and MAE compared to TimeMachine. As prediction lengths grow to 720, both models experience increasing error rates, but TTT maintains a consistent advantage. For instance, at horizon 720, TTT records an MSE of 0.517 compared to TimeMachine's 0.535. Overall, TTT consistently outperforms TimeMachine across most prediction horizons, particularly for shorter sequences and smaller prediction windows. As the sequence length increases, TTT's ability to manage long-term dependencies becomes increasingly evident, with models like Conv Stack 5 showing stronger performance at longer horizons.\n\n\n# 5.5 EVALUATION\n\n\nThe results of our experiments indicate that the TimeMachine-TTT model outperforms the SOTA models across various scenarios, especially when handling larger sequence and prediction lengths. Several key trends emerged from the analysis:\n\n- - Improved Performance on Larger Datasets: On larger datasets, such as Electricity, Traffic, and Weather, TTT models demonstrated superior performance compared to TimeMachine. For example, at a prediction length of 96, the TTT architecture achieved an MSE of 0.283 compared to TimeMachine's 0.309, reflecting a notable improvement. This emphasizes TTT's ability to effectively handle larger temporal windows.\n\n- - Better Handling of Long-Range Dependencies: TTT-based models, particularly Conv Stack 5 and Conv 3, demonstrated clear advantages in capturing long-range dependencies. As prediction lengths increased, such as at 720, TTT maintained better error rates, with Conv Stack 5 achieving an MAE of 0.373 compared to TimeMachine's 0.378. Although the difference narrows at longer\n\n\n<table><tr><td></td><td colspan=\"2\">Conv stack 5</td><td colspan=\"2\">TimeMachine</td><td colspan=\"2\">Conv 3</td><td colspan=\"2\">Conv 5</td><td colspan=\"2\">Conv stack 3</td><td colspan=\"2\">Inception</td><td colspan=\"2\">Linear</td><td colspan=\"2\">MLP</td></tr><tr><td>horizon</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>96</td><td>0.259</td><td>0.288</td><td>0.262</td><td>0.292</td><td>0.269</td><td>0.297</td><td>0.269</td><td>0.297</td><td>0.272</td><td>0.300</td><td>0.274</td><td>0.302</td><td>0.268</td><td>0.298</td><td>0.271</td><td>0.301</td></tr><tr><td>192</td><td>0.316</td><td>0.327</td><td>0.307</td><td>0.321</td><td>0.318</td><td>0.329</td><td>0.320</td><td>0.331</td><td>0.319</td><td>0.330</td><td>0.321</td><td>0.330</td><td>0.326</td><td>0.336</td><td>0.316</td><td>0.332</td></tr><tr><td>336</td><td>0.344</td><td>0.344</td><td>0.328</td><td>0.339</td><td>0.348</td><td>0.348</td><td>0.347</td><td>0.347</td><td>0.359</td><td>0.358</td><td>0.361</td><td>0.359</td><td>0.357</td><td>0.358</td><td>0.358</td><td>0.357</td></tr><tr><td>720</td><td>0.388</td><td>0.373</td><td>0.386</td><td>0.378</td><td>0.406</td><td>0.389</td><td>0.400</td><td>0.389</td><td>0.399</td><td>0.387</td><td>0.404</td><td>0.390</td><td>0.414</td><td>0.393</td><td>0.394</td><td>0.393</td></tr></table>\n\n\nTable 3: MSE and MAE performance metrics for TimeMachine, TTT blocks with original architectures (MLP & Linear), and TTT block with different convolutional architectures across all prediction horizons.\n\n\n<table><tr><td></td><td colspan=\"2\">TTT</td><td colspan=\"2\">TimeMachine</td></tr><tr><td>Pred. Length</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>96</td><td>0.283</td><td>0.322</td><td>0.309</td><td>0.337</td></tr><tr><td>192</td><td>0.332</td><td>0.356</td><td>0.342</td><td>0.359</td></tr><tr><td>336</td><td>0.402</td><td>0.390</td><td>0.414</td><td>0.394</td></tr><tr><td>720</td><td>0.517</td><td>0.445</td><td>0.535</td><td>0.456</td></tr><tr><td>1440</td><td>0.399</td><td>0.411</td><td>0.419</td><td>0.429</td></tr><tr><td>2880</td><td>0.456</td><td>0.455</td><td>0.485</td><td>0.474</td></tr><tr><td>4320</td><td>0.580</td><td>0.534</td><td>0.564</td><td>0.523</td></tr></table>\n\n\nTable 4: Average MSE and MAE for different prediction lengths and sequence length of 2880 comparing TimeMachine and TTT architectures.\n\n\n<table><tr><td></td><td colspan=\"2\">TTT</td><td colspan=\"2\">TimeMachine</td></tr><tr><td>Seq. Length</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>720</td><td>0.312</td><td>0.336</td><td>0.319</td><td>0.341</td></tr><tr><td>2880</td><td>0.366</td><td>0.384</td><td>0.373</td><td>0.388</td></tr><tr><td>5760</td><td>0.509</td><td>0.442</td><td>0.546</td><td>0.459</td></tr></table>\n\n\nTable 5: Average MSE and MAE for different sequence lengths comparing TimeMachine and Conv stack 5 architectures.\n\n\nhorizons, the TTT architectures remain more robust, particularly in handling extended sequences and predictions.\n\n- - Impact of Hidden Layer Architectures: While stacked convolutional architectures, such as Conv Stack 3 and Conv Stack 5, provided incremental improvements, simpler architectures like Conv 3 and Conv 5 also delivered competitive performance. Conv Stack 5 showed a reduction in MSE compared to TimeMachine, at horizon 96, where it achieved an MSE of 0.261 versus TimeMachine's 0.262. ModernTCN failed to meet the performance of simpler architectures.\n\n- - Effect of Sequence and Prediction Lengths: As the sequence and prediction lengths increased, all models exhibited higher error rates. However, TTT-based architectures, particularly Conv Stack 5 and Conv 3, handled these increases better than TimeMachine. For example, at a sequence length of 5760 and prediction length of 720, TTT recorded lower MSE and MAE values, demonstrating better scalability and adaptability to larger contexts. Moreover, shorter sequence lengths (e.g., 2880) performed better at shorter horizons, while longer sequence lengths showed diminishing returns for short-term predictions.\n\n\n# 6 CONCLUSION AND FUTURE WORK\n\n\nIn this work, we improved the state-of-the-art (SOTA) model for time series forecasting by replacing the Mamba modules in the original TimeMachine model with Test-Time Training (TTT) modules, which leverage linear RNNs to capture long-range dependencies. Extensive experiments demonstrated that the TTT architectures\u2014MLP and Linear\u2014performed well, with MLP slightly outperforming Linear. Exploring alternative architectures, particularly Conv Stack 5 and ModernTCN, significantly improved performance at longer prediction horizons, with ModernTCN showing notable efficiency in capturing short-term dependencies. The most significant gains came from increasing sequence and prediction lengths, where our TTT models consistently matched or outperformed the SOTA model, particularly on larger datasets like Electricity, Traffic, and Weather, emphasizing the model's strength in handling long-range dependencies. While convolutional stacks and ModernTCN showed promise, further improvements could be achieved by refining hidden layer configurations and exploring architectural diversity. We included some potential real world applications of the TTT module in Appendix C along with why we believe it's best suited for LTSF. Overall, this work demonstrates the potential of TTT modules in long-term forecasting, especially when combined with robust convolutional architectures and applied to larger datasets and longer horizons.\n\n\n# REFERENCES\n\n- Md Atik Ahamed and Qiang Cheng. Timemachine: A time series is worth 4 mambas for long-term forecasting, 2024. URL https://arxiv.org/abs/2403.09898.\n\n- Ekin Akyurek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. The surprising effectiveness of test-time training for abstract reasoning, 2024. URL https://arxiv.org/abs/2411.07279.\n\n- Sercan O. Arik, Nathanael C. Yoder, and Tomas Pfister. Self-adaptive forecasting for improved deep learning on non-stationary time-series, 2022. URL https://arxiv.org/abs/2202.02403.\n\n- Jonathan Z. Bakdash, Steve Hutchinson, Erin G. Zaroukian, Laura R. Marusich, Saravanan Thirumuruganathan, Charmaine Sample, Blaine Hoffman, and Gautam Das. Malware in the future? forecasting of analyst detection of cyber events. arXiv preprint arXiv:1707.03243, 2017.\n\n- Muxi Chen, Zhijian Xu, Ailing Zeng, and Qiang Xu. Fraud: Frequency domain augmentation for time series forecasting, 2023a. URL https://arxiv.org/abs/2302.09292.\n\n- Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, and Tomas Pfister. Tsmixer: An all-mlp architecture for time series forecasting, 2023b. URL https://arxiv.org/abs/2303.06053.\n\n- Xupeng Chen, Ran Wang, Amirhossein Khalilian-Gourtani, Leyao Yu, Patricia Dugan, Daniel Friedman, Werner Doyle, Orrin Devinsky, Yao Wang, and Adeen Flinker. A neural speech decoding framework leveraging deep learning and speech synthesis. Nature Machine Intelligence, pp. 1-14, 2024.\n\n- Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, and Rose Yu. Long-term forecasting with tide: Time-series dense encoder, 2024. URL https://arxiv.org/abs/2304.08424.\n\n- John Doe and Jane Smith. Time series forecasting for energy consumption. *Energies*, 15(3):773, 2023. doi: 10.3390/en15030773. URL https://www.mdpi.com/1996-1073/15/3/773.\n\n- John Doe and Jane Smith. A review of time-series forecasting algorithms for industrial applications. *Machines*, 12(6):380, 2024. doi: 10.3390/machines12060380. URL https://www.mdpi.com/2075-1702/12/6/380.\n\n- Luo Donghao and Wang Xue. ModernTCN: A modern pure convolution structure for general time series analysis. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=vpJMJerXHU.\n\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.\n\n- Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://arxiv.org/abs/2312.00752.\n\n- Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces, 2022a. URL https://arxiv.org/abs/2111.00396.\n\n- Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models, 2022b. URL https://arxiv.org/abs/2206.11893.\n\n- Ankit Gupta, Harsh Mehta, and Jonathan Berant. Simplifying and understanding state space models with diagonal linear rnns, 2023. URL https://arxiv.org/abs/2212.00768.\n\n- Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models, 2024. URL https://arxiv.org/abs/2305.18466.\n\n- Haowen Hou and F. Richard Yu. Rwkv-ts: Beyond traditional recurrent neural network for time series tasks, 2024. URL https://arxiv.org/abs/2401.09093.\n\n- Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2021.\n\n- Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=cGDAkQo1C0p.\n\n- Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An investigation on linear mapping, 2023. URL https://arxiv.org/abs/2305.10721.\n\n- Bryan Lim and Stefan Zohren. Time series forecasting with deep learning: A survey. arXiv preprint arXiv:2004.13408, 2020.\n\n- Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiumia Lai, Lingna Ma, and Qiang Xu. Scinet: Time series modeling and forecasting with sample convolution and interaction, 2022a. URL https://arxiv.org/abs/2106.09305.\n\n- Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on Learning Representations, 2022b.\n\n- Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023a.\n\n- Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting, 2023b. URL https://arxiv.org/abs/2205.14415.\n\n- Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting, 2024. URL https://arxiv.org/abs/2310.06625.\n\n- Haowei Ni, Shuchen Meng, Xieming Geng, Panfeng Li, Zhuoying Li, Xupeng Chen, Xiaotong Wang, and Shiyao Zhang. Time series modeling for heart rate prediction: From arima to transformers. arXiv preprint arXiv:2406.12199, 2024.\n\n- Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers, 2023. URL https://arxiv.org/abs/ 2211.14730.\n\n- Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org, 2023.\n\n- Badri N. Patro and Vijay S. Agneeswaran. Simba: Simplified mamba-based architecture for vision and multivariate time series, 2024. URL https://arxiv.org/abs/2403.15360.\n\n- Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts, 2020. URL https://arxiv.org/abs/1909.13231.\n\n- Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. Learning to (learn at test time): Rnns with expressive hidden states, 2024. URL https://arxiv.org/abs/2407.04620.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n\n- Renhao Wang, Yu Sun, Yossi Gandelsman, Xinlei Chen, Alexei A. Efros, and Xiaolong Wang. Test-time training on video streams, 2023. URL https://arxiv.org/abs/2307.05014.\n\n- Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Transformers in time series: A survey, 2023. URL https://arxiv.org/abs/2202.07125.\n\n- Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting, 2022. URL https://arxiv.org/abs/2106.13008.\n\n- Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis, 2023. URL https://arxiv.org/abs/2210.02186.\n\n- Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting?, 2022. URL https://arxiv.org/abs/2205.13504.\n\n- Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=vSVLM2j9eie.\n\n- Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting, 2021. URL https://arxiv.org/abs/2012.07436.\n\n- Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting, 2022. URL https:// arxiv.org/abs/2201.12740.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638432.0032446", "msecs": "3.0", "relativeCreated": "28833.742141723633", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# How Low Can We Go: Trading Memory for Error in Low-Precision Training\n\n# Abstract\nLow-precision arithmetic trains deep learning models using less energy, less memory and less time. However, we pay a price for the savings: lower precision may yield larger round-off error and hence larger prediction error. As applications proliferate, users must choose which precision to use to train a new model, and chip manufacturers must decide which precisions to manufacture. We view these precision choices as a hyperparameter tuning problem, and borrow ideas from meta-learning to learn the tradeoff between memory and error. In this paper, we introduce Pareto Estimation to Pick the Perfect Precision (PEPPP). We use matrix factorization to find non-dominated configurations (the Pareto frontier) with a limited number of network evaluations. For any given memory budget, the precision that minimizes error is a point on this frontier. Practitioners can use the frontier to trade memory for error and choose the best precision for their goals.\n\n\n# 1 INTRODUCTION\n\n\nTraining modern-day neural networks is becoming increasingly expensive as task and model sizes increase. The energy consumption of the corresponding computation has increased alarmingly, and raises doubts about the sustainability of modern training practices (Schwartz et al., 2019). Low-precision training can reduce consumption of both computation (De Sa et al., 2018) and memory (Sohoni et al., 2019a), thus minimizing the cost and energy to train larger models and making deep learning more accessible to resource-limited users.\n\n\nLow-precision training replaces 32-bit or 64-bit floating point numbers with fixed or floating point numbers that allocate fewer bits for the activations, optimizers, and weights. A rich variety of methods appear in recent literature, including bit-centering (De Sa et al., 2018), loss scaling (Micikevicius et al., 2017) and mixed-precision training (Zhou et al., 2016). There is however a fundamental tradeoff: lowering the number of bits of precision increases the quantization error, which may disrupt convergence and increase downstream error (Zhang et al., 2016; Courbariaux et al., 2014; Gupta et al., 2015). The low-precision configuration is thus a hyperparameter to be chosen according to the resource constraints of the practitioner.\n\n\n![(a) error vs memory on CIFAR-10 (Krizhevsky et al., 2009)](images/bf3eed83e98a16a1d63f290ba48a56b1ef09695237b8ca9d02b4c90af7095ce5.jpg)\n\n\n![(b) # activation bits in non-dominated configurations Figure 1: Test error vs memory for ResNet-18 across 99 low-precision floating point configurations. Figure (a) shows the tradeoff on CIFAR-10. (Non-dominated points are blue circles.) Figure (b) shows that the best precision to use varies depending on the memory budget, on 87 image datasets. See Section 4 for experimental details.](images/ca82afa05cb37f487616729bcd307fef9b46547a927f1c9a1d36718af2b3418e.jpg)\n\n\nHow should we choose this hyperparameter? Many believe the highest allowable precision (given a memory budget) generally produces the lowest error model. However, there are typically many ways to use the same amount of memory. As shown in Figure 1(a), some of these configurations produce much lower error than others! Figure 1(b) shows that no one configuration dominates all others. We might consult previous literature to choose a precision; but this approach fails for new applications.\n\n\nOur goal is to efficiently pick the best low-precision configuration under a memory budget. Efficiency is especially important for resource-constrained practitioners, such as individual users or early-stage startups. To promote efficiency, we use a meta-learning (Lemke et al., 2015; Vanschooren, 2018; Hospedales et al., 2020) approach: we train a small number of cheap very-low-precision models on the dataset to choose the perfect precision. The gains from choosing the right low-precision format can offset the cost of this extra training \u2014 but each precision must be chosen carefully to realize the benefits of low-precision training.\n\n\nWe use ideas from multi-objective optimization to characterize the tradeoff between memory and error and identify the Pareto frontier: the set of non-dominated solutions. Users will want to understand the tradeoff between error and memory so they can determine the resources needed to adequately train a model for a given machine learning task. This tradeoff may also influence the design of application-specific low-precision hardware, with profound implications for the future (Hooker, 2020). For example, among all 99 low-precision configurations we tried, we identified some configurations that are Pareto optimal across many different tasks (listed in Appendix D). These results could help hardware manufacturers decide which precision settings are worth manufacturing.\n\n\nComputing the Pareto frontier by training models for all low-precision configurations is expensive and unnecessary. Cloud computing platforms like Google Cloud and Amazon EC2 charge more for machines with the same CPU and double memory:  $25\\%$  more on Google  $\\mathrm{Cloud}^1$  and  $100\\%$  more on Amazon  $\\mathrm{EC2}^2$  as of mid-September 2021. We use techniques from meta-learning to leverage the information from other low-precision training runs on related datasets. This approach allows us to estimate the Pareto frontier without evaluating all of the low-precision configurations.\n\n\nOur system, Pareto Estimation to Pick the Perfect Precision (PEPPP), has two goals. The first goal is to find the Pareto frontiers of a collection of related tasks, and is called meta-training in the meta-learning literature. Meta-training requires a set of measurements, each collected by training and testing a neural network with a given precision on a dataset. This information can be gathered offline with a relatively large resource budget, or by crowdsourcing amongst the academic or open-source community. Still, it is absurdly expensive to exhaustively evaluate all measurements: that is, every possible low-precision configuration on every task. Instead, we study how to choose a subset of the possible measurements to achieve the best estimate. The second goal we call meta-test: using the information learned on previous tasks, how can we transfer that information to a new task to efficiently estimate its Pareto frontier? This goal corresponds to a resource-constrained individual or startup who wants to determine the best low-precision configuration for a new dataset.\n\n\nBoth meta-training and meta-test rely on matrix completion and active learning techniques to avoid exhaustive search: we make a subset of all possible measurements and predict the rest. We then estimate the Pareto frontier to help the user make an informed choice of the best configuration. We consider two sampling schemes: uniform and non-uniform sampling. Uniform sampling is straightforward. Non-uniform sampling estimates the Pareto frontier more efficiently by making fewer or even no high-memory measurements.\n\n\nTo the best of our knowledge, PEPPP is the first to study the error-memory tradeoff in low-precision training and inference without exhaustive search. Some previous works show the benefit of low-precision arithmetic in significant energy reduction at the cost of a small accuracy decrease (Hashemi et al., 2017), and propose hardware-software codesign frameworks to select the desired model (Langroudi et al., 2019). Many other papers focus on low-precision inference only and use it for model compression (Wu et al., 2018; Cai & Vasconcelos, 2020). Another line of work seeks state-of-the-art (SOTA) performance with low-precision training, using carefully designed learning schedules (Sun et al., 2019; 2020). Our work imagines low-precision as a way to reduce training or deployment costs of customized models trained on proprietary datasets (not SOTA models on ML benchmarks). Even so, we show in Section 4 that our work picks promising models for every memory budget, which enables near-SOTA results on CIFAR-10.\n\n\nFigure 2 shows a flowchart of PEPPP. The rest of this paper is organized as follows. Section 2 introduces notation and terminology. Section 3 describes the main ideas we use to actively sample configurations and approximate Pareto frontiers. Section 4 shows experimental results.\n\n\n![Figure 2: The PEPPP workflow. We begin with a collection of (meta-) training datasets and low precision configurations. In the meta-training phase, we sample dataset-configuration pairs to train, and compute the misclassification error. We use matrix factorization to compute a low dimensional embedding of every configuration. In the meta-test phase, our goal is to pick the perfect precision (within our memory budget) for the meta-test dataset. We compute the memory required for each configuration, and we select a subset of fast, informative configurations to evaluate. By regressing the errors of these configurations on the configuration embeddings, we find an embedding for the meta-test dataset, which we use to predict the error of every other configuration (including more expensive ones) and select the best subject to our memory budget.](images/583df5bab9b72a01338cc0e6beef3e4a0014806c0431755d35b706c81e7b074b.jpg)\n\n\n# 2 NOTATION AND TERMINOLOGY\n\n\nLow-precision formats. We use floating point numbers for low-precision training in this paper.\n\n\nAs an example, Figure 3 shows an 8-bit floating point number with 3 exponent bits and 4 mantissa bits. A specific low-precision representation with a certain number of bits for each part is called a low-precision format. We may use different low-precision formats for the weights, activations, and optimizer. In the case of using two formats to train and represent a\n\n\n![Figure 3: An 8-bit floating point number representing  $(-1)^{\\mathrm{sign}}\\cdot 2^{\\mathrm{exponent} - 7}\\cdot 1.b_{3}b_{2}b_{1}b_{0}$](images/33960eaeb3ba45416a9deefa3a5df94f3025b3da72bbfa261173c0dd729733c0.jpg)\n\n\nneural network (as discussed in Section 3.1), these two formats are called Format  $A$  and  $B$ . A specific combination of these two formats is a hyperparameter setting of the neural network, and we call it a low-precision configuration.\n\n\nMath basics. We define  $[n] = \\{1, \\dots, n\\}$  for a positive integer  $n$ . With a Boolean variable  $\\mathcal{X}$ , the indicator function  $\\mathbb{1}(\\mathcal{X})$  equals 1 if  $\\mathcal{X}$  is true, and 0 otherwise. With a scalar variable  $x$ , we use  $x_{+}$  to denote  $\\max \\{x, 0\\} \\subseteq$  and  $\\subset$  denote subset and strict subset, respectively.\n\n\nLinear algebra. We denote vector and matrix variables respectively by lowercase letters  $(a)$  and capital letters  $(A)$ . All vectors are column vectors. The Euclidean norm of a vector  $a \\in \\mathbb{R}^n$  is  $\\|a\\| := \\sqrt{\\sum_{i=1}^{n} a_i^2}$ . To denote a part of a matrix  $A \\in \\mathbb{R}^{n \\times d}$ , we use a colon to denote the varying dimension:  $A_{i,:}$  and  $A_{:,j}$  (or  $a_j$ ) denote the  $i$ th row and  $j$ th column of  $A$ , respectively, and  $A_{ij}$  denotes the  $(i,j)$ -th entry. With an ordered index set  $\\mathcal{S} = \\{s_1, \\ldots, s_k\\}$  where  $s_1 < \\ldots < s_k \\in [d]$ , we denote  $A_{:\\mathcal{S}} := [A_{:,s_1} \\cdots A_{:,s_k}] \\in \\mathbb{R}^{n \\times k}$ . Given two vectors  $x, y \\in \\mathbb{R}^n$ ,  $x \\preceq y$  means  $x_i \\leq y_i$  for each  $i \\in [n]$ . Given a matrix  $A \\in \\mathbb{R}^{n \\times d}$  and a set of observed indices as  $\\Omega \\subseteq [n] \\times [d]$ , the partially observed matrix  $P_{\\Omega}(A) \\in \\mathbb{R}^{n \\times d}$  has entries  $(P_{\\Omega}(A))_{ij} = A_{ij}$  if  $(i,j) \\in \\Omega$ , and 0 otherwise.\n\n\nPareto frontier. Multi-objective optimization simultaneously minimizes  $n$  costs  $\\{c_i\\}_{i \\in [n]}$ . A feasible point  $c^{(1)} = (c_1^{(1)}, \\ldots, c_n^{(1)})$  is Pareto optimal if for any other feasible point  $c^{(2)} = (c_1^{(2)}, \\ldots, c_n^{(2)})$ ,\n\n\n$c^{(2)} \\preceq c^{(1)}$  implies  $c^{(2)} = c^{(1)}$  (Boyd & Vandenberghe, 2004). The set of Pareto optimal points is the Pareto frontier.\n\n\nTasks, datasets, measurements and evaluations. A task carries out a process (classification, regression, image segmentation, etc.) on a dataset. Given a deep learning model and a dataset, the training and testing of the model on the dataset is called a measurement. In our low-precision context, we evaluate a configuration on a dataset to make a measurement.\n\n\nMeta-learning. Meta-learning transfers knowledge from past (meta-training) tasks to better understand the new (meta-test) task. To evaluate the error of meta-training, we use meta-leave-one-out cross-validation (meta-LOOCV): on a collection of tasks, each time we treat one task as meta-test and the others as meta-training, and average the results over all splits.\n\n\nError matrix and memory matrix. Given a neural network, the errors of different low-precision configurations on meta-training datasets form an error matrix, whose  $(i,j)$ -th entry  $E_{ij}$  is the test error of the  $j$ th configuration on the  $i$ th dataset. To compute the error matrix, we split the  $i$ -th dataset into training and test subsets (or use a given split), train the neural network at the  $j$ -th configuration on the training subset, and evaluate the test error on the test subset. The memory required for each measurement forms a memory matrix  $M$ , which has the same shape as the corresponding error matrix. Example error and memory matrices are shown in Figure 4.\n\n\n<table><tr><td colspan=\"5\">low-precision configurations</td></tr><tr><td></td><td>a</td><td>b</td><td>c</td><td>d</td></tr><tr><td>1</td><td>0.85</td><td>0.71</td><td>0.32</td><td>0.28</td></tr><tr><td>2</td><td>0.80</td><td>0.80</td><td>0.80</td><td>0.69</td></tr><tr><td>3</td><td>0.80</td><td>0.68</td><td>0.46</td><td>0.45</td></tr></table>\n\n\n<table><tr><td colspan=\"5\">low-precision configurations</td></tr><tr><td></td><td>a</td><td>b</td><td>c</td><td>d</td></tr><tr><td rowspan=\"3\">1 datasets</td><td>181</td><td>269</td><td>272</td><td>295</td></tr><tr><td>180</td><td>269</td><td>272</td><td>295</td></tr><tr><td>590</td><td>924</td><td>927</td><td>1032</td></tr><tr><td colspan=\"5\">(b) memory matrix M (MB)</td></tr></table>\n\n\nFigure 4: Example error and memory matrices for some datasets and low-precision configurations. Dataset 1: CIFAR-10, 2: CIFAR-100 (fruit and vegetables), 3: ImageNet-stick (Deng et al., 2009). Configuration Format A (exponent bits, mantissa bits), Format B (exponent bits, mantissa bits). a: (3, 1), (6, 7); b: (3, 4), (7, 7); c: (4, 3), (8, 7); d: (5, 3), (6, 7).\n\n\n# 3 METHODOLOGY\n\n\nPEPPP operates in two phases: meta-training and meta-test. First in meta-training, we learn configuration embeddings by training a neural network of a specific architecture at different low-precision configurations on different datasets. As listed in Appendix A, we study image classification tasks and a range of low-precision formats that vary in the number of bits for the exponent and mantissa for the activations, optimizer, and weights. Our hope is to avoid enumerating every possible configuration since exhaustive search is prohibitive in practice. To this end, we make a few measurements and then predict the other test errors by active learning techniques. We also compute the memory matrix to understand the memory consumption of each measurement.\n\n\nThen in meta-test, we assume that we have completed meta-training, and hence know the configuration embeddings. The meta-test goal is to predict the tradeoff between memory usage and test error on a new (meta-test) dataset. This step corresponds to inference in traditional machine learning; it must be quick and cheap to satisfy the needs of resource-constrained practitioners. To select the configuration that has smallest test error and takes less than the memory limit, we measure a few selected (informative and cheap) configurations on the meta-test dataset, and use the information of their test errors to predict the rest. The problem of choosing measurements is known as the \"cold-start\" problem in the literature on recommender systems. Then we use the model built on the meta-training datasets to estimate values for the other measurements on the meta-test dataset.\n\n\n# 3.1 META-TRAINING\n\n\nOn the  $n$  meta-training datasets, we first theoretically compute the memory needed to evaluate each of the  $d$  low-precision configurations to obtain the full memory matrix  $M \\in \\mathbb{R}^{n \\times d}$ . The total memory consists of memory needed for model weights, activations, and the optimizer (gradients, gradient accumulators, etc.) (Sohoni et al., 2019b). Among these three types, activations typically dominate, as shown in Figure 5. Thus using lower precision for activations drastically reduces memory usage. Additionally, empirical studies (Zhou et al., 2016) have shown that adopting higher precision formats for the optimizer can substantially improve the accuracy of the trained model. Since the optimizer usually requires a relatively small memory, it is often the best to use a higher precision format for this component.\n\n\nThus for each low-precision configuration, it is typical to use a lower precision for network weights and activations, and a higher precision for the optimizer, resulting in combinatorially many choices in total. An example of the memory usage in this low-precision scenario is also shown in Figure 5.\n\n\nIn practice, ML tasks are often correlated: for example, meta-training datasets might be subsets of a large dataset like CIFAR-100 or ImageNet. The ranking of different configurations tends to be similar on similar tasks. For example, we computed the test errors of 99 low-precision configurations on 87 datasets (both listed in Appendix A) to form a performance vector in  $\\mathbb{R}^{99}$  for each dataset. We use the Kendall tau correlation to characterize the alignment between the ranking of errors incurred by different configurations on two datasets: the Kendall tau correlation is 1 if the order is the same and -1 if the order is reversed. As shown in Figure 6(a), similar datasets have larger correlations: for example, datasets with indices 38-87 correspond to ImageNet subproblems such as distinguishing types of fruit or boat. Notice also that some dataset pairs have configuration performance rankings that are negatively correlated. The corresponding error matrix\n\n\n![Figure 5: Memory usage under two training paradigms. Both train a ResNet-18 on CIFAR-10 with batch size 32.](images/2a70afb2bf381683e5dc10e6cdfbcc2a69950b0735c8b1bf9ad622b0a2e2e63c.jpg)\n\n\n$E$  concatenates the performance vector for each dataset. It is not low rank, but its singular values decay rapidly, as shown in Figure 6(b). Hence we expect low rank approximation of this matrix from a few measurements to work well: it is not necessary to measure every configuration.\n\n\nIn the uniform sampling scheme, we sample measurements uniformly at random to obtain a partially observed error matrix  $P_{\\Omega}(E)$ . We then estimate the full error matrix  $E$  using a low rank matrix completion method to form estimate  $\\widehat{E}$ . In this paper, we use SOFTIMPUTE (Mazumder et al., 2010; Hastie et al., 2015) (Algorithm 1, Appendix B). Using the estimated error matrix  $\\widehat{E}$  and computed memory matrix  $M$ , we compute the Pareto frontier for each dataset to understand the (estimated) error-memory tradeoff. Section 4 Figure 10 shows an example.\n\n\nIn the non-uniform sampling scheme, we sample measurements with non-uniform probabilities, and use a weighted variant of SOFTIMPUTE, weighting by the inverse sampling probability, to complete the error matrix (Ma & Chen, 2019). Then we estimate\n\n\n![(a) dataset correlation](images/c05558055a70d7882859e1f1181e4f2652c2ddd1a7a9c80a98fee186323204a7.jpg)\n\n\n![(b) singular value decay Figure 6: Kendall tau correlation of test error of all configurations between all pairs of datasets, and singular value decay of corresponding error matrix. Strong correlations allow PEPP to succeed with a few measurements. Details in Appendix A.](images/c53c7c4f029ae09d435248a221efdf675b8eefc9e6f1432e9ede309d3db7c532.jpg)\n\n\nthe Pareto frontier in the same way as above. Non-uniform sampling is useful to reduce the memory needed for measurements. To this end, we construct a probability matrix  $P$  by entry-wise transformation  $P_{ij} = \\sigma (1 / M_{ij})$ , in which the monotonically increasing  $\\sigma : \\mathbb{R} \\rightarrow [0,1]$  maps inverse memory usage to a sampling probability. In this way, we make more low-memory measurements, and thus reduce the total memory usage.\n\n\n# 3.2 META-TEST\n\n\nHaving estimated the embedding of each configuration, our goal is to quickly compute the error-memory tradeoff on the meta-test dataset, avoiding the exhaustive search of all possible measurements.\n\n\nWe first compute the memory usage of each low-precision configuration on the meta-test dataset. With this information and guided by meta-training, we evaluate only a few (cheap but informative) configurations and predict the rest. Users then finally select the non-dominated configuration with highest allowable memory. An example of the process is shown in Figure 7.\n\n\nPEPPP uses Experiment Design with Matrix Factorization (ED-MF) described below to choose informative configurations. With more time, ED-MF evaluates more configurations, improving our estimates. We may also set a hard cutoff on memory: we do not evaluate low-precision configurations exceeding the memory limit. This setting has been studied in different contexts, and is called active learning or sequential decision making.\n\n\nED-MF picks measurements to minimize the variance of the resulting estimate. Specifically, we factorize the true (or estimated) error matrix  $E \\in \\mathbb{R}^{n \\times d}$  (or  $\\widehat{E}$ ) into its best rank- $k$  approximation, and get dataset embeddings  $X \\in \\mathbb{R}^{n \\times k}$  and configuration embeddings  $Y \\in \\mathbb{R}^{d \\times k}$  as  $E \\approx X^{\\top}Y$  (Fusi et al., 2018; Yang et al., 2019). On a meta-test dataset, we denote the error and memory vectors of the configurations as  $e^{\\mathrm{new}} \\in \\mathbb{R}^d$  and  $m^{\\mathrm{new}} \\in \\mathbb{R}^d$ , respectively. We model the error on the new dataset as  $e^{\\mathrm{new}} = Y^{\\top}x^{\\mathrm{new}} + \\epsilon \\in \\mathbb{R}^d$ , where  $x^{\\mathrm{new}} \\in \\mathbb{R}^k$  is the embedding of the new dataset and  $\\epsilon \\in \\mathbb{R}^d$  accounts for the errors from both measurement and low rank decomposition. We estimate the embedding  $x^{\\mathrm{new}}$  from a few measurements (entries) of  $e^{\\mathrm{new}}$  by least squares. Note that this requires at least  $k$  measurements on the meta-test dataset to make meaningful estimations, in which  $k$  is the rank for matrix factorization.\n\n\nIf  $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$ , the variance of the estimator is  $\\left(\\sum_{j\\in S}y_jy_j^\\top\\right)^{-1}$ , in which  $y_{j}$  is the  $j$ th column of  $Y$ . D-optimal experiment design selects measurements on the new dataset by minimizing (a scalarization of) the variance,\n\n\n![](images/8faf4c5a78abbc7492cb768b16d122c0456f5052d78ef06aa1e94989c80cd019.jpg)\n\n\nminimize  $\\log \\operatorname*{det}\\left(\\sum_{j\\in S}y_jy_j^\\top\\right)^{-1}$  subject to  $|S|\\leq l$ $S\\subseteq [d],$  (1)\n\n\nto find  $S$ , the set of indices of configurations to evaluate. The positive integer  $l$  bounds the number of measurements. Given a memory cap  $m_{\\mathrm{max}}$ , we replace the second constraint above by  $S \\subseteq T$ , in which  $T = \\{j \\in [d] \\mid m_j^{\\mathrm{new}} \\leq m_{\\mathrm{max}}\\}$  is the set of feasible configurations. Since Problem 1 is combinatorially hard, we may either relax it to a convex optimization\n\n\nproblem by allowing decision variables to have non-integer values between 0 and 1, or use a greedy method to incrementally choose measurements (initialized by measurements chosen by the column-pivoted QR decomposition (Gu & Eisenstat, 1996; Golub & Van Loan, 2012)). We compare these two approaches in Appendix C.\n\n\nFigure 7: Meta-test on CIFAR-10. After meta-training on all other datasets in Appendix A Table 2, we use ED-MF to choose six informative measurements (orange squares) with a 275MB memory limit for each measurement on CIFAR-10. Then we estimate test errors of other configurations by ED-MF, and restrict our attention to configurations that we estimate to be non-dominated (red x's). Note some of these are in fact dominated, since we plot true (not estimated) test error! Finally we select the estimated non-dominated configuration with highest allowable memory (blue square).\n\n\nIn Section 4.2, we compare ED-MF with competing techniques and show it works the best to estimate the Pareto frontier and select the final configuration.\n\n\n# 4 EXPERIMENTS AND DISCUSSIONS\n\n\nThe code for PEPPP and experiments is in the anonymous GitHub repository at https://github.com/barterer/lp. We use QPyTorch (Zhang et al., 2019) to simulate low-precision formats on standard hardware. The 87 datasets and 99 low-precision configurations in experiments are listed in Appendix A. The datasets consist of natural and medical images from various domains. Apart from CIFAR-10, the datasets include 20 CIFAR-100 partitions from mutually exclusive subsets, based on the superclass labels. They also include 50 subsets of ImageNet (Deng et al., 2009), which contains over 20,000 classes grouped into multiple major hierarchical categories like fungus and amphibian; each of the 50 datasets come from different hierarchies. Finally, we use 16 datasets from the visual domain benchmark (Wallace & Hariharan, 2020) to increase the diversity of domains.\n\n\nThe 99 low-precision configurations use mixed-precision as described in Section 3.1. Format A (for the activations and weights) uses 5 to 9 bits; Format B (for the optimizer) ranges from 14 to 20 bits. For each, these bits may be split arbitrarily between the exponent and mantissa. Across all configurations, we use ResNet-18, ResNet-34 and VGG (Simonyan & Zisserman, 2014) variants (11, 13, 16, 19) with learning rate 0.001, momentum 0.9, weight decay 0.0005 and batch size 32. Each training uses only 10 epochs as an early stopping strategy (Yao et al., 2007) to prevent overfitting. All the measurements take 35 GPU days on NVIDIA\u00ae GeForce\u00ae RTX 3090.\n\n\nDeveloping a method to select the above hyperparameters at the same time as the low-precision format is an important topic for future research, but not our focus here. In Section 4.3 we demonstrate how PEPP can naturally extend to efficiently select both optimization and low-precision hyperparameters.\n\n\nA number of metrics may be used to evaluate the quality of the obtained Pareto frontier in a multi-objective optimization problem (Wu & Azarm, 2001; Li et al., 2014; Audet et al., 2020): the distance between approximated and true frontiers (convergence), the uniformity of the distances between neighboring Pareto optimal points (uniformity), how well the frontier is covered by the approximated points (spread), etc. In our setting, the memory is accurate and the test error is estimated, so we do not have full control of the uniformity and spread of the 2-dimensional frontiers between test error and memory. As illustrated in Figure 8, we evaluate the quality of our estimated Pareto frontiers by the following two metrics:\n\n\nConvergence (Deb et al., 2002) between the sets of true and estimated Pareto optimal points  $\\mathcal{P}$ ,  $\\widehat{\\mathcal{P}}$  is  $\\frac{1}{|\\widehat{\\mathcal{P}}|} \\sum_{v \\in \\widehat{\\mathcal{P}}} \\mathrm{dist}(v, \\mathcal{P})$ , where the distance between point  $v$  and set  $\\mathcal{P}$  is  $\\mathrm{dist}(v, \\mathcal{P}) = \\min \\{\\|v - w\\| : w \\in \\mathcal{P}\\}$ . This is a surrogate for the distance between Pareto frontiers.\n\n\n![(a) convergence](images/c67b113bc487bd92647d5645c647b4cd6296ee97ad11f3dc882fa67db7fab9ae.jpg)\n\n\n![(b) HyperDiff](images/5e94b37ac20e45690800757e6b30ccee3e03578f362cc5e461902584a1c59ccc.jpg)\n\n\n![Figure 8: Illustration of Pareto frontier metrics. (a) Convergence is the average distance from each estimated Pareto optimal point to its closest true point: average  $(d_{1}, d_{2}, d_{3})$ . (b) HyperDiff is the absolute difference in area of feasible regions given by the true and estimated Pareto optimal points: the shaded area between Pareto frontiers.](images/3ebfaa8e4306bb7a51bb5d7bf7dbcc119f354612bb42fc125bdde32719147b2e.jpg)\n\n\nHypervolume difference (HyperDiff) (Zitzler & Thiele, 1998; Coello & Sierra, 1999; Wu & Azarm, 2001) is the absolute difference between the volumes of solution spaces dominated by the true and estimated Pareto frontiers. This metric improves with better convergence, uniformity, and spread. Its computation requires an upper boundary for each resource.\n\n\nWhen computing these metrics, we normalize the memories by proportionally scaling them to between 0 and 1. To evaluate the matrix completion performance, we use the relative error defined as  $\\|\\widehat{v} - v\\| / \\|v\\|$ , in which  $\\widehat{v}$  is the predicted vector and  $v$  is the true vector.\n\n\n# 4.1 META-TRAINING\n\n\nWe study the effectiveness of uniform and non-uniform sampling schemes. For simplicity, we regard all 87 available datasets as meta-training in this section. In SOFTIMPUte, we use  $\\lambda = 0.1$  (chosen by cross-validation from a logarithmic scale); and we choose a rank 5 approximation, which accounts for  $78\\%$  of the variance in the error matrix (as shown in Figure 6(b)).\n\n\nIn the uniform sampling scheme, we investigate how many samples we need for accurate estimates: we sample the error matrix at a number of different ratios, ranging from  $5\\%$  to  $50\\%$ , and complete the error matrix from each set of sampled measurements. In uniform sampling, the sampling ratio is also the percentage of memory we need to make the measurements in parallel, compared to exhaustive search. Hence we show the relationship between Pareto frontier estimation performance and sampling ratio in Figure 9. We can see the estimates are more accurate at larger sampling ratios, but sampling  $20\\%$  of the entries already suffices for good performance.\n\n\nAs a more intuitive example, Figure 10 shows the error-memory tradeoff on CIFAR-10. Compared to the estimated Pareto optimal points at sampling ratio  $5\\%$ , the ones at  $20\\%$  are closer to the true Pareto frontier (better convergence), lie more evenly (better uniformity) and cover the true Pareto frontier better (better spread), as shown by the respective convergence and HyperDiff values.\n\n\nThe non-uniform sampling scheme has a similar trend and is shown in Appendix E.2.\n\n\n# 4.2 META-LEAVE-ONE-OUT CROSS-VALIDATION (META-LOOCV)\n\n\nNow suppose that we have already collected measurements on the meta-training datasets to form a meta-training error matrix  $E$  (or its low rank approximation  $\\widehat{E}$ ). On the meta-test dataset, PEPP estimates the Pareto frontier by the active learning technique ED-MF. We compare ED-MF with a few other active learning techniques: Random selection with matrix factorization (RANDOM-MF),\n\n\n![(a) relative error](images/bf5ce4196bddea2bcb33a6aef3d9f9551f850f0657c707ae92eb767c1ecf54d8.jpg)\n\n\n![(b) convergence](images/c392a2d0c157d4d7eee8e70fbcfecc08bbc7a1477cf044f598e944a351b05a93.jpg)\n\n\n![(c) HyperDiff](images/07c86db75cf0491e2fbc5b4ca2da6277433f9aba22cd15ee15758bb487dcda77.jpg)\n\n\n![(a) sampling ratio  $5\\%$](images/6149c2b954562b7d9e16df93bb6cee488f27471b7cce98997eb272e9464f9adb.jpg)\n\n\n![(b) sampling ratio  $20\\%$](images/935cb5e34aba90c2fa5e3318d947b2f41232c361c7b16501bad4f88dbd8bc4da.jpg)\n\n\n![Figure 9: Pareto frontier estimation in PEPPP metatraining, with uniform sampling of configurations. The violins show the distribution of the performance on individual datasets, and the error bars (blue) show the range. The red error bars show the standard deviation of the error on CIFAR-10 across 100 random samples of the error matrix. Figure (a) shows the matrix completion error for each dataset; Figure (b) and (c) show the performance of the Pareto frontier estimates. Modest sampling ratios (around 0.1) already yield good performance. Figure 10: Error vs memory on CIFAR-10 with true and estimated Pareto frontiers from uniform sampling in PEPPP meta-training. A  $20\\%$  uniform sample of entries yields a better estimate of the Pareto frontier (convergence 0.03 and HyperDiff 0.02) compared to a  $5\\%$  sample (convergence 0.09 and HyperDiff 0.16).](images/51d898005fece4b8e3c2c6f60d56fa0881786c6819c847c441c2e8e5c1153887.jpg)\n\n\nQR decomposition with column pivoting and matrix factorization (QR-MF) and two Bayesian optimization techniques (BO-MF and BO-FULL), to understand whether the strong assumptions in ED-MF (low rank and Gaussian errors) are a hindrance or a help. An introduction to these techniques can be found in Appendix E.1.\n\n\nWe use rank 3 for matrix factorization:  $Y_{\\cdot ,j} \\in \\mathbb{R}^3$  for each  $j \\in [d]$ . In BO, we tune hyperparameters on a logarithmic scale and choose the RBF kernel with length scale 20, white noise with variance 1, and  $\\xi = 0.01$ . Table 1 shows the meta-LOOCV settings for each acquisition technique. We compare the techniques at a range of number of configurations to measure in each meta-LOOCV split,\n\n\nresembling what practitioners do in hyperparameter tuning: evaluate an informative subset and infer the rest. Setting I is the most basic, Setting IV and VI are the most practical. We only show results of Setting I and IV in the main paper, and defer the rest to Appendix E.\n\n\nIn Setting I, we do meta-LOOCV with the full meta-training error matrix in each split and do not cap the memory for meta-test. This means we evaluate every configuration on the meta-training datasets. We can see from Figure 11(a) and 11(b) that:\n\n- - ED-MF stably outperforms under both metrics, especially with fewer measurements.\n\n- - QR-MF overall matches the performance of ED-MF.\n\n- - BO-MF, BO-FULL and RANDOM-MF underperform ED-MF and QR-MF at lower memory usage, but often match their performance with higher memory.\n\n\nPractitioners may have only collected part of the meta-training performance, and desire or are limited by a memory cap when they do meta-test on the new dataset. In Setting IV, we cap the single-configuration memory usage for meta-test at 816MB, the median memory of all possible measurements across configurations and datasets. Additionally, we uniformly sample  $20\\%$  configurations from the meta-training error matrix in each split. In Figure 11(c) and 11(d), we can see similar trends as Setting I, except that QR-MF is slightly worse.\n\n\nUltimately, users would want to select a configuration that both achieves a small error and takes lower memory than the limit. As shown in Figure 2, PEPPP offers users the predicted Pareto frontier and chooses the non-dominated configuration with the highest allowable memory and hence the lowest error. We compare the acquisition techniques with the \"random high-memory\" baseline that randomly chooses a configuration that takes the highest allowable memory: an approach that follows the \"higher memory, lower error\" intuition. Figure 12 shows an example.\n\n\nTable 1: Meta-LOOCV experiment settings\n\n\n<table><tr><td>meta-training error matrix</td><td colspan=\"2\">memory cap on meta-test?</td></tr><tr><td></td><td>no</td><td>yes</td></tr><tr><td>full</td><td>I</td><td>II</td></tr><tr><td>uniformly sampled</td><td>III</td><td>IV</td></tr><tr><td>non-uniformly sampled</td><td>V</td><td>VI</td></tr></table>\n\n\n![(a) Setting I convergence](images/3ee79fd8a74f8ffaaab0159e33cef2fa4eb0643eb7cf6e3f1278f16e01acb89f.jpg)\n\n\n![(c) Setting IV convergence](images/42693977e879fa3cfff4b6b6cc18e8c5c90ddcf236516152caf01451925860f1.jpg)\n\n\n![(d) Setting IV HyperDiff](images/57fddde2422db373e4984d676196eaf74f026d5c6054a2a6afb4a7bf45651ded.jpg)\n\n\n![](images/ffccd97c3a5b3c40a464fa0dc3c3b36cf02b1c9865136fea6faaca803b37bff8.jpg)\n\n\n![(b) Setting I HyperDiff](images/a7566352a50ab9c0d4de4437675fccb810dd0adc609fdf35607a56002c813165.jpg)\n\n\nOverall, among all approaches and across all memory usage, ED-MF outperforms, especially at a smaller number of measurements. Compared to BO techniques, ED-MF also enjoys the benefit of having less hyperparameters to tune. Although techniques like QR-MF and RANDOM-MF are easier to implement, the additional cost of ED-MF is much smaller than the cost of making measurements: neural network training and testing.\n\n\n# 4.3 TUNING OPTIMIZATION HYPERPARAMETERS\n\n\nThe low-precision networks still underfit after 10 epochs of training. This situation is typical: underfitting due to budget constraints is unfortunately common in deep learning. Luckily, meta-learning the best precision will succeed so long as the validation errors are correctly ordered, even if they all overestimate the error of the corresponding fully trained model. Indeed, our validation errors correlate well with the errors achieved after further training: on CIFAR-\n\n\n10, the Kendall tau correlation between ResNet-18 errors at 10 epochs and 100 epochs is 0.73, shown in Figure 19. The lowest error at 100 epochs is  $9.7\\%$ , only approximately  $2\\%$  higher than SOTA (Bungert et al., 2021).\n\n\n![Figure 11: Pareto frontier estimates in meta-LOOCV Setting I and IV (with a  $20\\%$  meta-training sampling ratio and an 816MB meta-test memory cap). Each error bar is the standard error across datasets. The x axis measures the memory usage relative to exhaustively searching the permissible configurations. ED-MF consistently picks the configurations that give the best PF estimates. RANDOM-MF random highest-memory](images/99d0cc17daed4e499011157ccfefefaa99c0dc2adb0a34cf08ce60d374631ec5.jpg)\n\n\n![BO-MF BO-FULL](images/5f5882e4587b8709b044284e0b637e89cd70dabb2b0fbe108dfff43dcde0008a.jpg)\n\n\n![Figure 12: Relative performance with respect to ED-MF in meta-test Setting IV when making 3 measurements (memory usage  $\\sim 10\\%$ ) on 10 ImageNet partitions. ED-MF outperforms in most cases.](images/3175af61d969fa694d59d06b5d5f9aaa5b3ee2397158d982114d93f3b5a58cc1.jpg)\n\n\nPreviously, we have shown that PEPPP can estimate the error-memory tradeoff and select a promising configuration with other hyperparameters fixed. In practice, users may also want to tune hyperparameters like learning rate to achieve the lowest error. In Appendix E.4, we tune learning rate in addition to precision, and show that the methodology can be used in broader settings of hyperparameter tuning.\n\n\n# 4.4 META-LEARNING ACROSS ARCHITECTURES\n\n\nThe meta-learning in previous sections were conducted on ResNet-18. In Appendix E.5, we show that on 10 ImageNet partitions, PEPP is also capable of estimating the error-memory tradeoff of ResNet-34, VGG-11, VGG-13, VGG-16 and VGG-19 competitively. Moreover, the meta-learning across architectures works better than considering each architecture separately.\n\n\n# 5 CONCLUSION\n\n\nThis paper proposes PEPPP, a meta-learning system to select low-precision configurations that leverages training information from related tasks to efficiently pick the perfect precision given a memory budget. Built on low rank matrix completion with active learning, PEPPP estimates the Pareto frontier between memory usage and model performance to find the best low-precision configuration at each memory level. By reducing the cost of hyperparameter tuning in low-precision training, PEPPP allows practitioners to efficiently train accurate models.\n\n\n# REFERENCES\n\n- Charles Audet, Jean Bigeon, Dominique Cartier, S\u00e9bastien Le Digabel, and Ludovic Salomon. Performance indicators in multiobjective optimization. European journal of operational research, 2020.\n\n- Stephen R Becker, Emmanuel J Cand\u00e8s, and Michael C Grant. Templates for convex cone problems with applications to sparse signal recovery. Mathematical programming computation, 3(3):165, 2011.\n\n- C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer, 2006. ISBN 9780387310732.\n\n- Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n\n- Leon Bungert, Tim Roith, Daniel Tenbrinck, and Martin Burger. A bregman learning framework for sparse neural networks. arXiv preprint arXiv:2105.04319, 2021.\n\n- Zhaowei Cai and Nuno Vasconcelos. Rethinking differentiable search for mixed-precision neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2349-2358, 2020.\n\n- Carlos A Coello Coello and Margarita Reyes Sierra. Multiobjective evolutionary algorithms: classifications, analyses, and new innovations. In Evolutionary Computation. CiteSeer, 1999.\n\n- Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks with low precision multiplications. arXiv preprint arXiv:1412.7024, 2014.\n\n- Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher R Aberger, Kunle Olokotun, and Christopher R\u00e9. High-accuracy low-precision training. arXiv preprint arXiv:1803.03383, 2018.\n\n- Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation, 6(2):182-197, 2002.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255. IEEE, 2009.\n\n- Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.\n\n- Nicolo Fusi, Rishit Sheth, and Melih Elibol. Probabilistic matrix factorization for automated machine learning. In Advances in neural information processing systems, pp. 3348-3357, 2018.\n\n- Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU press, 2012.\n\n- Ming Gu and Stanley C Eisenstat. Efficient algorithms for computing a strong rank-revealing qr factorization. SIAM Journal on Scientific Computing, 17(4):848-869, 1996.\n\n- Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International conference on machine learning, pp. 1737-1746. PMLR, 2015.\n\n- David A Harville. Matrix algebra from a statistician's perspective, 1998.\n\n- Soheil Hashemi, Nicholas Anthony, Hokchhay Tann, R Iris Bahar, and Sherief Reda. Understanding the impact of precision quantization on the accuracy and energy of neural networks. In Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017, pp. 1474-1479. IEEE, 2017.\n\n- Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix completion and low-rank svd via fast alternating least squares. The Journal of Machine Learning Research, 16(1):3367-3402, 2015.\n\n- Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020.\n\n- Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439, 2020.\n\n- Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455-492, 1998.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\n- Hamed F Langroudi, Zachariah Carmichael, David Pastuch, and Dhireesha Kudithipudi. Cheetah: Mixed low-precision hardware & software co-design framework for dnns on the edge. arXiv preprint arXiv:1908.02386, 2019.\n\n- Christiane Lemke, Marcin Budka, and Bogdan Gabrys. Metalearning: a survey of trends and technologies. Artificial intelligence review, 44(1):117-130, 2015.\n\n- Miqing Li, Shengxiang Yang, and Xiaohui Liu. Diversity comparison of Pareto front approximations in many-objective optimization. IEEE Transactions on Cybernetics, 44(12):2568-2584, 2014.\n\n- Wei Ma and George H Chen. Missing not at random in matrix completion: The effectiveness of estimating missingness probabilities under a low nuclear norm assumption. In Advances in Neural Information Processing Systems, volume 32, pp. 14900-14909, 2019.\n\n- Vivek Madan, Mohit Singh, Uthaipon Tantipongpipat, and Weijun Xie. Combinatorial algorithms for optimal design. In Conference on Learning Theory, pp. 2210-2258. PMLR, 2019.\n\n- Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. The Journal of Machine Learning Research, 11:2287-2322, 2010.\n\n- Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.\n\n- Jonas Mo\u010dkus. On Bayesian methods for seeking the extremum. In *Optimization techniques IFIP technical conference*, pp. 400-404. Springer, 1975.\n\n- Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. arXiv preprint arXiv:1907.10597, 2019.\n\n- Jack Sherman and Winifred J Morrison. Adjustment of an inverse matrix corresponding to a change in one element of a given matrix. The Annals of Mathematical Statistics, 21(1):124-127, 1950.\n\n- Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\n- Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczy\u0144ski, Jian Zhang, and Christopher R\u00e9. Low-memory neural network training: A technical report. arXiv preprint arXiv:1904.10631, 2019a.\n\n- Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczy\u0144ski, Jian Zhang, and Christopher R\u00e9. Low-memory neural network training: A technical report, 2019b.\n\n- Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi (Viji) Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. Advances in Neural Information Processing Systems, 33, 2020.\n\n- Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018.\n\n- Bram Wallace and Bharath Hariharan. Extending and analyzing self-supervised learning across domains. In European Conference on Computer Vision, pp. 717-734. Springer, 2020.\n\n- Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt Keutzer. Mixed precision quantization of convnets via differentiable neural architecture search. arXiv preprint arXiv:1812.00090, 2018.\n\n- Jin Wu and Shapour Azarm. Metrics for quality assessment of a multiobjective design optimization solution set. J. Mech. Des., 123(1):18-25, 2001.\n\n- Chengrun Yang, Yuji Akimoto, Dae Won Kim, and Madeleine Udell. Oboe: Collaborative filtering for automl model selection. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1173-1183, 2019.\n\n- Chengrun Yang, Jicong Fan, Ziyang Wu, and Madeleine Udell. Automl pipeline selection: Efficiently navigating the combinatorial space. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1446-1456, 2020.\n\n- Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289-315, 2007.\n\n- Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. The zipml framework for training models with end-to-end low precision: The cans, the cannons, and a little bit of deep learning. arXiv preprint arXiv:1611.05402, 2016.\n\n- Tianyi Zhang, Zhiqiu Lin, Guandao Yang, and Christopher De Sa. QPyTorch: A low-precision arithmetic simulation framework, 2019.\n\n- Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.\n\n- Eckart Zitzler and Lothar Thiele. Multiobjective optimization using evolutionary algorithms\u2014a comparative case study. In International conference on parallel problem solving from nature, pp. 292-301. Springer, 1998.\n\n\nAll the tables in this appendix are shown in the last page.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638432.941356", "msecs": "941.0", "relativeCreated": "29771.85344696045", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Calibration-then-Calculation: A Variance Reduced Metric Framework\n\n# Abstract\nDeep learning has been widely adopted across various fields, but there has been little focus on evaluating the performance of deep learning pipelines. With the increased use of large datasets and complex models, it has become common to run the training process only once and compare the result to previous benchmarks. However, this procedure can lead to imprecise comparisons due to the variance in neural network evaluation metrics. The metric variance comes from the randomness inherent in the training process of deep learning pipelines. Traditional solutions such as running the training process multiple times are usually not feasible in deep learning due to computational limitations. In this paper, we propose a new metric framework, Calibrated Loss, that addresses this issue by reducing the variance in its vanilla counterpart. As a result, the new metric has a higher accuracy to detect effective modeling improvement. Our approach is supported by theoretical justifications and extensive experimental validations in the context of Deep Click-Through Rate Prediction Models and Image Classification Models.\n\n\n# 1 INTRODUCTION\n\n\nThe progress in machine learning is largely influenced by experimental outcomes, particularly in the era of deep learning. Researchers often evaluate the performance of new methods by comparing them to previous benchmark results, in order to demonstrate their superiority. However, it is well known that the performance of deep learning models can vary greatly, even when using the same pipeline (Picard, 2021), where, in this work, we define the pipeline broadly, which includes but is not limited to the selection of feature sets, model architectures, optimization algorithms, initialization schemes, and hyper-parameters. This variability poses a significant challenge to evaluating deep learning methods because it is non-trivial to determine whether the performance gain is a result of the proposed method or simply due to randomness.\n\n\nIn fact, it has been shown that by selecting a fortunate random initialization seed, one can achieve a model performance that is significantly better than average (Picard, 2021). This difference can be substantial enough to be used as a strong argument for publications in selective venues (Picard, 2021).\n\n\nThis issue is exacerbated in industry where the production model performance is hard to get improvement, while there are hundreds of machine learning engineers working on the same model at the same time. The performance gain of a modeling proposal is usually small and within the metric variance, making it difficult to judge the effectiveness of the modeling proposal.\n\n\nTo address this issue, a common approach is to run the training pipeline multiple times and report the average, standard deviation, minimum, and maximum performance scores (Picard, 2021). However, with the rise of large training data and big models, this approach is not always practical due to limited computational resources (Bouthillier & Varouquaux, 2020).\n\n\nIn this work, we take a different approach to solving this problem by designing a new metric that can evaluate proposed deep learning methods more reliably. This new metric has a smaller variance and achieves more accurate comparisons between different deep learning pipelines. We validate our approach in the context of Deep Click-Through Rate Prediction Models and Image Classification Models.\n\n\nOur main contributions are:\n\n- 1) Formulating the deep learning pipeline evaluation problem and proposing to tackle it by designing new metrics.\n\n- 2) Proposing a new metric framework, Calibrated Loss, which can mitigate the above deep learning pipeline evaluation issue.\n\n- 3) Conducting extensive experiments to demonstrate the effectiveness of the proposed metric, using synthetic dataset, ads click dataset and image classification dataset.\n\n- 4) Providing theoretical guarantees under a particular setting of linear regression that the proposed metric has a smaller variance than its vanilla counterpart.\n\n\n# 2 PRELIMINARIES AND PROBLEM SETTING\n\n\nIn this work, we are examining the standard supervised learning setting, where we assume that the training data and test data are randomly drawn from an unknown distribution in an i.i.d. manner, denoted as  $\\mathcal{D}$ .\n\n\nOur goal is to develop a good pipeline that maps from a training distribution to a possibly random model,  $h \\in \\mathcal{H}$ , that generalizes well during the test time. As we mentioned in Introduction, the pipeline includes the whole procedures of training a model, including the selection of model architectures, optimization algorithms, initialization schemes, and hyper-parameters. Model performance is evaluated by a metric,  $e$ , and thus the expected performance of a model  $h$  during the test time is\n\n\n$$\n\n$$\nR _ {e} (h) = \\mathbb {E} _ {\\mathcal {D}} [ e (h (X), Y) | h ]. \\tag {1}\n$$\n\n$$\n\n\nIn practice,  $R_{e}(h)$  is estimated by finite-sample average on the test dataset  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$ . That is,\n\n\n$$\n\n$$\n\\hat {R} _ {e} (h, \\hat {\\mathcal {D}} _ {\\text {t e s t}}) = \\frac {1}{| \\hat {\\mathcal {D}} _ {\\text {t e s t}} |} \\sum_ {(x, y) \\in \\hat {\\mathcal {D}} _ {\\text {t e s t}}} e (h (x), y). \\tag {2}\n$$\n\n$$\n\n\nIt's worth noting that the expected risk,  $R_{e}(h)$ , is a random variable, as  $h$  is random and it depends on a specific model that is produced by the underlying deep learning pipeline. The output model is random due to data randomness from the sample collection and intrinsic randomness during the training process in the deep learning pipeline, such as data order and randomness from stochastic gradient descent. Therefore, a proper evaluation and comparison of different deep learning pipelines should take into account the distribution of  $R_{e}(h)$  (Bouthillier et al., 2021). It's also important to note that the term \"deep learning pipeline\" in this context is general, as we consider different model configurations (e.g. different model hyperparameters) as different \"deep learning pipelines\", even though they may belong to the same model class.\n\n\nTo compare the performance of different deep learning pipelines, we should compare the distribution of  $R_{e}(h,\\mathcal{D}_{\\mathrm{test}})$ . When comparing two pipelines,  $A$  and  $B$ , we say that pipeline  $A$  is better than pipeline  $B$  with respect to metric  $e$  if and only if the probability that pipeline  $A$  produces a better model (i.e. smaller risk), measured by the metric  $e$ , is greater than 0.5. This is represented by the inequality:\n\n\n$$\n\n$$\nP \\left(R _ {e} \\left(h _ {A}\\right) <   R _ {e} \\left(h _ {B}\\right)\\right) > 0. 5, \\tag {3}\n$$\n\n$$\n\n\nwhere  $h_A$  and  $h_B$  are random variables representing the output models produced by pipeline  $A$  and  $B$  respectively.\n\n\nOur objective is to compare the performance of two pipelines,  $A$  and  $B$ , with respect to the metric  $e$  by running the training pipeline only once. Ideally, to estimate  $P(R_{e}(h_{A}) < R_{e}(h_{B}))$ , we could use the Monte Carlo method, but this requires a huge amount of computational resources which is not realistic. In this work, we propose a different approach: we aim to come up with an alternative metric that has roughly the same mean but a smaller variance, i.e., for a new metric  $e_1$ , we would like to have\n\n\n$$\n\n$$\n\\mathbb {E} \\left[ R _ {e _ {1}} (h) \\right] \\approx \\mathbb {E} \\left[ R _ {e} (h) \\right] \\text {a n d} v a r \\left(R _ {e _ {1}} (h)\\right) <   v a r \\left(R _ {e} (h)\\right),\n$$\n\n$$\n\n\nwhere the randomness is from the pipeline producing  $h$ . As a result, the new metric is able to compare the performance of pipelines  $A$  and  $B$  with limited computational resources more accurately.\n\n\nAssuming that pipeline  $A$  is better than  $B$  with respect to the metric  $e$  (i.e. pipeline  $A$  is more likely to produce a better model than pipeline  $B$  in the ground truth if measured by metric  $e$ ), we say that a metric  $e_1$  is better than  $e_2$  with respect to metric  $e$  if and only if the probability that pipeline  $A$  produces a better model than pipeline  $B$  measured by metric  $e_1$  is greater than the probability measured by metric  $e_2$ . This is represented by the inequality:\n\n\n$$\n\n$$\nP \\left(R _ {e _ {1}} \\left(h _ {A}\\right) <   R _ {e _ {1}} \\left(h _ {B}\\right)\\right) > P \\left(R _ {e _ {2}} \\left(h _ {A}\\right) <   R _ {e _ {2}} \\left(h _ {B}\\right)\\right) \\tag {4}\n$$\n\n$$\n\n\nIn other words, using metric  $e_1$  is more likely to accurately detect that pipeline  $A$  is better than pipeline  $B$ , which aligns with the ground truth. Here, we allow for a general form of the risk function, which may not admit the expectation form; i.e.,  $R_{e_1}(h)$  may not necessarily have the form  $\\mathbb{E}_{\\mathcal{D}}[e_1(h(X),Y)]$ .\n\n\nHere, we assume without loss of generality that pipeline  $A$  is better than  $B$  with respect to the metric  $e$ . We define the accuracy of a metric  $\\bar{e}$  with respect to metric  $e$  and pipeline  $A$  and  $B$  as:\n\n\n$$\n\n$$\n\\operatorname {A c c} (\\bar {e}) \\triangleq P \\left(R _ {\\bar {e}} \\left(h _ {A}\\right) <   R _ {\\bar {e}} \\left(h _ {B}\\right)\\right). \\tag {5}\n$$\n\n$$\n\n\nOur goal is to find a metric  $\\bar{e}$  associated with the risk function  $R_{\\bar{e}}$  that has higher accuracy than the original metric  $e$  for a wide range of pipelines  $A$  and  $B$ . In the next section, we will present a new metric framework, Calibrated Loss. In the context of Deep Click-Through Rate Prediction Models, a special instance of Calibrated Loss, Calibrated Log Loss achieves higher accuracy than the vanilla Log Loss. The intuition is that the bias in the function  $h$  is always volatile and carries on a large amount of randomness. Calibrating the bias will usually not change the comparison between two pipelines but can reduce the randomness. In Section 4, we will present a theoretical analysis that justifies this intuition by showing that our new metric has a smaller variance under the linear regression setting. Through extensive experiments in Section 5, we will show that Calibrated Log Loss achieves higher accuracy than Log Loss for a wide range of pipelines, including those with different batch sizes, number of features, model architectures, regularization weights, model sizes, etc.\n\n\n# 3 CALIBRATED LOSS FRAMEWORK\n\n\nCalibrated Log Loss (Binary Classification) In the field of Deep Click-Through Rate Prediction Models, it is common for models to overfit when trained for more than one epoch (Zhou et al., 2018; Zhang et al., 2022). As a result, models are often only trained for a single epoch in practice (Zhang et al., 2022), leaving it uncertain if the model has been fully optimized. This leads to the volatility of the bias term in the final layer of neural networks, creating additional randomness. To address this issue, we propose the following risk function:\n\n\n$$\n\n$$\nR _ {e _ {1}} (h) = \\min _ {c} \\mathbb {E} _ {\\mathcal {D}} [ Y \\log (h ^ {c} (X)) + (1 - Y) \\log (1 - h ^ {c} (X)) | h ],\n$$\n\n$$\n\n\nwhere  $h^c (X) = (1 + e^{-\\log \\mathrm{it}(h(X)) + c})^{-1}$  and  $\\mathrm{logit}(p) = \\log (p / (1 - p))$ .\n\n\nTo execute the aforementioned procedure on a finite-sample test set, we split the test data  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$  into two parts: a test-validation dataset  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and a remaining test dataset  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ . By using the test-validation dataset  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ , we are able to correct the bias term, and then calculate the Log Loss using  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$  with the bias-adjusted predictions. This is referred to as Calibrated Log Loss. The calculation procedure is outlined in Algorithm 1.\n\n\nMathematically speaking, we define bias-adjusted predictions as:  $q_{i} = h^{c}(x_{i})$  for  $x_{i}$  in the test set where  $c$  is the bias-adjusted term we are optimizing.\n\n\nTo optimize  $c$ , the following optimization program is solved, which is the log loss between bias-adjusted predictions  $q_{i}$  and labels  $y_{i}$ :\n\n\n$$\n\n$$\n\\min  _ {c} \\left\\{- \\sum_ {\\substack {(x, y) \\in \\\\ \\hat {\\mathcal {D}} _ {\\text {val - test}}}} (y \\log \\left(h ^ {c} (x)\\right) + (1 - y) \\log \\left(1 - h ^ {c} (x)\\right)) \\right\\}. \\tag{6}\n$$\n\n$$\n\n\nIt can be easily shown that, after optimization, the bias-adjusted predictions  $q_{i}$  are well-calibrated in  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ , meaning that  $\\sum_{i\\in \\hat{\\mathcal{D}}_{\\mathrm{val - test}}}q_i = \\sum_{i\\in \\hat{\\mathcal{D}}_{\\mathrm{val - test}}}y_i$ .\n\n\nLet  $c^*$  be the minimizer of the optimization problem (6). The final risk and metrics are\n\n\n$$\n\n$$\n\\hat{R}_{e_{1}}(h,\\hat{\\mathcal{D}}_{\\text{test}}) = \\frac{1}{|\\hat{\\mathcal{D}}_{\\text{remaining - test}}|}\\sum_{\\substack{(x,y)\\in \\\\ \\hat{\\mathcal{D}}_{\\text{remaining - test}}}}e_{1}(h(x),y),\n$$\n\n$$\n\n\nand\n\n\n$$\n\n$$\ne _ {1} (h (x), y) = y \\log (h ^ {c ^ {*}} (x)) + (1 - y) \\log (1 - h ^ {c ^ {*}} (x)).\n$$\n\n$$\n\n\nExplanations The optimization problem (6) corrects the bias term of original predictions  $h(x)$  using test-validation dataset  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ . The bias-adjusted predictions  $h^{c^*}(x)$  is guaranteed to be well-calibrated in  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ , hence the name Calibrated Log Loss.\n\n\n# ALGORITHM 1 CALCULATE CALIBRATED LOG LOSS\n\n- 1: Input: Model  $M$ , labeled test data  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$\n\n- 2: Output: Calibrated Log Loss:  $\\dot{R}_{e_1}(h, \\mathcal{D}_{\\mathrm{test}})$ .\n\n- 3: Partition  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$  into  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ .\n\n- 4: Compute model predictions on  $\\hat{D}_{\\mathrm{val - test}}$  and  $\\hat{D}_{\\mathrm{remaining - test}}$ , and obtain the model predictions  $p_i^{\\mathrm{val - test}}$  and  $p_i^{\\mathrm{remaining - test}}$ .\n\n- 5: Solve the optimization problem (6) using  $p_i^{\\mathrm{val - test}}$  and  $y_{i}^{\\mathrm{val - test}}$  and obtain the learned bias term  $c^{*}$ .\n\n- 6: Calculate bias-adjusted predictions  $q_{i}^{\\mathrm{remaining - test}}$  using formula  $q_{i} = h^{c^{*}}(x_{i})$ .\n\n- 7: Calculate the Calibrated Log Loss  $\\hat{R}_{e_1}(h,\\hat{\\mathcal{D}}_{\\mathrm{test}})$  as the Log Loss of  $q_{i}^{\\mathrm{remaining - test}}$  and  $y_{i}^{\\mathrm{remaining - test}}$ .\n\n\nGeneralization to Multiclass Classification Instead of optimizing problem (6) to calculate calibrated predictions, we use \"Temperature Scaling\" method proposed in (Guo et al., 2017a). The rest are exactly the same.\n\n\nGeneralization to Quadratic Loss Calibrated Quadratic Loss is calculated in a similar manner as Calibrated Log Loss, i.e. first perform calibration on  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and calculate bias-adjusted predictions on  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ . Here, we define the Quadratic Loss and Calibrated Quadratic Loss:\n\n\n$$\n\n$$\ne (h (x), y) = (y - h (x)) ^ {2}, \\text {a n d}\n$$\n\n$$\n\n\n$$\n\n$$\ne _ {1} (h (x), y) = (y - h (x) - (\\mathbb {E} _ {\\mathcal {D}} [ Y ] - \\mathbb {E} _ {\\mathcal {D}} [ h (X) | h ]) ^ {2}).\n$$\n\n$$\n\n\n# 4 THEORY ON LINEAR REGRESSION\n\n\nIn this section, we provide theoretical justification that our new metric has a smaller variance than its vanilla counterpart under Linear Regression setting, where the randomness only comes from the data randomness. We choose to provide a theoretical guarantee under Linear Regression due to its simplicity. We empirically verify our method's performance under Logistic Regression and Neural Networks in the next section. Note that in Linear Regression, Quadratic Loss is used instead of Log Loss. As a result, in our theory, we compare the variance of Calibrated Quadratic Loss with vanilla Quadratic Loss.\n\n\nTheorem 4.1. Suppose that the features  $X \\in \\mathbb{R}^d$  and the label  $Y$  are distributed jointly Gaussian. We consider linear regression  $h(x) = \\beta^\\top x + \\alpha$ . Let  $\\hat{\\beta}_n$  be the coefficient learned from the training data with sample size  $n$ . Then, we have\n\n\n$$\n\n$$\n\\left(1 + \\frac {1}{n}\\right) \\mathbb {E} [ e _ {1} (h (X), Y) | \\hat {\\beta} _ {n} ] = \\mathbb {E} [ e (h (X), Y) | \\hat {\\beta} _ {n} ],\n$$\n\n$$\n\n\nwhere the expectation is taken over the randomness over both the training and test samples.\n\n\nLet  $\\hat{\\alpha}_n$  be the learned intercept. Note that the original risk and the calibrated risk are\n\n\n$$\n\n$$\n\\begin{array}{l} R _ {e} (h) = \\mathbb {E} \\left[ e (h (X), Y) | \\hat {\\beta} _ {n}, \\hat {\\alpha} _ {n} \\right], \\text {a n d} \\\\ R _ {e _ {1}} (h) = \\mathbb {E} [ e _ {1} (h (X), Y) | \\hat {\\beta} _ {n}, \\hat {\\alpha} _ {n} ] = \\mathbb {E} [ e _ {1} (h (X), Y) | \\hat {\\beta} _ {n} ]. \\\\ \\end{array}\n$$\n\n$$\n\n\nTherefore, Theorem 4.1 implies that\n\n\n$$\n\n$$\n(1 + 1 / n) \\mathbb {E} \\left[ R _ {e _ {1}} (h) \\right] = \\mathbb {E} \\left[ R _ {e} (h) \\right].\n$$\n\n$$\n\n\nFurthermore, to make  $e$  and  $e_1$  comparable, we should scale  $e_1$  to  $(1 + 1/n)e_1$ . We demonstrate that after scaling,  $(1 + 1/n)R_{e_1}(h)$  has a smaller variance than  $R_e(h)$  in the next corollary. In practice, as  $(1 + 1/n)$  is a constant as long as the training sample size is fixed, we can directly compare two pipelines using  $R_{e_1}(h)$ .\n\n\nCorollary 4.2. Suppose that  $h_1(x)$  and  $h_2(x)$  are two different learned linear functions in different feature sets. Then, we have\n\n\n$$\n\n$$\n\\mathbb {E} \\left[ R _ {e} \\left(h _ {1}\\right) \\right] = \\mathbb {E} \\left[ R _ {e} \\left(h _ {2}\\right) \\right] \\Leftrightarrow \\mathbb {E} \\left[ R _ {e _ {1}} \\left(h _ {1}\\right) \\right] = \\mathbb {E} \\left[ R _ {e _ {1}} \\left(h _ {2}\\right) \\right] \\tag {7}\n$$\n\n$$\n\n\nand\n\n\n$$\n\n$$\n\\operatorname {v a r} \\left(\\left(1 + 1 / n\\right) R _ {e _ {1}} (h)\\right) <   \\operatorname {v a r} \\left(R _ {e} (h)\\right)\n$$\n\n$$\n\n\nfor any  $h$  learned from linear regression.\n\n\nCorollary 4.2 indicates that Calibrated Quadratic Loss has a smaller variance than vanilla Quadratic Loss without changing the mean after appropriate scaling. Note that smaller variance and higher accuracy (Inequality 4) are highly correlated under mild conditions, but smaller variance alone does not guarantee higher accuracy. In the next section, we will empirically demonstrate that the new metric has a smaller variance and achieves higher accuracy. All proofs can be found in Appendix A.\n\n\n# 5 EXPERIMENT RESULTS\n\n\n# 5.1 ESTIMATION OF ACCURACY\n\n\nRecall that accuracy of a metric  $\\bar{e}$  is defined as:\n\n\n$$\n\n$$\n\\operatorname {A c c} (\\bar {e}) \\triangleq P \\left(R _ {\\bar {e}} \\left(h _ {A}\\right) <   R _ {\\bar {e}} \\left(h _ {B}\\right)\\right). \\tag {8}\n$$\n\n$$\n\n\nTo get an estimation of  $\\mathrm{Acc}(\\bar{e})$ , we run pipelines  $A$  and  $B$  for  $m$  times, obtaining models  $h_{A_i}$  and  $h_{B_i}$  for  $i \\in [m]$ .  $\\mathrm{Acc}(\\bar{e})$  can be estimated as:\n\n\n$$\n\n$$\n\\widehat {\\operatorname {A c c}} (\\bar {e}) = \\frac {1}{m ^ {2}} \\sum_ {(i, j)} \\mathbb {1} \\left(\\hat {R} _ {\\bar {e}} \\left(h _ {A _ {i}}, \\hat {\\mathcal {D}} _ {\\text {t e s t}}\\right) <   \\hat {R} _ {\\bar {e}} \\left(h _ {B _ {j}}, \\hat {\\mathcal {D}} _ {\\text {t e s t}}\\right)\\right) \\tag {9}\n$$\n\n$$\n\n\n$\\widehat{\\mathrm{Acc}} (\\bar{e})$  is an unbiased estimator of  $\\mathrm{Acc}(\\bar{e})$ , and in the experiments below, we report  $\\widehat{\\mathrm{Acc}} (\\bar{e})$  as our accuracy metric. In all the tables in this section, without loss of generality, we write the tables as pipeline A is better than pipeline B in the sense of  $P(R_{e}(h_{A}) < R_{e}(h_{B})) > 0.5$ .\n\n\n# 5.2 SYNTHETIC DATA\n\n\nIn Appendix B, we consider a linear regression model to give empirical evidence to support our theory. We further consider logistic regression model to demonstrate the effectiveness of Calibrated Log Loss in synthetic data setting. All the details and results can be found in Appendix B.\n\n\n# 5.3 AVAZU CTR PREDICTION DATASET\n\n\nDataset The Avazu CTR Prediction dataset  ${}^{1}$  is a common benchmark dataset for CTR predictions. Due to computational constraints in our experiments, we use the first 10 million samples, shuffle the dataset randomly, and split the whole dataset into  ${80}\\% {\\widehat{D}}_{\\text{train }},2\\% {\\widehat{D}}_{\\text{val - test }}$  and  ${18}\\% {\\widehat{D}}_{\\text{remaining - test }}$  .\n\n\nBase Model We use the xDeepFM model (Lian et al., 2018) open sourced in Shen (2017) as our base model. We primarily conduct experiments using xDeepFM models, including hyperparameter related experiments and feature related experiments. To demonstrate our new metric can also handle comparisons between different model architectures, we also conduct experiments using DCN (Wang et al., 2017), DeepFM (Guo et al., 2017b), FNN (Zhang et al., 2016), and DCNMix (Wang et al., 2021).\n\n\nExperiment Details We consider neural networks with different architectures, different training methods, different hyper-parameters, and different levels of regularization as different pipelines. Such comparisons represent common practices for research and development in both industry and academia. For each pipeline, we train the model 60 times with different initialization seeds and data orders to calculate  $\\widehat{\\mathrm{Acc}} (\\bar{e})$ . Note that we use \"Log Loss\" as our ground truth metric to determine the performance rank of different pipelines. Due to computational constraints, we cannot afford to run the experiments for multiple rounds. Instead, we run the experiments for one round and report accuracy. Note that in the neural network experiments, we do not re-sample the training data each time, as there is intrinsic randomness in the neural network training process. This is the main difference from the Linear Regression and Logistic Regression experiments.\n\n\nPipelines with Different Number of Features In this set of experiments, for pipeline  $A$ , we use all the features available. For pipeline  $B$ , we remove some informative features. We tested the removal of 6 dense features and 1 sparse features respectively.\n\n\nTable 1: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (features)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>Baseline</td><td>remove dense</td><td>81.8%</td><td>88.8%</td></tr><tr><td>Baseline</td><td>remove sparse</td><td>78.6%</td><td>85.9%</td></tr></table>\n\n\nTable 2: Mean and Standard Deviation of Log Loss and Calibrated Log Loss under Neural Networks (features)\n\n\n<table><tr><td>Pipeline</td><td>Log Loss Mean</td><td>Calibrated Log Loss Mean</td><td>Log Loss Std</td><td>Calibrated Log Loss Std</td></tr><tr><td>dense</td><td>0.37408</td><td>0.37403</td><td>0.00047</td><td>0.00038</td></tr><tr><td>sparse</td><td>0.37404</td><td>0.37398</td><td>0.0005</td><td>0.00042</td></tr></table>\n\n\nFrom the result in Table 1, we can clearly see that Calibrated Log Loss has a higher accuracy, indicating its effectiveness when comparing the performance of pipelines with different features.\n\n\nFrom the result in Table 2, we can see that Calibrated Log Loss has a smaller standard deviation (16% - 19% smaller) while the mean of Log Loss and Calibrated Log Loss is almost on par (within 0.02% difference).\n\n\nPipelines with Different Model Architectures In this set of experiments, for pipeline  $A$ , we use one model architecture. For pipeline  $B$ , we use another model architecture. We tested a variety of different model architectures, including DCN (Wang et al., 2017), DeepFM (Guo et al., 2017b), FNN (Zhang et al., 2016), and DCNMix (Wang et al., 2021).\n\n\nTable 3: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (model architectures)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>DCN</td><td>DCNMix</td><td>64.4%</td><td>71.5%</td></tr><tr><td>DeepFM</td><td>DCN</td><td>77.2%</td><td>83.9%</td></tr><tr><td>DeepFM</td><td>FNN</td><td>76.9%</td><td>79.9%</td></tr><tr><td>FNN</td><td>DCNMix</td><td>61.5%</td><td>72.0%</td></tr><tr><td>DeepFM</td><td>DCNMix</td><td>84.8%</td><td>93.4%</td></tr></table>\n\n\nFrom the result in Table 3, we can clearly see that Calibrated Log Loss has higher accuracy, again indicating its effectiveness when comparing the performance of pipelines with different model architectures. In Table 4, we report the mean and standard deviation of Log Loss and Calibrated Log Loss, consistent with previous results.\n\n\nTable 4: Mean and Standard Deviation of Log Loss and Calibrated Log Loss under Neural Networks (model architectures)\n\n\n<table><tr><td>Pipeline</td><td>Log Loss Mean</td><td>Calibrated Log Loss Mean</td><td>Log Loss Std</td><td>Calibrated Log Loss Std</td></tr><tr><td>DCN</td><td>0.38021</td><td>0.38011</td><td>0.00044</td><td>0.00033</td></tr><tr><td>DeepFM</td><td>0.37971</td><td>0.3796</td><td>0.00059</td><td>0.00037</td></tr><tr><td>FNN</td><td>0.38029</td><td>0.38006</td><td>0.00064</td><td>0.0004</td></tr><tr><td>DCNMix</td><td>0.38046</td><td>0.38037</td><td>0.00048</td><td>0.00034</td></tr></table>\n\n\nPipelines with Different Model Hyperparameters In this set of experiments, we compare pipelines with different model hyperparameters, including neural network layer size, Batch Normalization (BN) (Ioffe & Szegedy, 2015), Dropout (Srivastava et al., 2014), and regularization weight.\n\n\nIn the first experiment, we compare a pipeline using a baseline model size with a pipeline using a smaller model size. In the second experiment, we compare a pipeline using Batch Normalization with a pipeline not using Batch Normalization. In the third experiment, we compare a pipeline not using Dropout with a pipeline using Dropout with dropout probability 0.7. In the fourth experiment, we compare a pipeline not using regularization with a pipeline using L2 regularization with regularization weight  $10^{-6}$ .\n\n\nTable 5: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (hyperparameters)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>Baseline Size</td><td>Smaller Size</td><td>69.6%</td><td>73.6%</td></tr><tr><td>BN</td><td>no BN</td><td>80.2%</td><td>89.7%</td></tr><tr><td>no Dropout</td><td>p = 0.7</td><td>95.0%</td><td>99.3%</td></tr><tr><td>no regularization</td><td>10-6</td><td>95.2%</td><td>98.8%</td></tr></table>\n\n\n![(a) Log Loss Plot Figure 1: Batch Normalization Experiment](images/10c97d6bc5fd44ae18ddeda3b3cc0c850b6042abd2d92567fba791b85eab631f.jpg)\n\n\n![(b) Calibrated Log Loss Plot](images/1419d9a784c7f0b9c8b5b475f4d8c9de29ac93f7238bb94c371aff35acb4c744.jpg)\n\n\nFrom figure 1, we can see that by using Calibrated Log Loss, it becomes easier to separate pipeline using Batch Normalization from pipeline without Batch Normalization.\n\n\nFrom the result in Table 5, we can see that Calibrated Log Loss has a higher accuracy regardless of the hyperparameters we are tuning, indicating its effectiveness when comparing the performance of pipelines with different hyperparameters, which is a very common task in Deep Learning. In Appendix B.3 Table 12, we report the mean and standard deviation of Log Loss and Calibrated Log Loss, again consistent with previous results.\n\n\nPipelines with Different Levels of Regularization In this set of experiments, we take a closer look at one hyperparameter we conduct in the previous section: regularization weight. For pipeline  $A$ , we use the baseline model. For pipeline  $B$ , we use different L2 regularization weights.\n\n\nFrom the result in Table 6, we can see that Calibrated Log Loss has a higher accuracy across all different regularization weights, indicating its robustness to different values of regularization weight. As we increase the regularization weight in pipeline  $B$ , the accuracies of both metrics increase. This is because pipeline  $A$  and  $B$  differ more with larger regularization weight, making performance comparison easier.\n\n\nTable 6: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (regularization weight)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>0</td><td>3*10-7</td><td>63.2%</td><td>69.3%</td></tr><tr><td>0</td><td>5*10-7</td><td>82.2%</td><td>88.2%</td></tr><tr><td>0</td><td>7*10-7</td><td>86.6%</td><td>92.4%</td></tr><tr><td>0</td><td>10-6</td><td>95.2%</td><td>98.8%</td></tr><tr><td>0</td><td>2*10-6</td><td>98.8%</td><td>100.0%</td></tr></table>\n\n\nFrom the result in Appendix B.3 Table 13, we can see that Calibrated Log Loss has a much smaller standard deviation (15% - 40% smaller) than Log Loss while the mean of Log Loss and Calibrated Log Loss is almost on par (within 0.05% difference), again consistent with previous results.\n\n\n# 5.4 CIFAR10\n\n\nDataset We use the default CIFAR10 train dataset as  $\\hat{\\mathcal{D}}_{\\mathrm{train}}$ . We split CIFAR10 test dataset into  $20\\%$ $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and  $80\\%$ $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ .\n\n\nBase Model We use the ResNet18 model (He et al., 2016) as our base model. We compare ResNet18 with other configurations of ResNet and DenseNet (Huang et al., 2017). We used the open source implementations  ${}^{2}$  to conduct all the experiments.\n\n\nExperiment Details Same as Avazu CTR experiments, for each pipeline, we train the model 60 times with different initialization seeds and data orders to calculate  $\\widehat{\\mathrm{Acc}} (\\bar{e})$ . Note that we use \"Classification Accuracy\" as our ground truth metric to determine the performance rank of different pipelines. We train the model for 200 epochs. We run the experiments for one round and report accuracy. Note that in the neural network experiments, we do not re-sample the training data each time, as there is intrinsic randomness in the neural network training process.\n\n\nCalibration Stage Predicting the image class in CIFAR10 is a Multiclass classification task, which is different from the binary classification task in the CTR prediction context. We use the \"Temperature Scaling\" (Guo et al., 2017a) as our calibration method.\n\n\nExperiment Results From the result in Table 7, we can see that Calibrated Log Loss has a higher\n\n\nTable 7: Accuracy of Log Loss and Calibrated Log Loss (CIFAR10)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>resnet18</td><td>resnet101</td><td>5.6%</td><td>74.7%</td></tr><tr><td>resnet18</td><td>resnet152</td><td>8.1%</td><td>87.4%</td></tr><tr><td>resnet18</td><td>DenseNet121</td><td>51.5%</td><td>85.2%</td></tr><tr><td>resnet18</td><td>resnet34</td><td>13.5%</td><td>56.7%</td></tr></table>\n\n\naccuracy, indicating its effectiveness when comparing the performance of pipelines using different image classification models.\n\n\nWe can also see that sometimes the accuracy of Log Loss is below  $50\\%$  (recall that Pipeline A is better than Pipeline B), which indicates that Log Loss and Classification Accuracy will give different results when comparing the performance of pipeline  $A$  and pipeline  $B$  (e.g. pipeline A performs better if using metric Log Loss while pipeline B performs better if using metric Classification Accuracy). This metric inconsistency can be mitigated by Calibrated Log Loss. This phenomenon indicates that some models may have bad calibration (and hence bad Log Loss) while their performances are actually good if measured by accuracy.\n\n\n# 6 RELATED WORK\n\n\nIn recommendation systems, recommending proper items to users is a fundamental task. In order to do accurate recommendations, it is essential to build Click-Through Rate (CTR) prediction models to rank the items and achieve business goals. Prior to Deep Learning Era, traditional machine learning models (Friedman, 2001; Koren et al., 2009; Rendle, 2010; Desrosiers & Karypis, 2010; Canini et al., 2012) like logistic regression, boosted decision trees and factorization machines are commonly used to build CTR models. In the Deep Learning Era, not surprisingly, CTR prediction models have been transitioned to deep models as well (Cheng et al., 2016; Guo et al., 2017b; Covington et al., 2016; Wang et al., 2017; Zhou et al., 2018; Naumov et al., 2019).\n\n\nA typical Deep Click-Through Rate (CTR) prediction model consists of embedding layers and multilayer perceptron (MLP) layers on top of embedding layers. Embedding layers transform raw discrete IDs to low dimensional vectors. Following embedding layers, MLP layers then learn the interactions of different features represented the low dimensional vectors, and finally output the final prediction (i.e. CTR). Currently, Deep Click-Through Rate (CTR) prediction models achieve state-of-the-art performance for CTR tasks, and are commonly used in industries to power various recommendation tasks, including personalized ads recommendations, content recommendations, etc.\n\n\nThere are a number of commonly used metrics (Yi et al., 2013) to evaluate the performance of CTR prediction models. Area Under the ROC Curve (AUC) (Fawcett, 2006; 2004) along with its variants Zhu et al. (2017) and Log Loss are the most common metrics. For example, He et al. (2014); Wang et al. (2017); McMahan et al. (2013) use Log Loss as their core metric, while Zhou et al. (2018); McMahan et al. (2013) use AUC as their core metric. However, AUC has been criticized for not taking into account the predictive probability (Yi et al., 2013). Log Loss, on the other hand, takes predictive probability into account. As a result, in applications where predictions are expected to be calibrated (e.g. Ads Recommendation Systems), Log Loss is preferred.\n\n\n# 7 CONCLUSION AND DISCUSSION\n\n\n# CONCLUSION\n\n\nIn this paper, we have presented a new approach to comparing the performance of different deep learning pipelines. We proposed a new metric framework, Calibrated Loss, which has a higher accuracy and smaller variance than its vanilla counterpart for a wide range of pipelines. Our experiments in section 5 demonstrated the superiority of Calibrated Loss, and we believe this new metric can be used to more effectively and efficiently compare the performance of different pipelines in similar settings. Future work includes expanding this idea to evaluate NLP pipelines, and establish theoretical guarantees under more general settings.\n\n\n# LIMITATIONS\n\n\nOur method sacrifices accuracy when comparing some specific pipelines. For example, if pipeline  $B$  can reliably improve the model calibration in test distribution over pipeline  $A$ , Calibrated Log Loss will not be able to correctly detect the benefits of pipeline  $B$ , while Log Loss is able to. However, for most pipeline comparisons conducted in industry and academia like feature engineering, tuning parameters, etc., Calibrated Log Loss has a huge accuracy boost over Log Loss as we demonstrated in Section 5.\n\n\n# POTENTIAL APPLICATIONS\n\n\nOur method may have applications in AutoML domain. AutoML (Automated Machine Learning) systems are designed to automate the process of selecting, designing, and tuning machine learning models, and a key component of these systems is the selection of the best-performing pipeline (e.g. hyperparameters, model architectures etc.). The new metric can be used as a more accurate way of comparing the performance and selecting the best one. The new metric is in particular useful when performing hyperparameter tuning.\n\n\n# REFERENCES\n\n- Xavier Bouthillier and Gael Varoquaux. Survey of machine-learning experimental methods at NeurIPS2019 and ICLR2020. PhD thesis, Inria Saclay Ile de France, 2020.\n\n- Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, et al. Accounting for variance in machine learning benchmarks. Proceedings of Machine Learning and Systems, 3:747-769, 2021.\n\n- Kevin Canini, Tushar Chandra, Eugene Ie, Jim McFadden, Ken Goldman, Mike Gunter, Jeremiah Harmsen, Kristen LeFevre, Dmitry Lepikhin, Tomas Lloret Llinares, et al. Sibyl: A system for large scale supervised machine learning. Technical Talk, 1:113, 2012.\n\n- Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems, pp. 7-10, 2016.\n\n- Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems, pp. 191-198, 2016.\n\n- Christian Desrosiers and George Karypis. A comprehensive survey of neighborhood-based recommendation methods. Recommender systems handbook, pp. 107-144, 2010.\n\n- Tom Fawcett. Roc graphs: Notes and practical considerations for researchers. Machine learning, 31 (1):1-38, 2004.\n\n- Tom Fawcett. An introduction to roc analysis. Pattern recognition letters, 27(8):861-874, 2006.\n\n- Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pp. 1189-1232, 2001.\n\n- Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pp. 1321-1330. PMLR, 2017a.\n\n- Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-machine based neural network for ctr prediction. arXiv preprint arXiv:1703.04247, 2017b.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising, pp. 1-9, 2014.\n\n- Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708, 2017.\n\n- Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448-456. pmlr, 2015.\n\n- Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30-37, 2009.\n\n- Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 1754-1763, 2018.\n\n- H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1222-1230, 2013.\n\n- Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091, 2019.\n\n- David Picard. Torchmanual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision. CoRR, abs/2109.08203, 2021. URL https://arxiv.org/abs/2109.08203.\n\n- Steffen Rendle. Factorization machines. In 2010 IEEE International conference on data mining, pp. 995-1000. IEEE, 2010.\n\n- Weichen Shen. Deepctr: Easy-to-use, modular and extendible package of deep-learning based ctr models. https://github.com/shenweichen/deepctr, 2017.\n\n- Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.\n\n- Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17, pp. 1-7. 2017.\n\n- Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the Web Conference 2021, pp. 1785-1797, 2021.\n\n- Jeonghee Yi, Ye Chen, Jie Li, Swaraj Sett, and Tak W Yan. Predictive model performance: Offline and online evaluations. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1294-1302, 2013.\n\n- Weinan Zhang, Tianming Du, and Jun Wang. Deep learning over multi-field categorical data. In European conference on information retrieval, pp. 45-57. Springer, 2016.\n\n- Zhao-Yu Zhang, Xiang-Rong Sheng, Yujing Zhang, Biye Jiang, Shuguang Han, Hongbo Deng, and Bo Zheng. Towards understanding the overfitting phenomenon of deep click-through rate prediction models. arXiv preprint arXiv:2209.06053, 2022.\n\n- Guorui Zhou, Xiaogiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1059-1068, 2018.\n\n- Han Zhu, Junqi Jin, Chang Tan, Fei Pan, Yifan Zeng, Han Li, and Kun Gai. Optimized cost per click in taobao display advertising. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 2191-2200, 2017.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638434.2270255", "msecs": "227.0", "relativeCreated": "31057.523012161255", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# An Agnostic Approach to Federated Learning with Class Imbalance\n\n# Abstract\nFederated Learning (FL) has emerged as the tool of choice for training deep models over heterogeneous and decentralized datasets. As a reflection of the experiences from different clients, severe class imbalance issues are observed in real-world FL problems. Moreover, there exists a drastic mismatch between the imbalances from the local and global perspectives, i.e. a local majority class can be the minority of the population. Additionally, the privacy requirement of FL poses an extra challenge, as one should handle class imbalance without identifying the minority class. In this paper we propose a novel agnostic constrained learning formulation to tackle the class imbalance problem in FL, without requiring further information beyond the standard FL objective. A meta algorithm, CLIMB, is designed to solve the target optimization problem, with its convergence property analyzed under certain oracle assumptions. Through an extensive empirical study over various data heterogeneity and class imbalance configurations, we showcase that CLIMB considerably improves the performance in the minority class without compromising the overall accuracy of the classifier, which significantly outperforms previous arts. In fact, we observe the greatest performance boost in the most difficult scenario where every client only holds data from one class.\n\n\n# 1 INTRODUCTION\n\n\nClass imbalance is ubiquitous in real world supervised learning problems. Examples span from medical applications (Lee & Shin, 2020; Roy et al., 2019; Choudhury et al., 2019), fraud detection (Yang et al., 2019; Chan et al., 1999), to consumer based applications (Wang et al., 2021b; Wu et al., 2020; Long et al., 2020). In these scenarios, data belonging to a subset of classes constitute a great proportion of the population while data from the minority classes, generated by uncommon events, are scarce (He & Garcia, 2009). Having a non-uniform number of samples per class deteriorates the performance of the classifier in the minority class (Huang et al., 2016), resulting in low training and testing accuracy. More importantly, unintended consequences of mistreating the minority class can be catastrophic (Van Hulse et al., 2007) if the problem is not handled appropriately.\n\n\nFrom the perspectives of data heterogeneity and privacy, the issue of class imbalance is even more significant in the setting of Federated Learning (FL). Due to the heterogeneity of the local data distributions, there can be a significant mismatch between the local and global imbalance, i.e. the class that is a minority locally can actually be a majority class globally. Moreover, for the purpose of privacy protection in FL, one should tackle the class-imbalance problem in an agnostic manner, i.e. the proposed algorithms should not require the minority class to be identified.\n\n\nIn the centralized training setting, techniques like balanced sampling, loss re-weighting, and gradient tuning have achieved many successes (Johnson & Khoshgoftaar, 2019). However, due to the extra difficulty of handling the class imbalance problem in FL, previous arts that rely on the explicit identification of the minority class do not directly apply. While research along this line is quite limited, a commonly used heuristic is to estimate the portion of a class using the norm of the gradient per class. When used as proxies, these quantities are then utilized to reweight the losses corresponding to different classes (Wang et al., 2021a; Yang et al., 2020). A key drawback of (Wang et al., 2021a) is that a subset of the client's local dataset needs to be shared, which is not ideal for privacy preserving purposes. Moreover, the effectiveness of such approaches degrades when there is a notable mismatch between the local imbalance and the global imbalance (Wang et al., 2021a).\n\n\nIn this work, we target the most difficult yet possibly most interesting setting in FL, where there is a significant mismatch between the local and global imbalance. Concretely, on certain clients, a minority class from a global perspective is in the majority locally. Practitioners often encounters such a mismatch due to the highly heterogeneous data configuration in FL and the principle of locality (Wang et al., 2021a). For example, when the local data are produced by a minority user, the corresponding minority data could occupy a large portion of the local distribution. In this scenario, due to the scarceeness of the minority data from a global perspective, the corresponding underrepresented client will usually experience poor service quality from the trained model, giving rise to potentially severe fairness issue.\n\n\nTo overcome the aforementioned challenges, we propose a constrained learning formulation to handle the class imbalance issue in FL while accounting for both heterogeneity and privacy. In brief, we impose constraints on the standard FL formulation so that the empirical loss on every client should not overly exceed the average empirical loss. Such constraints are shown to force the classifier to account for all classes equally and hence mitigate the detrimental effect of class imbalance, under a type of heterogeneous data configuration that captures the mismatch between the local and global imbalance. The advantages of our formulation are threefold: First, in contrast to previous arts which usually rely on some heuristics to reweight the loss functions for different classes, our approach is principled, yielding a simple optimization interpretation. Second, unlike existing methods, our formulation requires no additional information compared to the original FL formulation and it treats data from different classes agnostically, and hence is less likely to leak the private information of the clients. Third, from our extensive empirical study, our formulation can significantly improve the testing accuracy on the minority class, without compromising the overall performance.\n\n\nContribution. We summarize our contributions as follows.\n\n- - Problem Formulation: To address the challenge of class imbalance in FL with data heterogeneity, we propose a novel constrained FL formulation with explicit enforcement on the similarity between the local empirical losses. Using only information from the standard FL objective, our approach is completely agnostic to class distribution of the client data and its proxies, as opposed to the existing literature (which mostly violate privacy requirements of FL).\n\n- - Meta-Algorithm: We solve our constrained optimization problem via a primal-dual approach. For a fixed dual variable, the corresponding Lagrangian function enjoys a similar structure as the standard FL loss, but with non-uniform weights on the local objectives. Accordingly, we propose an efficient (meta-) algorithm called CLIMB that can use any FL optimization method as a subroutine, with nearly negligible communication overhead. Furthermore, we analyze the convergence properties of CLIMB under certain oracle assumptions.\n\n- - Accuracy Improvement in the minority class: On benchmark datasets, CLIMB with Fed-Avg as base solver achieves significant enhancement of the accuracy in the minority class without compromising the overall accuracy under various data heterogeneity and class imbalance scenarios.\n\n\n# 2 RELATED WORKS\n\n\nClass Imbalance in Centralized Setting. In the centralized learning setting, the number of samples per class is known. We categorize existing solutions that exploit such information as follows.\n\n\nBalanced sampling. In order to balance the data used for gradient calculation, the most common approaches are either to under sample from the majority classes (Liu et al., 2008), or to augment the minority class (Chawla et al., 2002; Guo & Viktor, 2004). Both methods seek to generate an artificial uniform distribution of data (Buda et al., 2018; Pouyanfar et al., 2018).\n\n\nLoss reweighting. Methods in this category focus on re-weighting the loss functions for different classes, with extra emphases on the mistakes made in the minority class (Cui et al., 2019; Ling & Sheng, 2008; Sun et al., 2007; Wang et al., 2016). There also exist works like (Lin et al., 2017) that adjust the scale of the loss sample-wise, without exploiting the global data distribution.\n\n\nGradient tuning. In the context of neural networks, some other works focus on tuning the gradient per class (Anand et al., 1993) showing faster rates of convergence in the class imbalance setting.\n\n\nTo do this, the gradient is reshaped in order to have an equal magnitude when projected to all the gradients per class, while keeping the same norm as the original gradient.\n\n\nClass Imbalance in Federated Learning. Combating with the issue of class imbalance is more challenging in FL since the data composition in such a setting is generally unknown and the minority classes are often difficult to identify Li et al. (2020); Wang et al. (2020b). While research in this direction are quite limited, to the best of our knowledge, we classify them into the following two classes based on whether a proxy of the data composition is explicitly built.\n\n\n(i) A common strategy in existing methods is to explicitly build proxies of the data composition based on some heuristics. These proxies are dynamically adjusted during the learning procedure and allow the aforementioned three techniques, balanced sampling, loss re-weighting and gradient tuning, to be utilized in the FL setting: Wang et al. (2021a) suggests that there exists a proportional relation between the magnitude of the gradients the corresponds to the last layer of the neural network and the sample quantity. Based on such observation, the proposed Ratio-Loss, a class-wise re-weighted version of the standard cross entropy loss. We note that to define the Ratio-Loss, one needs to collect a subset of client data as \"auxiliary data\", which may not be ideal in the setting of FL. On the same vein, Yang et al. (2020) argues that the gradient per class can be used as a proxy to infer the imbalances in the distributions of the clients, and thus clients should be selected according to how uniform the magnitude of gradient per class is. We also notice the work Astraea which introduces extra virtual components called mediators between the FL server and clients (Duan et al., 2019). The mediator is assumed to have access to the local data distributions of the clients so that client rescheduling and data augmentation can be carried out accordingly. Through the prepossessing on the mediator, the gradients communicated to the server are made balanced class-wise. However, when privacy is taken into consideration, methods like Astraea may not be appropriate as in essence it is directly built on the global data distribution.\n\n\n(ii) Without explicitly constructing an estimation of the data composition, there are works that resort to techniques like active learning and reinforcement learning to implicitly learn the data composition as the optimization progresses. Works along this research line usually rely on client selection to mitigate the effects of class imbalances. In (Goetz et al., 2019), to address the class imbalance problem, there will a higher probability to sampling clients that posses the global minority class. Other works, leverage client selection as a multi-armed bandit problem, and design a client selection policy to balance the gradients (Xia et al., 2020). Other arts leverage Q-learning techniques to select clients in order to minimize the overall loss Wang et al. (2020a). However, we believe in the most practical setting of FL, clients that are available per communication round is incoercible and hence these strategies have limited applicability.\n\n\nComparison with Previous Works. Our method significantly differs from the above strategies in the following ways: (1) Our method requires no knowledge of the data composition, nor any proxy of such information. Consequently, our approach is agnostic to the minority class and better preserves clients' privacy. Such a property draws clear distinction between our work and the works listed in class (i) above. (2) As will be more clear in the following section, we only perform clientwise re-weighting as opposed to the class-wise re-weighting schemes used in previous works, which further emphasize the agnostic nature of our approach. (3) In contrast to the methods in class (ii) above, our approach does not require active client selection, enjoying a broader applicability in the most practical and interesting settings.\n\n\n# 3 FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS\n\n\nWe consider the problem of multi-class classification. Let  $x \\in \\mathcal{X} \\subseteq \\mathbb{R}^d$  be the input and use  $y \\in \\mathcal{Y} = \\{1, \\dots, C\\}$  to denote the target label, where  $C$  is the total number of classes. In the context of FL, we assume to have  $N$  clients, each of which possesses its own private data distribution  $p_i(x, y)$ . Here,  $p_i(x, y)$  is a joint probability distribution of the input  $x$  and the output label  $y$ . One can decompose  $p_i(x, y)$  as  $p_i(x, y) = p(x|y)p_i(y)$ , where  $p(x|y)$  is the conditional distribution of the input  $x$  given class  $y$  and  $p_i(y)$  is the marginal distribution of class  $y$  on client  $i$ . We assume that the conditional distribution  $p(x|y)$  is identical on all devices, but the marginal distribution  $p_i(y)$  can vary significantly due to the heterogeneity of the data configuration.\n\n\nLet  $\\mathcal{H} = \\{\\phi : \\mathcal{X} \\times \\Theta \\to \\mathbb{R}^C\\}$  be a family of parameterized predictors with parameter  $\\theta \\in \\Theta \\subseteq \\mathbb{R}^Q$ . Let  $\\ell : \\mathbb{R}^C \\times \\mathcal{Y} \\to \\mathbb{R}_+$  be the loss function, then the local objective function of client  $i$  is defined as\n\n\n$$\n\n$$\nf _ {i} (\\theta) = \\mathbb {E} _ {(x, y) \\sim p _ {i}} [ \\ell (\\phi (x, \\theta), y) ]. \\tag {1}\n$$\n\n$$\n\n\nFor multi-class classification,  $\\ell$  is often chosen to be the cross entropy loss. In the standard formulation of FL, the goal is to minimize the global average of local objectives\n\n\n$$\n\n$$\n\\min  _ {\\theta \\in \\Theta} \\bar {f} (\\theta) := \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (\\theta). \\tag {2}\n$$\n\n$$\n\n\nWhile it is well known that, in the presence of class imbalance, the above vanilla formulation will produce models that perform poorly on the minority data, we consider the following configuration of heterogeneous local class distributions in order to make a quantitative analysis of such a phenomenon for a concrete class imbalance setting. More importantly, such a setting will also motivate our constrained FL formulation. We emphasize that the following setting is just used as a motivating example to showcase our claims, and our results apply generally to any FL setting.\n\n\nMotivating Example. Let  $u$  be the uniform distribution over the classes, i.e. for  $y \\sim u$ ,  $\\operatorname{Pr}(y = c) = \\frac{1}{C}, \\forall c \\in \\mathcal{Y}$  and let  $\\delta_c$  be the Dirac distribution of class  $c$ . We assume for some small but fixed  $\\alpha \\in [0,1]$ , the local class distribution of the client  $i$  is a mixture of the uniform distribution over all classes and the Dirac distribution of a fixed class  $c_i$ , i.e.  $p_i = \\alpha u + (1 - \\alpha)\\delta_{c_i}$ . We use  $N_c$  to denote the number of clients with  $c_i = c$ . Note that in the limit setting when  $\\alpha = 0$ , the aforementioned configuration captures the most heterogeneous setting: clients only have data from a single class.\n\n\nWe consider an extreme case of class imbalance under the above configuration. Without loss of generality, we consider the binary classification problem, i.e.  $C = 2$ , and we assume class 1 to be the minority class with  $N_{1} = 1$ . We define  $g_{i}(\\theta) \\coloneqq \\mathbb{E}_{x \\sim p(x|y = i)}[\\ell(\\phi(x,\\theta),i)]$  as the loss of the predictor  $\\phi(\\cdot,\\theta)$  on the data with  $y = i$ . We can calculate that\n\n\n$$\n\n$$\n\\bar {f} (\\theta) = \\left(\\frac {\\alpha}{2} + \\frac {1 - \\alpha}{N}\\right) g _ {1} (\\theta) + \\left(\\frac {\\alpha}{2} + \\frac {(1 - \\alpha) (N - 1)}{N}\\right) g _ {2} (\\theta). \\tag {3}\n$$\n\n$$\n\n\nClearly, when  $\\alpha$ , the portion of data with uniform label distribution, is small, e.g.  $\\alpha = 0$ , and  $N$ , the total number of clients, is large, the loss of the predictor on class 1 has negligible weight, which often leads to the poor performance on the minority class in the trained classifier as the majority classes dominate the gradient. Moreover, observe that when  $\\alpha$  is small, there is a significant mismatch between the local and global class imbalance: the global minority class 1 is in the majority on the corresponding client locally. Such phenomenon is pertinent to the FL setting due to the data heterogeneity and poses a great challenge for tackling the issue of class imbalance in FL.\n\n\n# 3.1 CONSTRAINED FL FORMULATION\n\n\nIn our work, to address the class imbalance challenge, we propose to minimize the following constrained FL (CFL) formulation\n\n\n$$\n\n$$\nP _ {\\epsilon} ^ {*} = \\min  _ {\\theta \\in \\Theta} \\bar {f} (\\theta) := \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (\\theta) \\tag {CFL}\n$$\n\n$$\n\n\n$$\n\n$$\n\\begin{array}{l} \\text {s . t .} f _ {i} (\\theta) - \\bar {f} (\\theta) \\leq \\epsilon , \\forall i \\in [ 1, \\dots , N ]. \\end{array}\n$$\n\n$$\n\n\nHere,  $\\epsilon$  is a tolerance constant that controls the enforced closeness in the training loss among clients, and we emphasize the dependence of the optimal value  $P_{\\epsilon}^{*}$  on  $\\epsilon$  by encoding it in the subscript. As a motivation, we show that the constraint in our formulation (CFL) can be translated to a constraint on the performance of the minority class, under the above class imbalance setting: Consider the setting where the tolerance constant is very small, i.e.  $\\epsilon$  is close to zero. WLOG, we assume that the first device is the one that has  $c_{i} = 1$ . Recall the definition of  $g_{i}$  above Eq.(3). One can compute that\n\n\n$$\n\n$$\nf _ {1} (\\theta) - \\bar {f} (\\theta) = \\frac {(1 - \\alpha) (N - 1)}{N} \\left(g _ {1} (\\theta) - g _ {2} (\\theta)\\right) \\leq \\epsilon \\Longleftrightarrow g _ {1} (\\theta) - g _ {2} (\\theta) \\leq \\frac {N \\epsilon}{(1 - \\alpha) (N - 1)}.\n$$\n\n$$\n\n\nTherefore, our formulation (CFL) enjoys a clear class-balancing interpretation in the highly heterogeneous setting when  $\\alpha$  is small and  $N$  is large. Note that this is achieved without the identification of the minority class and server collects no additional information compared to the vanilla FL.\n\n\n```\n1: Input: initial model  $\\theta^0$ , a subroutine ClientUpdate, dual step size  $\\eta_{D}$ , maximum round  $T$ ;  \n2: Initialize the dual variables  $\\lambda = [0,\\dots ,0]$ ;  \n3: for  $t = 1,2,\\ldots ,T$  do  \n4: Compute weights:  $\\forall i\\in [N],w_i = 1 + \\lambda_i - \\bar{\\lambda}$  with  $\\bar{\\lambda} = \\frac{1}{N}\\sum_{i = 1}^{N}\\lambda_{i}$ ;  \n5: Primal Update:  $\\theta^{t + 1}\\gets \\text{ClientUpdate} (\\{w_i\\}_{i = 1}^N,\\theta^t)$ ;  \n6: Dual Update:  $\\forall i\\in [N],\\lambda_i\\gets [\\lambda_i + \\eta_D(f_i(\\theta^{t + 1}) - \\bar{f} (\\theta^{t + 1}) - \\epsilon)]_+$ , with  $\\bar{f} = \\frac{1}{N}\\sum_{i = 1}^{N}f_{i}$ ;  \n7: end for  \n8: Output: model  $\\theta^{T + 1}$\n```\n\n\nWhile the above nice interpretation of the proposed constraint may not hold exactly for general class imbalance settings, in spirit, we want the resulting classifier from our algorithm to perform similarly on every class, or in other words to account for the minority class and majority class equally. In the following section, we discuss how to solve the proposed formulation by alternating the primal and dual updates of an equivalent Lagrangian formulation.\n\n\n# 3.2 ALGORITHM CONSTRUCTION\n\n\nIn order to solve problem (CFL), we resort to the method of Lagrange multipliers. By introducing the dual variables  $\\lambda = [\\lambda_1, \\dots, \\lambda_N] \\in \\mathbb{R}_+^N$ , we define the Lagrangian function as,\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) = \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (\\theta) + \\lambda_ {i} \\left(f _ {i} (\\theta) - \\frac {1}{N} \\sum_ {j = 1} ^ {N} f _ {j} (\\theta) - \\epsilon\\right) (4) \\\\ = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\left(1 + \\lambda_ {i} - \\bar {\\lambda}\\right) f _ {i} (\\theta) - \\lambda_ {i} \\epsilon , \\quad \\text {w i t h} \\bar {\\lambda} = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\lambda_ {i}. (5) \\\\ \\end{array}\n$$\n\n$$\n\n\nWith this in hand, we can construct a lower bound of the above constrained optimization problem using the Lagrangian  $\\mathcal{L}(\\theta ,\\lambda)$  as follows:\n\n\n$$\n\n$$\nD _ {\\epsilon} ^ {*} = \\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\min  _ {\\theta \\in \\Theta} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) \\leq \\min  _ {\\theta \\in \\Theta} \\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) = P _ {\\epsilon} ^ {*},\n$$\n\n$$\n\n\nwhere we often refer to  $D_{\\epsilon}^{*}$  as the dual problem. To solve the dual problem, we propose CLIMB, a method described in Algorithm 1, which is discussed in detail as follows.\n\n\nCLIMB proceeds by alternatingly optimizing over the primal variable  $\\theta$  and the dual variable  $\\lambda$ .\n\n\nPrimal Update. For a fixed  $\\lambda$ , the minimization of  $\\mathcal{L}$  with respect to  $\\theta$  is equivalent to a re-weighted version of the standard unconstrained FL objective (2):\n\n\n$$\n\n$$\n\\min  _ {\\theta \\in \\Theta} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) \\Longleftrightarrow \\min  _ {\\theta \\in \\Theta} \\frac {1}{N} \\sum_ {i = 1} ^ {N} (1 + \\lambda_ {i} - \\bar {\\lambda}) f _ {i} (\\theta). \\tag {6}\n$$\n\n$$\n\n\nImportantly, this simple and canonical form allows us to perform the update on  $\\theta$  using any FL solver as the base optimizer. For flexibility, we do not explicitly choose the base optimizer in our algorithm description, but refer to the update on  $\\theta$  as the subroutine ClientUpdate( $\\{w_i\\}_{i=1}^N, \\theta^t) \\to \\theta^{t+1}$ . Such a subroutine takes the non-uniform weights  $\\{w_i\\}_{i=1}^N$  on the local objectives and the current consensus model  $\\theta^t$  as inputs, and returns an updated model  $\\theta^{t+1}$ . Note that every call to ClientUpdate may consists of multiple communication rounds.\n\n\nDual Update. Once we have obtained the new consensus model  $\\theta^{t + 1}$ , we perform dual update on  $\\lambda$  by taking a single dual ascent step of the following equivalent objective (given some fixed  $\\theta$ ),\n\n\n$$\n\n$$\n\\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) \\Longleftrightarrow \\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\lambda_ {i} \\left(f _ {i} (\\theta) - \\frac {1}{N} \\sum_ {j = 1} ^ {N} f _ {j} (\\theta) - \\epsilon\\right). \\tag {7}\n$$\n\n$$\n\n\nTo evaluate the gradient of  $\\mathcal{L}$  with respect to  $\\lambda$ , we need to first broadcast the consensus model  $\\theta^{t + 1}$  and then aggregate the function values  $f(\\theta^{t + 1})$ . Since the broadcast model can be used for the next round of primal update, the only overhead of CLIMB compared to the standard FL solver ClientUpdate, is to transmit the functional value, which is negligible.\n\n\n# 3.3 THEORETICAL GUARANTEES\n\n\nBased on the recent progress in constrained learning (Chamon et al., 2021), we show that under the following mild regularity conditions, the duality gap between the dual problem  $D_{\\epsilon}^{*}$  and the primal problem  $P_{\\epsilon}^{*}$  can be controlled by some quantity that describes the capability of the parametric function class  $\\mathcal{H}$ .\n\n\nAssumption 3.1 The loss function  $\\ell$  in the definition of the local objective (1) is  $L$ -Lipschitz, i.e.  $\\| \\ell (x,\\cdot) - \\ell (z,\\cdot)\\| \\leq L\\| x - z\\|$ , and bounded by  $B$ .\n\n\nAssumption 3.2 The conditional distribution  $p(x|y)$  is non-atomic for all  $y \\in \\mathbb{R}^C$ .\n\n\nWhile usually the local data distribution is discrete, we can always augment it by randomly perturbing the data points with white noise, this is often used as data augmentation in vision tasks.\n\n\nAssumption 3.3 There exists a convex hypothesis class  $\\hat{\\mathcal{H}}$  such that  $\\mathcal{H} \\subseteq \\hat{\\mathcal{H}}$ , and there exists a constant  $\\xi > 0$  such that  $\\forall \\hat{\\phi} \\in \\hat{\\mathcal{H}}$ , there exists  $\\theta \\in \\Theta$  such that  $\\sup_{x \\in \\mathcal{X}} \\| \\hat{\\phi}(x) - \\phi(x, \\theta) \\| \\leq \\xi$ .\n\n\nA simple strategy to construct  $\\hat{\\mathcal{H}}$  is to take the convex hull of  $\\mathcal{H}$ . When  $\\mathcal{H}$  is sufficiently rich,  $\\xi$  can be expected to be small. Notice that this bound can be decreased by increasing the richness of the function class  $\\mathcal{H}$ . All the proofs can be found in the Appendix.\n\n\nTheorem 3.1 (Near Zero Duality Gap) Under Assumptions 3.1, 3.2, 3.3, and the Constrained Federated Learning problem is feasible in  $\\hat{\\mathcal{H}}$  with constraint  $\\epsilon - 2L\\xi$ , the Constrained Federated Learning problem has near zero-duality gap,\n\n\n$$\n\n$$\nP _ {\\epsilon} ^ {*} - D _ {\\epsilon} ^ {*} \\leq (2 | \\boldsymbol {\\lambda} _ {\\epsilon - 2 L \\xi} ^ {*} | _ {1} + 1) L \\xi , \\tag {8}\n$$\n\n$$\n\n\nwhere  $\\lambda_{\\epsilon -2L\\xi}^{*}$  is the optimal dual variable associated with the Constrained Federated Learning problem equation CFL with constraints  $\\epsilon -2L\\xi$  over the space of functions  $\\hat{\\mathcal{H}}$ .\n\n\nTheorem 3.1 establishes an upper bound on the duality gap of the Constrained Federated Learning Problem CFL. The gap depends on the Lipschitz constant of the loss function  $L$ , the richness of the function class  $\\xi$ , and the optimal dual variable of a more restrictive problem. Note that we required the Constrained Federated Learning problem to be feasible for constraint  $\\epsilon - 2L\\xi$ . In the case of the cross-entropy loss, as long as  $\\epsilon - 2L\\xi > 0$ , this can be satisfied by a classifier that assigns a uniform label for every sample, as each individual loss will be equal to each other.\n\n\nSince the minimization of the Lagrangian function  $\\mathcal{L}$  is non-convex, to show the convergence of CLIMB, we need an additional oracle assumption as follows.\n\n\nAssumption 3.4 (Approximate Solution) For every dual variable  $\\pmb{\\lambda} \\in \\mathbb{R}_{+}^{N}$ , and precision  $\\delta > 0$  there exists an oracle approximate solution  $\\theta_{\\lambda}$  such that  $\\mathcal{L}(\\theta_{\\lambda}, \\pmb{\\lambda}) \\leq \\min_{\\theta \\in \\mathbb{R}^Q} \\mathcal{L}(\\theta, \\pmb{\\lambda}) + \\delta$ .\n\n\nTheorem 3.2 (Convergence) Define the dual function  $d(\\lambda) = \\min_{\\theta \\in \\Theta} \\mathcal{L}(\\theta, \\lambda)$ . Under Assumptions 3.1 to 3.4, for a fixed tolerance  $r > 0$ , the iterates generated by Algorithm 1, achieve a neighborhood of the dual problem  $D_{\\epsilon}^{*}$  in at most  $T_r = \\mathcal{O}(1 / r)$  steps, i.e.,\n\n\n$$\n\n$$\nd \\left(\\boldsymbol {\\lambda} ^ {T _ {r}}\\right) \\geq D _ {\\epsilon} ^ {*} - \\delta - \\frac {\\eta_ {D}}{2} B ^ {2} - r, \\tag {9}\n$$\n\n$$\n\n\nDue to space limitation, a detailed discussion of the convergence theorem is deferred to the appendix.\n\n\n# 4 EXPERIMENTS\n\n\nIn this section, we evaluate our formulation (CFL) against the competitors on various FL scenarios at the presence of class imbalance. Our results highlight the benefits of our approach especially when the local data distributions are severely heterogeneous and there is a significant mismatch between local and global imbalance. To ensure a fair comparison, we adopt the same base optimizer, Fed-Avg\n\n\n<table><tr><td>Imbalance ratio</td><td>Dataset</td><td>Level of heterogeneity</td><td>Baseline (Eq.(2))</td><td>CLIMB (this work)</td><td>Ratio-Loss</td><td>Focal-Loss</td></tr><tr><td rowspan=\"12\">\u03c1 = 20</td><td rowspan=\"6\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.0532 (0.6754)</td><td>0.2080 (0.6829)</td><td>0.1140 (0.6727)</td><td>0.0450 (0.6445)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.137 (0.7121)</td><td>0.3230 (0.7121)</td><td>0.1790 (0.7037)</td><td>0.0430 (0.6914)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0 (0.5669)</td><td>0.2810 (0.6031)</td><td>0 (0.5746)</td><td>0 (0.6565)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.1279 (0.7098)</td><td>0.3240 (0.7167)</td><td>0.1790 (0.7054)</td><td>0.0552 (0.6905)</td></tr><tr><td rowspan=\"6\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.6889 (0.9375)</td><td>0.8552 (0.9556)</td><td>0.8472 (0.9544)</td><td>0.6186 (0.9278)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.7925 (0.9540)</td><td>0.8748 (0.9616)</td><td>0.8052 (0.9555)</td><td>0.7784 (0.9479)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.3425 (0.8260)</td><td>0.6987 (0.9158)</td><td>0.4134 (0.8484)</td><td>0.1944 (0.7938)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.4720 (0.8596)</td><td>0.7290 (0.9153)</td><td>0.6717 (0.9063)</td><td>0.4602 (0.8654)</td></tr><tr><td rowspan=\"12\">\u03c1 = 10</td><td rowspan=\"6\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.2058 (0.6841)</td><td>0.3629 (0.7041)</td><td>0.2164 (0.6839)</td><td>0.0414 (0.6543)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.1813 (0.7113)</td><td>0.3743 (0.7312)</td><td>0.2657 (0.7083)</td><td>0.1347 (0.6911)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.0492 (0.5933)</td><td>0.2280 (0.6358)</td><td>0.0315 (0.5825)</td><td>0 (0.5548)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.1064 (0.6380)</td><td>0.2734 (0.6696)</td><td>0.0993 (0.6211)</td><td>0.0298 (0.5982)</td></tr><tr><td rowspan=\"6\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.8473 (0.9534)</td><td>0.9305 (0.9584)</td><td>0.8851 (0.9558)</td><td>0.8469 (0.9470)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.8962 (0.9634)</td><td>0.9239 (0.9657)</td><td>0.8798 (0.9615)</td><td>0.8953 (0.9586)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.6045 (0.8906)</td><td>0.8084 (0.9356)</td><td>0.6981 (0.9119)</td><td>0.4690 (0.8670)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.7272 (0.9190)</td><td>0.8195 (0.9392)</td><td>0.7663 (0.9320)</td><td>0.6961 (0.9099)</td></tr></table>\n\n\nTable 1: The minority class testing accuracy and the overall testing accuracy (the quantity in the parentheses) after sufficiently many communication rounds. If there are multiple minority classes, we report the worst of them. Here  $N$ , the number of devices, is 500. The base FL solver is Fed-Avg with partial-participation: 100 devices participate in every communication round.\n\n\nin the current experiment<sup>1</sup>, for all formulations: the standard FL objective in Eq.(2), the proposed formulation in Eq.(CFL), Ratio-Loss (Wang et al., 2021a), and Focal-Loss (Lin et al., 2017). We emphasize that the novelty of our work lies in the new constrained FL formulation (CFL) and is orthogonal to the research of how the formulation is solved. In the following, we describe the datasets and models used in our experiments where more details are provided in the Appendix A.\n\n\nDatasets Two benchmark datasets are used in our experiments with the default train/test splits, which are MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al., 2009).\n\n\nHeterogeneity. We generate heterogeneity in the local data distributions according to the strategy from (Karimireddy et al., 2020; Hsu et al., 2019): Let  $\\alpha \\in [0,1]$  be some constant that determines\n\n\nthe level of heterogeneity. For a fixed  $\\alpha$ , we divide the dataset among  $N = 100$  (moderate) or  $N = 500$  (massive) clients as follows: for we allocate to each client a portion of  $\\alpha$  i.i.d. data and the remaining portion of  $(1 - \\alpha)$  by sorting according to label.\n\n\nData Imbalance. We simulate the phenomenon of class imbalance by removing data belong to the minority classes: Observe that the datasets included in our experiments both have 10 perfectly balanced classes. For the minority class(es), we retain only  $1 / \\rho$  portion of the corresponding data. Here,  $\\rho \\geq 1$  the ratio between the numbers of data in the majority class and in the minority class and is termed the imbalance ratio. For example, when there are 3 minority classes with  $\\rho = 10$ ,  $90\\%$  of the data belong to classes 0, 1, 2 (without loss of generality) are manually removed. In our experiments, we consider the setting of 1 or 3 minority classes and we take  $\\rho = 5, 10, 20$ .\n\n\nModels We follow the choice of model architectures in (Acar et al., 2020; McMahan et al., 2017). Specifically, we use a 2 hidden layer fully-connected neural network for MNIST, where the numbers of neurons are (128, 128). For CIFAR10, we use a CNN model consisting of 2 convolutional layers with  $645 \\times 5$  filters followed by 2 fully connected layers with 394 and 192 neurons. We note that higher testing accuracy on the included datasets can be obtained by using models with high capacity, but is orthogonal to our research.\n\n\n# 4.1 RESULTS SUMMARY\n\n\nTo evaluate the effectiveness of an approach against the challenge of class imbalance, one needs to take into consideration both the performance on the minority class(es) and the average performance on all the classes. We report both quantities after sufficient communication rounds (5000 rounds for CIFAR10 and 1000 rounds for MNIST) under various experiment settings in Tables 1 and 2. In every cell of these tables, the quantity above denote the minority class testing accuracy and the quantity in the parentheses is the average performance on all the classes. We also considered the case where there are multiple minority classes and we report the worst accuracy among the minority classes. Our approach outperforms previous arts in all cases, often by a large margin.\n\n\nImbalance Ratio and Number of Minority Classes. The imbalance ratio  $\\rho$  and the number of minority classes are two important quantities to measure the difficulty of a class imbalance problem. Under all the included choices, CLIMB consistently beats previous arts in both minority class testing accuracy and average performance on all the classes. Therefore, we conclude that CLIMB is able to boost the performance on the minority class without compromising the performance on the other classes. This is a rare merit when addressing the class imbalance problem. In fact, methods like Ratio-Lossis able to improve the testing accuracy on the minority class, but it also sacrifices the performance on the rest classes, leading to an inferior average testing accuracy.\n\n\nLevel of Heterogeneity. We test the performance of CLIMB under different levels of heterogeneity and observe that CLIMB outperforms the included methods considerably in all settings. It has the biggest advantage over the competitors in the most heterogeneous setting,  $\\alpha = 0$ . There are situations that existing methods completely fail in the minority class with less than  $10\\%$  accuracy, but CLIMB is still able to correctly classify most of the minority data, e.g. see Table  $2 \\rightarrow \\rho = 5 \\rightarrow \\mathrm{MNIST} \\rightarrow 3$  minority classes  $\\rightarrow \\alpha = 0$ .\n\n\nModerate vs. Large Number of Devices An important goal of FL is to exploit the computational resources of the IoT devices. Hence, the scalability to a large number of devices is a critical property of an FL method. We hence test CLIMB on both moderate ( $N = 100$ , see Table 2) and massive devices ( $N = 500$ , see Table 1) settings. To make things more practical, we instantiate the subroutine ClientUpdate using Fed-Avg with the partial-participation scheme in the massive device setting. Specifically, 100 devices participate model training after every Fed-Avg global communication round. We clearly observe the advantage of CLIMB in all of these setups.\n\n\n# CONCLUSION\n\n\nIn this paper we proposed a novel agnostic constrained learning formulation to tackle the problem of class imbalance in the Federated Learning setting. By introducing constraints in the learning procedure we enforced the performance to be similar in all clients, thus accounting for the class imbalances. In terms of privacy protection, our formulation requires no further information than the standard FL objective to be collected in the server and it never estimates the data composition as opposed to all previous approaches. Moreover, compared with previous arts which are usually heuristic based, our approach is principled as it is purely optimization based and can be efficiently solved via the proposed meta-algorithm CLIMB, yielding major practical benefits. Our extensive empirical study showcases the superiority of proposed constrained formulation over previous arts.\n\n\n<table><tr><td>Imbalance ratio</td><td>Dataset</td><td>Level of heterogeneity</td><td>Baseline (Eq.(2))</td><td>CLIMB (this work)</td><td>Ratio-Loss</td><td>Focal-Loss</td></tr><tr><td rowspan=\"16\">\u03c1 = 10</td><td rowspan=\"8\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0229(0.5734)</td><td>0.5575(0.6076)</td><td>0(0.4836)</td><td>0(0.4205)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.2753(0.7143)</td><td>0.5054(0.7246)</td><td>0.2929(0.6951)</td><td>0.2284(0.6860)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.29880.7348</td><td>0.4689(0.7511)</td><td>0.3825(0.7329)</td><td>0.26180.7249</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0402(0.5534)</td><td>0.2756(0.5598)</td><td>0(0.4678)</td><td>0(0.4527)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.1316(0.6189)</td><td>0.3399(0.6637)</td><td>0.0690(0.615)</td><td>0.0408(0.5976)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.2566(0.6659)</td><td>0.3292(0.6983)</td><td>0.1916(0.6504)</td><td>0.1213(0.6346)</td></tr><tr><td rowspan=\"8\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.3092(0.8630)</td><td>0.9175(0.9341)</td><td>0.4650(0.8630)</td><td>0.3078(0.8556)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.8597(0.9586)</td><td>0.9428(0.9675)</td><td>0.9189(0.9648)</td><td>0.8348(0.9529)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.8750(0.9640)</td><td>0.9377(0.9715)</td><td>0.9283(0.9706)</td><td>0.8882(0.9631)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0153(0.7133)</td><td>0.8029(0.9115)</td><td>0.06310.7222</td><td>0.07170.7401</td></tr><tr><td>\u03b1 = 0.1</td><td>0.7263(0.9189)</td><td>0.8828(0.9522)</td><td>0.7870(0.9341)</td><td>0.6674(0.9069)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.7950(0.9352)</td><td>0.8004(0.9416)</td><td>0.7801(0.9364)</td><td>0.7785(0.9274)</td></tr><tr><td rowspan=\"16\">\u03c1 = 5</td><td rowspan=\"8\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.2892(0.6382)</td><td>0.5987(0.6468)</td><td>0.0942(0.5506)</td><td>0.1491(0.5631)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.4101(0.7186)</td><td>0.6075(0.7351)</td><td>0.4011(0.7008)</td><td>0.3558(0.6994)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.5335(0.7536)</td><td>0.6063(0.7556)</td><td>0.5054(0.7427)</td><td>0.4359(0.7380)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0313(0.4742)</td><td>0.3813(0.5639)</td><td>0.0512(0.5108)</td><td>0(0.4514)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.5135(0.6636)</td><td>0.6420(0.6977)</td><td>0.4600(0.6632)</td><td>0.4213(0.6384)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.5251(0.6960)</td><td>0.6328(0.7202)</td><td>0.5751(0.6883)</td><td>0.5311(0.6749)</td></tr><tr><td rowspan=\"8\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.7245(0.8953)</td><td>0.9154(0.9224)</td><td>0.8020(0.9016)</td><td>0.7429(0.8992)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.9378(0.9666)</td><td>0.9582(0.9693)</td><td>0.9286(0.9651)</td><td>0.9408(0.9624)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.9448(0.9711)</td><td>0.9670(0.9734)</td><td>0.9561(0.9733)</td><td>0.9481(0.9686)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0160(0.7722)</td><td>0.8744(0.9137)</td><td>0(0.7370)</td><td>0.0544(0.7826)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.8294(0.9422)</td><td>0.9217(0.9606)</td><td>0.8520(0.9501)</td><td>0.7987(0.9361)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.8557(0.9536)</td><td>0.8818(0.9595)</td><td>0.8730(0.9536)</td><td>0.8321(0.9442)</td></tr></table>\n\n\nTable 2: The minority class testing accuracy and the overall testing accuracy (the quantity in the parentheses) after sufficiently many communication rounds. If there are multiple minority classes, we report the worst of them. Here  $N$ , the number of devices, is 100. The base FL solver is Fed-Avg with full-participation: all devices participate in every communication round.\n\n\n# REFERENCES\n\n- Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Conference on Learning Representations, 2020.\n\n- Rangachari Anand, Kishan G Mehrotra, Chilukuri K Mohan, and Sanjay Ranka. An improved algorithm for neural network classification of imbalanced training sets. IEEE Transactions on Neural Networks, 4(6):962-969, 1993.\n\n- Dimitri Bertsekas. Convex optimization theory. Athena Scientific, 2009.\n\n- Dimitri Bertsekas. Convex optimization algorithms. Athena Scientific, 2015.\n\n- Stephen Boyd and Almir Mutapcic. Subgradient methods.\n\n- Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n\n- Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. Neural Networks, 106:249-259, 2018.\n\n- Luiz Chamon and Alejandro Ribeiro. Probably approximately correct constrained learning. Advances in Neural Information Processing Systems, 33, 2020.\n\n- Luiz FO Chamon, Santiago Paternain, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained learning with non-convex losses. arXiv preprint arXiv:2103.05134, 2021.\n\n- Philip K Chan, Wei Fan, Andreas L Prodromidis, and Salvatore J Stolfo. Distributed data mining in credit card fraud detection. IEEE Intelligent Systems and Their Applications, 14(6):67-74, 1999.\n\n- Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.\n\n- Olivia Choudhury, Yoonyoung Park, Theodoros Salonidis, Aris Gkoulalas-Divanis, Issa Sylla, et al. Predicting adverse drug reactions on distributed health data using federated learning. In AMIA Annual symposium proceedings, volume 2019, pp. 313. American Medical Informatics Association, 2019.\n\n- Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9268-9277, 2019.\n\n- Moming Duan, Duo Liu, Xianzhang Chen, Yujuan Tan, Jinting Ren, Lei Qiao, and Liang Liang. Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications. In 2019 IEEE 37th international conference on computer design (ICCD), pp. 246-254. IEEE, 2019.\n\n- Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, and Anuj Kumar. Active federated learning. arXiv preprint arXiv:1909.12641, 2019.\n\n- Hongyu Guo and Herna L Viktor. Learning from imbalanced data sets with boosting and data generation: the databost-im approach. ACM Sigkdd Explorations Newsletter, 6(1):30-39, 2004.\n\n- Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):1263-1284, 2009.\n\n- Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.\n\n- Chen Huang, Yining Li, Chen Change Loy, and Xiaou Tang. Learning deep representation for imbalanced classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5375-5384, 2016.\n\n- Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal of Big Data, 6(1):1-54, 2019.\n\n- Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.\n\n- Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.\n\n- Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\n- Geun Hyeong Lee and Soo-Yong Shin. Federated learning on clinical benchmark data: Performance assessment. J Med Internet Res, 22(10):e20891, Oct 2020. ISSN 1438-8871. doi: 10.2196/20891. URL http://www.jmir.org/2020/10/e20891/.\n\n- Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data, 2020.\n\n- Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr D\u00f3lar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980-2988, 2017.\n\n- Charles X Ling and Victor S Sheng. Cost-sensitive learning and the class imbalance problem. Encyclopedia of machine learning, 2011:231-235, 2008.\n\n- Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. Exploratory undersampling for class-imbalance learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(2):539-550, 2008.\n\n- Guodong Long, Yue Tan, Jing Jiang, and Chengqi Zhang. Federated Learning for Open Banking, pp. 240-254. Springer International Publishing, Cham, 2020. ISBN 978-3-030-63076-8. doi: 10.1007/978-3-030-63076-8_17. URL https://doi.org/10.1007/978-3-030-63076-8_17.\n\n- Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pp. 1273-1282. PMLR, 20-22 Apr 2017. URL https://proceedings.mlr.press/v54/mcmahan17a.html.\n\n- Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S Kaseb, Kent Gauen, Ryan Dailey, Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-Ching Chen, et al. Dynamic sampling in convolutional neural networks for imbalanced data classification. In 2018 IEEE conference on multimedia information processing and retrieval (MIPR), pp. 112-117. IEEE, 2018.\n\n- Alejandro Ribeiro. Optimal resource allocation in wireless communication and networking. EURASIP Journal on Wireless Communications and Networking, 2012(1):1-19, 2012.\n\n- Abhijit Guha Roy, Shayan Siddiqui, Sebastian P\u00f6lsterl, Nassir Navab, and Christian Wachinger. Brantorrent: A peer-to-peer environment for decentralized federated learning. arXiv preprint arXiv:1905.06731, 2019.\n\n- Yanmin Sun, Mohamed S Kamel, Andrew KC Wong, and Yang Wang. Cost-sensitive boosting for classification of imbalanced data. Pattern recognition, 40(12):3358-3378, 2007.\n\n- Fabio Tardella. A new proof of the lyapunov convexity theorem. SIAM journal on control and optimization, 28(2):478-481, 1990.\n\n- Jason Van Hulse, Taghi M Khoshgoftaar, and Amri Napolitano. Experimental perspectives on learning from imbalanced data. In Proceedings of the 24th international conference on Machine learning, pp. 935-942, 2007.\n\n- Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li. Optimizing federated learning on non-iid data with reinforcement learning. In IEEE INFOCOM 2020-IEEE Conference on Computer Communications, pp. 1698-1707. IEEE, 2020a.\n\n- Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization, 2020b.\n\n- Lixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. Addressing class imbalance in federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 10165-10173, 2021a.\n\n- Shoujin Wang, Wei Liu, Jia Wu, Longbing Cao, Qinxue Meng, and Paul J Kennedy. Training deep neural networks on imbalanced data sets. In 2016 international joint conference on neural networks (IJCNN), pp. 4368-4374. IEEE, 2016.\n\n- Yi Wang, Imane Lahmam Bennani, Xiufeng Liu, Mingyang Sun, and Yao Zhou. Electricity consumer characteristics identification: A federated learning approach. IEEE Transactions on Smart Grid, 12(4):3637-3647, 2021b. doi: 10.1109/TSG.2021.3066577.\n\n- Qiong Wu, Xu Chen, Zhi Zhou, and Junshan Zhang. Fedhome: Cloud-edge based personalized federated learning for in-home health monitoring. IEEE Transactions on Mobile Computing, pp. 1-1, 2020. doi: 10.1109/TMC.2020.3045266.\n\n- Wenchao Xia, Tony QS Quek, Kun Guo, Wanli Wen, Howard H Yang, and Hongbo Zhu. Multiarmed bandit-based client scheduling for federated learning. IEEE Transactions on Wireless Communications, 19(11):7108-7123, 2020.\n\n- Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.\n\n- Miao Yang, Akitanoshou Wong, Hongbin Zhu, Haifeng Wang, and Hua Qian. Federated learning with class imbalance reduction, 2020.\n\n- Wensi Yang, Yuhang Zhang, Kejiang Ye, Li Li, and Cheng-Zhong Xu. Ffd: A federated learning based method for credit card fraud detection. In Keke Chen, Sangeetha Seshadri, and Liang-Jie Zhang (eds.), Big Data - BigData 2019, pp. 18-32, Cham, 2019. Springer International Publishing. ISBN 978-3-030-23551-2.\n\n- Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference on Machine Learning, pp. 7252-7261. PMLR, 2019.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "TextualGradientDescent prompt for update", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "165", "funcName": "_update_prompt", "created": "1768638434.2366009", "msecs": "236.0", "relativeCreated": "31067.098379135132", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "prompt": "Here is the role of the variable you will improve: <ROLE>System prompt that sets the AI reviewer's role and approach for paper evaluation</ROLE>.\n\nThe variable is the text within the following span: <VARIABLE> You are an expert academic reviewer tasked with evaluating research papers. </VARIABLE>\n\nHere is the context and feedback we got for the variable:\n\n<CONTEXT></CONTEXT>\n\nImprove the variable (System prompt that sets the AI reviewer's role and approach for paper evaluation) using the feedback provided in <FEEDBACK> tags.\nYou must follow the following constraints:\n\n<CONSTRAINTS>Constraint 1: Keep the system prompt under 500 characters\nConstraint 2: Keep the human prefix under 800 characters\nConstraint 3: The human prefix MUST retain the output format instruction: \\boxed{Accept} or \\boxed{Reject}\nConstraint 4: The human prefix should clearly state the task is predicting paper acceptance</CONSTRAINTS>\n\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.", "message": "TextualGradientDescent prompt for update"}
{"name": "textgrad", "msg": "TextualGradientDescent optimizer response", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "179", "funcName": "step", "created": "1768638493.0629601", "msecs": "62.0", "relativeCreated": "89893.4576511383", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "optimizer.response": "<think>\nOkay, let's tackle this. The user wants to improve the system prompt for an AI reviewer evaluating research papers. The original prompt is \"You are an expert academic reviewer tasked with evaluating research papers.\" The constraints are important here.\n\nFirst, the system prompt needs to be under 500 characters. Let me check the original length. \"You are an expert academic reviewer tasked with evaluating research papers.\" That's 55 characters. Wait, maybe I'm miscalculating. Let me count again. \"You are an expert academic reviewer tasked with evaluating research papers.\" Let's see: Y(1) o(2) u(3) (space) a(4) r(5) e(6) (space) a(7) n(8) (space) e(9) x(10) p(11) e(12) r(13) t(14) (space) a(15) c(16) a(17) d(18) e(19) m(20) i(21) c(22) (space) r(23) e(24) v(25) i(26) e(27) w(28) (space) t(29) a(30) s(31) k(32) e(33) d(34) (space) w(35) i(36) t(37) h(38) (space) e(39) v(40) a(41) l(42) u(43) a(44) t(45) i(46) n(47) g(48) (space) r(49) e(50) s(51) e(52) a(53) r(54) c(55) h(56) (space) p(57) a(58) p(59) e(60) r(61) s(62). So 62 characters. That's way under 500. So maybe the original is okay, but the user wants it improved. \n\nLooking at the constraints again. The user wants the system prompt to set the AI's role and approach for paper evaluation. The original is very brief. Maybe it's too vague. The feedback might have mentioned that the original is too short and lacks specific instructions. \n\nThe improved variable should include the task of predicting acceptance, and the output format must be \\boxed{Accept} or \\boxed{Reject}. Also, the human prefix (which I think refers to the input prompt given to the model) should have the task clearly stated as predicting paper acceptance. Wait, the constraints mention the human prefix, but the variable here is the system prompt. Maybe I need to check the role again. The variable is the system prompt, so the constraints might be about the system prompt's length and content. \n\nThe user's constraints mention keeping the system prompt under 500 characters, which the original already satisfies. But maybe the original is too generic. The improved prompt should specify that the AI is to evaluate papers and predict acceptance, with the output format. \n\nSo, the original is \"You are an expert academic reviewer tasked with evaluating research papers.\" To improve it, perhaps add that the task is to predict acceptance, and specify the output format. But the system prompt is supposed to set the role and approach. So maybe the improved prompt should include instructions on how to evaluate, like considering methodology, originality, significance, etc., and the output format. \n\nBut the user's constraints say the system prompt must be under 500 characters, which is already met. Also, the human prefix (which might be the input given to the model) must retain the output format instruction. Wait, the user's constraints mention the human prefix under 800 characters, but the variable here is the system prompt. Maybe the user is confused. Let me recheck the problem.\n\nThe variable is the system prompt. The constraints include Constraint 3: The human prefix MUST retain the output format instruction: \\boxed{Accept} or \\boxed{Reject}. Wait, the human prefix is the input to the model, which is the user's message. But the variable here is the system prompt. Maybe the user is mixing up the system prompt and the input. \n\nWait, the original variable is the system prompt. The constraints mention the human prefix (which is the input to the model) must have the output format. So perhaps the system prompt is separate. But the user's role is to improve the system prompt. \n\nSo the system prompt needs to set the AI's role and approach. The original is too short. The improved version should include the task of evaluating papers and predicting acceptance, with the output format. But the system prompt is the instruction given to the model, so it should tell the model what to do. \n\nSo perhaps the improved system prompt should be: \"You are an expert academic reviewer tasked with evaluating research papers and predicting their acceptance. Provide your decision in the format: \\boxed{Accept} or \\boxed{Reject}.\" But wait, the output format instruction is for the human prefix, not the system prompt. Hmm, this is confusing. \n\nWait, the user's constraints mention that the human prefix (which is the input to the model) must retain the output format instruction. So the system prompt is separate. The system prompt is the role, and the human prefix is the input. So the system prompt should be the role, and the human prefix (the input) should include the task and output format. \n\nBut the user is asking to improve the variable, which is the system prompt. So the original variable is the system prompt, and the user wants it to be improved. The constraints for the system prompt are: under 500 characters, and the human prefix (input) must have the output format. \n\nWait, maybe the user's constraints are a bit mixed. The original variable is the system prompt. The user wants to improve it. The constraints are for the system prompt and the human prefix. But the human prefix is not part of the variable. \n\nThis is getting a bit tangled. Let me focus on the original variable and the constraints. The original is \"You are an expert academic reviewer tasked with evaluating research papers.\" The user wants to improve it. The constraints are that the system prompt must be under 500 characters (which it is), the human prefix (input) must be under 800 characters and include the output format. But the variable is the system prompt. \n\nPerhaps the user is asking to improve the system prompt to include the task of predicting acceptance and the output format. But the output format is part of the human prefix, not the system prompt. However, the user's constraints say that the human prefix must retain the output format instruction. \n\nWait, maybe the user is referring to the system prompt needing to include the output format. But that's not clear. Let me re-read the constraints.\n\nConstraints:\nConstraint 1: Keep the system prompt under 500 characters\nConstraint 2: Keep the human prefix under 800 characters\nConstraint 3: The human prefix MUST retain the output format instruction: \\boxed{Accept} or \\boxed{Reject}\nConstraint 4: The human prefix should clearly state the task is predicting paper acceptance\n\nSo the system prompt is separate from the human prefix. The system prompt is the variable being improved. The human prefix is the input given to the model, which must include the output format and the task. \n\nBut the user is only improving the system prompt. So the original system prompt is \"You are an expert academic reviewer tasked with evaluating research papers.\" The user wants to improve it. The constraints for the system prompt are just the length. But the user's feedback might have indicated that the original is too vague. \n\nSo the improved system prompt should specify that the AI is to evaluate papers and predict acceptance, and perhaps include criteria. But the system prompt is the role, not the instructions. So maybe the improved system prompt should be: \"You are an expert academic reviewer tasked with evaluating research papers and predicting their acceptance. Your decision must be in the format: \\boxed{Accept} or \\boxed{Reject}.\" But the output format instruction is for the human prefix, not the system prompt. \n\nWait, the user's constraint 3 says the human prefix must retain the output format. So the system prompt doesn't need to include that. The human prefix (the input to the model) should have the task and output format. \n\nBut the user is only improving the system prompt. So the original system prompt is too generic. The improved version should be more specific about the role. For example, adding that the AI should assess the paper's contribution, methodology, and significance, and predict acceptance. \n\nSo the improved system prompt could be: \"You are an expert academic reviewer tasked with evaluating research papers. Assess the paper's originality, methodology, significance, and clarity. Predict whether it should be accepted, providing your decision in the format: \\boxed{Accept} or \\boxed{Reject}.\" Wait, but the output format instruction is for the human prefix. \n\nThis is confusing. Maybe the user made a mistake in the constraints, but I have to follow them. The system prompt should be under 500 characters. The original is under. The improved version should be more detailed. \n\nPerhaps the improved system prompt should be: \"You are an expert academic reviewer tasked with evaluating research papers. Your evaluation should consider the paper's originality, methodology, significance, and clarity. Predict whether it should be accepted, providing your decision in the format: \\boxed{Accept} or \\boxed{Reject}.\" \n\nBut the output format instruction is for the human prefix, not the system prompt. However, the user's constraints might be mixing up the system prompt and the human prefix. \n\nAlternatively, maybe the user wants the system prompt to include the output format. But that's not standard. Typically, the output format is part of the human input. \n\nGiven the confusion, perhaps the best approach is to make the system prompt more detailed about the evaluation criteria and the task, even if the output format is part of the human prefix. \n\nSo, the improved system prompt would be: \"You are an expert academic reviewer tasked with evaluating research papers. Assess the paper's originality, methodology, significance, and clarity. Predict whether it should be accepted, providing your decision in the format: \\boxed{Accept} or \\boxed{Reject}.\" \n\nBut the output format instruction is part of the human prefix. However, the user's constraints say that the human prefix must include it. So the system prompt doesn't need to include it. \n\nTherefore, the improved system prompt should focus on the role and approach, like specifying the criteria for evaluation. \n\nSo, the final improved system prompt could be: \"You are an expert academic reviewer tasked with evaluating research papers. Your evaluation should consider the paper's originality, methodology, significance, and clarity. Predict whether it should be accepted.\" \n\nBut the user's constraint 4 says the human prefix should clearly state the task is predicting paper acceptance. So the system prompt doesn't need to state that. \n\nHmm, this is tricky. Given the constraints, the system prompt should be concise, under 500 characters, and the human prefix (input) should have the output format. The original system prompt is too vague. The improved version should add more details about the evaluation criteria. \n\nThus, the improved system prompt would be: \"You are an expert academic reviewer tasked with evaluating research papers. Assess the paper's originality, methodology, significance, and clarity. Predict whether it should be accepted.\" \n\nThat's under 500 characters. It specifies the criteria and the task. The human prefix would then include the output format. \n\nSo the final answer would be the improved system prompt as above.\n</think>\n\n<IMPROVED_VARIABLE>You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.</IMPROVED_VARIABLE>", "message": "TextualGradientDescent optimizer response"}
{"name": "textgrad", "msg": "TextualGradientDescent updated text", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "187", "funcName": "step", "created": "1768638493.063267", "msecs": "63.0", "relativeCreated": "89893.76449584961", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "parameter.value": "You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.", "message": "TextualGradientDescent updated text"}
{"name": "textgrad", "msg": "TextualGradientDescent prompt for update", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "165", "funcName": "_update_prompt", "created": "1768638493.063365", "msecs": "63.0", "relativeCreated": "89893.86248588562", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "prompt": "Here is the role of the variable you will improve: <ROLE>Task instruction prefix that appears before each paper, specifying the task (accept/reject prediction) and output format (\\boxed{Accept} or \\boxed{Reject})</ROLE>.\n\nThe variable is the text within the following span: <VARIABLE> I am giving you a paper. I want to predict (...) \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate </VARIABLE>\n\nHere is the context and feedback we got for the variable:\n\n<CONTEXT></CONTEXT>\n\nImprove the variable (Task instruction prefix that appears before each paper, specifying the task (accept/reject prediction) and output format (\\boxed{Accept} or \\boxed{Reject})) using the feedback provided in <FEEDBACK> tags.\nYou must follow the following constraints:\n\n<CONSTRAINTS>Constraint 1: Keep the system prompt under 500 characters\nConstraint 2: Keep the human prefix under 800 characters\nConstraint 3: The human prefix MUST retain the output format instruction: \\boxed{Accept} or \\boxed{Reject}\nConstraint 4: The human prefix should clearly state the task is predicting paper acceptance</CONSTRAINTS>\n\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.", "message": "TextualGradientDescent prompt for update"}
{"name": "textgrad", "msg": "TextualGradientDescent optimizer response", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "179", "funcName": "step", "created": "1768638515.1518562", "msecs": "151.0", "relativeCreated": "111982.35368728638", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "optimizer.response": "<think>\nOkay, let's tackle this. The user wants to improve the task instruction prefix for a paper acceptance prediction task. The original variable is a bit wordy and might not be clear enough. Let me check the constraints again.\n\nFirst, the role is to specify the task (accept/reject prediction) and the output format. The original text mentions \"I am giving you a paper. I want to predict (...)\" and then the example with boxes. But maybe it's too long. The constraints say the system prompt must be under 500 characters, and the human prefix under 800. Also, the output format must be retained, and the task must be clear.\n\nLooking at the original variable: \"I am giving you a paper. I want to predict (...) \\boxed{Reject} - Note: ICLR generally has a ~30% acceptance rate\". The note about the acceptance rate might be helpful but could be redundant. Maybe the task is clear enough without it. Also, the original has some ellipsis which might not be necessary. \n\nThe user's feedback isn't provided, but the context is empty. So I need to make sure the instruction is concise, includes the task, output format, and maybe the note about the acceptance rate. Wait, the note might be part of the context, but the constraints don't mention it. However, the original includes it, so maybe it's important. But the user's feedback might have said to remove it? Wait, the original variable includes the note, but maybe the feedback wants to remove it to make it shorter. \n\nAlternatively, maybe the note is useful for the model to know the general acceptance rate, but the constraints don't mention that. The main points are: task is to predict accept/reject, output format is boxed. \n\nSo, the improved variable should be concise, under 800 characters, include the task, output format, and perhaps the note. Let me rephrase the original. \n\nOriginal: \"I am giving you a paper. I want to predict (...) \\boxed{Reject} - Note: ICLR generally has a ~30% acceptance rate\"\n\nMaybe simplify to: \"Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has ~30% acceptance rate.\"\n\nCheck character count. Let's count: \"Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has ~30% acceptance rate.\" That's 133 characters. Well under 800. Also, includes the task, output format, and the note. \n\nBut maybe the note is not necessary. The original includes it, but maybe the feedback said to remove it. Since the feedback isn't provided, but the user wants to improve based on the given context. Wait, the original variable includes the note, but the constraints don't mention it. However, the role says to specify the task and output format. The note might be additional context. \n\nAlternatively, maybe the note is part of the task instruction. Let me check the original again. The original says \"I want to predict (...) \\boxed{Reject} - Note: ICLR generally has a ~30% acceptance rate\". The note is a separate line. \n\nPerhaps the improved version should keep the note but make it more concise. Also, the original uses \"I am giving you a paper. I want to predict (...)\" which is a bit redundant. Maybe rephrase to \"Predict if the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR acceptance rate is ~30%.\" \n\nThat's more concise, includes the task, output format, and the note. Also, under the character limits. \n\nAnother consideration: the original uses \"accept/reject prediction\" as the task. The improved version says \"predict if the paper should be accepted or rejected\" which is clearer. \n\nI think that's better. Let me check the constraints again. The human prefix (the variable) must be under 800 characters. The example I have is under that. Also, the output format is retained. The task is clear. The note is included. \n\nSo the final improved variable would be: \"Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\"\n</think>\n\n<IMPROVED_VARIABLE>Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.</IMPROVED_VARIABLE>", "message": "TextualGradientDescent optimizer response"}
{"name": "textgrad", "msg": "TextualGradientDescent updated text", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "187", "funcName": "step", "created": "1768638515.1524353", "msecs": "152.0", "relativeCreated": "111982.93280601501", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "parameter.value": "Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.", "message": "TextualGradientDescent updated text"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638516.430122", "msecs": "430.0", "relativeCreated": "113260.61940193176", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Enforcing fairness in private federated learning via the modified method of differential multipliers\n\n# Abstract\nFederated learning with differential privacy, or private federated learning, provides a strategy to train machine learning models while respecting users' privacy. However, differential privacy can disproportionately degrade the performance of the models on under-represented groups, as these parts of the distribution are difficult to learn in the presence of noise. Existing approaches for enforcing fairness in machine learning models have considered the centralized setting, in which the algorithm has access to the users' data. This paper introduces an algorithm to enforce group fairness in private federated learning, where users' data does not leave their devices. First, the paper extends the modified method of differential multipliers to empirical risk minimization with fairness constraints, thus providing an algorithm to enforce fairness in the central setting. Then, this algorithm is extended to the private federated learning setting. The proposed algorithm, FPFL, is tested on a federated version of the Adult dataset and an \"unfair\" version of the FEMNIST dataset. The experiments on these datasets show how private federated learning accentuates unfairness in the trained models, and how FPFL is able to mitigate such unfairness.\n\n\n# 1 INTRODUCTION\n\n\nMachine learning requires data to build models. This data is often private and resides on users' devices. Federated learning (FL) (McMahan et al., 2017) is a strategy where multiple users (or other entities) collaborate in training a model under the coordination of a central server or service provider. In FL, devices share only data statistics (e.g. gradients of the model computed from their data) with the server, and therefore the users' data never leaves their devices.\n\n\nHowever, these statistics can still leak information about the users (see Shokri et al. (2017); Fredrikson et al. (2015) for practical attacks recovering the users' identity and reconstructing the users' face images). To protect against this leakage, differential privacy (DP) can be provided. Differential privacy offers a mathematical guarantee on the maximal amount of information any attacker can obtain about a user after observing the released model. In practice, often (Bhowmick et al., 2018; McMahan et al., 2018a;b; Truex et al., 2019; Granqvist et al., 2020) this guarantee is achieved by adding a certain amount of noise to the individuals' statistics. In this paper, federated learning with differential privacy will be referred to as private federated learning (PFL).\n\n\nThe models trained with PFL are often neural networks, since they work well in many use cases, and turn out to be resistant to the noise added for DP. These models have been successfully trained with PFL for tasks like next word prediction or speaker verification (McMahan et al., 2017; Granqvist et al., 2020). However, they are prone to perpetuating societal biases existing in the data (Caliskan et al., 2017) or to discriminate against certain groups even when the data is balanced (Buolamwini & Gebru, 2018). Moreover, when the training is differentially private, degradation in the performance of these models disproportionately impacts under-represented groups (Bagdasaryan et al., 2019). More specifically, the accuracy of the model on minority groups is deteriorated to a larger extent than the accuracy for the majority groups.\n\n\nIn the realm of FL, there has been some research studying how to achieve that the performance of the model is similar among devices (Hu et al., 2020; Huang et al., 2020; Li et al., 2019; 2021). However,\n\n\nthis notion of fairness falls short in terms of protecting users from under-represented groups falling foul of disproportionate treatment. For example, Castelnovo et al. (2021, Section 3.6) show that a model that performs well on the majority of the users will have a good score on individual fairness metrics, even if all the users suffering from bad performance belong to the same group. Conversely, there is little work proposing solutions to enforce group fairness, i.e. that the performance is similar among users from different groups. Current work in this area is either limited to reducing demographic parity in logistic regression (Du et al., 2021) or to minimizing the largest loss across the different groups with a minimax optimization (Mohri et al., 2019). Concurrently to this paper, Cui et al. (2021) proposed an approach to enforce group fairness in FL for general models and metrics. However, this approach does not consider DP. Moreover, most prior work does not consider the adverse effects that DP has on the models trained with private federated learning, or is restricted to logistic regression models (Abay et al., 2020).\n\n\nOn the other hand, work studying the trade-offs between privacy and group fairness proposes solutions that are either limited to simple models such as linear logistic regression (Ding et al., 2020), require an oracle (Cummings et al., 2019), scale poorly for large hypothesis classes (Jagielski et al., 2019), or only offer privacy protection for the variable determining the group (Tran et al., 2021; Rodr\u00edguez-G\u00e1lvez et al., 2021). Furthermore, the aforementioned work only considers the central learning paradigm, and the techniques are not directly adaptable to FL.\n\n\nFor all the above reasons, in this paper, we propose an algorithm to train neural networks with PFL while enforcing group fairness. We pose the problem as an optimization problem with fairness constraints and extend the modified method of differential multipliers (MMDM) (Platt & Barr, 1987) to solve such a problem with FL and DP. Hence, the resulting algorithm (i) is applicable to any model that can be learned using stochastic gradient descent (SGD) or any of its variants, (ii) can be tailored to enforce the majority of the group fairness metrics, (iii) can consider any number of attributes determining the groups, and (iv) can consider both classification and regression tasks.\n\n\nThe paper is structured as follows: in Section 2, we review the background on differential privacy, private federated learning, the MMDM algorithm, and group fairness; in Section 3, we present our approach for fair private federated learning; in Section 4, we describe our experimental results; and in Section 5, we conclude with a summary and interpretation of our findings.\n\n\n# 2 BACKGROUND\n\n\nIn this section, we review several aspects on the theory of private federated learning, the MMDM algorithm, and group fairness that are necessary to develop and understand the proposed algorithm.\n\n\n# 2.1 FEDERATED LEARNING\n\n\nThe federated learning setting focuses on learning a model that minimizes the expected value of a loss function  $\\ell$  using a dataset  $d = (z_{1},z_{2},\\ldots ,z_{n})\\in \\mathcal{D}$  of  $n$  samples distributed across  $K$  users. This paper will consider models parametrized by a fixed number of parameters  $\\boldsymbol {w}\\in \\mathbb{R}^{p}$  and differentiable loss functions  $\\ell$ , which includes neural networks.\n\n\nSince the distribution of the data samples  $Z$  is unknown and each user  $k$  is the only owner of their private dataset  $d^{k}$ , the goal of FL is to find the parameters  $\\boldsymbol{w}^{\\star}$  that minimize the loss function across the samples of the users. That is, defining  $L(d^{k},\\boldsymbol {w})\\coloneqq \\sum_{\\boldsymbol {z}\\in d^{k}}\\ell (\\boldsymbol {z},\\boldsymbol {w})$ , to solve\n\n\n$$\n\n$$\n\\boldsymbol {w} ^ {\\star} = \\underset {\\boldsymbol {w} \\in \\mathbb {R} ^ {p}} {\\arg \\min } \\frac {1}{n} \\sum_ {k = 1} ^ {K} L \\left(d ^ {k}, \\boldsymbol {w}\\right). \\tag {1}\n$$\n\n$$\n\n\nThis way, the parameters  $\\boldsymbol{w}$  can be learned with an approximation of SGD. McMahan et al. (2017) suggest that the central server iteratively samples  $m$  users; they compute and send back to the server an approximation of the model's gradient  $\\nabla_{\\boldsymbol{w}}L(d^{k},\\boldsymbol{w})$ ; and finally the server updates the parameters as dictated by gradient descent  $\\boldsymbol{w} \\gets \\boldsymbol{w} - \\frac{1}{n}\\sum_{k}\\eta \\nabla_{\\boldsymbol{w}}L(d^{k},\\boldsymbol{w})$ , where  $\\eta$  is the learning rate. If the users send back the exact gradient the algorithm is known as FederatedSGD. A generalisation of this, FederatedAveraging, involves users running several local epochs of mini-batch SGD and sending up the difference. However, the method in this paper considers an optimization with fairness constraints, requiring the communication of extra information in each iteration, and therefore will extend FederatedSGD.\n\n\n# 2.2 PRIVATE FEDERATED LEARNING\n\n\nPrivate federated learning combines federated learning with differential privacy. Differential privacy (Dwork et al., 2006; 2014) provides a mathematical guarantee by restricting the amount of information that a randomized function  $m$  of a dataset  $d$  leaks about any individual. More precisely, it ensures that the probability that the output of the function  $m$  on the dataset  $d$  belongs to any set of outcomes  $\\mathcal{O}$  is close to the probability that the output of the function  $m$  applied to a neighbouring dataset  $d'$  (i.e. equal to  $d$  but where all samples from any individual are removed or replaced) belongs to said set of outcomes. Formally, a randomized function  $m$  is  $(\\epsilon, \\delta)$ -differentially private if\n\n\n$$\n\n$$\n\\mathbb {P} [ \\boldsymbol {m} (d) \\in \\mathcal {O} ] \\leq e ^ {\\epsilon} \\mathbb {P} [ \\boldsymbol {m} (d ^ {\\prime}) \\in \\mathcal {O} ] + \\delta , \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\epsilon$  and  $\\delta$  are parameters that determine the strength of the guarantee. Two useful properties of DP are that it composes (i.e. the combination of DP functions is DP) and that post-processing does not undo its privacy guarantees (Dwork et al., 2014). These properties ensure that the modifications that we will make to FederatedSGD will keep the algorithm differentially private.\n\n\nTypically, the functions of interest are not differentially private. For this reason, they are privatized with \"privacy mechanisms\", which apply a random transformation to the output of the function on the dataset. A standard mechanism is the Gaussian mechanism, which adds Gaussian noise to the function's output, calibrating the noise variance with the privacy parameters  $(\\epsilon, \\delta)$  and the maximum individual's contribution to the function's output measured by the  $\\ell_2$  norm.\n\n\nIn the context of FederatedSGD, the functions to privatize are the gradient estimates sent to the server. The individual's contribution is the gradient computed on their data. This gradient is \"norm-clipped\" (Abadi et al., 2016), i.e. it is scaled down so its  $\\ell_2$  norm is at most a predetermined value  $C$ . Then, Gaussian noise is added to the sum of contributions, which is sent to the server.\n\n\nApplying the privacy mechanism to a sum of contributions is a form of central DP. An alternative is local DP (Dwork et al., 2014), where the privacy mechanism is applied to the statistics before leaving the device. However, models trained with local DP often suffer from low utility (Granqvist et al., 2020), and are thus not considered in this paper. Central DP can involve trust in the server that updates the model, or a trusted third party separated from the model, which is how DP was originally formulated. This paper instead assumes the noisy sum is computed through secure multi-party computation (Goryczka & Xiong, 2015), such as secure aggregation (Bonawitz et al., 2017). This is a cryptographic method that ensures the server only sees the sum of contributions. A crucial limitation of secure aggregation is that it can only compute sums, so the novel method introduced in Section 3 is restricted to working only on sums of individuals' statistics.\n\n\n# 2.3 THE MODIFIED METHOD OF DIFFERENTIAL MULTIPLIERS\n\n\nUltimately, our objective is to find a parametric model that minimizes a differentiable loss function and respects some fairness constraints. Thus, this subsection reviews a constrained differential optimization algorithm, the modified method of differential multipliers (MMDM) (Platt & Barr, 1987).\n\n\nThis algorithm tries to find a solution to the following constrained optimization problem:\n\n\n$$\n\n$$\n\\boldsymbol {w} ^ {\\star} = \\underset {\\boldsymbol {w} \\in \\mathbb {R} ^ {p}} {\\arg \\min } \\phi (\\boldsymbol {w}) \\quad \\text {s . t .} \\boldsymbol {g} (\\boldsymbol {w}) = \\boldsymbol {0}, \\tag {P1}\n$$\n\n$$\n\n\nwhere  $\\phi : \\mathbb{R}^p \\to \\mathbb{R}$  is the function to minimize,  $\\pmb{g}(\\pmb{w}) = (g_1(\\pmb{w}), g_2(\\pmb{w}), \\dots, g_r(\\pmb{w}))$  is the concatenation of  $r$  constraint functions, and  $\\{\\pmb{w} \\in \\mathbb{R}^p : \\pmb{g}(\\pmb{w}) = 0\\}$  is the solution subspace.\n\n\nThe algorithm consists of solving the following set of differential equations resulting from the Lagrangian of (P1) with an additional quadratic penalty  $cg(w)^2$  using gradient descent/ascent. This results in an algorithm that applies these updates iteratively:\n\n\n$$\n\n$$\n\\left\\{ \\begin{array}{l} \\boldsymbol {\\lambda} \\leftarrow \\boldsymbol {\\lambda} + \\gamma \\boldsymbol {g} (\\boldsymbol {w}) \\\\ \\boldsymbol {w} \\leftarrow \\boldsymbol {w} - \\eta \\left(\\nabla_ {\\boldsymbol {w}} \\phi (\\boldsymbol {w}) + \\boldsymbol {\\lambda} ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w}) + c \\boldsymbol {g} (\\boldsymbol {w}) ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w})\\right) \\end{array} , \\right. \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\lambda \\in \\mathbb{R}^r$  is a Lagrange (or dual) multiplier,  $c\\in \\mathbb{R}_+$  is a damping parameter,  $\\eta$  is the learning rate of the model parameters, and  $\\gamma$  is the learning rate of the Lagrange multiplier.\n\n\nIntuitively, these updates gradually fulfill (P1). Updating the parameters  $\\nabla_{\\boldsymbol{w}}\\phi(\\boldsymbol{w})$  and  $\\nabla_{\\boldsymbol{w}}g(\\boldsymbol{w})$  respectively enforce function minimization and constraint's satisfaction. Then, the multiplier  $\\lambda$  and the multiplicative factor  $cg(\\boldsymbol{w})$  control how strongly the constraints' violations are penalized.\n\n\nA desirable property of MMDM is that for small enough learning rates  $\\eta, \\gamma$  and large enough damping parameter  $c$ , there is a region comprising the vicinity of each constrained minimum such that if the parameters' initialization is in that region and the parameters remain bounded, then the algorithm converges to a constrained minimum (Platt & Barr, 1987). Intuitively, the term  $cg(\\boldsymbol{w}) \\nabla_{\\boldsymbol{w}} g(\\boldsymbol{w})$  enforces a quadratic shape on the optimization search space to the neighbourhood of the solution subspace, and this local behavior is stronger the larger the damping parameter  $c$ . Thus, the algorithm converges to a minimum if the parameters' initialization is in this locally quadratic region.\n\n\n# 2.4 GROUP FAIRNESS\n\n\nIn this subsection, we mathematically formalize what group fairness means. To ease the exposition, we describe this notion in the central setting, i.e. where the data is not distributed across users.\n\n\nConsider a dataset  $d = (z_{1}, z_{2}, \\ldots, z_{n})$  of  $n$  samples. The dataset is partitioned into groups from  $\\mathcal{A}$  such that each sample  $z_{i}$  belongs to group  $a_{i}$ . Group fairness considers how differently a model treats the samples belonging to each group. Many fairness metrics can be written in terms of the similarity of the expected value of a function of interest  $f$  of the model evaluated on the general population  $d$  with that on the population of each group  $d_{a} = \\{z_{i} \\in d : a_{i} = a\\}$  (Agarwal et al., 2018; Fioretto et al., 2020). That is, if we consider a supervised learning problem, where  $z_{i} = (x_{i}, y_{i}, a_{i})$  and where the output of the model is an approximation  $\\hat{\\pmb{y}}_{i}$  of  $\\pmb{y}_{i}$ , we say a model is fair if\n\n\n$$\n\n$$\n\\mathbb {E} [ f (\\boldsymbol {X}, \\boldsymbol {Y}, \\hat {\\boldsymbol {Y}}) \\mid A = a ] = \\mathbb {E} [ f (\\boldsymbol {X}, \\boldsymbol {Y}, \\hat {\\boldsymbol {Y}}) ] \\text {f o r a l l} a \\in \\mathcal {A}. \\tag {4}\n$$\n\n$$\n\n\nMost of the group fairness literature focuses on the case of binary classifiers, i.e.  $\\hat{y}_i \\in \\{0,1\\}$ , and on the binary group case, i.e.  $\\mathcal{A} = \\{0,1\\}$ . However, many of the fairness metrics can be extended to general output spaces  $\\mathcal{V}$  and categorical groups  $\\mathcal{A}$ . It is common that the function  $f$  is the indicator function  $\\mathbb{1}$  of some logical relationship between the random variables, thus turning (4) to an equality between probabilities. As an example, we describe two common fairness metrics that will be used later in the paper. For a comprehensive survey of different fairness metrics and their inter-relationships, please refer to Verma & Rubin (2018); Castelnovo et al. (2021).\n\n\nFalse negative rate (FNR) parity (or equal opportunity) (Hardt et al., 2016) This fairness metric is designed for binary classification and binary groups. It was originally defined as equal true positive rate between the groups, which is equivalent to an equal FNR between each group and the overall population. That is, if we let  $f(\\mathbf{X}, Y, \\hat{Y}) = \\mathbb{E}[\\mathbb{1}(\\hat{Y} = 0) \\mid Y = 1]$  then (4) reduces to\n\n\n$$\n\n$$\n\\mathbb {P} [ \\hat {Y} = 0 \\mid Y = 1, A = a ] = \\mathbb {P} [ \\hat {Y} = 0 \\mid Y = 1 ] \\text {f o r a l l} a \\in \\mathcal {A}. \\tag {5}\n$$\n\n$$\n\n\nThis is usually a good metric when the target variable  $Y$  is something positive such as being granted a loan or hired for a job, since we want to minimize the group disparity of misclassification among the individuals deserving such a loan or such a job (Hardt et al., 2016; Castelnovo et al., 2021).\n\n\nAccuracy parity (or overall misclassification rate) (Zafar et al., 2017) This fairness metric is also designed for binary classification and binary groups. Nonetheless, it applies well to general tasks and categorical groups. In this case, if we let  $f(\\boldsymbol{X}, Y, \\hat{Y}) = \\mathbb{1}(\\hat{Y} = Y)$  then (4) reduces to\n\n\n$$\n\n$$\n\\mathbb {P} [ \\hat {Y} = Y \\mid A = a ] = \\mathbb {P} [ \\hat {Y} = Y ] \\text {f o r a l l} a \\in \\mathcal {A}. \\tag {6}\n$$\n\n$$\n\n\nThis is usually a good metric when there is not a clear positive or negative semantic meaning to the target variable, and also when this variable is not binary.\n\n\n# 3 AN ALGORITHM FOR FAIR AND PRIVATE FEDERATED LEARNING (FPFL)\n\n\nIn this section, we describe the proposed algorithm to enforce fairness in PFL of parametric models learned with SGD. First, we describe an adaptation of the MMDM algorithm to enforce fairness in standard central learning. Then, we extend that algorithm to PFL.\n\n\n# 3.1 ADAPTING THE MMDM ALGORITHM TO ENFORCE FAIRNESS\n\n\nConsider a dataset  $d = (z_{1}, z_{2}, \\ldots, z_{n})$  of  $n$  samples. The dataset is partitioned into groups from  $\\mathcal{A}$  such that each sample  $z_{i}$  belongs to group  $a_{i}$ . Then, let us consider also a supervised learning setting where  $z_{i} = (x_{i}, y_{i}, a_{i})$ , the model's output  $\\hat{y}_{i}$  approximates  $y_{i}$ , and the model is parametrized by the parameters  $\\boldsymbol{w} \\in \\mathbb{R}^{p}$ . Finally, assume we have no information about the variables'  $(X, Y, A)$  distribution apart from the available samples.\n\n\nWe concern ourselves with the task of finding the model's parameters  $\\boldsymbol{w}^{\\star}$  that minimize a loss function  $\\ell$  across the data samples and enforce a measure of fairness on the model. That is, we substitute the expected values of the fairness constraints in (4) with empirical averages:\n\n\n$$\n\n$$\n\\boldsymbol {w} ^ {\\star} = \\underset {\\boldsymbol {w} \\in \\mathbb {R} ^ {p}} {\\arg \\min } \\frac {L (d , \\boldsymbol {w})}{n} \\quad \\text {s . t .} \\left| \\frac {F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}} \\right| \\leq \\alpha , \\text {f o r a l l} a \\in \\mathcal {A}, \\tag {P2}\n$$\n\n$$\n\n\nwhere  $L(d, \\boldsymbol{w}) \\coloneqq \\sum_{\\boldsymbol{z} \\in d} \\ell(\\boldsymbol{z}, \\boldsymbol{w}), F(d', \\boldsymbol{w}) \\coloneqq \\sum_{\\boldsymbol{z} \\in d'} f(\\boldsymbol{z}, \\boldsymbol{w}), d'$  is a subset of  $d$  that varies among different fairness metrics,  $n'$  is the number of samples in  $d'$ ,  $d'_a$  is the set of samples in  $d'$  such that  $a_i = a$ ,  $n'_a$  is the number of samples in  $d'_a$ ,  $f$  is the function employed for the fairness metric definition, and  $\\alpha$  is a tolerance threshold. The subset  $d'$  is chosen based on the function  $f$  used for the fairness metric: if the fairness function does not involve any conditional expectation (e.g. accuracy parity), then  $d' = d$ ; if, on the other hand, the subset involves a conditional expectation, then the  $d'$  is the subset of  $d$  where that condition holds, e.g. when the fairness metric is FNR parity  $d' = \\{(x, y, a) \\in d : y = 1\\}$ . We exclude the case where the conditioning in the expectation is  $\\hat{Y}$ , thus excluding predictive parity as a target fairness metric.\n\n\nNote that the constraints are not strict, meaning that there is a tolerance  $\\alpha$  for how much the function  $f$  can vary between certain groups and the overall population. The reason for this choice is twofold. First, it facilitates the training since the solution subspace is larger. Second, it is known that some fairness metrics, such as FNR parity, are incompatible with DP and non-trivial accuracy. However, if the fairness metric is relaxed, fairness, accuracy, and privacy can coexist (Jagielski et al., 2019; Cummings et al., 2019).\n\n\nThis way, we may re-write (P2) in the form of (P1) to solve the problem with MMDM. To do so we let  $\\phi(\\boldsymbol{w}) = L(d, \\boldsymbol{w}) / n$  and  $\\boldsymbol{g}(\\boldsymbol{w}) = (g_0(\\boldsymbol{w}), g_1(\\boldsymbol{w}), \\ldots, g_{|\\mathcal{A}| - 1}(\\boldsymbol{w}))$ , where\n\n\n$$\n\n$$\ng _ {a} (\\boldsymbol {w}) = \\left\\{ \\begin{array}{l l} h _ {a} (\\boldsymbol {w}) & \\text {i f} h _ {a} (\\boldsymbol {w}) \\geq 0 \\\\ 0 & \\text {o t h e r w i s e} \\end{array} \\right. \\quad \\text {a n d} \\quad h _ {a} (\\boldsymbol {w}) := \\left| \\frac {F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}} \\right| - \\alpha . \\tag {7}\n$$\n\n$$\n\n\nTherefore, the parameters are updated according to\n\n\n$$\n\n$$\n\\left\\{ \\begin{array}{l} \\boldsymbol {\\lambda} \\leftarrow \\boldsymbol {\\lambda} + \\gamma \\boldsymbol {g} (\\boldsymbol {w}) \\\\ \\boldsymbol {w} \\leftarrow \\boldsymbol {w} - \\eta \\left(1 / _ {n} \\nabla_ {\\boldsymbol {w}} L (d, \\boldsymbol {w}) + \\boldsymbol {\\lambda} ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w}) + c \\boldsymbol {g} (\\boldsymbol {w}) ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w})\\right) \\end{array} , \\right. \\tag {8}\n$$\n\n$$\n\n\nwhere we note that  $\\nabla_{\\pmb{w}}\\pmb{g}(\\pmb{w}) = (\\nabla_{\\pmb{w}}g_0(\\pmb{w}),\\nabla_{\\pmb{w}}g_1(\\pmb{w}),\\dots,\\nabla_{\\pmb{w}}g_{|\\mathcal{A}| - 1}(\\pmb{w}))$  and\n\n\n$$\n\n$$\n\\nabla_ {\\boldsymbol {w}} g _ {a} (\\boldsymbol {w}) = \\left\\{ \\begin{array}{l l} \\operatorname {s i g n} \\left(\\frac {F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}}\\right) \\left(\\frac {\\nabla_ {\\boldsymbol {w}} F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {\\nabla_ {\\boldsymbol {w}} F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}}\\right) & \\text {i f} h _ {a} (\\boldsymbol {w}) \\geq 0 \\\\ 0 & \\text {o t h e r w i s e} \\end{array} . \\right. \\tag {9}\n$$\n\n$$\n\n\nNow, the fairness-enforcing problem (P2) can be solved with gradient descent/ ascent or mini-batch stochastic gradient descent/ ascent, where instead of the full dataset  $d$ ,  $d'$ , and  $d_{a}^{\\prime}$ , one considers batches  $b$ ,  $b^{\\prime}$ , and  $b_{a}^{\\prime}$  (or subsets) of that dataset. Moreover, it can be learned with DP adapting the DP-SGD algorithm from (Abadi et al., 2016), where a clipping bound and Gaussian noise is included in both the network parameters' and multipliers' individual updates. Nonetheless, there are a series of caveats of doing so. First, the batch size  $|b^{\\prime}|$  should be large enough to, on average, have enough samples of each group  $a \\in \\mathcal{A}$  so that the difference  $\\left(\\frac{F(b', w)}{|b'|} - \\frac{F(b_{a}', w)}{|b_{a}'|}\\right)$  is well estimated. Second, in many situations of interest such as when we want to enforce FNR parity or accuracy parity, the function  $f$  employed for the fairness metric is not differentiable and thus  $\\nabla_{w} F(d', w)$  does not exist. To solve this issue, we resort to estimate the gradient  $\\nabla_{w} F(d', w)$  using a differentiable estimation of the function aggregate  $F(d', w)$ ; see Appendix A.1 for the details on these estimates in the above situations of interest.\n\n\nWe conclude this section noting the similarities and differences of this work and (Tran et al., 2021). Even though their algorithm is derived from first principles on Lagrangian duality, it is ultimately equivalent to an application of the basic method of differential multipliers (BMDM) to solve a problem equivalent to (P2) when  $\\alpha = 0$ . Nonetheless, the two algorithms differ in three main aspects:\n\n- 1. BMDM vs MMDM: BMDM is equivalent to MMDM when  $c = 0$ , that is, when the effect of making the neighbourhood of the solution subspace quadratic is not present. Moreover, the guarantee of achieving a local minimum that respects the constraints does not hold for  $c = 0$  unless the problem is simple (e.g. quadratic programming).\n\n- 2. How they deal with impossibilities in the goal of achieving perfect fairness together with privacy and accuracy. Tran et al. (2021) include a limit  $\\lambda_{\\mathrm{max}}$  to the Lagrange multiplier to avoid floating point errors and reaching trivial solutions, which in our case is taken care by the tolerance  $\\alpha$ , which, in contrast to  $\\lambda_{\\mathrm{max}}$ , is an interpretable parameter.\n\n- 3. This paper uses the exact expression in (9) for the gradient  $\\nabla_{\\boldsymbol{w}}\\left|\\frac{F(d', \\boldsymbol{w})}{n'} - \\frac{F(d_{a}', \\boldsymbol{w})}{n_{a}'}\\right|$ , while Tran et al. (2021) ignore the sign of the difference and use  $\\left|\\frac{\\nabla_{\\boldsymbol{w}} F(d', \\boldsymbol{w})}{n'} - \\frac{\\nabla_{\\boldsymbol{w}} F(d_{a}', \\boldsymbol{w})}{n_{a}'}\\right|$ .\n\n\nIn the next subsection, we extend the algorithm to PFL, which introduces two new differences with (Tran et al., 2021). Firstly, the privacy guarantees will be provided for the individuals, and not only to the group to which they belong; and secondly, the algorithm will be tailored to FL.\n\n\n# 3.2 EXTENDING THE ALGORITHM TO PRIVATE FEDERATED LEARNING\n\n\nIn the federated learning setting we now consider that the dataset  $d$  is distributed across  $K$  users such that each user maintains a local dataset  $d^{k}$  with  $n^k$  samples. This setting applies to both crossdevice and cross-silo FL, since each sample contains the group information  $a_{i}$ . Nonetheless, as in the central setting, the task is to find the model's parameters  $\\boldsymbol{w}^{\\star}$  that minimize the loss function across the data samples of the users while enforcing a measure of fairness to the model. That is, to solve (P2).\n\n\nTo achieve this goal, we might first combine the ideas from FederatedSGD (McMahan et al., 2017) and the previous section to extend the developed adaptation of MMDM to FL. Our adaptation of MMDM cannot be combined with FederatedAveraging; for more details see Appendix A.2. In order to perform the model updates from (8), the central server requires the following statistics:\n\n\n$$\n\n$$\n\\nabla_ {\\boldsymbol {w}} L (d, \\boldsymbol {w}); F \\left(d ^ {\\prime}, \\boldsymbol {w}\\right); \\left[ F \\left(d _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}; \\nabla_ {\\boldsymbol {w}} F \\left(d ^ {\\prime}, \\boldsymbol {w}\\right); \\left[ \\nabla_ {\\boldsymbol {w}} F \\left(d _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}; n ^ {\\prime}; \\left[ n _ {a} ^ {\\prime} \\right] _ {a \\in \\mathcal {A}}. \\tag {10}\n$$\n\n$$\n\n\nHowever, some of these statistics can be obtained from the others:  $F(d', \\boldsymbol{w}) = \\sum_{a \\in \\mathcal{A}} F(d'_a, \\boldsymbol{w})$ ,  $\\nabla_{\\boldsymbol{w}} F(d', \\boldsymbol{w}) = \\sum_{a \\in \\mathcal{A}} \\nabla_{\\boldsymbol{w}} F(d'_a, \\boldsymbol{w})$ , and  $n' = \\sum_{a \\in \\mathcal{A}} n'_a$ . Moreover, as mentioned in the previous section, one might use a sufficiently large batch  $b$  of the data instead of the full dataset for each update. Therefore, we consider an iterative algorithm where, at each iteration, the central server samples a cohort of  $m$  users  $S$  that report a vector with the sufficient statistics for the update, that is\n\n\n$$\n\n$$\n\\boldsymbol {v} ^ {k} = \\left[ \\nabla_ {\\boldsymbol {w}} L \\left(d ^ {k}, \\boldsymbol {w}\\right), \\left[ F \\left(\\left(d ^ {k}\\right) _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\nabla_ {\\boldsymbol {w}} F \\left(\\left(d ^ {k}\\right) _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\left(n ^ {k}\\right) _ {a} ^ {\\prime} \\right] _ {a \\in \\mathcal {A}} \\right]. \\tag {11}\n$$\n\n$$\n\n\nThis way, if we define the batch  $b$  as the sum of the  $m$  users' local datasets  $b \\coloneqq \\sum_{k \\in S} d^k$  and the batch  $b'$  analogously, then the aggregation of each user's vectors results in\n\n\n$$\n\n$$\n\\boldsymbol {v} = \\sum_ {k \\in S} \\boldsymbol {v} ^ {k} = \\left[ \\nabla_ {\\boldsymbol {w}} L (b, \\boldsymbol {w}), \\left[ F \\left(b _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\nabla_ {\\boldsymbol {w}} F \\left(b _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\left| b _ {a} ^ {\\prime} \\right| \\right] _ {a \\in \\mathcal {A}} \\right], \\tag {12}\n$$\n\n$$\n\n\nwhich contains all the sufficient statistics for the parameters' update.\n\n\nFinally, the resulting algorithm, termed Fair PFL or FPFL and described in Algorithm 1, inspired by the ideas from (McMahan et al., 2018b; Truex et al., 2019; Granqvist et al., 2020; Bonawitz et al., 2017) guarantees the users' privacy by (i) making sure the aggregation of the users' sent vectors is done securely with secure aggregation (Bonawitz et al., 2017), and (ii) clipping each users' vectors with a clipping bound  $C$  and employing the Gaussian mechanism to the sum of the clipped vectors (lines 13 and 15). The variance of the Gaussian noise is calculated according to the refined moments accountant privacy analysis from (Wang et al., 2019), taking into account the number of iterations  $T$ , the cohort size  $m$ , the total number of users (or population size)  $K$ , and the privacy parameters  $\\epsilon$  and  $\\delta$ . Note that the post-processing property of DP keeps the algorithm private even if the received noisy vector  $v$  is processed to extract the relevant information for the model's update.\n\n\nBatch size. This algorithm can also be employed using only a fraction of the user's data in each update, i.e. using a batch  $b^{k}$  of their local dataset  $d^{k}$ . Nonetheless, it is desirable (i) to delegate as much computation to the user as possible and (ii) to use as much users' data as possible to have a good approximation of the performance metrics  $F(d_{a}^{\\prime},w)$ , which are needed to enforce fairness.\n\n\n# 4 EXPERIMENTAL RESULTS\n\n\nWe study the performance of the algorithm in two classification tasks. The first task is a binary classification based on some demographic data from the publicly available Adult dataset (Dua & Graff, 2017). The fairness metric considered for this task is FNR parity. The second task is a multi-class classification where there are three different attributes. This task considers accuracy parity as the fairness metric and uses a modification of the publicly available FEMNIST dataset (Caldas et al., 2018), where only digits are considered and the classes are digits written with a black pen on a white sheet, digits written with a blue pen on a white sheet, and digits written with white chalk on a blackboard (see Figure 1).\n\n\n![Figure 1: Samples from the Unfair FEMNIST dataset. Digits written on a white sheet with black pen (top), on a white sheet with blue pen (middle), and on a blackboard with white chalk (bottom).](images/85dfd2b5c6ccd6c20a5a5b9aa63971fd2f1060733ee6962cf97bee00b226c963.jpg)\n\n\nFor the first task, we first compare the performance of\n\n\nthe MMDM algorithm with BMDM, Tran et al. (2021), and vanilla SGD centrally and without privacy. After that, for both tasks, we confirm how FederatedSGD deteriorates the performance of the model for the under-represented classes when clipping and noise (DP) are introduced. Finally, we demonstrate how FPFL can, under the appropriate circumstances, level the performance of the model across the groups without largely decreasing the overall performance of the model.\n\n\nIn all our experiments, the fairness metrics are defined as the maximum difference of the value of a performance measure between the general testing data and each of the groups described by that sensitive attribute. Moreover, the privacy parameters are  $(\\epsilon, \\delta) = (2, 1 / K)$ , where  $K$  is the population size. All experimental details and additional experiments are in Appendix B.\n\n\n# 4.1 RESULTS ON THE ADULT DATASET\n\n\nWe start our experiments comparing the performance and fairness in the central, non-federated, non-private setting. We trained a shallow network (consisting of one hidden layer with 10 hidden units and a ReLU activation function) with these algorithms, where we tried to enforce FNR parity with a tolerance  $\\alpha = 0.02$ . For MMDM, the damping parameter is  $c = 2$ . For Tran et al. (2021), the multiplier threshold is  $\\lambda_{\\mathrm{max}} = 0.05$ . The results after 1,000 iterations are displayed in Table 1, which also shows the gap in other common measures of fairness such as the equalized odds, the demographic parity, or the predictive parity, see e.g. Castelnovo et al. (2021).\n\n\nCompared to Federated SGD, the MMDM algorithm reduces the FNR gap from  $7\\%$  to  $0.5\\%$ , while hardly reducing the accuracy of the model. Similarly, the gap in the equalized odds (EO), which is a stronger fairness notion than the FNR parity, also decreases from around  $7\\%$  to  $3\\%$ . Moreover, the demographic parity (DemP) gap, which considers the probability of predicting one or the other target class, also improves. In terms of predictive parity (PP), which uses the precision as the performance function, the MMDM algorithm did not improve the parity among groups. The BMDM algorithm, which lacks a quadratic term of the fairness function, performs even slightly better in the central scenario. However, the algorithm from Tran et al. (2021), an approximation to BMDM as discussed in Section 3.1, gives only a small improvement. Therefore, we continue our analysis to the federated and private settings only with the BMDM and MMDM algorithms.\n\n\nThe second experiment is to study how federated learning, clipping, and DP decrease the performance for the under-represented groups, thus increasing the fairness gap on the different fairness metrics. For that, we trained the same network with FederatedSGD and versions of this algorithm where only clipping was performed and where both clipping and DP were included. They were trained with a cohort size of  $m = 200$  for  $T = 1,000$  iterations and the model with best training cohort accuracy was selected. The privacy parameters are  $(\\epsilon, \\delta) = (2,5 \\cdot 10^{-5})$ . The results\n\n\nTable 1: Performance in the central (i.e. non-federated) setting on the Adult dataset. The fairness metric optimized is FNR parity. The tolerance for BMDM and MMDM is  $\\alpha = 0.02$ .\n\n\n<table><tr><td>Algorithm</td><td>Accuracy</td><td>FNR gap</td><td>EO gap</td><td>DemP gap</td><td>PP gap</td></tr><tr><td>SGD</td><td>0.857</td><td>0.071</td><td>0.071</td><td>0.113</td><td>0.016</td></tr><tr><td>Tran et al. (2021) without privacy</td><td>0.856</td><td>0.048</td><td>0.048</td><td>0.108</td><td>0.036</td></tr><tr><td>BMDM</td><td>0.857</td><td>0.001</td><td>0.030</td><td>0.094</td><td>0.046</td></tr><tr><td>MMDM (this paper)</td><td>0.856</td><td>0.005</td><td>0.028</td><td>0.091</td><td>0.063</td></tr></table>\n\n\nTable 2: Performance of a neural network on the Adult dataset when trained with FL. The fairness metric considered for BMDM-FL and FPFL is FNR parity and the tolerance is  $\\alpha = 0.02$ .\n\n\n<table><tr><td>Algorithm</td><td>Accuracy</td><td>FNR gap</td><td>EO gap</td><td>DemP gap</td><td>PP gap</td></tr><tr><td>FL</td><td>0.851</td><td>0.121</td><td>0.121</td><td>0.122</td><td>0.037</td></tr><tr><td>BMDM-FL</td><td>0.854</td><td>0.043</td><td>0.043</td><td>0.101</td><td>0.016</td></tr><tr><td>Fair FL</td><td>0.855</td><td>0.036</td><td>0.039</td><td>0.108</td><td>0.033</td></tr><tr><td>FL + Clip</td><td>0.844</td><td>0.169</td><td>0.169</td><td>0.129</td><td>0.051</td></tr><tr><td>BMDM-FL + Clip</td><td>0.851</td><td>0.052</td><td>0.052</td><td>0.105</td><td>0.008</td></tr><tr><td>Fair FL + Clip</td><td>0.853</td><td>0.018</td><td>0.029</td><td>0.090</td><td>0.016</td></tr><tr><td>Private FL</td><td>0.847</td><td>0.148</td><td>0.148</td><td>0.126</td><td>0.041</td></tr><tr><td>BMDM-PFL</td><td>0.850</td><td>0.023</td><td>0.035</td><td>0.097</td><td>0.009</td></tr><tr><td>Fair Private FL</td><td>0.851</td><td>0.001</td><td>0.027</td><td>0.087</td><td>0.042</td></tr></table>\n\n\nare displayed in Table 2. The network becomes less fair under all the metrics considered, compared with the central setting. Moreover, the introduction of clipping largely increases the unfairness of the models reaching more than a  $16\\%$  gap in FNR. The addition of DP requires an increase in cohort size from 200 to 1,000, but does not have a larger effect on the unfairness of the models (see Table 5 in Appendix B.1 for the details). These observations are in line with (Bagdasaryan et al., 2019), where they note that the under-represented groups usually have the higher loss gradients and thus clipping affects them more than the majority groups.\n\n\nAfter that, we repeated the above experiment with FPFL and BMDM with DP. FPFL and BMDM with DP converged to a solution faster than FederatedSGD, and thus the models were trained for only  $T = 250$  iterations. Here the model with the best training cohort accuracy while respecting the fairness condition on the training cohort data was selected. Note that the fairness condition is evaluated with noisy statistics, so a model may be deemed as fair while slightly violating the desired constraints. The results are also included in Table 2 to aid the comparison. We note how, similarly to the central case, the model trained with FPFL manages to enforce the fairness constraints while keeping a similar accuracy. In contrast, BMDM seems to not be able to find network weights that respect the fairness conditions, as expected by the less favorable (\"less convex\") loss surface around these solutions. Moreover, clipping does not seem to affect largely the performance of FPFL since it compensates the gradient loss clipping with the fairness enforcement.\n\n\n# 4.2 RESULTS ON THE UNFAIR FEMNIST DATASET\n\n\nWe start our experiments confirming again the hypothesis and findings from Bagdasaryan et al. (2019) that clipping and DP disproportionately affect under-represented groups. For that, we trained a convolutional network (consisting of 2 convolution layers with kernel of size  $5 \\times 5$ , stride of 2, ReLU activation function, and 32 and 64 filters respectively) with FederatedSGD and versions of this algorithm where only clipping was performed and where both clipping and DP were included. For the FEMNIST experiments, the privacy parameters are  $(\\epsilon, \\delta) = (2, 2.5 \\cdot 10^{-4})$  and the damping parameter is  $c = 20$ . They were trained with a cohort size of  $m = 100$  for  $T = 2,000$  iterations and the model with best training cohort accuracy was selected. The results are displayed in Table 3. Similarly to before, clipping increases the accuracy gap from  $13\\%$  to almost  $17\\%$ . In this case, since the number of users  $K$  is small, the DP noise is large compared to the users' statistics. Therefore, the\n\n\naccuracy drops from more than  $94\\%$  with clipping to  $80.7\\%$  when DP is also used, and the accuracy gap increases to more than  $40\\%$ .\n\n\nThe second experiment tests whether FPFL can remedy the unfairness without decreasing accuracy too much. We trained the same convolutional network, again for  $T = 2,000$  iterations, and selected the model with the best training cohort accuracy that respected the fairness condition on the training cohort. When DP noise is not included, FPFL reduces the accuracy gap with FederatedSGD by around  $9\\%$  while keeping the accuracy within  $1\\%$ . We note how, as before, clipping largely does not affect the ability of FPFL to enforce fairness. However, note that since the data is more non-i.i.d. than before (i.e. there are more differences between the distribution of each user) the models that are deemed fair in the training cohort may not be as fair in the general population, and now we see a larger gap between the desired tolerance  $\\alpha = 0.04$  and the obtained accuracy gap from FPFL without noise (0.047 and 0.053 without and with clipping).\n\n\nTable 3: Performance of a convolutional network on the Unfair FEMNIST when trained with different algorithms: like Table 2, starting with FederatedSGD and ending with FPFL. The fairness metric considered for FPFL is accuracy parity and the tolerance is  $\\alpha = 0.04$ .\n\n\n<table><tr><td>Algo.</td><td>Population</td><td>Accuracy</td><td>Acc. gap</td></tr><tr><td colspan=\"4\">m = 100</td></tr><tr><td>FL</td><td>K</td><td>0.960</td><td>0.134</td></tr><tr><td>FFL</td><td>K</td><td>0.950</td><td>0.047</td></tr><tr><td>FL + Clip</td><td>K</td><td>0.946</td><td>0.166</td></tr><tr><td>FFL + Clip</td><td>K</td><td>0.954</td><td>0.053</td></tr><tr><td>PFL</td><td>K</td><td>0.807</td><td>0.409</td></tr><tr><td>FPFL</td><td>K</td><td>0.093</td><td>0.015</td></tr><tr><td colspan=\"4\">m = 2,000</td></tr><tr><td>PFL</td><td>100K</td><td>0.951</td><td>0.157</td></tr><tr><td>FPFL</td><td>100K</td><td>0.903</td><td>0.074</td></tr><tr><td>PFL</td><td>1,000K</td><td>0.951</td><td>0.153</td></tr><tr><td>FPFL</td><td>1,000K</td><td>0.927</td><td>0.073</td></tr></table>\n\n\nWhen DP is used, the noise is too large for FPFL to function properly and often the sign of the constraints' gradient, see (9), flips. Note that in the estimation of the performance function, i.e.  $F(d_{a}, \\boldsymbol{w}) / n_{a}$ , both the numerator and denominator are obtained from a noisy vector, thus making the estimators more sensitive to noise than the estimators for plain FederatedSGD. This is due to two main factors: (i) the larger the model, the larger the aggregation of the gradients for each weight, and thus more noise needs to be added; and (ii) the smaller the amount of users, the larger the noise that needs to be added.\n\n\nFor this reason, we considered the hypothetical scenario where the population, used for calculating the DP noise, is 100 and 1,000 times larger, which is a conservative assumption for federated learning deployments (Apple, 2017). Then, we repeated the experiment with DP FederatedSGD and FPFL and increased the cohort size to  $m = 2,000$ . In this scenario, DP FederatedSGD maintained an accuracy gap of more than  $15\\%$  while FPFL reduced this gap to less than a half in both cases. Nonetheless, the accuracy still decreases slightly, with a reduction of around  $5\\%$  and  $2\\%$  respectively compared with DP FederatedSGD.\n\n\n# 5 CONCLUSION\n\n\nIn this paper, we studied and proposed a solution to the problem of group fairness in private federated learning. For this purpose, we adapt the modified method of multipliers (MMDM) (Platt & Barr, 1987) to empirical loss minimization with fairness constraints, producing an algorithm for enforcing fairness in central learning. Then, we extend this algorithm to private federated learning.\n\n\nThrough experiments in the Adult (Dua & Graff, 2017) and a modified version of the FEM-NIST (Caldas et al., 2018) datasets, we first confirm previous knowledge that DP disproportionately affects performance on under-represented groups (Bagdasaryan et al., 2019), with the further observation that this is true for many different fairness metrics, and not just for accuracy parity. The proposed FPFL algorithm is able to remedy this unfairness even in the presence of DP.\n\n\nLimitations and future work. The FPFL algorithm is more sensitive to DP noise than other algorithms for PFL. This requires increasing the cohort size or ensuring that enough users take part in training. Still, for the experiments, the number of users required is still lower (more than an order of magnitude) than the usual amount of users available in deployments of federated learning settings (Apple, 2017). In the future, we intend to scale the algorithm to larger models and empirically integrate it with faster training methods like Federated Averaging (McMahan et al., 2017).\n\n\n# REFERENCES\n\n- Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security (CCS), pp. 308-318, 2016.\n\n- Annie Abay, Yi Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube Chuba, and Heiko Ludwig. Mitigating bias in federated learning. arXiv preprint arXiv:2012.02447, 2020.\n\n- Alekh Agarwal, Alina Beygelzimer, Miroslav Dud\u00edk, John Langford, and Hanna Wallach. A reductions approach to fair classification. In International Conference on Machine Learning, pp. 60-69. PMLR, 2018.\n\n- Differential Privacy Team Apple. Learning with privacy at scale. Technical report, Apple, 2017.\n\n- Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate impact on model accuracy. Advances in Neural Information Processing Systems (NeurIPS), 32: 15479-15488, 2019.\n\n- Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan Rogers. Protection against reconstruction and its applications in private federated learning. arXiv preprint arXiv:1812.00984, 2018.\n\n- Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 1175-1191, 2017.\n\n- Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pp. 77-91. PMLR, 2018.\n\n- Sebastian Caldas, Peter Wu, Tian Li, Jakub Kone\u010dny, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018. URL https://arxiv.org/abs/1812.01097.\n\n- Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183-186, 2017.\n\n- Alessandro Castelnovo, Riccardo Crupi, Greta Greco, and Daniele Regoli. The zoo of fairness metrics in machine learning. arXiv preprint arXiv:2106.00467, 2021.\n\n- Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 2921-2926. IEEE, 2017.\n\n- Sen Cui, Weishen Pan, Jian Liang, Changshui Zhang, and Fei Wang. Addressing algorithmic disparity and performance inconsistency in federated learning, 2021.\n\n- Rachel Cummings, Varun Gupta, Dhamma Kimpara, and Jamie Morgenstern. On the compatibility of privacy and fairness. In Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization, pp. 309-315, 2019.\n\n- Jiahao Ding, Xinyue Zhang, Xiaohuan Li, Junyi Wang, Rong Yu, and Miao Pan. Differentially private and fair classification via calibrated functional mechanism. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 622-629, 2020.\n\n- Wei Du, Depeng Xu, Xintao Wu, and Hanghang Tong. Fairness-aware agnostic federated learning. In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM), pp. 181-189. SIAM, 2021.\n\n- Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.\n\n- Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pp. 265-284. Springer, 2006.\n\n- Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9(3-4):211-407, 2014.\n\n- Ferdinando Fioretto, Pascal Van Hentenryck, Terrence WK Mak, Cuong Tran, Federico Baldo, and Michele Lombardi. Lagrangian duality for constrained deep learning. arXiv preprint arXiv:2001.09394, 2020.\n\n- Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pp. 1322-1333, 2015.\n\n- Slawomir Goryczka and Li Xiong. A comprehensive comparison of multiparty secure additions with differential privacy. IEEE Transactions on Dependable and Secure Computing, 2015. doi: 10.1109/TDSC.2015.2484326.\n\n- Filip Granqvist, Matt Seigel, Rogier van Dalen, \u00c1ine Cahill, Stephen Shum, and Matthias Paulik. Improving on-device speaker verification using federated learning with privacy. Interspeech, pp. 4328-4332, 2020.\n\n- Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in neural information processing systems, 29:3315-3323, 2016.\n\n- Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, and Yaoliang Yu. FedMGDA+: Federated learning meets multi-objective optimization. arXiv preprint arXiv:2006.11489, 2020.\n\n- Wei Huang, Tianrui Li, Dexian Wang, Shengdong Du, and Junbo Zhang. Fairness and accuracy in federated learning. arXiv preprint arXiv:2012.10069, 2020.\n\n- Matthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman. Differentially private fair learning. In International Conference on Machine Learning (ICML), pp. 3000-3008. PMLR, 2019.\n\n- Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning. arXiv preprint arXiv:1905.10497, 2019.\n\n- Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In Proceedings of the 38th International Conference on Machine Learning (ICML), pp. 6357-6368. PMLR, 2021.\n\n- Jingcheng Liu and Kunal Talwar. Private selection from private candidates. In Proceedings of the ACM SIGACT Symposium on Theory of Computing, 2019.\n\n- Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag\u00fcera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1273-1282. PMLR, 2017.\n\n- H Brendan McMahan, Galen Andrew, Ulfar Erlingsson, Steve Chien, Ilya Mironov, Nicolas Papernot, and Peter Kairouz. A general approach to adding differential privacy to iterative training procedures. arXiv preprint arXiv:1812.06210, 2018a.\n\n- H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. In International Conference on Learning Representations (ICLR), 2018b.\n\n- Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In International Conference on Machine Learning, pp. 4615-4625. PMLR, 2019.\n\n- John C Platt and Alan H Barr. Constrained differential optimization. In Proceedings of the 1987 International Conference on Neural Information Processing Systems, pp. 612-621, 1987.\n\n- Borja Rodr\u00edguez-G\u00e1lvez, Ragnar Thobaben, and Mikael Skoglund. A variational approach to privacy and fairness. In 2nd Privacy Preserving Artificial Intelligence Workshop (PPAI) of the AAAI Conference on Artificial Intelligence, 2021. URL https://arxiv.org/abs/2006.06332.\n\n- Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 3-18. IEEE, 2017.\n\n- Cuong Tran, Ferdinando Fioretto, and Pascal Van Hentenryck. Differentially private and fair deep learning: A Lagrangian dual approach. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 9932-9939, 2021.\n\n- Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and Yi Zhou. A hybrid approach to privacy-preserving federated learning. In Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security, pp. 1-11, 2019.\n\n- Sahil Verma and Julia Rubin. Fairness definitions explained. In 2018 IEEE/ACM International Workshop on Software Fairness (FairWare), pp. 1-7. IEEE, 2018.\n\n- Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, PEARU Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cirmrnan, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2.\n\n- Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled R\u00e9nyi differential privacy and analytical moments accountant. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1226-1235. PMLR, 2019.\n\n- Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th international conference on world wide web, pp. 1171-1180, 2017.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638517.921721", "msecs": "921.0", "relativeCreated": "114752.21848487854", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Automated Discovery of Pairwise Interactions from Unstructured Data\n\n# Abstract\nPairwise interactions between perturbations to a system can provide evidence for the causal dependencies of the underlying underlying mechanisms of a system. When observations are low dimensional, hand crafted measurements, detecting interactions amounts to simple statistical tests, but it is not obvious how to detect interactions between perturbations affecting latent variables. We derive two interaction tests that are based on pairwise interventions, and show how these tests can be integrated into an active learning pipeline to efficiently discover pairwise interactions between perturbations. We illustrate the value of these tests in the context of biology, where pairwise perturbation experiments are frequently used to reveal interactions that are not observable from any single perturbation. Our tests can be run on unstructured data, such as the pixels in an image, which enables a more general notion of interaction than typical cell viability experiments, and can be run on cheaper experimental assays. We validate on several synthetic and real biological experiments that our tests are able to identify interacting pairs effectively. We evaluate our approach on a real biological experiment where we knocked out 50 pairs of genes and measured the effect with microscopy images. We show that we are able to recover significantly more known biological interactions than random search and standard active learning baselines.\n\n\n# 1 INTRODUCTION\n\n\nAcross the sciences, measurement of pairwise interaction between perturbations often reveals the existence of underlying mechanisms that single perturbations cannot. For example, quantum entanglement experiments reject classical laws by showing that entangled particles' spins are perfectly anti-correlated once we make a measurement, when classical laws predict independence. In economics, people may be more willing to pay for goods when presented in a bundle than they are to pay for each good in isolation, which reveals complements in the underlying consumer preferences. And in biology\u2014the application area on which we focus\u2014the concept of synthetic lethality (Nijman, 2011) occurs when the simultaneous perturbation of two genes results in cell death while individual perturbations do not, thereby revealing complementary roles in the underlying cellular mechanisms.\n\n\nTo demonstrate a pairwise interaction a scientist has to carefully design three steps of the experiment:\n\n- 1. Measurement: the expert to selects properties of the system to measure to reveal an interaction. E.g., measuring cell viability may reveal synthetic lethality, while the cell's colour will not.\n\n- 2. Hypothesis testing: an interaction is a deviation from what we expect under a null hypothesis that assumes independent effects. The expert needs to predict the outcome under independence and compare this prediction to the actual outcome.\n\n- 3. Selection: there are typically many variables that one could perturb, but only a small fraction will exhibit surprising interactions. The expert selects pairs of perturbations from all possible pairs.\n\n\nMeasuring interactions is further complicated by the fact that measurement and hypothesis testing are interdependent. We can see this in synthetic lethality example, where if we choose to measure the fraction of cells that survived the respective perturbations, then in order to test independence we would need to test whether  $P(\\text{surviv}_a \\cap \\text{surviv}_b) = P(\\text{surviv}_a)P(\\text{surviv}_b)$ . If we had instead measured the fraction of cells that died, we would have needed to test whether  $P(\\text{die}_a \\cup \\text{die}_b) = P(\\text{die}_a) + P(\\text{die}_b) - P(\\text{die}_a)P(\\text{die}_b)$ . The state of the cells in the respective petri dishes is the same in both cases, but correctly testing (in)dependence depends on how the expert chose to measure\n\n\n![Figure 1: The traditional experimental design relies on human experts to choose features of the experimental outcome (e.g., did a cell survive a perturbation?), predict behavior under the null hypothesis to test for interactions, and select interacting pairs from a combinatorial space of perturbations. Our approach automates the selection of perturbants and interaction testing directly from raw signal data (e.g., microscope images of cells).](images/ac130eecd61700bbbe30205cd1b08dc3604b4b404896bdb992668c821df90e45.jpg)\n\n\n![](images/dde1852a2bfe954570085b0712b179f2c00b8c57690072bc2a579b1f5da7f07f.jpg)\n\n\nthat state. These three steps require significant expertise and knowledge to choose and conduct the measurements, which makes scientific discovery hard to automate and scale.\n\n\nModern high throughput screening platforms (Dove, 2007; Baillargeon et al., 2019; Blay et al., 2020; Bock et al., 2022; Fabio et al., 2023; Morris et al., 2023; Messner et al., 2023) enables running large scale perturbation experiments that collect information-rich unstructured measurements. For example, cell painting assays (Bra, 2016; Fay et al., 2023; Chandrasekaran et al., 2023) provide microscopy images of cells, capturing the same information a human expert would measure, but without pre-selecting a specific property like cell viability. This lets us measure all the potential properties of interest at once\u2014albeit as unstructured signal data, such as raw pixel images or sensor measurements, rather than preprocessed properties\u2014but it is not clear how to use these measurements for interaction testing and selection in order to efficiently discover pairwise interactions. This suggests the primary question that this paper seeks to address: if we dispense with prespecified structured measurements, can we automate the discovery of pairwise interactions by designing testing and selection procedures that efficiently detect interactions from unstructured data?\n\n\nTo test for interactions, we show that pairwise perturbations are separable if they provide no additional information beyond what is already known from the single perturbations. This separability property can be tested by comparing density ratios of the single and double perturbations to the control distribution. We also develop a test for the case where pairs of perturbations affect disjoint subsets of the outcome space. For example, if two perturbations affect different organelles in the cell, each of which affect disjoint pixels in an image. This strong form of non-interaction allows us to compose summary statistics from single perturbations to predict the corresponding statistics from double perturbations. In particular, this lets us compose embeddings of single perturbations to predict the embeddings of double perturbations, similar to word-vector analogies Mikolov et al. (2013).\n\n\nWith this notion of an interaction, we address the second task of selection via efficient experimental design. We can search the space of pairwise experiments by selecting pairs of perturbations that are likely to result in large test statistics. In doing so, we reduce the problem of finding interacting pairs of perturbations into an active matrix completion problem. By defining this notion of interaction, we avoid the need to characterize uncertainty over the pixel-level outcomes (or some embedding thereof). Instead we directly model the posterior over an unknown reward matrix defined by the test statistics and at every round select actions striking a balance between exploration and exploitation via information directed sampling (IDS; Russo & Van Roy, 2016; Xu et al., 2022).\n\n\nWe evaluated our approach on both synthetic and real biological experiments. On the synthetic tasks, we found that our tests were able to detect both forms of dependence, validating our theory. On a benchmark consisting of all pairs of gene knockouts for 50 genes in HUVEC cells, we found that our approach using IDS discovers pairs of genes with higher interaction scores significantly faster than baselines, resulting in  $15\\%$  more known biological interactions being discovered and potentially far more novel interactions. The interactions we detected were also complementary to those which would have been discovered using existing cosine similarity-based approaches, and as a result, the two approaches can be combined to get a more detailed estimate of the relationships between genes from\n\n\nperturbation experiments. In summary, we formulate two notions of pairwise interaction\u2014violations of separability and disjointedness\u2014and develop a system for discovering interactions through the following contributions:\n\n- - We show that two perturbations are separable if double perturbations provide no additional information than that which was revealed in single perturbations.\n\n- - Further, we show that two perturbations are disjoint if and only if their densities compose additively. This result also implies sufficient conditions for embeddings of perturbations to be composed additively to predict pairwise perturbations.\n\n- - We demonstrate that the test scores for detecting interactions can be used to efficiently search for pairs of perturbations that interact using active matrix completion algorithms.\n\n\n# 2 RELATED WORK\n\n\nCausal representation learning Our approach to the problem of detecting interactions builds on the modelling assumptions developed in nonlinear independent component analysis (Hyvarinen & Pajunen, 1999; Hyvarinen & Morioka, 2016; Hyvarinen et al., 2019) and causal representation learning (Sch\u00f6lkopf et al., 2021), where we assume we observe some nonlinear mixing function,  $g(\\cdot)$ , of the underlying latent variables. The causal representation learning literature typically focuses on disentangling latent variables, whereas we look for testible implications without disentanglement. Our data generating process assumptions are most similar to the interventional setting (Ahuja et al., 2023; Squires et al., 2023; Buchholz et al., 2023; Varici et al., 2024). If we could successfully disentangle latent variables, then our task would be straightforward, but disentanglement is challenging in practice because it is not possible to validate whether an algorithm has succeeded in disentangling latent variables without access to ground truth. Like us, Jiang & Aragam (2023) also attempt to learn latent dependence properties (they characterize the whole latent DAG, not just marginal dependence) without disentanglement, but they assume a fixed bipartite graph of dependence between latents and observations, which does not apply to the pixel-level observations that we study. Zhang et al. (2023) develop causal representation learning techniques to disentangle latent variables and characterize conditions for extrapolation. Like us, they focus on biological applications, but they rely on stronger polynomial assumptions to achieve disentanglement. Their conditions for extrapolation are complementary to our separability tests: they argue extrapolation is possible when interventions affect non-overlapping latents, while we aim to discover when interventions affect overlapping latents. Finally, our separability test is closely related to, and inspired by Wang et al. (2023), but whereas they assume separability of concepts to manipulate generative models, we aim to test for an analogous notion of independence in experimental data.\n\n\nRepresentation learning for gene knockouts Our experiments build on a number of recent works showing the effectiveness of embedding cells presented into a representation space Sypetkowski et al. (2023); Kraus et al. (2023); Xun et al. (2023). These works show that you can infer that genes code for proteins that form part of complexes by finding embeddings that are highly cosine similar. This works because knocking out proteins on the same pathway will induce the same morphological effect, but this approach is limited to effects that are revealed by single perturbations. There have also been a number of papers that have attempted to learn disentangled representations of cells (e.g. Lotfollahi et al., 2023; Bereket & Karaletsos, 2023; Lopez et al., 2023), mostly from gene expression data.\n\n\nDesign of Gene Knockout Experiments There is growing literature on developing algorithms for the design of gene knockout experiments. Typically the problem is studied in the context of discovering single gene-knockouts (Mehrjou et al., 2021) which result in a particular phenotypic effect of interest. A variety of methods including bandits (Pacchiano et al., 2023) and traditional experimental design (Lyle et al., 2023) approaches have been studied in this context. Further, approaches which aim to learn predictors for the effects of unseen gene knockouts typically operate on RNA-seq data (Huang et al., 2023). RNA-Seq data is far more structured than the image data we consider: the former is a high-dimensional vector of count data where each element corresponds to a particular gene's expression, while the latter is just an unstructured collection of pixel intensities.\n\n\nExperimental design. The task of efficiently designing experiments to gain the most information about a system can be formalized as sequential Bayesian optimal experimental design (BOED; Ryan et al., 2016; Foster, 2021; Rainforth et al., 2023), where the goal to design experiments  $x \\in \\mathcal{X}$  with outcomes  $y \\in \\mathcal{Y}$  governed by a generative process  $y \\sim p(y \\mid \\gamma, x)$  with parameters  $\\gamma$ . The experiments  $(x_1, \\ldots, x_T)$  are performed sequentially, with the objective of maximizing the information gain (Lindley, 1956; Sebastiani & Wynn, 2000). However, applying this framework in\n\n\nhigh-dimensional settings is challenging because estimating information gain becomes difficult. We sidestep this complexity by focusing on the specific task of detecting interactions.\n\n\nStatistical and biological interactions. We further provide a discussion about prior works about classical statistical interactions in regression models and biological interactions in Appendix A.\n\n\n# 3 TESTS FOR PAIRWISE INTERACTIONS\n\n\nIn this section we develop tests for both separable and disjoint interventions. The separability test, Section 3.1, allows us to test whether two perturbations act on disjoint sets of latent variables. Disjointedness, Section 3.2, implies compositional generalization of summary statistics, allowing us to reduce the search space by predicting the outcome of experiments without explicitly running them.\n\n\nSetup We have observations  $X$  in an observation space  $\\mathcal{X}$  of unstructured measurements such as pixels in an image, and a finite set of perturbations  $\\{T_i \\in \\mathcal{T}_i : i \\in [n]\\}$ . Although our theory accommodates generic perturbations, we restrict the discussion to binary perturbations for convenience, i.e.,  $\\mathcal{T}_i := \\{0, 1\\}$  for all  $i \\in [n]$ , and  $T_i = 1$  means perturbation  $i$  is applied. For all  $i, j \\in [n]$ , denote the perturbation indicator as follows:\n\n\n$$\n\n$$\n\\delta_ {0} = \\left\\{T _ {[ n ]} = 0 \\right\\} \\quad \\delta_ {i} = \\left\\{T _ {i} = 1, T _ {[ n ] \\backslash \\{i \\}} = 0 \\right\\} \\quad \\delta_ {i j} = \\left\\{T _ {i} = T _ {j} = 1, T _ {[ n ] \\backslash \\{i, j \\}} = 0 \\right\\}.\n$$\n\n$$\n\n\nIn this section, we assume that for a pair of perturbations  $T_{i}, T_{j}$ , we have access to experimental data from four distributions:  $p(x|\\delta_0)$ ,  $p(x|\\delta_i)$ ,  $p(x|\\delta_j)$ , and  $p(x|\\delta_{ij})$ ; when we discuss the active learning procedures in Section 4, we will assume that we have access to all single perturbations distributions,  $p(x|\\delta_i)$ , and we will adaptively select the pairs,  $i, j$ , on which to collect samples from  $p(x|\\delta_i,\\delta_j)$ . Finally, we assume the existence of a set of latent variables  $\\{Z_1,\\ldots,Z_L\\} \\subseteq \\mathcal{Z}$  in some latent space,  $\\mathcal{Z}$ , that capture all relevant information about the perturbation. We state this precisely as,\n\n\nAssumption 3.1.  $X \\perpeq (T_1, \\ldots, T_n) | Z$ , or equivalently,\n\n\n$$\n\n$$\np (x | T _ {1}, \\dots , T _ {n}) = \\int_ {\\mathcal {Z}} p (x | z) p (z | T _ {1}, \\dots , T _ {n}) \\mathrm {d} z.\n$$\n\n$$\n\n\nThroughout, we assume all distributions have well-defined densities or probability mass functions with respect to some  $\\sigma$ -finite base measure on their corresponding sample spaces. We use the same symbol to denote a distribution and its density. All proofs are deferred to Appendix B.\n\n\n# 3.1 SEPARABILITY TESTING\n\n\nIt would be trivial to verify whether two perturbations intervene on different latents if the underlying latent variables were observed directly. However, our observations are unstructured signal data, which we assume is generated by some deterministic mixing function.\n\n\nAssumption 3.2. There exists a diffeomorphism  $g: \\mathcal{Z} \\to \\mathcal{X}$  such that  $X = g(Z)$ .\n\n\nRemark 3.3. This diffeomorphism assumption between  $\\mathcal{Z}$  and  $\\mathcal{X}$  can be relaxed; our theory and methodology remain valid as long as the change of variable formula holds for the distributions of  $\\mathcal{Z}$  and  $X$ . For example, the distribution of  $X$  may have support on a low-dimensional manifold of  $\\mathcal{X}$ , with the latent space  $\\mathcal{Z}$  having a lower dimension than  $\\mathcal{X}$ . See Krantz & Parks (2008, Lemma 5.1.4) for the generalized change of variable formula for non-bijective transformations.\n\n\nThe key observation that we leverage is that when latent variables are independent, the change of variable formula implies that the density ratio of a perturbed distribution to the original (control) distribution has a simple form that only involves the distribution of the intervened latent variable,\n\n\n$$\n\n$$\n\\frac {p (x | \\delta_ {i})}{p (x | \\delta_ {0})} = \\frac {p _ {Z} (g ^ {- 1} (x) | \\delta_ {i}) \\left| \\det (J (g ^ {- 1} (x))) \\right|}{p _ {Z} (g ^ {- 1} (x) | \\delta_ {0}) \\left| \\det (J (g ^ {- 1} (x))) \\right|} = \\frac {p _ {Z _ {i}} ^ {\\dagger} ([ g ^ {- 1} (x) ] _ {i})}{p _ {Z _ {i}} ([ g ^ {- 1} (x) ] _ {i})}.\n$$\n\n$$\n\n\nHere  $p_{Z_i}^\\dagger$  denotes the perturbed distribution of the latent variable  $Z_i$  targeted by intervention  $i$ ,  $[ \\cdot ]_i$  denotes the projection operator to the subspace of  $Z_i$ . The testable conclusion that we can derive from this observation, is that when two variables are independent, we can predict the density ratio of the double perturbation,  $\\frac{p(x|\\delta_{ij})}{p(x|\\delta_0)}$ , as the product of the density ratios of the respective single perturbations,  $\\frac{p(x|\\delta_i)}{p(x|\\delta_0)}\\frac{p(x|\\delta_j)}{p(x|\\delta_0)}$ . To make this rigorous, we assume that there exists a causal factorization of the latent distribution, where each latent variable is conditionally independent of its non-descendants given its parents. We use  $\\mathrm{Pa}(Y)$  (resp.  $\\mathrm{Ch}(Y)$ ) to denote the parent (resp. children) of a random variable  $Y$ .\n\n\nAssumption 3.4. The latent distribution  $p_Z(z)$  can be factorized into  $L$  latent factors:\n\n\n$$\n\n$$\np _ {Z} (z | T _ {1}, \\dots , T _ {n}) = \\prod_ {l = 1} ^ {L} p _ {Z _ {l}} (z _ {l} | \\operatorname {P a} (z _ {l})),\n$$\n\n$$\n\n\nwhere  $\\forall l\\in [L],\\mathrm{Pa}(Z_l)\\subseteq T_{[n]}\\cup Z_{[L]}$ , and  $\\forall i\\in [n],\\mathrm{Ch}(T_i)\\subseteq Z_{[L]}$ . The random variable  $Z$  here should be interpreted as the concatenation of all latent factors  $\\{Z_1,\\ldots ,Z_L\\}$ .\n\n\nThe observed effects of two non-interacting perturbations are attributed to distinct causal pathways, rather than being confounded by a shared latent factor.\n\n\nDefinition 3.5. Two perturbations  $\\delta_i, \\delta_j$  are separable if  $\\mathrm{Ch}(T_i) \\cap \\mathrm{Ch}(T_j) = \\emptyset$ .\n\n\nWhen two perturbations act separately, the resulting density ratios of the observation distribution will have the predictable interactions that we derived above.\n\n\nTheorem 3.6. Suppose that Assumptions 3.4 and 3.2 hold, and that  $\\delta_i, \\delta_j$  are separable. Then,\n\n\n$$\n\n$$\n\\frac {p (x \\mid \\delta_ {i , j})}{p (x \\mid \\delta_ {0})} = \\frac {p (x \\mid \\delta_ {i})}{p (x \\mid \\delta_ {0})} \\frac {p (x \\mid \\delta_ {j})}{p (x \\mid \\delta_ {0})}. \\tag {1}\n$$\n\n$$\n\n\nThis provides a testable implication of separability, but to test it, we need to derive a real-valued test statistics. By taking logs and rearranging terms, we can rewrite Eq. (1), as testing,\n\n\n$$\n\n$$\n\\log p \\left(x \\mid \\delta_ {i j}\\right) + \\log p \\left(x \\mid \\delta_ {0}\\right) - \\log p \\left(x \\mid \\delta_ {i}\\right) - \\log p \\left(x \\mid \\delta_ {j}\\right) \\stackrel {?} {=} 0 \\tag {2}\n$$\n\n$$\n\n\nIf we instead take expectations of Eq. (2) with respect to  $p(x|\\delta_0)$ , then this amounts to testing if\n\n\n$$\n\n$$\n\\mathrm {D} _ {\\mathrm {K L}} \\left(p _ {0} \\| p _ {i}\\right) + \\mathrm {D} _ {\\mathrm {K L}} \\left(p _ {0} \\| p _ {j}\\right) \\stackrel {?} {=} \\mathrm {D} _ {\\mathrm {K L}} \\left(p _ {0} \\| p _ {i, j}\\right), \\tag {3}\n$$\n\n$$\n\n\nwhere  $p_i = p(x|\\delta_i), p_0 = p(x|\\delta_0)$  and  $\\mathrm{D}_{\\mathrm{KL}}(\\cdot ||\\cdot)$  denotes the Kullback-Leibler (KL) divergence. Note that KL divergence measures the difference between two distributions; in our context, it quantifies the distribution shift resulting from interventions. Eq. (3) reflects the intuition that the influence of two separable perturbations, measured by KL divergence, is additive. We use the KL score,  $|\\mathrm{D}_{\\mathrm{KL}}(p||p_i) + \\mathrm{D}_{\\mathrm{KL}}(p||p_j) - \\mathrm{D}_{\\mathrm{KL}}(p||p_{i,j})|$ , to quantify the violation of the separability of  $\\delta_i, \\delta_j$ , where these KL divergences are estimated using samples.\n\n\nIn practice, we observe that the simple K-nearest neighbor-based (KNN) estimator of the KL divergence (Wang et al., 2009) suffices for low-dimensional problems. However, for high-dimensional data such as images, more dedicated estimation procedures are required (Belghazi et al., 2018; Song & Ermon, 2020; Ghimire et al., 2021). We detail our estimation procedure in Appendix C.\n\n\n# 3.2 DISJOINTEDNESS TESTING\n\n\nOur second testing procedure examines whether two perturbations operate on disjoint domains such that their effects are cumulative. For example, if the morphological changes from knocking out two non-interacting genes can be separated into distinct visual features, such that their measures sum, then they are disjoint. We can define disjointedness of two perturbations formally as,\n\n\nDefinition 3.7. Two perturbations  $\\delta_{i},\\delta_{j}$  are disjoint if\n\n\n$$\n\n$$\np (x \\mid \\delta_ {i j}) - p (x \\mid \\delta_ {0}) = \\left(p (x \\mid \\delta_ {i}) - p (x \\mid \\delta_ {0})\\right) + \\left(p (x \\mid \\delta_ {j}) - p (x \\mid \\delta_ {0})\\right). \\tag {4}\n$$\n\n$$\n\n\nDisjointedness is important, because it implies that we can predict pairwise summary statistics of the distributions from individual perturbations. To see this, let  $h(\\cdot)$  denote any feature map from the observation,  $X$ . If we have disjoint perturbations  $\\delta_{i},\\delta_{j}$ , then,\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathbb {E} [ h (x) | \\delta_ {i, j} ] - \\mathbb {E} [ h (x) | \\delta_ {0} ] = \\int h (x) [ p (x | \\delta_ {i j}) - p (x | \\delta_ {0}) ] \\mathrm {d} x \\\\ = \\int h (x) \\left[ \\left(p (x | \\delta_ {i}) - p (x | \\delta_ {0})\\right) + \\left(p (x | \\delta_ {j}) - p (x | \\delta_ {0})\\right) \\right] d x \\\\ = \\mathbb {E} [ h (x) | \\delta_ {i} ] - \\mathbb {E} [ h (x) | \\delta_ {0} ] + \\mathbb {E} [ h (x) | \\delta_ {j} ] - \\mathbb {E} [ h (x) | \\delta_ {0} ]. \\tag {5} \\\\ \\end{array}\n$$\n\n$$\n\n\nThis implies that we can define average centered embedding vectors,  $\\vec{h}_i\\coloneqq \\mathbb{E}[h(x)|\\delta_i] - \\mathbb{E}[h(x)|\\delta_0]$  and  $\\vec{h}_j$ , and accurately predict  $\\vec{h}_{i,j} = \\vec{h}_i + \\vec{h}_j$  without running the experiments. Notably, growing\n\n\nevidence in the literature (Lotfollahi et al., 2019; Gaudelet et al., 2024) showing that these relationships often hold in real biological experiments (and our experiments support this; see Appendix E.2 for the choice of  $h$  we tested). Disjointedness explains the sufficient conditions for this to hold.\n\n\nWe can test whether Eq. (4) holds, by testing the following null hypothesis:\n\n\n$$\n\n$$\n\\mathrm {H} _ {0}: \\frac {1}{2} p (x | \\delta_ {i j}) + \\frac {1}{2} p (x | \\delta_ {0}) = \\frac {1}{2} p (x | \\delta_ {i}) + \\frac {1}{2} p (x | \\delta_ {j}).\n$$\n\n$$\n\n\nGiven interventional data from  $p(x|\\delta_0), p(x|\\delta_i), p(x|\\delta_j)$  and  $p(x|\\delta_{ij})$ , we can frame this as a standard two-sample test problem. We can test this using the maximal mean discrepancy (MMD) based two-sample test (Gretton et al., 2012), which compares two distributions based on their embeddings in some reproducing kernel Hilbert space (RKHS). The estimated MMD serves as a measure of the extent to which the perturbations violate the disjointedness. Interestingly, MMD test amounts to test Eq. (5) on a \"most discriminative\" feature map  $h$  in the RKHS. We provide a self-contained introduction about MMD and kernel mean embedding of distributions in Appendix E. Disjointness is a constraint on the densities of the observed pixels, but it is implied by assuming an appropriate mixture model on the latent variables. We discuss this in Appendix D.\n\n\n# 4 SELECTING PERTURBATION PAIRS TO EFFICIENTLY DISCOVER INTERACTIONS\n\n\nWith frameworks for testing separability and disjointedness in place, the next question is how to select which experiments to run. The space of possible perturbation pairs is too large to test all combinations, as experiments are costly. For instance, pairwise knockouts of 20,000 genes would require about 200 million experiments (not including replicates). In this section, we explore how experimental design and bandit algorithms can efficiently select perturbation pairs likely to interact.\n\n\nSelection of perturbation pairs as active matrix completion. The testing frameworks discussed in Section 3 prescribe test statistics which can be used to detect pairwise interactions. The test statistics, however, require samples from  $p(x|\\delta_{i,j})$ , which entails running pairwise perturbation experiments. We are thus interested in developing an approach for selecting pairs of perturbations which are likely to have high values for the test statistics and are thus likely to reveal pairwise interactions.\n\n\nThe test statistics for pairs of perturbations can be viewed as an (unknown) symmetric matrix  $\\mathbf{R} \\in \\mathbb{R}^{n \\times n}$ , where each entry  $\\mathbf{R}_{i,j}$  contains the value of the test statistic that we will observe if we run perturbation  $\\delta_{ij}$ .  $\\mathbf{R}$  is unknown a priori but we are allowed to select entries to observe. This can be viewed as an active matrix completion problem (Chakraborty et al., 2013). Active matrix completion is a variant of the standard matrix completion problem (Laurent, 2009) where values of entries of the matrix can be sequentially queried. Framing the problem of selecting perturbation pairs as an active matrix completion allows us to leverage existing efficient algorithms.\n\n\nAdaptive sampling for discovery. In particular, we use the framework of adaptive sampling for discovery (ASD; Xu et al., 2022) which provides a bandit-based approach for active matrix completion. ASD involves using information directed sampling (IDS; Russo & Van Roy, 2016) in a \"discovery\" setting, where an action is only selected once. This is an instantiation of the general sleeping experts setting (Kanade et al., 2009), where the set of available actions shrinks every round.\n\n\nDenote  $\\Delta$  the action space of possible experiment designs, which in this case is the set of perturbations  $\\Delta := \\{\\delta_{ij} : i,j \\in n, i > j\\}$ , where  $n$  is the total number of distinct perturbations. We denote the perturbation pair  $i,j$  selected at step  $k$  as  $a^{(k)} := (i^{(k)}, j^{(k)}) \\in \\Delta$ .  $\\mathcal{D}(\\Delta)$  represents the set of possible (categorical) distributions over  $\\Delta$ . Each time an action  $a^{(k)}$  is selected, the corresponding element of the (unknown) reward matrix  $\\mathbf{R}$  is revealed. In our setting, this reward corresponds to the test\n\n\nstatistic from either Section 3.1 or 3.2 for each perturbation pair. Let  $H_{t} = ((a^{(k)},\\mathbf{R}_{i^{(k)}},j^{(k)}))_{k = 1}^{t - 1}$  denote the history of actions and their corresponding rewards until round  $t$  , and  $\\Delta_t$  the set of remaining actions; i.e. the perturbation pairs that we have not yet tested experimentally. A policy  $\\pi$  is a map from  $H_{t}$  to  $\\mathcal{D}(\\Delta_t)$  . The IDS policy,  $\\pi_{\\mathrm{IDS}}$  maintains a posterior distribution over  $\\mathbf{R}$  given the data observed up to round  $t$  , denoted  $p(\\mathbf{R}\\mid H_t)$\n\n\n# ALGORITHM 1 ASD FOR SELECTING PERTURBATION PAIRS\n\n- 1: Initialize  $H_{1} = \\{\\}$ , batch size  $b$\n\n- 2: for  $t = 1, \\dots, T$  do\n\n- 3: Estimate posterior  $p(\\mathbf{R} \\mid H_t)$\n\n- 4: Compute information ratio  $\\Psi_t$\n\n- 5: Select  $(a_{i})_{i = 1}^{b}$  greedily to maximize  $\\Psi_t$\n\n- 6: Perform experiments and obtain  $(\\mathbf{R}_{a_i})_{i = 1}^b$\n\n- 7: Update  $H_{t + 1} = H_t\\cup \\{(a_i,\\mathbf{R}_{a_i})\\}_{i = 1}^b$\n\n- 8: end for\n\n\nWe can describe the sub-optimality of any action with respect to a set of beliefs by comparing the action's reward to that of the best action that could have been selected at time  $t$ , under the agent's current posterior over the reward matrix. Intuitively, we can evaluate this by sampling a plausible reward matrix from our posterior,  $\\dot{\\mathbf{R}} \\sim (\\mathbf{R}|H_t)$ , and then comparing the reward from action,  $a$ , to the reward an agent would have received from selecting the optimal action,  $a^* = \\arg \\max_{a \\in \\Delta_t} \\dot{\\mathbf{R}}(a)$ ; where  $\\dot{\\mathbf{R}}(a^{(k)}) := \\dot{\\mathbf{R}}_{i^{(k)}, j^{(k)}}$  for  $a^{(k)} = (i^{(k)}, j^{(k)})$ . This is known as the expected instantaneous regret incurred by an action and is defined as,  $\\Delta_t(a) = \\mathbb{E}_{\\dot{\\mathbf{R}} \\sim p(\\mathbf{R}|H_t)}[\\dot{\\mathbf{R}}(a^*) - \\dot{\\mathbf{R}}(a)]$ . Additionally, we define the information gain about the top  $T - t + 1$  remaining actions as follows:\n\n\n$$\n\n$$\ng (a) = M I (a _ {t, 1} ^ {*}, \\dots , a _ {t, T - t + 1} ^ {*}; \\mathbf {R} (a) \\mid H _ {t}, \\delta_ {t} = a).\n$$\n\n$$\n\n\nAlgorithm 1 summarizes the algorithmic procedure. The algorithm operates over a series of  $T$  rounds. In each round, the first step is to estimate the posterior distribution over  $\\mathbf{R}$  given the data observed thus far. Next step involves selecting a batch of perturbations based on the information ratio. The IDS policy at round  $t$  can then be computed by minimizing the information ratio  $\\Psi$ :\n\n\n$$\n\n$$\n\\pi_ {\\mathrm {I D S}} \\in \\arg \\min _ {\\pi \\in \\mathcal {D} (\\Delta_ {t})} \\Psi_ {\\pi , t} := \\frac {(\\Delta_ {t} ^ {\\top} \\pi) ^ {\\lambda}}{g _ {t} ^ {\\top} \\pi}\n$$\n\n$$\n\n\nwhere  $\\lambda$  is a parameter controls the tradeoff between lower instant regret (exploitation) and higher information gain (exploration). Due to the intractability of  $g_{t}(a)$ , we follow Russo & Van Roy (2016); Xu et al. (2022) to approximate  $g_{t}$  with the conditional variance  $v_{t}(a) = \\operatorname{Var}_{t}(\\mathbb{E}[\\mathbf{R}_{a}|a_{1}^{*},a])$ .  $v_{t}(a)$  is a lower bound on the information gain  $g_{t}(a)$  and hence can be used as a surrogate when maximizing  $g_{t}(a)$ . After selecting the perturbation pair  $i,j$ , we compute the pairwise test statistic,  $\\mathbf{R}_{i,j}$  using data from the experimental outcomes. We add these test statistics to our reward matrix, update our posteriors and then continue on to the next round. Linear regret is unavoidable for any bandit algorithm with no structural assumptions on  $\\mathbf{R}$ . We thus assume  $\\mathbf{R}$  is low-rank with a Gaussian prior on the entries, allowing ASD to achieve sublinear regret (Xu et al., 2022). In our motivating biological example the low-rank assumption amounts to assume that there are morphological similarities in the functional response of cells to genetic perturbations (e.g. because knockouts of different genes on the same pathway product the same morphological effect). We validate this empirically in Figure 5 and discuss the assumption further in subsection F.3.\n\n\nBatching. In high-throughput experimental screens, it is possible to run multiple experiments in parallel at the cost of a single experiment. So instead of a single action  $\\delta_t$ , we can select a set of actions  $\\{\\delta_t^1, \\ldots, \\delta_t^b\\}$  where  $b$  is the number of experiments we can run in parallel. However, as there are no efficient algorithms for combinatorial bandits in the discovery setting, we resort to a simple greedy scheme to select batches with ASD. Specifically, instead of picking a single action which minimizes the information ratio, we pick  $b$  actions with the lowest information ratio.\n\n\n# 5 EXPERIMENTS\n\n\nOur experiments aim to address three objectives: (1) verifying that our theoretical claims about interactions are detectable on known synthetic tasks; (2) evaluating the test statistics' ability to recover known biological relationships from real pairwise perturbation experiments; and (3) assessing our active learning pipeline's efficiency in detecting interactions. All experimental details are provided in Appendix F.\n\n\n# 5.1 TESTING ON SYNTHETIC SETTING\n\n\nWe validated the separability test on two synthetic interventional examples: one with 3-dimensional tabular data and one with images  $(3 \\times 128 \\times 128)$ . The disjointedness test was validated on an interventional tabular example. In each case, we generated observations by sampling from a latent distribution  $p_Z$  following a DAG structure, followed by a mapping  $g(\\cdot)$  to the observations. We then estimated the test statistics for each pair of perturbations. Detailed descriptions of the data generation processes, DAG structures, and mappings are provided in Appendix F.1.\n\n\nFig. 2 displays the separability test results for both tabular and image data, showing that our estimated KL scores accurately capture separability: inseparable pairs have large KL scores, while separable pairs have small scores. Fig. 4 shows the disjointedness tests, with the MMD test accurately identifying failures of disjointedness and being relatively insensitive to the choice of kernel.\n\n\n# 5.2 TESTING ON BIOLOGICAL INTERACTIONS\n\n\n![](images/f2255ed83edc51e49d9546a49dfa41c4330c27ec95e899d57bb0042e725ce7d6.jpg)\n\n\n![](images/75840335229e598185017b63aaa48981f045dbda8e73964b03b37ae5e2f88f10.jpg)\n\n\n![](images/3930f49cdd194c2cdd28328aa1547ee1a664f196c112e2246f103b909e30fba7.jpg)\n\n\n![Figure 3: (Left) Pairwise separability scores between different CRISPR guides targeting the two genes TSC2 and MTOR. Missing pairs indicate that data for those combinations were not collected in our experiments. Guides targeting the same gene show high KL scores (red), while guides targeting different genes show low scores (blue). (Right) Random samples of the actual single-cell images used in these experiments. Even trained experts find it very difficult to detect interactions from the image.](images/d0eb3841298d695f0a1e1f2334467f2ae1958d42a2aa5fbd08e3252529d12f18.jpg)\n\n\n![Figure 2: Separability test on synthetic tabular data using the KNN-based KL estimator (left) and NRE-based KL estimator (middle), and on synthetic images (right). Brighter colors indicate stronger interactions. Ground truth interacting pairs, A-B and C-D, are correctly identified in both examples.](images/21a6a383f8d28dd034330add7a74df41bc56ba141eda02c684012dab34c1f5a9.jpg)\n\n\nTo evaluate if our separability test can recover known biological interactions, we ran the following test: In gene knockout experiments, different CRISPR guides can target the same gene, cutting it at different locations but resulting in the same gene knockout. Intuitively, guides targeting the same gene should show high separability scores, while guides targeting different genes should show lower scores, assuming the genes are on distinct pathways. While it's rare for different guides targeting the same gene to be run in a single well, our dataset includes examples for two genes (TSC2 and MTOR). Using single\n\n\n![Figure 4: Disjointedness testing on synthetic example using the MMD-based statistics with a Matern 2.5 kernel (left) and an RBF kernel (right); brighter colors suggest stronger interactions. The ground truth interacting pairs are D-E and F-G, which are correctly identified by the test.](images/cda06cfb339c53043c5cfa237d96024b7c226d3b67ed35a250db93c215faca21.jpg)\n\n\ncell painting images, we tested separability between pairs of guides. The results, shown in Fig. 3, align with expectations: strong interaction scores were observed between guides targeting the same gene (e.g., MTOR guide 3 with MTOR guides 1 and 2), and much weaker scores between MTOR and TSC2 guides. Both MTOR and TSC2 impact multiple cell systems, so some interaction is expected, but the significantly lower scores between genes compared to within the same gene are encouraging.\n\n\nWe then evaluated our testing approaches on a collection of 50 genes with the goal of detecting gene interactions. The dataset was collected by performing CRISPR knockouts on all pairs from a set of 50 selected genes, resulting in 1,225 gene pairs. The targeted genes have a bias towards known gene-gene interactions. We performed in vitro double gene knockout experiments on HUVEC cells using three\n\n\nCRISPR guides per gene. We label perturbations according to the targeted gene, aggregating the effects of the individual CRISPR guides. The experimental protocol and data preprocessing followed the procedures described by Fay et al. (2023) and Sypetkowski et al. (2023), respectively.\n\n\nWe computed the test statistics using 1024-dimensional embeddings of the original cell painting images extracted from a pre-trained masked autoencoder (He et al., 2022; Kraus et al., 2023). Fig. 5 shows the matrices of the test statistics for all perturbation pairs; each entry represents the pairwise interaction scores. Qualitatively, both the disjointedness and separability statistics effectively uncover plausible biological relationships. Many genes in the apoptosis pathway (programmed cell death, e.g., gene 2-3: BAX, BCL2L1) and the proteasome (protein degradation e.g., gene 28-30: PSMA1, PSMB2, PSMD1) show high scores, expected from synthetic lethal (SL) relationships. SL is a complex phenomenon in which simultaneous inactivation of specific gene combinations leads to cell death or extreme sickness, while individual perturbations have little effect. Apoptosis, controlling cell survival, is tightly regulated; disruption of the anti-apoptotic BCL-2 family (BCL2, BCL2L1, MCL1) (Kale et al., 2018) interferes with critical barriers against cell death. Proteasome function is similarly regulated, as cells must maintain a delicate balance of different proteins (Rousseau & Bertolotti, 2018). Apoptosis and proteasome members are commonly found in SL screens in cancer cells due to mutations that compromise cellular buffering capabilities (Li et al., 2020; Ge et al., 2024; Han et al., 2017; Cron et al., 2013; Steckel et al., 2012; Das et al., 2020).\n\n\nAs shown in Fig. 5, the MMD-based statistics reveal strong interaction signals with proteasome components, which regulate global protein levels and interact with multiple essential pathways. While the separability score shows less distinct patterns, it highlights several gene pairs known or expected to interact physically or genetically, such as BCL2L1-MCL1 (genes 3-8) (Shang et al., 2020; Carter et al., 2023), BAX-BCL2L1 (genes 2-3) (Lindqvist & Vaux, 2014), BCL2L1-PSMD1 (genes 3-30) (Craxton et al., 2012), and PSMB2-PSMD1 (genes 29-30) (Voutsadakis, 2017).\n\n\n![Figure 5: Pairwise interaction scores from a selected set of 50 genes using disjointedness test with Matern 2.5 kernel and RBF kernel (left and middle, respectively), and separability test (right); brighter colors suggest stronger interactions. Genes from the same pathway are ordered adjacently. The associated pathways of each selected gene are described in Table 1 of Appendix F.2.1.](images/60e0de2a44af10f5b097a33b253670cc97659d339dcb28ddbf9cb35a5503fde1.jpg)\n\n\n![](images/d4e9ff885dc50529cfa01fc4c960055fb434c0d4fbca9809f112bf94cdd72e42.jpg)\n\n\n![](images/8ed25cb7ecca090dcb1598782c3aeca7ff425879c68a13bd27fa27835ae38c7e.jpg)\n\n\n# 5.3 INTERACTION DISCOVERY WITH ACTIVE MATRIX COMPLETION\n\n\nFinally, we showcase how to use our test statistics to adaptively select the pairwise experiment to run. As introduced in Section 4, the adaptive experimental design is framed as an active matrix completion problem (Algorithm 1). We applied Algorithm 1 on the gene-gene interaction detection example using the MMD-based scores.\n\n\nBaselines. To evaluate the effectiveness of our automated selection method, we compare against selection with random policy, upper confidence bound (UCB), Thompson sampling (TS) and uncertainty sampling (US). To instantiate UCB in the discovery setting, we use the uncertainty from the low-rank matrix posterior in place of the counts used in the standard multi-armed bandit setting and mask the actions once they are selected. Similarly, in US we pick pairs based solely on the uncertainty of the posterior. TS is instantiated in the same way as IDS, but the pairs are selected to minimize only the instant regret. Further details are discussed in Appendix F.\n\n\nEvaluation metrics. We evaluate each approach using three different metrics given a budget of 50 rounds, each with a batch size of 10 resulting in a total of 500 experiments covering  $50\\%$  of all\n\n\n![Figure 6: Empirical results on the active pairwise experiments selection for the gene-gene interaction detection example using MMD based test statistics. The solid lines represent the mean performance, whereas the shaded region represent all the runs (min-max). IDS (ours) outperforms all the baselines significantly in terms of top scoring pairs discovered as well as the regret. In terms of known interactions, IDS still outperforms the baselines by a small margin.](images/6f6e3c5f03ea4efcb374618feb12a5472e9f0d380d766b3cd86a396d87a9b308.jpg)\n\n\n![](images/2d24c9813f57b4cd0b62cfb15162cd518eafb3387bc5d71539ab8453f5da5da3.jpg)\n\n\n![](images/9912ee80f6bdfa2d9cb945e14482df27490dc5e346471e05c72b5e4a961511b9.jpg)\n\n\npossible pairs of the gene set. First, we access the fraction of the pairs with the top 5 percentile of scores recovered by each algorithm, measuring how well they explore high-scoring regions. Next, we evaluate the regret of each algorithm with respect to an optimal policy that always selects the highest-scoring pairs. Finally, we evaluate the number of known biological relations that appear in CORUM (Giurgiu et al., 2019), StringDB (Szklarczyk et al., 2021), Signor (Licata et al., 2020) and hu.MAP (Drew et al., 2021) to see how many each method is able to recover.\n\n\nResults. Fig. 6 (and Fig. 8 in Appendix F.3) show the empirical results. IDS successfully discovers all pairs in the top 5 percentile, while the baselines recover only about half. In terms of regret, IDS outperforms the baselines, with random performing the worst, demonstrating IDS's ability to exploit the low-rank structure of the reward matrix. Regarding known interactions discovered, IDS and TS slightly outperform other methods. After 50 rounds, IDS and TS show a  $12 - 15\\%$  improvement over the baselines in identifying known biological interactions. The significant difference between the performance on the fraction of top pairs and the number of known relations raises questions about the correlation between the prediction errors and the biological interactions.\n\n\n# 6 DISCUSSION\n\n\nThis paper presented a method for efficiently detecting interactions. We present interactivity scores for both separability and disjointedness which both allow us to measure interactions between perturbations, and run experiments only on pairs of perturbations for which we will fail to compositionally generalize. From an active learning perspective, disjointedness is powerful: when we can learn a good posterior over this test statistic, we effectively have a \"confidence score\" for whether two perturbations are likely to compose additively. This allows us to dramatically reduce the number of experiments we run by experimenting only where embeddings are unpredictable. Separability is a more intuitive notion of (in-)dependence in that we directly test for whether perturbations interact in latent space; we aim to explore whether this can be used to recover intervention targets.\n\n\nLimitations While many known interactions scored highly according to our test statistics, the overall correlation between known interactions and this metric was relatively low. It is not clear whether this is the result of a lack of specificity, or whether this is because we are in fact discovering real relationships that are not known to biology. To test this would require additional experimentation with orthogonal assays. Additionally, the results for the separability tests depended on the quality of the KL estimator. We used the SMILE estimator which is somewhat sensitive to the choice of the clipping parameter,  $\\tau$ . In our experiments, we simply used a default of  $\\tau = 5$ , but we need a robust method for choosing this hyperparameter to ensure this test is agnostic to modalities. Finally, the separability score test lacks a rigorous rejection criterion, and a characterization of type I and type II error. While this is less critical when the goal is to use the test statistic for active learning (introduced later in Section 4), a formal hypothesis testing framework is important from a statistical perspective. This issue has been addressed in a follow-up work.\n\n\n# REFERENCES\n\n- Cell painting, a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes. Nature Protocols, 11(9):1757-1774, 2016.\n\n- Constantin Ahlmann-Eltze, Wolfgang Huber, and Simon Anders. Deep learning-based predictions of gene perturbation effects do not yet outperform simple linear methods. *BioRxiv*, pp. 2024\u201309, 2024.\n\n- Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation learning. In International conference on machine learning, pp. 372-407. PMLR, 2023.\n\n- Yevhen Akimov and Tero Aittokallio. Re-defining synthetic lethality by phenotypic profiling for precision oncology. Cell chemical biology, 28(3):246-256, 2021.\n\n- Pierre Baillargeon, Virneliz Fernandez-Vega, Banu Priya Sridharan, Steven Brown, Patrick R Griffin, Hugh Rosen, Benjamin Cravatt, Louis Scampavia, and Timothy P Spicer. The scripps molecular screening center and translational research institute. SLAS DISCOVERY: Advancing Life Sciences R&D, 24(3):386-397, 2019.\n\n- Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International conference on machine learning, 2018.\n\n- Michael Bereket and Theofanis Karaletsos. Modelling cellular perturbations with the sparse additive mechanism shift variational autoencoder. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=DzaCE00jGV.\n\n- Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul A. Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep universal probabilistic programming. J. Mach. Learn. Res., 20:28:1-28:6, 2019. URL http://jmlr.org/papers/v20/18-403.html.\n\n- Vincent Blay, Bhairavi Tolani, Sunita P Ho, and Michelle R Arkin. High-throughput screening: today's biochemical and cell-based approaches. *Drug Discovery Today*, 25(10):1807-1821, 2020.\n\n- Christoph Bock, Paul Datlinger, Florence Chardon, Matthew A Coelho, Matthew B Dong, Keith A Lawson, Tian Lu, Laetitia Maroc, Thomas M Norman, Bicna Song, et al. High-content crisp screening. Nature Reviews Methods Primers, 2(1):1-23, 2022.\n\n- James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n\n- Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Scholkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing, 2023.\n\n- Bing Z. Carter, Po Yee Mak, Wenjing Tao, Edward Ayoub, Lauren B. Ostermann, Xuelin Huang, Sanam Loghavi, Steffen Boettcher, Yuki Nishida, Vivian Ruvolo, Paul E. Hughes, Phuong K. Morrow, Torsten Haferlach, Steven Kornblau, Muharrem Muftuoglu, and Michael Andreeff. Combined inhibition of bcl-2 and mcl-1 overcomes bax deficiency-mediated resistance of tp53-mutant acute myeloid leukemia to individual bh3 mimetics. Blood Cancer Journal, 13(1):57, 2023. doi: 10.1038/s41408-023-00830-w. URL https://doi.org/10.1038/s41408-023-00830-w.\n\n- Shayok Chakraborty, Jiayu Zhou, Vineeth Balasubramanian, Sethuraman Panchanathan, Ian Davidson, and Jieping Ye. Active matrix completion. In 2013 IEEE 13th international conference on data mining, pp. 81-90. IEEE, 2013.\n\n- Srinivas Niranj Chandrasekaran, Jeanelle Ackerman, Eric Alix, D. Michael Ando, John Arevalo, Melissa Bennion, Nicolas Boisseau, Adriana Borowa, Justin D. Boyd, Laurent Brino, Patrick J. Byrne, Hugo Ceulemans, Carolyn Ch'ng, Beth A. Cimini, Djork-Arne Clevert, Nicole Deflaux, John G Doench, Thierry Dorval, Regis Doyonnas, Vincenza Dragone, Ola Engkvist, Patrick W. Falcon, Briana Fritchman, Florian Fuchs, Sakshi Garg, Tamara J. Gilbert, David Glazer, David Gnutt, Amy Goodale, Jeremy Grignard, Judith Guenther, Yu Han, Zahra Hanifehlou, Santosh Hariharan, Desiree Hernandez, Shane R Horman, Gisela Hormel, Michael Huntley, Ilknur Icke, Makiyo Iida, Christina B. Jacob, Steffen Jaensch, Jawahar Khetan, Maria Kost-Alimova, Tomasz Krawiec, Daniel Kuhn, Charles-Hugues Lardeau, Amanda Lembke, Francis Lin, Kevin D. Little, Kenneth R. Lofstrom, Sofia Lotfi, David J. Logan, Yi Luo, Franck Madoux, Paula A. Marin Zapata, Brittany A. Marion, Glynn Martin, Nicola Jane McCarthy, Lewis Mervin, Lisa Miller, Haseeb Mohamed, Tiziana Monteverde, Elizabeth Mouchet, Barbara Nicke, Arnaud Ogier, Anne-Laure Ong, Marc Osterland, Magdalena Otrocka, Pieter J. Peeters, James Pilling, Stefan Prechtl, Chen Qian, Krzysztof Rataj, David E Root, Sylvie K. Sakata, Simon Scrace, Hajime Shimizu, David Simon, Peter Sommer, Craig Spruiell, Iffat Sumia, Susanne E Swalley, Hiroki Terauchi, Amandine Thibaudeau, Amy Unruh, Jelle Van de Waeter, Michiel Van Dyck, Carlo van Staden, Michal Warchol, Erin Weisbart, Am\u00e9lie Weiss, Nicolas Wiest-Daessle, Guy Williams, Shan Yu, Bolek Zapiec, Marek \u017dyla, Shantanu Singh, and Anne E. Carpenter. Jump cell painting dataset: morphological impact of 136,000 chemical and genetic perturbations. bioRxiv, 2023.\n\n- David R Cox. Interaction. International Statistical Review/Revue Internationale de Statistique, pp. 1-24, 1984.\n\n- A Craxton, M Butterworth, N Harper, L Fairall, J Schwabe, A Ciechanover, and G M Cohen. Noxa, a sensor of proteasome integrity, is degraded by 26s proteasomes by an ubiquitin-independent pathway that is blocked by mcl-1. Cell Death Differ, 19(9):1424-1434, Sep 2012. ISSN 1476-5403 (Electronic); 1350-9047 (Print); 1350-9047 (Linking). doi: 10.1038/cdd.2012.16.\n\n- Kyle R Cron, Kaya Zhu, Deepa S Kushwaha, Grace Hsieh, Dmitry Merzon, Jonathan Rameseder, Clark C Chen, Alan D D'Andrea, and David Kozono. Proteasome inhibitors block dna repair and radiosensitize non-small cell lung cancer. PLoS One, 8(9):e73710, 2013. ISSN 1932-6203 (Electronic); 1932-6203 (Linking). doi: 10.1371/journal.pone.0073710.\n\n- Shaoli Das, Xiang Deng, Kevin Camphausen, and Uma Shankavaram. Synthetic lethal drug combinations targeting proteasome and histone deacetylase inhibitors in tp53-mutated cancers. Arch Cancer Biol Ther, 1(2):42-47, 2020.\n\n- Amy Berrington de Gonz\u00e1lez and D. R. Cox. Interpretation of interaction: A review. The Annals of Applied Statistics, 1(2):371-385, 2007.\n\n- Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv. Communications on pure and applied mathematics, 36(2):183-212, 1983.\n\n- Alan Dove. High-throughput screening goes to school. Nature Methods, 4(6):523-532, 2007.\n\n- Kevin Drew, John B Wallingford, and Edward M Marcotte. hu. map 2.0: integration of over 15,000 proteomic experiments builds a global compendium of human multiprotein assemblies. Molecular systems biology, 17(5):e10016, 2021.\n\n- Stossi Fabio, K Singh Pankaj, Safari Kazem, Marini Michela, Labate Demetrio, and A Mancini Michael. High throughput microscopy and single cell phenotypic image-based analysis in toxicology and drug discovery. Biochemical Pharmacology, pp. 115770, 2023.\n\n- Marta M Fay, Oren Kraus, Mason Victors, Lakshmanan Arumugam, Kamal Vuggumudi, John Urbanik, Kyle Hansen, Safiye Celik, Nico Cernek, Ganesh Jagannathan, et al. Rrx3: Phenomics map of biology. bioRxiv, pp. 2023-02, 2023.\n\n- Adam Foster. Variational, Monte Carlo and policy-based approaches to Bayesian experimental design. PhD thesis, University of Oxford, 2021.\n\n- Thomas Gaudelet, Alice Del Vecchio, Eli M Carrami, Juliana Cudini, Chantriolnt-Andreas Kapourani, Caroline Uhler, and Lindsay Edwards. Season combinatorial intervention predictions with salt & peper. arXiv preprint arXiv:2404.16907, 2024.\n\n- Meiyi Ge, Jian Luo, Yi Wu, Guobo Shen, and Xi Kuang. The biological essence of synthetic lethality: Bringing new opportunities for cancer therapy. MedComm - Oncology, 3(1):e70, 2024. doi: https://doi.org/10.1002/mog2.70. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/mog2.70.\n\n- Sandesh Ghimire, Aria Masoomi, and Jennifer Dy. Reliable estimation of kl divergence using a discriminator in reproducing kernel hilbert space. In Advances in Neural Information Processing Systems, 2021.\n\n- Madalina Giurgiu, Julian Reinhard, Barbara Brauner, Irmtraud Dunger-Kaltenbach, Gisela Fobo, Goar Frishman, Corinna Montrone, and Andreas Ruepp. Corum: the comprehensive resource of mammalian protein complexes\u20142019. *Nucleic acids research*, 47(D1):D559\u2013D563, 2019.\n\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, 2014.\n\n- Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723-773, 2012.\n\n- Kyuho Han, Edwin E Jeng, Gaelen T Hess, David W Morgens, Amy Li, and Michael C Bassik. Synergistic drug combinations for cancer identified in a crisp screen for pairwise genetic interactions. Nat Biotechnol, 35(5):463-474, May 2017. ISSN 1546-1696 (Electronic); 1087-0156 (Print); 1087-0156 (Linking). doi: 10.1038/nbt.3834.\n\n- Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr D\u00f3l\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022.\n\n- Joeri Hermans, Volodimir Begy, and Gilles Louppe. Likelihood-free mcmc with amortized approximate ratio estimators. In International conference on machine learning, pp. 4239-4248. PMLR, 2020.\n\n- R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2019.\n\n- Kexin Huang, Romain Lopez, Jan-Christian Hutter, Takamasa Kudo, Antonio Rios, and Aviv Regev. Sequential optimal experimental design of perturbation screens guided by multi-modal priors. bioRxiv, pp. 2023-12, 2023.\n\n- Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ica. Advances in Neural Information Processing Systems, 29, 2016.\n\n- Aapo Hyv\u00e4rinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. Neural Networks, 12(3):429-439, 1999. ISSN 0893-6080. doi: https://doi.org/10.1016/S0893-6080(98)00140-3. URL https://www.sciencedirect.com/science/article/pii/S0893608098001403.\n\n- Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 859-868. PMLR, 2019.\n\n- Yibo Jiang and Bryon Aragam. Learning nonparametric latent causal graphs with unknown interventions. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=S8DFqgmEbe.\n\n- Justin Kale, Elizabeth J Osterlund, and David W Andrews. BCL-2 family proteins: changing partners in the dance towards death. Cell Death & Differentiation, 25(1):65\u201380, January 2018. ISSN 1476-5403. doi: 10.1038/cdd.2017.186. URL https://doi.org/10.1038/cdd.2017.186.\n\n- Varun Kanade, H Brendan McMahan, and Brent Bryan. Sleeping experts and bandits with stochastic action availability and adversarial rewards. In Artificial Intelligence and Statistics, pp. 272-279. PMLR, 2009.\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017. URL https://arxiv.org/abs/1412.6980.\n\n- Yinfei Kong, Daoji Li, Yingying Fan, and Jinchi Lv. Interaction pursuit in high-dimensional multi-response regression via distance correlation. The Annals of Statistics, 45(2):897-922, 2017.\n\n- Steven G Krantz and Harold R Parks. Geometric integration theory. Springer Science & Business Media, 2008.\n\n- Oren Kraus, Kian Kenyon-Dean, Saber Saberian, Maryam Fallah, Peter McLean, Jess Leung, Vasudev Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik, et al. Masked autoencoders are scalable learners of cellular morphology. arXiv preprint arXiv:2309.16064, 2023.\n\n- Monique Laurent. Matrix completion problems. Encyclopedia of Optimization, 3:221-229, 2009.\n\n- Shijie Li, Win Topatana, Sarun Juengpanich, Jiasheng Cao, Jiahao Hu, Bin Zhang, Diana Ma, Xiujun Cai, and Mingyu Chen. Development of synthetic lethality in cancer: molecular and cellular classification. Signal Transduction and Targeted Therapy, 5(1):241, 2020. doi: 10.1038/s41392-020-00358-6. URL https://doi.org/10.1038/s41392-020-00358-6.\n\n- Luana Licata, Prisca Lo Surdo, Marta Iannuccelli, Alessandro Palma, Elisa Micarelli, Livia Perfetto, Daniele Peluso, Alberto Calderone, Luisa Castagnoli, and Gianni Cesareni. Signor 2.0, the signaling network open resource 2.0: 2019 update. Nucleic acids research, 48(D1):D504-D510, 2020.\n\n- Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical Statistics, 27(4):986-1005, 1956.\n\n- Lisa M Lindqvist and David L Vaux. Bcl2 and related prosurvival proteins require bak1 and bax to affect autophagy. Autophagy, 10(8):1474-1475, Aug 2014. ISSN 1554-8635 (Electronic); 1554-8627 (Print); 1554-8627 (Linking). doi: 10.4161/auto.29639.\n\n- Romain Lopez, Natasa Tagasovska, Stephen Ra, Kyunghyun Cho, Jonathan Pritchard, and Aviv Regev. Learning causal representations of single cells via sparse mechanism shift modeling. In Mihaela van der Schaar, Cheng Zhang, and Dominik Janzing (eds.), Proceedings of the Second Conference on Causal Learning and Reasoning, volume 213 of Proceedings of Machine Learning Research, pp. 662-691. PMLR, 11-14 Apr 2023. URL https://proceedings.mlr.press/v213/lopez23a.html.\n\n- Mohammad Lotfollahi, F Alexander Wolf, and Fabian J Theis. scgen predicts single-cell perturbation responses. Nature methods, 16(8):715-721, 2019.\n\n- Mohammad Lotfollahi, Anna Klimovskaia Susmelj, Carlo De Donno, Leon Hetzel, Yuge Ji, Ignacio L Ibarra, Sanjay R Srivatsan, Mohsen Naghipourfar, Riza M Daza, Beth Martin, Jay Shendure, Jose L McFaline-Figueroa, Pierre Boyeau, F Alexander Wolf, Nafissa Yakubova, Stephan Gunnemann, Cole Trapnell, David Lopez-Paz, and Fabian J Theis. Predicting cellular responses to complex perturbations in high-throughput screens. Molecular Systems Biology, 19(6):e11517, 2023. doi: https://doi.org/10.15252/msb.202211517. URL https://www.embopress.org/doi/abs/10.15252/msb.202211517.\n\n- Clare Lyle, Arash Mehrjou, Pascal Notin, Andrew Jesson, Stefan Bauer, Yarin Gal, and Patrick Schwab. Discobax discovery of optimal intervention sets in genomic experiment design. In International Conference on Machine Learning, pp. 23170-23189. PMLR, 2023.\n\n- Arash Mehrjou, Ashkan Soleymani, Andrew Jesson, Pascal Notin, Yarin Gal, Stefan Bauer, and Patrick Schwab. Genedisco: A benchmark for experimental design in drug discovery. In International Conference on Learning Representations, 2021.\n\n- Christoph B Messner, Vadim Demichev, Ziyue Wang, Johannes Hartl, Georg Kustatscher, Michael M\u00fclleder, and Markus Ralser. Mass spectrometry-based high-throughput proteomics and its role in biomedical studies and systems biology. Proteomics, 23(7-8):2200013, 2023.\n\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\n- John A Morris, Jennifer S Sun, and Neville E Sanjana. Next-generation forward genetic screens: uniting high-throughput perturbations with single-cell analysis. Trends in Genetics, 2023.\n\n- Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Scholkopf, et al. Kernel mean embedding of distributions: A review and beyond. Foundations and Trends\u00ae in Machine Learning, 10(1-2):1-141, 2017.\n\n- Sebastian M.B. Nijman. Synthetic lethality: General principles, utility and detection using genetic screens in human cells. FEBS Letters, 585(1):1-6, 2011. ISSN 0014-5793.\n\n- Aldo Pacchiano, Drausin Wulsin, Robert A Barton, and Luis Voloch. Neural design for genetic perturbation experiments. In The Eleventh International Conference on Learning Representations, 2023.\n\n- Du Phan, Neeraj Pradhan, and Martin Jankowiak. Composable effects for flexible and accelerated probabilistic programming in numpyro. arXiv preprint arXiv:1912.11554, 2019.\n\n- Tom Rainforth, Adam Foster, Desi R Ivanova, and Freddie Bickford Smith. Modern bayesian experimental design. arXiv preprint arXiv:2302.14545, 2023.\n\n- Aaditya Ramdas, Sashank Jakkam Reddi, Barnab\u00e1s P\u00f3czos, Aarti Singh, and Larry Wasserman. On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015.\n\n- Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial intelligence and statistics, pp. 814-822. PMLR, 2014.\n\n- Geoffrey Roeder, Luke Metz, and Durk Kingma. On linear identifiability of learned representations. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9030-9039. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/roeder21a.html.\n\n- Adrien Rousseau and Anne Bertolotti. Regulation of proteasome assembly and activity in health and disease. Nature Reviews Molecular Cell Biology, 19(11):697-712, 2018. doi: 10.1038/s41580-018-0040-z. URL https://doi.org/10.1038/s41580-018-0040-z.\n\n- Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of thompson sampling. The Journal of Machine Learning Research, 17(1):2442-2471, 2016.\n\n- Elizabeth G Ryan, Christopher C Drovandi, James M McGree, and Anthony N Pettitt. A review of modern computational algorithms for bayesian optimal design. International Statistical Review, 84(1):128-154, 2016.\n\n- Bernhard Sch\u00f6lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612-634, 2021.\n\n- Paola Sebastiani and Henry P Wynn. Maximum entropy sampling and optimal bayesian experimental design. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 62(1):145-157, 2000.\n\n- Enyuan Shang, Trang T T Nguyen, Chang Shu, Mike-Andrew Westhoff, Georg Karpel-Massler, and Markus D Siegelin. Epigenetic targeting of mcl-1 is synthetically lethal with bcl-xl/bcl-2 inhibition in model systems of glioblastoma. Cancers (Basel), 12(8), Aug 2020. ISSN 2072-6694 (Print); 2072-6694 (Electronic); 2072-6694 (Linking). doi: 10.3390/cancers12082137.\n\n- Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information estimators. In International Conference on Learning Representations, 2020.\n\n- Christian Squires, Aryeh Seigal, Shreya S Bhate, and Caroline Uhler. Linear causal disentanglement via interventions. In International Conference on Machine Learning. PMLR, 2023.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638519.0136826", "msecs": "13.0", "relativeCreated": "115844.1801071167", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning\n\n# Abstract\nModel-agnostic meta-learning (MAML) is a popular method for few-shot learning but assumes that we have access to the meta-training set. In practice, training on the meta-training set may not always be an option due to data privacy concerns, intellectual property issues, or merely lack of computing resources. In this paper, we consider the novel problem of repurposing pretrained MAML checkpoints to solve new few-shot classification tasks. Because of the potential distribution mismatch, the original MAML steps may no longer be optimal. Therefore we propose an alternative meta-testing procedure and combine MAML gradient steps with adversarial training and uncertainty-based stepsize adaptation. Our method outperforms \"vanilla\" MAML on same-domain and cross-domains benchmarks using both SGD and Adam optimizers and shows improved robustness to the choice of base stepsize.\n\n\n# 1 INTRODUCTION\n\n\nDeep learning approaches have shown improvements based on massive datasets and enormous computing resources. Despite their success, it is still challenging to apply state-of-the-art methods in the real world. For example, in semiconductor manufacturing (Nishi & Doering, 2000), collecting each new data point is costly and time consuming because it requires setting up a new manufacturing process accordingly. Moreover, in the case of a \"destructive inspection\", the cost is very high because the wafer must be destroyed for measurement. Therefore, learning from small amounts of data is important for practical purposes.\n\n\nMeta-learning (learning-to-learn) approaches have been proposed for learning under limited data constraints. A meta-learning model optimizes its parameters for the best performance on the distribution of tasks. In particular, few-shot learning (FSL) formulates \"learning from limited data\" as an  $n$ -way  $k$ -shot problem, where  $n$  is the number of classes and  $k$  is the number of labeled samples per class. For each task in FSL, a support set is provided for training, while a query set is provided for evaluation. Ideally, a meta-learning model trained over a set of tasks (meta-training) will generalize well to new tasks (meta-testing).\n\n\nModel-agnostic meta-learning (MAML) (Finn et al., 2017) is a general end-to-end approach for solving few-shot learning tasks. MAML is trained on the meta-training tasks to learn a model initialization (also known as checkpoint) such that a few gradient steps on the support set will yield the best predictions on the query set.\n\n\nHowever, in practice it may not always be possible to retrain or finetune on the meta-training set. This situation may arise when the meta-training data is confidential, subject to restrictive licences, contains private user information, or protected intellectual property such as semiconductor manufacturing know-how. Another reason is that one may not have the computing resources necessary for running large-scale meta-training.\n\n\nIn this paper, we consider the novel problem of repurposing MAML checkpoints to solve new few-shot classification tasks, without the option of (re)training on the meta-training set. Since the meta-testing set (new tasks) may differ in distribution from the meta-training set, the implicit assumption \u2013in end-to-end learning\u2013 of identically distributed tasks may not hold, so there is no reason why the meta-testing gradient steps should match the meta-training. Therefore, we investigate various improvements over the default MAML gradient steps for test time adaptation.\n\n\nConceptually, our approach consists of collecting information while training the model on a new support set and then proposing ways to use this information to improve the adaptation. In this paper, we consider the variance of model parameters during ensemble training as a source of information to use. We propose algorithms that uses this information both to adapt the stepsizes for MAML as well as to generate \"task-specific\" adversarial examples to help robust adaptation to the new task.\n\n\nOur main contributions are the following:\n\n- - We motivate the novel problem of repurposing MAML checkpoints to solve cross-domain few-shot classification tasks, in the case where the meta-training set is inaccessible, and propose a method based on uncertainty-based stepsize adaptation and adversarial data augmentation, which has the particularity that meta-testing differs from meta-training steps.\n\n- - Compared to \"vanilla\" MAML, our method shows improved accuracy and robustness to the choice of base stepsizes on popular cross-domain and same-domain benchmarks, using both the SGD and Adam (Kingma & Ba, 2014) optimizers. Our results also indicate that adversarial training (AT) is helpful in improving the model performance during meta-testing.\n\n\nTo the best of our knowledge, our work is the first few-shot learning method to combine the use of ensemble methods for stepsize computation and generating adversarial examples from the meta-testing support set for improved robustness. Moreover, our empirical observation of improving over the default meta-testing procedure of MAML motivates further research on alternative ways to leverage published model checkpoints.\n\n\n# 2 RELATED WORK\n\n\n# 2.1 META-LEARNING AND MODEL-AGNOSTIC META-LEARNING\n\n\nFSL approaches deal with an extremely small amount of training data and can be classified into three categories. First, metric-based approaches solve few-shot tasks by training a feature extractor that maximizes inter-class similarity and intra-class dissimilarity (Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018). Second, memory-based approaches utilize previous tasks for new tasks with external memories (Santoro et al., 2016; Mishra et al., 2018). Third, optimization-based approaches search for good initial parameters during training and adapt the pretrained model for new tasks (Finn et al., 2017; Lee & Choi, 2018; Grant et al., 2018). We focus on the optimization-based approaches and suggest better training especially for the general family of MAML methods.\n\n\n# 2.2 UNCERTAINTY FOR MODEL TRAINING\n\n\nUncertainty is an important criterion for measuring the robustness of a neural network (NN). Bayesian neural networks (BNN) (Blundell et al., 2015) obtain model uncertainty by placing prior distributions over the weights  $p(\\omega)$ . This uncertainty has been used to adapt the stepsizes during continual learning in Uncertainty-guided Continual BNNs (UCB) (Ebrahimi et al., 2020). For each parameter, UCB scales its stepsize inversely proportional to the uncertainty of the parameter in the BNN to reduce changes in important parameters while allowing less important parameters to be modified faster in favor of learning new tasks. Our approach also decreases the step sizes for \"uncertain\" parameters, but using a different notion of uncertainty, and instead in the context of FSL with a pretrained MAML checkpoint.\n\n\nUnlike BNNs, deep ensembles (Lakshminarayanan et al., 2017) estimate uncertainty of NNs using ensembles over randomly initialized parameters combined with adversarial training (AT). Deep ensembles have been successfully used to boost predictive performance and AT has been used to improve robustness to adversarial examples. Lakshminarayanan et al. (2017) showed that they produce predictive uncertainty estimates comparable in quality to BNNs. Deep ensembles, however, are not directly applicable for FSL tasks, as training randomly initialized parameters from scratch with a limited amount of training data yields poor performance. Our approach is partly inspired from deep ensembles, adapting it to the FSL setting by using instead a parameter perturbations of the MAML checkpoint model rather than a random initialization. We use in particular a multiplicative Gaussian perturbation that rescales the parameters, as the information content of the weights is said to be invariant to their scale (Wen et al., 2018).\n\n\n# 2.3 ADVERSARIAL TRAINING FOR ATTACK AND DEFENSE\n\n\nDeep NNs are sensitive to adversarial attacks (Goodfellow et al., 2015; Madry et al., 2018; Moosavi-Dezfooli et al., 2016). These methods generate an adversarial sample to fool a trained model, where the generated image looks identical to the original one for humans. Goodfellow et al. (2015) proposed the fast gradient sign method (FGSM) which generates adversarial example using sign of input gradient. While stronger attacks have been proposed (such as using projected gradient descent (Madry et al., 2018)), we will focus on the FGSM approach in this paper for simplicity. In FSL, adversarial attack and defense have also been studied. ADversarialMeta-Learner (Yin et al., 2018) utilized one-step adversarial attack for generating adversarial samples during meta-training. There is little consideration, however, for the degradation of accuracy in the original sample in adversarial defense approaches. Xie et al. (2020) reported improvement using generated examples for adversarial attack on the large scale training. Motiian et al. (2017) propose adversarial domain adaptation for FSL. Despite similar keywords, their work is distinct from ours and relies on using generative adversarial networks discriminators to confuse domains, whereas we rely on adversarial examples for improved robustness. To our best knowledge, there is no prior FSL work which uses adversarial examples to enhance the model at test-time.\n\n\n# 2.4 DOMAIN ADAPTATION\n\n\nDomain adaptation methods (DA) (Ben-David et al., 2010) attempt to alleviate the distribution mismatch between source and target domains. Most of the recently proposed DA approaches are based on generative adversarial networks (GANs) (Goodfellow et al., 2014). GAN-based DA approaches require having access to the large amount of unlabeled data from both the source and target datasets (Tzeng et al., 2017; Zhang et al., 2019; Wilson & Cook, 2020). DA methods cannot be directly applied in the FSL scenario due to limited number of target domain samples (shots). Some domain-adaptive FSL (DA-FSL) methods have been proposed in the case where only a very few samples from the target domain are available (Motiian et al., 2017; Zhao et al., 2020).\n\n\nDA and DA-FSL methods cannot be directly applied to our setting because we assume that the meta-training dataset is inaccessible \u2014 mirroring real world situations in which access to the meta-training set is restricted by privacy and confidentiality concerns. Our approach differs from DA and DA-FSL by not requiring access to the meta-training dataset (source domain).\n\n\n# 3 PROPOSED METHOD\n\n\nAt meta-testing time, MAML normally uses the support set to compute fixed gradient steps, which were \"calibrated\" using end-to-end learning during meta-training. That is, the learned initialization is such that a fixed combination of stepsize and loss result in the desired result. However, those stepsizes and losses may be suboptimal on the new task, especially if the new task is out-of-domain with respect to the meta-training tasks.\n\n\nOur method is based on the assumption that the support set can be used to improve the meta-testing procedure itself, beyond merely serving as training examples. We start by leveraging the support set to estimate task-specific uncertainties over the model parameters. Then, we propose two improvements over the \"vanilla\" MAML gradient steps: we scale the gradient steps using layer-wise stepsizes computed from the support set and we train using task-specific adversarial examples.\n\n\n# 3.1 MOTIVATING HYPOTHESES\n\n\nAt meta-testing time, we start by assuming that we can estimate task-specific uncertainties over the model parameters. One possibility, which we adopt in Section 3.2, is to train deep ensembles (Lakshminarayanan et al., 2017) on the support set and use the ensemble to estimate variances over model parameters. Each model learns slightly different parameters, and yields slightly different gradients. We regard the variance over the parameters and input gradients as task-specific uncertainties. Given the uncertainty estimates, we propose two modifications to the original MAML gradient steps.\n\n\nProposal 1 (Task-specific step sizes) Use lower step sizes for model parameters with higher variance.\n\n\nIn our case, the variance could be further amplified if high-variance components were to be moved with large stepsizes. Therefore, we carefully move high-variance components with a lower stepsize with the hope is that we can limit the variance over the model parameters. This approach can be related with the fact that lower step sizes should be taken for SGD when the gradients are very noisy (Dieuleveut et al., 2020).\n\n\nProposal 2 (Task-specific adversarial examples) Use adversarial examples with higher adversarial perturbation on input components with higher variance over the input gradient.\n\n\nThe intuition is that if only slightly perturbed models (from the ensemble) disagree on parts of the input gradient, then it means that they disagree on what to learn and therefore those parts of the input are more vulnerable to attack. Therefore, we propose to use AT with stronger adversarial perturbation in the weak parts of the input, with the hope to incur improved robustness on those parts of the input.\n\n\nWe regard adversarial training as a form of data augmentation or regularization at meta-testing time, which we hope allows the model to overcome the limited size of the support set.\n\n\n# 3.2 UNCERTAINTY-BASED GRADIENT STEPS AT TEST-TIME\n\n\nWe improve over the default MAML gradient steps by implementing the ideas presented in the previous section. The resulting approach is detailed in Algorithm 1 and is a combination of uncertainty-based stepsize adaption (USA, based on Proposal 1) and uncertainty-based fast gradient sign method (UFGSM), adversarial training (AT), and generating additional adversarial examples from deep ensembles (EnAug) which are based on Proposal 2.\n\n\n```\nData: New task support set  $D^{\\mathrm{Spt}} = \\{(x_1,y_1),\\ldots ,(x_{n\\times k},y_{n\\times k})\\}$  with  $n$  ways and  $k$  shots.   \nRequire: Base stepsize  $\\alpha$  , pretrained weights  $\\theta_0$  , Gaussian variance  $\\sigma$  , AT coefficient  $\\epsilon$  , size of ensemble  $M$  , number of gradient steps  $T$  , selection coefficients in  $\\{0,1\\}$  for: base stepsize  $\\lambda_{\\alpha}$  , adversarial loss  $\\lambda_{\\mathrm{AT}}$  , and augmented cross-entropy  $\\lambda_{\\mathrm{Aug}}$  .   \n1  $D^{\\mathrm{Aug}}\\gets \\emptyset ,\\alpha^{\\mathrm{adap}}\\gets \\alpha$    \n2 for  $m = 1$  to  $M$  do   \n3  $\\theta_0^m\\gets \\theta_0(1 + \\mathcal{N}(0,\\sigma))$  \u25b7Initialize deep ensemble   \n4 end   \n5 for  $t = 1$  to  $T$  do   \n6 for  $m = 1$  to  $M$  do   \n7  $\\theta_t^m\\gets \\theta_{t - 1}^m -\\alpha^{\\mathrm{adap}}\\odot \\nabla_{\\theta_{t - 1}^m}[\\mathcal{L}(D^{\\mathrm{Spt}},\\theta_{t - 1}^m) + \\mathcal{L}_{\\mathrm{AT}}(D^{\\mathrm{Spt}},\\theta_{t - 1}^m)]$    \n8  $D^{\\mathrm{Aug}}\\gets D^{\\mathrm{Aug}}\\cup \\{(\\mathcal{A}_{\\theta_{t - 1}^m}(x,y),y)\\mid (x,y)\\in D^{\\mathrm{Spt}}\\}$  \u25b7EnAug   \n9 end   \n10  $\\alpha^{\\mathrm{adap}}\\gets \\mathrm{USA}(\\alpha ,\\theta_t^{1:M})$  \u25b7Algorithm 2   \n11  $D^{\\mathrm{Aug}}\\gets D^{\\mathrm{Aug}}\\cup \\{\\mathrm{UFGSM}(\\theta_0,\\theta_t^{1:M},x,y)\\mid (x,y)\\in D^{\\mathrm{Spt}}\\}$  \u25b7Algorithm 3   \n12 end   \n13  $\\alpha \\gets \\lambda_{\\alpha}\\alpha +(1 - \\lambda_{\\alpha})\\alpha^{\\mathrm{adap}}$    \n14 for  $t = 1$  to  $T$  do   \n15  $\\theta_t\\gets \\theta_{t - 1} - \\alpha \\odot \\nabla_{\\theta_{t - 1}}[\\mathcal{L}(D^{\\mathrm{Spt}},\\theta_{t - 1}) + \\lambda_{\\mathrm{AT}}\\mathcal{L}_{\\mathrm{AT}}(D^{\\mathrm{Spt}},\\theta_{t - 1}) + \\lambda_{\\mathrm{Aug}}\\mathcal{L}(D^{\\mathrm{Aug}},\\theta_{t - 1})]$    \n16 end\n```\n\n\nDenote  $\\mathcal{L}(D,\\theta) = 1 / |D|\\sum_{(x,y)\\in D}l_{\\theta}(x,y)$  the cross-entropy loss for model  $\\theta$  over the labeled dataset  $D = \\{(x,y)\\ldots \\}$ ,  $\\mathcal{A}_{\\theta}(x,y) = x + \\epsilon \\mathrm{sign}(\\nabla_{x}l_{\\theta}(x,y))$  the FGSM adversarial example computed from  $(x,y)$  and  $\\mathcal{L}_{\\mathrm{AT}}(D,\\theta) = 1 / |D|\\sum_{(x,y)\\in D}l_{\\theta}(\\mathcal{A}_{\\theta}(x,y),y)$  the resulting adversarial\n\n\n![(a) Uncertainty of layers](images/96283a6fe48814d04c04895478c16e85cf7aafdf9b4876c98ad155eb2c850080.jpg)\n\n\n![(b) Uncertainty-based stepsize adaption Figure 1: USA converts uncertainty into layer-wise adapted stepsize. Here  $\\alpha = 0.01$ . (a) is standard deviation of trained ensemble models' weights. Each layer has a difference uncertainty. (b) is adapted stepsize by USA. Each layer has a different stepsize based on the uncertainty.](images/0fa625b9ac33a4a7cd8c22b8b9c67cee0ad7150187c0b4c0263b2d8f9264dbcc.jpg)\n\n\ncross-entropy, which we refer to as AT. For easy reference, all notations are summarized in Appendix A.1.\n\n\nStarting from the pretrained MAML checkpoint  $\\theta_0$ , we perturb the model parameters with multiplicative Gaussian noise to create a deep ensemble  $(\\theta_0^m)_{1\\leq m\\leq M}$  (lines 2-4). Then, we repeat the following for  $T$  steps. At time  $t$ , run gradient descent on each model  $\\theta_t^m$  of the ensemble (line 7), where the loss is a combination of the usual cross-entropy  $\\mathcal{L}$  and AT loss  $\\mathcal{L}_{\\mathrm{AT}}$  as in (Lakshminarayanan et al., 2017) but on the support set, and the stepsizes  $\\alpha^{\\mathrm{adap}}$  are updated using USA (details below). Also, we generate adversarial examples using FGSM and UFGSM (details below) and store them into  $D^{\\mathrm{Aug}}$  (lines 8 and 11). Finally, we run  $T$  gradient steps on the original checkpoint, where the loss is a combination of cross-entropy on the support set  $D^{\\mathrm{Spt}}$ , AT loss on the support set, and cross-entropy on the ensemble-augmented support set  $D^{\\mathrm{Aug}}$  (lines 13-16). Note that we recover the original MAML steps for  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 0, \\lambda_{\\alpha} = 1)$ , while we refer to the case  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 1, \\lambda_{\\alpha} = 0)$  as our full method.\n\n\nUSA. We propose uncertainty-based stepsize adaptation (USA), which assigns lower step sizes to layers<sup>1</sup> with higher uncertainty (Algorithm 2) - we call this loosely an \"inverse-relationship\" below. Let  $\\alpha$  denote the default (scalar) stepsize and  $\\theta^{1:M}$  the parameters of the ensemble models, where  $M$  is the size of the ensemble and the number of layers in model is  $L$ . To compute the adapted layer-wise stepsizes  $\\alpha^{\\mathrm{adap}}$ , we compute  $u$ , the parameter-wise standard deviation of the model parameters over the ensemble (line 2), apply an inverse-relationship transformation which flips the max with the min (line 3), average over each layer (line 4) and  $L_{1}$ -normalize the result (line 5). The design choices for USA are explained in Appendix A.2. The resulting step sizes  $\\alpha^{\\mathrm{adap}}$  have an inverse-relationship with the variance of each layer. We give an example of applying USA to the 4-ConvNet architecture on miniImageNet (Figure 1). On the left, we plot the layer-wise standard deviations of the parameters and on the right the corresponding USA step sizes, which follow an inverse relationship with the standard deviations.\n\n\n![Figure 2: Applying UFGSM to 4-ConvNet on miniImageNet with  $\\epsilon = 0.05$ . Starting from the clean image  $x$ , we add the signed gradient  $\\mathrm{sign}(\\nabla_x l_\\theta(x, y))$  after rescaling it by the uncertainty over the input gradient  $u$ , to generate the adversarial example  $x'$  (UFGSM). Note how UFGSM generated a more natural image than FGSM (rightmost,  $u = 1$ ). UFGSM. We also propose uncertainty-based FGSM (UFGSM) to generate adversarial examples, with higher adversarial perturbation on input components with higher variance over the input gradient (Algorithm 3). Starting from an image  $x$  and label  $y$ , we compute the input gradient for each model from the ensemble and calculate the standard deviation  $u$  over the ensemble (line 2). Then, we linearly map  $u$  between 0 and 1 (line 3), compute the FGSM adversarial example for the pretrained model  $\\theta_0$  and rescale it using  $u$ , so that areas of higher variance get more perturbation. We give an example of applying UFGSM to miniImageNet in Figure 2.](images/c9456698fd027d2ea74f43c14b939f7d1ff4e0dcc11a5f0423b44dd9d98016db.jpg)\n\n\n```\n1 Function USA  $(\\alpha, \\theta_t^{1:M})$   \n2  $u = \\operatorname{Std}(\\theta_t^{1:M})$   \n3  $c = \\operatorname{Max}(u) - u + \\operatorname{Min}(u)$   \n4  $\\mu^l \\gets \\text{average of } c \\text{ over each layer } l$   \n5  $\\alpha^{\\text{adap}} = \\alpha \\mu^l / (1/L \\sum_{l=1}^L \\mu^l)$   \n6 return  $\\alpha^{\\text{adap}}$   \n7 end\n```\n\n\n```\nDefine:MinMaxNorm  $(x) = \\frac{x - \\operatorname{Min}(x)}{\\operatorname{Max}(x) - \\operatorname{Min}(x)}$    \n1 Function UFGSM  $(\\theta ,\\theta_t^{1:M},x,y)$    \n2  $u = \\mathrm{Std}(\\{\\nabla_xl_{\\theta '}(x,y)|\\theta '\\in \\theta_t^{1:M}\\})$    \n3  $u = \\mathrm{MinMaxNorm}(u)$    \n4  $x^{\\prime} = x + \\epsilon u\\odot \\mathrm{sign}(\\nabla_{x}l_{\\theta}(x,y))$    \n5 return  $x^{\\prime}$    \n6 end\n```\n\n\n# 4 EXPERIMENTAL EVALUATION\n\n\nWe train MAML on miniImageNet (Vinyals et al., 2016) training split; we then apply our method on the resulting checkpoint. We evaluate our model on the test split of miniImageNet \u2013 for the same-domain setting \u2013 as well as CUB-200-2011 (Welinder et al., 2010), Traffic Sign (Houben et al., 2013) and VGG Flower (Nilsback & Zisserman, 2008) \u2013 for the cross-domain setting. These datasets are denoted as Mini, Birds, Signs and Flowers respectively. A desirable feature for an optimizer is to maintain good performance in a broad range of step sizes (Asi & Duchi, 2019). Therefore, we evaluate our approach not only at the optimal stepsize, but also over a broad range of base step sizes $^{2}$  from  $10^{-4}$  to 1. We evaluate the performance with three metrics: All, Top-1 and Top-40%. If two methods have comparable Top-1 performance, but one has better Top-40% performance, then that method is more robust to the choice of base stepsize. Detailed experimental setup and pretrained model selection are included in Appendix A.3. Our code is available at https://anonymous.4open.science/r/08ef52cf-456a-4e36-a408-04e1ad0bc5a9/ (anonymous project).\n\n\n# 4.1 MAIN RESULTS\n\n\nOur main results are in Table 1. We compare the default MAML steps, which we take as our baseline (denoted as SGD), with our full method (denoted as SGD+All), which consists of combining USA, UFSGM, EnAug, and AT ( $\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 1$ ,  $\\lambda_{\\alpha} = 0$  in Algorithm 1). In terms of absolute performance (Top-1 row), our method outperforms the baseline on cross-domain tasks (Birds,\n\n\nTable 1: Main results. We compare the default MAML steps (SGD) with our method (SGD+All) on same-domain and cross-domain benchmarks.\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"2\">5-way 1-shot</td><td colspan=\"2\">5-way 5-shot</td><td colspan=\"2\">10-way 1-shot</td></tr><tr><td>SGD</td><td>SGD+All</td><td>SGD</td><td>SGD+All</td><td>SGD</td><td>SGD+All</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>37.21\u00b10.22</td><td>36.24\u00b10.22</td><td>38.14\u00b10.39</td><td>43.48\u00b10.38</td><td>21.31\u00b10.37</td><td>22.07\u00b10.4</td></tr><tr><td>Birds</td><td>30.07\u00b10.52</td><td>29.83\u00b10.46</td><td>33.29\u00b10.29</td><td>38.07\u00b10.33</td><td>17.54\u00b10.06</td><td>17.81\u00b10.12</td></tr><tr><td>Flowers</td><td>40.54\u00b10.48</td><td>40.86\u00b10.45</td><td>44.55\u00b10.43</td><td>53.10\u00b10.55</td><td>26.79\u00b10.41</td><td>28.07\u00b10.41</td></tr><tr><td>Signs</td><td>34.19\u00b10.76</td><td>34.76\u00b10.61</td><td>45.39\u00b10.45</td><td>51.02\u00b10.5</td><td>23.77\u00b10.13</td><td>24.88\u00b10.13</td></tr><tr><td>Avg.</td><td>35.50\u00b10.49</td><td>35.42\u00b10.44</td><td>40.34\u00b10.39</td><td>46.42\u00b10.44</td><td>22.35\u00b10.24</td><td>23.21\u00b10.26</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.71\u00b10.81</td><td>46.62\u00b10.58</td><td>62.46\u00b10.68</td><td>62.16\u00b10.66</td><td>31.10\u00b10.66</td><td>31.42\u00b10.82</td></tr><tr><td>Birds</td><td>36.03\u00b10.48</td><td>36.43\u00b10.58</td><td>50.71\u00b10.54</td><td>51.82\u00b10.64</td><td>23.16\u00b10.21</td><td>23.61\u00b10.39</td></tr><tr><td>Flowers</td><td>51.97\u00b10.79</td><td>54.36\u00b11.04</td><td>70.63\u00b10.76</td><td>74.52\u00b10.69</td><td>38.61\u00b10.49</td><td>40.68\u00b10.58</td></tr><tr><td>Signs</td><td>43.44\u00b10.99</td><td>44.10\u00b11.03</td><td>72.52\u00b10.84</td><td>74.13\u00b10.90</td><td>35.10\u00b10.21</td><td>35.11\u00b10.26</td></tr><tr><td>Avg.</td><td>44.54\u00b10.77</td><td>45.37\u00b10.81</td><td>64.08\u00b10.71</td><td>65.66\u00b10.72</td><td>31.99\u00b10.39</td><td>32.70\u00b10.51</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.07\u00b10.52</td><td>45.79\u00b10.44</td><td>55.38\u00b10.46</td><td>60.70\u00b10.91</td><td>28.65\u00b10.64</td><td>30.56\u00b10.77</td></tr><tr><td>Birds</td><td>34.88\u00b10.55</td><td>35.77\u00b10.55</td><td>45.50\u00b10.49</td><td>51.03\u00b10.63</td><td>21.78\u00b10.07</td><td>22.87\u00b10.30</td></tr><tr><td>Flowers</td><td>49.98\u00b10.69</td><td>53.16\u00b10.73</td><td>65.16\u00b10.73</td><td>73.55\u00b10.86</td><td>36.14\u00b10.53</td><td>39.75\u00b10.61</td></tr><tr><td>Traffic</td><td>41.81\u00b11.02</td><td>43.39\u00b11.01</td><td>67.34\u00b10.43</td><td>71.45\u00b10.90</td><td>33.56\u00b10.11</td><td>34.27\u00b10.17</td></tr><tr><td>Avg.</td><td>42.94\u00b10.70</td><td>44.52\u00b10.68</td><td>58.34\u00b10.53</td><td>64.18\u00b10.82</td><td>30.03\u00b10.34</td><td>31.86\u00b10.46</td></tr></table>\n\n\nFlowers, Signs), while the performance is comparable on same-domain tasks (Mini). In terms of robustness to the base stepsize (All and Top-40% rows), our method outperforms the baseline over a large range of stepsizes for 5-way 5-shot and 10-way 1-shot tasks, while for 5-way 1-shot the results are either comparable (All) or better (Top-40%) depending on the metric considered. More results can be found in Appendix A.6.\n\n\n# 4.2 DISCUSSION\n\n\nAblation Study. We perform an ablation study for the 5-way 1-shot case in Table 2 and plot the accuracy at different base step sizes for the Flowers dataset in Figure 3. Comparing SGD to SGD+AT in Table 2, we observe that adversarial training is beneficial over the baseline (Top1 and Top-40%), except for large step sizes with SGD, which is reflected in the All metric and in the dip in performance in Figure 3a. Notice also how the use of AT flattens the accuracy curve near the optimal stepsize. Comparing SGD to SGD+USA, we observe a very small but consistent improvement over the baseline in the vicinity of the optimal stepsize (Top-1 and Top-40%). Comparing SGD+USA to SGD+USA+UFGSM, we observe improvement over a wide range of step sizes, which is reflected in the curves and in All and Top-40% metrics. Comparing SGD+USA+UFGSM+EnAug+AT to SGD+USA+UFGSM+EnAug shows that AT+EnAug is beneficial in terms of absolute performance (Top-1) as well as creating robustness to the choice of stepsize (Top-40%). The drop for the All metric is explained by the dip in the curve for the largest step sizes. Overall, the best absolute performance (Top-1) is always obtained through some use of adversarial training, while using the full method consistently results in increased robustness to the choice of stepsize (best Top-40% performance). Appendix A.5 shows various AT results.\n\n\nUFGSM vs. FGSM. We compare UFGSM to FGSM (Goodfellow et al., 2015) in Table 3. The results show that our uncertainty-based approach consistently outperforms FGSM, which suggests that the uncertainty information extracted from the support set was useful.\n\n\nSGD vs. Adam. Our method can be used with SGD and Adam, however, we have mainly focused on SGD throughout the paper. SGD tends to yield the best results (Top-1 and Top-40%), as shown in Table 4 for the baseline and full method. More results for Adam can be found in Sections A.6 and A.8 of the appendix.\n\n\nBest checkpoint vs. Overfitted checkpoint. We also consider the problem of repurposing an overfitted checkpoint. We find that our method improves both absolute performance and robustness to stepsize. In fact, the gains are more substantial on the overfitted (worse) checkpoint than the best checkpoint (Appendix A.8).\n\n\nTable 2: Ablation Study for 5-way 1-shot classification with SGD optimizer. SGD+AT corresponds to using  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\alpha} = 1, \\lambda_{\\mathrm{Aug}} = 0)$  in Algorithm 1, SGD+USA to  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = \\lambda_{\\alpha} = 0)$ , SGD+USA+UFGSM to  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\alpha} = 0, \\lambda_{\\mathrm{Aug}} = 1)$  and SGD+USA+UFGSM+EnAug+AT to  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 1, \\lambda_{\\alpha} = 0)$ .\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"5\">5-way 1-shot</td></tr><tr><td>SGD</td><td>SGD+AT</td><td>SGD+USA</td><td>SGD+USA +UFGSM</td><td>SGD+USA +UFGSM +EnAug+AT</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>37.21\u00b10.22</td><td>34.60\u00b10.18</td><td>37.53\u00b10.27</td><td>38.71\u00b10.28</td><td>36.24\u00b10.22</td></tr><tr><td>Birds</td><td>30.07\u00b10.52</td><td>28.81\u00b10.45</td><td>30.39\u00b10.53</td><td>31.07\u00b10.50</td><td>29.83\u00b10.46</td></tr><tr><td>Flowers</td><td>40.54\u00b10.48</td><td>38.80\u00b10.44</td><td>41.09\u00b10.49</td><td>42.33\u00b10.60</td><td>40.86\u00b10.45</td></tr><tr><td>Signs</td><td>34.19\u00b10.76</td><td>33.42\u00b10.59</td><td>35.02\u00b10.82</td><td>36.10\u00b10.48</td><td>34.76\u00b10.61</td></tr><tr><td>Avg.</td><td>35.50\u00b10.49</td><td>33.91\u00b10.41</td><td>36.01\u00b10.36</td><td>37.05\u00b10.27</td><td>35.42\u00b10.44</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.71\u00b10.81</td><td>46.58\u00b10.65</td><td>46.72\u00b10.71</td><td>46.66\u00b10.77</td><td>46.62\u00b10.58</td></tr><tr><td>Birds</td><td>36.03\u00b10.48</td><td>36.40\u00b10.48</td><td>36.08\u00b10.49</td><td>36.02\u00b10.48</td><td>36.43\u00b10.58</td></tr><tr><td>Flowers</td><td>51.97\u00b10.79</td><td>54.55\u00b10.84</td><td>52.01\u00b10.77</td><td>52.30\u00b10.77</td><td>54.36\u00b11.04</td></tr><tr><td>Signs</td><td>43.43\u00b10.99</td><td>43.98\u00b10.98</td><td>43.42\u00b11.06</td><td>43.96\u00b10.64</td><td>44.10\u00b11.03</td></tr><tr><td>Avg.</td><td>44.54\u00b10.77</td><td>45.38\u00b10.74</td><td>44.56\u00b10.44</td><td>44.73\u00b10.41</td><td>45.37\u00b10.81</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.07\u00b10.52</td><td>45.53\u00b10.50</td><td>45.23\u00b10.59</td><td>45.76\u00b10.62</td><td>45.79\u00b10.44</td></tr><tr><td>Birds</td><td>34.88\u00b10.55</td><td>35.57\u00b10.60</td><td>35.17\u00b10.55</td><td>35.30\u00b10.48</td><td>35.77\u00b10.55</td></tr><tr><td>Flowers</td><td>49.98\u00b10.69</td><td>52.92\u00b10.68</td><td>50.39\u00b10.71</td><td>51.07\u00b10.67</td><td>53.16\u00b10.73</td></tr><tr><td>Signs</td><td>41.83\u00b11.02</td><td>43.19\u00b10.96</td><td>42.08\u00b11.06</td><td>43.18\u00b10.73</td><td>43.39\u00b11.01</td></tr><tr><td>Avg.</td><td>42.94\u00b10.70</td><td>44.30\u00b10.68</td><td>43.22\u00b10.43</td><td>43.83\u00b10.39</td><td>44.52\u00b10.68</td></tr></table>\n\n\n![(a) With SGD optimizer](images/3df392ad3021c71ce0e7946c9228b5982feb1bef6db2f2f73375c4d33d688279.jpg)\n\n\n![(b) With Adam optimizer Figure 3: Ablation study for 5-way 1-shot classification on Flowers dataset.](images/891c2152ff15791a2798f7419bd448857b22aa1092a3e328b022664f1e282708.jpg)\n\n\nTable 3: Comparing UFGSM against FGSM (Goodfellow et al., 2015) on 5-way 1-shot tasks. For all metrics and datasets, the proposed uncertainty-based method is better.\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"2\">SGD+USA</td></tr><tr><td>+UFGSM</td><td>+FGSM</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>38.71\u00b10.28</td><td>36.97\u00b10.54</td></tr><tr><td>Birds</td><td>31.07\u00b10.50</td><td>30.05\u00b10.55</td></tr><tr><td>Flowers</td><td>42.33\u00b10.60</td><td>41.17\u00b10.55</td></tr><tr><td>Signs</td><td>36.10\u00b10.48</td><td>34.78\u00b10.50</td></tr><tr><td>Avg.</td><td>37.05\u00b10.27</td><td>35.74\u00b10.27</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.66\u00b10.77</td><td>43.93\u00b10.85</td></tr><tr><td>Birds</td><td>36.02\u00b10.48</td><td>34.52\u00b10.74</td></tr><tr><td>Flowers</td><td>52.30\u00b10.77</td><td>50.63\u00b10.84</td></tr><tr><td>Signs</td><td>43.96\u00b10.64</td><td>42.62\u00b11.00</td></tr><tr><td>Avg.</td><td>44.73\u00b10.41</td><td>42.93\u00b10.47</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.76\u00b10.62</td><td>43.18\u00b10.87</td></tr><tr><td>Birds</td><td>35.30\u00b10.48</td><td>33.88\u00b10.75</td></tr><tr><td>Flowers</td><td>51.07\u00b10.67</td><td>49.47\u00b10.69</td></tr><tr><td>Signs</td><td>43.18\u00b10.73</td><td>41.92\u00b10.90</td></tr><tr><td>Avg.</td><td>43.83\u00b10.39</td><td>42.11\u00b10.46</td></tr></table>\n\n\nTable 4: Comparing SGD vs. Adam for default MAML steps (baseline) and our method (baseline+all) on 5-way 1-shot classification.\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"2\">Baseline</td><td colspan=\"2\">Baseline+All</td></tr><tr><td>SGD</td><td>Adam</td><td>SGD</td><td>Adam</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>37.21\u00b10.22</td><td>31.87\u00b10.20</td><td>36.24\u00b10.22</td><td>33.57\u00b10.29</td></tr><tr><td>Birds</td><td>30.07\u00b10.52</td><td>28.02\u00b10.39</td><td>29.83\u00b10.46</td><td>29.51\u00b10.46</td></tr><tr><td>Flowers</td><td>40.54\u00b10.48</td><td>40.33\u00b10.57</td><td>40.86\u00b10.45</td><td>44.58\u00b10.70</td></tr><tr><td>Signs</td><td>34.19\u00b10.76</td><td>33.98\u00b10.43</td><td>34.76\u00b10.61</td><td>35.10\u00b10.43</td></tr><tr><td>Avg.</td><td>35.50\u00b10.49</td><td>33.55\u00b10.22</td><td>35.42\u00b10.44</td><td>35.69\u00b10.29</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.71\u00b10.81</td><td>43.08\u00b10.10</td><td>46.62\u00b10.58</td><td>44.72\u00b10.35</td></tr><tr><td>Birds</td><td>36.03\u00b10.48</td><td>34.00\u00b10.63</td><td>36.43\u00b10.58</td><td>35.45\u00b10.67</td></tr><tr><td>Flowers</td><td>51.97\u00b10.79</td><td>51.28\u00b10.47</td><td>54.36\u00b11.04</td><td>56.04\u00b11.14</td></tr><tr><td>Signs</td><td>43.43\u00b10.99</td><td>42.53\u00b10.79</td><td>44.10\u00b11.03</td><td>43.01\u00b10.85</td></tr><tr><td>Avg.</td><td>44.54\u00b10.77</td><td>42.72\u00b10.32</td><td>45.37\u00b10.81</td><td>44.80\u00b10.54</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.07\u00b10.52</td><td>40.18\u00b10.14</td><td>45.79\u00b10.44</td><td>42.92\u00b10.35</td></tr><tr><td>Birds</td><td>34.88\u00b10.55</td><td>32.38\u00b10.61</td><td>35.77\u00b10.55</td><td>34.54\u00b10.66</td></tr><tr><td>Flowers</td><td>49.98\u00b10.69</td><td>48.18\u00b10.44</td><td>53.16\u00b10.73</td><td>54.08\u00b10.76</td></tr><tr><td>Signs</td><td>41.83\u00b11.02</td><td>40.90\u00b10.70</td><td>43.39\u00b11.01</td><td>42.26\u00b10.77</td></tr><tr><td>Avg.</td><td>42.94\u00b10.70</td><td>40.41\u00b10.29</td><td>44.52\u00b10.68</td><td>43.45\u00b10.40</td></tr></table>\n\n\n**Validating Proposal 1 and 2.** Our method is built on the hypotheses that we should take small step sizes on high uncertainty parameters and add more adversarial perturbation on high uncertainty input gradients. To validate those choices, we have tried taking large step sizes on high uncertainty parameters, and using less adversarial perturbation on high uncertainty input gradients, which resulted in lower accuracy and robustness to the stepsize (Appendix A.7).\n\n\nFreezing most uncertain layers. We observed that the batch normalization (BN) layers have the most uncertainty (Figure 1). In Proposal 1, we argued that the higher uncertainty component amplifies the error when we use bigger stepsize. As suggested by an anonymous reviewer, this motivates an additional experiment that we perform where we \"freeze\" the BN layer during meta-testing (i.e. we manually set the stepsize for the BN scale and shift parameters to zero). We performed the BN layer freezing experiment on  $\\mathrm{SGD + All}$  (see detailed results in Table 10 and Figure 12 in Appendix A.9). It turns out that  $\\mathrm{SGD + All}$  (w/ freezing BN) outperforms  $\\mathrm{SGD + All}$  (w/o freezing BN) on the All metric, with most improvement in the higher stepsize range ( $>0.1$ ). This is consistent with our intuition that updating high-uncertainty layers (such as BN) with large step sizes can be harmful.\n\n\n# 5 CONCLUSION\n\n\nIn this paper we considered the novel problem of repurposing pretrained MAML checkpoints for out-of-domain few-shot learning. Our method uses deep ensembles to estimate model parameter and input gradient uncertainties over the support set, and builds upon the default MAML gradient steps through the addition of uncertainty-based adversarial training and adaptive step sizes. Our experiments over popular few-shot benchmarks show that our method yields increased accuracy and robustness to the choice of base stepsize. More generally, our results motivate the use of adversarial learning as a data augmentation scheme for improving few-shot generalization. In the future, it would be interesting to apply our method to related settings such as domain adaption and transfer learning.\n\n\n# REFERENCES\n\n- Hilal Asi and John C. Duchi. The importance of better models in stochastic optimization. In National Academy of Sciences, 2019.\n\n- Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. In Machine Learning, 2010.\n\n- Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, 2015.\n\n- Aymeric Dieuleveut, Alain Durmus, Francis Bach, et al. Bridging the gap between constant step size stochastic gradient descent and markov chains. In Annals of Statistics. Institute of Mathematical Statistics, 2020.\n\n- Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided continual learning with bayesian neural networks. In International Conference on Learning Representations, 2020.\n\n- Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, 2017.\n\n- Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014.\n\n- Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.\n\n- Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-based meta-learning as hierarchical bayes. In International Conference on Learning Representations, 2018.\n\n- Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of traffic signs in real-world images: The german traffic sign detection benchmark. In International Joint Conference on Neural Networks, 2013.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In arXiv preprint arXiv:1412.6980, 2014.\n\n- Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, 2017.\n\n- Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In International Conference on Machine Learning, 2018.\n\n- Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\n\n- Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive metalearner. In International Conference on Learning Representations, 2018.\n\n- Seyed Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and accurate method to fool deep neural networks. In Computer Vision and Pattern Recognition, 2016.\n\n- Saeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gianfranco Doretto. Few-shot adversarial domain adaptation. In Advances in Neural Information Processing Systems, 2017.\n\n- Maria Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008.\n\n- Yoshio Nishi and Robert Doering. Handbook of semiconductor manufacturing technology. CRC press, 2000.\n\n- Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations, 2017.\n\n- Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International Conference on Machine Learning, 2016.\n\n- Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, 2017.\n\n- Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales. Learning to compare: Relation network for few-shot learning. In Computer Vision and Pattern Recognition, 2018.\n\n- Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Metadata: A dataset of datasets for learning to learn from few examples. In International Conference on Learning Representations, 2020.\n\n- Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and Ming-Hsuan Yang. Cross-domain few-shot classification via learned feature-wise transformation. In International Conference on Learning Representations, 2020.\n\n- Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Conference on Computer Vision and Pattern Recognition, 2017.\n\n- Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, 2016.\n\n- Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010.\n\n- Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudo-independent weight perturbations on mini-batches. In International Conference on Learning Representations, 2018.\n\n- Garrett Wilson and Diane J. Cook. A survey of unsupervised deep domain adaptation. In ACM Transactions on Intelligent Systems and Technology, 2020.\n\n- Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and Quoc V. Le. Adversarial examples improve image recognition. In Computer Vision and Pattern Recognition, 2020.\n\n- Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. Adversarial meta-learning. In arXiv preprint arXiv:1806.03316, 2018.\n\n- Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, 2014.\n\n- Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan. Domain-symmetric networks for adversarial domain adaptation. In Computer Vision and Pattern Recognition, 2019.\n\n- An Zhao, Mingyu Ding, Zhiwu Lu, Tao Xiang, Yulei Niu, Jiechao Guan, Ji Rong Wen, and Ping Luo. Domain-adaptive few-shot learning. In arXiv, 2020.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638520.2861435", "msecs": "286.0", "relativeCreated": "117116.6410446167", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Unified Discrete Diffusion for Simultaneous Vision-Language Generation\n\n# Abstract\nThe recently developed discrete diffusion model performs extraordinarily well in generation tasks, especially in the text-to-image task, showing great potential for modeling multimodal signals. In this paper, we leverage these properties and present a unified multimodal generation model, which can perform text-based, image-based, and even vision-language simultaneous generation using a single model. Specifically, we unify the discrete diffusion process for multimodal signals by proposing a unified Markov transition matrix and a unified objective. Moreover, we design a multimodal mutual attention module to highlight the inter-modal linkages, which is vital for multimodal generation. Extensive experiments indicate that our proposed method can perform comparably to the state-of-the-art solutions in various generation tasks.\n\n\n# 1 INTRODUCTION\n\n\n![Figure 1: Examples of various tasks supported by UniD3. The dark brown portions of the image and description represent the [MASK].](images/cc2b5f09ac9982cc9c03416dfc4b49f4a6f6d2c018ec7e9c0050376f60b90413.jpg)\n\n\nDiffusion models (Ho et al., 2020; Song et al., 2021b) have garnered significant interest on various high quality conditional image generation tasks, such as image super-resolution (Rombach et al., 2022), image inpainting (Lugmayr et al., 2022), image editing (Avrahami et al., 2022), image translation (Saharia et al., 2022a), among others. Concurrently, the Vector Quantized (VQ) models have also achieved rapid advances in image generations, especially on cross-modal tasks, examples include text-to-image (Kim et al., 2022), sketch-to-image (Esser et al., 2021b), image-to-video (Wu et al., 2021).\n\n\nDespite their success, all of these generation tasks are designed for only a single special modality, i.e. either modality translation or modality generation, with the help of a powerful diffusion model. In particular, the former modality translation translates the given conditional signals into the traget domain, while the latter modality generation only works for the unconditional image generation, e.g. image generation on CelebA Karras et al. (2017) or LSUN Yu et al. (2015) datasets. However, none of them consider to learn a join distribtuion for the mixture modality.\n\n\n![Figure 2: The pipeline of UniD3. With an offline model (red background part), the given inputs are represented by discrete token sequence in separate domain. The fusion embedding concatenates the tokens in different modal and embed them to the same space. The unified diffusion (in blue background) will construct the joint distribution of all modalities based on the fused embedding with a fixed unified Markov transition matrix.](images/897c71805f291805d2fee983dc2442c5735f174a67302da7f5091938a67de31c.jpg)\n\n\nHere, we take the use of discrete diffusion model into a new realm - multi-modality generation using a unified vision-language model. In contrast to the modality translation tasks outlined above, our multi-modality generative model does not require any conditional signals given in prior and is capable of simultaneously generating content pairs with the associated multi-modalities. UniD3, our new Unified Discrete Denoising Diffusion model, allows us to construct a joint vision-language probability distribution by mixing discrete image tokens with text tokens, leading to a capability of simultaneously generating cross-domain results. This is achieved by the two-stages framework, illustrated in Fig. 2, (1) An offline model to generate a compact yet expressive discrete representation for both images and texts via discrete VAE (dVAE) and Byte-Pair Encoding (BPE), respectively (the pink part in Fig. 2). (2) A novel unified discrete diffusion model to estimate the joint distribution of such latent visual and language codes (the cyan part in Fig. 2). Once trained, UniD3 can not only inherit the ability to manipulate the provided text or image, but is also able to unify the text and image generation, e.g., unconditional vision-language pairings generation, cross modal manipulation, text guided image completion, and image conditional text caption (Fig. 1 depicts the tasks that our model is capable of handling).\n\n\nBased on the empirical exploration, our model can achieve comparable image quality to the text condition in the unconditional case. In terms of image caption, our model is also comparable to some states of the art.\n\n\nIn summary, our key contributions include the following:\n\n- - We design a specific Markov transition matrix for our unified discrete denoising diffusion model, which leads to a sophisticated control of diffusion process, to estimate the joint distribution of language and image. The purposive design of transfer matrices based on task objectives and data properties is also pioneering for discrete diffusion models.\n\n- - We additionally propose mutual mechanism with fuse embedding to fulfil the objective of multi-modal integration. And we alter a unified objective function to offer the optimization with more concise restrictions.\n\n- - To the best of our knowledge, the UniD3 is the first work to solve both multi-modality generation and modality translation problems that is capable of handling the simultane\n\n\n![Figure 3: Illustration of transformer blocks with mutual attention. A unified transformer is composed of several blocks stacked on top of one another.](images/302a776957349843c03749a62a54d31e52492623a0fbf66b44d4a0ba153f0445.jpg)\n\n\nous unconditional vision-language generation and bi-directional vision-language synthesis tasks with a single model.\n\n\n# 2 PRELIMINARIES\n\n\nVector Quantised Model The Vector-Quantised Variational Auto Encoder (VQ-VAE) (Van Den Oord et al., 2017) learns to embed the high-dimensional data, e.g., image or audio, into a discrete representation. In particular, given a high dimensional data  $\\pmb{x} \\in \\mathbb{R}^{C \\times H \\times W}$ , the encoder  $E$  first converts it to the spatial latent features  $\\pmb{z} = \\{z_{i,j}\\} \\in \\mathbb{R}^{d \\times h \\times w}$ , and then transfer this continuous features into discrete space by looking up the closest features in the codebook  $\\mathcal{Z} = \\{z_k\\} \\in \\mathbb{R}^{K \\times d}$  to obtain the tokens  $\\pmb{z}_q$ :\n\n\n$$\n\n$$\n\\boldsymbol {z} _ {q} = \\operatorname {Q u a n t i s e} (\\boldsymbol {z}) = \\operatorname {Q u a n t i s e} (E (\\boldsymbol {x})) := \\arg \\min  _ {k} \\| z _ {i, j} - z _ {k} \\|, \\tag {1}\n$$\n\n$$\n\n\nwhere the dimensions  $h, w$  of latent feature  $\\mathbf{z}$  are substantially smaller than the original dimensions  $H, W$ . The reconstructions can be obtained through a decoder  $G$ :\n\n\n$$\n\n$$\n\\hat {\\boldsymbol {x}} = G \\left(\\boldsymbol {z} _ {q}\\right) = G (\\text {Q u a n t i s e} (E (\\boldsymbol {x}))). \\tag {2}\n$$\n\n$$\n\n\nRecently, there have been significant advances in learning more compact representation, yet higher-quality reconstruction, such as introducing new losses (Esser et al., 2021a), applying powerful backbone (Yu et al., 2021), using multiple channels representation (Lee et al., 2022) and spatially normalization (Zheng et al., 2022). However, in this paper, we focus on applying the technology in our novel UniD3, rather than exploring a new codebook learning approach.\n\n\nDiscrete Diffusion The discrete diffusion model was originally mentioned in (Sohl-Dickstein et al., 2015), with transitions converging to a binomial distribution. Subsequent work extended this to multinomial diffusion (Hoogeboom et al., 2021; Song et al., 2021a), while Austin et al. (2021) provided more options for transition matrices. Recent works integrated discrete diffusion models with VQ-VAE, allowing for high-quality image synthesis (Gu et al., 2022; Hu et al., 2022).\n\n\nHere we briefly describe the multinomial diffusion with the absorbing state, as employed in VQ-Diffusion (Gu et al., 2022). Besides  $K$  tokens from a discrete VAE, an additional [MASK] token is introduced. The forward process is defined as:\n\n\n$$\n\n$$\nq \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {t - 1}\\right) = \\operatorname {C a t} \\left(\\boldsymbol {x} _ {t}; \\boldsymbol {p} = \\boldsymbol {Q} _ {t} \\boldsymbol {x} _ {t - 1}\\right) = \\boldsymbol {x} _ {t} ^ {T} \\boldsymbol {Q} _ {t} \\boldsymbol {x} _ {t - 1}, \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\pmb{x}$  is a one-hot vector identifying the token index. Here  $[Q_t]_{i,j} = q(x_t = i|x_{t - 1} = j)\\in$ $\\mathbb{R}^{(K + 1)\\times (K + 1)}$  is the Markov transition matrix from  $t - 1$  to  $t$ , which can be expressed as:\n\n\n$$\n\n$$\n\\boldsymbol {Q} _ {[ t - 1 \\rightarrow t ]} = \\left[\\begin{array}{c c c c c}\\alpha_ {t} + \\beta_ {t}&\\beta_ {t}&\\beta_ {t}&\\dots&0\\\\\\beta_ {t}&\\alpha_ {t} + \\beta_ {t}&\\beta_ {t}&\\dots&0\\\\\\beta_ {t}&\\beta_ {t}&\\alpha_ {t} + \\beta_ {t}&\\dots&0\\\\\\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\\\gamma_ {t}&\\gamma_ {t}&\\gamma_ {t}&\\dots&1\\end{array}\\right], \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\alpha_{t} \\in [0,1]$  is the probability of retaining the token, and has a probability of  $\\gamma_{t}$  to be replaced by the [MASK] token, leaving a chance of  $\\beta_{t} = (1 - \\alpha_{t} - \\gamma_{t}) / K$  to be diffused.\n\n\nThe posterior of the diffusion process can be formulated as:\n\n\n$$\n\n$$\nq \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}, \\boldsymbol {x} _ {0}\\right) = \\frac {q \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {t - 1} , \\boldsymbol {x} _ {0}\\right) q \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {0}\\right)}{q \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {0}\\right)} = \\frac {\\left(\\boldsymbol {x} _ {t} ^ {T} \\boldsymbol {Q} _ {t} \\boldsymbol {x} _ {t - 1}\\right) \\left(\\boldsymbol {x} _ {t - 1} ^ {T} \\overline {{\\boldsymbol {Q}}} _ {t - 1} \\boldsymbol {x} _ {0}\\right)}{\\boldsymbol {x} _ {t} ^ {T} \\overline {{\\boldsymbol {Q}}} _ {t} \\boldsymbol {x} _ {0}}, \\tag {5}\n$$\n\n$$\n\n\ngiven  $\\overline{\\mathbf{Q}}_t = \\mathbf{Q}_t\\cdot \\cdot \\cdot \\mathbf{Q}_1$  , which can be calculated in closed form, and\n\n\n$$\n\n$$\nq \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {0}\\right) = \\operatorname {C a t} \\left(\\boldsymbol {x} _ {t}; \\boldsymbol {p} = \\overline {{\\boldsymbol {Q}}} _ {t} \\boldsymbol {x} _ {0}\\right) = \\boldsymbol {x} _ {t} ^ {T} \\overline {{\\boldsymbol {Q}}} _ {t} \\boldsymbol {x} _ {0}. \\tag {6}\n$$\n\n$$\n\n\nIn the reverse process, instead of explicitly predicting the posterior using a denoising neural network, the  $x_0$ -parameterisation might increase the stability and permit fast inference (skipping  $\\Delta t$  steps per iteration). The reverse transition with reparameterisation is given as:\n\n\n$$\n\n$$\np _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}\\right) \\propto \\sum_ {\\tilde {\\boldsymbol {x}} _ {0}} q \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}, \\tilde {\\boldsymbol {x}} _ {0}\\right) \\tilde {p _ {\\theta}} \\left(\\tilde {\\boldsymbol {x}} _ {0} \\mid \\boldsymbol {x} _ {t}\\right), \\tag {7}\n$$\n\n$$\n\n\nin which the neural network predicts the logits of the target data  $q(\\pmb{x}_0)$ .\n\n\n# 3 METHOD\n\n\nOur goal is to adapt the discrete diffusion model to learn the joint distribution of linguistic and visual features concurrently. First, we propose a transition matrix that allows the diffusion model to capture the implicit association between text and images. Second, we present a mutual attention transformer architecture with fuse embedding layer as the denoising function and a unified objective function, which fits our unified diffusion process objective and permits for more precise predictions.\n\n\nOur overall pipeline is illustrated in Fig. 2. Specifically, our solution begins by compressing the figures and texts into discrete token sequences using dVAE and BPE, respectively. A robust diffusion model with unified transition matrix is then constructed to fit the joint distribution across different modalities, which is further empowered by a transformer with a mutual attention mechanism.\n\n\n# 3.1 UNIFIED DIFFUSION PROCESS\n\n\nUnified Transition Matrix Discrete diffusion models with transition matrices can capture global links. The presence of a transition matrix determines the nature of the discrete diffusion model, which also provides us with more choices for token evolution. Thus we may wonder if it is feasible to design transition matrices that capture the global connections between various modalities.\n\n\nThe Markov transition matrix of the discrete diffusion model should satisfy the following requirements: 1. each column in  $Q_{t}$  should sum to one to conserve probability mass; 2. each column in the cumulative-product  $\\overline{Q}_{t}$  should converge to either a known stationary distribution or a learnt prior when  $t$  becomes large.\n\n\nOn the basis of these criteria, we construct a unified transition matrix  $[Q_t]_{i,j} = q(x_t = i|x_{t - 1} = j)$  capable of encapsulating discrete representations among various modalities. The following matrix  $Q_{t}\\in \\mathbb{R}^{(K + K^{*} + 1)\\times (K + K^{*} + 1)}$  illustrates a unified transition process with only text and image modalities:\n\n\n$$\n\n$$\n\\boldsymbol {Q} _ {t} = \\left[ \\begin{array}{c c c c c c c c c c c} \\alpha_ {t} + \\beta_ {t} & \\beta_ {t} & \\beta_ {t} & \\dots & \\beta_ {t} & 0 & 0 & 0 & \\dots & 0 & 0 \\\\ \\beta_ {t} & \\alpha_ {t} + \\beta_ {t} & \\beta_ {t} & \\dots & \\beta_ {t} & 0 & 0 & 0 & \\dots & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \\beta_ {t} & \\beta_ {t} & \\beta_ {t} & \\dots & \\alpha_ {t} + \\beta_ {t} & 0 & 0 & 0 & \\dots & 0 & 0 \\\\ \\hline 0 & 0 & 0 & \\dots & 0 & \\alpha_ {t} + \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\dots & \\beta_ {t} ^ {*} \\\\ 0 & 0 & 0 & \\dots & 0 & \\beta_ {t} ^ {*} & \\alpha_ {t} + \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\dots & \\beta_ {t} ^ {*} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\dots & 0 & \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\dots & \\alpha_ {t} + \\beta_ {t} ^ {*} \\\\ \\hline \\gamma_ {t} & \\gamma_ {t} & \\gamma_ {t} & \\dots & \\gamma_ {t} & \\gamma_ {t} & \\gamma_ {t} & \\gamma_ {t} & \\dots & \\gamma_ {t} \\\\ \\end{array} \\right],\n$$\n\n$$\n\n\nwhere  $\\alpha_{t} \\in [0,1]$  is the probability to keep this token,  $\\beta_{t}$  and  $\\beta_{t}^{*}$  are the probabilities of a token to be replaced by any other accessible tokens in different modality, and  $\\gamma_{t}$  is the absorbing probability.\n\n\nThe dimension of the matrix  $Q_{t}$  is  $(K + K^{*} + 1) \\times (K + K^{*} + 1)$ , where  $K$  and  $K^{*}$  are the number of states in different modals respectively, e.g.,  $K$  is the size from the codebook in discrete VAE and  $K^{*}$  is the dictionary size of BPE.\n\n\nThe matrix comprises five sections:\n\n- - The final row and column form a section associated with the transition of the absorbing state. Intuitively, if the token is [MASK] at  $t - 1$  step, then the token must be [MASK] at time  $t$ . Conversely, any other modal token has the equal possibility  $\\gamma_{t}$  of being diffused to [MASK].\n\n- - The remainder of the matrix is composed of four quadrants, the first and third of which are fully zeros. Specifically, these two sub-matrices prevent tokens transitioning from one modality to another. The form of the second and fourth quadrants closely resembles that for multinomial diffusion. Here, in addition to some probability of being converted into a [MASK] token, each token also has some chance of transiting to a different state within the same modality, or remaining unaltered.\n\n- - The dimensions of these four quadrants are not identical, which are  $K^{*} \\times K$ ,  $K \\times K$ ,  $K \\times K^{*}$  and  $K^{*} \\times K^{*}$ , respectively.\n\n\nIt is worth noting that  $\\alpha_{t}$  and  $\\gamma_{t}$  are the same in all modalities, whereas  $\\beta_{t}$  varies based on the number of states in different modalities. Mathematically,  $\\beta_{t} = (1 - \\alpha_{t} - \\gamma_{t}) / K$  and  $\\beta_{t}^{*} = (1 - \\alpha_{t} - \\gamma_{t}) / K^{*}$ .\n\n\nThe sum of each column in this transition matrix is one to preserve probability mass, and also all the mass of the stationary distribution falls on the [MASK] token, which satisfies the prerequisite for a discrete diffusion model transition matrix.\n\n\nThe computation of  $\\overline{\\mathbf{Q}}_t\\mathbf{x}_0$ , needed for  $q(\\mathbf{x}_t|\\mathbf{x}_0)$  in Eq. 6, can be efficiently obtained in closed form:\n\n\n$$\n\n$$\n\\overline {{Q}} _ {t} \\boldsymbol {x} _ {0} = \\overline {{\\alpha}} _ {t} \\boldsymbol {x} _ {0} + \\left[ \\overline {{\\gamma}} _ {t} - \\mathbb {1} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} - \\mathbb {1} ^ {*} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} ^ {*} \\right] \\boldsymbol {x} _ {[ \\mathbb {M} ]} + \\mathbb {1} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} + \\mathbb {1} ^ {*} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} ^ {*},\n$$\n\n$$\n\n\nwhere  $\\mathbb{1}(\\pmb{x}_0) = \\left\\{ \\begin{array}{ll} 1 & \\text{if argmax } \\pmb{x}_0 \\in [0, K), \\\\ 0 & \\text{otherwise.} \\end{array} \\right.$ ,  $\\mathbb{1}^*(\\pmb{x}_0) = \\left\\{ \\begin{array}{ll} 1 & \\text{if argmax } \\pmb{x}_0 \\in [K, K + K^*), \\\\ 0 & \\text{otherwise.} \\end{array} \\right.$ ,\n\n\n$$\n\n$$\n\\boldsymbol {x} _ {[ \\mathbb {M} ]} = \\boldsymbol {x} \\leftarrow \\operatorname {a r g m a x} \\boldsymbol {x} = K + K ^ {*} \\text {a n d} \\bar {\\alpha} _ {t}, \\bar {\\beta} _ {t}, \\bar {\\beta} _ {t} ^ {*}, \\bar {\\gamma} _ {t} \\text {a r e t h e c o r r e s p o n d i n g c u m u l a t i v e p r o d u c t .} \\tag {9}\n$$\n\n$$\n\n\nThe detailed proof is provided in Appendix E.\n\n\nUnified Objective For conditional generation tasks such as text-to-image generation, the goal is to maximize  $p(\\pmb{x}|\\pmb{y})$  by finding  $\\pmb{x}$ , where  $\\pmb{y}$  is the given text condition. In our task, it can be approximated that the model try to maximize the joint distribution  $p(\\pmb{x},\\pmb{y})$  simultaneously. In practice, we minimize the Kullback-Leibler divergence between  $q(\\pmb{x}_{t - 1}|\\pmb{x}_t,\\pmb{x}_0)$  and  $p_{\\theta}(\\pmb{x}_{t - 1}|\\pmb{x}_t)$  in both image and text directions, as shown in the following:\n\n\n$$\n\n$$\n\\mathcal {L} _ {0} = - \\mathbb {E} _ {q \\left(\\boldsymbol {x} _ {1} \\mid \\boldsymbol {x} _ {0}\\right)} \\left[ \\log p _ {\\theta} \\left(\\boldsymbol {x} _ {0} ^ {i m g} \\mid \\boldsymbol {x} _ {1}, \\boldsymbol {x} _ {0} ^ {t x t}\\right) + \\log p _ {\\theta} \\left(\\boldsymbol {x} _ {0} ^ {t x t} \\mid \\boldsymbol {x} _ {1}, \\boldsymbol {x} _ {0} ^ {i m g}\\right) \\right], \\tag {10}\n$$\n\n$$\n\n\n$$\n\n$$\n\\mathcal {L} _ {t - 1} = \\mathbb {E} _ {q \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {0}\\right)} \\left[ D _ {\\mathrm {K L}} \\left(q \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}, \\boldsymbol {x} _ {0}\\right) \\| \\left[ p _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {i m g} \\mid \\boldsymbol {x} _ {t}\\right); p _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {t x t} \\mid \\boldsymbol {x} _ {t}\\right) \\right]\\right) \\right], \\tag {11}\n$$\n\n$$\n\n\nwhere  $p_{\\theta}(\\boldsymbol{x}_{t-1})$  is the integration of the logits  $p_{\\theta}(\\boldsymbol{x}_{t-1}^{img})$  and  $p_{\\theta}(\\boldsymbol{x}_{t-1}^{tax})$  from separate modal, and they can be obtained with  $\\boldsymbol{x}_0$ -parameterisation as following:\n\n\n$$\n\n$$\np _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {i m g} \\mid \\boldsymbol {x} _ {t}\\right) \\propto \\sum_ {\\tilde {\\boldsymbol {x}} _ {0} ^ {i m g}} q \\left(\\boldsymbol {x} _ {t - 1} ^ {i m g} \\mid \\boldsymbol {x} _ {t}, \\tilde {\\boldsymbol {x}} _ {0} ^ {i m g}, \\boldsymbol {x} _ {0} ^ {t x t}\\right) \\tilde {p _ {\\theta}} \\left(\\tilde {\\boldsymbol {x}} _ {0} ^ {i m g} \\mid \\boldsymbol {x} _ {t}\\right), \\tag {12}\n$$\n\n$$\n\n\n$$\n\n$$\np _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {t x t} \\mid \\boldsymbol {x} _ {t}\\right) \\propto \\sum_ {\\tilde {\\boldsymbol {x}} _ {0} ^ {t x t}} q \\left(\\boldsymbol {x} _ {t - 1} ^ {t x t} \\mid \\boldsymbol {x} _ {t}, \\tilde {\\boldsymbol {x}} _ {0} ^ {t x t}, \\boldsymbol {x} _ {0} ^ {i m g}\\right) \\tilde {p} _ {\\theta} \\left(\\tilde {\\boldsymbol {x}} _ {0} ^ {t x t} \\mid \\boldsymbol {x} _ {t}\\right). \\tag {13}\n$$\n\n$$\n\n\nDue to the  $x_0$ -parameterisation, our model is also capable of achieving fast sampling by increasing the step size. And the last term of the variational lower bound loss  $\\mathcal{L}_T$  is a constant and can be ignored during the training:\n\n\n$$\n\n$$\n\\mathcal {L} _ {T} = D _ {\\mathrm {K L}} \\left(q \\left(\\boldsymbol {x} _ {T} \\mid \\boldsymbol {x} _ {0}\\right) \\| p \\left(\\boldsymbol {x} _ {T}\\right)\\right), \\tag {14}\n$$\n\n$$\n\n\nas the prior  $p(\\boldsymbol{x}_T)$  is fixed with the unified transition matrix:\n\n\n$$\n\n$$\np \\left(\\boldsymbol {x} _ {T}\\right) = \\left[ \\bar {\\beta} _ {T}, \\bar {\\beta} _ {T}, \\dots , \\bar {\\beta} _ {T} ^ {*}, \\bar {\\beta} _ {T} ^ {*}, \\dots , \\bar {\\gamma} _ {T} \\right]. \\tag {15}\n$$\n\n$$\n\n\nThe full expression of the loss function can be found in Appendix C.\n\n\n# 3.2 DENOISING FUNCTION FOR MULTIMODAL\n\n\nMutual Attention As indicated by Eqs. 12 & 13, the neural network is responsible for prediction of the distribution  $\\tilde{p}_{\\theta}(\\tilde{\\boldsymbol{x}}_0|\\boldsymbol{x}_t)$ . However, the input to the neural network covers all modalities, and a simple self-attention mechanism can scarcely highlight the inter-modal linkages. In addition, the distributions predicted by the neural network need to be decoupled according to the various modalities throughout the reparameterisation. In other words, the network predictions should be expressed in terms of different modalities, e.g.,  $\\tilde{p}_{\\theta}(\\tilde{\\boldsymbol{x}}_0^{img}|\\boldsymbol{x}_t)$  and  $\\tilde{p}_{\\theta}(\\tilde{\\boldsymbol{x}}_0^{txt}|\\boldsymbol{x}_t)$ , with interconnections.\n\n\nTherefore, we propose the mutual attention mechanism and construct the unified transformer as shown in Fig. 3. The unified transformer contains several transformer blocks, each of which consists of one self-attention and two parallel mutual attention operations, and one feed-forward module. Each block receives a sequence of mixed-modal tokens as input that traverses a layer of self-attention to capture the inherent connection within the modalities. Afterwards, the hidden tokens are decoupled according to the location of the different modalities and fed to the corresponding mutual attention layers. Mutual attention is a modified version of the cross-attention layer, with the conditional inputs to cross-attention being maintained constant while the inputs to mutual attention are derived from hidden features. Next, the outputs from the various mutual attention are concatenated into one mixed-modal token sequence for transmission to the next block. At the end of the unified transformer, we layer-normalise the sequence of tokens from the blocks, which is then decoupled to different predictive distributions by fully-connected layers.\n\n\nIn the conventional multi-model generative model, conditional modality  $C$  is given and fixed. We have the Cross attention block to inject the conditional information  $C$  into the target modality  $X$ :\n\n\n$$\n\n$$\n\\operatorname {C A} (X, C) = \\operatorname {A t t n} (X, C; W) = \\operatorname {s o f t m a x} \\left(\\frac {Q K ^ {T}}{\\sqrt {2}}\\right) V, \\tag {16}\n$$\n\n$$\n\n\nwhere  $Q = W_{Q}X, K = W_{K}C \\& V = W_{V}C$ , and  $W$  is the learnable parameters. Noted that the cross attention  $CA(X,C)$  could convert to self-attention  $SA(X)$  when  $X = C$ .\n\n\nIn our model, each modality may become a component of what needs to be generated. Therefore, we propose mutual attention to enable tokens of different modalities in a sequence to be conditional on each other, allowing the capture of the relationships between the various modalities. Mathematically, our mutual attention can be expressed as\n\n\n$$\n\n$$\n\\mathrm {M A} (T _ {i}, T _ {j}) = \\operatorname {A t t n} (T _ {i}, T _ {j}; W) = \\operatorname {s o f t m a x} (\\frac {Q _ {i} K _ {j} ^ {T}}{\\sqrt {2}}) V _ {j},\n$$\n\n$$\n\n\nwhere  $Q_{i} = W_{Q}T_{i}, K = W_{K}T_{j} \\& V = W_{V}T_{j}$ , and  $T_{i} \\& T_{j}$  are the tokens in a different modality, e.g. image and text. The Unified block contains a self-attention and several mutual attention in parallel. Given a hidden token vector  $H' = [H_i', H_j']$ , the whole pipeline of Unified block is\n\n\n$$\n\n$$\n\\begin{array}{l} 1. T ^ {\\prime} = S A \\left(H ^ {\\prime}\\right) \\\\ 2. D e c o u p l e: T ^ {\\prime} \\rightarrow T _ {i} ^ {\\prime}, T _ {j} ^ {\\prime} \\\\ 3. \\left\\{ \\begin{array}{l} T _ {i} = \\operatorname {M A} \\left(T _ {i} ^ {\\prime}, T _ {j} ^ {\\prime}\\right) \\\\ T _ {j} = \\operatorname {M A} \\left(T _ {j} ^ {\\prime}, T _ {i} ^ {\\prime}\\right) \\end{array} \\right. \\\\ 4. C o u p l e: T \\leftarrow T _ {i}, T _ {j} \\\\ \\end{array}\n$$\n\n$$\n\n\n![this bird has a white belly, a spotted breast, a short tail, and pointed bill.](images/be3ba2e73ec33a7e8f0c7659132ab3d83c39a3321a6ccb02bc2238cac405112d.jpg)\n\n\n![a white bird with a black crown and a long beak. this bird is all black and has a small yellow eyes and white beak.](images/522ae7abc6e60b30bd016da5a1f6e902067ea2c124a985b3257b0920ddec7cae.jpg)\n\n\n![a clean and tide kitchen with wooden cabinets for use.](images/5ce07f9fef59c0e50194be76fbb3baed0fdcb7fb136572eb3c0ec9c755dedc11.jpg)\n\n\n![a tall giraffe is standing in a forest.](images/865e92291e7a191fb329a9789ce636bce0a2c0ff03f05db3160b52fd080415b3.jpg)\n\n\n![a tall building with a park area.](images/72be18aa80d83d42df4eccb6d7f61a4c8bcde972dadcf62f5180c0b95b49eca7.jpg)\n\n\n![](images/527131bb59ea1574d8f2ab9b790cbbb8f8b2fae64cdb22b79d2355378e8b0dc3.jpg)\n\n\n![this bird has wings that are grey and has a white belly. the small bird is mostly brown and has a light red head. Figure 4: Generated vision-language Pairs from CUB-200 and MSCOCO. Both the image and caption are generated simultaneously. The quality of the created photos and text is comprehensible, and there is a correlation between the descriptions and the visuals.](images/95923822c90433451db30ebe5a577a9108e8f4e50a38eff6f2da664498b46b10.jpg)\n\n\n![this bird has wings that are black and has a yellow and black belly.](images/e057321fedae1534a673629a4c1dc5ce5cc5cfa38484efbbcb7a1f74f551300f.jpg)\n\n\n![colorful flowers in an orange vase on the window.](images/b06ee4ec99a9307d24bc28278d4fe8f9ecf1809cf7ca185efbcc332cfadbb331.jpg)\n\n\n![there is a bench by the trees in the seaside park.](images/c7bbed106ab869a47e6ef9a61541aa80f7844489a65a671a9da0814a9351068f.jpg)\n\n\n![a window on the front of a brick building.](images/ff73cc3f4fd15a16f9cce2924de2ae597fcdb6ac1632b0ce25c4f879291152aa.jpg)\n\n\n![](images/b3096ae11cbdf82c0e3fc7712ceb28743f0a864e9e13248109d4e74336bad61d.jpg)\n\n\nFused Embedding In addition, discrete tokens of various modalities should be embedded in the same space. Considering that two modalities have  $K$  and  $K^{*}$  states respectively, we firstly create an embedding layer with size  $K + K^{*} + 1$ , with the additional state for the [MASK] token. We then create a learnable spatial position encoding for the image modality and a learnable sequence position encoding for the text modality. The final fused embedding is obtained by adding the embedded vectors to the positional encoding of their associated modalities.\n\n\n# 4 EXPERIMENTS\n\n\nThe description of the datasets and model experimental details can be found in Appendix B.\n\n\n# 4.1 QUANTITATIVE RESULTS\n\n\nIn the unconditional scenario, our model is able to sample from all modalities while maintaining the relationship between them. For conditional generation, we conducted experiments on text-based image generation and image-based caption generation, respectively.\n\n\nMetrics We evaluate the multimodal capacity of our model in three distinct areas: Image sampling quality, text sampling quality and the similarity between sampled image and text. Fr\u00e9chet inception distance (FID) (Heusel et al., 2017) and Inception Score (IS) (Salimans et al., 2016) are used to evaluate the realism and variety of the produced images. The BLEU-4, METEOR and SPICE are used to assess the quality of a picture caption. And CLIP (Radford et al., 2021) is used to identify the similarity between visuals and text.\n\n\nImage Quality We quantitatively compare our unconditional and text-conditional results with several state-of-the-art text-to-image and bidirectional conditional generation methods, including GAN-based methods, diffusion-based methods and autoregressive methods. The comparison results are given in Table 1, in which  $\\text{Pair}$  means the unconditional generated vision-language pairings and  $T2I$  represents the text-to-image generation.\n\n\nThe first section of the table solely covers modality translation methods. In the second part, these models are claimed to be endowed with cross modal capabilities. However, such methods are not able to achieve simultaneous vision-language pair generation and necessitate given signals. For a fair\n\n\n![Figure 5: Presentation of the results of cross modal vision and language manipulation and infilling. The dark brown portions of the image and description represent the [MASK], while the strike-through represent the caption manipulation. Image and text compliment each other simultaneously.](images/0a15e5ddbaeffe1921a7d22d22d77a10c46d0440e523142f380ea33d09fe0a93.jpg)\n\n\ncomparison, we exhibit the quantitative generative results based on various modalities individually. We provide more experimental results for text conditional generation in Appendix G.2\n\n\n<table><tr><td rowspan=\"2\"></td><td colspan=\"2\">CUB</td><td colspan=\"2\">MSCOCO</td></tr><tr><td>FID (\u2193)</td><td>IS (\u2191)</td><td>FID (\u2193)</td><td>IS (\u2191)</td></tr><tr><td>DM-GAN (Zhu et al., 2019)</td><td>16.09</td><td>4.75</td><td>32.64</td><td>27.3</td></tr><tr><td>DF-GAN (Tao et al., 2022)</td><td>14.81</td><td>5.1</td><td>21.42</td><td>-</td></tr><tr><td>VQ-Diffusion (Gu et al., 2022)</td><td>11.94</td><td>-</td><td>19.75</td><td>-</td></tr><tr><td>LDM w/o G (Rombach et al., 2022)</td><td>-</td><td>-</td><td>23.31</td><td>20.23</td></tr><tr><td>LAFITE(LF) (Zhou et al., 2022)</td><td>27.53</td><td>4.32</td><td>18.04</td><td>27.2</td></tr><tr><td>Unifying (Huang et al., 2021)</td><td>-</td><td>-</td><td>29.9</td><td>-</td></tr><tr><td>L-Verse (Kim et al., 2022)</td><td>-</td><td>-</td><td>45.8</td><td>-</td></tr><tr><td>OFA (Wang et al., 2022)</td><td>-</td><td>-</td><td>10.5</td><td>31.1</td></tr><tr><td>Ours(Pair)</td><td>17.38</td><td>6.11</td><td>28.63</td><td>24.44</td></tr><tr><td>Ours(T2I)</td><td>16.19</td><td>6.02</td><td>25.11</td><td>21.03</td></tr></table>\n\n\nTable 1: Evaluation of image synthesis on the CUB-200-2011 and MS-COCO dataset. The first section is the text-conditioned results while the model in second part includes multimodal solution.\n\n\n<table><tr><td></td><td>B-4 (\u2191)</td><td>M (\u2191)</td><td>S (\u2191)</td></tr><tr><td>BUTD (Anderson et al., 2018)</td><td>36.3</td><td>27.7</td><td>21.4</td></tr><tr><td>ORT (Herdade et al., 2019)</td><td>38.6</td><td>28.7</td><td>22.6</td></tr><tr><td>AoA (Huang et al., 2019)</td><td>38.9</td><td>29.3</td><td>-</td></tr><tr><td>X-LAN (Pan et al., 2020)</td><td>39.7</td><td>29.5</td><td>23.4</td></tr><tr><td>SimVLM (Wang et al., 2021)</td><td>40.6</td><td>33.7</td><td>25.4</td></tr><tr><td>OSCAR (Li et al., 2020)</td><td>40.5</td><td>29.7</td><td>22.8</td></tr><tr><td>Unifying (Huang et al., 2021)</td><td>37.3</td><td>28.2</td><td>21.9</td></tr><tr><td>L-verse (Kim et al., 2022)</td><td>39.9</td><td>31.4</td><td>23.3</td></tr><tr><td>OFA (Wang et al., 2022)</td><td>41.0</td><td>30.9</td><td>24.2</td></tr><tr><td>Ours(I2T)</td><td>39.6</td><td>29.3</td><td>23.4</td></tr></table>\n\n\nTable 2: Evaluation of image caption on the MSCOCO Karpathy split. The first section is the text-conditioned results while the model in second part includes multimodal solution.\n\n\nText Quality In the case of image captions, we compared several other text description methods, the details are shown in Table 2, where  $I2T$  is the image-based text-generation task. Similarly, the first part of the table has pure image caption solutions and the bi-directional methods are demonstrated in the second part. Some results for image caption tasks are provided in Appendix G.1\n\n\nSimilarity In the multimodal generation phase, all the other methods for comparison need the input of at least one specified modality; instead our method generates both visual and verbal features. The generated vision-language pairings are shown in Fig. 4 while more pair samples are provided in Appendix G.4. In this experiment, we use CLIP scores to evaluate the similarity between our generated visual and verbal modalities, while generated images with given texts were used for other methods. The comparison results are given in Table 3, where a higher correlation between the text/image samples of our model can be found.\n\n\n# 4.2 CROSS MODAL MANIPULATION\n\n\nTo demonstrate the modification potential of our vision-language federation, we present the outcomes of picture alteration with description modification in Fig. 5 and more samples can be found in Appendix G.3. In this experiment, we obscured a portion of the image and modified the text description as well, the tokens in the masked region are merely designated as [MASK] tokens. We anticipate that the model will complete the image based on the amended text, in conjunction with the description based on the visible image region. Depending on the unmasked images, our model may supplement the caption that is derived from a portion of the image. Moreover, based on the amended description, the model may enhance the appearance of the masked component. These findings indicate that our model is capable of modifying text and pictures in both ways, in addition to jointly generating text and images.\n\n\n# 4.3 ABLATION STUDY\n\n\nIn the ablation test, we utilized a rapid sampling scheme, with an interval between each step of 10. First, we evaluated the outcomes without mutual attention, where we replaced all mutual attention in the model structure with causal self-attention. In addition, we evaluated the performance without the unified transition matrix, we employed a matrix similar to Equation 4, with the out-of-bound outputs substituted by the minimal ordinal number of the corresponding modal.\n\n\n<table><tr><td>(\u2191)</td><td>CUB</td><td>COCO</td></tr><tr><td>Eval set</td><td>0.286</td><td>0.306</td></tr><tr><td>VQ-Diff (Gu et al., 2022)</td><td>-</td><td>0.257</td></tr><tr><td>iVQ-Diff (Tang et al., 2022)</td><td>-</td><td>0.304</td></tr><tr><td>Unifying (Huang et al., 2021)</td><td>-</td><td>0.309</td></tr><tr><td>OFA (Wang et al., 2022)</td><td>-</td><td>0.344</td></tr><tr><td>Ours(Pair)</td><td>0.280</td><td>0.301</td></tr><tr><td>Ours(T2I)</td><td>0.293</td><td>0.306</td></tr><tr><td>Ours(I2T)</td><td>0.302</td><td>0.312</td></tr></table>\n\n\nTable 3: The CLIP similarity between the generated captions and images.\n\n\n<table><tr><td>(\u2193)</td><td>FID</td></tr><tr><td>Ours</td><td>17.38</td></tr><tr><td>w/ Fast Sampling \u0394 t = 10</td><td>20.96</td></tr><tr><td>w/o Mutual Attention</td><td>23.19</td></tr><tr><td>w/o Unified Transition Matrix</td><td>32.63</td></tr><tr><td>Ours (T2I)</td><td>16.19</td></tr><tr><td>w/ Fast Sampling \u0394 t = 10</td><td>18.33</td></tr><tr><td>w/o Mutual Attention</td><td>21.65</td></tr><tr><td>w/o Unified Transition Matrix</td><td>28.93</td></tr></table>\n\n\nTable 4: Ablations studies. The experiments are conducted on the CUB-200.\n\n\n# 5 RELATED WORK\n\n\nDenoising Diffusion Model Sohl-Dickstein et al. (2015) initially presented the diffusion model, which consists of a base Gaussian diffusion and a basic binomial diffusion. Ho et al. (2020) recently reigned interest in the extraordinary generating capacity of diffusion models. This is extended in subsequent research to enhance quality with the classifier guidance (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021) and speed of sampling (Song et al., 2021a; Salimans & Ho, 2021). In addition to continuous diffusion models that have received the significant attention, there is also substantial research based on discrete diffusion models (Hoogeboom et al., 2021; Austin et al., 2021) for text generation or image segmentation. With the help of VQ-VAE, Esser et al. (2021a); Hu et al. (2022); Gu et al. (2022); Tang et al. (2022) discrete diffusion models have become capable of generating high quality images.\n\n\nModality Translation Text-to-image generation is one of the key aspects of modality translation tasks. On simpler datasets, traditional GAN-based models (Xu et al., 2018; Zhu et al., 2019) may create high-quality pictures based on text hints, but face difficulties on more complex datasets. As a result of the emergence of transformers employing the attention mechanism (Vaswani et al., 2017), conventional autoregressive models (Van den Oord et al., 2016) have gained more powerful generating capacities as ART (Auto-regressive Transformers) (Ramesh et al., 2021; Esser et al., 2021b; Yu et al., 2022). Other types of modality translation models involve the addition of prospective modalities as criteria for improved generation outcomes (Gafni et al., 2022). There are also some works that focus on addressing bidirectional generation between different modalities (Kim et al., 2022; Huang et al., 2021; Wang et al., 2022); however, these solutions are task-specific, necessitating different input formats and specific models with distinct modalities.\n\n\n# 6 CONCLUSION\n\n\nThis work presents UniD3, a novel unified framework for multi-modality generation tasks. We firstly introduced a unified transition matrix for the diffusion process which permits the states of fused tokens to be corrupted in a stable way. We demonstrated that designing transition matrices based on the objective and data characteristics is beneficial for discrete diffusion models. To capture the connection from the various modalities, we suggested a mutual attention with fuse embedding for noiseless state recovery. Our method is also generic and competent to perform modality translation tasks for the specified modality. The specific designs of transition matrix are intriguing for further exploring, as well as the performance of more modalities is also one of the future directions.\n\n\n# 7 ETHICS\n\n\nIn this work, we propose UniD3, a new method for multimodal multi-task generation based on the discrete denoising diffusion model. All datasets evaluated in our experiments are open source, publicly accessible, and used with permission.\n\n\nSimilar to other generation algorithms, our method has both beneficial and negative societal effects, depending on its application and usage.\n\n- - Positively, UniD3 may explore the limits of deep learning generation through a unique pattern of graphic pair generation, while letting the typical user's imagination run wild and decreasing the entrance barrier for visual content production.\n\n- - On the other hand, UniD3 may be used to make modified photographs of great quality that are difficult to discern from the originals. Ways to fool humans and propagate falsehoods that are malicious. UniD3 can be abused by nefarious users, which may have serious repercussions.\n\n- - In our code distribution. We shall outline the permitted uses of our system and give the corresponding licences.\n\n\nHowever, we observe that the present discriminators used to distinguish generated images are ineffective at identifying images generated by diffusion models. There is still the necessity for more exploration to discern between authentic and fake images.\n\n\n# REFERENCES\n\n- Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6077-6086, 2018.\n\n- Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981-17993, 2021.\n\n- Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18208-18218, 2022.\n\n- Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780-8794, 2021.\n\n- Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis. Advances in Neural Information Processing Systems, 34, 2021a.\n\n- Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873-12883, 2021b.\n\n- Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203.13131, 2022.\n\n- Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696-10706, 2022.\n\n- Simao Herdade, Armin Kappeler, Kofi Boakye, and Joao Soares. Image captioning: Transforming objects into words. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\n- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020.\n\n- Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454-12465, 2021.\n\n- Minghui Hu, Yujie Wang, Tat-Jen Cham, Jianfei Yang, and Ponnuthurai N Suganthan. Global context with discrete diffusion in vector quantised modelling for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11502-11511, 2022.\n\n- Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. Attention on attention for image captioning. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4634-4643, 2019.\n\n- Yupan Huang, Hongwei Xue, Bei Liu, and Yutong Lu. Unifying multimodal transformer for bidirectional image and text generation. In Proceedings of the 29th ACM International Conference on Multimedia, pp. 1138-1147, 2021.\n\n- Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.\n\n- Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, and Kyunghoon Bae. L-verse: Bidirectional generation between image and text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16526-16536, 2022.\n\n- Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, et al. Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages, 2(3):18, 2017.\n\n- Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11523-11532, 2022.\n\n- Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pp. 121-137. Springer, 2020.\n\n- Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr D\u00f3ll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740-755. Springer, 2014.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.\n\n- Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11461-11471, 2022.\n\n- Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pp. 8162-8171. PMLR, 2021.\n\n- Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks for image captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10971-10980, 2020.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. PMLR, 2021.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.\n\n- Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video transformer. In VISIGRAPP (5: VISAPP), 2021.\n\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821-8831. PMLR, 2021.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684-10695, 2022.\n\n- Chitwan Sahara, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pp. 1-10, 2022a.\n\n- Chitwan Sahara, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b.\n\n- Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2021.\n\n- Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.\n\n- Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256-2265. PMLR, 2015.\n\n- Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021a.\n\n- Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021b.\n\n- Zhicong Tang, Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen. Improved vector quantized diffusion models. arXiv preprint arXiv:2205.16007, 2022.\n\n- Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A simple and effective baseline for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16515-16525, 2022.\n\n- Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelCNN decoders. Advances in neural information processing systems, 29, 2016.\n\n- Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\n- Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. California Institute of Technology, 2011.\n\n- Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pp. 23318-23340. PMLR, 2022.\n\n- Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In International Conference on Learning Representations, 2021.\n\n- Wansen Wu, Tao Chang, and Xinmeng Li. Visual-and-language navigation: A survey and taxonomy. arXiv preprint arXiv:2108.11544, 2021.\n\n- Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attingan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1316-1324, 2018.\n\n- Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n\n- Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In International Conference on Learning Representations, 2021.\n\n- Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\n\n- Chuanxia Zheng, Long Tung Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35, 2022.\n\n- Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17907-17917, 2022.\n\n- Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5802-5810, 2019.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638521.496442", "msecs": "496.0", "relativeCreated": "118326.9395828247", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Optimal Transport for Long-Tailed Recognition with Learnable Cost Matrix\n\n# Abstract\nIt is attracting attention to the long-tailed recognition problem, a burning issue that has become very popular recently. Distinctive from conventional recognition is that it posits that the allocation of the training set is supremely distorted. Predictably, it will pose challenges to the generalisation behaviour of the model. Approaches to these challenges revolve into two groups: firstly, training-aware methods, with the aim of enhancing the generalisability of the model by exploiting its potential in the training period; and secondly, post-hoc correction, liberally coupled with training-aware methods, which is intended to refine the predictions to the extent possible in the post-processing stage, offering the advantages of simplicity and effectiveness. This paper introduces an alternative direction to do the post-hoc correction, which goes beyond the statistical methods. Mathematically, we approach this issue from the perspective of optimal transport (OT), yet, choosing the exact cost matrix when applying OT is challenging and requires expert knowledge of various tasks. To overcome this limitation, we propose to employ linear mapping to learn the cost matrix without necessary configurations adaptively. Testing our methods in practice, along with high efficiency and excellent performance, our method surpasses all previous methods and has the best performance to date.\n\n\n# 1 INTRODUCTION\n\n\nClassification problems in the real world are generally challenged by the long-tailed label distribution, i.e., having a small number of samples for a majority of labels, and a dominant number of samples for a minority of labels (Van Horn & Perona, 2017; Buda et al., 2018; Liu et al., 2019) on the training data set. It is also known as imbalanced recognition, which has been widely studied in the past decades (Cardie & Howe, 1997; Chawla et al., 2002; Qiao & Liu, 2009; Cui et al., 2019). These distribution biases pose a significant challenge to predictive modeling; conceivably, models often suffer from poor generalisation and undesirable estimation bias (Cao et al., 2019; Kang et al., 2019; Zhou et al., 2020).\n\n\nRecently, a renewed interest in the problem of long-tail recognition has emerged following the context of neural networks, as numerous publications in the literature endeavour to resolve the problem albeit in different ways including decouple (Kang et al., 2019), meta-learning (Ren et al., 2020; Wang et al., 2020a; Li et al., 2021), post-hoc correction (Tang et al., 2020; Hong et al., 2020), etc (Liu et al., 2019; Cao et al., 2019; Tang et al., 2020). One of the representative methods of post-hoc correction, Logit Adjustment Menon et al. (2020), provides a statistical correction to the prediction, receiving widespread attention for its simplicity and validity, but the downside is that its optimisation is conducted on individual samples, without any indication that rectified marginal distribution satisfies the desired distribution we are striving for.\n\n\nFiguring out how to solve the above problem, our explicit modeling of the problem mathematically turns into an equational constraint, meanwhile to minimise the difference between refined distribution and the original one, this minimisation is motivated upon the inner-product similarity. A little further, the resulting problem can be linked to OT. Drawing on this linkage, we develop it further by proposing a linear mapping to automatically learn cost matrix, thereby circumventing the requirement for expert knowledge to configure this matrix. In short, we contribute as follows:\n\n\n- We propose an alternative direction based on convex optimisation to do post-hoc correction, which goes beyond previous direction from the statistical view.\n\n- - Imposing marginal distributions to align ideal marginal distributions, we derive an optimisation problem tied to the OT that is solved using Sinkhorn, a highly efficient algorithm. More further, for better learning of the cost matrix, we present a linear mapping enabling very elegant learning with one-layer neural network.\n\n- - The experimental evidence shows the high efficiency of the algorithm, and the best performance achieved on three benchmarks, also supports our claim that addressing the post-hoc problem via OT is helpful and effective.\n\n\n# 2 PRELIMINARIES\n\n\nIn this section, we begin by giving a notational definition, followed by an introduction to the long-tailed recognition problem, and finally, we briefly review the OT, and give a quick recap of Logit Adjustment Menon et al. (2020).\n\n\nNotations: In what follows, for two matrices  $\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{N \\times K}$ , we denote  $\\langle \\mathbf{X}, \\mathbf{Y} \\rangle = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\mathbf{X}_{nk} \\mathbf{Y}_{nk}$  as the Frobenius dot-product.  $\\delta(\\cdot)$  stands for the Dirac function,  $p(\\cdot)$  represents the probability distribution.  $U(\\boldsymbol{r}, \\boldsymbol{c}) = \\{\\boldsymbol{P} \\in \\mathbb{R}_+^{N \\times K} | \\boldsymbol{P} \\mathbf{1}_K = \\boldsymbol{r}, \\boldsymbol{P}^\\intercal \\mathbf{1}_N = \\boldsymbol{c}\\}$ , where  $\\mathbf{1}_N$  and  $\\mathbf{1}_K$  are  $N$ -dimension and  $K$ -dimension vector whose elements are all 1.  $\\boldsymbol{r}$  and  $\\boldsymbol{c}$  refer to the vectors of size  $N$  and  $K$  respectively,  $U(\\boldsymbol{r}, \\boldsymbol{c})$  include all matrices with row and column sums  $\\boldsymbol{r}$  and  $\\boldsymbol{c}$  respectively\n\n\n# 2.1 PROBLEM FORMULATION\n\n\nHaving a collection of training samples  $\\{(x_n^s, y_n^s)\\}_{n=1}^{N_s}$ , validation samples  $\\{(x_n^v, y_n^v)\\}_{n=1}^{N_v}$  and test samples  $\\{(x_n^t, y_n^t)\\}_{n=1}^{N_t}$  for classification with  $K$  labels and input  $\\mathbf{x} \\in \\mathbb{R}^d$ , long-tailed recognition assumes that the class-prior distribution for training data  $p(y^s)$  is different from that for validation data  $p(y^v)$  and test data  $p(y^t)$ . Specifically, long-tailed recognition means the distribution  $p(y^s)$  is highly skewed, some classes have the dominant number of samples, while tailed labels own a very small number of samples. We can use imbalance ratio to measure the skewness in training data set, which can be defined as  $R = \\frac{N_{max}^s}{N_{min}^s}$ , where  $N_{max}^s$  and  $N_{min}^s$  denote the largest and smallest number of samples in the training data set, respectively. In this paper, we assume that the marginal distribution of the test set is known, we consider it as an implicit prior knowledge to be applied. Stepping back, even if we do not know the marginal distribution of the test dataset in advance. There are still ways to estimate the marginal distribution of the test dataset relatively precisely, such as methods in Hendrycks et al. (2018); Azizzadenesheli et al. (2019).\n\n\nObviously, most models trained on imbalanced training data set would suffer from extremely limited generalisation ability. Hence the ultimate goal is to learn a model that minimises the empirical risk, which can be formulated as:\n\n\n$$\n\n$$\n\\mathcal {J} \\left(\\Phi \\left(\\boldsymbol {x} _ {n} ^ {s}\\right), y _ {n} ^ {s}\\right) = \\frac {1}{N _ {s}} \\sum_ {n = 1} ^ {N _ {s}} \\mathcal {L} \\left(\\Phi \\left(\\boldsymbol {x} _ {n} ^ {s}\\right), y _ {n} ^ {s}\\right), \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\Phi (\\pmb{x}_n^s)\\in \\mathbb{R}^K$  denotes logits with associated sample,  $\\Phi (\\cdot):\\mathbb{R}^d\\to \\mathbb{R}^K$  represents the mapping via neural networks,  $\\mathcal{L}$  stands for the loss function, typically cross entropy loss function for classification problem.\n\n\n# 2.2 REMIN DERS ON OPTIMAL TRANSPORT\n\n\nOT is used to calculate the cost of transporting one probability measure to another. We next present a brief introduction to OT with the aim of helping us better view the long-tailed problem from an OT perspective.\n\n\nConsider probability measures  $r$  and  $c$  of the two random variables  $X$  and  $Y$  respectively, when the cost function  $\\mathcal{C}(X,Y): X \\times Y \\to \\mathbb{R}_+$  is defined to denote the cost of transporting  $X$  to  $Y$ , the OT distance can be defined as:\n\n\n$$\n\n$$\nd (r, c) = \\min  _ {\\boldsymbol {\\pi} \\in \\Pi (r, c)} \\int_ {X \\times Y} \\mathcal {C} (\\boldsymbol {x}, \\boldsymbol {y}) \\pi (\\boldsymbol {x}, \\boldsymbol {y}) d \\boldsymbol {x} d \\boldsymbol {y}, \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\Pi(r, c) = \\left\\{\\int_{\\mathcal{Y}} \\pi(\\boldsymbol{x}, \\boldsymbol{y}) d\\boldsymbol{y} = r, \\int_{X} \\pi(\\boldsymbol{x}, \\boldsymbol{y}) d\\boldsymbol{x} = c\\right\\}$  is the joint probability measure with  $r$  and  $c$ . When we extend the above continuous case to the discrete situation, we consider following discrete distributions:\n\n\n$$\n\n$$\n\\boldsymbol {r} = \\sum_ {i = 1} ^ {N} p _ {i} (\\boldsymbol {x} _ {i}) \\delta (\\boldsymbol {x} _ {i}) \\quad \\boldsymbol {c} = \\sum_ {j = 1} ^ {K} p _ {i} (\\boldsymbol {y} _ {j}) \\delta (\\boldsymbol {y} _ {j}) \\tag {3}\n$$\n\n$$\n\n\nwhere  $p_i(\\boldsymbol{x}_i)$  and  $p_i(\\boldsymbol{y}_j)$  represent the probability mass to the sample  $\\boldsymbol{x}_i$  and  $\\boldsymbol{y}_j$  respectively. In this context, OT distance can be expressed as:\n\n\n$$\n\n$$\nd _ {M} (\\boldsymbol {r}, \\boldsymbol {c}) = \\min  _ {\\boldsymbol {P} \\in U (\\boldsymbol {r}, \\boldsymbol {c})} \\langle \\boldsymbol {P}, \\boldsymbol {M} \\rangle . \\tag {4}\n$$\n\n$$\n\n\nwhere  $M$  stands for the cost matrix constructed by  $M_{ij} = \\mathcal{C}(\\pmb{x}_i, \\pmb{y}_j)$ . The goal of OT is to find a transportation matrix  $\\pmb{P}$  that minimizes the distance  $d_M(\\pmb{r}, \\pmb{c})$\n\n\nAs we can see, OT is a distance measure between two probability distributions under some cost matrix (Villani, 2008). However, when we use network simplex or interior point methods to solve the above optimization problem, it often comes at the cost of heavy computational demands. To tackle this issue, OT with entropy constraint is proposed to allow the optimisation at small computational cost in sufficient smoothness (Cuturi, 2013). By adding a Lagrangian multiplier to the entropy constraint, the new formulation can be defined as follows:\n\n\n$$\n\n$$\nd _ {M} ^ {\\lambda} (\\boldsymbol {r}, \\boldsymbol {c}) = \\langle \\boldsymbol {P} ^ {\\lambda}, \\boldsymbol {M} \\rangle \\quad \\text {w h e r e} \\quad \\boldsymbol {P} ^ {\\lambda} = \\underset {\\boldsymbol {P} \\in U (\\boldsymbol {r}, \\boldsymbol {c})} {\\arg \\min } \\langle \\boldsymbol {P}, \\boldsymbol {M} \\rangle - \\lambda h (\\boldsymbol {P}), \\tag {5}\n$$\n\n$$\n\n\nwhere  $\\lambda \\in [0, +\\infty]$ ,  $h(P) = -\\sum_{n=1}^{N}\\sum_{k=1}^{K}P_{nk}\\log P_{nk}, d_M^\\lambda(r,c)$  is also known as dual-Sinkhorn divergence, besides, it can be calculated with matrix scaling algorithms for cheaper computational demand. The following lemma guarantees the convergence and uniqueness of the solution of the algorithm.\n\n\nLemma 1 For  $\\lambda >0$ , the solution  $P^{\\lambda}$  is unique and has the form  $P^{\\lambda} = \\mathbf{diag}(u)K\\mathbf{diag}(v)$ , where  $\\mathbf{u}$  and  $\\mathbf{v}$  are two non-negative vectors uniquely defined up to a multiplicative factor and  $\\mathbf{K} = e^{-M / \\lambda}$  is the element-wise exponential of  $-M / \\lambda$ .\n\n\nThe above lemma states the uniqueness of  $P^{\\lambda}$  (Sinkhorn, 1974), and  $P^{\\lambda}$  can be efficiently computed via Sinkhorn's fixed point iteration  $\\mathbf{u}, \\mathbf{v} \\gets r. / K\\mathbf{v}, c. / K^{\\intercal}\\mathbf{u}$ .\n\n\n# 2.3 A QUICK RECAP OF LOGIT ADJUSTMENT\n\n\nWe give a brief introduction to Logit Adjustment (Menon et al., 2020; Hong et al., 2020). For the model  $\\Phi(\\cdot)$ , it is trained by the standard cross-entropy loss function on imbalanced training data set, and evaluated on test data. In this algorithm, the test logit is adjusted as follows:\n\n\n$$\n\n$$\n\\Phi \\left(\\boldsymbol {x} _ {n} ^ {t}\\right) = \\Phi \\left(\\boldsymbol {x} _ {n} ^ {t}\\right) - \\log p \\left(y ^ {s}\\right) \\tag {6}\n$$\n\n$$\n\n\nThis simple procedure is derived from the Bayes optimal rule. It is apparent that Logit Adjustment involves a post hoc correction on an individual sample, which does not necessarily guarantee that the marginal distribution of the whole dataset matches the desired distribution.\n\n\n# 3 METHODOLOGY\n\n\nThe first part of this section explores post-hoc correction from an OT perspective, proceeds to the automatic learning of the cost matrix via linear mapping. Lastly, we demonstrate how it can be achieved simply with one-layer neural network.\n\n\n# 3.1 POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE\n\n\nSince Logit Adjustment applies adjustment at the individual sample level. It doesn't assure that the marginal distribution of the overall data set fulfils our desired distribution. In this respect, we clearly put the constraint into an equation:\n\n\n$$\n\n$$\n\\boldsymbol {Y} ^ {\\intercal} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\tag {7}\n$$\n\n$$\n\n\nwhere  $\\mathbf{Y} \\in \\mathbb{R}^{N \\times K}$  indicates the refined prediction value in matrix form,  $\\pmb{\\mu}$  represents the expected distribution on the test set. Alternatively, it is desirable to preserve another characteristic of  $\\mathbf{Y}$ , namely, remaining almost as similar to the original prediction as possible. We consider inner-product based similarity to measure the similarity, which is a straightforward yet useful similarity measure. In mathematical terms, this can be expressed as follows:\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y}} {\\operatorname {m a x i m i z e}} \\quad \\langle C (\\hat {\\boldsymbol {Z}}), \\boldsymbol {Y} \\rangle , \\tag {8}\n$$\n\n$$\n\n\nwhere  $\\hat{Z}$  represents the original prediction in matrix form,  $C(\\cdot)$  denotes to perform some transformation to  $\\hat{Z}$ , it can be some simple function, like Logarithmic function  $\\log(z)$ , exponential function  $z^{\\alpha}$ . Here we select  $-\\log(\\cdot)$  as the cost function. This choice was driven by the requirement that the cost matrix must be positive definite, whereas the transformation of the original prediction by  $-\\log(\\cdot)$  satisfies this condition, in addition, as log likelihood represents the local probability density of the associated samples, it can also be used to substitute  $\\hat{Z}$  for the similarity approximation. In brief, the resulting numerical form can be put in formal terms as follows:\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y}} {\\text {m i n i m i z e}} \\quad \\langle - \\log (\\hat {\\boldsymbol {Z}}), \\boldsymbol {Y} \\rangle \\tag {9}\n$$\n\n$$\n\n\n$$\n\n$$\n\\text {s u b j e c t} \\quad \\boldsymbol {Y} ^ {\\mathsf {T}} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\quad \\boldsymbol {Y} \\mathbf {1} _ {K} = \\mathbf {1} _ {N}. \\tag {10}\n$$\n\n$$\n\n\nExtra constraint on  $\\mathbf{Y}$  are imposed simply cos the tuned estimation has to fulfil the basic probabilistic requirement that its sum is one. Comparing Eq. (9-10) with Eq. (4), we can see that if we substitute  $\\mathbf{P}$  with  $\\mathbf{Y}$  and substitute  $\\mathbf{r}$  and  $\\mathbf{c}$  with  $\\mathbf{1}_N$  and  $\\boldsymbol{\\mu}$  respectively, we find that the above optimization problem is actually a special case of OT.\n\n\nIn preliminaries, the entropy regularised OT is introduced (EOT), by adding entropy regularisation to OT, the given equation can be solved efficiently by Sinkhorn algorithm. Specifically, the optimisation problem is:\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y}} {\\text {m i n i m i z e}} \\quad \\left\\langle - \\log (\\hat {\\boldsymbol {Z}}), \\boldsymbol {Y} \\right\\rangle + \\lambda \\boldsymbol {Y} \\log (\\boldsymbol {Y}) \\tag {11}\n$$\n\n$$\n\n\n$$\n\n$$\n\\text {s u b j e c t} \\quad \\boldsymbol {Y} ^ {\\intercal} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\quad \\boldsymbol {Y} \\mathbf {1} _ {K} = \\mathbf {1} _ {N}. \\tag {12}\n$$\n\n$$\n\n\nThe associated algorithmic flow is outlined in detail in Algorithm 1.\n\n\n```txt\nInput: Cost matrix  $M$ , trade-off parameter  $\\lambda$ , max number of iterations  $N_T$ , iteration number  $t$ , error threshold  $\\epsilon$ , current error  $\\delta$ , two probability vectors  $\\mathbf{r}$  and  $\\mathbf{c}$ ,  $|\\cdot|$  denotes the vector norm.\n```\n\n\n```txt\n1. Initialise  $\\pmb{K} = e^{-M / \\lambda},\\pmb{u}_{old} = \\mathbf{1}_N,\\pmb {v} = \\mathbf{1}_K,t = 0;$\n```\n\n\n```txt\n2 while  $t \\leq N_T$  and  $\\delta \\leq \\epsilon$  do\n```\n\n\n```txt\n3  $\\begin{array}{rl} & {\\pmb {u} = \\pmb {r}. / \\pmb {K}\\pmb {v};}\\\\ & {\\pmb {v} = \\pmb {c}. / \\pmb{K}^{\\intercal}\\pmb {u};}\\\\ & {\\sigma = |\\pmb{u}_{old} - \\pmb {u}|;}\\\\ & {\\pmb{u}_{old} = \\pmb {u};}\\\\ & {t = t + 1;} \\end{array}$    \n4   \n5   \n6   \n7\n```\n\n\n```txt\ns end\n```\n\n\n```latex\n9 Output  $\\pmb{Y} = \\mathbf{diag}(\\pmb{u})\\pmb{K}\\mathbf{diag}(\\pmb{v})$\n```\n\n\nAssigning  $\\lambda$  with 1, it is observed that we equate our objective function to the KL divergence, thus illustrating the extensive nature of our approach. DARP (Kim et al., 2020a) has previously applied it to long-tailed semi-supervised classification.\n\n\nRemark We would like to illustrate the non-applicable scenarios of our method. Firstly, our method requires a large number of samples for evaluation. This is because if the batch size is small, we can not guarantee that the desired marginal distribution can be satisfied within the batch. In some online scenarios, the sample-wise correction method is more suitable. In addition, our method assumes that the marginal distribution is already known, and in the paper we assume in particular that it is consistent with a uniform distribution.\n\n\n![Figure 1: Our proposed framework OTLM. The logit  $\\Phi(x)$  is fed into a single-layer feed-forward network to infer  $\\tilde{\\mathbf{Z}}$ . The cost matrix  $M$  is set to  $-\\log \\tilde{\\mathbf{Z}}$ . By solving the optimal transport problem via Sinkhorn algorithm, we can obtain the refined prediction  $\\mathbf{Y}$ .](images/9133a22319add0bd3e8fd0a9710c0179f099b050b88f2c773937afae5d50fe95.jpg)\n\n\n# 3.2 COST FUNCTION LEARNING VIA LINEAR MAPPING\n\n\nSimple functions is likely to be sub-optimal for the real data sets; this suggests the design of a better cost function to better fit the long-tailed recognition problem. However, manually designed cost functions require expert knowledge in different domains. Thus, we propose to use a linear mapping to automatically learn the cost function, which relieves the need to configuration.\n\n\nMore specifically, for predictions  $\\tilde{Z}$  generated by Softmax operation via leveraging linear transformation matrix  $\\mathbf{W}$ ,  $\\mathbf{W} \\in \\mathbb{R}^{K \\times K}$  is learned so that the following objective function is minimised.\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y} \\in \\mathbb {R}} {\\text {m i n i m i z e}} \\quad - \\left\\langle \\boldsymbol {Y}, \\log \\tilde {\\boldsymbol {Z}} \\right\\rangle + \\lambda \\left\\langle \\boldsymbol {Y}, \\log \\boldsymbol {Y} \\right\\rangle , \\tag {13}\n$$\n\n$$\n\n\n$$\n\n$$\n\\boldsymbol {Y} \\mathbf {1} _ {K} = \\mathbf {1} _ {N} \\quad \\boldsymbol {Y} ^ {\\intercal} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\quad \\tilde {\\boldsymbol {Z}} _ {n k} = \\frac {\\exp \\left(\\boldsymbol {W} ^ {\\intercal} \\boldsymbol {\\Phi} \\left(\\boldsymbol {x} _ {n}\\right)\\right) _ {k}}{\\sum_ {k} ^ {K} \\exp \\left(\\boldsymbol {W} ^ {\\intercal} \\boldsymbol {\\Phi} \\left(\\boldsymbol {x} _ {n}\\right)\\right) _ {k}}.\n$$\n\n$$\n\n\nThe resulting formula can be illustrated using a simple one-layer network of weight parameter  $W$ , together with an error function in Eq. (13). By normalising each row of  $W$ , each element can be understood as the probability value of confusing two classes, and somehow as a transition matrix between classes. We initialise  $W$  with an identity matrix and use small learning rate to learn  $W$ . One could also absorb the Logit Adjustment term  $-\\log p(y^{s})$  as the fixed bias parameter into the network. Motivated by this description, we can use a general gradient descent algorithm, such as SGD, to optimise the error function. Take into account the implementation in practice, the direct calculation of Eq. (13) can be done using the Sinkhorn iterations (Cuturi, 2013; Frogner et al., 2015; Peyr\u00e9 et al., 2019) in mini-batch training efficiently. Besides, We term the proposed method as OTLM (Optimal Transport via Linear Mapping), Figure 1 illustrates the overview of OTLM.\n\n\nCompute the gradients w.r.t  $W$  With the optimisation in Algorithm 1, it was conducted on the overall data set with a comparatively large number of data. Quite a different scenario now, as minibatch training is more favoured when it comes to neural networks. For this reason, the optimisation workflow in Algorithm 1, as pointed by Peyre et al. (2019); Viehmann (2019), will have a problem of batch stabilisation. To this end, we perform logarithmic transformation to step 3-4 in Algorithm 1.\n\n\n$$\n\n$$\n\\log \\boldsymbol {u} = \\log \\boldsymbol {r} - \\log (\\boldsymbol {M} \\boldsymbol {v}) = \\log \\boldsymbol {r} - \\operatorname {l o g s u m e x p} \\left(\\frac {1}{\\lambda} \\boldsymbol {M} - \\log \\boldsymbol {v}\\right) \\tag {14}\n$$\n\n$$\n\n\n$$\n\n$$\n\\log \\boldsymbol {v} = \\log \\boldsymbol {c} - \\log \\left(\\boldsymbol {M} ^ {\\intercal} \\boldsymbol {u}\\right) = \\log \\boldsymbol {c} - \\operatorname {l o g s u m e x p} \\left(\\frac {1}{\\lambda} \\boldsymbol {M} - \\log \\boldsymbol {u}\\right). \\tag {15}\n$$\n\n$$\n\n\nwith log-sum-exp operation  $\\mathrm{logsumexp}(\\boldsymbol{x}) = \\log (\\sum_{i}\\exp \\boldsymbol{x}_{i})$ . Assuming convergence is achieved in the Sinkhorn's loops, it is not necessary for us manually to compute the derivative of the loss function in Eq. (13) w.r.t  $W$ . As a matter of fact, as long as we make sure that the entire optimisation process is differentiable, modern deep learning libraries can then automatically implement end-to-end derivations, as it allows us to differentiate between the results of Sinkhorn's loops as a mere composition of elementary operations.\n\n\nPerformance degradation caused by imbalanced distributions can be addressed with meta-learning based methods (Ren et al., 2020; Wang et al., 2020a; Li et al., 2021). Recent works have illustrated neu\n\n\nral networks can learn more meaningful representations from the validation data set  $\\{(\\pmb{x}_n^v, y_n^v)\\}_{n=1}^{N_v}$ . Inspired by the methods based on meta learning, Here we also take a validation data set to optimise the parameter  $\\pmb{W}$ , but with the still significant difference in that we require no labelling information, thus avoiding a large expanse of labeling samples.\n\n\n# 4 EXPERIMENTS\n\n\nIn this section, we first conduct experiments comparing our approach versus extant post-hoc correction methods on three data sets, including CIFAR-100-LT (Cao et al., 2019), ImageNet-LT (Liu et al., 2019), and iNaturalist (Van Horn et al., 2018) with varying backbones. Finally, we empirically make a comparison of our algorithm with alternative cutting-edge long-tailed recognition methods. Observe that, for one thing, a plausible coupling of our algorithm to any training-aware long-tailed recognition method exists. As an illustration of the potency and strong generalisation of our approach, we conducted post-hoc correction of the pseudo-predictions of both methods, which are fairly typical for the training stage of all data sets: The first is based on the cross entropy (CE), and the other is the recently proposed  $RIDE$  (Wang et al., 2020b), which is based on multi experts.\n\n\n# 4.1 DATA SETS AND BASELINES\n\n\nThe baseline methods and data set are briefly described here, with implementation details placed in the appendix. In our experiments, we assume that the marginal distribution of the test set is uniform due to the characteristics of the three data sets.\n\n\nBaselines: We compare our methods with (i) post-hoc correction methods including Logit Adjustment (Menon et al., 2020), DARP (Kim et al., 2020a), (ii) state-of-the-art methods including Focal Loss (Lin et al., 2017), LDAM (Cao et al., 2019), BBN (Zhou et al., 2020), Balanced Softmax (Ren et al., 2020), Causal Norm (Tang et al., 2020), LADE (Hong et al., 2020), M2m (Kim et al., 2020b), Decouple (Kang et al., 2019), LFME (Xiang et al., 2020), RIDE (Wang et al., 2020b)\n\n\nLong-tailed data set: We take experiments on three data sets including CIFAR-100-LT, ImageNet-LT, and iNaturalist. We build the imbalanced version of CIFAR-100 by downsampling samples per class following the profile in Liu et al. (2019); Kang et al. (2019) with imbalanced ratio 10, 50, and 100. For all the benchmarks, we evaluate the performance on the test data set using a model trained on the training data set, and report the results using top-1 accuracy.\n\n\n# 4.2 MAIN RESULTS\n\n\nTable 1 illustrates the results on the CIFAR-100-LT data set. For CE-based training-aware methods, it first shows that Logit Adjustment is indeed a simple yet effective method for post-hoc correction. Remarkably, it outperforms the baseline methods by  $2.6\\%$ ,  $4.1\\%$ , and  $4.1\\%$  under the imbalance ratio of 10, 50, and 100 respectively. With a quick look at the results based on the OT approach, the superior results stress the advantages of our approach over the DARP and Logit adjustments. OT algorithm can further improve the performance by  $0.62\\%$ ,  $0.43\\%$ ,  $0.70\\%$ , for OTLM, it outperforms\n\n\nTable 1: Comparison on the top-1 accuracy with post-hoc correction methods on CIFAR-100-LT data set using ResNet-32 backbone, ImageNet-LT and iNaturalist data set using ResNet-50 and ResNeXt-50-32x4d backbones. Better results are marked in bold, the small red font indicates the increase in accuracy, compared to Logit Adjustment and RIDE, '-' means the results are not reported. CE indicates that a cross-entropy loss function is used in the training stage.\n\n\n<table><tr><td>Data set</td><td></td><td colspan=\"2\">CIFAR-100</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Imabalced Ratio</td><td>10</td><td>50</td><td>100</td><td>-</td><td>-</td></tr><tr><td>Baseline (CE)</td><td>59.00</td><td>45.50</td><td>41.00</td><td>43.10</td><td>65.00</td></tr><tr><td>Logit Adjustment+CE</td><td>61.63</td><td>49.62</td><td>45.11</td><td>51.62</td><td>69.44</td></tr><tr><td>DARP+CE</td><td>61.78</td><td>49.52</td><td>45.13</td><td>51.20</td><td>71.18</td></tr><tr><td>OT+CE</td><td>62.25+0.62</td><td>50.05+0.43</td><td>45.81+0.70</td><td>51.87+0.25</td><td>71.20+1.76</td></tr><tr><td>OTLM+CE</td><td>62.18+0.55</td><td>50.35+0.73</td><td>46.10+0.99</td><td>52.35+0.73</td><td>72.30+2.86</td></tr><tr><td>RIDE</td><td>-</td><td>-</td><td>50.20</td><td>57.50</td><td>72.90</td></tr><tr><td>OT+RIDE</td><td>-</td><td>-</td><td>50.82+0.62</td><td>58.45+0.95</td><td>75.85+2.95</td></tr><tr><td>OTLM+RIDE</td><td>-</td><td>-</td><td>51.43+1.23</td><td>58.71+1.21</td><td>76.12+3.22</td></tr></table>\n\n\nLogit Adjustment by  $0.55\\%$ ,  $0.73\\%$ ,  $0.99\\%$  under the three imbalance ratios on CIFAR-100-LT data set. For RIDE-based training-aware methods, there is also a  $0.6\\%$  improvement in the accuracy of our method.\n\n\nTable 1 also provides the results on ImageNet-LT and iNaturalist data sets. The fact again attracts our attention that for CE-based training-aware methods, OT, which is based on convex optimisation, consistently outperform Logit Adjustment by a large margin (1.76%) on iNaturalist data set. The accuracy boosting indicates the great potential of methods based on optimisation for post-hoc correction. Besides, not surprisingly, OTLM can further enhance the prediction accuracy, it outperforms Logit Adjustment by 0.73% and 2.86% on ImageNet-LT and iNaturalist respectively. As for RIDE, the gain in accuracy is also significant, about 3% on iNaturalist.\n\n\n# 4.3 COMPARISON WITH THE STATE-OF-THE-ART METHODS\n\n\nArmed with a demonstration of the validity of OT and OTLM, we switched to a comparison of our performance with existing methods that achieved the most advanced results on three benchmarks. As shown in Table 2, in particular, on ImageNet-LT and iNaturalist data sets, the experimental results are remarkable and impressive, our method outperforms RIDE by  $1.0\\%$  and  $3.0\\%$ , respectively. Since iNaturalist is a very difficult and fine-grained data set consisting of 8,142 categories. These huge performance gains come at a fraction of the cost, as we show in the next subsection.\n\n\nTable 2: Comparison on the top-1 accuracy with state-of-the-arts on CIFAR-100-LT data set using ResNet-32 backbone, on ImageNet-LT and iNaturalist data set using e ResNet-50 and ResNeXt-50-32x4d backbones respectively. - denotes the results are not reported, results underlined are the ones being compared, best results are marked in bold, the small red font denotes performance gain.\n\n\n<table><tr><td>Data Set</td><td colspan=\"3\">CIFAR-100</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Imabalced Ratio</td><td>10</td><td>50</td><td>100</td><td>-</td><td>-</td></tr><tr><td>Cross Entropy (CE)</td><td>59.0</td><td>45.5</td><td>38.3</td><td>43.1</td><td>65.0</td></tr><tr><td>Focal Loss</td><td>55.8</td><td>44.3</td><td>38.4</td><td>-</td><td>-</td></tr><tr><td>CB-Focal</td><td>-</td><td>-</td><td>-</td><td>43.7</td><td>61.1</td></tr><tr><td>OLTR</td><td>-</td><td>-</td><td>-</td><td>41.9</td><td>63.9</td></tr><tr><td>NCM</td><td>-</td><td>-</td><td>-</td><td>47.3</td><td>63.1</td></tr><tr><td>norm</td><td>-</td><td>-</td><td>-</td><td>49.4</td><td>69.3</td></tr><tr><td>cRT</td><td>-</td><td>-</td><td>-</td><td>49.6</td><td>67.6</td></tr><tr><td>LWS</td><td>-</td><td>-</td><td>-</td><td>49.9</td><td>69.5</td></tr><tr><td>LDAM</td><td>-</td><td>-</td><td>-</td><td>-</td><td>64.6</td></tr><tr><td>LDAM+DRW</td><td>58.7</td><td>46.6</td><td>42.0</td><td>-</td><td>68.0</td></tr><tr><td>BBN</td><td>59.1</td><td>47.0</td><td>42.6</td><td>-</td><td>66.3</td></tr><tr><td>Causal Norm</td><td>59.6</td><td>50.3</td><td>44.1</td><td>51.8</td><td>63.9</td></tr><tr><td>M2m</td><td>58.2</td><td>-</td><td>43.5</td><td>-</td><td>-</td></tr><tr><td>LFME</td><td>-</td><td>-</td><td>43.8</td><td>-</td><td>-</td></tr><tr><td>Balanced Softmax</td><td>61.6</td><td>49.9</td><td>45.1</td><td>-</td><td>-</td></tr><tr><td>LADE</td><td>61.7</td><td>50.5</td><td>45.4</td><td>51.9</td><td>70.0</td></tr><tr><td>RIDE (6 experts)</td><td>-</td><td>-</td><td>50.2</td><td>57.5</td><td>72.9</td></tr><tr><td>Logit Adjustment</td><td>61.6</td><td>49.6</td><td>45.1</td><td>51.6</td><td>69.4</td></tr><tr><td>DARP</td><td>61.8</td><td>49.5</td><td>45.1</td><td>51.2</td><td>71.2</td></tr><tr><td>OT+CE</td><td>62.3+0.7</td><td>50.1</td><td>45.8</td><td>51.9</td><td>71.2</td></tr><tr><td>OTLM+CE</td><td>62.2</td><td>50.4+0.8</td><td>46.1</td><td>52.4</td><td>72.3</td></tr><tr><td>OT+RIDE (6 experts)</td><td>-</td><td>-</td><td>50.8+0.6</td><td>58.5+1.0</td><td>75.9+3.0</td></tr><tr><td>OTLM+RIDE (6 experts)</td><td>-</td><td>-</td><td>51.4+1.2</td><td>58.7+1.2</td><td>76.1+3.2</td></tr></table>\n\n\n# 4.4 COMPUTATION COST OF OT AND OTLM\n\n\nAs we have highlighted, the additional computational cost of OT and OTLM is particularly small, compared to that of training-aware methods. In Table 3, exact time of the evaluations on the ImageNet-LT and iNaturalist data sets is provided. Please note, the times here are measured from the start of each method to the final best performance. For the comparison between OT and OTLM, because\n\n\nTable 3: Time for different methods to execute on the ImageNet-LT and iNaturalist data sets. The times here are measured from the start of each method to the final best performance. Their running time are counted in seconds. Coupled with the prior results, an observation can be made that OT can be the best coordinate to trade off performance and efficiency in post-hoc correction methods.\n\n\n<table><tr><td>Data Set</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Logit Adjustment</td><td>0.3</td><td>1.2</td></tr><tr><td>OT</td><td>32.7</td><td>82.5</td></tr><tr><td>OTLM</td><td>134.4</td><td>256.5</td></tr></table>\n\n\nOTLM runs on the GPU, one optimization iteration of OTLM is in fact much faster than OT. Except for OTLM, which was run on an NVidia card (V100), the results come from a 28-core machine (2.20 Ghz Xeon). Firstly, we can observe that all post-hoc correction methods here are not quite time-consuming. For comparison, ImageNet-LT and iNaturalist can be trained on 4 NVidia cards (V100) for extremely long periods of time. Using ResNet-50-32x4d on the iNaturalist training data set, for example, with a batch size of 256, the amount of training time (in seconds) to perform 1 iteration is approximately 850. Also, if one differs the performance of each method, we gather that OT and OTLM provide the best balance of performance and running costs to match.\n\n\n# 4.5 CONVERGENCE BEHAVIOR OF OTLM\n\n\nTo verify that our adopted OTLM algorithm is really efficient. With ResNet-50 as the backbone, we plotted curves depicting the decreasing absolute error  $\\sigma$  with the number of iterations  $t$  on the ImageNet-LT validation data set. Image on the left in Figure 2 gives us a visualisation of the optimisation process. It is found that the absolute error  $\\sigma$  converges at around 60 iterations. In fact, we discovered that 10 or even fewer iterations were enough to yield satisfactory performance.\n\n\n![Figure 2: The left image shows the convergence of OTLM: The absolute error  $\\sigma$  decreases with iterations on the ImageNet-LT validation data set (20k samples), and we used ResNet-50 as the backbone to obtain the original predictions  $Z$ . The right image demonstrates the block in  $W$ , we trained on CIFAR-100 to produce the original predictions with OTLM to minimise the difference between the ideal and actual distributions. For ease of display, the values of the diagonal elements are relatively large and we will replace these values with zeros.](images/1e6fa388a2f2907bb4493cf441d4da8147d0f8579170acb3e35999f1319d3e7c.jpg)\n\n\n![](images/f746a69db0cbd8c10f982d58baa44ba83e8ee5afb892a594b0717dc9a46c636a.jpg)\n\n\n# 4.6 DETAILED ANALYSIS ON DISTRIBUTION MATCHING\n\n\nUpon distribution fitting, preferably the estimation bias is lessened, which implies that the KL divergence distance between the refined prediction and the target distribution is decreased. Table 4 displays the KL distances on the ImageNet-LT and iNaturalist data sets, and we can find that OT possesses a smaller KL distance than Logit Adjustment, which suggests that OT decreases the estimation bias more than Logit Adjustment by a much larger margin.\n\n\n# 4.7 VISUALISATION ON WEIGHT PARAMETER IN OTLM\n\n\nAs a natural consequence, one wonders about the exact structure of this weight matrix  $\\mathbf{W}$ , and no better way to present its results than through visualisation. Towards this end, we applied OTLM\n\n\nTable 4: KL distance between refined probability distribution and target distribution computed on ImageNet-LT and iNaturalist data sets. Smaller distance indicates two distributions are more similar.\n\n\n<table><tr><td>Data Set</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Logit Adjustment</td><td>1.78e-05</td><td>5.81e-06</td></tr><tr><td>DARP</td><td>8.49e-08</td><td>1.07e-08</td></tr><tr><td>OT</td><td>8.62e-10</td><td>4.72e-10</td></tr></table>\n\n\nto refine the original predictions on CIFAR-100 data set with an imbalanced ratio  $R = 100$  and ResNet-32 backbone, and trained a one-layer model until it converged to extract the value of  $W$ , where the shape of is  $100 \\times 100$ . We visualised a sub-block of size  $20 \\times 20$ , the illustration is presented on the right image in Figure 2. The very physical meaning of the elements in  $W$  can be interpreted as the probability of transition between classes. The visualised matrix is very dense, this non-sparsity suggests that the OTLM manages to correct the predictions of the model in the training stage, whilst minimising the distance between the desired and the actual distribution.\n\n\n# 5 RELATED WORK\n\n\nLong-tailed recognition: The great majority of current methods applied to long-tailed recognition can be divided into either training-aware and post-hoc correction methods. The training-aware methods aim to improve the generalisation ability of models trained on imbalanced training data set. Among them, re-sampling and re-weighting are two major methods. Re-sampling controls the class numbers by the means of over-sampling (Chawla et al., 2002) and under-sampling (Barandela et al., 2004; Drummond et al., 2003). The re-weighting approaches achieve balanced learning by giving smaller weights to majority classes and larger weights to minority classes (Tang et al., 2008; Zadrozny et al., 2003; Lin et al., 2017; Lee et al., 2017; Cui et al., 2019; Khan et al., 2019). Other approaches have also been proposed to resolve the imbalanced problem. For example, Cao et al. introduces a theoretically-principled loss function motivated by minimizing the generalisation error bound (Cao et al., 2019), Kang et al. proposes to decouple the learning phases into representation learning and classifier tuning (Kang et al., 2019). Apart from training-aware methods, post-hoc correction also offers us an avenue for long-tailed recognition problem, it can be applied to the normalisation of classifier weights (Kang et al., 2019; Zhang et al., 2019; Kim & Kim, 2020) or logits based on label frequency (Provost, 2000; Collell et al., 2016). Among them, Logit Adjustment (Menon et al., 2020) is a simple yet powerful method to use. Compared with Logit Adjustment, our approach performs the post-hoc correction from the optimisation view.\n\n\nOptimal transport: OT distances is a important family of distances for probability measures (Villani, 2008; Santambrogio, 2015). It has been extensively applied in different fields in machine learning. Courty et al. (Courty et al., 2016) proposes a regularised unsupervised optimal transportation model to perform the domain adaption. Other applications include 3D shape matching (Su et al., 2015), generative model (Arjovsky et al., 2017; Salimans et al., 2018; Bunne et al., 2019; Deshpande et al., 2019; Bhagoji et al., 2019), graph matching (Xu et al., 2019a;b), model designs Kandasamy et al. (2018); Chizat & Bach (2018). To the best of our knowledge, we are the first to perform the post-hoc correction from the OT perspective.\n\n\n# 6 CONCLUSION\n\n\nIn this paper, we present two techniques for post-hoc corrections from an optimisation perspective. Our approach reviews the basic mathematical formulation of the distribution alignment problem and relates the resulting formulation to OT, which we find, moreover, offers greater possibilities and flexibility to solve the problem. In order to avoid manual configuration of the cost matrix, we propose to learn the cost matrix automatically by linear mapping, which leads to further performance improvements. The experimental results confirm the superiority of our proposed approach. I believe our work can open up more possibilities and imagination for post-hoc corrections.\n\n\n# REFERENCES\n\n- Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pp. 214-223. PMLR, 2017.\n\n- Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning for domain adaptation under label shifts. arXiv preprint arXiv:1903.09734, 2019.\n\n- Ricardo Barandela, Rosa M Valdovinos, J Salvador S\u00e1nchez, and Francesc J Ferri. The imbalanced training sample problem: Under or over sampling? In Joint IAPR international workshops on statistical techniques in pattern recognition (SPR) and structural and syntactic pattern recognition (SSPR), pp. 806-814. Springer, 2004.\n\n- Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Lower bounds on adversarial robustness from optimal transport. arXiv preprint arXiv:1909.12272, 2019.\n\n- Paula Branco, Lu\u00eds Torgo, and Rita P Ribeiro. A survey of predictive modeling on imbalanced domains. ACM Computing Surveys (CSUR), 49(2):1-50, 2016.\n\n- Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. *Neural Networks*, 106:249\u2013259, 2018.\n\n- Charlotte Bunne, David Alvarez-Melis, Andreas Krause, and Stefanie Jegelka. Learning generative models across incomparable spaces. In International Conference on Machine Learning, pp. 851-861. PMLR, 2019.\n\n- Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. arXiv preprint arXiv:1906.07413, 2019.\n\n- Claire Cardie and Nicholas Howe. Improving minority class prediction using case-specific feature weights. 1997.\n\n- Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.\n\n- Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.\n\n- Guillem Collell, Drazen Prelec, and Kaustubh Patil. Reviving threshold-moving: a simple plug-in bagging ensemble for binary and multiclass imbalanced data. arXiv preprint arXiv:1606.08698, 2016.\n\n- Nicolas Courty, R\u00e9mi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853-1865, 2016.\n\n- Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9268-9277, 2019.\n\n- Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26:2292-2300, 2013.\n\n- Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David Forsyth, and Alexander G Schwing. Max-sliced wasserstein distance and its use for gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10648-10656, 2019.\n\n- Chris Drummond, Robert C Holte, et al. C4.5, class imbalance, and cost sensitivity: why undersampling beats over-sampling. In Workshop on learning from imbalanced datasets II, volume 11, pp. 1-8. Citeseer, 2003.\n\n- Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso Poggio. Learning with a wasserstein loss. arXiv preprint arXiv:1506.05439, 2015.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise. arXiv preprint arXiv:1802.05300, 2018.\n\n- Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, and Buru Chang. Disentangling label distribution for long-tailed visual recognition. arXiv preprint arXiv:2012.00321, 2020.\n\n- Chen Huang, Yining Li, Chen Change Loy, and Xiaou Tang. Learning deep representation for imbalanced classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5375-5384, 2016.\n\n- Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric Xing. Neural architecture search with bayesian optimisation and optimal transport. arXiv preprint arXiv:1802.07191, 2018.\n\n- Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. arXiv preprint arXiv:1910.09217, 2019.\n\n- Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 103-112, 2019.\n\n- Byungju Kim and Junmo Kim. Adjusting decision boundary for class imbalanced learning. IEEE Access, 8:81674-81685, 2020.\n\n- Jaehyung Kim, Youngbum Hur, Sejun Park, Eunho Yang, Sung Ju Hwang, and Jinwoo Shin. Distribution aligning refinery of pseudo-label for imbalanced semi-supervised learning. arXiv preprint arXiv:2007.08844, 2020a.\n\n- Jaehyung Kim, Jongheon Jeong, and Jinwoo Shin. M2m: Imbalanced classification via major-to-minor translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13896-13905, 2020b.\n\n- Wonji Lee, Chi-Hyuck Jun, and Jong-Seok Lee. Instance categorization by support vector machines to adjust weights in adaboost for imbalanced data classification. Information Sciences, 381:92-103, 2017.\n\n- Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. arXiv preprint arXiv:2103.12579, 2021.\n\n- Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr D\u00f3lar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980-2988, 2017.\n\n- Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2537-2546, 2019.\n\n- Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. arXiv preprint arXiv:2007.07314, 2020.\n\n- Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355-607, 2019.\n\n- Foster Provost. Machine learning from imbalanced data sets 101. In Proceedings of the AAAI'2000 workshop on imbalanced data sets, volume 68, pp. 1-3. AAAI Press, 2000.\n\n- Xingye Qiao and Yufeng Liu. Adaptive weighted learning for unbalanced multicategory classification. Biometrics, 65(1):159-168, 2009.\n\n- Jiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Balanced meta-softmax for long-tailed visual recognition. arXiv preprint arXiv:2007.10740, 2020.\n\n- Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving gans using optimal transport. arXiv preprint arXiv:1803.05573, 2018.\n\n- Filippo Santambrogio. Optimal transport for applied mathematicians. *Birkh\u00e4user*, NY, 55(58-63):94, 2015.\n\n- Richard Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. ii. Proceedings of the American Mathematical Society, 45(2):195-198, 1974.\n\n- Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. arXiv preprint arXiv:2001.07685, 2020.\n\n- Zhengyu Su, Yalin Wang, Rui Shi, Wei Zeng, Jian Sun, Feng Luo, and Xianfeng Gu. Optimal mass transport for shape matching and comparison. IEEE transactions on pattern analysis and machine intelligence, 37(11):2246-2259, 2015.\n\n- Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classification by keeping the good and removing the bad momentum causal effect. arXiv preprint arXiv:2009.12991, 2020.\n\n- Yuchun Tang, Yan-Qing Zhang, Nitesh V Chawla, and Sven Krasser. Svms modeling for highly imbalanced classification. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(1):281-288, 2008.\n\n- Grant Van Horn and Pietro Perona. The devil is in the tails: Fine-grained classification in the wild. arXiv preprint arXiv:1709.01450, 2017.\n\n- Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8769-8778, 2018.\n\n- Thomas Viehmann. Implementation of batched sinkhorn iterations for entropy-regularized wasserstein loss. arXiv preprint arXiv:1907.01729, 2019.\n\n- C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.\n\n- Renzhen Wang, Kaiqin Hu, Yanwen Zhu, Jun Shu, Qian Zhao, and Deyu Meng. Meta feature modulator for long-tailed recognition. arXiv preprint arXiv:2008.03428, 2020a.\n\n- Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella X Yu. Long-tailed recognition by routing diverse distribution-aware experts. arXiv preprint arXiv:2010.01809, 2020b.\n\n- Liuyu Xiang, Guiguang Ding, and Jungong Han. Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification. In European Conference on Computer Vision, pp. 247-263. Springer, 2020.\n\n- Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1492-1500, 2017.\n\n- Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph partitioning and matching. arXiv preprint arXiv:1905.07645, 2019a.\n\n- Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-wasserstein learning for graph matching and node embedding. In International conference on machine learning, pp. 6932-6941. PMLR, 2019b.\n\n- Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate example weighting. In Third IEEE international conference on data mining, pp. 435-442. IEEE, 2003.\n\n- Junjie Zhang, Lingqiao Liu, Peng Wang, and Chunhua Shen. To balance or not to balance: A simple-yet-effective approach for learning with long-tailed distributions. arXiv preprint arXiv:1912.04486, 2019.\n\n- Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9719\u20139728, 2020.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638522.765305", "msecs": "765.0", "relativeCreated": "119595.80254554749", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# VEnhancer: Generative Space-Time Enhancement for Video Generation\n\n# Abstract\nWe present \\emph{VEnhancer}, a generative space-time enhancement method that can improve the existing AI-generated videos spatially and temporally through one video diffusion model. Given a generated low-quality video, our approach can increase its spatial and temporal resolution simultaneously with arbitrary up-sampling space and time scales by adding more details in spatial domain and synthesize detailed motion in temporal domain. Furthermore, VEnhancer is able to remove generated spatial artifacts and temporal flickering of generated videos. To achieve this, basing on a pretrained generative video prior, we train a \\textbf{S}pace-\\textbf{T}ime Controller and inject it to the prior as a condition on low-frame-rate and low-resolution videos. To effectively train this ST-Controller, we design \\textit{space-time data augmentation} to create diversified video training pairs as well as \\textit{video-aware conditioning} for realizing different augmentation parameters in both spatial and temporal dimensions. Benefiting from the above designs, VEnhancer can be end-to-end trained to enable multi-function in one single model. Extensive experiments show that VEnhancer surpasses existing state-of-the-art video super-resolution and space-time super-resolution methods in enhancing AI-generated videos. Moreover, VEnhancer is able to greatly improve the performance of open-source state-of-the-art text-to-video methods on video generation benchmark, VBench.\n\n\n# 1 INTRODUCTION\n\n\nWith the advances of text-to-image generation (Rombach et al., 2022; Podell et al., 2023; Chen et al., 2023b; Gao et al., 2024) and large-scale video datasets with text description (Bain et al., 2021), there is fast development of text-to-video generative models (Guo et al., 2023; Chen et al., 2023c; Ho\n\n\net al., 2022; Blattmann et al., 2023b; Wang et al., 2023b;a; Chen et al., 2023a; 2024a; Gupta et al., 2023; Wu et al., 2023b). These developments enable users to generate compelling videos through textual descriptions of the desired content. One common solution (Ho et al., 2022; Blattmann et al., 2023b; Wang et al., 2023b; Blattmann et al., 2023a; Gupta et al., 2023) to obtain high-quality videos is to adopt cascaded pipelines, which stacks several video diffusion models, including text-to-video, temporal super-resolution and spatial super-resolution (S-SR) diffusion. These pipeline significantly reduce computation cost when generating high-resolution and high-frame-rate videos, but also pose several issues. First, using two different models for spatial and temporal enhancement separately is redundant, as they are strongly correlated. Also, the proposed diffusion-based spatial or temporal super-resolution (Blattmann et al., 2023b; Ho et al., 2022; Blattmann et al., 2023a; Wang et al., 2023b; Lin et al., 2024) have a limited flexibility, as they can only handle fixed interpolation ratio (i.e., predicting three frames between two consecutive frames) or fixed upscaling factors (i.e.,  $4 \\times$ ). Second, directly training diffusion models on synthesized video pairs may limit its generalization ability, as these models tailored on classic super-resolution task hallucinate high-frequency details, but cannot semantically improve the visual quality of input videos, such as eliminating distortions, artifacts, or recreating new contents (see LaVie-SR's results in Figure 1).\n\n\nAnother common approach is training another diffusion model to remove artifacts and to refine distorted content in the generated videos (Henschel et al., 2024; Zhang et al., 2023b). One example is I2VGen-XL (Zhang et al., 2023b), which follows the idea of image generation model SDXL (Podell et al., 2023)\u2013 it first upscales videos to higher resolutions using bilinear interpolation, and trains another diffusion refinement model, which will be used for video regeneration through a nois-ingenoising process (Meng et al., 2021). However, this method usually produces over-smoothed videos without realistic texture details (see Figure 1, I2VGen-XL), since the adopted bilinear upsampling cannot generate more spatial details. More importantly, the nois-ingenoising process (i.e., starting from  $t = 600$ ) will substantially change the original video content, which cannot always be acceptable in practical applications.\n\n\nIn short, current generative video enhancement methods face several challenges. First, sequentially applying temporal and spatial super-resolution is redundant, as they are independently trained, but using similar training datasets. Thus, such design is both sub-optimal and inefficient during the inference. Second, existing refinement methods struggle in balancing between video quality and fidelity to the original content. More importantly, they cannot perform effective super-resolution for increasing spatial and temporal details, which limits their practicality. Third, previous generative video enhancement methods lack the flexibility in dealing with different upscaling factors and refinement strengths for spatial or temporal super-resolution and video refinement.\n\n\nTo this end, we propose VEnhancer, a generative space-time enhancement method that supports both spatial and temporal super-resolution with flexible space and time up-sampling scales, as well as has the ability to remove visual artifacts and flickering with good maintenance of video content. It is built upon a pretrained and fixed generative video prior (Zhang et al., 2023b), which supplies the generative ability for video enhancement. To condition the video generation on low-frame-rate and low-resolution videos, we design Space-Time Controller (ST-Controller) for effective conditioning in both spatial and time dimensions. Furthermore, to handle different up-sampling scales and reduce artifacts or flickering with different degrees, we propose a space-time data augmentation algorithm to construct the training data. In particular, at the training stage, we sample different step sizes for skipping frames, downscaling factors, and noise levels to synthesize diversified condition videos. To ensure the proposed ST-Controller be aware of the associated data augmentation applied to each input video, we propose the video-aware conditioning. In particular, for key frame, the condition latent, the embeddings of the associated downscaling factor  $s$  and noise level  $\\sigma$  by noise augmentation are incorporated into ST-Controller through video-aware conditioning.\n\n\nWith these designs, VEnhancer is a single end-to-end trainable network that can handle both spatial and temporal super-resolution, as well as video refinement. Moreover, it also supports arbitrary space and time up-sampling scales, and also supports flexible control on refinement strength and generative strength as user may prefer. Extensive experiments have demonstrated VEnhancer's ability in enhancing generated videos (see Figure 1). In these experiments, it outperforms state-of-the-art real-world and generative video super-resolution methods for spatial super-resolution only. In the space-time super-resolution, VEnhancer also surpasses state-of-the-art methods as well as cascaded spatial and temporal diffusion super-resolution models. At last, on the public video\n\n\ngeneration benchmark, VBenchmark (Huang et al., 2023), VEnhancer can significantly improve the overall performance of existing text-to-video algorithms.\n\n\nOur contributions can be summarized as below:\n\n- 1. We propose VEnhancer, a generative space-time enhancement method that can achieve generative spatial and temporal super-resolution for different upsampling factors, as well as controllable video refinement in one video diffusion model for the first time.\n\n- 2. To achieve the unified generative space-time enhancement, we devise ST-Controller for effective multi-frame condition injection based on a pretrained and fixed generative video prior. Besides, space-time data augmentation and the associated video-aware conditioning are proposed for training ST-Controller in an end-to-end manner.\n\n- 3. VEnhancer surpasses existing state-of-the-art video super-resolution methods and space-time super-resolution methods in enhancing generated videos. Also, it could improve the performance of open-source text-to-video methods on public video generation benchmark.\n\n\n# 2 RELATED WORK\n\n\n# 2.1VIDEO GENERATION\n\n\nRecently, there have been substantial efforts in training large-scale T2V (Wang et al., 2024; Ho et al., 2022; Guo et al., 2023; Chen et al., 2023c; Gupta et al., 2023; Blattmann et al., 2023b; Wang et al., 2023b;a) models on large scale datasets. Some works (Blattmann et al., 2023b; Wang et al., 2023b;a) inflate a pre-trained text-to-image (T2I) model by inserting temporal layers and fine-tuning them or all parameters on video data, or adopts a joint image-video training strategy. In order to achieve high-quality video generation, (Ho et al., 2022; Wang et al., 2023b; Blattmann et al., 2023b) adopts multi-stage pipelines. In particular, cascaded video diffusion models are designed: One T2V base model that is followed by one or more frame interpolation and video super-resolution models. VideoLDM (Blattmann et al., 2023b), LaVie (Wang et al., 2023b), and Upscale-A-Video (Zhou et al., 2023) all develop the video super-resolution model based on  $4 \\times$  sd (StableDiffusion (Rombach et al., 2022)) upscaler, which has an additional downsampled image for conditioning the generation. One drawback of this base model is losing quite a lot generative ability compared with T2I base models. On the contrary, I2VGEN-XL follows SDXL (Podell et al., 2023) and uses noisig-denoising process (Meng et al., 2021) to refine the generated artifacts. However, this strategy could improve stability but cannot increase the space-time resolution. VEnhancer is based on a generative video prior, and could address temporal/spatial super-resolution and refinement in a unified model.\n\n\n# 2.2VIDEO ENHANCEMENT\n\n\nVideo Super-Resolution. Video Super-Resolution (VSR) is proposed to enhance video quality by upsampling low-resolution (LR) frames into high-resolution (HR) ones. Traditional VSR approaches(Cao et al., 2021; Chan et al., 2021; 2022a; Isobe et al., 2020a;b;c; Liang et al., 2024; 2022; Wang et al., 2019; Xue et al., 2019) often rely on fixed degradation models to synthesize training data pairs, which leads to a noticeable performance drop in real-world scenarios. To bridge this gap, recent advances(Chan et al., 2022b; Xie et al., 2023) in VSR have embraced more diversified degradation models to better simulate real-world low-resolution videos. To achieve photo-realistic reconstruction, Upscale-A-Video(Zhou et al., 2023) integrates diffusion prior to produce detailed textures, upgrading VSR performance into next level. Space-Time Super-Resolution. Space Time Video Super-Resolution (STVSR) aims to simultaneously increase the resolutions of video frames in both spatial and temporal dimensions. Deep-learning based approaches(Haris et al., 2020; Kim et al., 2020; Xiang et al., 2020; Chen et al., 2022) have achieved remarkable results on STVSR. STARNet(Haris et al., 2020) increases the spatial resolution and frame rate by leveraging the mutual information between space and time. FISR(Kim et al., 2020) propose a joint framework with a multiscale temporal loss to upscale the spatial-temporal resolution of videos. (Xiang et al., 2020) proposes a one-stage STVSR framework, which incorporates different sub-modules for LR frame features interpolation, temporal information aggregation and HR reconstruction. VideoINR(Chen et al., 2022) utilize the continuous video representation to achieve STVSR at arbitrary spatial resolution and frame rate. Although these methods obtain smooth and high-resolution output videos, but they fail in generating realistic texture details.\n\n\n![Figure 2: The overall framework. It consists of space-time data augmentation for constructing training data, and the associated video-aware conditioning for realizing diversified conditions across frames, as well as ST-Controller for multi-frame condition injection based on generative video prior.](images/cfc7e1100181230ee15b12543b6903001724745994470e9b265521b3f33347bf.jpg)\n\n\n# 3 PRELIMINARIES:VIDEO DIFFUSION MODELS\n\n\nOur method is built on a pretrained video diffusion model (Zhang et al., 2023b), which is developed based on Stable Diffusion 2.1. Given an video  $\\mathbf{x} \\in \\mathbb{R}^{F \\times H \\times W \\times 3}$ , the encoder  $\\mathcal{E}$  first encodes it into latent representation  $\\mathbf{z} = \\mathcal{E}(\\mathbf{x})$  frame-by-frame, where  $\\mathbf{z} \\in \\mathbb{R}^{F \\times H' \\times W' \\times C}$ . Then, the forward diffusion and reverse denoising are conducted in the latent space. In the forward process, the noise is gradually added to the latent vector  $\\mathbf{z}$  in total  $T$  steps. And for each time-step  $t$ , the diffusion process is formulated as follows:\n\n\n$$\n\n$$\n\\mathbf {z} _ {t} = \\alpha_ {t} \\mathbf {z} + \\sigma_ {t} \\boldsymbol {\\epsilon}, \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\epsilon \\in \\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , and  $\\alpha_{t}$ ,  $\\sigma_{t}$  specify the noise schedule in which the corresponding log signal-to-noise-ratio  $(\\log [\\alpha_t^2 /\\sigma_t^2 ])$  decreases monotonically with  $t$ . And at time-step  $T$ ,  $q(\\mathbf{z}_T) = \\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . As for backward pass, a diffusion model is used for iteratively denoising under the guidance of the text prompt  $c_{text}$ . By adopting v-prediction parameterization (Salimans & Ho, 2022), the U-Net denoiser  $f_{\\theta}$  learns to make predictions of  $\\mathbf{v}_t\\equiv \\alpha_t\\pmb {\\epsilon} - \\sigma_t\\mathbf{z}$ . The optimization objective is simply formulated as:\n\n\n$$\n\n$$\n\\mathcal {L} _ {L D M} = \\mathbb {E} _ {\\mathbf {z}, c _ {t e x t}, \\boldsymbol {\\epsilon} \\sim \\mathcal {N} (\\mathbf {0}, \\mathbf {I}), t} \\left[ \\| \\mathbf {v} - f _ {\\theta} (\\mathbf {z} _ {t}, t, c _ {t e x t}) \\| _ {2} ^ {2} \\right]. \\tag {2}\n$$\n\n$$\n\n\nAt the end, the generated videos are obtained through the VAE decoder:  $\\hat{\\mathbf{x}} = \\mathcal{D}(\\mathbf{z})$ .\n\n\n# 4 METHODOLOGY\n\n\nIn this section, we introduce the main components of our method. The overall framework is illustrated in Figure 2. First, we present our architecture design in Section 4.1. Then we elaborate on the proposed space-time data augmentation in Section 4.2. In Section 4.3, we give a detailed description on our designed video-aware conditioning.\n\n\n# 4.1 ARCHITECTURE DESIGN\n\n\nThe architecture is designed based on a pretrained video diffusion model. This video diffusion model is able to generate temporal-coherent content and high-quality texture details through iterative denoising. To upsample and refine a low-frame-rate and low-resolution videos in both spatial and temporal dimensions, the visual information should be incorporated into the video diffusion model carefully in order to obtain high-quality results with good fidelity to the input videos. High-quality generated videos stem from powerful generative models, while fidelity requires the algorithm to preserve the visual information of the input. Inspired by (Zhang et al., 2023a), we keep the pretrained video diffusion model untouched for preserving generative capability, and create a Space-Time Controller (ST-Controller) to obtain effective multi-frame condition injection for generative video enhancement. The architecture is illustrated in Figure 3.\n\n\nThe pretrained video diffusion model follows the design of stacking a sequence of interleaved spatial and temporal layers within the 3D-UNet (Blattmann et al., 2023b) architecture (gray blocks in Figure 3). Specifically, each spatial convolution layer (or attention layer) is followed by a temporal\n\n\nconvolution layer (or attention layer). The spatial layers are the same as those in Stable Diffusion 2.1, including ResBlocks (He et al., 2016), self-attention (Vaswani et al., 2017) layers, and cross-attention layers. The temporal convolution and attention layers are incorporated with their output layers initialized to zero and finetuned with video datasets. Specifically, the temporal convolution is one-dimensional convolution layer with a kernel size of 3, and the temporal attention is one-dimensional attention layer (Wang et al., 2023a). In this 3D-UNet, the video features that aligned by temporal layers in encoder will be skipped to the decoder, in which concatenation operation will be performed to combine skipped features with decoder features.\n\n\nTo build our proposed ST-Controller, we make a copy (both the architectures and weights) of the multi-frame encoder and middle block in 3D-UNet (orange blocks in Fig. 3) as the trainable condition network. This condition network takes low-frame-rate and low-resolution condition latents as well as full frames of noisy latents as inputs. Specifically, the condition latents and the associated augmentation parameters are incorporated into the condition network through our proposed video-aware conditioning. The output multi-scale temporal-coherent video features will be injected into the original 3D-UNet through newly added zero convolutions (yellow blocks in Fig. 3). The output features of the middle block in condition network will be added\n\n\n![Figure 3: The architecture of VEnhancer.](images/38e6f2747369dddd271745c1732923ba878e98f209fb9648d71000892b83e917.jpg)\n\n\nback to the features of the middle block in 3D-UNet. While for output features of encoder blocks in condition network, their features will be added to the skipped video features in 3D-UNet, which are also produced by encoder blocks. The copied condition network, video-aware conditioning, and the newly added zero convolutions are trained simultaneously.\n\n\n# 4.2 SPACE-TIME DATA AUGMENTATION\n\n\nIn this section, we discuss about how to achieve unified space-time super-resolution with arbitrary up-sampling space and time scales, as well as refinement with varying degrees. To this end, we propose a novel data augmentation strategy for both space and time axes. Details are discussed below.\n\n\nTime axis. Given a sequence of high-frame-rate and high-resolution video frames  $I^{1:f} = [I^1, I^2, \\dots, I^f]$  with frame length  $f$ , we use a sliding window across time axis to select frames. The frame sliding window size  $m$  is randomly sampled from a predefined set, ranging from 1 to 8. This corresponds to time scales from  $1 \\times 8 \\times$ . Note that  $1 \\times$  time scale requires no frame interpolation, thus the multi-task problem downgrades to video super-resolution. After the frame skipping, we obtain a sequence of key frames  $I^{1:m:f} = [I^1, I^{1+m}, I^{1+2 \\times m}, \\dots, I^f]$ .\n\n\nSpace axis. Then, we perform spatial downsampling for these obtained key frames. Specifically, the downscaling factor  $s$  is randomly sampled from [1, 8], which represents  $1 \\times \\sim 8 \\times$  space super-resolution. When  $s = 1$ , there is no need to perform spatial super-resolution. All frames in one sequence are downsampled with the same downscaling factor  $s$ . Thus, we arrive at low-frame-rate and low-resolution video frames:  $I_{\\downarrow s}^{1:m:f}$ . In practice, we should upsample them back to the original spatial sizes by bilinear interpolation before being passed to the networks, so we obtain  $I_{\\downarrow s,\\uparrow s}^{1:m:f}$ . Note that each space or time scale corresponds to different difficulty level, and thus the sampling is not uniform. Particularly, we set sampling probabilities of scales  $4 \\times$  and  $8 \\times$  based on a ratio of  $1:2$ , which is determined by their associated scale values.\n\n\nThen, we use the encoder part of a pretrained variational autoencoder (VAE)  $\\mathcal{E}$  to project the input sequence to the latent space frame-wisely:\n\n\n$$\n\n$$\nz _ {s} ^ {1: m: f} = \\left[ \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {1}\\right), \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {1 + m}\\right), \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {1 + 2 \\times m}\\right), \\dots , \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {f}\\right) \\right]. \\tag {3}\n$$\n\n$$\n\n\nNoise augmentation in latent space. At this stage, we conduct noise augmentation to noise the latent condition information in varying degrees in order to achieve controllable refinement. This noise\n\n\naugmentation process is the same as the diffusion process equation 1 used in the video diffusion model. Specifically, the condition latent sequence is corrupted by:\n\n\n$$\n\n$$\nz _ {s, t ^ {\\prime}} ^ {1: m: f} = \\alpha_ {t ^ {\\prime}} z _ {s} ^ {1: m: f} + \\sigma_ {t ^ {\\prime}} \\epsilon^ {1: m: f}, \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\alpha_{t'}$ ,  $\\sigma_{t'}$  determine the signal-to-noise-ratio at time-step  $t'$ , and  $t' \\in \\{1, \\dots, T'\\}$ . Note that the pretrained video diffusion model adopts 1,000 steps ( $T = 1000$  in equation 1). While the noise augmentation only needs to corrupt the low-level information,  $T'$  is set to 300 empirically. For more intuitive denotation, we use  $\\sigma$  instead of  $t'$ . Finally, we arrive at  $z_{s,\\sigma}^{1:m:f} = \\mathcal{E}(I_{\\downarrow s, \\uparrow s})_{\\sigma}^{1:m:f}$ .\n\n\nThe whole process of space-time data augmentation is summarized as follows:\n\n\n$$\n\n$$\nI ^ {1: f} \\rightarrow I ^ {1: m: f} \\rightarrow I _ {\\downarrow s} ^ {1: m: f} \\rightarrow I _ {\\downarrow s, \\uparrow s} ^ {1: m: f} \\rightarrow \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s}\\right) _ {\\sigma} ^ {1: m: f} \\rightarrow \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s}\\right) _ {\\sigma} ^ {1: m: f}. \\tag {5}\n$$\n\n$$\n\n\n# 4.3 VIDEO-AWARE CONDITIONING\n\n\nBesides data augmentation, the corresponding conditioning mechanism should also be designed in order to influence the model training and avoid averaging performance for different space or time scales and noise augmentation. In practice, the condition latent sequence  $z_{s,\\sigma}^{1:m:f}$ , the corresponding downscaling factor  $s$ , and augmented noises  $\\sigma$  are all considered as for conditioning. Please refer to Figure. 4 for more intuitive demonstration.\n\n\nGiven the synthesized condition latent sequence  $z_{s,\\sigma}^{1:m:f}$ , we use one convolution with zero-initialization  $-\\mathrm{Conv}_{\\mathrm{zero}}$  for connecting it to the condition network. Specifically, we have:\n\n\n$$\n\n$$\nf _ {o u t} ^ {1: f} = \\operatorname {C o n v} \\left(z _ {t} ^ {1: f}\\right), \\tag {6}\n$$\n\n$$\n\n\n$$\n\n$$\nf _ {o u t} ^ {1: m: f} = \\operatorname {C o n v} (z _ {t} ^ {1: m: f}) + \\operatorname {C o n v} _ {\\mathrm {z e r o}} (z _ {s, \\sigma} ^ {1: m: f}), (7)\n$$\n\n$$\n\n\nwhere Conv is the first convolution in the condition network,  $z_{t}^{1:f}$  and  $z_{t}^{1:m:f}$  denote the full frames and key frames of noisy latents at timestep  $t$ , respectively. Note that Conv and Convzero share the same hyperparameter configuration (i.e., kernel size, padding, et.al.), As it is shown, only key-frame features in condition network will be added with the condition features, while others remain unchanged. This strategy enables progressive condition injection as the weights of Convzero grows from zero starting point.\n\n\n![](images/5aaf7915b8f99b325432428416afb94b71918cde7f2ed9e28519c81bebf15ef4.jpg)\n\n\n![Figure 4: Video-aware conditioning. For frame that has condition image as input (key frame), we add the condition latent to the noisy latent after one convolution layer. Besides, the embeddings of noise level  $\\sigma$  and downscaling factor  $s$  are added to the existing  $t$  embedding, which will be broadcast to all spatial layers.](images/d9ed791e28f11031d6c5876e43244328fb8379d29f22c0ab3c4517544011bc84.jpg)\n\n\nFor conditioning regarding downscaling factor  $s$  and noise augmentation  $\\sigma$ , we incorporate them to the existing time embedding in the condition network. Specifically, for timestep  $t$ , sinusoidal encoding (Ho et al., 2020; Rombach et al., 2022; Vaswani et al., 2017) is used to provide the model with a positional encoding for time. Then, one MLP (two linear layers with a SiLU (Elfwing et al., 2018) activation layer in between) is applied. Specifically, we have:\n\n\n$$\n\n$$\nt _ {e m b} = \\mathrm {M L P} _ {t} (\\text {S i n u s o i d a l} (t)), \\quad t _ {e m b} ^ {1: f} = \\text {R e p e a t} (t _ {e m b}, f), \\tag {8}\n$$\n\n$$\n\n\nwhere  $t_{emb}^{1:f}$  is obtained by Repeat  $t_{emb}$  by  $f$  times in the frame axis. This time embedding sequence will be broadcast to all ResBlocks in the condition network for timestep injection.\n\n\nAlso, we elucidate the conditioning for noise augmentation. As mentioned in equation 4, noise augmentation shares the same way as diffusion process, but with much smaller maximum timestep (i.e.,  $T' = 300$ ). Thus, we reuse the encoding and mapping for timestep  $t$  in diffusion process. After this, we add a linear layer with zero initialization (denoted as Linear_zero). To conclude, we have:\n\n\n$$\n\n$$\n\\sigma_ {e m b} = \\operatorname {L i n e a r} _ {\\text {z e r o}, \\sigma} (\\mathrm {M L P} _ {t} (\\text {S i n u s o i d a l} (\\sigma)). \\tag {9}\n$$\n\n$$\n\n\nTo achieve video-aware conditioning, we add  $\\sigma_{emb}$  only to the key frames. We repeat  $\\sigma_{emb}$  by  $k$  times to obtain  $\\sigma_{emb}^{1:k}$ , where  $k$  is the number of key frames. The video-aware controlling is presented as follows:\n\n\n$$\n\n$$\nt _ {e m b} ^ {1: m: f} = t _ {e m b} ^ {1: m: f} + \\sigma_ {e m b} ^ {1: k}, \\tag {10}\n$$\n\n$$\n\n\nwhere the addition operation is performed frame-wisely.\n\n\nRegarding downscaling factor  $s$ , the corresponding encoding, mapping and controlling are similar as above. In particular, we newly introduce one  $\\mathrm{MLP}_{\\mathrm{zero}}$ , in which the output layer is zero-initialized. The video-aware conditioning is performed as:\n\n\n$$\n\n$$\ns _ {e m b} = \\mathrm {M L P} _ {\\text {z e r o}, s} (\\text {S i n u s o i d a l} (s)), \\quad s _ {e m b} ^ {1: k} = \\text {R e p e a t} \\left(s _ {e m b}, k\\right), \\tag {11}\n$$\n\n$$\n\n\n$$\n\n$$\nt _ {e m b} ^ {1: m: f} = t _ {e m b} ^ {1: m: f} + s _ {e m b} ^ {1: k}. \\tag {12}\n$$\n\n$$\n\n\nWith our proposed space-time data augmentation and video-aware conditioning, VEnhancer can be well-trained in an end-to-end manner, and yields great performance in handling diversified conditions for generative enhancement. Here we provide a demonstration in Figure 5. For  $4 \\times$  video superresolution, we modify the input downscaling factor  $s$  to produce different results. It is shown that more texture details are generated as  $s$  grows (from smooth to sharp). This indicates that  $s$  can determine how many details are generated through our proposed video-aware conditioning.\n\n\n![Figure 5: The effectiveness of video-aware conditioning. For video super-resolution  $(4\\times)$ , we modify the input downscaling factor from  $s = 1$  to  $s = 8$ , and more texture details are generated. Zoom in for best view.](images/bbdfbce7c4a22214f43db95b7df6126becf0fbf10147fa3c723ddc4f785166c6.jpg)\n\n\n# 5 EXPERIMENTS\n\n\nDatasets. We collect around 350k high-quality and high-resolution video clips from Panda-70M (Chen et al., 2024b) dataset and the Internet to constitute our training set. We train VEnhancer on resolution  $1280 \\times 720$  with center cropping, and the target FPS is fixed to 24 by frame skipping. Regarding test dataset, we collect generated videos from state-of-the-art text-to-video methods. Practically, we select videos with large motions and diverse contents. This test dataset is denoted as AIGC2023, which is used to evaluate VEnhancer and baselines for video super-resolution and space-time super-resolution tasks. For evaluation on VBench, all generated videos based on the provided prompt suite are considered, resulting in around 5k videos.\n\n\nImplementation Details. The batch size is set to 256. AdamW (Loshchilov & Hutter, 2017) is used as the optimizer, and the learning rate is set to  $10^{-5}$ . During training, we dropout the text prompt with a probability of  $10\\%$ . The training process lasts about four days with 16 NVIDIA A100 GPUs. During inference, we use DPM-Solver (Lu et al., 2022) and perform 15 sampling steps with classifier-free guidance ( $cfg$ ) (Ho & Salimans, 2022).\n\n\nMetrics. Regarding evaluation for video super-resolution and space-time super-resolution on AIGC2023 test dataset, we use both image quality assessment (IQA) and video quality assessment (VQA) metrics. Specifically, MUSIQ (Ke et al., 2021) and DOVER (Wu et al., 2023a) are adopted. Moreover, we refer to video generation benchmark, VBench (Huang et al., 2023), for more comprehensive evaluation. Specifically, we choose Dynamic Degree (i.e., whether it contains large motions), Motion smoothness (i.e., how smooth the video is), and Aesthetic Quality for evaluation. Regarding evaluation for video generation, we consider all 16 evaluation dimensions from VBench.\n\n\n# 5.1 COMPARISON WITH VIDEO SUPER-RESOLUTION METHODS\n\n\nFor video super-resolution, VEnhancer is compared with the state-of-the-art real-world video super-resolution method, RealBasicVSR (Chan et al., 2022b), and the state-of-the-art generative video super-resolution method, LaVie-SR (Wang et al., 2023b) (super-resolution). For more comprehensive comparison, we also include I2VGen-XL' (Zhang et al., 2023b) refinement model as our baseline.\n\n\nTable 1: Quantitative comparison for video super-resolution  $(4\\times)$  on AIGC2023 test dataset. Red and blue indicate the best and second best performance. The top 3 results are marked as gray .\n\n\n<table><tr><td></td><td>DOVER\u2191</td><td>MUSIQ\u2191</td><td>Aesthetic Quality</td><td>Dynamic Degree</td><td>Motion Smoothness</td></tr><tr><td>LaVie-SR Wang et al. (2023b)</td><td>0.8427</td><td>55.8428</td><td>0.6692</td><td>0.525</td><td>0.9710</td></tr><tr><td>I2VGen-XL(refiner) Zhang et al. (2023b)</td><td>0.5603</td><td>25.5988</td><td>0.6439</td><td>0.475</td><td>0.9835</td></tr><tr><td>RealBasicVSR Chan et al. (2021)</td><td>0.8252</td><td>50.5978</td><td>0.6622</td><td>0.550</td><td>0.9729</td></tr><tr><td>Ours</td><td>0.8586</td><td>59.4474</td><td>0.6671</td><td>0.550</td><td>0.9781</td></tr></table>\n\n\n![](images/68e212d6ede20b5fcc60370d46fc8352480f2a346b5fc36ebc312146e7237456.jpg)\n\n\n![](images/d0e3a93b88cf91779d1c20c936b13a1bca7170438abf218fc111cf49ce70a739.jpg)\n\n\n![](images/f69e82847ccd0fd9384f3ed01fc2427d09d42343e96b0ee431d4d0e3dc240e36.jpg)\n\n\n![](images/0107f12fa51c2d5f1539b887d6c69bc67b13e2897908e64bf0c5f99af730c6c6.jpg)\n\n\n![](images/994c105e78bbd064d79ff09a4736d96a9c06bb4c5b7369a9450492e314431d05.jpg)\n\n\n![](images/89d64654194c475ed1b98b5888ddfdbfa4a8209d538bd35a337783f256244af1.jpg)\n\n\n![](images/319a0e197cd5eb6bc76ec742fc077eadd20e66f25a188381a8600828a8c303fc.jpg)\n\n\n![](images/1f88cbf19123aaf61ceee5c3d20b7781adaed67a3973781c5e437f43f38417cb.jpg)\n\n\n![](images/3061d7be88193fe28c8e5d2257c99672679dd3ec03f19022b56e0bce9def7eac.jpg)\n\n\n![](images/bfea63d696a3dd7e0b04f5410095b318f780ec1bf4ee945dc441d8a55460ae1a.jpg)\n\n\n![](images/e6400c8db0e7370145f3c04a1f0c0e4d31ce838c5e31348d0eae1fe9130b29a8.jpg)\n\n\n![](images/a03cdaeee5bbb84cdb98982b806177301f17ffa60e928a5c05671570d168e7d4.jpg)\n\n\n![Figure 6: Visual comparison for video super-resolution  $(4\\times)$  on AIGC2023 test dataset. Input resolution:  $512\\times 312$ ; output resolution:  $2048\\times 1280$ . Prompt: Iron Man flying in the sky.](images/b74b072b25ab349085849feda257baacb69a05aa33f19cbeb63949320838aa51.jpg)\n\n\n![](images/025a0d34e2585e5b86fcfeeffb0519efa261ac8856f6cef60655a04b3d48cdab.jpg)\n\n\n![](images/b2394c6de2ab73a2d02f7cd5f7c782f093127709ccbe687af58b6d549fc21c76.jpg)\n\n\nAs shown in Table 1, VEnhancer outperforms both generative video super-resolution method (LaVie-SR), real-world video super-resolution method (RealBasicVSR), and generative video refinement method (I2VGen-XL, refiner) in most metrics, suggesting its outstanding enhancement ability for videos. Note that LaVie-SR surpasses RealBasicVSR in image/video quality (MUSIQ, DOVER, and Aesthetic Quality), as diffusion-based methods are better at generating sharp details. But LaVie-SR achieves the worst Motion Smoothness, indicating its insufficient capability in balancing video smoothness and video quality. I2VGen-XL's refiner could obtain the highest score in Motion Smoothness, but sacrifices the magnitude of motion significantly (worst Dynamic Degree). Moreover, it achieves very unsatisfactory results in metrics evaluating image and video quality. Because it uses bilinear interpolation for  $\\times 4$  upsampling, which produces very blurry results. Nevertheless, VEnhancer achieves overall best results with good balance among image/video quality, motion smoothness, and motion magnitude.\n\n\nThe visual comparison is presented in Figure. 6. The prompt is \"Iron man flying in the sky\". The input video is already consistent with the prompt, but lacks details on the iron man suit. RealBasicVSR could remove some noises or artifacts of the generated videos as it incorporates complex degradation for model training. However, it fails in generating realistic details but produces over-smoothed results, since its generative ability is limited. On the other hand, the results of LaVie-SR contains more artifacts than input. Without successfully removing artifacts, the generative super-resolution model will enlarge the existing defects. The refiner of I2VGen-XL could achieve successful refinement but produces over-smoothed results. In contrast, VEnhancer could first remove unpleasing artifacts and refine the distorted content (e.g., head region), and then generate faithful details (e.g., helmet and armor) that are consistent with the text prompt.\n\n\n# 5.2 COMPARISON WITH SPACE-TIME SUPER-RESOLUTION METHODS\n\n\nFor space-time super-resolution task, we compare two state-of-the-art space-time super-resolution methods: VideoINR (Chen et al., 2022) and Zooming-Slow-Mo (Xiang et al., 2020) (Zoom for short). We also consider LaVie's cascaded T-SR and S-SR DM-based pipeline: LaVie-FI (frame interpolation) + LaVie-SR (super-resolution) for more thorough comparison.\n\n\nTable 2: Quantitative comparison for space-time super-resolution  $(4\\times)$  on AIGC2023 test dataset. Red and blue indicate the best and second best performance. The top 3 results are marked as gray .\n\n\n<table><tr><td></td><td>DOVER\u2191</td><td>MUSIQ\u2191</td><td>Aesthetic Quality</td><td>Dynamic Degree</td><td>Motion Smoothness</td></tr><tr><td>LaVie-FI + LaVie-SR Wang et al. (2023b)</td><td>0.8159</td><td>53.2128</td><td>0.6566</td><td>0.60</td><td>0.9857</td></tr><tr><td>VideoINR Chen et al. (2022)</td><td>0.7608</td><td>34.1060</td><td>0.6624</td><td>0.60</td><td>0.9933</td></tr><tr><td>Zooming Slow-Mo Xiang et al. (2020)</td><td>0.7328</td><td>33.8470</td><td>0.6624</td><td>0.55</td><td>0.9908</td></tr><tr><td>Ours</td><td>0.8609</td><td>51.2940</td><td>0.6710</td><td>0.60</td><td>0.9937</td></tr></table>\n\n\nAs shown in Table 2, we observe that our method surpasses all baselines in DOVER and Aesthetic Quality, showing its superior capability in generating sharp and realistic video content. Besides, it obtains highest scores in Motion Smoothness and Dynamic Degree, indicating VEnhancer's excellent ability in synthesizing stable temporal details. The cascaded T-SR and S-SR approach (LaVie-FI + LaVie-SR) obtains good scores in DOVER and MUSIQ, demonstrating DM-based methods' advantage in generation. However, its performance in temporal aspect is unsatisfactory due to its inferior capability in temporal refinement. We notice that state-of-the-art space-time super-resolution methods (VideoINR and Zooming Slow-Mo) behave well in Motion Smoothness. As both of them are optimized with reconstruction loss, the produced results are very smooth across frames. At a cost, they perform poorly in metrics regarding quality, such as DOVER and MUSIQ.\n\n\nThe visual comparison is illustrated in Figure. 7. The first and third columns present the low-resolution key frames. Note that the input frames are not consistent especially in the region of guitar strings. The cascaded T-SR and S-SR approach, LaVie-FI + LaVie-SR, can produce very sharp results for all frames (key and predicted ones). However, it generates messy contents which are not semantically aligned with prompt. Moreover, the generated details are changing across time, indicating severe flickering. For reconstruction-based methods (VideoINR and Zoom), the produced results are similar: lacking details and failing in improving the consistency of the original input frames. On the contrary, VEnhancer is not only able to achieve unified space-time super-resolution, but can also improve the temporal consistency of the generated videos by refinement (i.e., guitar strings and raccoon hands).\n\n\n# 5.3 EVALUATION ON IMPROVING VIDEO GENERATION\n\n\nHere we evaluate VEnhancer's ability in improving state-of-the-art T2V methods. The baselines include open-source T2V methods-VideoCrafter-2 (Chen et al., 2024a) (VC-2 for short), Lavie (Wang et al., 2023b), Open-Sora, CogVideoX (Yang et al., 2024), and professional video generation products-Pika and Gen-2. In particular, we enhance T2V results of CogVideoX-5B and VC-2.\n\n\nThe quantitative results are organized in Table 3. Before enhancement, CogVideoX-5B and VC-2 achieve the best and second best in Semantic compared with other baselines, demonstrating their superiority in generating video contents that are highly consistent to the VBenchmark's prompt suite. Regarding Quality, they lag behind Pika. But with VEnhancer, VC-2 and CogVideoX-5B are able to achieve the highest and second highest scores in Quality. Besides, their scores regarding Semantic improves a lot, especially VC-2' (3.31% increase). This indicates that VEnhancer can improve the\n\n\nTable 3: VBenchmark Evaluation Results. This table compares the performance of open-source T2V methods and professional video generation products regarding two aspects (Quality and Semantic). A higher score indicates better performance. Red and blue indicate the best and second best performance. The top 3 results are marked as gray .\n\n\n<table><tr><td></td><td>LaVie\nWang et al. (2023b)</td><td>Open-Sora</td><td>Pika</td><td>Gen-2</td><td>CogVideoX-5B\nYang et al. (2024)</td><td>VC-2\nChen et al. (2024a)</td><td>CogVideoX-5B\n+Venhancer</td><td>VC-2\n+VEnhancer</td></tr><tr><td>Quality</td><td>78.78%</td><td>80.71%</td><td>82.92%</td><td>82.47%</td><td>82.75%</td><td>82.20%</td><td>82.99%</td><td>83.28%</td></tr><tr><td>Semantic</td><td>70.31%</td><td>73.30%</td><td>71.77%</td><td>73.03%</td><td>77.04%</td><td>73.42%</td><td>77.52%</td><td>76.73%</td></tr><tr><td>Overall</td><td>77.08%</td><td>79.23%</td><td>80.69%</td><td>80.58%</td><td>81.61%</td><td>80.44%</td><td>81.90%</td><td>81.97%</td></tr></table>\n\n\nsemantic content and video quality at the same time, showing a powerful generative enhancement ability. More importantly, the advantage of adopting a two-stage pipeline is observed: the first T2V model focuses on generating semantic content and motions with good fidelity to the prompts, while the following enhancement model can improve the semantic in low-level and image quality, as well as temporal consistency. For visual results, please see the video demonstration in supp.\n\n\n# 5.4 ABLATION STUDIES\n\n\nThe Effectiveness of Noise Augmentation. During training, the noise level regarding noise augmentation is randomly sampled within a predefined range. While during inference, one can change the noise level to achieve refinement with different strengths. In general, higher noise corresponds to stronger refinement and regeneration. We present the visual comparison among different noise levels in Figure 8. The first frame of one AI-generated video is presented in the left. It is of low-resolution and lacks details. Also, the original video has very obvious flickering. If we set  $\\sigma = 0$ , VEnhancer will generate unpleasant noises in the background. As there is domain mismatch between the training data and testing data, the enhancement fails in handling unseen and challenging scenarios. Fortunately, we can mitigate this by adding noise in the condition latents for corrupting the noisy and unknown low-level details. As we increase the noise level, the artifacts are gradually vanishing. When  $\\sigma = 250$ , the result is noise-clean, and has abundant semantic details.\n\n\nArbitrary Up-sampling Scales for Spatial Super-Resolution. Here we show that VEnhancer is able to up-sample videos with arbitrary scales. From Figure 9, we observe that VEnhancer could produce satisfactory results on different scales  $(2.5\\times,3\\times,3.5\\times,4\\times,$  and  $4.5\\times)$ , suggesting its flexibility and generalization in adapting to different tasks. In particular, given one frame of the generated video  $(312\\times 512)$ , VEnhancer could improve the generated details when the up-sampling scale grows up. When  $s = 2.5\\sim 3.5$ , the panda's hand is less realistic. But it becomes better when  $s = 4$  or  $s = 4.5$ . It is also noticed that the panda' fur is becoming more realistic as  $s$  grows.\n\n\nArbitrary Up-sampling Scales for Temporal Super-Resolution. In this part, we show VEnhancer is able to achieve arbitrary up-sampling in time axis. Given two low-resolution key frames, we aim to up-sample them to high-resolution ones, and also interpolate several frames (ranging from 2 to 4) between them. As shown in Figure 10, the results are consistent across frames, showing not flicking or distortions. Besides, the spatial quality has also been significantly improved. As shown in the last row,  $5 \\times$  frame interpolation yields smooth frames with generated contents: the shadow in the right leg is changing, showing a very natural transition. This indicates that diffusion-based frame interpolation has great capability in both motion and content generation.\n\n\n# 6 CONCLUSION AND LIMITATION\n\n\nIn this work, we propose a generative space-time enhancement method - VEnhancer for video generation. It can achieve spatial super-resolution, temporal super-resolution and video refinement in one video diffusion model. We base on a pretrained generative video prior and build a Space-Time Controller (ST-Controller) for effective condition injection. Space-time data augmentation and video-aware conditioning are proposed to train ST-Controller in an end-to-end manner. Extensive experiments have demonstrated our superiority over state-of-the-art video super-resolution and space-time super-resolution methods in enhancing AI-generated videos. However, our work has several limitations. First, as it is based on diffusion models, the inference takes more time than one-step methods. Second, it may face challenges in handling AI-generated long videos, since the long-term (over 10s) consistency has not been addressed in this work.\n\n\n# REFERENCES\n\n- Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021.\n\n- Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023a.\n\n- Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22563-22575, 2023b.\n\n- Jiezhang Cao, Yawei Li, Kai Zhang, and Luc Van Gool. Video super-resolution transformer. arXiv preprint arXiv:2106.06847, 2021.\n\n- Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Basicvsr: The search for essential components in video super-resolution and beyond. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4947-4956, 2021.\n\n- Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Basicvsr++: Improving video super-resolution with enhanced propagation and alignment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5972-5981, 2022a.\n\n- Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Investigating tradeoffs in real-world video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5962-5971, 2022b.\n\n- Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023a.\n\n- Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocaster2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024a.\n\n- Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-\u03b1: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023b.\n\n- Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Genton: Delving deep into diffusion transformers for image and video generation. arXiv preprint arXiv:2312.04557, 2023c.\n\n- Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479, 2024b.\n\n- Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vedit Goel, Zhangyang Wang, Humphrey Shi, and Xiaolong Wang. Videoinr: Learning video implicit neural representation for continuous space-time super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2047-2057, 2022.\n\n- Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:3-11, 2018.\n\n- Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024.\n\n- Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023.\n\n- Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos\u00e9 Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023.\n\n- Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita. Space-time-aware multi-resolution video enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2859-2868, 2020.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024.\n\n- Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n\n- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020.\n\n- Jonathan Ho, William Chan, Chitwan Sahara, Jay Whang, Ruiqi Gao, Alexey Gritsanko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.\n\n- Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. arXiv preprint arXiv:2311.17982, 2023.\n\n- Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, and Qi Tian. Video superresolution with recurrent structure-detail network. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XII 16, pp. 645-660. Springer, 2020a.\n\n- Takashi Isobe, Songjiang Li, Xu Jia, Shanxin Yuan, Gregory Slabaugh, Chunjing Xu, Ya-Li Li, Shengjin Wang, and Qi Tian. Video super-resolution with temporal group attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8008-8017, 2020b.\n\n- Takashi Isobe, Fang Zhu, Xu Jia, and Shengjin Wang. Revisiting temporal modeling for video super-resolution. arXiv preprint arXiv:2008.05765, 2020c.\n\n- Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5148-5157, 2021.\n\n- Soo Ye Kim, Jihyong Oh, and Munchurl Kim. Fisr: Deep joint frame interpolation and superresolution with a multi-scale temporal loss. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 11278-11286, 2020.\n\n- Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu Timofte, and Luc V Gool. Recurrent video restoration transformer with guided deformable attention. Advances in Neural Information Processing Systems, 35:378-393, 2022.\n\n- Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Rakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van Gool. Vrt: A video restoration transformer. IEEE Transactions on Image Processing, 2024.\n\n- Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Wanli Ouyang, Yu Qiao, and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior, 2024.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\n- Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022.\n\n- Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.\n\n- Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684-10695, 2022.\n\n- Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\n- Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023a.\n\n- Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. Edvr: Video restoration with enhanced deformable convolutional networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 0-0, 2019.\n\n- Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023b.\n\n- Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In International Conference on Computer Vision (ICCV), 2023a.\n\n- Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7623-7633, 2023b.\n\n- Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P Allebach, and Chenliang Xu. Zooming slow-mo: Fast and accurate one-stage space-time video super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3370-3379, 2020.\n\n- Liangbin Xie, Xintao Wang, Shuwei Shi, Jinjin Gu, Chao Dong, and Ying Shan. Mitigating artifacts in real-world video super-resolution models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 2956-2964, 2023.\n\n- Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. Video enhancement with task-oriented flow. International Journal of Computer Vision, 127:1106-1125, 2019.\n\n- Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024.\n\n- Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3836-3847, 2023a.\n\n- Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023b.\n\n- Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-video: Temporal-consistent diffusion model for real-world video super-resolution. arXiv preprint arXiv:2312.06640, 2023.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638524.0202518", "msecs": "20.0", "relativeCreated": "120850.74925422668", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# ChemAgent: Self-updating Memories in Large Language Models Improves Chemical Reasoning\n\n# Abstract\nChemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code ef- effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science.\n\n\n# 1 INTRODUCTION\n\n\nChemical reasoning presents unique challenges in the realm of artificial intelligence, demanding sophisticated reasoning and precise calculations beyond typical reasoning tasks (McQuarrie, 2008; Atkins et al., 2014; Talanquer, 2022; Guo et al., 2023b; Cao et al., 2024). For example, the GPT solution with CoT prompting in Figure 1 contains numerous errors in both the calculation process and the chemical constants used. Even in short reasoning chains, a single error can cascade, reducing answer quality and escalating the probability of additional errors (Liao et al., 2024; Sun et al., 2024b).\n\n\nRecent advancements in large language models (LLMs) have demonstrated capabilities in simpler scientific tasks or chemical scenarios that do not require complex reasoning (Boiko et al., 2023; Atkins et al., 2023; Wang et al., 2024a; Hu & Shu, 2023; Xiao et al., 2024; Darvish et al., 2024; Skreta et al., 2024). However, their application to complex chemical reasoning reveals significant limitations Pei et al. (2024); Li et al. (2024a). LLMs often (1) struggle to effectively utilize domain-specific formulas, (2) exhibit incorrect reasoning steps, and (3) produce errors when combining textual reasoning with Python code for calculations (Zhong et al., 2024b); here, syntax errors may arise, causing the code to fail to compile. As shown in Figure 1 (StructChem output), errors often arise from determining a constant's incorrect form or unit.\n\n\nSome Previous approaches to address these challenges in chemical reasoning tasks have focused on decomposing the reasoning process (Zhong et al., 2023), e.g., formatting the reasoning steps through techniques such as self-reflection (Shinn et al., 2023) and StructChem prompting (Ouyang et al., 2024). However, these methods often rely heavily on human-curated knowledge (Chan et al., 2023), or fixed workflows (Fu et al., 2022; Wang et al., 2023; Zhou et al., 2023). Unlike human learners who utilize a library system to retain and apply previous experiences, these methods lack\n\n\n# PROBLEM (HYDROGEN ATOM ENERGY LEVELS)\n\n\nAn electron in a hydrogen atom transitions from an excited state with the principal quantum number  $n = 4$  to the ground state  $(n = 1)$ . Calculate: a) The energy of the emitted photon in joules (J), b) The wavelength of the emitted photon in nanometers (nm), c) The frequency of the emitted photon in terahertz (THz).\n\n\n![Figure 1: Comparison of problem-solving approaches for a hydrogen atom energy transition problem. The figure illustrates three different methods: (a) shows a standard Chain-of-Thought approach with calculation errors (in steps 3 and 4) in Wang et al. (2024a). (b) demonstrates the StructChem (Ouyang et al., 2024) method with formula generation and step-by-step reasoning but fails due to an incorrect constant and incorrect unit conversion (in steps 1 and 4). (c) presents the ChemAgent solution, featuring task decomposition, memory retrieval from the library, and reasoning, leading to the accurate final answer.](images/c4d1fe7fcb6aff12c6920faf030081d79c1369535709acd6e471b2d6d9509a4f.jpg)\n\n\n![](images/6b9104f5524bf62f8ce5259565a4c01bcf872049ab59a45af48f8131d7943c2f.jpg)\n\n\n![](images/09dbdf7506263c8919abff0077f7f7751005888ea55e84facb58c558ed6a6797.jpg)\n\n\nthe ability to remember and learn from past analogous problems. For instance, humans can abstract and store theorems or solution strategies from previous tasks and utilize this memory for future problem-solving. $^{1}$\n\n\nBuilding on these insights, ChemAgent introduces a dynamic \"LIBRARY\" system that facilitates iterative problem-solving by continuously updating and refining its content based on task decomposition. The LIBRARY in ChemAgent serves as a comprehensive repository where decomposed chemical tasks are stored. These tasks are broken down into sub-tasks, and their solutions are compiled in the library for future reference. As new tasks are encountered, the library is updated with new sub-tasks and corresponding solutions, ensuring its content remains relevant and progressively improves over time. In parallel, inspired by human cognitive mechanisms (Osman, 2004; Kaufman, 2011), our system incorporates three main memory components: Planning Memory for high-level strategies, Execution Memory for specific task solutions, and Knowledge Memory for fundamental chemistry principles. These memories are stored externally, allowing the model to retrieve them efficiently during the problem-solving process. Unlike previous works relevant to LLM's external memory system (Huang et al., 2024a; Zhong et al., 2024a; Zhang et al., 2023; Li et al., 2024b; Guo et al., 2023a; Zhang et al., 2024b), we carefully integrate all these components in a complete agentic framework and allow them to update dynamically.\n\n\nThese memories are stored in a structured tree-based format that allows for efficient retrieval during the problem-solving process. When ChemAgent encounters a new problem, it first decomposes the task into manageable sub-tasks, then leverages the library to retrieve relevant sub-tasks and solutions stored in its memory components. The retrieved information is validated and refined using analogous sub-tasks previously encountered, optimizing both task decomposition and solution generation. The library is dynamically updated by adding new sub-tasks and solutions as they are encountered and validated. This iterative process ensures that the memory is continuously enriched with new strategies and solutions, mimicking human problem-solving improvements through practice. By maintaining an evolving domain knowledge base, ChemAgent enables LLMs to autonomously navigate the problem-solving process, thereby enhancing their performance on similar tasks over time.\n\n\nOur experiments are conducted on four chemical reasoning datasets from SciBench (Wang et al., 2024a) with GPT-3.5, GPT-4 (OpenAI et al., 2024), and open-source models like Llama3\n\n\n![Figure 2: The diagram of our overall framework. It contains (a) library-enhanced reasoning and (b) library construction. (a) illustrates how ChemAgent utilizes the library to address a new task for the test set. And (b) demonstrates the construction of the library over the dev set, including Plan Memory  $\\mathcal{M}_p$  and Execution Memory  $\\mathcal{M}_e$ .](images/11429a3d16f9569a5c20c31d8aeba45b07216b206661ae7319c988910307b8a0.jpg)\n\n\n![](images/bd0c74abbecaceecb224a980485e107b447712119560561bf69ed709d17b7a21.jpg)\n\n\n(Llama Team, 2024). Experimental results indicate that ChemAgent significantly enhances the accuracy, though the degree of improvement achieved by our model varies accordingly due to the varying sizes of these datasets. Compared to the base model (take GPT-4 for example), the application of memory self-improving leads to an average increase in accuracy by  $37\\%$ , with a maximum improvement of  $46\\%$ . In comparison with the current state-of-the-art model, StructChem (Ouyang et al., 2024), our approach achieves an average improvement of  $10\\%$  and a maximum improvement of  $15\\%$  across different datasets. Furthermore, the improvement is more pronounced for stronger base models: GPT-4 exhibits greater enhancements compared to GPT-3.5 when augmented with our framework. Additionally, we analyze the role of our library system and examine the benefits of different memory updates across various stages. This analysis provides insights into how our iterative library updates contributed to performance improvements in sub-task resolution and optimal task decomposition. By continuously enriching the library with structured memory components, ChemAgent ensures that the problem-solving process is progressively refined and optimized, leading to substantial improvements in accuracy.\n\n\n# 2 METHOD\n\n\n# 2.1 PRELIMINARIES\n\n\nAnalogous to how students organize and reference their problem-solving approaches for exams, our motivation for developing a library system in ChemAgent is to enhance LLMs' ability to tackle complex problems by providing structured access to a repository of previous sub-tasks and solutions.\n\n\nThe overall reasoning framework is shown in Figure 2 (a). Formally, in a simplified form, given a complex and open-ended chemical problem  $\\mathcal{P}_S$  as input, our method aims to generate a solution  $\\mathcal{O}_S$ . The problem  $\\mathcal{P}_S$ , which comprises problem descriptions  $\\mathcal{T}_S$  and initial conditions  $\\mathcal{C}_S$ , is firstly decomposed into a series of sub-tasks  $\\mathcal{P}_{S_i}$ . The solution  $\\mathcal{O}_S$  is then synthesized from the subsolutions  $\\mathcal{O}_{S_i}$  of the sub-tasks. Each  $\\mathcal{O}_{S_i}$  includes intermediate steps, such as formulae, reasoning steps, code, and calculations. Then, a final answer  $\\mathcal{A}_S$  is derived from the overall solution  $\\mathcal{O}_S$ . We evaluate the performance via the accuracy of  $\\mathcal{A}_S$  against a ground truth  $\\mathcal{A}_{S_g}$ .\n\n\n# 2.2 COMPOSITION OF THE LIBRARY\n\n\nWe divide library into three main memory components: planning memory, execution memory, and knowledge memory. Figure 3 provides detailed examples. The exact context of memory and corresponding prompts are in Appendix G.\n\n\n- Planning Memory  $(\\mathcal{M}_p)$ : This component stores high-level strategies and methodologies for approaching complex problems.\n\n\n![Figure 3: Given a task  $\\mathcal{P}$ , the relevant memory examples are provided in the library. Specifically, while Execution Memory  $(\\mathcal{M}_e)$  and Plan Memory  $(\\mathcal{M}_p)$  are derived from prior experiences, Knowledge memory  $(\\mathcal{M}_k)$  is generated by LLM based on the problem prompt. The conditions  $\\mathcal{C}$  are not explicitly presented here but are embedded within  $\\mathcal{P}$  and the [GOAL] of  $\\mathcal{M}_e$ . Figure 2 and Figure 4 illustrates this workflow. In the following sections, we explain how each part is constructed and utilized. All prompts used in our framework can be found in Appendix G.](images/d97efd4a2ba11c0158298bbff145ff67856931ddfc2732a22ef5d9872f2ed09b.jpg)\n\n- - Execution Memory  $(\\mathcal{M}_e)$ : This contains highly structured descriptions of specific problem contexts and their corresponding solutions, serving as detailed execution plans.\n\n- - Knowledge Memory  $(\\mathcal{M}_k)$ : This houses fundamental chemistry principles and formulas, acting as a ready reference. This component is generated temporarily during the solution of a specific task and is not intended for permanent retention.\n\n\nThus, ChemAgent enables LLMs to tackle problems proactively, form new memories from these attempts, utilize existing memories to solve complex problems, and continuously refine their solutions. Built upon the library of three types of memories, ChemAgent operates in two main stages:\n\n- 1. Initial Library Construction: Complex problems in the development set are decomposed into structured atomic sub-tasks, which serve as building blocks of the library, consisting of static long-term memory  $(\\mathcal{M}_p$  and  $\\mathcal{M}_e)$ .\n\n- 2. Inference Phase and Dynamic Memory Updating:\n\n- (a) Memory-Enhanced Reasoning: The memory is dynamically enriched and improved during runtime.\n\n- (b) Evaluation & refinement: Each sub-task's solution is evaluated on their veracity and relevance to the main task, which leads to a correction of the solution or a refinement of the overall plan.\n\n\n# 2.3 DECOMPOSITION AS ATOMIC BLOCKS\n\n\nHuman problem-solvers naturally break down complex chemistry problems into smaller, manageable sub-tasks (Zhou et al., 2023). This decomposition enhances the understanding of individual components and their interactions, facilitating the resolution of the original problem in a structured manner (Johnson et al., 2017). These sub-tasks not only improve the reasoning process but also function as atomic building blocks within the execution memory, each paired with a corresponding sub-solution. Key characteristics of this decomposition include:\n\n\nHierarchical Decomposition: Using the strategy stored in Planning Memory, we break down complex chemistry problems into hierarchical sub-tasks. If a sub-task is too difficult to complete, it is further decomposed until it can be executed in a single step by the LLM. We store each decomposed sub-task and its intermediate sub-solutions, enabling direct retrieval of solutions for similar future problems. This hierarchical approach ensures each sub-task acts as an independent atomic block, aiding future problem-solving.\n\n\nStructured Sub-tasks: To maximize the efficiency of decomposition, sub-tasks are structured into four distinct parts: (i) Task Query defining the specific question, (ii) Sub-task Objectives outlining\n\n\nclear goals, (iii) Step-by-step Solution detailing the method to address the sub-task, and (iv) Guidance for Solutions suggesting potential approaches.\n\n\nAtomic Blocks as Memory: Decomposed sub-tasks serve as building blocks for memory. Identifying and recalling similar problems can be challenging, but sub-tasks often share commonalities that make it easier to leverage past experiences. These atomic blocks also function as examples in few-shot prompting, standardizing response formats and bolstering the overall problem-solving process.\n\n\nBuilding on the concept of sub-tasks as fundamental units, we outline the processes involved in our framework, which includes both a dynamic library system and the structured memory inside our library. This section details how we initially construct the library and memory (\u00a72.4), utilize and update them for new tasks (\u00a72.5), and evaluate and refine the generated solutions (\u00a72.6). Specifically, our approach leverages the development set to build a library, which is designed to dynamically self-improve during runtime.\n\n\n# 2.4 LIBRARY CONSTRUCTION\n\n\nThe Library is constructed using the development set. As outlined in \u00a72.3, we leverage sub-tasks decomposed from each problem as the execution memory units. Each execution memory unit  $\\mathcal{U}_i$  is defined as follows, where  $\\mathcal{C}$  represents the conditions of a given problem  $\\mathcal{P}$ , and  $\\mathcal{T}_i$  and  $\\mathcal{O}_i$  denote the sub-task and its corresponding sub-solution, respectively:\n\n\n$$\n\n$$\n\\mathcal {U} _ {i} = \\left(\\mathcal {C}, \\mathcal {T} _ {i}, \\mathcal {O} _ {i}\\right) \\quad \\text {f o r} \\quad i = 1, 2, \\dots , k\n$$\n\n$$\n\n\nGiven a problem  $\\mathcal{P}$  and its corresponding solution  $S$  in the development set, our method begins by identifying and extracting the conditions from  $\\mathcal{P}$ . We then verify these conditions for accuracy to ensure that the subsequent steps operate with precise and correctly parsed data. Based on the identified conditions, we instruct the LLMs to generate detailed\n\n\nsub-tasks. For each identified sub-task  $\\mathcal{T}_i$ , the corresponding sub-solutions  $\\mathcal{O}_i$  are parsed from  $\\mathcal{S}$  and assigned accordingly. Inspired by curriculum learning (Bengio et al., 2009), we then rank the memory units  $\\mathcal{U}$  based on their difficulty. In addition to ranking, we discard any memory units that do not meet a predefined confidence threshold, as evaluated by the LLMs. This ensures that the memory utilized for future problem-solving is both relevant and reliable, enhancing the LLM's ability to tackle increasingly complex chemistry problems. The detailed memory construction process is further described in Algorithm 1.\n\n\n```\nInput: Development set  $\\mathcal{D}$  LLM  $\\mathcal{F}$  prompts  $\\{p_{\\mathrm{split}},p_{\\mathrm{ref}},p_{\\mathrm{rank}}\\}$    \nOutput: Static memory  $\\mathcal{M}$  consisting of units  $\\mathcal{U} =$  {condition, question, solution}   \nfor  $(\\mathcal{P},\\mathcal{S})$  in  $\\mathcal{D}$  do Conditions  $\\mathcal{C}\\gets \\mathcal{F}(p_{\\mathrm{split}}||\\mathcal{P})$  // Verify conditions and refine if necessary  $\\mathcal{C}\\gets \\mathcal{F}(p_{\\mathrm{ref}}||\\mathcal{P}||\\mathcal{C})$  Sub-tasks  $\\mathcal{T}\\gets \\mathcal{F}(p_{\\mathrm{task}}||\\mathcal{P}||\\mathcal{C})$  Sub-solutions  $\\mathcal{O}\\gets \\mathcal{F}(p_{\\mathrm{sol}}||\\mathcal{C}||\\mathcal{P}||\\mathcal{S})$  Assert len  $(\\mathcal{C}) = \\text{len} (\\mathcal{T}) = \\text{len} (\\mathcal{O})$  for i in len(C) do Add  $(\\mathcal{C}_i,\\mathcal{T}_i,\\mathcal{O}_i)$  into U  $\\mathcal{U}\\gets \\mathcal{F}(p_{\\mathrm{rank}}||\\mathcal{U})$    \nreturn U\n```\n\n\n# 2.5 LIBRARY-ENHANCED REASONING\n\n\nDuring testing, we first decompose a given problem into several sub-tasks. For each sub-task, we retrieve related memory units  $\\mathcal{U}_r$  to aid in solving it. Specifically, we compute the similarity between the given sub-task and units stored in the memory. Memory units with similarity above a predefined threshold  $\\theta$  are used to assist the model in determining the answer for the sub-task.\n\n\nFormally, let  $(\\mathcal{C}_j,\\mathcal{T}_j)$  represent a sub-task decomposed from a new problem. We retrieve memory units  $\\mathcal{U}_r$  that satisfy\n\n\n$$\n\n$$\n\\begin{array}{l} \\text {S i m i l a r i t y} (\\mathcal {T} _ {j}, \\mathcal {T} _ {\\mathcal {U} _ {i}}) \\geq \\theta \\quad \\text {f o r} \\quad \\mathcal {U} _ {i} \\in \\mathcal {M} _ {e}. \\end{array}\n$$\n\n$$\n\n\nSpecifically, the similarity between tasks is calculated using Llama3's embeddings:\n\n\n$$\n\n$$\n\\operatorname {S i m i l a r i t y} \\left(T _ {a}, T _ {b}\\right) = \\frac {\\operatorname {E m b e d} \\left(T _ {a}\\right) \\cdot \\operatorname {E m b e d} \\left(T _ {b}\\right)}{\\left| \\left| \\operatorname {E m b e d} \\left(T _ {a}\\right) \\right| \\right| \\times \\left| \\left| \\operatorname {E m b e d} \\left(T _ {b}\\right) \\right| \\right|}.\n$$\n\n$$\n\n\nIn scenarios where the execution memory does not contain similar sub-tasks, we incorporate a self-improving mechanism. Initially, we enrich the memory with the required information by leveraging the internal parametric knowledge of LLMs. The LLM is instructed to identify the topic of the given sub-task (e.g., quantum chemistry or thermodynamics) and generate self-created chemistry\n\n\nproblems related to that topic, adhering to specific guidelines. This forms a kind of \"synthetic\" execution memory. We encourage the LLMs to generate diverse problems and solutions beyond the provided examples, thereby enhancing the robustness and adaptability of the memory system.\n\n\nMoreover, the memory is continuously updated with newly solved sub-tasks and their solutions during runtime. For each newly solved sub-task  $\\mathcal{T}_j$ , sub-answers from previous sub-problems are incorporated into the conditions as  $\\mathcal{C}_j$ , and the execution memory is updated based on solution  $\\mathcal{O}_j$ :\n\n\n$$\n\n$$\n\\mathcal {M} _ {e} = \\mathcal {M} _ {e} \\cup \\left\\{\\left(\\mathcal {C} _ {j}, \\mathcal {T} _ {j}, \\mathcal {O} _ {j}\\right) \\right\\}.\n$$\n\n$$\n\n\nThe plan memory will be updated by a summary of the overall strategy knowledge used,  $\\mathcal{K}_j$  (e.g. formulas, conception, order of resolution):\n\n\n$$\n\n$$\n\\mathcal {M} _ {p} = \\mathcal {M} _ {p} \\cup \\left\\{\\left(\\mathcal {T} _ {j}, \\mathcal {K} _ {j}\\right) \\right\\}.\n$$\n\n$$\n\n\nThis dynamic updating ensures that the memory evolves and improves over time, progressively enhancing the problem-solving capabilities. More details on self-evolution are in \u00a73.3.\n\n\n# 2.6 EVALUATE & REFINE MODULE\n\n\nTo enhance the flexibility and reliability of ChemAgent, we propose an evaluation & refinement module, to correct the planning trajectory and verify the response to each sub-task.\n\n\nAs illustrated in Figure 4, after addressing a specific sub-task  $\\mathcal{P}_i$ , ChemAgent examines the sub-solution  $\\mathcal{O}_i$ . The system evaluates whether the sub-solution conflicts with fundamental knowledge in  $\\mathcal{M}_k$  or contains common errors, such as incorrect units. If discrepancies are identified, a new sub-solution  $\\mathcal{O}'_i$  is generated by refining the original  $\\mathcal{O}_i$  based on the relevant knowledge in  $\\mathcal{M}_k$  and the identified mistakes.\n\n\n![Figure 4: Overall framework of the evaluation & refinement module. ChemAgent continuously modifies the solution or the comprehensive strategy until it either reaches the maximum number of trials or meets the evaluator's criteria.](images/5096752b4b5d514a04291518aa93f434d3446f2ba58cdca1723fceac15a8af9c.jpg)\n\n\nFurthermore, if a sub-task fails due to insufficient conditions or if the evaluation determines that the sub-task's question does not align with the main task (e.g., calculating energy using wavelength instead of calculating wavelength using given energy), the sub-tasks from  $\\mathcal{P}_i$  to  $\\mathcal{P}_n$  will be restructured as  $\\mathcal{P}_i'$  to  $\\mathcal{P}_m'$ , taking into account the main task and all preceding sub-tasks.\n\n\nThe evaluation & refinement module can use a different LLM than the one used for the base reasoning process. For instance, if GPT-3.5 is the base model, GPT-4 can handle evaluation & refinement. The evaluation component judges solutions without modifying them, while the refinement component adjusts solutions based on these evaluations. This separation clarifies error identification, helping humans understand where and why mistakes occur.\n\n\n# 3 EXPERIMENT\n\n\n# 3.1 SETUP\n\n\nWe use four chemistry datasets from SciBench (Wang et al., 2024a), and the detailed distribution of the specific fields covered by each problem in the four datasets is shown in Figure 10. Each dataset is divided into a development set  $(\\mathcal{D}_d)$  and a test set  $(\\mathcal{D}_t)$ , with exact sizes provided in Table 6. According to previous research (Ouyang et al., 2024), SciBench chemical sets are representative of chemical reasoning tasks, addressing a broad range of queries typically encountered in this field.\n\n\nDuring the reasoning stage, we configure the planning memory  $(\\mathcal{M}_p)$  to provide a maximum of two related memory instances (2-shot) for each query, and the execution memory  $(\\mathcal{M}_e)$  to provide up to four related instances (4-shot). However, during the construction of library, only the knowledge memory  $(\\mathcal{M}_k)$  is used, as the standard solutions are already available in the development set  $(\\mathcal{D}_d)$ .\n\n\nTable 1: Performance on the test sets of four datasets: QUAN, CHEMMC, ATKINS, and MATTER. Results are compared with baselines under two different setups: a zero-shot setting with no demonstrations and a few-shot setting with three demonstrations. Accuracy scores are computed using the approximation detailed in Section 4.3. The best results for each setup are highlighted in **bold**, and the second-best results are **underlined**. In some settings, the two modules described in Sections 2.4 (Memory) and 2.6 (Evaluation & refinement) are disabled, helping illustrate each module's impact on the overall performance.\n\n\n<table><tr><td>Models</td><td>CHEMMC</td><td>MATTER</td><td>ATKINS</td><td>QUAN</td><td>Avg.</td></tr><tr><td colspan=\"6\">Baselines, all based on GPT-4 (gpt-4-1106-preview)</td></tr><tr><td>Few-shot + Direct reasoning</td><td>28.21</td><td>14.29</td><td>20.69</td><td>14.71</td><td>19.48</td></tr><tr><td>Few-shot + Python</td><td>38.46</td><td>34.69</td><td>57.01</td><td>44.12</td><td>43.57</td></tr><tr><td>StructChem</td><td>58.97</td><td>30.67</td><td>59.81</td><td>41.18</td><td>47.66</td></tr><tr><td colspan=\"6\">ChemAgent, all based on GPT-4 (gpt-4-1106-preview)</td></tr><tr><td>ChemAgent</td><td>74.36</td><td>48.98</td><td>61.18</td><td>44.12</td><td>57.16</td></tr><tr><td>w/o Evaluation &amp; Refinement</td><td>61.54</td><td>44.89</td><td>57.94</td><td>44.12</td><td>52.12</td></tr><tr><td>w/o Memory, Evaluation &amp; Refinement</td><td>58.97</td><td>38.78</td><td>57.94</td><td>41.18</td><td>49.22</td></tr><tr><td colspan=\"6\">all based on Llama 3.1-7b</td></tr><tr><td>Direct reasoning</td><td>28.20</td><td>10.20</td><td>22.43</td><td>8.82</td><td>17.41</td></tr><tr><td>ChemAgent</td><td>56.41</td><td>12.24</td><td>19.63</td><td>14.71</td><td>25.75</td></tr><tr><td colspan=\"6\">all based on Llama 3.1-70b</td></tr><tr><td>Direct reasoning</td><td>33.33</td><td>30.61</td><td>30.84</td><td>23.53</td><td>29.48</td></tr><tr><td>ChemAgent</td><td>64.10</td><td>32.65</td><td>43.93</td><td>29.41</td><td>42.52</td></tr><tr><td colspan=\"6\">all based on Qwen 2.5-72b</td></tr><tr><td>Direct reasoning</td><td>48.72</td><td>32.65</td><td>57.01</td><td>35.50</td><td>43.47</td></tr><tr><td>ChemAgent</td><td>69.23</td><td>44.90</td><td>56.07</td><td>44.12</td><td>53.58</td></tr></table>\n\n\nWe evaluate the accuracy by comparing their outputs with the correct answers, using a relative tolerance of 0.01.\n\n\nWe consider three baselines in alignment with the evaluation paradigm in SciBench (detailed instructions are provided in Appendix G): (1) Few-shot + Direct reasoning involves directly feeding the problem into the LLM without any additional instructions, the source of the data is StructChem (Ouyang et al., 2024). (2) Few-shot + Python combines few-shot examples with a tool-augmented strategy. Here, we provide six examples for every query, each consisting of a problem, its corresponding solution, and Python code. These examples are taken from  $\\mathcal{D}_d$ . The results are from the original benchmark, SciBench (Wang et al., 2024a). (3) StructChem (Ouyang et al., 2024) employs structured instruction, confidence-based review, and refinement to guide LLMs through precise step-by-step reasoning.\n\n\n# 3.2 RESULTS\n\n\nWe report the performance of all methods regarding accuracy score for each sub-dataset and an average score across the entire dataset. The results are summarized in Tables 1, 4 and 5. Additional results and analysis of experiments conducted on other LLMs can be found in Appendix B.\n\n\nFirstly, ChemAgent consistently outperforms the other baselines across various datasets and settings. Specifically, in terms of the average score, ChemAgent improves by  $9.50\\%$  (47.66 vs. 57.16) over StructChem, which is a 2.93 times increase and by  $37\\%$  (19.48 vs. 57.16) over direct reasoning, which is a 2.93 times increase. Notably, the performance gain varies across different datasets. In the CHEMMC dataset, our method exhibits the largest improvement, with a  $46\\%$  increase (28.21 vs. 74.36) compared to the direct reasoning setup. Secondly, the results also highlight the crucial role of memory in our library. The version of our framework equipped with memory consistently outperforms the version without memory across all cases. Specifically, there is an absolute improvement of  $2.90\\%$  in terms of the average score. This underscores the importance of memory in retaining and leveraging past information to improve the accuracy and reliability of solving chemistry problems. Additionally, the Evaluation & Refinement module plays a significant role when using stronger LLMs. For instance, incorporating Evaluation & Refinement with GPT-4 increases ChemAgent's performance by  $5.04\\%$  compared to ChemAgent with memory alone. In summary, ChemAgent demonstrates superior performance across various LLM backbones, highlighting the critical importance of the library system, memory design, and evaluation modules in\n\n\nenhancing problem-solving capabilities. Appendix C details the specific aspects and reasons for our method's improved performance.\n\n\n# 3.3 SELF-EVOLUTION DURING RUNTIME\n\n\nMoreover, we aim to show that library systems with evolving memory perform better when exposed to an increasing number of problems. Much like humans improve their skills through practice, these systems benefit from continuous exposure to new tasks.\n\n\nWe allow ChemAgent to dynamically update and enrich its library during the test stage to analyze this self-evolution process. Specifically, in iteration  $\\mathcal{I}_i$ , ChemAgent uses all accumulated long-term memory from iterations  $\\mathcal{I}_1$  to  $\\mathcal{I}_{i-1}$  as its library. When solving a new problem  $\\mathcal{P}$ , all related responses and knowledge from that process are added to the library if the solution is correct. This means that in subsequent iterations, the system can leverage the newly acquired information (updated  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$ ) to improve its performance. We employ 2-shot  $\\mathcal{M}_p$  and 4-shot  $\\mathcal{M}_e$  during reasoning but simplify the setup by removing the evaluation and refinement modules. To ensure accuracy and prevent target leakage, the memory derived from problem  $\\mathcal{P}_i$  in iteration  $\\mathcal{I}_j$  is excluded when solving  $\\mathcal{P}_i$  again in  $\\mathcal{I}_k$  for any  $j < i$ .\n\n\n![Figure 5: Self-evolving analysis. We test ChemAgent twice for each iteration, and the difference between the two results serves as the error margin. All the experiments here are done on MATTER dataset.](images/95adae2d9c377ebed9474a236074302b8c0f6913519066c1901f5ddd53844195.jpg)\n\n\nThis iterative process demonstrates that as the memory pool grows with each new example, ChemAgent's problem-solving performance improves. Figure 5 shows that as the number of iterations increases, the agent's performance gradually improves and converges to a score higher than the baseline (44.89%). This improvement indicates that ChemAgent can enhance its performance through a simple correct-or-not evaluation on past solutions instead of human-written high-quality trajectories.\n\n\n# 3.4 COST ANALYSIS\n\n\nOn average, each problem requires 0.012 million tokens without the Evaluation & Refinement module, equivalent to around  $0.09 per example. When this module is applied, the average token consumption per problem increases to approximately 0.023 million, costing about$ 0.1725 per example. Note that the initial library construction is not included in these calculations as it is a one-time setup. Based on the results in Figure 6, the computational time required by our method is reasonable. While the cost and resource consumption are slightly higher than StructChem, the improvements justify the additional expense.\n\n\n# 3.5 ERROR ANALYSIS\n\n\n![Figure 6: Cost Analysis. The size of each bubble corresponds to the average number of inferences for each method, while the  $y$ -axis indicates the average accuracy across the four datasets.](images/351f3762b68fd4c9a055594bc3e46260258d9163560448f7c46f548f90a4b0b8.jpg)\n\n\nWe conduct an analysis of the trajectories for the failed examples and find three types of errors, shown in Figure 7.\n\n\n(1) Lack of Understanding of the Question. We observe that plans often fail when the problem text contains critical hidden information (e.g., in Figure 7, the terms 'reversibly' and 'adiabatically' are key) or includes excessive, redundant details (e.g., the exact conditions such as \"250K\" are irrelevant to the solution). This challenge is understandable, as even human solvers can be misled by such information and err in their approach. This issue is prevalent and independent of whether the task is tackled by our model or directly queried to the LLM, suggesting that these errors may be\n\n\n![Figure 7: Error analysis. This example highlights a typical incorrect solution, which can be attributed to three main types of errors. Specifically, this problem pertains to an adiabatic process.](images/57d0674cc5aa18b36216a513c20f2715a26997102df2926f250970a7197a167e.jpg)\n\n\ninherent limitations of the LLM's capacity. Hence, improving the ability of the foundation model to handle such nuances could significantly enhance performance.\n\n- (2) Inaccurate Reasoning. As indicated before (Ouyang et al., 2024), the planning capabilities of LLMs remain insufficient for handling complex problems. Incorrect planning for the reasoning chains exacerbates the problem-solving process, as subsequent decisions and actions are based on initial problem decompositions. This issue persists until the Evaluation & Refinement module detects an error, which may not happen promptly enough to correct the trajectory.\n\n- (3) Incorrect Memory Selection. While ChemAgent with memory demonstrates superior performance compared to setups without memory, it sometimes invokes misleading information, even when the similarity between the invoked memory and the problem is high. This indicates a need for more sophisticated memory retrieval and utilization strategies.\n\n\nIn Figure 7, the invoked memory and sub-task 1.1 share considerable similarities\u2014both concern entropy change during a compression process. However, the subtle difference is that the current problem involves an adiabatic process, whereas the examples in memory do not. This seemingly minor distinction can lead to significant changes in the problem-solving strategy. Distinguishing between misleading and beneficial memories remains a challenging issue, as invoked memories and problem texts may be semantically similar yet differ in critical aspects.\n\n\n# 4 ABLATION STUDY\n\n\n# 4.1 MEMORY COMPONENT ANALYSIS\n\n\nTo understand why our method performs particularly well and which memory component contributes the most, we conduct an ablation study by independently removing each memory component. We test these different settings on all four sub-datasets using GPT-4 as the base LLM. The results are shown in Table 2. As mentioned in \u00a72.3, the execution memory also serves as a few-shot prompting strategy. Therefore, when we remove  $\\mathcal{M}_e$ , we add two fixed human-written few-shot examples (provided with our code) into each query of each sub-task. These examples are selected from the development set  $(\\mathcal{D}_d)$  corresponding to each dataset to ensure they do not provide misleading information.\n\n\nFirstly, removing any memory component results in an overall performance drop. And notably, the relevant knowledge in LLM itself  $(\\mathcal{M}_k)$  significantly impacts the overall performance of ChemA-gent. Delving into why  $\\mathcal{M}_k$  contributes the most, we find that this might be due to the insufficient  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$  in the Atkins dataset. As shown in Table 6, ATKINS has the lowest ratio of the development set to the test set, while it also has the largest test set. This imbalance may result in a small\n\n\nTable 2: The best performing score is highlighted in bold and second-best is underlined. The number of problems in each textbook weighs the average score. We also provide the results of ChemAgent without any of these memories as a baseline. Evaluation & refinement module is removed.\n\n\n<table><tr><td>Memory component</td><td>CHEMMC</td><td>MATTER</td><td>ATKINS</td><td>QUAN</td><td>Avg.</td></tr><tr><td>None</td><td>58.97</td><td>38.78</td><td>57.94</td><td>41.18</td><td>51.53</td></tr><tr><td>+MpMeMk</td><td>61.54</td><td>44.89</td><td>57.94</td><td>44.12</td><td>53.71</td></tr><tr><td>+MpMe</td><td>66.66</td><td>44.89</td><td>52.34</td><td>41.18</td><td>51.53</td></tr><tr><td>+MeMk</td><td>58.97</td><td>46.94</td><td>61.68</td><td>29.41</td><td>53.27</td></tr><tr><td>+MpMkfew-shot</td><td>74.36</td><td>42.86</td><td>57.94</td><td>32.35</td><td>53.71</td></tr></table>\n\n\nand sometimes irrelevant memory pool for  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$  to search from. We hypothesize that this issue can be mitigated when the model is exposed to more chemical questions over time. However, due to limited computational resources, we cannot scale up to verify this hypothesis in this research.\n\n\nIt is also worth noting that although the  $M_p, M_k, \\text{fewshot}$  setting achieves the same accuracy as the complete memory setting, most of its improvement is evident in the CHEMMC dataset, showing surprisingly higher improvements than others. Since Table 6 and Figure 10 demonstrate that the CHEMMC dataset has a higher ratio of  $\\mathcal{D}_d$  to  $\\mathcal{D}_t$  and a relatively narrow distribution in subfields, and since human experts carefully wrote the few-shot examples, these high-quality, expert-written examples may serve as a superior memory pool in such circumstances. In the next section, we show that better memory leads to better results, which might explain this phenomenon.\n\n\n# 4.2 MEMORY QUALITY INFLUENCE\n\n\nWe investigate the impact of memory quality through a series of experiments. Specifically, we compare the performance of memory generated by GPT-4 against that generated by GPT-3.5 on the MATTER dataset. Additionally, we create a \"hybrid memory\" by mixing memories generated by both GPT-3.5 and GPT-4 to observe its performance. These experiments were conducted without including the relevant knowledge within the LLM itself  $(\\mathcal{M}_k)$ . As mentioned in \u00a72.1, unlike  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$ ,  $\\mathcal{M}_k$\n\n\nTable 3: Results of memory quality analysis, with the best scores highlighting in bold. Here, memory refers to  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$ . The relevant knowledge in LLM itself  $(\\mathcal{M}_k)$  is removed from the framework. Test on MATTER dataset.\n\n\n<table><tr><td></td><td>GPT-3.5</td><td>GPT-4</td></tr><tr><td>GPT-3.5 Memory</td><td>11.22</td><td>36.73</td></tr><tr><td>GPT-4 Memory</td><td>13.26</td><td>44.89</td></tr><tr><td>Hybrid Memory</td><td>10.20</td><td>28.57</td></tr></table>\n\n\nis not preserved in the memory pool shared with other LLMs. Including this type of memory in the ablation study would result in the pollution of provided memory by the  $\\mathcal{M}_k$  generated during use.\n\n\nAs shown in Table 3, our data analysis indicates that memory quality significantly impacts the accuracy of responses from ChemAgent. Whether GPT-3.5 or GPT-4 is used, the memory produced by GPT-4 consistently outperforms that generated by GPT-3.5, showing an  $8\\%$  variation. This underscores the substantial impact of memory quality on the accuracy of generated responses. This observation also explains why the improvements seen with GPT-3.5 are less pronounced compared to those with GPT-4; the lower quality of memory produced by GPT-3.5 is the limiting factor.\n\n\nInterestingly, hybrid memory performs the worst among the three settings. This can be attributed to the simultaneous invocation of different memories for the same question\u2014one generated by GPT-3.5 and the other by GPT-4\u2014which may confuse the LLM, increasing the likelihood of producing irrelevant or incorrect answers. Additionally, invoking an excessive number of memory tracks during resolution may also contribute to this issue. A more detailed analysis regarding the number of memory tracks invoked is provided in Appendix D.\n\n\n# 5 CONCLUSION\n\n\nOur research presents a novel approach to enhancing large language models for solving complex chemical problems through self-exploration and memory formation. This method enables models to construct and utilize a library, significantly improving response accuracy. Experiments using datasets and models like GPT-3.5, GPT-4, and Llama3 demonstrate substantial performance gains, with the ChemAgent architecture achieving up to a  $36\\%$  improvement. The structured library, built through memory decomposition into planning, execution, and knowledge, enhances problem-solving capabilities, which holds promise for generalization to other domains.\n\n\n# REFERENCES\n\n- Peter Atkins, Julio De Paula, and Ronald Friedman. Physical chemistry: quanta, matter, and change. Oxford University Press, USA, 2014.\n\n- Peter Atkins, Julio De Paula, and James Keeler. Atkins' physical chemistry. Oxford university press, 2023.\n\n- Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41-48, 2009.\n\n- Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,16, pp. 17682-17690, 2024.\n\n- Daniil A. Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature, 624:570 - 578, 2023.\n\n- Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. ChemCrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.\n\n- He Cao, Yanjun Shao, Zhiyuan Liu, Zijing Liu, Xiangru Tang, Yuan Yao, and Yu Li. PRESTO: Progressive pretraining enhances synthetic chemistry outcomes. arXiv preprint arXiv:2406.13193, 2024.\n\n- Aaron Chan, Zhiyuan Zeng, Wyatt Lake, Brihi Joshi, Hanjie Chen, and Xiang Ren. KNIFE: Distilling meta-reasoning knowledge with free-text rationales. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML, 2023.\n\n- Kourosh Darvish, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa, Sagnik Som, Miroslav Bogdanovic, Yang Cao, Han Hao, Haoping Xu, Al\u00e1n Aspuru-Guzik, Animesh Garg, and Florian Shkurti. Organa: A robotic assistant for automated chemistry experimentation and characterization. ArXiv, abs/2401.06949, 2024.\n\n- Ernest Davis and Scott Aaronson. Testing GPT-4 with wolfram alpha and code interpreter plug-ins on math and science problems. arXiv preprint arXiv:2308.05713, 2023.\n\n- Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, and Huajun Chen. SciKnowEval: Evaluating multi-level scientific knowledge of large language models. arXiv preprint arXiv:2406.09098, 2024.\n\n- Chrisantha Fernando, Dylan Sunil Banarse, Henrik Michalewski, Simon Osindero, and Tim Roktaschel. Promptbreeder: Self-referential self-improvement via prompt evolution. In *Forty-first International Conference on Machine Learning*, 2023.\n\n- Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations, 2022.\n\n- Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16477-16508, 2023.\n\n- Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, et al. Critic: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, 2023.\n\n- Jing Guo, Nan Li, Jian Qiang Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, and Ming Xu. Empowering working memory for large language model agents. ArXiv, abs/2312.17259, 2023a.\n\n- Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, et al. What can large language models do in chemistry? a comprehensive benchmark on eight tasks. Advances in Neural Information Processing Systems, 36:59662-59688, 2023b.\n\n- Joseph F Hair Jr, Wiliam C Black, Barry J Babin, and Rolph E Anderson. Multivariate data analysis. In Multivariate data analysis, pp. 785-785. Prentice Hall, 2010.\n\n- Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint arXiv:2408.08435, 2024.\n\n- Zhiting Hu and Tianmin Shu. Language models, agent models, and world models: The law for machine reasoning and planning. arXiv preprint arXiv:2312.05230, 2023.\n\n- Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations, 2023.\n\n- Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey. ArXiv, abs/2402.02716, 2024a.\n\n- Yuqing Huang, Rongyang Zhang, Xuesong He, Xuyang Zhi, Hao Wang, Xin Li, Feiyang Xu, Deguang Liu, Huadong Liang, Yi Li, Jian Cui, Zimu Liu, Shijin Wang, Guoping Hu, Guiquan Liu, Qi Liu, Defu Lian, and Enhong Chen. Chemeval: A comprehensive multi-level chemical evaluation for large language models. arXiv preprint arXiv:2409.13989, 2024b.\n\n- Shuyang Jiang, Yuhao Wang, and Yu Wang. Selfevolve: A code evolution framework via large language models. arXiv preprint arXiv:2306.02907, 2023.\n\n- Zhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xiaoran Jin, and Hang Li. Design of chain-of-thought in math problem solving. arXiv preprint arXiv:2309.11054, 2023.\n\n- Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2901-2910, 2017.\n\n- Scott Barry Kaufman. Intelligence and the cognitive unconscious. The Cambridge handbook of intelligence, pp. 442-467, 2011.\n\n- Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In *The Eleventh International Conference on Learning Representations*, 2022.\n\n- Jiawei Li, Yizhe Yang, Yu Bai, Xiaofeng Zhou, Yinghao Li, Huashan Sun, Yuhang Liu, Xingpeng Si, Yuhao Ye, Yixiao Wu, et al. Fundamental capabilities of large language models and their applications in domain scenarios: A survey. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11116-11141, 2024a.\n\n- Kun Li, Xin Jing, and Chengang Jing. Vector storage based long-term memory research on llm. International Journal of Advanced Network, Monitoring and Controls, 2024b.\n\n- Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, et al. MMSci: A multimodal multi-discipline dataset for PhD-level scientific comprehension. arXiv preprint arXiv:2407.04903, 2024c.\n\n- Chang Liao, Yemin Yu, Yu Mei, and Ying Wei. From words to molecules: A survey of large language models in chemistry. arXiv preprint arXiv:2402.01439, 2024.\n\n- AI@Meta Llama Team. The llama 3 herd of models, July 2024. URL https://llama.meta.com/.\n\n- Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521, 2022.\n\n- Donald A McQuarrie. Quantum chemistry. University Science Books, 2008.\n\n- Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu, Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Amir Mohammad Elahi, Maximilian Greiner, Caroline T. Holick, Tanya Gupta, Mehrdad Asgari, Christina Glaubitz, Lea C. Klepsch, Yannik K\u00f6ster, Jakob Meyer, Santiago Miret, Tim Hoffmann, Fabian Alexander Kreth, Michael Ringleb, Nicole Roesner, Ulrich S. Schubert, Leanne M. Stafast, Dinga Wonanke, Michael Pieler, Philippe Schwaller, and Kevin Maik Jablonka. Are large language models superhuman chemists?, 2024. URL https://arxiv.org/abs/2404.01475.\n\n- OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, et al. GPT-4 technical report, 2024.\n\n- Magda Osman. An evaluation of dual-process theories of reasoning. Psychonomic bulletin & review, 11(6):988-1010, 2004.\n\n- Siru Ouyang, Zhuosheng Zhang, Bing Yan, Xuan Liu, Yejin Choi, Jiawei Han, and Lianhui Qin. Structured chemistry reasoning with large language models. In *Forty-first International Conference on Machine Learning*, 2024.\n\n- Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. Is a question decomposition unit all we need? In 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, 2022.\n\n- Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, and Rui Yan. Leveraging biomolecule and natural language through multi-modal learning: A survey. arXiv preprint arXiv:2403.01528, 2024.\n\n- Cheng Qian, Shihao Liang, Yujia Qin, Yining Ye, Xin Cong, Yankai Lin, Yesai Wu, Zhiyuan Liu, and Maosong Sun. Investigate-consolidate-exploit: A general strategy for inter-task agent self-evolution. arXiv preprint arXiv:2401.13996, 2024.\n\n- Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve. arXiv preprint arXiv:2407.18219, 2024.\n\n- Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\n\n- Marta Skreta, Zihan Zhou, Jia Lin Yuan, Kourosh Darvish, Al\u00e1n Aspuru-Guzik, and Animesh Garg. Replan: Robotic replanning with perception and language models. ArXiv, abs/2401.04157, 2024.\n\n- Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. SciEval: A multi-level large language model evaluation benchmark for scientific research. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,17, pp. 19053-19061, 2024a.\n\n- Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. Trustl'm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561, 2024b.\n\n- Vicente Talanquer. The complexity of reasoning about and with chemical representations. JACS Au, 2(12):2658-2669, 2022. doi: 10.1021/jacsau.2c00498.\n\n- David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, et al. SciRIFF: A resource to enhance language model instruction-following over scientific literature. arXiv preprint arXiv:2406.07835, 2024.\n\n- Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. SciBench: Evaluating college-level scientific problem-solving abilities of large language models. In *Forty-first International Conference on Machine Learning*, 2024a.\n\n- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023.\n\n- Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv preprint arXiv:2409.07429, 2024b.\n\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.\n\n- Kangda Wei, Dawn Lawrie, Benjamin Van Durme, Yunmo Chen, and Orion Weller. When do decompositions help for machine reading? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 3599-3606, 2023.\n\n- Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2, 2022.\n\n- Yi Xiao, Xiangxin Zhou, Qiang Liu, and Liang Wang. Bridging text and molecule: A survey on multimodal frameworks for molecule. arXiv preprint arXiv:2403.13830, 2024.\n\n- Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2023.\n\n- Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. arXiv preprint arXiv:2406.04271, 2024.\n\n- Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Yuan Zhang, Chao Wang, Juntong Qi, and Yan Peng. Leave it to large language models! correction and planning with memory integration. Cyborg and Bionic Systems, 5, 2023.\n\n- Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu Wang. Small language models need strong verifiers to self-correct reasoning. arXiv preprint arXiv:2404.17140, 2024a.\n\n- Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model based agents. ArXiv, abs/2404.13501, 2024b.\n\n- Ming Zhong, Siru Ouyang, Minhao Jiang, Vivian Hu, Yizhu Jiao, Xuan Wang, and Jiawei Han. ReactIE: Enhancing chemical reaction extraction with weak supervision. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 12120\u201312130, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023-findings-acl.767. URL https://aclanthology.org/2023-findings-acl.767.\n\n- Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):19724-19731, Mar. 2024a. doi: 10.1609/aaai.v38i17.29946. URL https://ojs.aaai.org/index.php/AAAI/article/view/29946.\n\n- Xianrui Zhong, Yufeng Du, Siru Ouyang, Ming Zhong, Tingfeng Luo, Qirong Ho, Hao Peng, Heng Ji, and Jiawei Han. Actionie: Action extraction from scientific literature with programming languages. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12656-12671, 2024b.\n\n- Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WZH7099tgfm.\n\n- Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532, 2024.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638524.8236718", "msecs": "823.0", "relativeCreated": "121654.16932106018", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Output-Constrained Decision Trees\n\n# Abstract\nWhen there is a correlation between any pair of targets, one needs a prediction method that can handle vector-valued output. In this setting, multi-target learning is particularly important as it is widely used in various applications. This paper introduces new variants of decision trees that can handle not only multi-target output but also the constraints among the targets. We focus on the customization of conventional decision trees by adjusting the splitting criteria to handle the constraints and obtain feasible predictions. We present both an optimization-based exact approach and several heuristics, complete with a discussion on their respective advantages and disadvantages. To support our findings, we conduct a computational study to demonstrate and compare the results of the proposed approaches.\n\n\n# 1 INTRODUCTION\n\n\nDecision trees (DTs) play a fundamental role in machine learning and data science for several key reasons. First, DTs have proven to be accurate predictive models in a wide range of applications. Moreover, the heuristic approaches used for their construction are computationally efficient, resulting in extremely fast training times. Accuracy and speed make them ideal for large datasets or real-time applications. Second, DTs, particularly those that are not too deep, are inherently explainable. By following the branches of such a trained tree, one can easily understand the logic behind the model's predictions. This feature is particularly valuable in sectors where interpretability is critical, such as in healthcare or finance. Lastly, DTs serve as the base learners in powerful ensemble methods. These methods, which include techniques like random forests and gradient boosting, utilize numerous DTs to create robust and accurate predictive models.\n\n\nWhen there is a correlation between any pair of targets, we need a prediction method that can handle vector-valued output. In this setting, multi-target learning is particularly important as it is widely used in fields such as forecasting, monitoring, manufacturing, and recommendation systems. DTs can easily be customized for handling multi-target regression by modifying the splitting criteria to handle vector predictions at each node.\n\n\nMotivation. Despite the wealth of existing literature on DTs, to the best of our knowledge, none consider cases where constraints exist among multiple targets. This is notable, as in applications, decision-makers are aware of these constraints, and data analysts should impose them for feasible predictions. Here is an illustrative example from the field of operations management where machine learning approaches are becoming more prominent in the context of decision-making; see e.g., (Mi\u0161i\u0107 & Perakis, 2019).\n\n\nConsider a dataset of products in a retail store, each with distinct attributes. This dataset includes the inventory amounts of these products across five different warehouses, represented as five-dimensional target vectors (training set). The goal is to predict the inventory amounts for new products across these warehouses (test set).\n\n\nTo achieve this, one can train a DT and obtain the partitions of the existing products represented by the leaf nodes of the trained tree. The prediction for a new product is then simply the average of the inventory amounts (mean vector) within the leaf node that it falls into. This leads to the prediction of five different inventory amounts, i.e., one vector for each warehouse.\n\n\nHowever, it is crucial to consider an additional constraint in our scenario: Due to budget restrictions, a store can only transport a product in a maximum of two out of the five warehouses. This is also evident in the training data set where each target vector has at most two nonzero values. Consequently,\n\n\n![Figure 1: The trade-off between accuracy and feasibility. The red circle in the right plot marks the depth of the tree which returns the lowest mean squared error value for the test set. The corresponding red bar in the left plot shows the corresponding percentage of infeasible predictions. The sample size is 4000.](images/d6bdc19df95d34dbaea9eb56a66a1ba4b1a522ba2c4424d8f9d57ba3b851a5fd.jpg)\n\n\n![](images/2bb04fadddcf6e320cd4122856a38a38c98455662b65df6dea09f31ed66cada8.jpg)\n\n\nif one continues to use the mean vector approach for prediction, it may not only yield inaccurate results but also infeasible predictions because it does not account for this constraint.\n\n\nThis erroneous estimation would also persist with many well-known DT training algorithms, since their estimation at a leaf node is also based on taking the mean or the median of the target vectors within the node. However, one may argue that growing a deeper DT may resolve the infeasibility issue as there will be fewer products at each leaf node. Even if we overlook the loss of interpretability due to the large size of such a tree, it is well-known that deep trees can easily overfit to the training data, potentially leading to poor generalization for new products. For instance, Figure 1 shows our results with one of our datasets from our computational study in Section 3. The bar plot demonstrates that feasible predictions can be achieved with larger tree depths, but this comes with a significant decrease in accuracy as shown in the scatter plot. The scatter plot, on the other hand, shows that while shallow trees offer much better accuracy at the expense of most of their predictions are being infeasible.\n\n\nOutput-Constraints. To obtain accurate and feasible predictions, we propose an optimization-based exact approach. This approach incorporates target constraints into the splitting procedure of common binary tree construction methods. Our approach ensures that the estimated target vector at each node meets the required constraints. However, we observe that the exact approach could have a lengthy computation time, especially for deep trees. To address this, we offer two heuristic approaches. These are based on using existing samples as predictions or relaxing the constraints with a penalty function at each split operation.\n\n\nUp until now, our focus has been on regression problems. It is reasonable to question why we have not considered multi-target classification problems as well. The reason is, if the number of classes for each target and the number of targets are relatively small, we can assign a label to each feasible combination of target values. This approach effectively reduces the problem to a standard single-target classification problem, although possibly with many labels.\n\n\nRelated Work. As machine learning methods improve in accuracy, they may still produce predictions and outcomes that lack physical consistency or feasibility, due to biases or data sparsity (Zhang et al., 2022). To overcome this challenge, hybrid methods that integrate optimization techniques with machine learning models have gained significant attention in recent years due to their ability to leverage the strengths of both approaches. One key aspect of hybrid approaches is their ability to handle constraints from various sources to enhance the feasibility and accuracy of machine learning methods. These constraints can originate from the dataset, user-defined specifications, or domain knowledge (Nanfack et al., 2022). Such machine learning approaches with imposed constraints are developed as practical applications across different domains, such as mechanics (Yan et al., 2022), geomechanics (Chen & Zhang, 2020), and electronics (Lee et al., 2019).\n\n\nTraditional supervised learning methods primarily focus on predicting a single output, using data comprised of instances represented by features and a single response variable. Multi-target regression (also referred to as multi-output regression), is used to predict multiple continuous target variables simultaneously. One of the earliest methods for multi-target regression with decision trees was introduced by De'ath (2001). He extended the univariate recursive partitioning approach from Classification and Regression Trees (CART) (Breiman, 2017) to handle multiple responses, calling this method Multi-Response Trees (MRT). MRTs are built similarly to CART, but the node impurity is redefined as the sum of squared errors across all response variables. Struyf & D\u017eeroski (2006) developed a system for building multi-objective regression trees (MORTs), which include constraints on tree size and accuracy, allowing users to balance interpretability and accuracy by limiting tree size or setting a minimum accuracy threshold. (Adiyeke & Baydo\u011fan, 2022) introduced a novel method combining multi-target regression with unsupervised learning. Specifically, they modified tree construction by incorporating a similarity-based clustering indicator in ensemble methods.\n\n\nDecision trees (DTs) offer some flexibility in enforcing constraints, and many hybrid DT approaches incorporate instance-level constraints to bind a subset of instances in generating meaningfully combined results. For instance, the clustering tree algorithm by Struyf & D\u017eeroski (2007) integrates domain knowledge to impose must-link and cannot-link constraints. DT methods have also been developed to address adversarial examples, which can be considered instance-level constraints (Chen et al., 2019; Calzavara et al., 2019), or attribute-level constraints to assign several features to the predictors (Nanfack et al., 2022). Ben-David (1995) was one of the first to apply attribute-level constraints by imposing monotonicity among the model's predictors, and Cardoso & Sousa (2010) extended this in an ordinal classification setting.\n\n\nOne of the earliest hybrid approaches of imposed constraints was implemented by Lee & Kang (1990), who introduced constraints to deep learning architectures to approximate ordinary and partial differential equations. This approach laid the foundation for the general framework known as physics-informed neural networks, proposed by Raissi et al. (2019), where constraints and boundary conditions are incorporated (Rudin et al., 2022). Similarly, Christopher et al. (2024) integrated constraints and physical principles to generative diffusion processes. To the best of our knowledge, our proposed approach is the first method designed to handle target-level constraints in multi-target regression. This represents a novel contribution, building on the principles of constraint handling and extending them to more complex, interdependent target structures in predictive modeling.\n\n\nContributions. We introduce novel approaches to handle constraints among target variables in DTs. The first method lends itself to an optimization-based exact method that ensures the feasibility of predictions by incorporating target constraints into the splitting procedure of common binary tree construction methods. We also propose two heuristic methods. These methods offer practical solutions to the computation time issue that might be associated with the exact approach, especially for deep trees. We provide the technical details of our proposed methods, exploring their strengths, potential limitations, and the conditions under which they would perform sensibly. To validate our approaches, we provide a reproducible computational study using our publicly available implementation.\n\n\n# 2 OUTPUT-CONSTRAINED DECISION TREES\n\n\nSuppose that we have the training dataset  $\\mathcal{D} = \\{(\\pmb{x}_i, \\pmb{y}_i) : i \\in \\mathcal{I}_{\\mathcal{D}}\\}$  with  $\\pmb{x}_i \\in \\mathbb{R}^p$  and  $\\pmb{y}_i \\in \\mathbb{Y} \\subseteq \\mathbb{R}^k$  denoting the input vector and the target (output) vector for the data point  $i$ , respectively. The set  $\\mathbb{Y}$  shows the feasible set for the target vectors. It is important to observe that the target vector of each sample lies in  $\\mathbb{Y}$ , and hence, they are also feasible.\n\n\nA DT is grown by splitting the dataset into non-overlapping subsets recursively (Breiman et al. 1984). Each subset is represented by a node of the tree. The most common approach of tree construction uses binary splits; that is, at each iteration at most two child nodes, left  $(\\leftarrow)$  and right  $(\\rightarrow)$ , are grown from a parent node. Let  $\\mathcal{N} \\subseteq \\mathcal{D}$  be the subset denoted by the parent node, and  $\\mathcal{I}_{\\mathcal{N}} \\subseteq \\mathcal{I}_{\\mathcal{D}}$  indicate the indices of the samples in that node. To obtain a pair of child nodes, a feature  $j$  and a split value  $v$  is selected to obtain the left and right index sets\n\n\n$$\n\n$$\n\\mathcal {I} _ {\\mathcal {N}} ^ {\\leftarrow} (j, v) = \\{i \\in \\mathcal {I} _ {\\mathcal {N}}: x _ {i j} \\leq v \\}\n$$\n\n$$\n\n\nand\n\n\n$$\n\n$$\n\\overrightarrow {\\mathcal {I} _ {\\mathcal {N}}} (j, v) = \\{i \\in \\mathcal {I} _ {\\mathcal {N}}: x _ {i j} > v \\},\n$$\n\n$$\n\n\nrespectively. Then, the gain by such a split is evaluated by\n\n\n$$\n\n$$\n\\alpha_ {\\mathcal {N}} ^ {\\leftarrow} (j, v) \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}} ^ {\\leftarrow} (j, v)} \\ell (\\boldsymbol {y} _ {\\mathcal {N}}, \\boldsymbol {y} _ {i}) + \\alpha_ {\\mathcal {N}} ^ {\\rightarrow} (j, v) \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}} ^ {\\rightarrow} (j, v)} \\ell (\\boldsymbol {y} _ {\\mathcal {N}}, \\boldsymbol {y} _ {i}), \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\pmb{y}_{\\mathcal{N}} \\in \\mathbb{Y}$  is the prediction of the parent node,  $\\ell: \\mathbb{R}^k \\times \\mathbb{R}^k \\mapsto \\mathbb{R}$  is the loss function,  $\\alpha_{\\mathcal{N}}^{-}(j,v) = \\frac{|\\mathcal{I}_{\\mathcal{N}}^{-}(j,v)|}{|\\mathcal{I}_{\\mathcal{N}}|}$  as well as  $\\alpha_{\\mathcal{N}}^{+}(j,v) = 1 - \\alpha_{\\mathcal{N}}^{-}(j,v)$  are the scaling coefficients representing the proportional weights of the respective subsets. Among all possible splits, the one that achieves the largest gain is used to construct the child nodes. The splitting continues until one of the stopping conditions, such as reaching the maximum depth or the minimum number of samples in a node, is satisfied. Then, the samples in the last parent constitute a leaf node. Considering the feasibility requirements on the predictions, the output vector  $\\pmb{y}_{\\mathcal{N}}$  of such a leaf node should be feasible; i.e., it should belong to the feasible set,  $\\pmb{y}_{\\mathcal{N}} \\in \\mathbb{Y}$ .\n\n\n# 2.1 EXACT APPROACH\n\n\nThe common loss functions (e.g., mean squared error, mean Poisson deviance) used in constructing binary DT regressors are separable. Moreover, the loss in the multi-target case is obtained by summing up individual losses for each target. Consider, for instance, the well-known mean squared loss for which the prediction at a node with the subset  $\\mathcal{N}$  is given by\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {Y}} \\frac {1}{2 | \\mathcal {I} _ {\\mathcal {N}} |} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\| \\boldsymbol {y} - \\boldsymbol {y} _ {i} \\| ^ {2}.\n$$\n\n$$\n\n\nIf there are no restrictions on the output vectors, i.e.,  $\\mathbb{Y} = \\mathbb{R}^k$ , then it is not difficult to observe by simple differentiation that  $\\pmb{y}_{\\mathcal{N}}$  is the mean of the target vectors in  $\\mathcal{N}$  minimizing the subset variance. This is, in fact, the output of a node used by the well-known implementations, like scikit-learn (Pedregosa et al. 2011). The same holds for the mean Poisson deviance loss function. In fact, the following theorem shows that the mean vector is still the optimal choice when the constraints on the output vectors result in a convex feasible set. The proof of this theorem follows from observing that the mean operation is equivalent to taking the convex combination of the feasible target vectors in the node. The details of this proof are given formally in Section A of the supplementary file. There, we also present a counter example showing that the same argument does not apply when the loss function is the mean absolute deviation and its unconstrained minimum, i.e., the median of the target vectors, is selected as the node's output (see Remark B in Section A of the supplementary file).\n\n\nTHEOREM 1 Let the feasible set  $\\mathbb{Y} \\subseteq \\mathbb{R}^k$  be convex. If the loss function is the mean squared error or the mean Poisson deviance, then the mean of the target vectors in a node minimizes the loss.\n\n\nThis theorem implies that a restricted class of feasible sets can be handled with standard DT implementations using certain loss functions. For instance, simple bounds or, more generally, linear constraints on the target vectors lead to convex feasible sets. However, the same conclusion does not hold when the feasible set is not convex. Going back to our illustrative example of inventory amount prediction, suppose for simplicity that each warehouse has the same capacity of  $C$  units. If we use only these bounds as constraints, then any standard implementation returning the mean vector for each node output would not violate this constraint. However, if we also impose the budget restriction \"at most two among five warehouses can be used for transportation,\" then we need auxiliary binary variables to represent this restriction as a constraint, and hence, the feasible set  $\\mathbb{Y}$  becomes non-convex. Formally,\n\n\n$$\n\n$$\n\\mathbb {Y} = \\left\\{\\boldsymbol {y} \\in \\mathbb {R} ^ {5}: \\sum_ {t = 1} ^ {5} z _ {t} \\leq 2, 0 \\leq y _ {t} \\leq C z _ {t}, \\text {a n d} z _ {t} \\in \\{0, 1 \\} \\text {f o r} t = 1, \\dots , 5 \\right\\}, \\tag {2}\n$$\n\n$$\n\n\nwhere the binary variable  $z_{t}$  becomes one if  $y_{t}$  units of the product is transported to warehouse  $t$ . Note that when  $z_{t}$  is zero, then it also forces  $y_{t}$  to become zero implying that product is not transported to the corresponding warehouse.\n\n\nWe can now state the formal prediction model that needs to be solved at each node:\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {Y}} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\ell (\\boldsymbol {y}, \\boldsymbol {y} _ {i}), \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\mathcal{I}_{\\mathcal{N}} \\subseteq \\mathcal{I}_{\\mathcal{D}}$  with  $\\mathcal{N} \\subseteq \\mathcal{D}$ . Dealing with nonconvex sets can pose a challenge when solving large-scale constrained models. Fortunately, the dimension of the problem  $(k)$  is quite small for many applications. In case of a mixed integer linear optimization problem, like our example on warehouse inventories, both free and commercial powerful solvers are available, e.g., COIN-OR (Lougee-Heimer, 2003), GUROBI (Gurobi Optimization, LLC, 2023). However, it is also important to remember that this optimization problem must be solved for each pair of feature-split values at every node. This increases the computation time compared to basic DT implementation that ignores the constraints, especially for deep trees. Next, we will also propose several heuristic approaches to develop faster -albeit potentially less accurate or infeasible- alternatives.\n\n\n# 2.2 HEURISTIC APPROACHES\n\n\nA common method for training machine learning models with restrictions involves relaxing these constraints and adding a term to the loss function. This term serves to penalize predictions that are not feasible. In our setting, this idea leads to the following model:\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} ^ {R} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {R} ^ {k}} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\ell (\\boldsymbol {y}, \\boldsymbol {y} _ {i}) + \\lambda v _ {\\mathbb {Y}} (\\boldsymbol {y}), \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\lambda > 0$  is the predefined penalty coefficient, and  $v_{\\mathbb{Y}}: \\mathbb{R}^k \\mapsto \\mathbb{R}_+$  is the penalty function that assigns a positive value to the infeasible predictions,  $y \\notin \\mathbb{Y}$ . This technique is a reminiscent of the approach known as Lagrangean relaxation. However, it is important to note that Lagrangean relaxation would introduce a separate penalty coefficient, known as Lagrangean multiplier, for each constraint (Bertsekas, 1982). The objective function of (4) shows the trade-off between accuracy and feasibility. For instance, using the feasible region defined in (2) leads to the following splitting problem for the mean-squared loss:\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} ^ {R} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {R} ^ {5}, \\boldsymbol {z} \\in \\{0, 1 \\} ^ {5}} \\frac {1}{2 | \\mathcal {I} _ {\\mathcal {N}} |} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\| \\boldsymbol {y} - \\boldsymbol {y} _ {i} \\| ^ {2} + \\lambda \\sum_ {t = 1} ^ {5} \\left(\\left(z _ {t} - 2\\right) + \\left(y _ {t} - C z _ {t}\\right)\\right). \\tag {5}\n$$\n\n$$\n\n\nHere,  $\\lambda$  in (4) plays the role of balancing optimality versus feasibility. As  $\\lambda$  increases, the optimization algorithm tends to favor feasible solutions over accurate ones. Conversely, for small values of  $\\lambda$ , accuracy is prioritized. Therefore, one is not guaranteed to obtain a feasible solution with the relaxation approach. Additionally, depending on the structure of the penalty function  $v_{\\mathbb{Y}}$ , problem (4) can be difficult to optimize. Note that tuning  $\\lambda$  requires attention as the optimization problem changes at each node. Attempting to adjust a different penalty coefficient for each node could quickly become a computationally formidable task.\n\n\nOur second heuristic is based on observing that all samples in a node belong to the feasible set. Thus, selecting one of them results in a feasible prediction. Considering the spread of the output values in the subset, a natural candidate could be the medoid as it would be a representative sample within the cluster designated by the node. The medoid of the subset is obtained by solving\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} ^ {M} = \\arg \\min  _ {\\boldsymbol {y} _ {i ^ {\\prime}} \\in \\mathcal {N}} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\ell \\left(\\boldsymbol {y} _ {i ^ {\\prime}}, \\boldsymbol {y} _ {i}\\right). \\tag {6}\n$$\n\n$$\n\n\nThe solution to this problem involves evaluating the pairwise loss values and then summing these values for each sample. As a result, the medoid of a subset of samples can be found very quickly. In case there are multiple samples that could be the medoids, one of them can be selected arbitrarily. Clearly, the medoid at each node is a feasible prediction but not necessarily the optimal solution of (3). At this point, we note that a tree can be grown very deep so that each leaf node contains a single sample. While this allows for feasible predictions, such deep trees are susceptible to overfitting and can quickly lose interpretability (see also Figure 1).\n\n\nSimilar to the exact approach of the previous subsection, both heuristic approaches above are applied to every feature and split value at each node. An alternative heuristic would be growing a tree as in basic implementation without considering the constraints. Then, the feasibility of DT's prediction can be guaranteed by solving the exact model (3), the medoid model (6), or the relaxation model (4) with an appropriate choice of the penalty coefficient- at the leaf nodes only. This approach would improve the training time significantly.\n\n\n# 3 COMPUTATIONAL STUDY\n\n\nThis section is dedicated to testing the performance of our exact and heuristic approaches proposed for training output-constrained decision trees (OCDTs). We obtain different implementation variants by using one of the proposed approaches at a decision node for splitting (S) or at a leaf (L) node for prediction. We also use the shorthand notation O, R, M, A for the exact optimization, relaxation, medoid, and average approaches, respectively. For instance, the variant that uses relaxation approach for splitting and the medoid approach for prediction is denoted by S(R):L(M). Likewise, S(A):L(A) simply refers to standard DT implementation in software packages that ignores the constraints. In our study, we have used the scikit-learn implementation only for this variant (Pedregosa et al., 2011).\n\n\nOur objective in this section is two-fold: First, we aim to assess the effectiveness of our proposed approaches on an existing dataset. Second, we aim to evaluate their performance in a controlled setting using an extended version of the same dataset. Our computational experiments can be reproduced by using the dedicated GitHub repo<sup>2</sup>. For all methods compared, we experimented with varying maximum depth values of 3, 6, 9, 12, and 15, while ensuring a minimum of 10 data points to split a node and at least 5 data points in each leaf. The selection of penalty coefficients,  $\\lambda$ , for each variant can be guided by evaluating several values over a reasonable range through cross-validation. Ideally, the chosen coefficient should minimize MSE while ensuring that no infeasible predictions are generated. The results were obtained using a computer equipped with an Intel(R) Core(TM) i7-9750H CPU 2.60GHz processor, featuring 12 CPUs, and 64GB of RAM.\n\n\n# 3.1 SCORE DATASET\n\n\nAs a part of our computational study, we employ the scores dataset available online (Kimmons, 2012). This dataset consists of 1000 records, simulating various personal and socio-economic factors of students at a public school. The dataset features three target variables, representing math, reading, and writing scores. To tailor the data to our research objectives, we have imposed two constraints: First, if the reading score falls below 50, the writing score is set to zero. Second, if the combined reading and writing scores amount to less than 110, the math score is also set to zero. The full description of our mathematical model is available in Section B of the supplementary file.\n\n\nWe conducted a thorough five-fold cross-validation experiment on the dataset using five different tree-depth parameter values, employing a total of nine distinct variants which are trained with the same parameters. These variants are classified into two primary components: split evaluation and leaf predictions. Split evaluation entails the assessment of each candidate for binary splitting based on the prediction generated by a specific method. Conversely, leaf prediction refers to the prediction method employed by each leaf node. For instance, the methodology involving the medoid split evaluation and optimal leaf prediction entails that each split during tree growth is determined by evaluating the total mean squared error (MSE) values of the resulting child nodes using their medoids as predictions. Subsequently, the resulting DT solves an optimization problem specific to the dataset for data falling to each leaf to assign the prediction of that particular node. The penalty coefficient  $\\lambda$  is selected by monitoring the trade-off between feasibility and accuracy through five-fold cross-validation.\n\n\nWe evaluated the performance of each variant based on the MSE value obtained during the cross-validation process. Our analysis focused on comparing the performance of an OCDT variant with different split evaluation and leaf prediction approaches against the baseline DT implementation (scikit-learn). The results are visualized in the left side of Figure 2 illustrating the key metrics concerning the performance of the variants. The bar plot reflects the (scaled) average MSE values with the left axis indicating the corresponding values. On top of each bar, the MSE values are displayed alongside the average number of infeasibilities resulting from the methods' predictions (in parenthesis). The scatter plot shows the average training time of each method with the right axis denoting the corresponding values.\n\n\nIn general, it appears that it is important to assess the performance of each variant individually. We find that the variant utilizing relaxation in the optimization problem in both split evaluation and leaf prediction,  $S(R) : L(R)$  exhibits the worst MSE score. However, the variant employing relaxation\n\n\n![Figure 2: Performance metrics plot (left) and the Friedman test for statistical differences (right) on the scores dataset. The values on each bar represent the average MSE value and the average number of infeasible predictions (in parenthesis) on the test sets. The scatter plot shows the training time for each variant. Note that MSE and computation times are scaled according to the maximum value for the corresponding performance measure obtained by one of the variants.](images/d759bec6808ea51c2bf1bad09f120741cdd224c9bad438dce17f8ba68683bee4.jpg)\n\n\n![](images/a8391e071bd991b506808318d085eac9c0dff9793512688c505bc2e8a32c19b6.jpg)\n\n\nin split evaluation and optimization in leaf prediction,  $S(R) : L(O)$  achieves the best MSE score without violating any constraints. Overall, we observe that the variants solving optimization problems at the leaf nodes for predictions demonstrate the top three performances for this dataset with no constraint violations. This affirms both the efficiency of the proposed variants and their adherence to constraints. We note that the baseline scikit-learn implementation,  $S(A) : L(A)$  violated numerous constraints (averaging 16.4 infeasible predictions), aligning with our earlier findings.\n\n\nThe comparison of the proposed variants for statistical significance is conducted using the Friedman test (Friedman, 1940), and if a significant difference is detected by the Friedman test, it is followed by the Nemenyi test (Nemenyi, 1963). The right side of Figure 2 shows the test results. We observe that the  $p$ -value is practically zero indicating that the null hypothesis of identical variants can be rejected at the 0.95 confidence level. This means that there is a statistically significant difference in the performances of the variants with S (R): L (O) having a notably better performance when compared against the others.\n\n\n# 3.2 EXTENDED DATASETS\n\n\nIn this section, we extended the previous dataset (Kimmons 2012) to mimic the motivating inventory control example that we have given in Section I. To test the proposed approaches in this controlled environment, we imposed other restrictions on the dataset, like each target vector having at most one nonzero value. The details of our data generation process along with the underlying mathematical model is given in Section C of the supplementary file. We call this dataset class.\n\n\nIn accordance with the experiments in Section 3.1, we apply the same methodology with five-fold cross-validation. This is done to compare our nine variants using the same parameters. To leverage the advantage of data generation, we evaluated our proposed variants across nine different datasets, varying in size  $(|\\mathcal{I}_{\\mathcal{D}}|)$  and the number of targets  $(k)$ . These datasets comprised of combinations of 500, 1000, and 2000 samples with total number of target values of 3, 5 and 7.\n\n\nThe MSE score and computation time are depicted on the left side of Figure 3, where the average metrics across all nine datasets are given. On the right side of Figure 3 we see that  $p$ -value is close to 0, which indicates that the methods have statistically different performances at the 0.95 confidence level. We observe that none of the proposed algorithms produce infeasible predictions, unlike the baseline scikit-learn method, S(A):L(A) which, despite yielding the lowest MSE among all approaches, generated the only positive average number of infeasible predictions. This suggests that the scikit-learn method compromised constraints to improve the prediction performance, a trade-off that is clearly not suitable for applications.\n\n\nThe box plot in Figure 4 illustrates the MSE values of all the variants across nine datasets with different sizes in terms of sample-sizes and targets. The proposed variants show comparable MSE scores, with most of the methods achieving scores within a close range, as indicated by the overlapping\n\n\n![Figure 3: Performance metrics plot (left) and the Friedman test for statistical differences (right) on class datasets. The values on each bar of the left plot represent the average MSE value and the average number of infeasible predictions (in parenthesis) on the test sets. The scatter plot shows the training time for each variant. Note that MSE and computation times are scaled according to the maximum value for the corresponding performance measure obtained by one of the variants.](images/ea99b744bf5065b2de52e1733bbc5ecd95b1ec6e9542e885200cbd0489f85569.jpg)\n\n\n![](images/882d8c00bb55aefbb529f955d0916f33c240c7a2651c296cd17f29e92645c454.jpg)\n\n\n![Figure 4: MSE box plot on the class datasets.](images/d63e0ff817c61272c47a7af1313c894f47174d8784d719a1532a0cee31511ae0.jpg)\n\n\ninterquartile ranges. While there is some variation, the results suggest that none of the methods stand out as consistently superior in terms of accuracy.\n\n\nFigure 5 depicts that the training times of the variants vary significantly with those involving optimization and relaxation, such as S (R): L (R) and S (R): L (M), taking notably longer. This observation aligns with the computational complexity expected from optimization-based approaches. Another noteworthy observation is that the methods with relatively short training times, like S (A): L (M), tend to have higher MSE values. This trade-off between model complexity and training time versus prediction accuracy is evident across the board. Furthermore, variants like S (O): L (M) strike a balance, achieving moderate MSE values while maintaining competitive training times, suggesting a promising option for practical use in time-sensitive applications.\n\n\nFigure 6 illustrates a schematic representation of a tree trained on a class dataset. This tree structure is obtained by solving the optimization problem for the split evaluation as well as for the leaf prediction; i.e., the selected variant is  $S(\\mathsf{O}) : \\mathsf{L}(\\mathsf{O})$ . In this figure, the green nodes signify the leaf nodes, while the red nodes represent the decision nodes where the data is split. Additionally, the splits after each decision node are depicted with arrows indicating the left and the right child nodes, accompanied by\n\n\n![Figure 5: Training time box plot on the class datasets.](images/a2a177166d8584ea2899caa573943b423d10617c0985d2e400c464eb417dc84c.jpg)\n\n\n![Figure 6: Visualization of an OCDT model on a class dataset.](images/9713a27d8cdc5a9db062b2c0c34fc7ad87fee56800f83fa43268dc638e9b3f95.jpg)\n\n\ntext specifying the split feature and threshold. It is easy to observe on the tree plot that the OCDT made splits are both accurate and feasible. As we outline in Section C of the supplementary file, the target values are generated in a way that they are dependent on Grade and GradeAvgPrevElec. This is clearly depicted in the tree plot as the majority of splits employed by the OCDT do use those features. Additionally, the second-level splits in the tree plot are dependent on feature Class being one or higher, which aligns with the conditions imposing constraints to assign some targets to zero. This visualization example shows that OCDTs have the potential to be interpretable like their conventional counterparts available in standard DT implementations.\n\n\nOur computational study shows that the proposed approach is highly effective in meeting constraints while delivering strong performance on various datasets. By incorporating optimization in split evaluations and leaf predictions, our methodology offers more accurate and feasible predictions compared to traditional methods, even on complex datasets. These results emphasize the usability and versatility of our approach, particularly in situations where compliance with constraints is essential for success.\n\n\n# 4 CONCLUSION\n\n\nWe have introduced new methods to handle constraints among target variables in decision trees, an area yet unexplored in the existing literature. We have proposed an optimization-based exact approach and several heuristic approaches to ensure feasible predictions and maintaining computational efficiency. Through a computational study with existing and generated datasets, we have demonstrated the effectiveness of the proposed methods in producing feasible predictions, showcasing their potential for applications in multi-target learning.\n\n\nLimitations. The work presented here has a few limitations. First, the optimization-based exact method can be computationally demanding for deep trees, which may impede its application for larger datasets. Second, the heuristic approaches proposed may not always produce accurate or even feasible predictions, limiting their reliability. Third, when the constraints among targets are highly non-convex, solving the prediction problem at each node can prove challenging, potentially affecting the overall performance of the decision tree. Finally, in its current form, it is unclear how these output-constrained decision trees can serve as the base learners for ensemble methods.\n\n\nFuture research. We aim to investigate the integration of our approach into bagging methods like random forests, as trees often act as base learners in these methods. We are also interested in exploring a warm-start method in the exact approach, where an optimization problem needs solving at each node. Given that these nodes share a parent-child relationship, the subsets they create could aid in devising this method for subsequent problems at the child nodes once the parent node is resolved. We are also considering the inclusion of target constraints in optimization-based algorithms for training trees like, for example, optimal classification trees (Bertsimas & Dunn, 2017). Although these algorithms can currently handle smaller problem sizes compared to standard implementations like CART (Breiman et al., 1984), this is a promising research direction. We are also looking into the potential advantages of considering constraints in clustering, since training a decision tree can be seen as a supervised clustering method.\n\n\n# REFERENCES\n\n- Esra Adiyeke and Mustafa G\u00f6k\u00e7e Baydo\u011fan. Semi-supervised extensions of multi-task tree ensembles. Pattern Recognition, 123(1):1-16, 2022. ISSN 0031-3203.\n\n- Arie Ben-David. Monotonicity maintenance in information-theoretic machine learning algorithms. Machine Learning, 19:29-43, 1995.\n\n- D.P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Computer Science and Applied Mathematics: A Series of Monographs and Textbooks. Academic Press, 1982.\n\n- Dimitris Bertsimas and Jack Dunn. Optimal classification trees. Machine Learning, 106(7):1039-1082, 2017.\n\n- Leo Breiman. Classification and Regression Trees. Routledge, New York, 2017.\n\n- Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. Cart. Classification and Regression Trees, 1984.\n\n- Stefano Calzavara, Claudio Lucchese, and Gabriele Tolomei. Adversarial training of gradient-boosted decision trees. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pp. 2429-2432, 2019.\n\n- Jaime S Cardoso and Ricardo Sousa. Classification models with global constraints for ordinal data. In 2010 Ninth International Conference on Machine Learning and Applications, pp. 71-77. IEEE, 2010.\n\n- Hongge Chen, Huan Zhang, Duane Boning, and Cho-Jui Hsieh. Robust decision trees against adversarial examples. In International Conference on Machine Learning, pp. 1122-1131. PMLR, 2019.\n\n- Yuntian Chen and Dongxiao Zhang. Physics-constrained deep learning of geomechanical logs. IEEE Transactions on Geoscience and Remote Sensing, 58(8):5932-5943, 2020.\n\n- Jacob K Christopher, Stephen Baek, and Ferdinando Fioretto. Constrained synthesis with projected diffusion models, 2024. URL https://arxiv.org/abs/2402.03559\n\n- Glenn De'ath. Multivariate regression trees: A new technique for modeling species-environment relationships. Ecology, 83(4):1105-1117, 2001. ISSN 1939-9170.\n\n- Milton Friedman. A comparison of alternative tests of significance for the problem of m rankings. The annals of mathematical statistics, 11(1):86-92, 1940.\n\n- Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. https://www.gurobi.com\n\n- Royce Kimmons. Exam Scores, 2012. http://roycekimmons.com/tools/generated_data/exams. Accessed on May 20, 2024.\n\n- Hoon Lee, Sang Hyun Lee, and Tony QS Quek. Constrained deep learning for wireless resource management. In ICC 2019-2019 IEEE International Conference on Communications (ICC), pp. 1-6. IEEE, 2019.\n\n- Hyuk Lee and In Seok Kang. Neural algorithm for solving differential equations. Journal of Computational Physics, 91(1):110-131, 1990.\n\n- Robin Lougee-Heimer. The common optimization interface for operations research: Promoting open-source software in the operations research community. IBM Journal of Research and Development, 47(1):57-66, 2003.\n\n- Velibor V. Mi\u0161i\u0107 and Georgia Perakis. Data analytics in operations management: A review. Manufacturing & Service Operations Management, 22(1):158-169, 2019.\n\n- G\u00e9rald\u00edn Nanfack, Paul Temple, and Benoit Fr\u00e9nay. Constraint enforcement on decision trees: A survey. ACM Computing Surveys (CSUR), 54(10s):1-36, 2022.\n\n- Peter Bjorn Nemenyi. Distribution-Free Multiple Comparisons. Princeton University, 1963.\n\n- F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.\n\n- Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686-707, 2019.\n\n- Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable machine learning: Fundamental principles and 10 grand challenges. Statistic Surveys, 16:1-85, 2022.\n\n- Jan Struyf and Sa\u0161o D\u017eeroski. Clustering trees with instance level constraints. In Machine Learning: ECML 2007: 18th European Conference on Machine Learning, Warsaw, Poland, September 17-21, 2007. Proceedings 18, pp. 359-370. Springer, 2007.\n\n- Jan Struyf and Sa\u0161o D\u017eeroski. Constraint based induction of multi-objective regression trees. In Knowledge Discovery in Inductive Databases: 4th International Workshop, Porto, 2006.\n\n- Bicheng Yan, Dylan Robert Harp, Bailian Chen, and Rajesh Pawar. A physics-constrained deep learning model for simulating multiphase flow in 3d heterogeneous porous media. *Fuel*, 313: 122693, 2022. ISSN 0016-2361. URL https://www.sciencedirect.com/science/article/pii/S001623612102559X\n\n- Chunjie Zhang, Renguang Zuo, Yihui Xiong, Xinfu Zhao, and Kuidong Zhao. A geologically-constrained deep learning algorithm for recognizing geochemical anomalies. Computers and Geosciences, 162:105100, 2022. ISSN 0098-3004. URL https://www.sciencedirect.com/science/article/pii/S0098300422000620\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638525.9845817", "msecs": "984.0", "relativeCreated": "122815.07921218872", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Adversarial Data Robustness via Implicit Neural Representation\n\n# Abstract\nDespite its effectiveness, adversarial training requires that users possess a detailed understanding of training settings. However, many common users lack such expertise, making adversarial training impossible and exposing them to potential threats. We propose ``adversarial data robustness'', allowing the data to resist adversarial perturbations. Then, even if adversaries attack those data, these post-attack data can still ensure downstream models' robustness at users' end. This leads to our new setup, where we store the data as a learnable representation via Implicit Neural Representation (INR). Then, we can train such a representation adversarially to achieve data robustness. This paper analyzes the possible attacks to this setup and proposes a defense strategy. We achieve a comparable robustness level without resorting to model-level adversarial training.\n\n\n# 1 INTRODUCTION\n\n\nAdversarial training, a common defense mechanism against adversarial attacks, operates at the model level by specifically training deep learning models with adversarial examples. However, model-level adversarial training may undermine the desired performance of deep learning models when the robustness requirement is incorporated. Moreover, introducing large-scale base models also makes model-level adversarial training difficult. Therefore, it is essential to investigate alternatives to enhance these models' robustness without resorting to model-level adversarial training.\n\n\nAs adversarial perturbations are directly injected to images (Szegedy et al., 2014; Madry et al., 2018), a straightforward solution is to make those images robust to adversarial attacks, rather than optimizing a model adversarily like existing solutions (Madry et al., 2018; Zhang et al., 2020). This way, even when adversaries corrupt images as usual, ready-to-use models accessible to ordinary users can continue functioning properly. We call this adversarial data robustness, and propose \"training data adversially\" to achieve this goal. For images, one straightforward approach is to consider image pixels as learnable parameters and optimize these learnable parameters adversarily. However, such a straightforward solution significantly undermines image quality since the pixel values are directly manipulated. Therefore, we employ the Implicit Neural Representation (INR) as the cornerstone for our purposes. INRs represent data by optimizing a neural network to continuously map the coordinates to the corresponding data values (Dupont et al., 2021; Mildenhall et al., 2020; Sitzmann et al., 2020; Chen et al., 2022). Once the optimization converges, image pixels are stored as network weights, and the user can query neural networks with the corresponding coordinates to recover the whole image. Moreover, such a network-based representation framework is compatible with classical adversarial optimization for networks, enabling the generalization of existing model-level adversarial defense techniques.\n\n\nWe envision a setup for adversarial data robustness where typical model users or even the model developers are not burdened with extensive modifications to established frameworks. In this setup, images are pre-converted to their respective INRs and then sent to the model users. These users can then invoke straightforward functions encapsulated within an API to obtain the images for subsequent tasks. Since the generation process of representations incorporates adversarial robustness, images obtained from the post-attack representations maintain stable performance for downstream applications. For example, when the modifications to deep learning models are not permitted, or such modifications are costly, the model developers are not capable of training the model adversarially. By adversarily training the data, the adversarial data robustness can guarantee the function\n\n\n![Figure 1: In the traditional setup, images can be manipulated by adversarial perturbations to fool downstream models. In our setup, we represent images as INRs. By applying a defense strategy at the INR creation stage, the data can defend against adversarial perturbations during data creation and transmission. Downstream models' robustness to adversarial attacks can be ensured without adversarial training for models.](images/ccdc9169b35cf42cf86308814d4b8bb68afccdeb26f9918abba700c058271288.jpg)\n\n\nality of the downstream model without altering the model's parameters. This situation has become common, especially when downstream tasks use frozen large models (Guo et al., 2023), as their weights are difficult to modify.\n\n\nWe first explore possible attack types to the INR used as the data representation. Since only INR representation is used during the data transmission, an attack approach is to add adversarial perturbations to the network parameters for data representation. We, therefore, consider two strategies: 1) Attack during creation, which transfers adversarial perturbations from post-attack images to network parameters; and 2) Attack during transmission, which directly injects perturbations into the representation during its transmission. Given that the network parameters of INR are more susceptible to alterations due to external perturbations (Shu & Zhu, 2019), our emphasis is on balancing between attack efficacy and the quality of images generated from post-attack representations. We propose Double Projection Gradient Descent (DPGD) as a method for conducting attacks during data transmission, which can achieve a better balance by implementing gradient constraints on the image during the backpropagation process.\n\n\nWe propose defense-in-creation to defense above attacks within this setup. As Figure 1 depicts, our goal is to achieve adversarial data robustness in the creation process of INR. The INR generated by our method can defend against attacks during creation and attacks during transmission, ensuring the performance of specific downstream applications. To generate robust INR, we solve a min-max optimization problem by incorporating a robustness loss on top of the existing INR training framework. The weights assigned to these two losses can be used to strike a balance between reconstruction quality and robustness. Our research pioneers data robustness exploration, thereby charting a new course towards AI safety. Our major contribution can be concluded as follows:\n\n- - Our adversarial data robustness can guarantee the robustness of deep learning models against adversarial attacks, without necessitating model-level adversarial training.\n\n- - We examine potential attack types to INR used for the data representation. A Double Projection Gradient Descent (DPGD) is proposed to ensure adversarial patterns' invisibility when directly injected into INR parameters used for data representation.\n\n- - We have formulated a defense-in-creation strategy to defense the possible attacks.\n\n\n# 2 RELATED WORK\n\n\nImplicit neural representations. Implicit Neural Representation (INR) is a technique that leverages neural networks to create a mapping between a coordinate and its corresponding signal value. This method offers a continuous and memory-efficient way to model various signals, including 1D audio (Gao et al., 2022), 2D images (Tancik et al., 2020), 3D shapes (Park et al., 2019), 4D light fields (Sitzmann et al., 2021), and 5D radiance fields (Mildenhall et al., 2020). Direct supervision can be used to train accurate INR models for these signals by comparing the network output to ground truth data or indirect means, such as calculating the loss between the output after differentiable operators and a variant of the ground truth signal. This makes INR a powerful tool for solving inverse problems by taking advantage of the well-known forward processes in these problems. INR has found widespread applications in various fields, including computer vision and graphics (Tewari et al., 2022), computational physics (Karniadakis et al., 2021), biomedical engineering (Liu et al.,\n\n\n2022; Zhu et al., 2022), material science (Chen et al., 2020), and fluid mechanics (Raissi et al., 2020; Reyes et al., 2021). As INR is compatible with adversarial training framework, we propose to use INR as cornerstone to achieve data-level robustness.\n\n\nAdversarial attack and defense. Adversarial attacks and defense have emerged as significant research areas in machine learning. To explore the vulnerability of DNNs, various attacks have been proposed (Carlini & Wagner, 2017; Croce & Hein, 2020; Goodfellow et al., 2015; Madry et al., 2018), which can be generally categorized into white-box attacks and black-box attacks (Goodfellow et al., 2015). Most white-box attacks use gradients to obtain the perturbations on the inputs which maximize the loss function, including the Fast Gradient Sign Method (FGSM) (Szegedy et al., 2014), Projection Gradient Descent (PGD) (Madry et al., 2018) and Carlini and Wagner (CW) (Carlini & Wagner, 2017). Black-box attacks involve two methods, where attackers lack access to the victim models' information. Query-based methods approximate perturbations through a large number of queries (Andriushchenko et al., 2020; Cheng et al., 2018; Guo et al., 2019), while transfer-based methods utilize a surrogate model to generate adversarial examples with higher transferability (Liu et al., 2016; Wang & He, 2021). To counter these attacks, researchers have developed various defense mechanisms (Cohen et al., 2019; Li et al., 2021). One common approach is adversarial training, which has shown relative resistance to most existing attacks. The vanilla adversarial training strategy involves incorporating adversarial examples into the training data to create a min-max game during optimization (Madry et al., 2018). Numerous variants of adversarial training algorithms have been developed to improve the performance of adversarial robustness, including early-stopping strategy (Rice et al., 2020), TRADES (Zhang et al., 2019), FAT (Zhang et al., 2020), and CFA (Wei et al., 2023). Other techniques for adversarial defense include defensive distillation (Papernot et al., 2016), gradient regularization (Ross & Doshi-Velez, 2018), gradient masking (Folz et al., 2020), and input denoising (Liao et al., 2018). LINAC (Rusu et al., 2022) proposes to transform images into INR to train downstream networks to improve the robustness. However, these defense methods, which improve robustness via adversially training downstream networks, are not feasible for networks with unmodifiable parameters and could undermine the desired performance. Therefore, we propose a method to achieve adversarial data robustness to solve this problem.\n\n\n# 3 ADVERSARIAL DATA ROBUSTNESS\n\n\n# 3.1 OUR SETUP\n\n\nOur setup is rooted in the reality that many model users often fail to understand the settings associated with adversarial training. Moreover, model-level adversarial training can potentially compromise the intended performance of deep learning models. Therefore, when the adversarial training to models are difficult for common users, we aim to make the data robust to adversarial perturbations. Then, those robust data can ensure robust deep learning models.\n\n\nGiven that adversarial training has been an effective defense mechanism against adversarial attacks, the robust data can only be created via adversarial training. We thus propose storing the image as a network-based representation and enhancing this representation's robustness to potential adversarial perturbations. As depicted in Figure 1, even if this representation is attacked during its creation or transmission, users can still restore its clean state, ensuring robust performance of downstream tasks. We consider Implicit Neural Representation (INR) as the cornerstone of our formulation. For an image  $I$ , its INR is given by a MLP  $f_{\\theta}:\\mathbb{R}^2\\to \\mathbb{R}^3$  that maps the input spatial coordinate  $\\mathbf{p} = (p_x,p_y)\\in \\mathbb{R}^2$  to its corresponding pixel value  $I(\\mathbf{p})\\in \\mathbb{R}^3$  as  $I(\\mathbf{p}) = f_{\\theta}(\\mathbf{p})$ .  $\\theta$  represents the MLP's trainable parameters, which leave access for our adversarial data robustness.\n\n\nGenerally,  $f_{\\theta}$  can only reach its optimum status via iterative optimization. The optimization can be finished by overfitting  $f_{\\theta}$  to the image by minimizing a reconstruction loss between true pixel values  $I(\\mathbf{p}_{m,n})$  and predicted values  $f_{\\theta}(\\mathbf{p}_{m,n})$  as\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {r e c o n}} = \\frac {1}{M \\times N} \\sum_ {m, n} \\| f _ {\\theta} (\\mathbf {p} _ {m, n}) - I (\\mathbf {p} _ {m, n}) \\| _ {2} ^ {2}, \\tag {1}\n$$\n\n$$\n\n\nwhere  $(m,n)$  is the index of the corresponding pixel location, and  $(M,N)$  is size of the image. After optimization, the image can be stored or transmitted as network weights  $\\theta$ .\n\n\n![(a)](images/87b24a3fb2a1bc736048938fd74428d83fa4bda0494ad0bed6c0fa00eb565770.jpg)\n\n\n![(b)](images/5c5afad58216684fa43d367672314ca0f7b01834263d7df896b4b98011535ea3.jpg)\n\n\n![(c)](images/1f03163d253a330cacd4fffb90a50a224c6e4bf5e348542c88ebbaa26468e4fa.jpg)\n\n\n![(d) Figure 2: Samples after attack during creation. (a) are the ground truth images. (b) are the results of (a) being attacked by PGD. (c) are the images reconstructed from INRs optimized for (b). (d) are the residuals between (b) and (c). The output results of the classifier are labeled in the top left corner of each image. The images reconstructed from the representations attacked during their creation are almost the same to the images directly attacked by PGD, with a low average MSE value.](images/b336747d02b7c5ca00b335fb5be02abd7406181c238f9bb0c730ca1fc13985de.jpg)\n\n\nOnce such a representation is received by model users, they can obtain the image  $\\hat{I}$  from this representation by querying INR  $f_{\\theta}$  at each pixel coordinate as  $\\hat{I} (\\mathbf{p}_{m,n}) = f_{\\theta}(\\mathbf{p}_{m,n})$ , where  $(m,n)$  is the pixel index on the image. We define  $\\Psi :f_{\\theta}\\mapsto \\hat{I}$  as the mapping from INR to the reconstructed image. The image  $\\hat{I}$  is then fed into the downstream model to obtain the result.\n\n\nIn light of this setup, we examine the potential attack types to INR. Subsequently, we will delve into the associated adversarial training to ensure data robustness.\n\n\n# 3.2 ATTACK TYPES TO INR\n\n\nAn adversarial attack is to perturb the data used for downstream tasks (Szegedy et al., 2014). Usually, it is performed by introducing adversarial perturbations to input data, which is a designed slight alteration to data that can significantly change the output of downstream models. Since the data has been implicitly represented in our setup, the only way to conduct an adversarial attack is to inject adversarial perturbations into the representation. Similar to an adversarial attack directly for images, two criteria should be guaranteed: 1) invisibility: the perturbations are subtle and remain invisible within the content (Laidlaw et al., 2021); and 2) attacking efficacy: the invisible perturbations can still impair the efficacy of downstream applications (Szegedy et al., 2014). On the basis of these criteria, we explore attack types that can introduce adversarial perturbations to the data representation.\n\n\nAttack during creation. The first option is to attack the representation by transferring adversarial perturbations from corrupted images to the respective network parameters of INRs. In this case, adversaries introduce perturbations to images and subsequently strive to impart these perturbations onto the network parameters of the INR during its construction. Consequently, the adversarial perturbations, when recovered from the representation along with the image content, intensify the loss function for downstream tasks (e.g., cross-entropy loss for image classification).\n\n\nFormally, the adversarial perturbation for the  $i$ -th image  $I_{i}$  can be modeled as the solution of the following optimization problem, which is the same as the classical adversarial attacks (Madry et al., 2018):\n\n\n$$\n\n$$\n\\max  _ {\\Delta I _ {i}} \\quad \\mathcal {L} _ {C E} \\left(g _ {\\phi} \\left(I _ {i} + \\Delta I _ {i}\\right), y _ {i}\\right), \\quad \\text {s . t .} \\| \\Delta I _ {i} \\| _ {p} \\leq \\epsilon , \\tag {2}\n$$\n\n$$\n\n\nwhere  $g_{\\phi}$  denotes the neural network for downstream tasks (e.g., classification (Szegedy et al., 2014)),  $\\Delta I_i$  is the adversarial perturbation for the  $i$ -th image,  $y_i$  is the true label of  $I_i$  for classification task, and  $\\mathcal{L}_{CE}(\\cdot)$  denotes the cross entropy loss function. The  $\\| \\cdot \\|_p$  denotes the  $\\ell_p$ -norm, and the constraint limits the size of perturbation with maximum allowable value  $\\epsilon$  to avoid being recognized by humans. Unless otherwise stated, the infinite norm is adopted by setting  $p = \\infty$  in this paper following previous works (Dong et al., 2023).\n\n\nINRs can accurately capture the very fine details of images (Chen et al., 2021; Str\u00fcmpler et al., 2022). Therefore, during the creation of an INR, the INR weights can accurately represent adversarial perturbation  $\\Delta x$ . The reconstructed image from such an INR retains the adversarial information and can deceive the downstream applications. As shown in Figure 2, the images reconstructed from INR under attack during creation resemble the corrupted images with adversarial perturbation, allowing the reconstructed image to fool the downstream applications.\n\n\n![Figure 3: Framework of defense-in-creation. We encode an image into the parameter of its corresponding INR,  $\\theta$ . Left: we calculate the distance between the reconstructed image and the to-be-fitted image to establish a reconstruction loss for high-quality reconstruction. Right: we introduce adversarial perturbation into the parameter  $\\theta$  during INR creation process, and design a robustness loss to ensure downstream models' robustness by comparing its prediction with the groundtruth.](images/1317a0a9c0b6a1410b2351019b63b34f1561739d020434add906d9d69b17d797.jpg)\n\n\nAttack during transmission. The second strategy is to attack the representation directly. This attack typically occurs during the transmission of the data representation. When the adversaries access the representation, they can directly inject adversarial perturbations into the network parameters. The process of conducting an adversarial attack to the INR representation of  $i$ -th image during its transmission can be concluded as follows:\n\n\n$$\n\n$$\n\\max  _ {\\Delta \\theta_ {i}} \\quad \\mathcal {L} _ {C E} \\left(g _ {\\phi} \\left(\\Psi \\left(f _ {\\theta_ {i} + \\Delta \\theta_ {i}}\\right)\\right), y _ {i}\\right), \\quad \\text {s . t .} \\| \\Delta \\theta_ {i} \\| _ {p} \\leq \\delta , \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\Delta \\theta_{i}$  is the adversarial perturbation for the  $i$ -th image INR, and we also use  $\\ell_{\\infty}$ -norm in Equation 3 to limit the size of the perturbation.\n\n\nA simple method to conduct attack during transmission is to manipulate the INR parameters using the traditional gradient-based attack methods, such as Fast Gradient Sign Method (FGSM) (Szegedy et al., 2014), Projected Gradient Descent (PGD) (Madry et al., 2018), and Carlini and Wagner (CW) (Carlini & Wagner, 2017). These attacks are referred to as FGSM for INR, PGD for INR, and CW for INR, respectively. While these methods are initially designed to operate on images, integrating INR into the differentiable pipeline allows us to derive the gradient value of INR parameters by leveraging the loss value of the downstream classifier. The whole process of PGD attack for INR is shown in Algorithm 1. Given an optimized INR, we start with randomly perturbed parameters within the maximum perturbation. In every iteration, we reconstruct the image from the INR and feed it into the classifier to compute the loss. Then, we iteratively apply gradient updates and ensure that the perturbed parameters remain within the allowable range through projection. However, as INR parameters are sensitive to direct manipulations, such a straightforward solution can easily undermine the quality of images obtained from the post-attack representation.\n\n\nTo better balance the attacking efficacy and the image quality, we propose a Double Projection Gradient Descent (DPGD) that aims to directly manipulate the INR parameters while preserving the reconstructed image's quality. The detailed process of DPGD can be found in Algorithm 2. Unlike PGD for INR, we incorporate an additional projection in each iteration. This projection involves projecting the gradient backpropagated to the reconstructed image onto a boundary controlled by the factor  $\\zeta$ . The motivation of this step is to directly limit the difference between the reconstructed image and the original image at the pixel level. For detailed derivation, please refer to Section B in appendix. The projected gradient is then further backpropagated to update the INR parameters. The quality of images derived from the representations remains high by constraining the gradient on the reconstructed image. The factor  $\\zeta$  can control the balance between the attacking efficacy and the image quality.\n\n\n# 3.3 DEFENSE-IN-CREATION\n\n\nTo achieve data-level robustness, we propose a defense-in-creation strategy that generates robust INR during the creation process to defend against adversarial attacks effectively. The perturbations generated in attacks during creation can be eliminated in optimizing phase, and the INR derived using defense-in-creation can further defend against potential attacks during transmission. By employing this approach, we can ensure the performance of downstream networks at a data level. Besides, since the basic function of INR is to store the content of images, we also consider the quality of the reconstructed image in our method.\n\n\n```txt\n1: Let  $\\theta_{adv} = \\theta$\n```\n\n\n```txt\n2: Random start:  $\\theta_0 = \\theta_{adv} + \\Delta \\theta_0, \\theta_0 \\sim \\mathcal{U}(-\\delta, \\delta)$ .\n```\n\n\n```txt\n3: for  $t = 1,2,\\dots ,N$  do\n```\n\n\n```txt\n4: Reconstruct image  $\\hat{I}_{t - 1} = \\Psi (f_{\\theta_{t - 1}})$\n```\n\n\n```txt\n5:  $\\theta_{t} = \\theta_{t - 1} + \\alpha \\cdot \\mathrm{sign}(\\nabla_{\\theta_{t - 1}}\\mathcal{L}_{CE}(g_{\\phi}\\left(\\hat{I}_{t - 1}\\right),y));$\n```\n\n\n```txt\n6: Update  $\\theta_t = \\mathrm{Proj}_{\\|\\theta_t - \\theta\\|_p \\leq \\delta}(\\theta_t)$ .\n```\n\n\n```txt\n7: end for\n```\n\n\n```txt\n8: return  $\\theta_{adv} = \\theta_N$\n```\n\n\n```txt\n1: Reconstruct original image  $\\hat{I}_{org} = \\Psi (f_{\\theta})$\n```\n\n\n```txt\n2: Let  $\\theta_{adv} = \\theta$\n```\n\n\n```txt\n3: Random start:  $\\theta_0 = \\theta_{adv} + \\Delta \\theta_0, \\theta_0 \\sim \\mathcal{U}(-\\delta, \\delta)$ .\n```\n\n\n```txt\n4: for  $t = 1,2,\\dots ,N$  do\n```\n\n\n```txt\n5: Reconstruct image  $\\hat{I}_{t - 1} = \\Psi (f_{\\theta_{t - 1}})$\n```\n\n\n```txt\n6:  $Grad_{I} = \\frac{\\partial\\mathcal{L}_{CE}(g_{\\phi}(\\hat{I}_{t - 1}),y)}{\\partial\\hat{I}_{t - 1}};$\n```\n\n\n```txt\n7: Update  $Grad_{I} = \\mathrm{Proj}\\left\\| \\hat{I}_{t - 1} + Grad_{I} - \\hat{I}_{org}\\right\\|_{p} \\leq \\zeta (Grad_{I})$ ;  $\\triangleright$  Projecting gradient of image pixels\n```\n\n\n```javascript\n8:  $Grad_{\\theta} = \\frac{\\partial\\hat{I}_{t - 1}}{\\partial\\theta_{t - 1}}\\cdot Grad_{I};$\n```\n\n\n```latex\n9:  $\\theta_{t} = \\theta_{t - 1} + \\alpha \\cdot \\mathrm{sign}(Grad_{\\theta})$\n```\n\n\n```txt\n10: Update  $\\theta_{t} = \\mathrm{Proj}_{\\| \\theta_{t} - \\theta \\|_{p}\\leq \\delta}(\\theta_{t})$  Projecting gradient of INR parameters\n```\n\n\n```txt\n11: end for\n```\n\n\n```txt\n12: return  $\\theta_{adv} = \\theta_N$\n```\n\n\nAs shown in Figure 3, defense-in-creation is implemented by designing a robust loss on top of the original reconstruction loss defined in Equation 1. The robustness loss is obtained by adding adversarial perturbations to the INR parameters and calculating the loss after feeding the reconstructed image into the classifier  $g_{\\phi}$ . The whole defense process can be formulated as follows:\n\n\n$$\n\n$$\n\\min  _ {\\theta} \\lambda_ {1} \\mathcal {L} _ {r e c o n} + \\lambda_ {2} \\max  _ {\\Delta \\theta} \\mathcal {L} _ {C E} \\left(g _ {\\phi} \\left(\\Psi \\left(f _ {\\theta + \\Delta \\theta}\\right)\\right), y _ {i}\\right), \\quad \\text {s . t .} \\| \\Delta \\theta \\| _ {p} \\leq \\delta , \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\Delta \\theta$  is the adversarial perturbation added in the training phase. The parameters  $\\lambda_{1}$  and  $\\lambda_{2}$  balance the reconstruction quality and robustness. The detailed solution to the above optimization problem is shown in Algorithm 3. We use the robustness loss generated from Algorithm 1 to enhance the robustness, and employ reconstruction loss to guarantee the image quality. By minimizing the loss obtained from classifier  $g_{\\phi}$ , we can effectively remove the adversarial perturbation embedded in the target image when performing attack during creation. By introducing adversarial samples during the training process of INR, INR can gradually learn to have better robustness against parameter perturbations, thereby defending against attack during transmission.\n\n\n# 4 EXPERIMENTS AND RESULTS\n\n\n# 4.1 EXPERIMENTAL SETTINGS\n\n\nDataset. We follow established settings (Zhang et al., 2020; Jin et al., 2023) on adversarial attack and defense to evaluate our proposed mechanism. Our experiments are conducted on three real-world datasets: CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and SVHN (Netzer et al., 2011). CIFAR-10/100 dataset contains  $60K$  color images with the size\n\n\n```\nInput: Image  $I$  , ground truth label  $y$  , and pre-trained downstream model  $g_{\\phi}$    \nParameter: Number of iterations  $T$  , learning rate  $\\eta$    \nOutput: INR  $f_{\\theta}$  for image  $I$    \n1: Initialize model parameters  $\\theta$    \n2: for  $t = 1$  to  $T$  do   \n3: Generate INR adversarial example  $f_{\\theta_{adv}}$  using Algorithm 1;   \n4: Compute reconstruction loss  $\\mathcal{L}_{recon}$  as Equation 1;   \n5: Compute robustness loss as  $\\mathcal{L}_{CE}(g_{\\phi}(\\Psi (f_{\\theta_{adv}})),y)$  .   \n6: Compute the total loss  $\\mathcal{L}_{total}$  as Equation 4;   \n7: Update parameters using adversarial example:  $\\theta \\gets \\theta -\\eta \\cdot \\nabla_{\\theta}\\mathcal{L}_{total}$    \n8: end for   \n9: return Trained INR parameters  $\\theta$\n```\n\n\nof  $32 \\times 32$ , including  $50K$  training images and  $10K$  test images in 10 and 100 classes, respectively. SVHN is a dataset collected by Google Street View, consisting of  $32 \\times 32$  color images of house numbers, with 73, 257 training images and 26, 032 test images in 10 classes. The classifier is trained using training images. We train INRs for test images to evaluate our attack approaches and defense methods. As suggested in previous work (Wei et al., 2023; Jin et al., 2022), we evaluate the performance of attacks and defenses based on the PreActResNet-18 (He et al., 2016) and WideResNet34-10 (Zagoruyko & Komodakis, 2016) architectures on CIFAR-10/100 and SVHN.\n\n\nImplementation Details. We implement our method using PyTorch. The INR is a 5-hidden layer MLP with 256 channels per layer and ReLU activation functions for all the data. Following Mildenhall et al. (2020), we also use a positional encoding for pixel coordinates with 5 frequencies. For adversarial defense training, 10-step PGD attack for INR is applied, with maximum perturbation size for parameters  $\\delta = 0.0006$ . We use the Adam optimizer with default values and a learning rate of 0.001. A cosine learning rate decay schedule is used with the minimum value of the multiplier 0.0001. We optimize each INR for 1000 iterations. Other hyper-parameters are specified in each experiment. We conduct experiments on FGSM (Szegedy et al., 2014), PGD (Madry et al., 2018), Carlini and Wagner (CW) (Carlini & Wagner, 2017), and AutoAttack (AA) (Croce & Hein, 2020) for attack during creation. The maximum perturbation size  $\\epsilon$  is set to  $8/255$  for FGSM and PGD. PGD is used with steps 100, and step size  $0.2/255$ . We apply FGSM for INR, PGD for INR, CW for INR, and DPGD for attack during transmission. The steps of PGD for INR, and DPGD are set to 100, and maximum perturbation size for INR parameters  $\\delta = 0.0006$ , with step size  $2.5 \\cdot \\delta/$  steps in all experiments. CW attack is applied by optimizing 100-step PGD (Zhou et al., 2022; Huang et al., 2021). All the experiments are performed on NVIDIA Tesla V100 GPUs.\n\n\nBaselines. We found no method specifically for enhancing the robustness at the data level. Therefore, we compare with other three settings for comparison: 1) Normal training: optimizing the INR with only reconstruction loss; 2) Direct pixel manipulation<sup>1</sup>: direct manipulating the pixel values without INR encoding; 3) Model-level adversarial training: adversarial training for model robustness including AT (Madry et al., 2018), TRADES (Zhang et al., 2019), FAT (Zhang et al., 2020), GAIRAT (Zhang et al., 2021), and LAS-AT (Jia et al., 2022). Although the latter method aims to enhance the robustness through training the downstream classifier, we conduct a comparison with them, and the analysis can be found in Section 4.3.\n\n\nEvaluation metrics. We assess all methods' performance by evaluating image quality, attack effectiveness, and defense ability. For image quality, we evaluate the performance by using PSNR, SSIM, and LPIPS (Zhang et al., 2018). Higher PSNR and SSIM value indicates better performance, while lower LPIPS value indicates better performance for both attack and defense. To assess the effectiveness of attacks, we utilize the attack success rate (ASR). In terms of defense, we evaluate the defense ability by considering the classification accuracy of the downstream classifier.\n\n\n# 4.2 EXPERIMENTAL RESULTS FOR DIFFERENT ATTACKS\n\n\nThis section evaluates the performance of the attacks used in our research. Specifically, we assess the attacks during creation and transmission to evaluate their invisibility and attacking efficacy.\n\n\nTable 1: Evaluation of attack during creation with different image-based attack methods. The results are averaged on all examples in CIFAR-10 test dataset.\n\n\n<table><tr><td>Method</td><td>Clean images</td><td>Clean INR</td><td>Method</td><td>FGSM on images</td><td>Creation after FGSM</td><td>PGD on images</td><td>Creation after PGD</td><td>CW on images</td><td>Creation after CW</td><td>AA on images</td><td>Creation after AA</td></tr><tr><td>Accuracy</td><td>94.97%</td><td>94.73%</td><td>ASR</td><td>60.45%</td><td>59.26%</td><td>100.00%</td><td>99.83%</td><td>99.93%</td><td>99.84%</td><td>100.00%</td><td>99.89%</td></tr><tr><td>PSNR</td><td>-</td><td>48.42</td><td>PSNR</td><td>30.14</td><td>31.01</td><td>32.70</td><td>33.70</td><td>32.68</td><td>34.12</td><td>32.71</td><td>33.85</td></tr><tr><td>SSIM</td><td>1.000</td><td>0.998</td><td>SSIM</td><td>0.921</td><td>0.935</td><td>0.956</td><td>0.967</td><td>0.957</td><td>0.970</td><td>0.956</td><td>0.968</td></tr><tr><td>LPIPS</td><td>0.000</td><td>0.002</td><td>LPIPS</td><td>0.146</td><td>0.134</td><td>0.082</td><td>0.071</td><td>0.082</td><td>0.067</td><td>0.082</td><td>0.070</td></tr></table>\n\n\nTable 2: Reconstruction qualities and accuracies of downstream classifiers compared with normal training and direct pixel manipulation. The results are averaged on all examples in each dataset with different classifier architectures. Image quality evaluations are obtained by comparing with original clean images. The classifier architectures used for CIFAR-10/100 and SVHN are PreActResNet-18 and WideResNet34-10, respectively. More results can be found in the appendix.\n\n\n<table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Method</td><td rowspan=\"2\">PSNR</td><td rowspan=\"2\">Accuracy without attacks</td><td colspan=\"3\">Attack during creation</td><td colspan=\"4\">Attack during transmission</td></tr><tr><td>PGD</td><td>CW</td><td>AA</td><td>DPGD</td><td>FGSM</td><td>PGD</td><td>CW</td></tr><tr><td rowspan=\"4\">CIFAR-10</td><td>Normal image</td><td>-</td><td>94.97%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>39.55%</td><td>0.00%</td><td>0.07%</td></tr><tr><td>Pixel manipulation</td><td>33.16</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>-</td><td>99.88%</td><td>33.80%</td><td>27.88%</td></tr><tr><td>Normal training</td><td>48.38</td><td>94.73%</td><td>0.17%</td><td>0.16%</td><td>0.11%</td><td>20.80%</td><td>57.36%</td><td>18.84%</td><td>17.72%</td></tr><tr><td>Defense-in-creation</td><td>42.92</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>99.60%</td><td>100%</td><td>98.68%</td><td>61.36%</td></tr><tr><td rowspan=\"4\">CIFAR-100</td><td>Normal image</td><td>-</td><td>76.92%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>8.44%</td><td>0.01%</td><td>0.00%</td></tr><tr><td>Pixel manipulation</td><td>29.63</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>-</td><td>99.96%</td><td>30.08%</td><td>0.56%</td></tr><tr><td>Normal training</td><td>48.26</td><td>76.44%</td><td>0.16%</td><td>0.12%</td><td>0.11%</td><td>4.32%</td><td>17.68%</td><td>2.16%</td><td>1.76%</td></tr><tr><td>Defense-in-creation</td><td>40.50</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>98.96%</td><td>100%</td><td>98.44%</td><td>74.52%</td></tr><tr><td rowspan=\"4\">SVHN</td><td>Normal image</td><td>-</td><td>95.80%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>41.84%</td><td>1.32%</td><td>0.96%</td></tr><tr><td>Pixel manipulation</td><td>35.78</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>-</td><td>100%</td><td>76.60%</td><td>18.24%</td></tr><tr><td>Normal training</td><td>53.03</td><td>95.80%</td><td>1.28%</td><td>1.20%</td><td>0.18%</td><td>39.04%</td><td>65.96%</td><td>36.24%</td><td>35.76%</td></tr><tr><td>Defense-in-creation</td><td>46.98</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>99.88%</td><td>100%</td><td>99.88%</td><td>84.84%</td></tr></table>\n\n\nAttack during creation. We validate the attack during creation, and the findings are presented in Table 1. We compare the results of this attack on the INR parameters with those obtained by directly inputting the images attacked by FGSM, PGD, CW, and AA into the downstream classifier. The reconstructed images display comparable attack effectiveness and image quality to those manipulated using FGSM, PGD, CW, and AA methods. Due to the excellent representation capabilities of INR, the performance of the attack during creation is determined by attacks applied to the images.\n\n\n![Figure 4: Evaluation of attack during transmission. Left: we present the ASRs and PSNRs across varying gradient control factors in DPGD. Right: we compare the ASR v.s. PSNR curves with varying the maximum perturbation across different methods. The results are averaged on all examples in CIFAR-10 test dataset.](images/e7ac40b9dc77f8b2c90e27b6d44a9a91c41646ee192d80a8299f861a038e41e3.jpg)\n\n\n![](images/c11888adfa90cd788c6725f0e3f1063302e8607a2f85b09c5d4365e72d27bb29.jpg)\n\n\nAttack during transmission. We evaluate the impact of the image gradient control factor  $\\zeta$  on the invisibility and attacking efficacy in the DPGD algorithm. We fix the maximum perturbation at  $\\delta = 0.0006$ , and the ASR and image quality results are shown in Figure 4(left). The results indicate that the factor  $\\zeta$  can regulate the balance between invisibility and attacking efficacy. Specifically, a larger value of  $\\zeta$  indicates higher attacking efficacy, while a smaller value of  $\\zeta$  indicates a closer resemblance to the original image. We further compare our DPGD algorithm with FGSM for INR, PGD for INR, and CW for INR in Figure 4(right). We take  $\\zeta = 0.9$  and 1.3, respectively, and vary the maximum perturbation  $\\delta$  to obtain curves illustrating the relationship between image quality and ASR. At the same level of image quality, our method achieves higher ASR, indicating a higher attacking efficacy. Therefore, our proposed DPGD achieves better performance.\n\n\n# 4.3 EXPERIMENTAL RESULTS FOR DEFENSE\n\n\nIn this section, we present experimental results of our proposed adversarial defense method and compare them with the baselines.\n\n\nTable 3: Accuracies compared with model-level adversarial training. The classifier structures are all WideResNet34-10. The results are averaged on all examples in CIFAR-10 test dataset.\n\n\n<table><tr><td>Defense method</td><td>Defense-in-creation</td><td>AT</td><td>TRADES</td><td>FAT</td><td>GAIRAT</td><td>LAS-AT</td></tr><tr><td>Natural</td><td>94.32%</td><td>85.90%</td><td>85.72%</td><td>87.97%</td><td>86.30%</td><td>86.23%</td></tr><tr><td>PGD</td><td>92.51%</td><td>53.42%</td><td>53.40%</td><td>47.48%</td><td>40.30%</td><td>53.58%</td></tr></table>\n\n\nRobustness and reconstruction quality. We further evaluate our proposed adversarial defense for achieving data robustness. We set  $\\lambda_{1} = 1$ ,  $\\lambda_{2} = 0.0022$ ,  $0.0006$ ,  $0.001$  for CIFAR-10, CIFAR-100, and SVHN, respectively. The results are shown in Table 2. We study the performance of the proposed defense in defending against various attack methods. Our method achieves the highest accuracy and produces high-quality reconstructions across different attacks, datasets, and classifier architectures. While normal training achieves the best reconstruction results, it is vulnerability to all attacks, resulting in low accuracy under various attack scenarios. Although pixel manipulation can defend against attacks during creation, it leads to lower accuracy when encountering attacks during transmission, such as PGD for INR and CW for INR.\n\n\nComparison with different  $\\lambda_{2}$ . To evaluate the balance between reconstruction quality and robustness in our defense-in-creation strategy, we adjust the weight ratio between reconstruction loss  $\\lambda_{1}$  and weight of robustness loss  $\\lambda_{2}$ . We conduct a series of experiments under attacks during transmission by fixing  $\\lambda_{1} = 1$  and varying the weight of robustness loss  $\\lambda_{2}$ . As shown in Figure 5, a smaller value of  $\\lambda_{2}$  leads to higher reconstruction quality, while a larger value of  $\\lambda_{2}$  enhances robustness against attacks.\n\n\n![Figure 5: Balance between reconstruction quality and robustness against attacks during transmission. The results are averaged on all examples in CIFAR-10 test dataset.](images/9a5a470ac00ee922da0bfc9886a7df36417c8bd027da3cbb6792468b9c43da83.jpg)\n\n\nComparison with model-level adversarial training. Our method aims to enhance data-level adversarial robustness, and its motivation differs from traditional adversarial training approaches. As attacks on INRs are different from those on images, we apply PGD for INR in our method. Furthermore, to ensure a fair comparison, we use the predicted labels generated by the downstream classifier instead of the ground truth labels in the creation process of INR. As illustrated in Table 3, our method demonstrates better performance with natural data and stronger robustness compared with model-level adversarial training methods.\n\n\n# 5 CONCLUSION\n\n\nWe demonstrate adversarial data robustness in this paper by utilizing implicit neural representations. Instead of adversially training deep learning models, we focus on adversially training the data. Doing so ensures that the data retains robust performance even if it is subjected to attacks before it reaches the model users. To accomplish this, we initially represent the data implicitly and focus on adversarial training of this representation during its formation. By analyzing the potential attacks, we unveil an adversarial training scheme that bolsters the robustness of implicit data representations against potential adversarial attacks. In our proposed approach, ready-to-use image classifiers exhibit adversarial robustness on par with models that undergo model-level adversarial training.\n\n\nLimitations. Though our method is successful in achieving robustness at the data level, it cannot handle some extreme cases. For example, if malicious users gain access to the reconstructed images from the INR, they can directly apply adversarial perturbations to the image pixels, thereby circumventing attacks on the INR. This problem can be solved through system design by protecting the process from image reconstruction to downstream model input, to ensure that the recovered images are not accessible. Besides, we only consider risks caused by malicious technical designs. However, as security is not solely a technical problem, we still need to collaborate with various parties to consider the legal and social impact. We will further explore the feasibility of our approach in other data types and downstream tasks in the future.\n\n\n# ETHICS STATEMENT\n\n\nOur goal is to ensure the security of deep neural networks. We did not employ crowdsourcing and did not involve human subjects in our experiments. When utilizing existing assets such as code, data, or models, we have properly cited the original creators.\n\n\n# REFERENCES\n\n- Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-efficient black-box adversarial attack via random search. In European Conference on Computer Vision, 2020.\n\n- Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy, 2017.\n\n- Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\n\n- Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics Express, 2020.\n\n- Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vedit Goel, Zhangyang Wang, Humphrey Shi, and Xiaolong Wang. VideoINR: Learning video implicit neural representation for continuous space-time super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, JinFeng Yi, and Cho-Jui Hsieh. Query-efficient hard-label black-box attack: An optimization-based approach. In International Conference on Learning Representations, 2018.\n\n- Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, 2019.\n\n- Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International Conference on Machine Learning, 2020.\n\n- Junhao Dong, Seyed-Mohsen Moosavi-Dezfooli, Jianhuang Lai, and Xiaohua Xie. The enemy of my enemy is my friend: Exploring inverse adversaries for improving adversarial training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Emilien Dupont, Adam Golinski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. COin: Compression with implicit neural representations. In Neural Compression: From Information Theory to Applications ICLRW, 2021.\n\n- Joachim Folz, Sebastian Palacio, Joern Hees, and Andreas Dengel. Adversarial defense based on structure-to-signal autoencoders. In IEEE Winter Conference on Applications of Computer Vision, 2020.\n\n- Ruohan Gao, Zilin Si, Yen-Yu Chang, Samuel Clarke, Jeannette Bohg, Li Fei-Fei, Wenzhen Yuan, and Jiajun Wu. Objectfolder 2.0: A multisensory object dataset for sim2real transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In *Joshua Bengio and Yann LeCun* (eds.), International Conference on Learning Representations, 2015.\n\n- Chuan Guo, Jacob Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Weinberger. Simple black-box adversarial attacks. In International Conference on Machine Learning, 2019.\n\n- Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and Steven Hoi. From images to textual prompts: Zero-shot visual question answering with frozen large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, 2016.\n\n- Hanxun Huang, Yisen Wang, Sarah Erfani, Quanquan Gu, James Bailey, and Xingjun Ma. Exploring architectural ingredients of adversarially robust deep neural networks. Advances in Neural Information Processing Systems, 2021.\n\n- Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. LAS-AT: adversarial training with learnable attack strategy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Gaojie Jin, Xinping Yi, Wei Huang, Sven Schewe, and Xiaowei Huang. Enhancing adversarial training with second-order statistics of weights. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Gaojie Jin, Xinping Yi, Dengyu Wu, Ronghui Mu, and Xiaowei Huang. Randomized adversarial training via taylor expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 2021.\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), International Conference on Learning Representations, 2015.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n\n- Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against unseen threat models. In International Conference on Learning Representations, 2021.\n\n- Yanxi Li, Zhaohui Yang, Yunhe Wang, and Chang Xu. Neural architecture dilation for adversarial robustness. Advances in Neural Information Processing Systems, 2021.\n\n- Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against adversarial attacks using high-level representation guided denoiser. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\n- Renhao Liu, Yu Sun, Jiabei Zhu, Lei Tian, and Ulugbek S Kamilov. Recovery of continuous 3d refractive index maps from discrete intensity-only measurements using neural fields. Nature Machine Intelligence, 2022.\n\n- Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In International Conference on Learning Representations, 2016.\n\n- Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017.\n\n- Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\n\n- Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision, 2020.\n\n- Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In Proceedings of the NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.\n\n- Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and Privacy, 2016.\n\n- Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.\n\n- Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 2020.\n\n- Brandon Reyes, Amanda A Howard, Paris Perdikaris, and Alexandre M Tartakovsky. Learning unknown physics of non-newtonian fluids. Physical Review Fluids, 2021.\n\n- Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversially robust deep learning. In International Conference on Machine Learning, 2020.\n\n- Andrew Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.\n\n- Andrei A Rusu, Dan Andrei Calian, Sven Gowal, and Raia Hadsell. Hindering adversarial attacks with implicit neural representations. In International Conference on Machine Learning, 2022.\n\n- Hai Shu and Hongtu Zhu. Sensitivity analysis of deep neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019.\n\n- Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Advances in Neural Information Processing Systems, 2020.\n\n- Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 2021.\n\n- Yannick Str\u00fcmpler, Janis Postels, Ren Yang, Luc Van Gool, and Federico Tombari. Implicit neural representations for image compression. In European Conference on Computer Vision, 2022.\n\n- Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014.\n\n- Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Advances in Neural Information Processing Systems, 2020.\n\n- Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Wang Yifan, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Advances in neural rendering. In Computer Graphics Forum, 2022.\n\n- Xiaosen Wang and Kun He. Enhancing the transferability of adversarial attacks through variance tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\n\n- Zeming Wei, Yifei Wang, Yiwen Guo, and Yisen Wang. CFA: Class-wise calibrated fair adversarial training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British Machine Vision Conference 2016. British Machine Vision Association, 2016.\n\n- Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning, 2019.\n\n- Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In International Conference on Machine Learning, 2020.\n\n- Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli. Geometry-aware instance-reweighted adversarial training. In International Conference on Learning Representations, 2021.\n\n- Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\n- Dawei Zhou, Nannan Wang, Bo Han, and Tongliang Liu. Modeling adversarial noise for adversarial training. In International Conference on Machine Learning, 2022.\n\n- Hao Zhu, Zhen Liu, You Zhou, Zhan Ma, and Xun Cao. DNF: diffractive neural field for lensless microscopic imaging. Optics Express, 2022.\n\n\n![Figure 6: A visualization of our INR architecture.](images/a5371be97e66712ad169f513b693d85da4c8218a359d50b3728f8df172c743b3.jpg)\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638527.0348995", "msecs": "34.0", "relativeCreated": "123865.39697647095", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills\n\n# Abstract\nGeneralizable manipulation skills, which can be composed to tackle long-horizon and complex daily chores, are one of the cornerstones of Embodied AI. However, existing benchmarks, mostly composed of a suite of simulatable environments, are insufficient to push cutting-edge research works because they lack object-level topological and geometric variations, are not based on fully dynamic simulation, or are short of native support for multiple types of manipulation tasks. To this end, we present ManiSkill2, the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. ManiSkill2 includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D-input data simulated by fully dynamic engines. It defines a unified interface and evaluation protocol to support a wide range of algorithms (e.g., classic sense-plan-act, RL, IL), visual observations (point cloud, RGBD), and controllers (e.g., action type and parameterization). Moreover, it empowers fast visual input learning algorithms so that a CNN-based policy can collect samples at about 2000 FPS with 1 GPU and 16 processes on a regular workstation. It implements a render server infrastructure to allow sharing rendering resources across all environments, thereby significantly reducing memory usage. We open-source all codes of our benchmark (simulator, environments, and baselines) and host an online challenge open to interdisciplinary researchers.\n\n\n![Figure 1: ManiSkill2 provides a unified, fast, and accessible system that encompasses well-curated manipulation tasks (e.g., stationary/mobile-base, single/dual-arm, rigid/soft-body).](images/f64f737450aaa31ead3f8bb5f4d37ef8fcb587e2b568293b26a918d26499249a.jpg)\n\n\n# 1 INTRODUCTION\n\n\nMastering human-like manipulation skills is a fundamental but challenging problem in Embodied AI, which is at the nexus of vision, learning, and robotics. Remarkably, once humans have learnt to manipulate a category of objects, they are able to manipulate unseen objects (e.g., with different\n\n\nappearances and geometries) of the same category in unseen configurations (e.g., initial poses). We refer such abilities to interact with a great variety of even unseen objects in different configurations as generalizable manipulation skills. Generalizable manipulation skills are one of the cornerstones of Embodied AI, which can be composed to tackle long-horizon and complex daily chores (Ahn et al., 2022; Gu et al., 2022). To foster further interdisciplinary and reproducible research on generalizable manipulation skills, it is crucial to build a versatile and public benchmark that focuses on object-level topological and geometric variations as well as practical manipulation challenges.\n\n\nHowever, most prior benchmarks are insufficient to support and evaluate progress in learning generalizable manipulation skills. In this work, we present ManiSkill2, the next generation of SAPIEN ManiSkill Benchmark (Mu et al., 2021), which extends upon fully simulated dynamics, a large variety of articulated objects, and large-scale demonstrations from the previous version. Moreover, we introduce significant improvements and novel functionalities, as shown below.\n\n\n1) A Unified Benchmark for Generic and Generalizable Manipulation Skills: There does not exist a standard benchmark to measure different algorithms for generic and generalizable manipulation skills. It is largely due to well-known challenges to build realistically simulated environments with diverse assets. Many benchmarks bypass critical challenges as a trade-off, by either adopting abstract grasp (Ehsani et al., 2021; Szot et al., 2021; Srivastava et al., 2022) or including few object-level variations (Zhu et al., 2020; Yu et al., 2020; James et al., 2020). Thus, researchers usually have to make extra efforts to customize environments due to limited functionalities, which in turn makes reproducible comparison difficult. For example, Shridhar et al. (2022) modified 18 tasks in James et al. (2020) to enable few variations in initial states. Besides, some benchmarks are biased towards a single type of manipulation, e.g., 4-DoF manipulation in Yu et al. (2020). To address such pain points, ManiSkill2 includes a total of 20 verified and challenging manipulation task families of multiple types (stationary/mobile-base, single/dual-arm, rigid/soft-body) with over 2000 objects and 4M demonstration frames, to support generic and generalizable manipulation skills. All the tasks are implemented in a unified OpenAI Gym (Brockman et al., 2016) interface with fully-simulated dynamic interaction, supporting multiple observation modes (point cloud, RGBD, privileged state) and multiple controllers. A unified protocol is defined to evaluate a wide range of algorithms (e.g., sense-plan-act, reinforcement and imitation learning) on both seen and unseen assets as well as configurations. In particular, we implement a cloud-based evaluation system to publicly and fairly compare different approaches.\n\n\n2) Real-time Soft-body Environments: When operating in the real world, robots face not only rigid bodies, but many types of soft bodies, such as cloth, water, and soil. Many simulators have supported robotic manipulation with soft body simulation. For example, MuJoCo (Todorov et al., 2012) and Bullet Coumans & Bai (2016-2021) use the finite element method (FEM) to enable the simulation of rope, cloth, and elastic objects. However, FEM-based methods cannot handle large deformation and topological changes, such as scooping flour or cutting dough. Other environments, like SoftGym (Lin et al., 2020) and ThreeDWorld (Gan et al., 2020), are based on Nvidia Flex, which can simulate large deformations, but cannot realistically simulate elasto-plastic material, e.g., clay. PlasticineLab (Huang et al., 2021) deploys the continuum-mechanics-based material point method (MPM), but it lacks the ability to couple with rigid robots, and its simulation and rendering performance have much room for improvement. We have implemented a custom GPU MPM simulator from scratch using Nvidia's Warp (Macklin, 2022) JIT framework and native CUDA for high efficiency and customizability. We further extend Warp's functionality to support more efficient host-device communication. Moreover, we have supported a 2-way dynamics coupling interface that enables any rigid-body simulation framework to interact with the soft bodies, allowing robots and assets in ManiSkill2 to interact with soft-body simulation seamlessly. To our knowledge, ManiSkill2 is the first embodied AI environment to support 2-way coupled rigid-MPM simulation, and also the first to support real-time simulation and rendering of MPM material.\n\n\n3) Multi-controller Support and Conversion of Demonstration Action Spaces: Controllers transform policies' action outputs into motor commands that actuate the robot, which define the action space of a task. Martin-Mart\u00edn et al. (2019); Zhu et al. (2020) show that the choice of action space has considerable effects on exploration, robustness and sim2real transferability of RL policies. For example, task-space controllers are widely used for typical pick-and-place tasks, but might be suboptimal compared to joint-space controllers when collision avoidance (Szot et al., 2021) is required. ManiSkill2 supports a wide variety of controllers, e.g., joint-space controllers for motion planning and task-space controllers for teleoperation. A flexible system is also implemented to\n\n\ncombine different controllers for different robot components. For instance, it is easy to specify a velocity controller for the base, a task-space position controller for the arm, and a joint-space position controller for the gripper. It differs from Zhu et al. (2020), which only supports setting a holistic controller for all the components. Most importantly, ManiSkill2 embraces a unique functionality to convert the action space of demonstrations to a desired one. It enables us to exploit large-scale demonstrations generated by any approach regardless of controllers.\n\n\n4) Fast Visual RL Experiment Support: Visual RL training demands millions of samples from interaction, which makes performance optimization an important aspect in environment design. Isaac Gym (Makoviychuk et al., 2021) implements a fully GPU-based vectorized simulator, but it lacks an efficient renderer. It also suffers from reduced usability (e.g., difficult to add diverse assets) and functionality (e.g., object contacts are inaccessible). EnvPool (Weng et al., 2022) batches environments by a thread pool to minimize synchronization and improve CPU utilization. Yet its environments need to be implemented in  $\\mathbf{C} + +$  , which hinders fast prototyping (e.g., customizing observations and rewards). As a good trade-off between efficiency and customizability, our environments are fully scripted in Python and vectorized by multiple processes. We implement an asynchronous RPC-based render server-client system to optimize throughput and reduce GPU memory usage. We manage to collect samples with an RGBD-input PPO policy at about 2000 FPS  $^1$  with 1 GPU and 16 CPU processors.\n\n\n# 2 BUILDING ENVIRONMENTS FOR GENERALIZABLE MANIPULATION SKILLS\n\n\nBuilding high-quality environments demands cross-disciplinary knowledge and expertise, including physical simulation, rendering, robotics, machine learning, software engineering, etc. Our workflow highlights a verification-driven iterative development process, which is illustrated in Appendix A.1. Different approaches, including task and motion planning (TAMP), model predictive control (MPC) and reinforcement learning (RL), can be used to generate demonstrations according to characteristics and difficulty of tasks, which verify environments as a byproduct.\n\n\n# 2.1 HETEROGENEOUS TASK FAMILIES\n\n\nManiSkill2 embraces a heterogeneous collection of 20 task families. A task family represents a family of task variants that share the same objective but are associated with different assets and initial states. For simplicity, we interchangeably use task short for task family. Distinct types of manipulation tasks are covered: rigid/soft-body, stationary/mobile-base, single/dual-arm. In this section, we briefly describe 4 groups of tasks. More details can be found in Appendix C.\n\n\nSoft-body Manipulation: ManiSkill2 implements 6 soft-body manipulation tasks that require agents to move or deform soft bodies into specified goal states through interaction.\n\n- 1) Fill: filling clay from a bucket into the target beaker;\n\n- 2) Hang: hanging a noodle on the target rod;\n\n- 3) Excavate: scooping up a specific amount of clay and lifting it to a target height;\n\n- 4) Pour: pouring water from a bottle into the target beaker. The final liquid level should match the red line on the beaker.\n\n- 5) Pinch: deforming plasticine from an initial shape into a target shape; target shapes are generated by randomly pinching initial shapes, and are given as RGBD images or point clouds from 4 views.\n\n- 6) Write: writing a target character on clay. Target characters are given as 2D depth maps.\n\n\nA key challenge of these tasks is to reason how actions influence soft bodies, e.g., estimating displacement quantity or deformations, which will be illustrated in Sec 5.2.\n\n\nPrecise Peg-in-hole Assembly: Peg-in-hole assembly is a representative robotic task involving rich contact. We include a curriculum of peg-in-hole assembly tasks that require an agent to place an object into its corresponding slot. Compared to other existing assembly tasks, ours come with two noticeable improvements. First, our tasks target high precision (small clearance) at the level of millimeters, as most day-to-day assembly tasks demand. Second, our tasks emphasize the contact-rich insertion process, instead of solely measuring position or rotation errors.\n\n\n1) PegInsertionSide: a single peg-in-hole assembly task inspired by MetaWorld (Yu et al., 2020). It involves an agent picking up a cuboid-shaped peg and inserting it into a hole with a clearance of\n\n\n3mm on the box. Our task is successful only if half of the peg is inserted, while the counterparts in prior works only require the peg head to approach the surface of the hole.\n\n- 2) PlugCharger: a dual peg-in-hole assembly task inspired by RLBench (James et al., 2020), which involves an agent picking up and plugging a charger into a vertical receptacle. Our assets (the charger and holes on the receptacle) are modeled with realistic sizes, allowing a clearance of  $0.5\\mathrm{mm}$ , while the counterparts in prior works only examine the proximity of the charger to a predefined position without modeling holes at all.\n\n- 3) AssemblingKits: inspired by Transporter Networks (Zeng et al., 2020), this task involves an agent picking up and inserting a shape into its corresponding slot on a board with 5 slots in total. We devise a programmatic way to carve slots on boards given any specified clearance (e.g.,  $0.8\\mathrm{mm}$ ), such that we can generate any number of physically-realistic boards with slots. Note that slots in environments of prior works are visual marks, and there are in fact no holes on boards.\n\n\nWe generate demonstrations for the above tasks through TAMP. This demonstrates that it is feasible to build precise peg-in-hole assembly tasks solvable in simulation environments, without abstractions from prior works.\n\n\nStationary 6-DoF Pick-and-place: 6-DoF pick-and-place is a widely studied topic in robotics. At the core is grasp pose Mahler et al. (2016); Qin et al. (2020); Sundermeyer et al. (2021). In ManiSkill2, we provide a curriculum of pick-and-place tasks, which all require an agent to pick up an object and move it to a goal specified as a 3D position. The diverse topology and geometric variations among objects call for generalizable grasp pose predictions.\n\n- 1) PickCube: picking up a cube and placing it at a specified goal position;\n\n- 2) StackCube: picking up a cube and placing it on top of another cube. Unlike PickCube, the goal placing position is not explicitly given; instead, it needs to be inferred from observations.\n\n- 3) PickSingleYCB: picking and placing an object from YCB (Calli et al., 2015);\n\n- 4) PickSingleEGAD: picking and placing an object from EGAD (Morrison et al., 2020);\n\n- 5) PickClutterYCB: The task is similar to PickSingleYCB, but multiple objects are present in a single scene. The target object is specified by a visible 3D point on its surface.\n\n\nOur pick-and-place tasks are deliberately designed to be challenging. For example, the goal position is randomly selected within a large workspace  $(30 \\times 50 \\times 50 cm^3)$ . It poses challenges to sense-plan-act pipelines that do not take kinematic constraints into consideration when scoring grasp poses, as certain high-quality grasp poses might not be achievable by the robot.\n\n\nMobile/Stationary Manipulation of Articulated Objects: We inherit four mobile manipulation tasks from ManiSkill1 (Mu et al., 2021), which are PushChair, MoveBucket, OpenCabinetDoor, and OpenCabinetDrawer. We also add a stationary manipulation task, TurnFaucet, which uses a stationary arm to turn on faucets of various geometries and topology (details in Appendix C.3).\n\n\nBesides the above tasks, we have one last task, AvoidObstacles, which tests the navigation ability of a stationary arm to avoid a dense collection of obstacles in space while actively sensing the scene.\n\n\n# 2.2 MULTI-CONTROLLER SUPPORT AND CONVERSION OF DEMONSTRATION ACTION SPACES\n\n\nThe selection of controllers determines the action space. ManiSkill2 supports multiple controllers, e.g., joint position, delta joint position, delta end-effector pose, etc. Unless otherwise specified, controllers in ManiSkill2 translate input actions, which are desired configurations (e.g., joint positions or end-effector pose), to joint torques that drive corresponding joint motors to achieve desired actions. For instance, input actions to joint position, delta joint position, and delta end-effector pose controllers are, respectively, desired absolute joint positions, desired joint positions relative to current joint positions, and desired  $\\mathbb{S}\\mathbb{E}(3)$  transformations relative to the current end-effector pose. See Appendix B for a full description of all supported controllers.\n\n\nDemonstrations are generated with one controller, with an associated action space. However, researchers may select an action space that conforms to a task but is different from the original one. Thus, to exploit large-scale demonstrations, it is crucial to convert the original action space to many different target action spaces while reproducing the kinematic and dynamic processes in demonstrations. Let us consider a pair of environments: a source environment with a joint position controller used to generate demonstrations through TAMP, and a target environment with a delta end-effector pose controller for Imitation / Reinforcement Learning applications. The objective is to convert the source action  $a_{\\mathrm{src}}(t)$  at each timestep  $t$  to the target action  $a_{\\mathrm{tgt}}(t)$ . By definition, the\n\n\ntarget action (delta end-effector pose) is  $a_{\\mathrm{tgt}}(t) = \\bar{T}_{\\mathrm{tgt}}(t) \\cdot T_{\\mathrm{tgt}}^{-1}(t)$ , where  $\\bar{T}_{\\mathrm{tgt}}(t)$  and  $T_{\\mathrm{tgt}}(t)$  are respectively desired and current end-effector poses in the target environment. To achieve the same dynamic process in the source environment, we need to match  $\\bar{T}_{\\mathrm{tgt}}(t)$  with  $\\bar{T}_{\\mathrm{src}}(t)$ , where  $\\bar{T}_{\\mathrm{src}}(t)$  is the desired end-effector pose in the source environment.  $\\bar{T}_{\\mathrm{src}}(t)$  can be computed from the desired joint positions  $(a_{\\mathrm{src}}(t)$  in this example) through forward kinematics  $(FK(\\cdot))$ . Thus, we have\n\n\n$$\n\n$$\na _ {\\mathrm {t g t}} (t) = \\bar {T} _ {\\mathrm {t g t}} (t) \\cdot T _ {\\mathrm {t g t}} ^ {- 1} (t) = \\bar {T} _ {\\mathrm {s r c}} (t) \\cdot T _ {\\mathrm {t g t}} ^ {- 1} (t) = F K (\\bar {a} _ {\\mathrm {s r c}} (t)) \\cdot T _ {\\mathrm {t g t}} ^ {- 1} (t)\n$$\n\n$$\n\n\nNote that our method is closed-loop, as we instantiate a target environment to acquire  $T_{\\mathrm{tgt}}^{-1}(t)$ . For comparison, an open-loop method would use  $T_{\\mathrm{src}}^{-1}(t)$  and suffers from accumulated execution errors.\n\n\n# 3 REAL-TIME SOFT BODY SIMULATION AND-RENDERING\n\n\nIn this section, we describe our new physical simulator for soft bodies and their dynamic interactions with the existing rigid-body simulation. Our key contributions include: 1) a highly efficient GPU MPM simulator; 2) an effective 2-way dynamic coupling method to support interactions between our soft-body simulator and any rigid-body simulator, in our case, SAPIEN. These features enable us to create the first real-time MPM-based soft-body manipulation environment with 2-way coupling.\n\n\nRigid-soft Coupling: Our MPM solver is MLS-MPM (Hu et al., 2018) similar to PlasticineLab (Huang et al., 2021), but with a different contact modeling approach, which enables 2-way coupling with external rigid-body simulators. The coupling works by transferring rigid-body poses to the soft-body simulator and transferring soft-body forces to the rigid-body simulator. At the beginning of the simulation, all collision shapes in the external rigid-body simulator are copied to the soft-body simulator. Primitive shapes (box, capsule, etc.) are represented by analytical signed distance functions (SDFs); meshes are converted to SDF volumes, stored as 3D CUDA textures. After each rigid-body simulation step, we copy the poses and velocities of the rigid bodies to the soft-body simulator. During soft-body simulation steps, we evaluate the SDF functions at MPM particle positions and apply penalty forces to the particles, accumulating forces and torques for rigid bodies. In contrast, PlasticineLab evaluates SDF functions on MPM grid nodes, and applies forces to MPM grids. Our simulator supports both methods, but we find applying particle forces produces fewer artifacts such as penetration. After soft-body steps, we read the accumulated forces and torques from the soft-body simulator and apply them to the external rigid-body simulator. This procedure is summarized in Appendix D.1. Despite being a simple 2-way coupling method, it is very flexible, allowing coupling with any rigid-body simulator, so we can introduce anything from a rigid-body simulator into the soft-body environment, including robots and controllers.\n\n\nPerformance Optimization: The performance of our soft-body simulator is optimized in 4 aspects. First, the simulator is implemented in Warp, Nvidia's JIT framework to translate Python code to native C++ and CUDA. Therefore, our simulator enjoys performance comparable to C and CUDA. Second, we have optimized the data transfer (e.g., poses and forces in the 2-way coupling) between CPU and GPU by further extending and optimizing the Warp framework; such data transfers are performance bottlenecks in other JIT frameworks such as Taichi (Hu et al., 2019), on which PlasticineLab is based. Third, our environments have much shorter compilation time and startup time compared to PlasticineLab thanks to the proper use of Warp compiler and caching mechanisms. Finally, since the simulator is designed for visual learning environments, we also implement a fast surface rendering algorithm for MPM particles, detailed in Appendix D.2. Detailed simulation parameters and performance are provided in Appendix D.3.\n\n\n# 4 PARALLELIZING PHYSICAL SIMULATION AND-RENDERING\n\n\nManiSkill2 aims to be a general and user-friendly framework, with low barriers for customization and minimal limitations. Therefore, we choose Python as our scripting language to model environments, and the open-source, highly flexible SAPIEN (Xiang et al., 2020) as our physical simulator. The Python language determines that the only way to effectively parallelize execution is through multi-process, which is integrated in common visual RL training libraries (Raffin et al., 2021; Weng et al., 2021). Under the multi-process environment paradigm, we make engineering efforts to enable our environments to surpass previous frameworks by increased throughput and reduced GPU memory usage.\n\n\n![(a)](images/dd6b2d38ca309fab924cf0752a642f11c3e3dd8df65022c201d0a98c2fd645d7.jpg)\n\n\n![(b) Figure 2: Two pipelines for visual RL sample collection. (a) Sequential pipeline. (b) Our pipeline with asynchronous rendering and render server improves CPU utilization, reduces data transfer, and saves memory.](images/44d55175b2f6f501e6d464ca19b815b37484af073610b2567c71bf5358ebb9b3.jpg)\n\n\nWhat are the Goals in Visual-Learning Environment Optimization? The main goal of performance optimization in a visual-learning environment is to maximize total throughput: the number of samples collected from all (parallel) environments, measured in steps (frames) per second. Another goal is to reduce GPU memory usage. Typical Python-based multi-process environments are wasteful in GPU memory usage, since each process has to maintain a full copy of GPU resources, e.g., meshes and textures. Given a fixed GPU memory budget, less GPU memory spent on rendering resources means more memory can be allocated for neural networks.\n\n\nAsynchronous Rendering and Server-based Renderer: Fig. 2(a) illustrates a typical pipeline (sequential simulation and rendering) to collect samples from multi-process environments. It includes the following stages: (1) do physical simulation on worker processes; (2) take pictures (update renderer GPU states and submit draw calls); (3) wait for GPU render to finish; (4) copy image observations to CPU; (5) compute rewards and get other observations (e.g., robot proprioceptive info); (6) copy images to the main python process and synchronize; (7) copy these images to GPU for policy learning; (8) forward the policy network on GPU; (9) copy output actions from the GPU to the simulation worker processes.\n\n\nWe observe that the CPU is idling during GPU rendering (stage 3), while reward computation (stage 5) often does not rely on rendering results. Thus, we can increase CPU utilization by starting reward computation immediately after stage 2. We refer to this technique as asynchronous rendering.\n\n\nAnother observation is that images are copied from GPU to CPU on each process, passed to the main python process, and uploaded to the GPU again. It would be ideal if we can keep the data on GPU at all times. Our solution is to use a render server, which starts a thread pool on the main Python process and executes rendering requests from simulation worker processes, summarized in figure 2(b). The render server eliminates GPU-CPU-GPU image copies, thereby reducing data transfer time. It allows GPU resources to share across any number of environments, thereby significantly reducing memory usage. It enables communication over network, thereby having the potential to simulate and render on multiple machines. It requires minimal API changes \u2013 the only change to the original code base is to receive images from the render server. It also has additional benefits in software development with Nvidia GPU, which we detail in Appendix E.2.\n\n\nComparison: We compare the throughput of ManiSkill2 with 3 other frameworks that support visual RL: Habitat 2.0 (Szot et al., 2021), RoboSuite 1.3 (Zhu et al., 2020), and Isaac Gym (Makoviychuk et al., 2021). We build a PickCube environment in all simulators. We use similar physical simulation parameters (500Hz simulation frequency and 20Hz control frequency\u00b2), and we use the GPU pipeline for rendering. Images have resolution 128x128. All frameworks are given a computation budget of 16 CPU cores (logical processors) of an Intel i9-9960X CPU with 128G memory and 1 RTX Titan GPU with 24G memory. We test with random actions and also with a randomly initialized nature CNN (Mnih et al., 2015) as policy network. We test all frameworks on 16, 32, 64, 128, 256, 512 parallel environments and report the highest performance. Results are shown in Table 1(a) (more\n\n\n<table><tr><td></td><td>ManiSkill2 Server</td><td>ManiSkill2 Sync</td><td>Habitat 2.0</td><td>RoboSuite 1.3</td><td>Isaac Gym</td></tr><tr><td>Total FPS (rand. action)</td><td>2344\u00b168</td><td>942\u00b119</td><td>1275\u00b110</td><td>924\u00b13</td><td>865\u00b135</td></tr><tr><td>Total FPS (nature CNN)</td><td>2181\u00b175</td><td>931\u00b14</td><td>1224\u00b113</td><td>894\u00b115</td><td>835\u00b15</td></tr><tr><td>Optimal #Envs</td><td>64</td><td>32</td><td>64</td><td>32</td><td>512</td></tr></table>\n\n\n(a)\n\n\n<table><tr><td>#Envs</td><td>ManiSkill2 Server</td><td>Habitat 2.0</td></tr><tr><td>4</td><td>4.9G</td><td>6.4G</td></tr><tr><td>8</td><td>5.1G</td><td>12.9G</td></tr><tr><td>64</td><td>5.8G</td><td>(OOM)</td></tr></table>\n\n\n(b)\n\n\nTable 1: (a) Comparison of sample collection speed (FPS) on PickCube across different frameworks. (b) GPU memory usage on multi-process environments with 74 YCB objects each.\n\n\ndetails in Appendix E.3). An interesting observation is that our environment performs the best when the number of parallel environments exceeds the number of CPU cores. We conjecture that this is because execution on CPU and GPU are naturally efficiently interleaved through OS or driver-level schedulers when the requested computation far exceeds the available resources.\n\n\nAdditionally, we demonstrate the advantage of memory sharing thanks to our render server. We extend the PickClutterYCB environment to include 74 YCB objects per scene, and create the same setting in Habitat 2.0. As shown in Table 1(b), even though we enable GPU texture compression for Habitat and use regular textures for ManiSkill2, the memory usage of Habitat grows linearly as the number of environments increases, while ManiSkill2 requires very little memory to create additional environments since all meshes and textures are shared across environments.\n\n\n# 5 APPLICATIONS\n\n\nIn this section, we show how ManiSkill2 supports widespread applications, including sense-plan-act frameworks and imitation / reinforcement learning. In this section, for tasks that have asset variations, we report results on training objects. Results on held-out objects are presented in Appendix F. Besides, we demonstrate that policies trained in ManiSkill2 have the potential to be directly deployed in the real world.\n\n\n# 5.1 SENSE-PLAN-ACT\n\n\nSense-plan-act (SPA) is a classical framework in robotics. Typically, a SPA method first leverages perception algorithms to build a world model from observations, then plans a sequence of actions through motion planning, and finally execute the plan. However, SPA methods are usually open-loop and limited by motion planning, since perception and planning are independently optimized. In this section, we experiment with two perception algorithms, Contact-GraspNet (Sundermeyer et al., 2021) and Transporter Networks (Zeng et al., 2020).\n\n\nContact-GraspNet for PickSingleYCB: The SPA solution for PickSingleYCB is implemented as follows. First, we use Contact-GraspNet (CGN) to predict potential grasp poses along with confidence scores given the observed partial point cloud. We use the released CGN model pretrained on ACRONYM (Eppner et al., 2021). Next, we start with the predicted grasp pose with the highest score, and try to generate a plan to achieve it by motion planning. If no plan is found, the grasp pose with the next highest score is attempted until a valid plan is found. Then, the agent executes the plan to reach the desired grasp pose and closes its grippers to grasp the object. Finally, another plan is generated and executed to move the gripper to the goal position.\n\n\nFor each of the 74 objects, we conduct 5 trials (different initial states). The task succeeds if the object's center of mass is within  $2.5\\mathrm{cm}$  of the goal. The success rate is  $43.24\\%$ . There are two main failure modes: 1) predicted poses are of low confidence ( $27.03\\%$  of trials), especially for objects (e.g., spoon and fork) that are small and thin; 2) predicted poses are of low grasp quality or unreachable ( $29.73\\%$  of trials), but with high confidence. See Appendix F.1 for examples.\n\n\nTransporter Network for AssemblingKits: We benchmark Transporter Networks (TPN) on our AssemblingKits. The original success metric (pose accuracy) used in TPN is whether the peg is placed within 1cm and 15 degrees of the goal pose. Note that our version requires pieces to actually fit into holes, and thus our success metric is much stricter. We train TPN from scratch with image data sampled from training configurations using two cameras, a base camera and a hand camera. To\n\n\n<table><tr><td>Obs. Mode</td><td>PickCube</td><td>StackCube</td><td>Fill</td><td>Hang</td><td>Excavate</td><td>Pour</td><td>Pinch</td><td>Write</td></tr><tr><td>Point Cloud</td><td>0.22 \u00b1 0.06</td><td>0.04 \u00b1 0.02</td><td>0.45 \u00b1 0.04</td><td>0.35 \u00b1 0.15</td><td>0.08 \u00b1 0.04</td><td>0.02 \u00b1 0.02</td><td>0.00 \u00b1 0.00</td><td>0.00 \u00b1 0.00</td></tr><tr><td>RGBD</td><td>0.01 \u00b1 0.02</td><td>0.00 \u00b1 0.00</td><td>0.62 \u00b1 0.07</td><td>0.20 \u00b1 0.12</td><td>0.21 \u00b1 0.04</td><td>0.00 \u00b1 0.00</td><td>0.00 \u00b1 0.00</td><td>0.00 \u00b1 0.00</td></tr></table>\n\n\nTable 2: Mean and standard deviation of the success rate of behavior cloning on rigid-body and soft-body tasks. For rigid-body assembly tasks not shown in the table, success rate is 0.\n\n\n<table><tr><td>Obs. Mode</td><td>PickCube</td><td>StackCube</td><td>PickSingleYCB</td><td>PegInsSide</td><td>PlugCharger</td><td>AssemblingKits</td><td>TurnFaucet</td><td>AvoidObstacles</td></tr><tr><td>Point Cloud</td><td>0.94 \u00b1 0.03</td><td>0.95 \u00b1 0.02</td><td>0.51 \u00b1 0.05</td><td>0.01 \u00b1 0.01</td><td>0.01 \u00b1 0.02</td><td>0.00 \u00b1 0.00</td><td>0.04 \u00b1 0.03</td><td>0.00 \u00b1 0.00</td></tr><tr><td>RGBD</td><td>0.91 \u00b1 0.05</td><td>0.87 \u00b1 0.04</td><td>0.18 \u00b1 0.07</td><td>0.01 \u00b1 0.01</td><td>0.01 \u00b1 0.01</td><td>0.00 \u00b1 0.00</td><td>0.03 \u00b1 0.03</td><td>0.00 \u00b1 0.00</td></tr></table>\n\n\nTable 3: Mean and standard deviation of success rates of DAPG+PPO on rigid-body tasks. Training budget is 25M time steps.\n\n\naddress our high-precision success criterion, we increase the number of bins for rotation prediction from 36 in the original work to 144. During evaluation, we employ motion planning to move the gripper to the predicted pick position, grasp the peg, then generate another plan to move the peg to the predicted goal pose and drop it into the hole. The success rate over 100 trials is  $18\\%$ , and the pose accuracy is  $96\\%$ . See Appendix F.2 for more details.\n\n\n# 5.2 IMITATION & REINFORCEMENT LEARNING WITH DEMONSTRATIONS\n\n\nFor the following experiments, unless specified otherwise, we use the delta end-effector pose controller for rigid-body environments and the delta joint position controller for soft-body environments, and we translate demonstrations accordingly using the approach in Sec.2.2. Visual observations are captured from a base camera and a hand camera. For RGBD-based agents, we use IMPALA (Espeholt et al., 2018) as the visual backbone. For point cloud-based agents, we use PointNet (Qi et al., 2017) as the visual backbone. In addition, we transform input point clouds into the end-effector frame, and for environments where goal positions are given (PickCube and PickSingleYCB), we randomly sample 50 green points around the goal position to serve as visual cues and concatenate them to the input point cloud. We run 5 trials for each experiment and report the mean and standard deviation of success rates. Further details are presented in Appendix F.3.\n\n\nImitation Learning: We benchmark imitation learning (IL) with behavior cloning (BC) on our rigid and soft-body tasks. All models are trained for 50k gradient steps with batch size 256 and evaluated on test configurations.\n\n\nResults are shown in Table 2. For rigid-body tasks, we observe low or zero success rates. This suggests that BC is insufficient to tackle many crucial challenges from our benchmark, such as precise control in assembly and large asset variations. For soft-body tasks, we observe that it is difficult for BC agents to precisely estimate action influence on soft body properties (e.g. displacement quantity or deformation). Specifically, agents perform poorly on Excavate and Pour, as Excavate is successful only if a specified amount of clay is scooped up and Pour is successful when the final liquid level accurately matches a target line. On the other hand, for Fill and Hang, which do not have such precision requirements (for Fill, the task is successful as long as the amount of particles in the beaker is larger than a threshold), the success rates are much higher. In addition, we observe that BC agents cannot well utilize target shapes to guide precise soft body deformation, as they are never successful on Pinch or Write. See Appendix F.5 for further analysis.\n\n\nRL with Demonstrations: We benchmark demonstration-based online reinforcement learning by augmenting Proximal Policy Gradient (PPO) (Schulman et al., 2017) objective with the demonstration objective from Demonstration-Augmented Policy Gradient (DAPG) (Rajeswaran et al., 2017). Our implementation is similar to Jia et al. (2022), and further details are presented in Appendix F.3. We train all agents from scratch for 25 million time steps. Due to limited computation resources, we only report results on rigid-body environments.\n\n\nResults are shown in Table 3. We observe that for pick-and-place tasks, point cloud-based agents perform significantly better than RGBD-based agents. Notably, on PickSingleYCB, the success rate is even higher than Contact-GraspNet with motion planning. This demonstrates the potential\n\n\n![Simulation Figure 3: Left: Simulation and real world setup for PickCube. Right: Motion planning execution results for Pinch in simulation and in the real world: (a) initial state; (b) letter \"M\"; (c) letter \"S\".](images/7a07dbb345a60a7cd55c39228a734b046eb1a726de9e8a856e922795455ad67a.jpg)\n\n\n![Real](images/c7c36b92349b7fcb1c50eb60066566380aa6891289bd19adc17eb6f2ce3c2d47.jpg)\n\n\n![(a)](images/0ee92fed74707b8816abf16c98a113ab2867391fc389cc3e1ce569d446117c87.jpg)\n\n\n![(b)](images/e0101b5681f62e463b89a22041706d65691901961d9fdbe112aa9b70c4d0531e.jpg)\n\n\n![(c)](images/957abea9fabebd9f7d8d612939e6f85b9cd05a8ed7386dc3b7ffb129e88ebbce.jpg)\n\n\nof obtaining a single agent capable of performing general pick-and-place across diverse object geometries through online training. We also further examine factors that influence point cloud-based manipulation learning in Appendix F.6. However, for all other tasks, notably the assembly tasks that require high precision, the success rates are near zero. In Appendix F.7, we show that if we increase the clearance of the assembly tasks and decrease their difficulty, agents can achieve much higher performance. This suggests that existing RL algorithms might have been insufficient yet to perform highly precise controls, and our benchmark poses meaningful challenges for the community.\n\n\nIn addition, we examine the influence of controllers for agent learning, and we perform ablation experiments on PickSingleYCB using point cloud-based agents. When we replace the delta end-effector pose controller in the above experiments with the delta joint position controller, the success rate falls to  $0.22 \\pm 0.18$ . The profound impact of controllers demonstrates the necessity and significance of our multi-controller conversion system.\n\n\n# 5.3 SIM2REAL\n\n\nManiSkill2 features fully-simulated dynamic interaction for rigid-body and soft-body. Thus, policies trained in ManiSkill2 have the potential to be directly deployed in the real world.\n\n\nPickCube: We train a visual RL policy on PickCube and evaluate its transferability to the real world. The setup is shown in Fig. 3-L, which consists of an Intel RealSense D415 depth sensor, a 7DoF ROKAE xMate3Pro robot arm, a Robotiq 2F-140 2-finger gripper, and the target cube. We first acquire the intrinsic parameters for the real depth sensor and its pose relative to the robot base. We then build a simulation environment aligned with the real environment. We train a point cloud-based policy for 10M time steps, where the policy input consists of robot proprioceptive information (joint position and end-effector pose) and uncolored point cloud backprojected from the depth map. The success rate in simulation is  $91.0\\%$ . We finally directly evaluate this policy in the real world 50 times with different initial states, where we obtain  $60.0\\%$  success rate. We conjecture that the performance drop is caused by the domain gap in depth maps, as we only used Gaussian noise and random pixel dropout as data augmentation during simulation policy training.\n\n\nPinch: We further evaluate the fidelity of our soft-body simulation by executing the same action sequence generated by motion planning in simulation and in the real world and comparing the final plasticine shapes. Results are shown in Fig. 3-R, which demonstrates that our 2-way coupled rigid-MPM simulation is able to reasonably simulate plasticine's deformation caused by multiple grasps.\n\n\n# 6 CONCLUSION\n\n\nTo summarize, ManiSkill2 is a unified and accessible benchmark for generic and generalizable manipulation skills, providing 20 manipulation task families, over 2000 objects, and over 4M demonstration frames. It features highly efficient rigid-body simulation and rendering system for sample collection to train RL agents, real-time MPM-based soft-body environments, and versatile multi-controller conversion support. We have demonstrated its applications in benchmarking sense-plan-act and imitation / reinforcement learning algorithms, and we show that learned policies on ManiSkill2 have the potential to transfer to the real world.\n\n\n# REFERENCES\n\n- Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.\n\n- Kent Beck, Mike Beedle, Arie Van Bennekum, Alistair Cockburn, Ward Cunningham, Martin Fowler, James Grenning, Jim Highsmith, Andrew Hunt, Ron Jeffries, et al. Manifesto for agile software development, 2001.\n\n- Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n\n- Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 international conference on advanced robotics (ICAR), pp. 510-517. IEEE, 2015.\n\n- Hilko Cords and Oliver G. Staadt. Instant liquids: Fast screen-space surface generation for particle-based liquids. In Poster proceedings of ACM Siggraph/Eurographics Symposium on Computer Animation, 2008.\n\n- Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016-2021.\n\n- Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: A framework for visual object manipulation. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4495-4504, 2021.\n\n- Clemens Eppner, Arsalan Mousavian, and Dieter Fox. Acronym: A large-scale grasp dataset based on simulation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 6222-6227. IEEE, 2021.\n\n- Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pp. 1407-1416. PMLR, 2018.\n\n- Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, et al. Threadworld: A platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020.\n\n- Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel LK Yamins, James J DiCarlo, Josh McDermott, Antonio Torralba, et al. The threadworld transport challenge: A visually guided task-and-motion planning benchmark for physically realistic embodied ai. arXiv preprint arXiv:2103.14025, 2021.\n\n- Jiayuan Gu, Devendra Singh Chaplot, Hao Su, and Jitendra Malik. Multi-skill mobile manipulation for object rearrangement. arXiv preprint arXiv:2209.02778, 2022.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Yuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin Zhu, Andre Pradhana, and Chenfanfu Jiang. A moving least squares material point method with displacement discontinuity and two-way rigid body coupling. ACM Transactions on Graphics (TOG), 37(4):150, 2018.\n\n- Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, and Fredo Durand. Taichi: a language for high-performance computation on spatially sparse data structures. ACM Transactions on Graphics (TOG), 38(6):1-16, 2019.\n\n- Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B Tenenbaum, and Chuang Gan. Plasticinelab: A soft-body manipulation benchmark with differentiable physics. arXiv preprint arXiv:2104.03311, 2021.\n\n- Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019-3026, 2020.\n\n- Zhiwei Jia, Xuanlin Li, Zhan Ling, Shuang Liu, Yiran Wu, and Hao Su. Improving policy optimization with generalist-specialist learning. In International Conference on Machine Learning, pp. 10104-10119. PMLR, 2022.\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\n\n- Chengshu Li, Cem Gokmen, Gabrael Levine, Roberto Martin-Martin, Sanjana Srivastava, Chen Wang, Josiah Wong, Ruohan Zhang, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In 6th Annual Conference on Robot Learning.\n\n- Xingyu Lin, Yufei Wang, Jake Olkin, and David Held. Softgym: Benchmarking deep reinforcement learning for deformable object manipulation. In Conference on Robot Learning, 2020.\n\n- Minghua Liu, Xuanlin Li, Zhan Ling, Yangyan Li, and Hao Su. Frame mining: a free lunch for learning robotic manipulation from 3d point clouds. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=d-JYso87y6s.\n\n- Miles Macklin. Warp: A high-performance python framework forgpu simulation and graphics. https://github.com/nvidia/warp, March 2022. NVIDIA GPU Technology Conference (GTC).\n\n- Jeffrey Mahler, Florian T Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry, Kai Kohlhoff, Torsten Kr\u00f6ger, James Kuffner, and Ken Goldberg. Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards. In IEEE International Conference on Robotics and Automation (ICRA), pp. 1957\u20131964. IEEE, 2016.\n\n- Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performancegpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021.\n\n- Roberto Martin-Martin, Michelle A Lee, Rachel Gardner, Silvio Savarese, Jeannette Bohg, and Animesh Garg. Variable impedance control in end-effector space: An action space for reinforcement learning in contact-rich tasks. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1010-1017. IEEE, 2019.\n\n- Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529-533, 2015.\n\n- Douglas Morrison, Peter Corke, and J\u00fcrgen Leitner. Egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation. IEEE Robotics and Automation Letters, 5 (3):4368-4375, 2020.\n\n- Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Cathera Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\n- Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652-660, 2017.\n\n- Yuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu, and Hao Su. S4g: Amodal single-view single-shot se (3) grasp detection in cluttered scenes. In Conference on robot learning, pp. 53-65. PMLR, 2020.\n\n- Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1-8, 2021. URL http://jmlr.org/papers/v22/20-1364.html.\n\n- Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.\n\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\n- Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for robotic manipulation. arXiv preprint arXiv:2209.05451, 2022.\n\n- Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00edn-Mart\u00edn, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on Robot Learning, pp. 477-490. PMLR, 2022.\n\n- Martin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, and Dieter Fox. Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 13438-13444. IEEE, 2021.\n\n- Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. Habitat 2.0: Training home assistants to rearrange their habitat. Advances in Neural Information Processing Systems, 34:251-266, 2021.\n\n- Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.\n\n- Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. arXiv preprint arXiv:2107.14171, 2021.\n\n- Jiayi Weng, Min Lin, Shengyi Huang, Bo Liu, Denys Makoviichuk, Viktor Makoviychuk, Zichen Liu, Yufan Song, Ting Luo, Yukun Jiang, et al. Envpool: A highly parallel reinforcement learning environment execution engine. arXiv preprint arXiv:2206.10558, 2022.\n\n- Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11097-11107, 2020.\n\n- Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 1094-1100. PMLR, 2020.\n\n- Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and Johnny Lee. Transporter networks: Rearranging the visual world for robotic manipulation. Conference on Robot Learning (CoRL), 2020.\n\n- Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Martin-Martin. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638528.481607", "msecs": "481.0", "relativeCreated": "125312.10446357727", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Support-set bottlenecks for video-text representation learning\n\n# Abstract\nThe dominant paradigm for learning video-text representations \u2013 noise contrastive learning \u2013 increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semantically-related \u2013 for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample\u2019s caption must be reconstructed as a weighted combination of a support set of visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX, ActivityNet, and MSVD for video-to-text and text-to-video retrieval.\n\n\n# 1 INTRODUCTION\n\n\nNoise contrastive learning (Gutmann & Hyvarinen, 2010) is emerging as one of the best approaches to learn data representations both for supervised (Khosla et al., 2020) and unsupervised regimes (Chen et al., 2020c). The idea is to learn a representation that discriminates any two data samples while being invariant to certain data transformations. For example, one might learn a representation that identifies a specific image up to arbitrary rotations (Misra & van der Maaten, 2020). In a multi-modal setting, the transformations can separate different modalities, for example, by extracting the audio and visual signals from a video. The resulting noise contrastive representation associates audio and visual signals that come from the same source video, differentiating others (Patrick et al., 2020).\n\n\nThe noise contrastive approach is motivated by the fact that the transformations that are applied to the data samples leave their 'meaning' unchanged. For example, rotating an image does not change the fact that it contains a cat or not (Gidaris et al., 2018). However, in most cases, we expect to find many data samples that share the same content without being necessarily related by simple transformations (e.g. think of any two images of cats). Existing noise contrastive formulations are unaware of these relationships and still try to assign different representations to these samples (Wu et al., 2018), despite the fact that they are semantically equivalent. If the representation is learned for a downstream task such as semantic video retrieval, this might degrade performance.\n\n\nThis suggest that there might be other learning signals that could complement and improve pure contrastive formulations. In this paper, we explore this idea in the case of learning from two modalities: videos and text, in the form of video transcripts or captions. Given a state-of-the-art contrastive formulation that learns from these two modalities, we investigate complementary pretext objectives to improve it. First, we consider the (instance) captioning task, namely mapping a video to the corresponding text, casting this as a conditional stochastic text generation problem. We show that this brings only a modest benefit.\n\n\nWe observe that the captioning task is highly sample-specific, as the goal is to produce a caption which describes a specific video and not any other video, and thus it suffers from the same disadvantages (discouraging concept sharing among samples) as contrastive learning. Thus, we propose\n\n\n![Fig. 1: Cross-modal discrimination and cross-captioning. Our model learns from two complementary losses: (a) Cross-modal contrastive learning learns strong joint video-text embeddings, but every other sample is considered a negative, pushing away even semantically related captions (orange arrows). (b) We introduce a generative task of cross-captioning, which alleviates this by learning to reconstruct a sample's text representation as a weighted combination of a support-set, composed of video representations from other samples.](images/1c91305c049a8f80771f0f0bd39786287895c94293b9e8ccc07105316bd5134f.jpg)\n\n\n![](images/436d25e4814ed30d5e1584b0af72f4f6435be0d2e99d0442443d1af69e98320d.jpg)\n\n\nto address this issue by switching to a different text generation task. The idea is to modify the text generator to take as input a learnable mixture of a support-set of videos, which we call cross-instance captioning. The mixture weights are generated by comparing the learned video representations to captions' representations in an online way over the batch. The limited set of support samples acts as a bottleneck that encourages extraction of shared semantics. In this manner, the embeddings can associate videos that share similar captions even if the contrastive loss tries to push them apart.\n\n\nWe show that, when the captioning task is added in this manner, it brings a sensible improvement to already very strong video representation learning results, further improving our own state-of-the-art baseline by a significant margin.\n\n\n# 2 RELATED WORKS\n\n\nLearning data representations from unlabelled data has been a long standing goal of machine learning. These approaches are called \"self-supervised learning\" because the learning signals, termed pretext tasks, are obtained from the data itself. In the image and video domain, pretext tasks include colorization (Zhang et al., 2016), rotation (Gidaris et al., 2018), or clustering (Asano et al., 2020a;b; Caron et al., 2018; Ji et al., 2018), while in the natural language domain, masked language modeling (Devlin et al., 2019), and next word prediction (Mikolov et al., 2013; Pennington et al., 2014) are extremely popular. These pretext tasks can be broadly classified into two classes: generative and discriminative.\n\n\nDiscriminative approaches learn representations by differentiating input samples, using objectives such as the contrastive loss (Gutmann & Hyvarinen, 2010; Hadsell et al., 2006). Discriminative approaches have proven to be particularly successful for image (Chen et al., 2020c; He et al., 2020; Misra & van der Maaten, 2020; Wu et al., 2018) and video (Han et al., 2019; Morgado et al., 2020; Patrick et al., 2020) representation learning. Generative approaches, on the other hand, try to reconstruct its input. GANs (Donahue & Simonyan, 2019; Goodfellow et al., 2014; Radford et al., 2015), autoencoders (Hinton & Salakhutdinov, 2006) and sequence-to-sequence models (Sutskever et al., 2014) are popular generative models. In this work, we show the importance of combining both discriminative and generative objectives to learn effective video-text representations.\n\n\nThe success of representation learning has also been due to advances in model architectures, such as the Transformer (Vaswani et al., 2017). BERT (Devlin et al., 2019) demonstrated that a transformer architecture pretrained on large-scale textual data can learn transferable text representations that can be fine-tuned on a variety of downstream tasks. Subsequent works (Clark et al., 2020; Lewis et al., 2020a;b; Radford et al., 2019; Raffel et al., 2019), have improved upon the transformer architecture or training objective to learn even better representations. Inspired by the success of transformers in the NLP domain, several works have leveraged transformers to learn transferable image (Chen et al., 2020a; Desai & Johnson, 2020; Sariyildiz et al., 2020) or multi-modal image-text representations (Chen et al., 2019; Li et al., 2020a; 2019; Lu et al., 2019; Su et al., 2019; Tan & Bansal, 2019). In this work, we leverage the transformer architecture to better encode and represent text and video.\n\n\n![Fig. 2: (a) Our cross-modal framework with the discriminative (contrastive) objective and the generative objective. The model learns to associate video-text pairs in a common embedding space with text and video encoders (top). Meanwhile, the text must also be reconstructed as a weighted combination of video embeddings from a support-set (bottom), selected via attention, which enforces representation sharing between different samples. (b) Weights matrices (attention maps) used in each cross-captioning objective (see section 3.1.2).](images/78d794b3160c7f4719768a52eeb8a56695dbc530ed87535b536fb26df095e183.jpg)\n\n\nLarge-scale training data has enabled the more effective pretraining of image (Sun et al., 2017; Yalniz et al., 2019), video (Ghadiyaram et al., 2019; Thomee et al., 2016) and textual representations (Raffel et al., 2019). The release of the HowTo100M dataset (Miech et al., 2019), a large-scale instructional video dataset, has spurred significant interest in leveraging large-scale pretraining to improve video-text representations for tasks such as video question-answering (Lei et al., 2018), text-video retrieval (Liu et al., 2019) and video captioning (Zhou et al., 2018b) on smaller datasets such as YouCookII (Zhou et al., 2018a), MSVD (Venugopalan et al., 2015a), MSR-VTT (Xu et al., 2016), LSMDC (Rohrbach et al., 2017), DiDeMo (Hendricks et al., 2018) and ActivityNet (Krishna et al., 2017). Although semantically rich and diverse, instructional videos from the web are super noisy and therefore a few approaches have been proposed to combat this. A few works (Luo et al., 2020; Sun et al., 2019a;b; Zhu & Yang, 2020) extend the BERT model to accept both visual and textual tokens to learn high-level semantic video-text representations. Other works have leveraged the contrastive loss (Miech et al., 2020) and show that using the raw audio (Alayrac et al., 2020; Rouditchenko et al., 2020) and other modalities (Gabeur et al., 2020) can be used to better align and improve video-text representations. While all these approaches rely on a contrastive objective, VidTranslate (Korbar et al., 2020) shows that a generative objective can also be used to learn joint video-text representations. In contrast to Korbar et al. (2020), we show that combining contrastive and generative objectives to pre-train video-text representations on large-scale data such as HowTo100M is very effective. The generative objective serves as regularizer to mitigate the strictness of the instance discrimination task of the constrastive objective, showing benefits similar to approaches such as clustering (Caron et al., 2020; Li et al., 2020b) and feature mixing (Kalantidis et al., 2020) which have been applied in the image domain.\n\n\n# 3 METHOD\n\n\nWe consider the problem of learning multimodal representations from a corpus  $\\mathcal{C}$  of video-text pairs  $(v, t)$ , where  $v$  is a video and  $t$  is its corresponding text (caption or transcription). Our goal is to learn a pair of representation maps  $c_v = \\Psi(v)$  and  $c_t = \\Phi(t)$ , with outputs in a  $d$ -dimensional embedding space  $c_v, c_t \\in \\mathbb{R}^d$ , where semantically similar instances are close to each other.\n\n\n# 3.1 OBJECTIVE FOR LEARNING MULTIMODAL REPRESENTATIONS\n\n\nWe consider two learning objectives, also illustrated in Figure 1. The first is the contrastive objective, pushing embeddings  $c_t$  and  $c_v$  to be close if text  $t$  and video  $v$  come from the same sample and push-\n\n\ning them apart otherwise. This assumes that every sample is its own class and does not benefit from modelling similarities across instances. The second objective is generative captioning. In its most basic variant, it maximizes the probability of generating the text  $t$  given the corresponding video  $v$ . However, we suggest that variants that explicitly promote concept sharing between instances will result in better downstream performance, in tasks such as video retrieval. These variants, illustrated in Figure 2, have in common that the caption  $t$  is reconstructed from a learned weighted combination over other videos  $\\hat{v}$ . This is a form of attention (Bahdanau et al., 2014) which encourages the network to learn about which videos share similar semantics, compensating for the contrastive loss and grouping them implicitly.\n\n\nIn the following, we denote with  $\\mathcal{B} \\subset \\mathcal{C}$  a batch of multi-modal samples, i.e. a finite collection of video-text pairs  $(t,v) \\in \\mathcal{C}$ . For simplicity, we denote the batch as  $\\mathcal{B} = \\{(t^i,v^i)\\}_{i=1}^B$ .\n\n\n# 3.1.1 CONTRASTIVE OBJECTIVE\n\n\nTo define the contrastive objective, let  $s(a,b) = \\frac{a^\\top b}{\\|a\\|\\|b\\|}$  be the similarity measure between vectors  $a$  and  $b$ . Following Faghri et al. (2018), we adopt the hinge-based triplet ranking loss with hard negative mining:\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {c o n t r a s t}} = \\frac {1}{B} \\sum_ {i = 1} ^ {B} \\left[ \\max  _ {j} \\left[ \\alpha - s \\left(c _ {t} ^ {i}, c _ {v} ^ {i}\\right) + s \\left(c _ {t} ^ {i}, c _ {v} ^ {j}\\right) \\right] _ {+} + \\max  _ {j} \\left[ \\alpha - s \\left(c _ {t} ^ {i}, c _ {v} ^ {i}\\right) + s \\left(c _ {t} ^ {j}, c _ {v} ^ {i}\\right) \\right] _ {+} \\right], \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\alpha$  is the correlation margin between positive and negative pairs and  $[\\cdot]_{+} = \\max\\{0, \\cdot\\}$  is the hinge function. In our experiments, we set  $\\alpha = 0.2$ .\n\n\n# 3.1.2 CROSS-CAPTIONING OBJECTIVES\n\n\nIn the conventional captioning, the decoder seeks to optimize the negative log-likelihood of a text sequence  $t$  given its corresponding video  $v$ :\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {c a p t i o n}} = - \\frac {1}{B} \\sum_ {i = 1} ^ {B} \\log p \\left(t ^ {i} \\mid e _ {v} ^ {i}\\right). \\tag {2}\n$$\n\n$$\n\n\nHere, the log-likelihood is obtained via auto-regressive decoding (Vaswani et al., 2017) from an intermediate video embedding  $e_v^i = \\Phi'(v^i)$ . For the cross-captioning objective, we modify this loss to condition the generation process on a weighted average of the embeddings of the other videos in the batch, which we call the support-set. The weights themselves, which can be interpreted as a batch-wise attention, are obtained as a softmax distribution with temperature  $T$  over batch indices based on the video embeddings, as follows:\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {c r o s s - c a p t i o n i n g}} = - \\frac {1}{B} \\sum_ {i = 1} ^ {B} \\log p \\left(t ^ {i} \\mid \\bar {e} _ {v} ^ {i}\\right), \\quad \\bar {e} _ {v} ^ {i} = \\sum_ {j \\in \\mathcal {S} _ {i}} \\frac {\\exp \\left\\langle c _ {t} ^ {i} , c _ {v} ^ {j} \\right\\rangle / T}{\\sum_ {k \\in \\mathcal {S} _ {i}} \\exp \\left\\langle c _ {t} ^ {i} , c _ {v} ^ {k} \\right\\rangle / T} \\cdot e _ {v} ^ {j}. \\tag {3}\n$$\n\n$$\n\n\nBy default, the summation in the softmax is conducted over a support set  $S_{i}$  containing all indices except  $i$ . In the experiments, we consider the following attention types for reconstruction. Identity captioning  $(S_{i} = \\{i\\})$  generates the caption from the corresponding video and reduces to the standard captioning objective, eq. (2). Full support  $(S_{i} = \\{1,\\dots ,B\\})$  considers all videos as possible candidates for captioning. Hybrid captioning sets the weights in eq. (3) as the average of the weights for identity captioning and full support. Cross-captioning  $(S_{i} = \\{j\\neq i\\})$  considers all but the video that one wishes to caption. This variant forces the network to extract all information required for captioning from other videos in the batch. Figure 2 compares graphically these attention mechanisms.\n\n\nConsidering both discriminative and generative objectives for learning multimodal representations, our full objective is  $\\mathcal{L} = \\mathcal{L}^{\\mathrm{contrast}} + \\lambda \\mathcal{L}^{\\mathrm{cross - captioning}}$ , where  $\\lambda$  balances two objectives. We set  $\\lambda = 10$  to ensure similar magnitudes for both losses in our experiments. In the training phase, we use Adam (Kingma & Ba, 2015) to minimize our loss. At inference time, we directly use  $\\Phi(t)$  and  $\\Psi(v)$  to encode video and text representations for retrieval.\n\n\n# 3.2 MODEL ARCHITECTURE\n\n\nWe now discuss the details of the encoders and decoder components in our architecture, illustrated in fig. 2. For the text decoder  $p(t|e_v)$  in eq. (2) and (3), we use a pre-trained T-5 decoder (Raffel et al., 2019).\n\n\nFor the video representation  $c_{v} = \\Psi (v) = \\Psi^{\\prime \\prime}(\\Psi^{\\prime}(v))$ , we use a video encoder  $e_v = \\Psi '(v)$  followed by a multi-layer transformer pooling head  $c_{v} = \\Psi^{\\prime \\prime}(e_{v})$ . The encoder  $\\Psi^{\\prime}(v)$  concatenates the output of pretrained ResNet-152 (He et al., 2016) and R(2+1)D-34 (Tran et al., 2018) networks applied to individual video frames, resulting in a code  $e_v = [e_{v1}\\dots e_{vM}]$  where  $M$  is the maximum duration of a video clip. For the pooling head  $c_{v} = \\Psi^{\\prime \\prime}(e_{v})$ , we consider a transformer architecture to attend to important context and summarize it into a fixed-length representation  $c_{v}$ . For this, we follow MMT (Gabeur et al., 2020), but with two important differences. First, while MMT uses 7 expert features that results in  $7\\times$  the sequence length, we only use a transformer to attend to early-fused motion and appearance features as the video representation, thus significantly reducing the sequence length and computational cost. Second, instead of stacking 6 transformer layers to encode the visual stream as in MMT, we only use a shallow two-layer transformer architecture with additional pre-encoders, further increasing model efficiency. As temporal 1D-convolutional neural networks (CNNs) (LeCun et al., 1998) were shown to effectively capture temporal dependencies in videos (Dong et al., 2019), we integrate CNNs into our transformer pooling heads to better capture video temporal signals. In more detail, we compute  $c_{v} = \\Psi^{\\prime \\prime}(e_{v})$  by chaining two transformer layers, each of the type:\n\n\n$$\n\n$$\n\\psi (e) = \\mathrm {B N} (\\mathrm {F F N} \\left(e _ {\\text {a t t n}}\\right) + e _ {\\text {a t t n}}), \\quad e _ {\\text {a t t n}} = \\mathrm {B N} (\\mathrm {M H A} (f (e)) + f (e)). \\tag {4}\n$$\n\n$$\n\n\nHere  $f$  is a pre-encoder that refines the video representation; we found empirically that a 1D CNN works well for this purpose. Then, we apply multi-head self-attention (MHA) (Vaswani et al., 2017) followed by a feed-forward network (FNN) with batch normalization (BN) (Ioffe & Szegedy, 2015). The architecture maps the input sequence  $e_v$  to a new 'contextualized' sequence of representation vectors; we take the first one as  $c_v$ .\n\n\nThe text representation decomposes in the same way as  $c_{t} = \\Phi(t) = \\Phi''(\\Phi'(t))$ . The text encoder  $e_{t} = \\Phi'(t)$  uses a pretrained T-5 network resulting in a code  $e_{t} = [e_{t1} \\cdots e_{tN}]$ , where  $N$  is the maximum length of a sentence. The pooling head  $c_{t} = \\Phi''(e_{t})$  follows the same design as the video case, but  $f$  is set to a recurrent neural network (RNN) instead of a CNN. Please refer to the appendix for details.\n\n\nIn practice, for computational reasons, we use eq. (3) to finetune the parameters of all networks except the video encoder  $\\Psi'(v)$ , which is fixed.\n\n\n# 4 EXPERIMENTS\n\n\nWe validate empirically the ability of our method to learn better representations for the downstream tasks of text-to-video and video-to-text retrieval. First, in sec. 4.2 we ablate various model components on the MSR-VTT dataset. Then, in sec. 4.3 we show that our best model significantly outperforms state-of-the-art retrieval systems on three datasets, MSR-VTT, ActivityNet and VATEX. Finally, in sec. 4.4 we analyse qualitatively the effect of the attention mechanism used during training.\n\n\n# 4.1 EXPERIMENTAL SETUP\n\n\nDatasets. HowTo100M (Miech et al., 2019) is a large-scale instructional video collection of 1.2 million YouTube videos, along with automatic speech recognition transcripts. We use this dataset for our pre-training experiments. MSR-VTT (Xu et al., 2016) contains 10000 videos, where each video is annotated with 20 descriptions. We report results on the 1k-A split (9000 training, 1000 testing) as in Liu et al. (2019). VATEX (Wang et al., 2019) is a multilingual (Chinese and English) video-text dataset with 34,911 videos. We use the official training split with 25991 videos and report on the validation split as in HGR (Chen et al., 2020b). The ActivityNet Caption (Krishna et al., 2017) dataset consists of densely annotated temporal segments of 20K YouTube videos. We use the 10K training split to train from scratch/ finetune the model and report the performance on the 5K 'val1' split. The MSVD (Chen & Dolan, 2011) dataset consists of 80K English descriptions for\n\n\nTable 1: Model Architecture and Training Details Ablation. Text  $\\rightarrow$  Video retrieval performance on MSR-VTT. Recall@1, 5, and Median Recall are shown. (a) Video Encoder. Stronger features and combination improves performance.\n\n\n<table><tr><td>Feature source</td><td>R@1\u2191</td><td>R@5\u2191</td><td>MdR \u2193</td></tr><tr><td>R-152</td><td>20.8</td><td>46.2</td><td>6.0</td></tr><tr><td>R(2+1)D-34</td><td>23.7</td><td>53.2</td><td>4.0</td></tr><tr><td>R(2+1)D-34 +R-152</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n(c) Text Encoder. Stronger encoding of text improves retrieval.\n\n\n<table><tr><td>Text Encoder</td><td>R@1 \u2191</td><td>R@5 \u2191</td><td>MdR \u2193</td></tr><tr><td>W2V (GloVe)</td><td>22.1</td><td>49.8</td><td>6.0</td></tr><tr><td>T5-Small</td><td>24.5</td><td>51.2</td><td>3.0</td></tr><tr><td>T5-Base</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n(b) Feature Aggregation. Learning temporal attention yields strong gains over pooling.\n\n\n<table><tr><td>Temporal reduction</td><td>R@1 \u2191</td><td>R@5 \u2191</td><td>MdR \u2193</td></tr><tr><td>Max</td><td>21.8</td><td>49.5</td><td>8.0</td></tr><tr><td>Mean</td><td>22.5</td><td>51.3</td><td>6.0</td></tr><tr><td>Multi-Head Attn</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n(d) Text Decoder. Stronger decoding of text improves retrieval.\n\n\n<table><tr><td>Text Encoder</td><td>Text Decoder</td><td>R@1 \u2191</td><td>R@5 \u2191</td><td>MdR \u2193</td></tr><tr><td>T5-Base</td><td>T5-Small</td><td>26.2</td><td>54.2</td><td>3.0</td></tr><tr><td>T5-Base</td><td>T5-Base</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n(e) Contrastive Loss. Inter-modal Triplet loss yields the best performance.\n\n\n<table><tr><td>Contrastive</td><td>R@1 \u2191</td><td>R@5 \u2191</td><td>MdR \u2193</td></tr><tr><td>InfoNCE (inter+intra)</td><td>10.7</td><td>28.5</td><td>15.0</td></tr><tr><td>InfoNCE (inter)</td><td>10.8</td><td>29.0</td><td>14.5</td></tr><tr><td>Triplet (inter+intra)</td><td>26.8</td><td>56.2</td><td>3.0</td></tr><tr><td>Triplet (inter)</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n(f) Support-set Size. Retrieval degrades when reconstructing from too small and too large sets.\n\n\n<table><tr><td rowspan=\"2\">Size</td><td colspan=\"7\">Batch-size</td><td colspan=\"2\">Memory bank</td></tr><tr><td>8</td><td>16</td><td>32</td><td>64</td><td>128</td><td>256</td><td>512</td><td>2k</td><td>8k</td></tr><tr><td>R@1/5</td><td>18.5/45.6</td><td>20.7/49.9</td><td>25.2/54.6</td><td>27.2/55.2</td><td>28.0/56.1</td><td>26.9/55.0</td><td>25.3/53.5</td><td>26.8/54.7</td><td>26.2/52.7</td></tr></table>\n\n\n1970 videos from YouTube, with each video associated with around 40 sentences each. We use the standard split of 1200, 100, and 670 videos for training, validation, and testing (Liu et al., 2019; Venugopalan et al., 2015b; Xu et al., 2015).\n\n\nEvaluation Metrics. To measure the text-to-video and video-to-text retrieval performance, we choose Recall at K (R@K) and Median Rank (MedR), which are common metrics in information retrieval.\n\n\n# 4.2 ABLATIONS\n\n\nIn Tab. 1, we first only ablate the cross-modal retrieval part of our network architecture, while the generative objectives are analysed in Tab. 2.\n\n\nVideo Encoder. In Tab. 1a, we show the effect of the choice of visual input features. We find that for text-to-video retrieval at Recall at 1 and 5 ( $R@1$ ,  $R@5$ ), features obtained from a video R(2+1)D-34 ResNet achieve  $2.9\\%$  and  $7.0\\%$  higher performance compared to only image-frame based features from a ResNet-152. A further  $3.5\\%$  and  $2.0\\%$  can be gained by concatenating both features, yielding the strongest  $MdR$  of  $3.0\\%$ .\n\n\nFeature Aggregation. While the features from both video and image-based visual encoders have reduced spatial extent after a fully-connected layer, the temporal dimension can be reduced in various ways. In Tab. 1b, we find that our multi-head, parameter-\n\n\nized attention reduction yields strong gains over the mean- or max-pooling baselines of over  $4\\%$  for\n\n\nTable 2: Effect of learning objectives. Text  $\\rightarrow$  Video retrieval on MSR-VTT.\n\n\n<table><tr><td></td><td>R@1\u2191</td><td>R@5\u2191</td><td>MdR\u2193</td></tr><tr><td>None</td><td>25.9</td><td>53.0</td><td>4.0</td></tr><tr><td>Identity</td><td>26.4</td><td>51.9</td><td>4.0</td></tr><tr><td>Full</td><td>25.8</td><td>53.9</td><td>3.0</td></tr><tr><td>Hybrid</td><td>26.0</td><td>54.8</td><td>3.0</td></tr><tr><td>Cross</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n$R@1$ . This shows that learning attention over the temporal dimension of fixed feature sets can give strong gains even without fine-tuning the encoder.\n\n\nText Encoder. In Tab. 1c, we find decent gains of  $2.7\\%$  and  $0.4\\%$  for  $\\mathbf{R}@\\mathbf{1},\\mathbf{5}$  for using T5-base, instead of T5-small. We do not use the T-5-Large model, as in Korbar et al. (2020), due to the prohibitively large relative model size increase of  $+220\\%$ .\n\n\nText Decoder. In Tab. 1d, we find that using a larger text decoder gives a  $1\\%$  increase in performance when using the cross-captioning objective.\n\n\nContrastive Loss. To validate the choice of a triplet loss in eq. (1), in Tab. 1e, we compare the results of the InfoNCE contrastive loss (Oord et al., 2018) with a triplet loss, with both the intra and inter-intra modality variants. We find that InfoNCE (Oord et al., 2018) loss does not work well in our case, likely due to the difficulty in tuning this loss to have the right combination of temperature and batch-size.\n\n\nSupport-Set Size. Lastly, in Tab. 1f, we show the effect of the size of the support set used for cross-instance captioning. We find that our reconstruction loss indeed acts as a bottleneck, with both smaller and very large sizes degrading the performance.\n\n\nCaptioning Objective. In Tab. 2, we show the effect of the different variants of our learning objective eq. (3). First, we find that the naive addition of a reconstruction objective (\"Identity\") does not improve the contrastive-only baseline (\"None\") much. Considering reconstruction from other videos improves the performance more. In particular, the \"Hybrid\" variant, which combines \"Identity\" and \"Full\" (sec. 3.1.2) improves Recall at 1 and 5 from  $25.9\\%$  and  $53.0\\%$  to  $26.0\\%$  and  $54.8\\%$ , respectively. However, the best result by far  $(27.2 / 55.2\\%)$  is obtained forcing captions to be reconstructed only from other videos, via our cross-instance attention mechanism (\"Cross\"). This variant cannot use information contained in a video to generate the corresponding caption and thus entirely relies on the model to discover meaningful relationship between different videos. This newly-proposed scheme seems to have the most beneficial effect for semantic retrieval.\n\n\nTable 3: Retrieval performance on the MSR-VTT dataset. Models in the second group are additionally pretrained on HowTo100M.\n\n\n<table><tr><td></td><td colspan=\"4\">Text\u2192Video</td><td colspan=\"4\">Video\u2192Text</td></tr><tr><td></td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td></tr><tr><td>Random Baseline</td><td>0.1</td><td>0.5</td><td>1.0</td><td>500.0</td><td>0.1</td><td>0.5</td><td>1.0</td><td>500.0</td></tr><tr><td>JSFusion (Yu et al., 2018)</td><td>10.2</td><td>31.2</td><td>43.2</td><td>13.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HT100M (Miech et al., 2019)</td><td>12.1</td><td>35.0</td><td>48.0</td><td>12.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>JPoSE (Wray et al., 2019)</td><td>14.3</td><td>38.1</td><td>53.0</td><td>9.0</td><td>16.4</td><td>41.3</td><td>54.4</td><td>8.7</td></tr><tr><td>CE (Liu et al., 2019)</td><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td><td>20.6</td><td>50.3</td><td>64.0</td><td>5.3</td></tr><tr><td>MMT (Gabeur et al., 2020)</td><td>24.6</td><td>54.0</td><td>67.1</td><td>4.0</td><td>24.4</td><td>56.0</td><td>67.8</td><td>4.0</td></tr><tr><td>Ours</td><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td><td>26.6</td><td>55.1</td><td>67.5</td><td>3.0</td></tr><tr><td>VidTranslate (Korbar et al., 2020)</td><td>14.7</td><td>-</td><td>52.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HT100M (Miech et al., 2019)</td><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td><td>16.8</td><td>41.7</td><td>55.1</td><td>8.0</td></tr><tr><td>NoiseEstimation (Amrani et al., 2020)</td><td>17.4</td><td>41.6</td><td>53.6</td><td>8.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>UniVL (Luo et al., 2020)</td><td>21.2</td><td>49.6</td><td>63.1</td><td>6.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>AVLnet (Rouditchenko et al., 2020)</td><td>27.1</td><td>55.6</td><td>66.6</td><td>4.0</td><td>28.5</td><td>54.6</td><td>65.2</td><td>4.0</td></tr><tr><td>MMT (Gabeur et al., 2020)</td><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td><td>27.0</td><td>57.5</td><td>69.7</td><td>3.7</td></tr><tr><td>Ours-pretrained</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td><td>28.5</td><td>58.6</td><td>71.6</td><td>3.0</td></tr></table>\n\n\n# 4.3 COMPARISON TO STATE-OF-THE-ART\n\n\nIn this section, we compare the results of our method to other recent text-to-video and video-to-text retrieval approaches on various datasets. In Tab. 3 to 5, we show the results of our model applied to text-to-video and video-to-text retrieval on MSR-VTT, VATEX, ActivityNet and MSVD with and without pre-training on HowTo100M. Without pre-training, our method outperforms all others in all metrics and datasets. In particular, for the VATEX dataset, our retrieval performance at recall at 1 and 5 is  $45.9\\%$  and  $82.4\\%$ , exceeding recent state-of-the-art methods (Chen et al., 2020b) by a margin of  $9\\%$ . For ActivityNet, our model outperforms MMT by a margin of  $4\\%$  at recall at 1. With\n\n\nTable 4: Retrieval performance on the VATEX dataset\n\n\n<table><tr><td></td><td colspan=\"4\">Text \u2192 Video</td><td colspan=\"4\">Video \u2192 Text</td></tr><tr><td></td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td></tr><tr><td>Random Baseline</td><td>0.2</td><td>0.7</td><td>1.05</td><td>2000.5</td><td>0.02</td><td>0.1</td><td>1.02</td><td>2100.5</td></tr><tr><td>VSE (Kiros et al., 2014)</td><td>28.0</td><td>64.3</td><td>76.9</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VSE++ (Faghri et al., 2018)</td><td>33.7</td><td>70.1</td><td>81.0</td><td>2.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Dual (Dong et al., 2019)</td><td>31.1</td><td>67.4</td><td>78.9</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HGR (Chen et al., 2020b)</td><td>35.1</td><td>73.5</td><td>83.5</td><td>2.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Ours</td><td>44.6</td><td>81.8</td><td>89.5</td><td>1.0</td><td>58.1</td><td>83.8</td><td>90.9</td><td>1.0</td></tr><tr><td>Ours-pretrained</td><td>45.9</td><td>82.4</td><td>90.4</td><td>1.0</td><td>61.2</td><td>85.2</td><td>91.8</td><td>1.0</td></tr></table>\n\n\nTable 5: Retrieval performance on ActivityNet\n\n\n<table><tr><td></td><td colspan=\"4\">Text \u2192 Video</td><td colspan=\"4\">Video \u2192 Text</td></tr><tr><td></td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@50\u2191</td><td>MdR\u2193</td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@50\u2191</td><td>MdR\u2193</td></tr><tr><td>Random Baseline</td><td>0.02</td><td>0.1</td><td>1.02</td><td>2458</td><td>0.02</td><td>0.1</td><td>1.02</td><td>2458</td></tr><tr><td>FSE(Zhang et al., 2018)</td><td>18.2</td><td>44.8</td><td>89.1</td><td>7.0</td><td>16.7</td><td>43.1</td><td>88.4</td><td>7.0</td></tr><tr><td>CE (Liu et al., 2019)</td><td>18.2</td><td>47.7</td><td>91.4</td><td>6.0</td><td>17.7</td><td>46.6</td><td>90.9</td><td>6.0</td></tr><tr><td>HSE (Zhang et al., 2018)</td><td>20.5</td><td>49.3</td><td>-</td><td>-</td><td>18.7</td><td>48.1</td><td>-</td><td>-</td></tr><tr><td>MMT (Gabeur et al., 2020)</td><td>22.7</td><td>54.2</td><td>93.2</td><td>5.0</td><td>22.9</td><td>54.8</td><td>93.1</td><td>4.3</td></tr><tr><td>Ours</td><td>26.8</td><td>58.1</td><td>93.5</td><td>3.0</td><td>25.5</td><td>57.3</td><td>93.5</td><td>3.0</td></tr><tr><td>MMT-pretrained (Gabeur et al., 2020)</td><td>28.7</td><td>61.4</td><td>94.5</td><td>3.3</td><td>28.9</td><td>61.1</td><td>94.3</td><td>4.0</td></tr><tr><td>Ours-pretrained</td><td>29.2</td><td>61.6</td><td>94.7</td><td>3.0</td><td>28.7</td><td>60.8</td><td>94.8</td><td>2.0</td></tr></table>\n\n\nTable 6: Retrieval performance on the MSVD dataset\n\n\n<table><tr><td></td><td colspan=\"4\">Text \u2192 Video</td><td colspan=\"4\">Video \u2192 Text</td></tr><tr><td></td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td></tr><tr><td>VSE (Kiros et al., 2014)</td><td>12.3</td><td>30.1</td><td>42.3</td><td>14.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VSE++ (Faghri et al., 2018)</td><td>15.4</td><td>39.6</td><td>53.0</td><td>9.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Multi. Cues (Mithun et al., 2018)</td><td>20.3</td><td>47.8</td><td>61.1</td><td>6.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CE (Liu et al., 2019)</td><td>19.8</td><td>49.0</td><td>63.8</td><td>6.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Ours</td><td>23.0</td><td>52.8</td><td>65.8</td><td>5.0</td><td>27.3</td><td>50.7</td><td>60.8</td><td>5.0</td></tr><tr><td>Ours-pretrained</td><td>28.4</td><td>60.0</td><td>72.9</td><td>4.0</td><td>34.7</td><td>59.9</td><td>70.0</td><td>3.0</td></tr></table>\n\n\npre-training on HowTo100M, our performance further increases across the board. Notably, unlike MMT which uses 7 features, our model uses only 2 features and achieves state-of-the-art in most metrics.\n\n\n# 4.4 ANALYSIS\n\n\nIn order to better understand the effect of our learning objective, we visualize the soft attention of our best-performing cross-instance reconstruction model in fig. 3. As we can see in the top-left square, which shows the pairwise attention between all pairs of videos in the batch, it is highly focused, with the model mostly attending one or two other instances in the batch.\n\n\nFor the first video's caption reconstruction (second row), we find that the model solely attends to another musical\n\n\n![Fig. 3: Support-set attention map. Attention scores of all pairs in a batch (top-left square) and a subset of rows/columns (other squares) on VTT.](images/c7efd7d29452a2cce015646dea8cd2bfd03ff80d4fd0e4e9a9bd38ec0e784c96.jpg)\n\n\nperformance video that is in the batch, ignoring the others. For the second video (third row), the model focuses on another sample that shows the sea but differs in most other aspects since there are no semantically-equivalent clips in the batch. The third video shares a similar scenario. These examples show that the bottleneck is effective at forcing the model to avoid memorising the videocaption association of each clip in isolation, and attempt to match other clips more broadly, since an exact (or very close) match is not guaranteed.\n\n\n# 5 CONCLUSION\n\n\nIn this work, we studied classic contrastive learning methods such as the triplet loss to learn video-text representations for cross-model retrieval. We suggested that the contrastive approach might pull apart videos and captions even when they are semantically equivalent, which can hinder downstream retrieval performance. To mitigate this effect, we propose to consider a captioning pretext task as an additional learning objective. In particular, we show that cross-instance captioning can encourage the representation to pull together videos that share a similar caption, and are thus likely to be equivalent for retrieval. Leveraging these ideas, our model achieves state-of-the-art performance on the text-to-video and video-to-text retrieval tasks, on three datasets.\n\n\nWhile we demonstrated these ideas in the specific case of text-to-video retrieval, they can in principle generalize to any setting that utilizes a contrastive loss, including self-supervised learning, provided that it is possible to learn reasonable conditional generators of a modality or data stream given another.\n\n\n# REFERENCES\n\n- Jean-Baptiste Alayrac, A. Recasens, Ros\u00e1lia G. Schneider, R. Arandjelovi\u0107, Jason Ramapuram, J. Fauw, Lucas Smaira, S. Dieleman, and Andrew Zisserman. Self-supervised multimodal versatile networks. *ArXiv*, abs/2006.16228, 2020.\n\n- Humam Alwassel, Bruno Korbar, Dhruv Mahajan, Lorenzo Torresani, Bernard Ghanem, and Du Tran. Self-supervised learning by cross-modal audio-video clustering. In NeurIPS, 2020.\n\n- Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein. Noise estimation using density estimation for self-supervised multimodal learning. arXiv preprint arXiv:2003.03186, 2020.\n\n- Yuki M. Asano, Mandela Patrick, Christian Rupprecht, and Andrea Vedaldi. Labelling unlabelled videos from scratch with multi-modal self-supervision, 2020a.\n\n- Yuki M Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020b.\n\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n\n- M. Caron, I. Misra, J. Mairal, Priya Goyal, P. Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.\n\n- Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, 2018.\n\n- Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299-6308, 2017.\n\n- David L Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp. 190-200. Association for Computational Linguistics, 2011.\n\n- Mark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from pixels. In ICML, 2020a.\n\n- Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained video-text retrieval with hierarchical graph reasoning. 2020b.\n\n- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020c.\n\n- Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019.\n\n- Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training text encoders as discriminators rather than generators, 2020.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pp. 248-255, 2009.\n\n- Michael Denkowski and Alon Lavie. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the ninth workshop on statistical machine translation, pp. 376-380, 2014.\n\n- Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. arXiv preprint arXiv:2006.06666, 2020.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In ACL, 2019.\n\n- Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In NeurIPS, 2019.\n\n- Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang, and Xun Wang. Dual encoding for zero-example video retrieval. In CVPR, 2019.\n\n- Haodong Duan, Yue Zhao, Yuanjun Xiong, Wentao Liu, and Dahua Lin. Omni-sourced webly-supervised learning for video recognition. In ECCV, 2020.\n\n- Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. In BMVC, 2018.\n\n- Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, 2020.\n\n- Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Large-scale weakly-supervised pre-training for video action recognition. In CVPR, 2019.\n\n- Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. *ICLR*, 2018.\n\n- Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.\n\n- Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS, 2010.\n\n- Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In CVPR, 2006.\n\n- Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In ICCV Workshops, 2019.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning, 2020.\n\n- Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with temporal language. In Empirical Methods in Natural Language Processing (EMNLP), 2018.\n\n- Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 2006.\n\n- Jingyi Hou, Xinxiao Wu, Wentian Zhao, Jiebo Luo, and Yunde Jia. Joint syntax representation learning and visual cue translation for video captioning. In ICCV, 2019.\n\n- Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n\n- Xu Ji, Jo\u00e3o F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classification and segmentation, 2018.\n\n- Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. In NeurIPS, 2020.\n\n- Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning, 2020.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\n- Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.\n\n- Bruno Korbar, Fabio Petroni, Rohit Girdhar, and Lorenzo Torresani. Video understanding as machine translation, 2020.\n\n- Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In CVPR, 2017.\n\n- H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: a large video database for human motion recognition. In ICCV, 2011.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\n- Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa: Localized, compositional video question answering. In EMNLP, pp. 1369-1379, 2018.\n\n- Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer. Pre-training via paraphrasing, 2020a.\n\n- Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL, 2020b.\n\n- Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In AAAI, 2020a.\n\n- Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven C. H. Hoi. Prototypical contrastive learning of unsupervised representations, 2020b.\n\n- Lianian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language, 2019.\n\n- Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei. Jointly localizing and describing events for dense video captioning. In CVPR, 2018.\n\n- Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004.\n\n- Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In BMVC, 2019.\n\n- Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019.\n\n- Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. Univl: A unified video and language pre-training model for multimodal understanding and generation, 2020.\n\n- Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, 2019.\n\n- Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In CVPR, 2020.\n\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\n- Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In CVPR, 2020.\n\n- Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze, and Amit K Roy-Chowdhury. Learning joint embedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, pp. 19-27, 2018.\n\n- Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-visual instance discrimination with cross-modal agreement. arXiv preprint arXiv:2004.12943, 2020.\n\n- Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n\n- Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.\n\n- Mandela Patrick, Yuki M. Asano, Polina Kuznetsova, Ruth Fong, Jo\u00e3o F. Henriques, Geoffrey Zweig, and Andrea Vedaldi. Multi-modal self-supervision from generalized data transformations, 2020.\n\n- Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532-1543, 2014.\n\n- AJ Piergiovanni, Anelia Angelova, and Michael S. Ryoo. Evolving losses for unsupervised video representation learning. In CVPR, 2020.\n\n- Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d residual networks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5533-5541, 2017.\n\n- Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n\n- Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Chris Pal, Hugo Larochelle, Aaron Courville, and Bernt Schiele. Movie description. International Journal of Computer Vision, 2017.\n\n- Andrew Rouditchenko, Angie Boggust, David Harwath, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Rogerio Feris, Brian Kingsbury, Michael Picheny, Antonio Torralba, et al. Avlnet: Learning audio-visual language representations from instructional videos. arXiv preprint arXiv:2006.09199, 2020.\n\n- Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning visual representations with caption annotations, 2020.\n\n- Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human action classes from videos in the wild. In *CRCV-TR-12-01*, 2012.\n\n- Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations, 2019.\n\n- Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. 2017.\n\n- Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning video representations using contrastive bidirectional transformer, 2019a.\n\n- Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In ICCV, 2019b.\n\n- Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NeurIPS, pp. 3104-3112, 2014.\n\n- Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In EMNLP, 2019.\n\n- Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64-73, 2016.\n\n- Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In CVPR, 2018.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIps, 2017.\n\n- Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4566-4575, 2015.\n\n- Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. Sequence to sequence - video to text. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015a.\n\n- Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. Sequence to sequence - video to text. In ICCV, 2015b.\n\n- Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaou Tang, and Luc Van Gool. Temporal segment networks for action recognition in videos. IEEE transactions on pattern analysis and machine intelligence, 41(11):2740-2755, 2018.\n\n- Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4581-4591, 2019.\n\n- Michael Wray, Diane Larlus, Gabriela Csurka, and Dima Damen. Fine-grained action retrieval through multiple parts-of-speech embeddings. In ICCV, 2019.\n\n- Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In CVPR, 2018.\n\n- Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 305\u2013321, 2018.\n\n- Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A large video description dataset for bridging video and language. In CVPR, 2016.\n\n- Ran Xu, Caiming Xiong, Wei Chen, and Jason J Corso. Jointly modeling deep video and compositional text to bridge vision and language in a unified framework. Citeseer, 2015.\n\n- I. Zeki Yalniz, Herv\u00e9 J\u00e9gou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised learning for image classification, 2019.\n\n- Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In ECCV, 2018.\n\n- Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In ECCV, pp. 374-390, 2018.\n\n- Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\n\n- Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, and Zheng-Jun Zha. Object relational graph with teacher-recommended learning for video captioning. In CVPR, 2020.\n\n- Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional videos. 2018a.\n\n- Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end dense video captioning with masked transformer. In CVPR, 2018b.\n\n- Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In CVPR, 2020.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638529.7566693", "msecs": "756.0", "relativeCreated": "126587.16678619385", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Relevance-Based Embeddings for Efficient Relevance Retrieval\n\n# Abstract\nIn many machine learning applications, the most relevant items for a particular query should be efficiently extracted. The relevance function is usually an expensive similarity model making the exhaustive search infeasible. A typical solution to this problem is to train another model that separately embeds queries and items to a vector space, where similarity is defined via the dot product or cosine similarity. This allows one to search the most relevant objects through fast approximate nearest neighbors search at the cost of some reduction in quality. To compensate for this reduction, the found candidates are re-ranked by the expensive similarity model. In this paper, we investigate an alternative approach that utilizes the relevances of the expensive model to make relevance-based embeddings (RBE). The idea is to describe each query (item) by its relevance for a set of support items (queries) and use these new representations to obtain query (item) embeddings. We theoretically prove that relevance-based embeddings are powerful enough to approximate any complex similarity model (under mild conditions). An important ingredient of RBE is the choice of support items. We investigate several strategies and demonstrate that significant improvements can be obtained compared to random choice. Our experiments on diverse datasets illustrate the power of relevance-based embeddings.\n\n\n# 1 INTRODUCTION\n\n\nFinding the most relevant element (item)  $i$  to a query  $q$  among a large set of candidates  $I$  is a key task for a wide range of machine learning problems, for example, information retrieval, recommender systems, question-answering systems, or search engines. In such problems, the final score (relevance) is often predicted by a pairwise function  $R: I \\times Q \\to \\mathbb{R}$ , where  $Q$  is a query space and  $R$  approximates some ground truth relevance such as click probability, time spent or something else. Depending on the task, the relevance function  $R$  can utilize query attributes (e.g., the text of the query or a set of numerical features describing the user, such as age, time spent on the service, etc.), item attributes, or attributes describing the query-item pair (e.g., statistics based on counts of each query term in the document in information retrieval tasks).\n\n\nThe problem of relevance retrieval for a query  $q$  can be written as  $\\arg \\max_{i\\in I}R(i,q)$ . For practical applications, it is usually required to return not one but  $K$  best items (for directly displaying to the user or further re-ranking). Most recommender systems are characterized by a large size of the item space  $I$  (millions to hundreds of millions), so an exhaustive search is not feasible. This problem is often solved by training an auxiliary model  $\\tilde{R}$ , called a Siamese, two-tower, or dual encoder (DE), in which late binding is used:  $\\tilde{R}(i,q) = S(F_I(i),F_Q(q))$ , where  $F_I:I\\to \\mathbb{R}^d$ ,  $F_Q:Q\\to \\mathbb{R}^d$ , and  $S$  is some lightweight similarity measure, usually dot product or cosine similarity.\n\n\nWhile a lot of effort has been put into developing dual-encoder models, the cross-encoder (CE) ones are generally more powerful (Wu et al., 2019; Yadav et al., 2022). Moreover, as mentioned above, in practice it is typical to also have features that describe a query-item pair: e.g., counts of query terms in the document (information retrieval), information about previous user-item interactions (recommender systems), and so on. Such features cannot be used by dual encoders.\n\n\nIn a recent paper, Yadav et al. (2022) suggested an alternative approach: to approximate the relevance of a given query to all the items using the relevance of this query to a fixed set of randomly chosen support items. In more detail, the authors apply the matrix factorization to the query-item relevance\n\n\nmatrix to represent it as a product of its submatrix containing only a few columns (relevances to support items) and some other, explicitly computable.\n\n\nMotivated by the above-mentioned work, we propose and analyze the concept of relevance-based embeddings (RBE). The main idea is to describe users by their relevance to some pre-selected support items and describe items by their relevance to some support users. Then, such representations can be used in various ways: as in Yadav et al. (2022), they can be multiplied by a certain matrix to obtain relevance approximations, or they can be passed into a neural network to potentially obtain better approximations (trained for a desired loss), or they can be additionally combined with the original node features to get even more powerful embeddings.\n\n\nWe theoretically prove the power of relevance-based embeddings. Namely, we show that (under mild conditions) they are informative enough to approximate any continuous similarity function. In particular, when the similarity function utilizes pairwise features, dual encoders based on the individual user and item features inevitably lose this information; however, relevance-based representations contain this information and thus can approximate the desired function.\n\n\nFrom a practical perspective, an important aspect of our approach is how to properly choose support elements. Previous studies (Morozov & Babenko, 2019; Yadav et al., 2022) sampled them uniformly at random, while we show that there is significant room for improvement. We investigate different options: from simple heuristics (e.g., popular or diverse elements) to methods directly optimizing the relevance approximation quality. Surprisingly, even very simple strategies like clustering the elements and choosing the cluster centers as support items already give significant improvements that can be further improved by more advanced and theoretically justified strategies.\n\n\nTo evaluate the performance of RBE, we conduct experiments on textual and recommendation datasets. We compare our approach with dual encoders and with Yadav et al. (2022) and get an average improvement of  $33\\%$  over this baseline for various datasets (from  $8\\%$  to  $69\\%$ , see Table 2).\n\n\n# 2 RELATED WORK\n\n\nIn this section, we discuss research areas and representative papers related to our study.\n\n\nRelevance retrieval problem is widespread in the context of building information retrieval systems (Kowalski, 2007), such as text search engines (Huang et al., 2013), image search (Gordo et al., 2016), entertainment recommender systems (Covington et al., 2016), question answering systems (Karpukhin et al., 2020), e-commerce systems (Yu et al., 2018), and other practical applications.\n\n\nUsually, such problems are solved by learning query and item embeddings into a certain space and then searching for approximate nearest elements in this space, followed by rearrangement using a heavier ranker. In particular, Covington et al. (2016); Huang et al. (2013) explicitly use this approach, offering two-tower models (a.k.a. dual encoders). Note that there are simple alternatives to dual encoders that use, e.g., BM25 scores applicable to texts (Logeswaran et al., 2019; Zhang & Stratos, 2021) or other cheaper or more expensive alternatives (Humeau et al., 2019; Luan et al., 2021). However, there is usually trading-off complexity for quality.\n\n\nIt is also worth mentioning the works trying to facilitate the training of the dual encoder through distilling a heavier ranker model (Wu et al., 2019; Hofst\u00e4tter et al., 2020; Lu et al., 2020; Qu et al., 2020; Liu et al., 2021). Although these works aim at simplifying the learning of light ranking using the heavy one, they differ from our approach in two aspects. First, distillation means that there are still two heavy (comparable in orders of magnitude of trainable parameters) models. Second, this approach requires two different complex model architectures that can be a significant disadvantage for, e.g., recommender systems with a wide variety of image/textual/statistical/sequential features.\n\n\nAs for the nearest neighbors search in a common query-item space, a wide variety of algorithms exist, including locality-sensitive hashing (LSH) (Indyk & Motwani, 1998; Andoni & Indyk, 2008), partition trees (Bentley, 1975; Dasgupta & Freund, 2008; Dasgupta & Sinha, 2013), and similarity graphs (Navarro, 2002). LSH-based and tree-based methods provide strong theoretical guarantees, however, it has been shown that graph-based methods usually perform better (Malkov & Yashunin, 2018; Aum\u00fcller et al., 2020), which explains their widespread use in practical applications.\n\n\nAnother research direction is methods that combine nearest neighbors search with heavy ranker calls (Morozov & Babenko, 2019; Chen et al., 2022) instead of separately embedding queries and items in a common space where the search for the nearest items can be efficiently performed. Such methods show better quality in comparison with separate embeddings, however, their practical application may be limited due to a significant change in the structure of the search index. In particular, in practice, microservices with neural networks and microservices with document indexes are different services, which allows for increasing GPU utilization on the one hand and using specialized (including sharded) solutions with a large amount of memory on the other. Therefore, in this paper, we focus on the basic scenario with a separate investment in space and a separate search for the nearest elements in it.\n\n\nA recent paper by Yadav et al. (2022) is the most relevant for our research. The idea is to apply the matrix factorization to the query-item relevance matrix in order to represent it as a product of its submatrix containing only a few columns (relevances for random support items) and some other, explicitly computable. Despite the simplicity of the idea and implementation, the authors have shown in detail the superiority of their algorithm over more complex approaches, such as dual encoders. Our work is motivated by this study: we show that the approach of Yadav et al. (2022) has theoretical guarantees and suggest several improvements that significantly boost the performance. As an enhancement of the original approach, in another paper, Yadav et al. (2023) proposed the idea of selecting support items per each query independently, but the time complexity of each query processing becomes linear in the number of elements, which makes the approach infeasible in most practical applications. On the contrary, our support items selection is performed in the pre-processing stage and does not increase the query time.\n\n\nFinally, the most recent article (Yadav et al., 2024) proposes an alternative solution to the problem. Their AXN algorithm dynamically learns the difference between the DE and CE (or other query-document embeddings) predictions for each query independently (at the query time). Learning the difference is carried out through iteratively choosing a set of anchor (supporting) elements, calculating CE scores for them, and learning linear regression with embeddings of these elements as features and CE scores as targets. Some disadvantages of this method are: 1) it requires previously trained embeddings of queries and documents; 2) it has increased query processing time due to the need for iterative refining of all item relevances for each query. For completeness of our study, we use the AXN algorithm as one of our baselines.\n\n\n# 3 RELEVANCE-BASED EMBEDDINGS\n\n\nIn general, the information (attributes) used to calculate the ground-truth item-to-query relevances  $R(i,q)$  can be divided into three types: depending only on the query  $q$ , only on the item  $i$ , and on both of them. The key problem when constructing separate embeddings of items and queries in the common space (that can be used for searching for the nearest elements) is the inability to use information that depends on both query and item, which lowers the quality of relevance search.\n\n\nTo address the above-mentioned issue, we introduce relevance-based representations that describe each query by its relevance to a pre-selected set of items and, vice-versa, each item by its relevance to a pre-selected set of queries. We prove that, under certain conditions, any relevance function can be well estimated using only such individual vectors.\n\n\n# 3.1 PRELIMINARIES\n\n\nLet  $Q$  and  $I$  be compact topological spaces of queries and items, respectively. Assume that we are given a relevance function  $R: I \\times Q \\to \\mathbb{R}$ . In practice,  $R$  is our relevance model which may be computationally expensive and rely on pairwise features.\n\n\nLet  $S_I \\subset I$  and  $S_Q \\subset Q$  be some finite ordered sets of support items and support queries:  $S_I = \\{i_1, \\ldots, i_m\\}$ ,  $S_Q = \\{q_1, \\ldots, q_n\\}$ . Let  $R(i, S_Q)$  be a relevance vector of the item  $i$  w.r.t. the set of support queries  $S_Q$ :  $R(i, S_Q) = (R(i, q_1), \\ldots, R(i, q_n))$ . Similarly,  $R(S_I, q)$  is a relevance vector of the query  $q$  w.r.t. the set of support items  $S_I$ :  $R(S_I, q) = (R(i_1, q), \\ldots, R(i_m, q))$ . By  $R(S_I, S_Q)$  we denote a relevance matrix composed in a similar way.\n\n\n# 3.2 CUR APPROXIMATION\n\n\nRelevance vectors can be utilized in different ways and one of the possible approaches is to use the CUR decomposition (this approach was used by Yadav et al. (2022)). Using our notation, the relevance for a query  $q$  is approximated as:\n\n\n$$\n\n$$\n\\tilde {R} (I, q) := \\langle R (I, S _ {Q}) \\times \\operatorname {p i n v} (R \\left(S _ {I}, S _ {Q}\\right)), R \\left(S _ {I}, q\\right) \\rangle , \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\operatorname{pinv}(X)$  is the pseudo-inverse matrix of  $X$ . As support queries, Yadav et al. (2022) take the set of train queries:  $S_{Q} = Q_{train} \\subset Q$ .\n\n\nRegarding the computational complexity, we note that the CUR approximation requires computing the matrix  $R(I, Q_{train})$  which takes  $O(M \\cdot |S_Q|) = (M \\cdot |Q_{train}|)$  CE calls, where  $M$  is the total number of items in a database, which can be infeasible for large databases.\n\n\nOur first theoretical result provides guarantees for the CUR approximation (1). Namely, we show that its regularized version approximates the true relevance arbitrarily precisely in  $L_{2}$ . Formally, let  $\\mathrm{pinv}_{\\lambda}(A) = (A^T A + \\lambda E)^{-1} A^T$  with  $E$  being the identity matrix. Then, the regularized CUR approximation  $\\mathrm{CUR}_{\\lambda}$  is defined by (1) with  $\\mathrm{pinv}_{\\lambda}$  instead of pinv. For  $\\mathrm{CUR}_{\\lambda}$ , we prove the following result (see Appendix A.3 for the proof).\n\n\nTheorem 3.1. Suppose that  $I$  and  $Q$  are equipped with the structure of a measure space and the integral of  $R^4 (i,q)$  over  $I\\times Q$  is finite. Then, the  $\\mathrm{CUR}_{\\lambda}$  approximation of  $R$  can be chosen arbitrarily close to the true  $R$  in  $L_{2}(I\\times Q)$  provided enough uniformly sampled support items and queries and sufficiently small  $\\lambda$ .\n\n\nOur result gives theoretical support for the good performance of the CUR decomposition demonstrated in Yadav et al. (2022). In the next section, we show that stronger theoretical guarantees can be provided if we allow transforming query and item vectors into more powerful embeddings.\n\n\n# 3.3 RELEVANCE-BASED EMBEDDINGS\n\n\nIn this section, we propose extending the CUR-based approximation by allowing transformations of relevance vectors, e.g., with a neural architecture. With such embeddings, we prove a stronger result: that any continuous relevance function can be uniformly approximated.\n\n\nWe say that a function on  $I$  is a relevance-based embedding if it has a representation of the form  $e_I(i) = f_I(R(i, S_Q), \\theta_I)$  where  $S_Q$  is a set of support queries and  $f_I$  is some ML-architecture with parameters  $\\theta_I$  which parametrizes a mapping  $\\mathbb{R}^n \\to \\mathbb{R}^d$ . Analogously,  $e_Q(q) = f_Q(R(S_I, q), \\theta_Q)$  is a relevance-based embedding of the query  $q$ .\n\n\nThe following theorem holds (the proof can be found in Appendix A.1 and the guarantees for the RBE approach on a sphere are discussed in Appendix A.2).\n\n\nTheorem 3.2. Let  $I$  and  $Q$  be compact topological spaces, and  $R: I \\times Q \\to \\mathbb{R}$  be a continuous function. Then,  $R$  can be uniformly approximated up to an arbitrarily small absolute error by a function  $\\tilde{R}(i, q)$ :\n\n\n$$\n\n$$\n\\tilde {R} (i, q) = \\left\\langle f _ {I} \\left(R (i, S _ {Q}), \\theta_ {I}\\right), f _ {Q} \\left(R \\left(S _ {I}, q\\right), \\theta_ {Q}\\right) \\right\\rangle , \\tag {2}\n$$\n\n$$\n\n\nwhere  $S_{I} \\in I$  and  $S_{Q} \\in Q$  are some finite sets of support items and queries and  $f_{I}, f_{Q}$  are neural architectures with the universal approximation property (e.g., MLPs).\n\n\nNote that compared to Theorem 3.1, this theorem has weaker requirements and stronger convergence. Trainable embeddings allow us to soften the requirements ( $R$  can be any continuous function) and prove uniform convergence instead of  $L_{2}$  convergence).\n\n\nThis theorem shows that the true relevance function can be uniformly approximated with arbitrary precision by some functions of the relevance vectors. Our relevance-based framework is visualized in Figure 1. Note that the CUR\n\n\n![Figure 1: RBE visualization: support queries are red, support items are yellow; test queries are blue, and their relevance scores for the support items are used to approximate the remaining values](images/5f9e39b563ef389679b0b558a9d3515a03276050af062573f6cf88e72b6c7790.jpg)\n\n\napproximation discussed in Section 3.2 fits our framework with  $\\theta_I \\coloneqq \\mathrm{pinv}(R(S_I, S_Q))$  and  $f_I(R(i, S_Q), \\theta_I) \\coloneqq R(i, S_Q) \\times \\theta_I$ ,  $f_Q(R(S_I, q), \\theta_Q) \\coloneqq R(S_I, q)$ . Extending the CUR approximation to arbitrary embeddings allows us to obtain even stronger approximation guarantees. Moreover, such trainable embeddings are more flexible as they can adapt to a particular quality function one aims to optimize.\n\n\nRegarding the computation complexity, the pre-processing step requires  $O(M \\cdot |S_Q|) + N$  computations of CE, where  $N$  is the size of the training set (this part is similar to the training of dual encoders). An advantage of RBE is that we may have a significantly smaller set of support queries  $|S_Q| \\ll |Q_{train}|$ , while still utilizing the remaining queries for training the mappings  $f_{Q}$  and  $f_{I}$ . Also, the number of CE computations can be further reduced to just  $O(N)$  if we replace the transformation  $f_{I}(R(i,S_{Q}),\\theta_{I})$  with trainable embeddings of items  $\\theta_I(i) \\in R^d$ , see Section 3.5 for the discussion.\n\n\nTo sum up, the main outcome of our analysis is that even though the relevance function  $R(\\cdot, \\cdot)$  is arbitrary (and, in particular, can rely on pairwise features only available for query-item pairs), it can be well approximated by relevance-based embeddings of individual users and items. This is a clear advantage of RBE over conventional dual-encoder models.\n\n\n# 3.4 SELECTING SUPPORT ITEMS\n\n\nLet us revise the statement of Theorem 3.2. The theorem states that there exist such sets  $S_{Q}$ ,  $S_{I}$  that the relevance function  $R$  could be effectively approximated by separated embeddings  $f_{I}, f_{Q}$ . In the related works (Morozov & Babenko, 2019; Yadav et al., 2022), this selection is random, implicitly assuming that the elements are in some sense equivalent. However, for example, when building a recommender service, the popularity of different objects has a strongly skewed distribution, which is why more information is known about a small set of highly popular items than about a large set of unpopular ones. Thus, it is natural to assume that the choice of support items may have a significant effect on performance. We investigate this direction and compare several approaches from simple heuristics to more complex ones optimizing the approximation quality.\n\n\n# SIMPLE HEURISTICS OUR HEURISTIC STRATEGIES INCLUDE:\n\n- - Random Support items are chosen uniformly at random (Morozov & Babenko, 2019; Yadav et al., 2022). For better reproducibility, we also present the results of using the first  $|S_I|$  items as support ones, assuming that the order of queries is pseudorandom.\n\n- - Popular As mentioned above, a recommender service usually has a small set of very popular elements that many users interact with. As a result, a lot of information can be collected from these interactions thus making the popular elements more informative. Since it is not always possible to get popularity explicitly, we consider the following surrogate: choose the objects with the highest average relevance for the training set.\n\n- - Clusters centers When it comes to the allocation of a representative subset of vectors, it is reasonable to consider the allocation of clusters. We consider various clustering algorithms and select the cluster centers as support elements. The number of clusters is set to the number of required support elements.\n\n- - Most diverse This strategy is a greedy algorithm maximizing the minimum distance between the support elements. We first choose the element furthest from the center (by Euclidean distance) and then, at each step, an element is selected whose minimum distance to the current support elements is maximal.\n\n\nThe approaches that require item representations can be applied to their relevance vectors.\n\n\n$l_{2}$ -greedy approach Let us now discuss a more theoretically justified approach that we call  $l_{2}$ -greedy. It aims at selecting the key elements that allow for a better approximation of the relevance matrix  $R(i,q)$ . In this strategy, we greedily select items so that the MSE error of the CUR approximation (Mahoney & Drineas, 2009) is minimized for the train queries.\n\n\nWe note that the CUR approximation replaces every item with a linear combination of support items so that the MSE between the true relevances and their CUR approximations on the train set of queries is minimized (see Appendix A.3 for more details). Our goal is to optimize the overall MSE for all items, which is:\n\n\n$$\n\n$$\n\\sum_ {i} \\| R (i, S _ {Q}) ^ {T} - R \\left(S _ {I}, S _ {Q}\\right) ^ {T} \\times \\operatorname {p i n v} \\left(R \\left(S _ {I}, S _ {Q}\\right) ^ {T}\\right) \\times R (i, S _ {Q}) ^ {T} \\| _ {2} ^ {2}. \\tag {3}\n$$\n\n$$\n\n\nWe minimize this expression over all possible choices of  $S_I$ ,  $|S_I| = m$ . We propose a greedy approach in which support items are selected one by one optimizing (3) at each step. Due to space constraints, the implementation details are placed in Appendix B.\n\n\nComputation complexity In their default implementation, support items selection strategies popular, cluster centers, most diverse, and  $l_{2}$ -greedy require computing the relevance scores of all items to support queries that can be done in  $O(M \\cdot n)$ . If this is infeasible, one can use downsampling to reduce the number of candidates for support items. Our preliminary experiments (see Appendix E) show that even significant downsampling gives reasonable performance of the obtained support items. Moreover, for heuristic approaches, instead of the relevance vectors, one can use cheaper embeddings (e.g., the original feature vectors) when they are available. Moreover, for cluster centers, there can be predefined clusters in data. For instance, in recommender services, it is typical that items are annotated with their categories that can be used as clusters without any additional cost.\n\n\n# 3.5 ADDITIONAL PRACTICAL CONSIDERATIONS\n\n\nDynamic set of items We note that relevance-based embeddings can naturally handle scenarios where the set of items frequently changes. The embeddings  $f_{I}(R(i, S_{Q}), \\theta_{I})$  can be easily calculated for the new items without the need of re-training the embedding model  $f_{I}$  (similarly to feature-based dual encoders). On the other hand, if the set of items  $I$  is finite or changes not so frequently (e.g., the set of movies currently available in a recommender service), the transformation  $f_{I}(R(i, S_{Q}), \\theta_{I})$  can be replaced with trainable embeddings  $\\theta_{I}(i) \\in R^{d}$ . In contrast to items, the query set  $Q$  cannot be assumed to be finite: queries can be represented by texts of unlimited length or characterized by real-valued features.\n\n\nImproving approximation quality As discussed above, it is natural to assume the heavy ranker  $R$  to be the most expensive part in terms of computational complexity. Thus, during the calculation of  $\\tilde{R}$ , we are mainly limited by the sizes of the support sets  $S_{Q}$  and  $S_{I}$ . In contrast, calculating  $f_{Q}$ ,  $f_{I}$ , or the dot product between them is assumed to be significantly cheaper. Thus,  $f_{Q}$  and  $f_{I}$  may embed the relevance vectors in a higher-dimensional space. In particular, this may help to eliminate the disadvantages of the dot product, in comparison with other (Shevkunov & Prokhorenkova, 2021) ways of measuring the distances or similarities between objects.\n\n\nWhile relevance-based embeddings have theoretical guarantees, they hold in the limit, when the sets of support queries and items are sufficiently large. In practice, to improve the approximation performance and reduce the number of CE calls per query, the mapping  $f_{Q}$  (or  $f_{I}$ ) could be extended by enriching it with the features of the original query (or item).\n\n\nScalability The cost of the pre-processing and the ways to scale it are discussed in Section 3. At the inference, we need to compute the query representation, which requires  $m$  relevance computations. Then, the inference is similar to dual encoders: the item representations are pre-calculated and placed in the Approximate Nearest Neighbours index like HNSW (Malkov & Yashunin, 2018), which takes the embedding of the query as input. These  $m$  additional computations are taken into account in our experiments: when comparing with dual encoders, we reduce the final re-ranking budget by  $m$ .\n\n\nOther applications Although the goal of RBEs is to approximate the relevance function, relevance vectors can also be considered good general representations. Thus, their application is not limited to relevance predictions, as shown below in Appendix D.2.\n\n\n# 4 EXPERIMENTAL SETUP\n\n\nLet us describe our experimental setup. In all datasets that we use in this work, there is a heavy ranker that provides relevance, which we consider to be close to the ground truth. With this ranker, we build the complete table  $(R: I \\times Q \\to \\mathbb{R})$  of the relevance scores. We denote the predicted scores by  $\\hat{R}$ .\n\n\nThe task is to find the most relevant items for a given query. The quality is evaluated as\n\n\n$$\n\n$$\n\\operatorname {H i t R a t e} (P, T) := \\sum_ {q _ {i} \\in Q _ {\\text {t e s t}}} \\frac {| \\operatorname {B e s t} _ {P} (\\hat {R} , q _ {i}) \\cap \\operatorname {B e s t} _ {T} (R , q _ {i}) |}{| \\operatorname {B e s t} _ {T} (R , q _ {i}) | | Q _ {\\text {t e s t}} |}, \\operatorname {H i t R a t e} (K) := \\operatorname {H i t R a t e} (K, K),\n$$\n\n$$\n\n\nwhere  $\\operatorname{Best}_K(\\mathcal{R}, q) \\subset I$  is defined as the set of  $K$  items  $i_1, \\ldots, i_K$  with the highest relevances  $\\mathcal{R}(i_1, q), \\ldots, \\mathcal{R}(i_K, q)$  to a given query  $q \\in Q$ ;  $Q_{test}$  is a set of test queries that do not participate in the training:  $S_Q \\subset Q_{train}$ ,  $Q_{train} \\sqcup Q_{test} = Q$ . For all our experiments,  $|Q_{test}| \\approx 0.3|Q|$ ,  $|S_I| = 100$ ,  $S_Q = Q_{train}$ .\n\n\n# 4.1 DATASETS\n\n\nZESHEL The Zero-Shot Entity Linking (ZESHEL) dataset was constructed by Logeswaran et al. (2019) from Wikia. The task of zero-shot entity linking is to link mentions of objects in the text to an object from the list of entities with related descriptions. The dataset consists of 16 different domains. Each domain contains disjoint sets of entities, and during testing, mentions should be associated with invisible entities solely based on entity descriptions. We run the experiments on five domains from ZESHEL selected by Yadav et al. (2022). As a heavy ranker  $R$ , we use the cross-encoder trained by Yadav et al. (2022) and publicly available. Table 7 in Appendix shows the dataset statics for the domains used in this paper.\n\n\nQuestion-Answering To additionally cover the question-answering domain, we conducted experiments on the MsMarco (Nguyen et al., 2016) based dataset provided by huggingface. As a heavy ranker, we use all-mpnet-base-v2 from the SentenceTransformers library (Reimers & Gurevych, 2019), which is trained, among other datasets, on the MsMarco data. We took  $\\sim 10\\mathrm{K}$  test queries and 0.8M passages corresponding to them (QA). For the experiments in Table 1, we used the smaller (QA.Small) version with 82K passages (only test passages).\n\n\nRecSys To evaluate the generalization of the proposed approach to other tasks and domains, we collected a dataset from a production service providing recommendations of items to users. As a heavy ground-truth ranker  $R$ , we use the CatBoost gradient boosting model (Prokhorenkova et al., 2018) trained on a wide range of features, including categories and other static attributes of items, social information (age, language, etc.) of users, simple item statistics, user statistics, real-time statistics on user and item interaction, factors derived from the matrix factorizations, and multiple two-tower neural networks, receiving the features listed above as their features.\n\n\nTwo versions of the dataset are presented. In the first one, CatBoost was trained to predict the time that a user is going to spend on a given item immediately after the click (in one session). In the second version, CatBoost was trained on the pairwise PairLogit target to predict the item with the longest time spent for some long time after the click (including new sessions). The first version is denoted in the tables as RecSysLT and the second as RecSys. This dataset allows us to evaluate the generalizability of our approach across different domains and different types of heavy rankers since gradient-boosting models differ significantly from neural approaches.\n\n\nRecSys2 To further increase diversity of the considered domains, we collected a dataset from another production recommender service. Here, the CatBoost gradient boosting model is used as a\n\n\nTable 1: Support item selection applied to the CUR relevance approximation, HitRate(100) (larger is better) is reported\n\n\n<table><tr><td>Support items</td><td>Yugioh</td><td>P.Wrest.</td><td>StarTrek</td><td>Dr.Who</td><td>Military</td><td>RecSys</td><td>RecSysLT</td><td>RecSys2</td><td>QA.Small</td></tr><tr><td>random (i.e. AnnCUR)</td><td>0.4724</td><td>0.4280</td><td>0.2287</td><td>0.1919</td><td>0.2455</td><td>0.6697</td><td>0.5842</td><td>0.1478</td><td>0.5828</td></tr><tr><td>first 100</td><td>0.4845</td><td>0.4182</td><td>0.2489</td><td>0.1975</td><td>0.2599</td><td>0.6490</td><td>0.5609</td><td>0.1551</td><td>0.5491</td></tr><tr><td>popular</td><td>0.2429</td><td>0.3001</td><td>0.1154</td><td>0.1197</td><td>0.1907</td><td>0.7623</td><td>0.6695</td><td>0.1422</td><td>0.5536</td></tr><tr><td>KMeans</td><td>0.5083</td><td>0.4850</td><td>0.3226</td><td>0.2517</td><td>0.3042</td><td>0.7070</td><td>0.6184</td><td>0.1661</td><td>0.5578</td></tr><tr><td>BisectingKMeans</td><td>0.4825</td><td>0.4592</td><td>0.2839</td><td>0.2159</td><td>0.2752</td><td>0.7035</td><td>0.6213</td><td>0.1483</td><td>0.5394</td></tr><tr><td>MiniBatchKMeans</td><td>0.5077</td><td>0.4737</td><td>0.2912</td><td>0.2365</td><td>0.2826</td><td>0.7033</td><td>0.5981</td><td>0.1721</td><td>0.5529</td></tr><tr><td>AgglomerativeClustering</td><td>0.5105</td><td>0.4911</td><td>0.3264</td><td>0.2531</td><td>0.2448</td><td>0.7050</td><td>0.6265</td><td>0.1557</td><td>0.5666</td></tr><tr><td>SpectralCocclustering</td><td>0.4618</td><td>0.4443</td><td>0.2540</td><td>0.2076</td><td>0.2551</td><td>0.6998</td><td>0.6094</td><td>0.1594</td><td>0.5415</td></tr><tr><td>SpectralBiclustering</td><td>0.4654</td><td>0.4708</td><td>0.2628</td><td>0.1845</td><td>0.2533</td><td>0.7409</td><td>0.5972</td><td>0.1607</td><td>0.5358</td></tr><tr><td>SpectralClusteringNN</td><td>0.5087</td><td>0.4690</td><td>0.2742</td><td>0.2048</td><td>0.2507</td><td>0.6936</td><td>0.5740</td><td>0.2343</td><td>0.5741</td></tr><tr><td>ByMin</td><td>0.5333</td><td>0.4290</td><td>0.3325</td><td>0.2278</td><td>0.2483</td><td>0.6504</td><td>0.6182</td><td>0.1329</td><td>0.5925</td></tr><tr><td>l2-greedy</td><td>0.5618</td><td>0.5119</td><td>0.3677</td><td>0.2960</td><td>0.3357</td><td>0.7197</td><td>0.6565</td><td>0.1478</td><td>0.6100</td></tr></table>\n\n\nTable 2: Evaluating neural relevance-based embeddings, HitRate(100) (larger is better) is reported\n\n\n<table><tr><td>Model</td><td>Yugioh</td><td>P.Wrest.</td><td>StarTrek</td><td>Dr.Who</td><td>Military</td><td>RecSys</td><td>RecSysLT</td><td>RecSys2</td><td>QA</td></tr><tr><td>Popular</td><td>0.0917</td><td>0.2410</td><td>0.0884</td><td>0.0821</td><td>0.1127</td><td>0.5077</td><td>0.2886</td><td>0.0142</td><td>0.0001</td></tr><tr><td>AnnCUR</td><td>0.4724</td><td>0.4280</td><td>0.2287</td><td>0.1919</td><td>0.2455</td><td>0.6697</td><td>0.5842</td><td>0.1478</td><td>0.5522</td></tr><tr><td>AnnCUR+KMeans</td><td>0.5083</td><td>0.4850</td><td>0.3226</td><td>0.2517</td><td>0.3042</td><td>0.7070</td><td>0.6184</td><td>0.1661</td><td>0.5027</td></tr><tr><td>RBE+KMeans</td><td>0.5431</td><td>0.4979</td><td>0.3399</td><td>0.2539</td><td>0.3019</td><td>0.7137</td><td>0.6300</td><td>0.3729</td><td>0.5505</td></tr><tr><td>AnnCUR+l2-greedy</td><td>0.5618</td><td>0.5119</td><td>0.3677</td><td>0.2960</td><td>0.3357</td><td>0.7197</td><td>0.6565</td><td>0.1478</td><td>0.5700</td></tr><tr><td>RBE+l2-greedy</td><td>0.5849</td><td>0.5249</td><td>0.3867</td><td>0.2992</td><td>0.3349</td><td>0.7234</td><td>0.6682</td><td>0.3964</td><td>0.6022</td></tr></table>\n\n\nheavy ranker and a dual encoder as a baseline. Both CE and DE are trained on a large set of external data and features. A smaller subsample of this data was used to train RBE, since we believe that RBE is able to show good quality even on a significantly smaller size of the training data.\n\n\n# 4.2 BASELINES\n\n\nAs our main baseline, we consider the AnnCUR (Yadav et al., 2022) recommendation algorithm that approximates relevances with the CUR decomposition as discussed in Section 3.2. What is important for further discussion, a broad comparison of this method with different basic approaches, including various dual encoders, is carried out by Yadav et al. (2022). In most of our experiments, we rely on these results, comparing only with AnnCUR. However, we explicitly provide the comparison with dual encoders for the new datasets RecSysLT and RecSys in Section 5. For better interpretability of the results, we also provide metrics for a baseline that always selects the most popular items (not to be confused with Table 1, where the \"popular\" refers to selecting support items).\n\n\n# 4.3 RBE IMPLEMENTATION\n\n\nFollowing Theorem 3.2, we train lightweight neural networks  $f_{I}$  and  $f_{Q}$ , which are significantly faster than the heavy ranker  $R$ , and independently transform the relevance vectors  $R(i,S_{Q}),R(S_{I},q)$  into embeddings. The training is performed by the Adam algorithm on sampled batches with a listwise loss function (see Appendix C), similar to the training of various DEs.\n\n\nAs follows from Section 3.2, the CUR decomposition gives a reasonably good approximation of the relevance function. Hence, we split the RBE representation into the CUR representation and the trainable prediction of its error. In the experiments, such decomposition improves the convergence and training stability. Technical implementation details are placed in Appendix C.\n\n\n# 5 EXPERIMENTAL RESULTS\n\n\nSupport items selection Following Section 3.4, we check various ways of choosing support elements as opposed to the existing approaches that use random selection. All clustering algorithms are taken from the scikit-learn (Pedregosa et al., 2011) library, SpectralClusteringNN is a SpectralClustering with \"nearest neighbors\" affinity. The algorithms are used with their default parameters since even this simple setting already allows us to get significant improvements over the random selection.\n\n\nThe results are shown in Table 1, where the best three results for each dataset are highlighted. Clearly, there is a significant superiority of almost any approach based on clustering or diversity over the random selection. The theoretically justified  $l_{2}$ -greedy algorithm is the clear winner, second and third places are taken by KMeans and AgglomerativeClustering. However, due to the significantly worse quality of AgglomerativeClustering on the Military dataset, KMeans will be used in further experiments. Another observation is that on RecSys and RecSysLT, there is a clear superiority of the choice of popular items as the support ones. It is worth mentioning that for this dataset, the elements extracted by popularity are also quite stratified by their categories and an explicit restriction on the number of elements from one category changes the top slightly. However, this may not be true for other data (e.g., we do not observe a similar feature for RecSys2).\n\n\nNeural relevance-based embeddings Following the description in Section 4.3, we also apply non-trivial trainable relevance mappings  $f_{I}(R(I,S_{Q}),\\theta_{I}),f_{Q}(R(S_{I},q),\\theta_{Q})$  to check whether this modification improves prediction quality in practice. The results are shown in Table 2. To better interpret the values, the quality of the constant output consisting of popular (in the same sense as in the previous paragraph) elements is also given. It can be seen that in most cases, except for one dataset (Military), trainable relevance mappings improve the final quality of the search for relevant elements and the improvements are obtained for both KMeans and  $l_{2}$ -greedy support elements selection. Improvement from the trainable mappings is most noticeable for the QA dataset. Note that the transformation that we use is not claimed to be optimal and is given rather to demonstrate that with the help of an easy transformation, one can get an increased quality on various datasets.\n\n\nTable 3: Dual encoder embeddings vs support relevances, different tops, RecSysLT\n\n\n<table><tr><td>X</td><td></td><td>100</td><td>200</td><td>300</td><td>400</td><td>500</td><td>600</td><td>700</td><td>800</td><td>900</td></tr><tr><td>Dual Encoder</td><td>HR(X+100, X)</td><td>0.7048</td><td>0.6803</td><td>0.6739</td><td>0.6739</td><td>0.6760</td><td>0.6792</td><td>0.6827</td><td>0.6868</td><td>0.6904</td></tr><tr><td>AXNDE</td><td>HR(X+100, X)</td><td>0.7065</td><td>0.6769</td><td>0.6740</td><td>0.6769</td><td>0.6820</td><td>0.6883</td><td>0.6958</td><td>0.7054</td><td>0.7161</td></tr><tr><td>RBE+l2-greedy</td><td>HR(X, X)</td><td>0.6682</td><td>0.6955</td><td>0.7221</td><td>0.7406</td><td>0.7538</td><td>0.7639</td><td>0.7720</td><td>0.7792</td><td>0.7853</td></tr></table>\n\n\nTable 4: Dual encoder embeddings vs support relevances, different tops, RecSys2\n\n\n<table><tr><td>X</td><td></td><td>100</td><td>200</td><td>300</td><td>400</td><td>500</td><td>600</td><td>700</td><td>800</td><td>900</td></tr><tr><td>Dual Encoder</td><td>HR(X+100, X)</td><td>0.3792</td><td>0.3198</td><td>0.2928</td><td>0.2784</td><td>0.2702</td><td>0.2661</td><td>0.2647</td><td>0.2652</td><td>0.2671</td></tr><tr><td>AXNDE</td><td>HR(X+100, X)</td><td>0.3843</td><td>0.3333</td><td>0.3015</td><td>0.2879</td><td>0.2839</td><td>0.2835</td><td>0.2836</td><td>0.2855</td><td>0.2893</td></tr><tr><td>RBE+l2-greedy</td><td>HR(X, X)</td><td>0.3964</td><td>0.4471</td><td>0.4693</td><td>0.4833</td><td>0.4929</td><td>0.5008</td><td>0.5070</td><td>0.5119</td><td>0.5162</td></tr></table>\n\n\nTable 5: Dual encoder embeddings vs support relevances, RecSysLT\n\n\n<table><tr><td>K</td><td></td><td>100</td><td>200</td><td>300</td><td>400</td><td>500</td><td>600</td><td>700</td><td>800</td><td>900</td></tr><tr><td>Dual Encoder</td><td>HR(K+100, 100)</td><td>0.7048</td><td>0.7977</td><td>0.8518</td><td>0.8855</td><td>0.9086</td><td>0.9258</td><td>0.9385</td><td>0.9484</td><td>0.9561</td></tr><tr><td>AXNDE</td><td>HR(K+100, 100)</td><td>0.7065</td><td>0.7970</td><td>0.8538</td><td>0.8902</td><td>0.9153</td><td>0.9331</td><td>0.9465</td><td>0.9572</td><td>0.9660</td></tr><tr><td>RBE+l2-greedy</td><td>HR(K, 100)</td><td>0.6682</td><td>0.8359</td><td>0.9026</td><td>0.9342</td><td>0.9522</td><td>0.9632</td><td>0.9704</td><td>0.9760</td><td>0.9799</td></tr></table>\n\n\nComparison with dual encoders In this part, we compare RBE with dual encoders. For RecSys and RecSys2 datasets, we consider the embeddings produced by the dual encoder used in the production of the service (i.e., the one that is proved to be the best in this task). To fairly compare the performance, we replace the relevance vectors in the RecSys experiments with embeddings obtained by the DE, keeping the transformation of the relevance vectors and the training parameters unchanged. The only difference between the pipelines for RBE and DE is that since we used  $|S_I| = 100$  requests to a heavy ranker to obtain an RBE, we have to decrease the budget for RBE CE calls when calculating the top: compared to RBE, DE uses  $|S_I|$  more CE calls for the re-ranking. We also applied the AXN $^6$  method in addition to DE-embedding as an alternative approach to improve DE-like methods using CE calls. Similarly to DE, AXN uses an increased budget for CE calls. Thus, for the dual encoder and AXN, we use the metric HitRate  $(X + |S_I|, X)$  and for RBE \u2014 HitRate  $(X, X)$ , which gives\n\n\nTable 6: Dual encoder embeddings vs support relevances, RecSys2\n\n\n<table><tr><td>K</td><td></td><td>100</td><td>200</td><td>300</td><td>400</td><td>500</td><td>600</td><td>700</td><td>800</td><td>900</td></tr><tr><td>Dual Encoder</td><td>HR(K+100, 100)</td><td>0.3792</td><td>0.4228</td><td>0.4514</td><td>0.4738</td><td>0.4927</td><td>0.5091</td><td>0.5239</td><td>0.5373</td><td>0.5493</td></tr><tr><td>AXNDE</td><td>HR(K+100, 100)</td><td>0.3843</td><td>0.4296</td><td>0.4557</td><td>0.4834</td><td>0.5020</td><td>0.5198</td><td>0.5351</td><td>0.5485</td><td>0.5630</td></tr><tr><td>RBE+l2-greedy</td><td>HR(K, 100)</td><td>0.3964</td><td>0.5435</td><td>0.6253</td><td>0.6819</td><td>0.7253</td><td>0.7599</td><td>0.7888</td><td>0.8135</td><td>0.8343</td></tr></table>\n\n\nthe former an advantage for small top sizes. However, starting from  $X = 200$ , our algorithm is superior to both dual encoder and  $AXN_{DE}$  baselines, as can be seen from Table 3. As for Table 4, our algorithm is superior to both dual encoder and  $AXN_{DE}$  baselines from  $X = 100$ .\n\n\nA second comparison, where the size of the desired top is fixed, while the number of extracted elements changes, is presented in Tables 5 and 6. Similarly to the previous comparison, RBE calls to the heavy ranker are taken into account: for the dual encoder and AXN, HitRate  $(K + |S_I|, 100)$  is calculated, and for RBE \u2014 HitRate  $(K, 100)$ . In this comparison, RBE outperforms the dual encoder and  $AXN_{DE}$  starting at  $K = 200$  for RecSysLT and at  $K = 100$  for RecSys2. Let us note that in the production service, the actual size of the top used to select candidates before ranking exceeds the values indicated in the table.\n\n\nA similar comparison with the dual encoder on the data from ZESHEL can be found in Yadav et al. (2022): it is shown that AnnCUR outperforms the dual encoder. Thus, on ZESHEL we compare only with AnnCUR.\n\n\nTransfer Learning Often in practice, embeddings trained to solve one problem are also applied to other ones. Following this, we conducted an additional experiment on predicting the categories of items in the RecSys dataset and obtained that the relevance vectors are informative vector representations. Moreover, the proposed algorithms for selecting key elements improve the quality of the prediction. Due to space constraints, the detailed results are provided in Appendix D.2.\n\n\n# 6 CONCLUSION\n\n\nIn this paper, we present the concept of relevance-based embeddings. We justify our approach theoretically and show its practical effectiveness on various datasets. We demonstrate that RBE allows one to obtain better quality compared with existing approaches. An important contribution of our work is the study of different strategies for choosing the support elements for RBE: we show that a proper choice of the support elements allows one to significantly boost performance.\n\n\nPromising directions for future research include a deeper investigation of support element selection strategies as well as applying the proposed RBE to other algorithms, e.g., based on using a heavy ranker during the nearest neighbor search (Morozov & Babenko, 2019) or adaptive nearest neighbor search (Yadav et al., 2024). Regarding the latter approach, we note that  $AXN$  can be naturally combined with any query-item representations, e.g., RBE. The obtained  $AXN_{RBE}$  model has to perform at least as well as RBE due to the presence of the regularization that balances between the query-item representation and the proposed modification.\n\n\n# REFERENCES\n\n- Andoni, A. and Indyk, P. Near-optimal hashing algorithms for near neighbor problem in high dimension. Communications of the ACM, 51(1):117-122, 2008.\n\n- Aum\u00fcller, M., Bernhardsson, E., and Faithfull, A. Ann-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. Information Systems, 87:101374, 2020.\n\n- Bentley, J. L. Multidimensional binary search trees used for associative searching. Communications of the ACM, 18(9):509-517, 1975.\n\n- Chen, R., Liu, B., Zhu, H., Wang, Y., Li, Q., Ma, B., Hua, Q., Jiang, J., Xu, Y., Deng, H., et al. Approximate nearest neighbor search under neural similarity metric for large-scale recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pp. 3013-3022, 2022.\n\n- Covington, P., Adams, J., and Sargin, E. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems, pp. 191-198, 2016.\n\n- Cybenko, G. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303-314, 1989.\n\n- Dasgupta, S. and Freund, Y. Random projection trees and low dimensional manifolds. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pp. 537-546, 2008.\n\n- Dasgupta, S. and Sinha, K. Randomized partition trees for exact nearest neighbor search. In Conference on learning theory, pp. 317-337. PMLR, 2013.\n\n- Gordo, A., Almaz\u00e1n, J., Revaud, J., and Larlus, D. Deep image retrieval: Learning global representations for image search. In Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, pp. 241-257. Springer, 2016.\n\n- Hofst\u00e4tter, S., Althammer, S., Schr\u00f6der, M., Sertkan, M., and Hanbury, A. Improving efficient neural ranking models with cross-architecture knowledge distillation. arXiv preprint arXiv:2010.02666, 2020.\n\n- Huang, P.-S., He, X., Gao, J., Deng, L., Acero, A., and Heck, L. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management, pp. 2333-2338, 2013.\n\n- Humeau, S., Shuster, K., Lachaux, M.-A., and Weston, J. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. arXiv preprint arXiv:1905.01969, 2019.\n\n- Indyk, P. and Motwani, R. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pp. 604-613, 1998.\n\n- Karpukhin, V., O\u011fuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\n\n- Kowalski, G. J. Information retrieval systems: theory and implementation, volume 1. springer, 2007.\n\n- Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural networks, 6(6):861-867, 1993.\n\n- Liu, F., Jiao, Y., Massiah, J., Yilmaz, E., and Havrylov, S. Trans-encoder: unsupervised sentence-pair modelling through self-and mutual-distillations. arXiv preprint arXiv:2109.13059, 2021.\n\n- Logeswaran, L., Chang, M.-W., Lee, K., Toutanova, K., Devlin, J., and Lee, H. Zero-shot entity linking by reading entity descriptions. arXiv preprint arXiv:1906.07348, 2019.\n\n- Lu, W., Jiao, J., and Zhang, R. Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp. 2645\u20132652, 2020.\n\n- Luan, Y., Eisenstein, J., Toutanova, K., and Collins, M. Sparse, dense, and attentional representations for text retrieval. Transactions of the Association for Computational Linguistics, 9:329-345, 2021.\n\n- Mahoney, M. W. and Drineas, P. CUR matrix decompositions for improved data analysis. Proceedings of the National Academy of Sciences, 106(3):697-702, 2009.\n\n- Malkov, Y. A. and Yashunin, D. A. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4):824-836, 2018.\n\n- Morozov, S. and Babenko, A. Relevance proximity graphs for fast relevance retrieval. arXiv preprint arXiv:1908.06887, 2019.\n\n- Navarro, G. Searching in metric spaces by spatial approximation. The VLDB Journal, 11:28-46, 2002.\n\n- Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. MS MARCO: A human generated machine reading comprehension dataset. CoRR, abs/1611.09268, 2016. URL http://arxiv.org/abs/1611.09268.\n\n- Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournaepau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.\n\n- Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., and Gulin, A. Catboost: unbiased boosting with categorical features. Advances in neural information processing systems, 31, 2018.\n\n- Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W. X., Dong, D., Wu, H., and Wang, H. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2010.08191, 2020.\n\n- Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084.\n\n- Shevkunov, K. and Prokhorenkova, L. Overlapping spaces for compact graph representations. Advances in Neural Information Processing Systems, 34:11665-11677, 2021.\n\n- Wu, L., Petroni, F., Josifoski, M., Riedel, S., and Zettlemoyer, L. Scalable zero-shot entity linking with dense entity retrieval. arXiv preprint arXiv:1911.03814, 2019.\n\n- Yadav, N., Monath, N., Angell, R., Zaheer, M., and McCallum, A. Efficient nearest neighbor search for cross-encoder models using matrix factorization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2171\u20132194, 2022.\n\n- Yadav, N., Monath, N., Zaheer, M., and McCallum, A. Adaptive selection of anchor items for cur-based k-nn search with cross-encoders. arXiv preprint arXiv:2305.02996, 2023.\n\n- Yadav, N., Monath, N., Zaheer, M., Fergus, R., and McCallum, A. Adaptive retrieval and scalable indexing for k-nn search with cross-encoders. International Conference on Learning Representations, 2024.\n\n- Yu, J., Qiu, M., Jiang, J., Huang, J., Song, S., Chu, W., and Chen, H. Modelling domain relationships for transfer learning on retrieval-based question answering systems in e-commerce. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pp. 682-690, 2018.\n\n- Zhang, W. and Stratos, K. Understanding hard negatives in noise contrastive estimation. arXiv preprint arXiv:2104.06245, 2021.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638531.1304603", "msecs": "130.0", "relativeCreated": "127960.95776557922", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach\n\n# Abstract\nMany tasks in modern machine learning can be formulated as finding equilibria in sequential games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have received growing interest. It is tempting to apply gradient descent to solve minimax optimization given its popularity and success in supervised learning. However, it has been noted that naive application of gradient descent fails to find some local minimax and can converge to non-local-minimax points. In this paper, we propose Follow-the-Ridge (FR), a novel algorithm that provably converges to and only converges to local minimax. We show theoretically that the algorithm addresses the notorious rotational behaviour of gradient dynamics, and is compatible with preconditioning and positive momentum. Empirically, FR solves toy minimax problems and improves the convergence of GAN training compared to the recent minimax optimization algorithms.\n\n\n# 1 INTRODUCTION\n\n\nWe consider differentiable sequential games with two players: a leader who can commit to an action, and a follower who responds after observing the leader's action. Particularly, we focus on the zero-sum case of this problem which is also known as minimax optimization, i.e.,\n\n\n$$\n\n$$\n\\min_{\\mathbf{x}\\in \\mathbb{R}^{n}}\\max_{\\mathbf{y}\\in \\mathbb{R}^{m}}f(\\mathbf{x},\\mathbf{y}).\n$$\n\n$$\n\n\nUnlike simultaneous games, many practical machine learning algorithms, including generative adversarial networks (GANs) (Goodfellow et al., 2014; Arjovsky et al., 2017), adversarial training (Madry et al., 2018) and primal-dual reinforcement learning (Du et al., 2017; Dai et al., 2018), explicitly specify the order of moves between players and the order of which player acts first is crucial for the problem. Therefore, the classical notion of local Nash equilibrium from simultaneous games may not be a proper definition of local optima for sequential games since mini-\n\n\nmax is in general not equal to maximin. Instead, we consider the notion of local minimax (Jin et al., 2019) which takes into account the sequential structure of minimax optimization.\n\n\n![Figure 1: For a quadratic function  $f(x,y) = -3x^{2} + 4xy - y^{2}$ , our algorithm moves closer to the ridge every iteration and it moves along the ridge once it hits the ridge. Without the FR correction term, gradient dynamics can drift away from the ridge.](images/74ceaa5f2fec2357e20238a70a5fef054c663497c556827fa6adee229e02c220.jpg)\n\n\nThe vanilla algorithm for solving sequential minimax optimization is gradient descent-ascent (GDA), where both players take a gradient update simultaneously. However, GDA is known to suffer from two drawbacks. First, it has undesirable convergence properties: it fails to converge to some local minimax and can converge to fixed points that are not local minimax (Jin et al., 2019; Daskalakis and Panageas, 2018). Second, GDA exhibits strong rotation around fixed points, which requires using very small learning rates (Mescheder et al., 2017; Balduzzi et al., 2018) to converge.\n\n\nIn this paper, we propose Follow-the-Ridge (FR), an algorithm for minimax optimization that addresses both issues. Specifically, we elucidate the cause of undesirable convergence of GDA \u2013 the leader whose gradient step takes the system away from the ridge. By adding a correction term to the follower, we explicitly cancel out negative effects of the leader's update. Intuitively, the combination of the leader's update and the correction term is parallel to the ridge in the landscape (see Fig. 1), hence the name Follow-the-Ridge. Overall, our contributions are the following:\n\n- - We propose a novel algorithm for minimax optimization which has exact local convergence to local minimax points. Previously, this property was only known to be satisfied when the leader moves infinitely slower than the follower in gradient descent-ascent (Jin et al., 2019).\n\n- - We show theoretically and empirically that FR addresses the notorious rotational behaviour of gradient dynamics around fixed points (Balduzzi et al., 2018) and thus allows a much larger learning rate compared to GDA.\n\n- - We prove that our algorithm is compatible with standard acceleration techniques such as preconditioning and positive momentum, which can speed up convergence significantly.\n\n- - We further show that our algorithm also applies to general-sum Stackelberg games (Fiez et al., 2019; Zeuthen, 1935) with similar theoretical guarantees.\n\n- - Finally, we demonstrate empirically our algorithm improves the convergence performance in both toy minimax problems and GAN training compared to existing methods.\n\n\n# 2 PRELIMINARIES\n\n\n# 2.1 MINIMAX OPTIMIZATION\n\n\nWe consider sequential games with two players where one player is deemed the leader and the other the follower. We denote leader's action by  $\\mathbf{x} \\in \\mathbb{R}^n$ , and the follower's action by  $\\mathbf{y} \\in \\mathbb{R}^m$ . The leader aims at minimizing the cost function  $f(\\mathbf{x}, \\mathbf{y})$  while the follower aims at maximizing  $f(\\mathbf{x}, \\mathbf{y})$ . The only assumption we make on the cost function is the following.\n\n\nAssumption 1.  $f$  is twice differentiable everywhere, and thrice differentiable at critical points.  $\\nabla_{\\mathbf{yy}}^2 f$  is invertible (i.e., non-singular).\n\n\nThe global solution to the sequential game  $\\min_{\\mathbf{x}}\\max_{\\mathbf{y}}f(\\mathbf{x},\\mathbf{y})$  is an action pair  $(\\mathbf{x}^{*},\\mathbf{y}^{*})$ , such that  $\\mathbf{y}^*$  is the global optimal response to  $\\mathbf{x}^*$  for the follower, and that  $\\mathbf{x}^*$  is the global optimal action for the leader assuming the follower always play the global optimal response. We call this global solution the global minimax. However, finding this global minimax is often intractable; therefore, we follow Jin et al. (2019) and take local minimax as the local surrogate.\n\n\nDefinition 1 (local minimax).  $(\\mathbf{x}^{*},\\mathbf{y}^{*})$  is a local minimax for  $f(\\mathbf{x},\\mathbf{y})$  if (1)  $\\mathbf{y}^*$  is a local maximum of  $f(\\mathbf{x}^{*},\\cdot)$ ; (2)  $\\mathbf{x}^*$  is a local minimum of  $\\phi (\\mathbf{x})\\coloneqq f(\\mathbf{x},r(\\mathbf{x}))$ , where  $r(\\mathbf{x})$  is the implicit function defined by  $\\nabla_{\\mathbf{y}}f(\\mathbf{x},\\mathbf{y}) = 0$  in a neighborhood of  $\\mathbf{x}^*$  with  $r(\\mathbf{x}^{*}) = \\mathbf{y}^{*}$ .\n\n\nIn the definition above, the implicit function  $r(\\cdot):\\mathbb{R}^n\\to \\mathbb{R}^m$  is a local best response for the follower, and is a ridge in the landscape of  $f(\\mathbf{x},\\mathbf{y})$ . Local minimaxity captures an equilibrium in a two-player sequential game if both players are only allowed to change their strategies locally. For notational convenience, we define\n\n\n$$\n\n$$\n\\nabla f (\\mathbf {x}, \\mathbf {y}) = \\left[ \\nabla_ {\\mathbf {x}} f, \\nabla_ {\\mathbf {y}} f \\right] ^ {\\top}, \\nabla^ {2} f (\\mathbf {x}, \\mathbf {y}) = \\left[ \\begin{array}{c c} \\mathbf {H} _ {\\mathbf {x x}} & \\mathbf {H} _ {\\mathbf {x y}} \\\\ \\mathbf {H} _ {\\mathbf {y x}} & \\mathbf {H} _ {\\mathbf {y y}} \\end{array} \\right].\n$$\n\n$$\n\n\nIn principle, local minimax can be characterized in terms of the following first-order and second-order conditions, which were established in Jin et al. (2019).\n\n\nProposition 1 (First-order Condition). Any local minimax  $(\\mathbf{x}^*,\\mathbf{y}^*)$  satisfies  $\\nabla f(\\mathbf{x}^*,\\mathbf{y}^*) = 0$ .\n\n\nProposition 2 (Second-order Necessary Condition). Any local minimax  $(\\mathbf{x}^{*},\\mathbf{y}^{*})$  satisfies  $\\mathbf{H}_{\\mathbf{yy}}\\preccurlyeq \\mathbf{0}$  and  $\\mathbf{H}_{\\mathbf{xx}} - \\mathbf{H}_{\\mathbf{xy}}\\mathbf{H}_{\\mathbf{yy}}^{-1}\\mathbf{H}_{\\mathbf{yx}}\\succcurlyeq \\mathbf{0}$\n\n\nProposition 3 (Second-order Sufficient Condition). Any stationary point  $(\\mathbf{x}^{*},\\mathbf{y}^{*})$  satisfying  $\\mathbf{H}_{\\mathbf{yy}}\\prec \\mathbf{0}$  and  $\\mathbf{H}_{\\mathbf{xx}} - \\mathbf{H}_{\\mathbf{xy}}\\mathbf{H}_{\\mathbf{yy}}^{-1}\\mathbf{H}_{\\mathbf{yx}}\\succ \\mathbf{0}$  is a local minimax.\n\n\nThe concept of global/local minimax is different from Nash equilibrium and local Nash, which are the equilibrium concepts typically studied for simultaneous games (see Nash et al. (1950); Ratliff et al. (2016) for more details). In particular, we note that the concept of Nash equilibrium or local Nash does not reflect the order between the min-player and the max-player and may not exist even for simple function of local minimax is a superset of local Nash. Under some\n\n\n![Figure 2: Relation between local Nash, local minimax and GDA stable fixed points.](images/6ceadd09d4607b10ec74288dc294abaaaad23402a874827e02d5db0186a604ca.jpg)\n\n\ns (Jin et al., 2019). In general, the set of child assumptions, local minimax points\n\n\nare guaranteed to exist (Jin et al., 2019). However, the set of stable fixed points of GDA, roughly speaking the set of points that GDA locally converges to, is a different superset of local Nash (Jin et al., 2019). The relation between the three sets of points is illustrated in Fig. 2.\n\n\n# 2.2 STABILITY OF DISCRETE DYNAMICAL SYSTEMS\n\n\nGradient-based methods can reliably find local stable fixed points - local minima in single-objective optimization. Here, we generalize the concept of stability to games by taking game dynamics as a discrete dynamical system. An iteration of the form  $\\mathbf{z}_{t + 1} = w(\\mathbf{z}_t)$  can be viewed as a discrete dynamical system, where in our case  $w:\\mathbb{R}^{n + m}\\to \\mathbb{R}^{n + m}$ . If  $w(\\mathbf{z}) = \\mathbf{z}$ , then  $\\mathbf{z}$  is called a fixed point. We study the stability of fixed points as a proxy to local convergence of game dynamics.\n\n\nDefinition 2. Let  $\\mathbf{J}$  denote the Jacobian of  $w$  at a fixed point  $\\mathbf{z}$ . If it has spectral radius  $\\rho(\\mathbf{J}) \\leq 1$ , then we call  $\\mathbf{z}$  a stable fixed point. If  $\\rho(\\mathbf{J}) < 1$ , then we call  $\\mathbf{z}$  a strictly stable fixed point.\n\n\nIt is known that strict stability implies local convergence (e.g., see Galor (2007)). In other words, if  $\\mathbf{z}$  is a strictly stable fixed point, there exists a neighborhood  $U$  of  $\\mathbf{z}$  such that when initialized in  $U$ , the iteration steps always converge to  $\\mathbf{z}$ .\n\n\n# 3 UNDESIRABLE BEHAVIOURS OF GDA\n\n\nIn this section, we discuss the undesirable behaviours of GDA in more detail. Recall that the update rule of GDA is given by\n\n\n$$\n\n$$\n\\mathbf {x} _ {t + 1} \\leftarrow \\mathbf {x} _ {t} - \\eta \\nabla_ {\\mathbf {x}} f,\n$$\n\n$$\n\n\n$$\n\n$$\n\\mathbf {y} _ {t + 1} \\leftarrow \\mathbf {y} _ {t} + \\eta \\nabla_ {\\mathbf {y}} f, \\tag {1}\n$$\n\n$$\n\n\nwhere we assume the same learning rate for both the leader and the follower for simplicity<sup>1</sup>. As illustrated in Fig. 2, the set of stable fixed points of GDA can include points that are not local minimax and, perhaps even worse, some local minimax are not necessarily stable fixed points of GDA. Here, we first give an example that a stable fixed point of GDA is not a local minimax. Consider  $\\min_x\\max_yf(x,y) = 3x^2 +y^2 +4xy$ ; the only stationary point of this problem is  $(0,0)$  and the Jacobian of GDA at this point is\n\n\n$$\n\n$$\n\\mathbf {J} = \\mathbf {I} - \\eta \\left[ \\begin{array}{c c} 6 & 4 \\\\ - 4 & - 2 \\end{array} \\right].\n$$\n\n$$\n\n\nIt is easy to see that the eigenvalues of  $\\mathbf{J}$  are  $e_1 = e_2 = 1 - 2\\eta$ . Therefore, by Definition 2,  $(0,0)$  is a strictly stable fixed point of GDA. However, one can show that  $\\mathbf{H}_{\\mathbf{yy}} = 2 > 0$  which doesn't satisfy the second-order necessary condition of local minimax.\n\n\nSimilarly, one can easily find examples in which a local minimax is not in the set of stable fixed points of GDA, e.g.,  $\\min_{x\\in \\mathbb{R}}\\max_{y\\in \\mathbb{R}}f(x,y) = -3x^2 -y^2 +4xy$  (see Fig. 1). In this example, the two Jacobian eigenvalues are both greater than 1 no matter how small the learning rate is. In other words, GDA fails to converge to  $(0,0)$  for almost all initializations (Daskalakis and Panageas, 2018).\n\n\nAs we will discuss in the next section, the main culprit of the undesirable behaviours of GDA is the leader whose gradient update  $-\\eta \\nabla_{\\mathbf{x}}f$  pushes the whole system away from the ridge or attracts the system to non-local-minimax points. By contrast, the follower's step  $\\eta \\nabla_{\\mathbf{y}}f$  can pull the system closer to the ridge (see Fig. 1) or push it away from bad fixed points. To guarantee convergence to local minimax (or avoid bad fixed points), we have to use a very small learning rate for the leader (Jin et al., 2019; Fiez et al., 2019) so that the  $\\eta \\nabla_{\\mathbf{y}}f$  term dominates. In the next section, we offer an alternative approach which explicitly cancels out undesirable effects of  $-\\eta \\nabla_{\\mathbf{x}}f$ , thereby allowing us to use larger learning rates for the leader.\n\n\n# 4 FOLLOW THE RIDGE\n\n\nDespite its popularity, GDA has the tendency to drift away from the ridge or the implicit function, and can, therefore, fail to converge with any constant learning rate. To address these problems, we propose a novel algorithm for minimax optimization, which we term Follow-the-Ridge (FR). The\n\n\nalgorithm modifies gradient descent-ascent by applying an asymmetric preconditioner. The update rule is described in Algorithm. 1.\n\n\n```\nRequire: Learning rate  $\\eta_{\\mathbf{x}}$  and  $\\eta_{\\mathbf{y}}$ ; number of iterations  $T$ .  \n1: for  $t = 1, \\dots, T$  do  \n2:  $\\mathbf{x}_{t + 1} \\gets \\mathbf{x}_t - \\eta_{\\mathbf{x}} \\nabla_{\\mathbf{x}} f(\\mathbf{x}_t, \\mathbf{y}_t)$   \n3:  $\\mathbf{y}_{t + 1} \\gets \\mathbf{y}_t + \\eta_{\\mathbf{y}} \\nabla_{\\mathbf{y}} f(\\mathbf{x}_t, \\mathbf{y}_t) + \\eta_{\\mathbf{x}} \\mathbf{H}_{\\mathbf{yy}}^{-1} \\mathbf{H}_{\\mathbf{yx}} \\nabla_{\\mathbf{x}} f(\\mathbf{x}_t, \\mathbf{y}_t)$ $\\triangleright$  gradient descent  \n $\\triangleright$  modified gradient ascent\n```\n\n\nThe main intuition behind FR is the following. Suppose that  $\\mathbf{y}_t$  is a local maximum of  $f(\\mathbf{x}_t, \\cdot)$ . Let  $r(\\mathbf{x})$  be the implicit function defined by  $\\nabla_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y}) = 0$  around  $(\\mathbf{x}_t, \\mathbf{y}_t)$ , i.e., a ridge in the landscape of  $f(\\mathbf{x}, \\mathbf{y})$ . By definition, a local minimax has to lie on a ridge; hence, it is intuitive to follow the ridge during learning. However, if  $(\\mathbf{x}_t, \\mathbf{y}_t)$  is on the ridge, then  $\\nabla_{\\mathbf{y}} f(\\mathbf{x}_t, \\mathbf{y}_t) = 0$ , and one step of gradient descent-ascent will take  $(\\mathbf{x}_t, \\mathbf{y}_t)$  to  $(\\mathbf{x}_t - \\eta_{\\mathbf{x}} \\nabla_{\\mathbf{x}} f, \\mathbf{y}_t)$ , which is off the ridge. In other words, gradient descent-ascent tends to drift away from the ridge. The correction term we introduce is\n\n\n$$\n\n$$\n\\nabla_ {\\mathbf {x}} r (\\mathbf {x}) \\left(- \\eta_ {\\mathbf {x}} \\nabla_ {\\mathbf {x}} f (\\mathbf {x} _ {t}, \\mathbf {y} _ {t})\\right) = \\eta_ {\\mathbf {x}} \\mathbf {H} _ {\\mathbf {y y}} ^ {- 1} \\mathbf {H} _ {\\mathbf {y x}} \\nabla_ {\\mathbf {x}} f.\n$$\n\n$$\n\n\nIt would bring  $\\mathbf{y}_t$  to  $\\mathbf{y}_t + \\nabla_{\\mathbf{x}}r(\\mathbf{x})(\\mathbf{x}_{t+1} - \\mathbf{x}_t) \\approx r(\\mathbf{x}_{t+1})$ , thereby encouraging both players to stay along the ridge. When  $(\\mathbf{x}_t, \\mathbf{y}_t)$  is not on a ridge yet, we expect the  $-\\eta_{\\mathbf{x}}\\nabla_{\\mathbf{x}}f$  term and the  $\\eta_{\\mathbf{x}}\\mathbf{H}_{\\mathbf{yy}}^{-1}\\mathbf{H}_{\\mathbf{yx}}\\nabla_{\\mathbf{x}}f$  term to move parallel to the ridge, while the  $\\eta_{\\mathbf{y}}\\nabla_{\\mathbf{y}}f$  term brings  $(\\mathbf{x}_t, \\mathbf{y}_t)$  closer to the ridge (see Fig. 1). Our main theoretical result is the following theorem, which suggests that FR locally converges and only converges to local minimax.\n\n\nTheorem 1 (Exact local convergence). With a suitable learning rate, all strictly stable fixed points of FR are local minimax, and all local minimax points are stable fixed points of FR.\n\n\nThe proof is mainly based on the following observation. The Jacobian of FR dynamics at a fixed point  $(\\mathbf{x}^{*},\\mathbf{y}^{*})$  is  $(c\\coloneqq \\eta_{\\mathbf{y}} / \\eta_{\\mathbf{x}})$\n\n\n$$\n\n$$\n\\mathbf {J} = \\mathbf {I} - \\eta_ {\\mathbf {x}} \\left[ \\begin{array}{c c} \\mathbf {I} & \\\\ - \\mathbf {H} _ {\\mathbf {y y}} ^ {- 1} \\mathbf {H} _ {\\mathbf {y x}} & \\mathbf {I} \\end{array} \\right] \\left[ \\begin{array}{c c} \\mathbf {H} _ {\\mathbf {x x}} & \\mathbf {H} _ {\\mathbf {x y}} \\\\ - c \\mathbf {H} _ {\\mathbf {y x}} & - c \\mathbf {H} _ {\\mathbf {y y}} \\end{array} \\right],\n$$\n\n$$\n\n\nwhere the Hessians are evaluated at  $(\\mathbf{x}^{*},\\mathbf{y}^{*})$  .  $\\mathbf{J}$  is similar to\n\n\n$$\n\n$$\n\\mathbf {M} = \\left[ \\begin{array}{c c} \\mathbf {I} & \\\\ \\mathbf {H} _ {\\mathbf {y y}} ^ {- 1} \\mathbf {H} _ {\\mathbf {y x}} & \\mathbf {I} \\end{array} \\right] \\mathbf {J} \\left[ \\begin{array}{c c} \\mathbf {I} & \\\\ - \\mathbf {H} _ {\\mathbf {y y}} ^ {- 1} \\mathbf {H} _ {\\mathbf {y x}} & \\mathbf {I} \\end{array} \\right] = \\mathbf {I} - \\eta_ {\\mathbf {x}} \\left[ \\begin{array}{c c} \\mathbf {H} _ {\\mathbf {x x}} - \\mathbf {H} _ {\\mathbf {x y}} \\mathbf {H} _ {\\mathbf {y y}} ^ {- 1} \\mathbf {H} _ {\\mathbf {y x}} & \\mathbf {H} _ {\\mathbf {x y}} \\\\ & - c \\mathbf {H} _ {\\mathbf {y y}} \\end{array} \\right].\n$$\n\n$$\n\n\nTherefore, the eigenvalues of  $\\mathbf{J}$  are those of  $\\mathbf{I} + \\eta_{\\mathbf{y}}\\mathbf{H}_{\\mathbf{yy}}$  and those of  $\\mathbf{I} - \\eta_{\\mathbf{x}}(\\mathbf{H}_{\\mathbf{xx}} - \\mathbf{H}_{\\mathbf{xy}}\\mathbf{H}_{\\mathbf{yy}}^{-1}\\mathbf{H}_{\\mathbf{yx}})$ . As shown in second-order necessary condition 2,  $(\\mathbf{x}^{*},\\mathbf{y}^{*})$  being a local minimax implies  $\\mathbf{H}_{\\mathbf{yy}}\\preccurlyeq \\mathbf{0}$  and  $\\mathbf{H}_{\\mathbf{xx}} - \\mathbf{H}_{\\mathbf{xy}}\\mathbf{H}_{\\mathbf{yy}}^{-1}\\mathbf{H}_{\\mathbf{yx}}\\succcurlyeq \\mathbf{0}$ ; one can then show that the spectral radius of the Jacobian satisfies  $\\rho (\\mathbf{J})\\leq 1$ ; hence  $(\\mathbf{x}^{*},\\mathbf{y}^{*})$  is a stable fixed point by Definition 2. On the other hand, when  $\\rho (\\mathbf{J}) < 1$  by the sufficient condition in Proposition 3,  $(\\mathbf{x}^{*},\\mathbf{y}^{*})$  must be a local minimax.\n\n\nRemark 1 (All eigenvalues are real). We notice that all eigenvalues of  $\\mathbf{J}$ , the Jacobian of  $FR$ , are real since both  $\\mathbf{H}_{\\mathbf{yy}}$  and  $\\mathbf{H}_{\\mathbf{xx}} - \\mathbf{H}_{\\mathbf{xy}}\\mathbf{H}_{\\mathbf{yy}}^{-1}\\mathbf{H}_{\\mathbf{yx}}$  are symmetric matrices. As noted by Mescheder et al. (2017); Gidel et al. (2019); Balduzzi et al. (2018), the rotational behaviour (instability) of GDA is caused by eigenvalues with large imaginary part. Therefore,  $FR$  addresses the strong rotation problem around fixed points as all eigenvalues are real.\n\n\n# 4.1 ACCELERATING CONVERGENCE WITH PRECONDITIONING AND MOMENTUM\n\n\nWe now discuss several extension of FR that preserves the theoretical guarantees.\n\n\nPreconditioning: To speed up the convergence, it is often desirable to apply a preconditioner on the gradients that compensates for the curvature. For FR, the preconditioned variant is given by\n\n\n$$\n\n$$\n\\left[ \\begin{array}{l} \\mathbf {x} _ {t + 1} \\\\ \\mathbf {y} _ {t + 1} \\end{array} \\right] \\leftarrow \\left[ \\begin{array}{l} \\mathbf {x} _ {t} \\\\ \\mathbf {y} _ {t} \\end{array} \\right] - \\left[ \\begin{array}{l l} \\mathbf {I} & \\\\ - \\mathbf {H} _ {\\mathbf {y y}} ^ {- 1} \\mathbf {H} _ {\\mathbf {y x}} & \\mathbf {I} \\end{array} \\right] \\left[ \\begin{array}{l} \\eta_ {\\mathbf {x}} \\mathbf {P} _ {1} \\nabla_ {\\mathbf {x}} f \\\\ - \\eta_ {\\mathbf {y}} \\mathbf {P} _ {2} \\nabla_ {\\mathbf {y}} f \\end{array} \\right] \\tag {2}\n$$\n\n$$\n\n\nWe can show that with any constant positive definite preconditioners  $\\mathbf{P}_1$  and  $\\mathbf{P}_2$ , the local convergence behavior of Algorithm 1 remains exact. We note that preconditioning is crucial for successfully training GANs (see Fig. 9) and RMSprop/Adam has been exclusively used in GAN training.\n\n\nMomentum: Another important technique in optimization is momentum, which speeds up convergence significantly both in theory and in practice (Polyak, 1964; Sutskever et al., 2013). We show that momentum can be incorporated into FR (here, we include momentum outside the correction term which is equivalent to applying momentum to the gradient directly for simplicity. We give a detailed discussion in Appendix D.4), which gives the following update rule:\n\n\n$$\n\n$$\n\\left[ \\begin{array}{l} \\mathbf {x} _ {t + 1} \\\\ \\mathbf {y} _ {t + 1} \\end{array} \\right] \\leftarrow \\left[ \\begin{array}{l} \\mathbf {x} _ {t} \\\\ \\mathbf {y} _ {t} \\end{array} \\right] - \\left[ \\begin{array}{l l} \\mathbf {I} & \\\\ - \\mathbf {H} _ {\\mathbf {y y}} ^ {- 1} \\mathbf {H} _ {\\mathbf {y x}} & \\mathbf {I} \\end{array} \\right] \\left[ \\begin{array}{l} \\eta_ {\\mathbf {x}} \\nabla_ {\\mathbf {x}} f \\\\ - \\eta_ {\\mathbf {y}} \\nabla_ {\\mathbf {y}} f \\end{array} \\right] + \\gamma \\left[ \\begin{array}{l} \\mathbf {x} _ {t} - \\mathbf {x} _ {t - 1} \\\\ \\mathbf {y} _ {t} - \\mathbf {y} _ {t - 1} \\end{array} \\right]. \\tag {3}\n$$\n\n$$\n\n\nBecause all of the Jacobian eigenvalues are real, we can show that momentum speeds up local convergence in a similar way it speeds up single objective minimization.\n\n\nTheorem 2. For local minimax  $(\\mathbf{x}^{*},\\mathbf{y}^{*})$ , let  $\\alpha = \\min \\left\\{\\lambda_{min}(-\\mathbf{H}_{\\mathbf{yy}}),\\lambda_{min}(\\mathbf{H}_{\\mathbf{xx}} - \\mathbf{H}_{\\mathbf{xy}}\\mathbf{H}_{\\mathbf{yy}}^{-1}\\mathbf{H}_{\\mathbf{yx}})\\right\\}$ ,  $\\beta = \\rho \\left(\\nabla^{2}f(\\mathbf{x}^{*},\\mathbf{y}^{*})\\right)$ ,  $\\kappa \\coloneqq \\beta /\\alpha$ . Then FR converges asymptotically to  $(\\mathbf{x}^{*},\\mathbf{y}^{*})$  with a rate  $\\Omega (\\kappa^{-2})$ ; FR with a momentum parameter of  $\\gamma = 1 - \\Theta (\\kappa^{-1})$  converges asymptotically with a rate  $\\Omega (\\kappa^{-1})$ .\n\n\nExperiments of the speedup of momentum are provided in Appendix E.2. This is in contrast to gradient descent-ascent, whose complex Jacobian eigenvalues prevent the use of positive momentum. Instead, negative momentum may be more preferable (Gidel et al., 2019), which does not achieve the same level of acceleration.\n\n\n# 4.2 GENERAL STACKELBERG GAMES\n\n\n```\nRequire: Learning rate  $\\eta_{\\mathbf{x}}$  and  $\\eta_{\\mathbf{y}}$ ; number of iterations  $T$ .  \n1: for  $t = 1, \\dots, T$  do  \n2:  $\\mathbf{x}_{t + 1} \\gets \\mathbf{x}_t - \\eta_{\\mathbf{x}} D_{\\mathbf{x}} f(\\mathbf{x}_t, \\mathbf{y}_t) \\quad \\triangleright$  total derivative  $D_{\\mathbf{x}} f = \\nabla_{\\mathbf{x}} f - \\nabla_{\\mathbf{x}\\mathbf{y}}^2 g(\\nabla_{\\mathbf{y}\\mathbf{y}}^2 g)^{-1} \\nabla_{\\mathbf{y}} f$   \n3:  $\\mathbf{y}_{t + 1} \\gets \\mathbf{y}_t - \\eta_{\\mathbf{y}} \\nabla_{\\mathbf{y}} g(\\mathbf{x}_t, \\mathbf{y}_t) + \\eta_{\\mathbf{x}} (\\nabla_{\\mathbf{y}\\mathbf{y}}^2 g)^{-1} \\nabla_{\\mathbf{y}\\mathbf{x}}^2 g D_{\\mathbf{x}} f(\\mathbf{x}_t, \\mathbf{y}_t)$\n```\n\n\nHere, we further extend FR to general sequential games, also known as Stackelberg games. The leader commits to an action  $\\mathbf{x}$ , while the follower plays  $\\mathbf{y}$  in response. The leader aims to minimize its cost  $f(\\mathbf{x},\\mathbf{y})$ , while the follower aims at minimizing  $g(\\mathbf{x},\\mathbf{y})$ . For Stackelberg games, the notion of equilibrium is captured by Stackelberg equilibrium, which is essentially the solution to the following optimization problem:\n\n\n$$\n\n$$\n\\min_{\\mathbf{x}\\in \\mathbb{R}^{n}}\\left\\{f(\\mathbf{x},\\mathbf{y})|\\mathbf{y}\\in \\operatorname *{arg  min}_{\\mathbf{y}\\in \\mathbb{R}^{m}}g(\\mathbf{x},\\mathbf{y})\\right\\} .\n$$\n\n$$\n\n\nIt can be seen that minimax optimization is the special case when  $g = -f$ .\n\n\nSimilarly, one can define local Stackelberg equilibrium as a generalization of local minimax in general-sum games (Fiez et al., 2019). Stackelberg game has wide applications in machine learning. To name a few, both multi-agent reinforcement learning (Littman, 1994) and hyperparameter optimization (Maclaurin et al., 2015) can be formulated as finding Stackelberg equilibria.\n\n\nFor general-sum games, naive gradient dynamics, i.e., both players taking gradient updates with their own cost functions, is no longer a reasonable algorithm, as local Stackelberg equilibria in general may not be stationary points. Instead, the leader should try to use the total derivative of  $f(\\mathbf{x}, r(\\mathbf{x}))$ , where  $r(\\mathbf{x})$  is a local best response for the follower. Thus the counterpart of gradient descent-ascent in general-sum games is actually gradient dynamics with best-response gradient (Fiez et al., 2019):\n\n\n$$\n\n$$\n\\mathbf {x} _ {t + 1} \\leftarrow \\mathbf {x} _ {t} - \\eta \\left[ \\nabla_ {\\mathbf {x}} f - \\nabla_ {\\mathbf {x y}} ^ {2} g \\left(\\nabla_ {\\mathbf {y y}} ^ {2} g\\right) ^ {- 1} \\nabla_ {\\mathbf {y}} f \\right] (\\mathbf {x} _ {t}, \\mathbf {y} _ {t}), \\tag {4}\n$$\n\n$$\n\n\n$$\n\n$$\n\\mathbf {y} _ {t + 1} \\leftarrow \\mathbf {y} _ {t} - \\eta \\nabla_ {\\mathbf {y}} g (\\mathbf {x} _ {t}, \\mathbf {y} _ {t}).\n$$\n\n$$\n\n\nFR can be adapted to general-sum games by adding the same correction term to the follower. The combined update rule is given in Algorithm 2. Similarly, we show that FR for Stackelberg games locally converges exactly to local Stackelberg equilibria (see Appendix C.2 for rigorous proof.)\n\n\n# 5 RELATED WORK\n\n\nAs a special case of Stackelberg games (Ratliff et al., 2016) in the zero-sum setting, minimax optimization concerns the problem of solving  $\\min_{\\mathbf{x} \\in \\mathcal{X}} \\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\mathbf{x}, \\mathbf{y})$ . The problem has received wide attention due to its extensive applications in modern machine learning, in settings such as generative adversarial networks (GANs), adversarial training. The vast majority of this line of research focuses on convex-concave setting (Kinderlehrer and Stampacchia, 1980; Nemirovski and Yudin, 1978; Nemirovski, 2004; Mokhtari et al., 2019b;a). Beyond the convex-concave setting, Rafique et al. (2018); Lu et al. (2019); Lin et al. (2019); Nouiehed et al. (2019) consider nonconvex-concave problems, i.e., where  $f$  is nonconvex in  $\\mathbf{x}$  but concave in  $\\mathbf{y}$ . In general, there is no hope to find global optimum efficiently in nonconvex-concave settings.\n\n\nMore recently, nonconvex-nonconcave problem has gained more attention due to its generality. Particularly, there are several lines of work analyzing the dynamics of gradient descent-ascent (GDA) in nonconvex-nonconcave setting (such as GAN training). Though simple and intuitive, GDA has been shown to have undersirable convergence properties (Adolphs et al., 2019; Daskalakis and Panageas, 2018; Mazumdar et al., 2019; Jin et al., 2019) and exhibit strong rotation around fixed points (Mescheder et al., 2017; Balduzzi et al., 2018). To overcome this rotation behaviour of GDA, various modifications have been proposed, including averaging (Yazici et al., 2019), negative momentum (Gidel et al., 2019), extragradient (EG) (Korpelevich, 1976; Mertikopoulos et al., 2019), optimistic mirror descent (OGDA) (Daskalakis et al., 2018), consensus optimization (CO) (Mescheder et al., 2017) and symplectic gradient (SGA) (Balduzzi et al., 2018; Gemp and Mahadevan, 2018). However, we note that all these algorithms discard the underlying sequential structure of minimax optimization and adopt a simultaneous game formulation. In this work, we hold that GAN training is better viewed as a sequential game rather than a simultaneous game. The former is more consistent with the divergence minimization interpretation of GANs; there is also some empirical evidence showing that well-performing GAN generators are closer to a saddle-point instead of a local minimum (Berard et al., 2019), which suggests that local Nash, the typical solution concept for simultaneous games, may not be the most appropriate one for GANs.\n\n\nTo the best of our knowledge, the only two methods that can (and only) converge to local minimax are two time-scale GDA (Jin et al., 2019) and gradient dynamics with best response gradient (Fiez et al., 2019). In two time-scale GDA, the leader moves infinitely slower than the follower, which may cause slow convergence due to infinitely small learning rates. The dynamics in Fiez et al. (2019) is proposed for general-sum games. However, their main result for general-sum games require stronger assumptions and even in that case, the dynamics can converge to non-local-Stackelberg points in general-sum games. In contrast, in general-sum games, FR will not converge to non-local-Stackelberg points. Besides, Adolphs et al. (2019); Mazumdar et al. (2019) attempt to solve the undesirable convergence issue of GDA by exploiting curvature information, but they focus on simultaneous game on finding local Nash and it is unclear how to extend their algorithm to sequential games.\n\n\nFor GAN training, there is a rich literature on different strategies to make the GAN-game well-defined, e.g., by adding instance noise (Salimans et al., 2016), by using different objectives (Nowozin et al., 2016; Gulrajani et al., 2017; Arjovsky et al., 2017; Mao et al., 2017) or by tweaking the architectures (Radford et al., 2015; Brock et al., 2019). While these strategies try to make the overall optimization problem easily, our work deals with a specific optimization problem and convergence issues arise in theory and in practice; hence our algorithm is orthogonal to these work.\n\n\n# 6 EXPERIMENTS\n\n\nIn this section, we investigate whether the theoretical guarantees of FR carry over to practical problems. Particularly, our experiments have three main aims: (1) to test if FR converges and only converges to local minimax, (2) to test the effectiveness of FR in training GANs with saturating loss, (3) to test whether FR addresses the notorious rotation problem in GAN training.\n\n\n# 6.1 LOW DIMENSIONAL TOY EXAMPLES\n\n\nTo verify our claim on exact local convergence, we first compare FR with gradient descent-ascent (GDA), optimistic mirror descent (OGDA) (Daskalakis et al., 2018), extragradient (EG) (Korpelevich, 1976), symplectic gradient adjustment (SGA) (Balduzzi et al., 2018) and consensus optimization\n\n\n![(a) GDA diverges](images/c0d9e83556ad428c710a065284f445a9cd18436a93f03cb69c84210a663b075a.jpg)\n\n\n![(b) GDA converges to a bad fixed point that is non local minimax Figure 3: Trajectory of FR and other algorithms in low dimensional toy problems. Left: for  $g_{1}$ , (0,0) is local minimax. Middle: for  $g_{2}$ , (0,0) is NOT local minimax. Right: for  $g_{3}$ , (0,0) is a local minimax. The contours are for the function value. The red triangle marks the initial position.](images/6f5437de5b647fb1071a707114f55552e9d262d00e9f27bdd374b35af991b704.jpg)\n\n\n![(c) Limiting cycle](images/223dedb387ddde98c966d49dc78acc2b05466b58802db46c4ae8f375dfc481b4.jpg)\n\n\n(CO) (Mescheder et al., 2017) on three simple low dimensional problems:\n\n\n$$\n\n$$\ng _ {1} (x, y) = - 3 x ^ {2} - y ^ {2} + 4 x y\n$$\n\n$$\n\n\n$$\n\n$$\ng _ {2} (x, y) = 3 x ^ {2} + y ^ {2} + 4 x y\n$$\n\n$$\n\n\n$$\n\n$$\ng _ {3} (x, y) = \\left(4 x ^ {2} - (y - 3 x + 0. 0 5 x ^ {3}) ^ {2} - 0. 1 y ^ {4}\\right) e ^ {- 0. 0 1 (x ^ {2} + y ^ {2})}.\n$$\n\n$$\n\n\nHere  $g_{1}$  and  $g_{2}$  are two-dimensional quadratic problems, which are arguably the simplest nontrivial problems.  $g_{3}$  is a sixth-order polynomial scaled by an exponential, which has a relatively complicated landscape compared to  $g_{1}$  and  $g_{2}$ .\n\n\nIt can be seen that when running in  $g_{1}$ , where  $(0,0)$  is a local (and in fact global) minimax, only FR, SGA and CO converge to it; all other method diverges (the trajectories of OGDA and EG almost overlap). The main reason behind the divergence of GDA is that gradient of leader pushes the system away from the local minimax when it is a local maximum for the leader. In  $g_{2}$ , where  $(0,0)$  is not a local minimax, all algorithms except for FR converge to this undesired stationary point<sup>3</sup>. In this case, the leader is still to blame for the undesirable convergence of GDA (and other variants) since it gets trapped by the gradient pointing to the origin. In  $g_{3}$ , FR can converge to  $(0,0)$ , which is a local minimax, while all other methods apparently enter limit cycles around  $(0,0)$ . The experiments suggest that even on extremely simple instances, existing algorithms can either fail to converge to a desirable fixed point or converge to bad fixed points, whereas FR always exhibits desirable behaviors.\n\n\n# 6.2 GENERATIVE ADVERSARIAL NETWORKS\n\n\nOne particularly promising application of minimax optimization algorithms is training generative adversarial networks (GANs). To recover the divergence minimization objective (e.g., Jensen-Shannon divergence in standard GANs), we have to model the adversarial game as a sequential game. According to the formulation, the generator is the leader who commits to an action first, while the discriminator is the follower that helps the generator to learn the target data distribution.\n\n\n# 6.2.1 MIXTURE OF GAUSSIANS\n\n\nWe first evaluate 4 different algorithms (GDA, EG, CO and FR) on mixture of Gaussian problems with the original saturating loss. To satisfy the non-singular Hessian assumption, we add  $L_{2}$  regularization (0.0002) to the discriminator. For both generator and discriminator, we use 2-hidden-layers MLP with 64 hidden units each layer where tanh activations is used. By default, RMSprop (Tieleman and Hinton, 2012) is used in all our experiments while the learning rate is tuned for GDA. As our FR involves the computation of Hessian inverses which is computational prohibitive, we instead use conjugate gradient (Martens, 2010; Nocedal and Wright, 2006) to solve the linear system in the inner loop. To be specific, instead of solving  $\\mathbf{H}_{\\mathbf{yy}}\\mathbf{z} = \\mathbf{H}_{\\mathbf{yx}}\\nabla_{\\mathbf{x}}f$  directly, we solve  $\\mathbf{H}_{\\mathbf{yy}}^{2}\\mathbf{z} = \\mathbf{H}_{\\mathbf{yy}}\\mathbf{H}_{\\mathbf{yx}}\\nabla_{\\mathbf{x}}f$  to ensure that the problem is well-posed since  $\\mathbf{H}_{\\mathbf{yy}}^{2}$  is always positive semidefinite. For all experimental details, we refer readers to Appendix D.2.\n\n\n![](images/0acbcb0fcbbc365c7f87fd8ec7c3baeccf7db81355b9473995fd8ee99152b01d.jpg)\n\n\n![](images/1ac77fd2e157d843db86d2f41fc43a215a9f307255f86fbbe9d522d4c7375715.jpg)\n\n\n![](images/d991d935f1fe6bf766ec3fa0b8234b43959a10fe776f80c4da4e6d24fa6133df.jpg)\n\n\n![](images/b535d02b0717979db7f931d2bcffb4c9b90012973b7f6f36e00c852d386f3166.jpg)\n\n\n![](images/db4ae6748a6c708ce8fa43c6f2b17e129f51096c8b1f3fdcba1a7910fe8109d1.jpg)\n\n\n![](images/a0c5e93d60172fa413530261b1e01c52cf4e1c044c9052fa2a88e712cf868f3b.jpg)\n\n\n![](images/b6dc6ad2be3c702df6db84e662fdfd513174ef9b4ad81bc30c239523177a4fa7.jpg)\n\n\n![](images/065f46dcccc22d5ef43de6f3009f9d46b62dd3c8e15a73fca4872625d0a0a2a6.jpg)\n\n\n![Figure 4: Comparison between FR and other algorithms on GANs with saturating loss. First Row: Generator distribution. Only consensus optimization (CO) and FR capture all three modes. Second Row: Discriminator prediction. The discriminator trained by FR converges to a flat line, indicating being fooled by the generator. Third Row: Gradient norm as a function of iteration. Only in the case of FR, the gradient norm vanishes.](images/57801981ed8c22b1dca3109370a29cf0f553bf8bc7b70bd04b20a42d5844dc54.jpg)\n\n\n![](images/82340f189c1522d558e48e612e6b87a93b79fefa3bb785c5cd4fb8aeec714aef.jpg)\n\n\n![](images/19ec80833b2ae553408095fb3e76e89baa60bbc2ccc63b01017bc2221b66efa8.jpg)\n\n\n![](images/b2907d1b81ea768dc1e52585add1c417722f5078c6e60f3b44e26272997568a6.jpg)\n\n\n![Figure 5: Comparison between FR and GDA on 2D mixture of Gaussians. Left: GDA; Right: FR.](images/8c107bd3ee7d0fe487183902e10fe67000d3b52559c18341e02ab5c911a45d1c.jpg)\n\n\n![](images/c8aec1e2240d3967cb23289640af7ca355262c272482deb50abab82016b79bb1.jpg)\n\n\n![](images/1babe031222533a4d4e07a67b1f32023277d7830175b3ceba119596cd72ea9a4.jpg)\n\n\n![](images/9356b9e14387aa16e2bf5e6d26e8c52bd1a3676740503f751b7afb973f28b461.jpg)\n\n\nAs shown in Fig. 4, GDA suffers from the \"missing mode\" problem and both discriminator and generator fail to converge as confirmed by the gradient norm plot. EG fails to resolve the convergence issue of GDA and performs similarly to GDA. With tuned gradient penalties, consensus optimization (CO) can successfully recover all three modes and obtain much smaller gradient norm. However, we notice that the gradient norm of CO decreases slowly and that both the generator and the discriminator have not converged after 50,000 iterations. In contrast, the generator\n\n\n![Figure 6: Path-norm and path-angle of FR along the linear path.](images/1cbb144fc34c831c55292b45d483f7bb3a3164bd8af492a7a5555bc4281c24fb.jpg)\n\n\ntrained with FR successfully learns the true distribution with three modes and the discriminator is totally fooled by the generator. As expected, both players reach much lower gradient norm with FR, indicating fast convergence. Moreover, we find that even if initialized with GDA-trained networks (the top row of Fig. 4), FR can still find all the modes at the end of training.\n\n\nTo check whether FR fixes the strong rotation problem around fixed points, we follow Berard et al. (2019) to plot the gradient norm and path-angle (see Fig. 6). By interpolating between the initial parameters  $\\mathbf{z}$  and the final parameters  $\\mathbf{z}^*$ , they proposed to monitor the angle between the vector field  $\\mathbf{v}$  and the linear path from  $\\mathbf{z}$  to  $\\mathbf{z}^*$ . Specifically, they looked at the quantity - path-angle, defined as\n\n\n$$\n\n$$\n\\theta (\\alpha) = \\frac {\\left\\langle \\mathbf {z} ^ {*} - \\mathbf {z} , \\mathbf {v} _ {\\alpha} \\right\\rangle}{\\| \\mathbf {z} ^ {*} - \\mathbf {z} \\| \\| \\mathbf {v} _ {\\alpha} \\|} \\text {w h e r e} \\mathbf {v} _ {\\alpha} = \\mathbf {v} (\\alpha \\mathbf {z} + (1 - \\alpha) \\mathbf {z} ^ {*}).\n$$\n\n$$\n\n\nThey showed that a high \"bump\" around  $\\alpha = 0$  in the path-angle plot typically indicates strong rotation behaviour. We choose  $\\alpha = [0.6,1.2]$  and plot the gradient norm and path-angle along the linear path for the updates of FR. In particular, we only observe a sign-switch around the fixed point  $\\mathbf{z}^*$  without an obvious bump, suggesting that FR doesn't exhibit rotational behaviour around the fixed point. To further check if FR converges to local minimax, we check the second-order condition of local minimax by computing the eigenvalues of  $\\mathbf{H}_{\\mathbf{xx}} - \\mathbf{H}_{\\mathbf{xy}}\\mathbf{H}_{\\mathbf{yy}}^{-1}\\mathbf{H}_{\\mathbf{yx}}$  and  $\\mathbf{H}_{\\mathbf{yy}}$ . As expected, all eigenvalues of  $\\mathbf{H}_{\\mathbf{xx}} - \\mathbf{H}_{\\mathbf{xy}}\\mathbf{H}_{\\mathbf{yy}}^{-1}\\mathbf{H}_{\\mathbf{yx}}$  are non-negative while all eigenvalues of  $\\mathbf{H}_{\\mathbf{yy}}$  are non-positive.\n\n\nWe also run FR on 2-D mixture of Gaussian with the same architectures (see Fig. 5) and compare it to vanilla GDA. Though GDA captures all the modes, we note that both the generator and the discriminator don't converge which can be seen from the gradient norm plot in Fig. 12. In contrast, the discriminator trained by FR is totally fooled by the generator and gradients vanish. We stress here that the sample quality in GAN\n\n\n![Figure 7: Gradient norms of GDA and FR.](images/65f140eec961638352f15bbdcc44ad15adb4962e52cd64f6f1e762469d0df007.jpg)\n\n\n![](images/be8dac09f771051cc0cf1b2771e76964bd0c04e35a021eb45bf4d032bc3662ba.jpg)\n\n\nmodels is not a good metric of checking convergence as we shown in the above example.\n\n\n# 6.2.2 PRELIMINARY RESULTS ON MNIST\n\n\nIn a more realistic setting, we test our algorithm on image generation task. Particularly, we use the standard MNIST dataset (LeCun et al., 1998) but only take a subset of the dataset with class 0 and 1 for quick experimenting. To stabilize the training of GANs, we employ spectral normalization (Miyato et al., 2018) to enforce Lipschitz continuity on the discriminator. To ensure the invertibility of the discriminator's Hessian, we add the same amount of  $L_{2}$  regularization to the discriminator as in mixture of Gaussian experiments. In terms of network architectures, we use 2-hidden-layers MLP with 512 hidden units in each layer for both the discriminator and the generator. For the discriminator, we use Sigmoid activation in the output layer. We use RMSProp as our base optimizer in the experiments with batch size 2,000. We run both GDA and FR for 100,000 iterations.\n\n\n![Figure 8: Comparison between FR and GDA on MNIST dataset. Left: GDA; Right: FR.](images/0a8a1e2b111f4d43d03eaa9d214c1b1c9f002bebb11b852e661f89c2eaf24554.jpg)\n\n\n![](images/faf3e8e3472159f7e7d27e0584ef34766fbf875207dda99ad922f57adb2b6b42.jpg)\n\n\n![](images/db3810e82ad6bc7438a9d9d63b08026ddf2fa85f24442a5a87d536d5d80bcb68.jpg)\n\n\n![](images/8e1b4b8117b4df0f083fdca89db926103a81bf0cddce2f06345dcdd5c3708d83.jpg)\n\n\nIn Fig. 8, we show the generated samples of GDA and FR along with the gradient norm plots. Our main observation is that FR improves convergence as the gradient norms of both discriminator and generator decrease much faster than GDA; however the convergence is not well reflected by the quality of generated samples. We notice that gradients don't vanish to zero at the end of training. We conjecture that for high-dimensional data distribution like images, the network we used is not flexible enough to learn the distribution perfectly.\n\n\n# 7 CONCLUSION\n\n\nIn this paper, we studied local convergence of learning dynamics in minimax optimization. To address undesirable behaviours of gradient descent-ascent, we proposed a novel algorithm that locally converges to and only converges to local minimax by taking into account the sequential structure of minimax optimization. Meanwhile, we proved that our algorithm addresses the notorious rotational behaviour of vanilla gradient-descent-ascent around fixed points. We further showed theoretically that our algorithm is compatible with standard acceleration techniques, including preconditioning and positive momentum. Our algorithm can be easily extended to general-sum Stackelberg games with similar theoretical guarantees. Empirically, we validated the effectiveness of our algorithm in both low-dimensional toy problems and GAN training.\n\n\n# REFERENCES\n\n\nLeonard Adolphs, Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann. Local saddle point optimization: A curvature exploitation approach. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 486-495, 2019.\n\n- Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.\n\n- David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. In International Conference on Machine Learning, pages 363-372, 2018.\n\n- Hugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, and Simon Lacoste-Julien. A closer look at the optimization landscapes of generative adversarial networks. arXiv preprint arXiv:1906.04848, 2019.\n\n- Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. URL: https://openreview.net/forum?id=B1xsqj09Fm.\n\n- Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed: Convergent reinforcement learning with nonlinear function approximation. In International Conference on Machine Learning, pages 1133-1142, 2018.\n\n- Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in min-max optimization. In Advances in Neural Information Processing Systems, pages 9236-9246, 2018.\n\n- Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. In International Conference on Learning Representations (ICLR 2018), 2018.\n\n- Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction methods for policy evaluation. In International Conference on Machine Learning, pages 1049-1058, 2017.\n\n- Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliff. Convergence of learning dynamics in stackelberg games. arXiv preprint arXiv:1906.01217, 2019.\n\n- Oded Galor. Discrete dynamical systems. Springer Science & Business Media, 2007.\n\n- Ian Gemp and Sridhar Mahadevan. Global convergence to the equilibrium of gans using variational inequalities. arXiv preprint arXiv:1808.01531, 2018.\n\n- Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, R\u00e9mi Le Priol, Gabriel Huang, Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1802-1811, 2019.\n\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680, 2014.\n\n- Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in neural information processing systems, pages 5767-5777, 2017.\n\n- Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge University Press, 2nd edition, 2013.\n\n- Chi Jin, Praneeth Netrapalli, and Michael I Jordan. What is local optimality in nonconvex-nonconcave minimax optimization? arXiv preprint arXiv:1902.00618, 2019.\n\n- David Kinderlehrer and Guido Stampacchia. An introduction to variational inequalities and their applications, volume 31. Siam, 1980.\n\n- GM Korpelevich. The extragradient method for finding saddle points and other problems. Matecon, 12:747-756, 1976.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\n- Tianyi Lin, Chi Jin, and Michael I Jordan. On gradient descent ascent for nonconvex-concave minimax problems. arXiv preprint arXiv:1906.00331, 2019.\n\n- Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pages 157-163. Elsevier, 1994.\n\n- Songtao Lu, Ioannis Tsakakis, Mingyi Hong, and Yongxin Chen. Hybrid block successive approximation for one-sided non-convex min-max problems: algorithms and applications. arXiv preprint arXiv:1902.08294, 2019.\n\n- Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International Conference on Machine Learning, pages 2113-2122, 2015.\n\n- Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.\n\n- Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 2794-2802, 2017.\n\n- James Martens. Deep learning via hessian-free optimization. In International Conference on Machine Learning, pages 735-742, 2010.\n\n- Eric V Mazumdar, Michael I Jordan, and S Shankar Sastry. On finding local nash equilibria (and only local nash equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838, 2019.\n\n- Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg8jjC9KQ.\n\n- Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in Neural Information Processing Systems, pages 1825-1835, 2017.\n\n- Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1QRgziT-.\n\n- Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. Proximal point approximations achieving a convergence rate of  $O(1 / k)$  for smooth convex-concave saddle point problems: Optimistic gradient and extra-gradient methods. arXiv preprint arXiv:1906.01115, 2019a.\n\n- Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. arXiv preprint arXiv:1901.08511, 2019b.\n\n- John F Nash et al. Equilibrium points in n-person games. Proceedings of the national academy of sciences, 36(1):48-49, 1950.\n\n- Arkadi Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229-251, 2004.\n\n- Arkadi S Nemirovski and David Berkovich Yudin. Cesari convergence of the gradient method of approximating saddle points of convex-concave functions. In Doklady Akademii Nauk, volume 239, pages 1056-1059. Russian Academy of Sciences, 1978.\n\n- Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media, 2006.\n\n- Maher Nouiehed, Maziar Sanjabi, Jason D Lee, and Meisam Razaviyayn. Solving a class of nonconvex min-max games using iterative first order methods. arXiv preprint arXiv:1902.08297, 2019.\n\n- Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in neural information processing systems, pages 271-279, 2016.\n\n- Peter J Olver. Nonlinear systems. http://www-users.math.umn.edu/~olver/ln/_nls.pdf, 2015.\n\n- B.T. Polyak. Some methods of speeding up the convergence of iteration methods. Ussr Computational Mathematics and Mathematical Physics, 4(5):1-17, 1964. URL https://academic.microsoft.com/paper/1988720110.\n\n- Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\n- Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization: Provable algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060, 2018.\n\n- Lillian J. Ratliff, Samuel A. Burden, and S. Shankar Sastry. On the characterization of local nash equilibria in continuous games. IEEE Transactions on Automatic Control, 61(8):2301-2307, 2016. URL https://academic.microsoft.com/paper/2963994557.\n\n- Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in neural information processing systems, pages 2234-2242, 2016.\n\n- Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In Proceedings of The 30th International Conference on Machine Learning, pages 1139-1147, 2013. URL https://academic.microsoft.com/paper/104184427.\n\n- Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31, 2012.\n\n- Yasin Yazici, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, and Vijay Chandrasekhar. The unusual effectiveness of averaging in GAN training. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SJgw_sRqFQ.\n\n- F. Zeuthen. Heinrich von stackelberg: Marktformen und gleichgewicht. julius springer. 1934 (138 s.). pris r. m. 9,60. Nationalokonomisk Tidsskrift, 3, 1935. URL https://academic.microsoft.com/paper/2607294808.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638532.3006992", "msecs": "300.0", "relativeCreated": "129131.19673728943", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Preventing Reward Hacking with Occupancy Measure Regularization\n\n# Abstract\nReward hacking occurs when an agent performs very well with respect to a specified or learned reward function (often called a \"proxy\"), but poorly with respect to the true desired reward function. Since ensuring good alignment between the proxy and the true reward is remarkably difficult, prior work has proposed regularizing to a \"safe\" policy using the KL divergence between action distributions. The challenge with this divergence measure is that a small change in action distribution at a single state can lead to potentially calamitous outcomes. Our insight is that when this happens, the state occupancy measure of the policy shifts significantly\u2014the agent spends time in drastically different states than the safe policy does. We thus propose regularizing based on occupancy measure (OM) rather than action distribution. We show theoretically that there is a direct relationship between the returns of two policies under *any* reward function and their OM divergence, whereas no such relationship holds for their action distribution divergence. We then empirically find that OM regularization more effectively prevents reward hacking while allowing for performance improvement on top of the safe policy.\n\n\n# 1 INTRODUCTION\n\n\nA major challenge for the designers of goal-oriented AI systems is specifying a reward function that robustly captures their goals and values. When the specified reward is misaligned with the designer's intent, it is really just a proxy for the true reward. This can lead to a phenomenon called reward hacking: the resulting policy accumulates achieves high reward according to the proxy reward function, but not according to the true reward function (Russell et al., 2010; Amodei et al., 2016; Pan et al., 2022; Skalse et al., 2022). The resulting behavior is often undesirable and can be especially catastrophic when deploying these systems in safety-critical scenarios, such as autonomous driving (Krakovna et al., 2019; Turner et al., 2019; Knox et al., 2022).\n\n\nThe best solution to prevent reward hacking would be to ensure better alignment between the specified proxy and hidden true reward. However, in practice, reward functions are extremely difficult to properly design due to the ambiguities and complex variables underlying real-world scenarios (Ibarz et al., 2018). For example, recommender systems aim to optimize the value that users attain from their time spent on the online platforms, but since this goal is difficult to quantify, designers utilize proxies, such as click-through rates, engagement time, and other types of explicit feedback they receive from users, which do not always match how satisfied users implicitly are with their experience (Stray et al., 2022). Several examples of reward hacking have been reported throughout the literature (Kraknova & Legg, 2020).\n\n\nRather than specifying a reward function by hand, an alternative is to learn it from human feedback like demonstrations (Ho & Ermon, 2016), comparisons (Sadigh et al., 2017; Christiano et al., 2017), or both (Palan et al., 2019) However, even learned reward functions are often misaligned with our true objectives, failing to generalize outside the distribution of behavior used in training (McKinney et al., 2023; Tien et al., 2023).\n\n\nInstead of blindly optimizing a proxy reward function, one proposal to avoid reward hacking is to regularize the policy's chosen actions to be similar to those of a known safe policy (Yang et al., 2021). For example, RLHF for LLMs generally optimizes the learned reward in addition to a term that penalizes divergences from the pre-trained language model's output (Glaese et al., 2022; Ouyang et al., 2022). Intuitively, this kind of regularization pushes the learned policy away from \"unusual\"\n\n\nbehaviors for which the reward function may be misaligned. These could include unforeseen strategies in the case of a hand-specified reward function or out-of-distribution states in the case of a learned reward function. Initializing using a pre-trained policy has been shown to effectively speed up the learning process (Laidlaw et al., 2023; Uchendu et al., 2023), and a similar paradigm can also be used to ensure the safety of the agent during online inference (Gulcehre et al., 2023).\n\n\nHowever, regularizing based on the action distributions of policies has significant drawbacks. Small shifts in action distributions can lead to large differences in outcomes. And vice-versa, large shifts in action distributions may not actually result in any difference in outcome and thus wrongly interfere with training the policy.\n\n\nImagine an autonomous car driving along a coastal highway with a steep cliff to its right. For illustrative purposes, let us think of the states as positions, and actions as velocities. Suppose we have access to a safe policy that drives slowly and avoids falling off the cliff, but the car is optimizing a proxy reward function that incentivizes progress rather than staying on the road. If we try to regularize the car's action distributions to the safe policy, we may have to apply a lot of regularization since one wrong action going too far to the right\u2014a small change in the action distribution at a single state\u2014can lead to disaster. This makes it very difficult for the regularizer to be successful. To make matters worse, optimizing the proxy will lead to moving faster, likely through action changes at many states, which makes going more to the right at a single state a negligible divergence. It is difficult if not impossible to avoid a single catastrophic action while improving at all on the safe policy.\n\n\n# XU ET AL. (2020)\n\n\nIf action distributions induce poor regularizers for reward hacking, then what can we do instead? Thinking back to the car example, while the single catastrophic action didn't change the action distribution much, observe that it did change something quite drastically: the resulting distribution over states visited by the car. The learned policy will have a high probability of reaching states where the car is off the cliff and crashed, while the safe policy never reaches such states. Our proposal follows naturally from this observation: to avoid reward hacking, regularize based on divergence from the safe policy's occupancy measure, rather than action distributions. An occupancy measure (OM) represents the distribution of states (and optionally actions) seen by a policy when it interacts with its environment. Unlike action distribution-based metrics, OM takes into account not just the actions taken by the policy, but also the states that the agent reaches.\n\n\nIn this paper, we show both theoretically and empirically that regularizing policy optimization using occupancy measure divergence is more effective at preventing reward hacking. Theoretically, we prove that there is a direct relationship between the returns of two policies under any reward function and how much they deviate from each other in occupancy measure space, and that no such relationship holds for divergences between the action distributions of the two policies. Empirically, regularizing the occupancy measure of a policy is more challenging than regularizing its action distributions. Action distribution-based policy regularization implicitly assumes a distance metric over the space of policies. So far, this has been the Kullback-Leibler (KL) divergence, which has several attractive advantages, including the fact that it is easy to compute and optimize within common deep RL algorithms (Vieillard et al., 2021). To address this, we derive an algorithm called Occupancy-Regularized Policy Optimization (ORPO) that can also be easily incorporated into deep RL algorithms like Proximal Policy Optimization (PPO). ORPO approximates the occupancy measure divergence between policies using a discriminator network.\n\n\nWe use ORPO to optimize policies trained with misaligned proxy reward functions in multiple reward hacking benchmark environments (Pan et al., 2022) and compare our method's performance to that of action distribution regularization. The results of our experiments show that regularization based on occupancy measures more effectively prevents reward hacking while allowing performance improvement over the base policy. Our findings suggest that regularization based on occupancy measures should replace action distribution-based regularization to ensure the safety of goal-driven AI systems in the real world, while also allowing them to provide value.\n\n\n# 2 RELATED WORK\n\n\nSome prior works have focused on characterizing and defining theoretical models of reward hacking as a special case of Goodhart's Law (Goodhart, 1984; Krakovna, 2019; Skalse et al., 2022; Ngo et al.,\n\n\n2023). Skalse et al. (2022) formally define reward hacking, also known as reward gaming (Leike et al., 2018), as an increase in a proxy reward function accompanied by a noticeable drop in the true reward function. Kraknova & Legg (2020) provide a list of many examples of reward hacking from the literature, but only a few studies have been conducted to understand the practical effects of reward hacking. Pan et al. (2022) systematically investigate reward misspecification and show that increasing the optimization power of RL agents can result in sudden shifts, or phase changes, in the agents' reward hacking behavior; they also define the reward hacking benchmark environments that we use to validate our method.\n\n\nA few safety methods have been proposed to avoid reward hacking and/or mitigate its dangerous effects. An AI agent can be mildly optimized, such that they aim to achieve performance that is just \"good enough\" on the misspecified proxy reward function and not necessarily the absolute optimum (Taylor et al., 2020). Quantilizers proposed by Taylor (2016) is one such approach to avoid reward hacking behaviors by training agents that are mildly optimized; however, in practice, this isn't feasible to implement. Constrained reinforcement learning has been another safety method in which designers have tried to prevent the misbehavior of agents motivated by flawed reward functions (Dalal et al., 2018; Zhang et al., 2020; Chow et al., 2019); however, in general, these approaches are limited by the number of constraints that can be applied and the specification of weights that are to be applied to the constraints. In addition, their overly conservative approach to optimization restricts the agent from actually being helpful. Roy et al. (2022) try to overcome the problem of specifying weights for the constraints but still require the designer to specify the frequency of certain events, which is also difficult to do in practice.\n\n\nOther proposals to address the reward specification problem attempt to infer the true reward function based on the given proxy reward function, environment context, and/or feedback from humans (Hadfield-Menell et al., 2017; Reddy et al., 2020). However, these approaches are limited due to the assumptions they make about the reward functions or the environment. Additionally, involving humans-in-the-loop (Lee et al., 2021) can be more expensive than providing offline demonstrations like the ones required by our method.\n\n\nRegularizing policies to be similar to an offline policy based on their KL divergences was first proposed by Stiennon et al. (2020) and has since been widely employed in the context of optimizing LLMs using learned reward functions (Ouyang et al., 2022; Bai et al., 2022; Glaese et al., 2022). KL regularization for RLHF has been further studied by Vieillard et al. (2021), Gao et al. (2022), and Korbak et al. (2022). Different offline RL methods have also been proposed to ensure there isn't a distribution shift between an offline safe policy and the online learned policy (Kang et al., 2022; Ghosh et al., 2022). Vuong et al. (2022) uses discriminators like our algorithm to maintain closeness to the baseline; however, they use two discriminators to get a combination of reward maximization and regularization. Our method aims to do the same thing but only relies on one discriminator, and it is unique in its emphasis on solving reward hacking.\n\n\n# 3 OCCUPANCY MEASURE-BASED REGULARIZATION\n\n\nWe formalize our policy regularization method in the setting of an infinite-horizon Markov decision process (MDP). An agent takes actions  $a \\in \\mathcal{A}$  to transition between states  $s \\in S$  over a series of timesteps  $t = 0,1,2,\\ldots$ . The first state  $s_0$  is sampled from an initial distribution  $\\mu_0(s)$ , and when an agent takes action  $a_t$  in  $s_t$  at time  $t$ , the next state  $s_{t+1}$  is reached at timestep  $t + 1$  with transition probability  $p(s_{t+1} \\mid s_t, a_t)$ . The agent aims to optimize a reward function  $R: S \\times \\mathcal{A} \\to [0,1]$ , and rewards are accumulated over time with discount factor  $\\gamma \\in [0,1)$ . A policy  $\\pi$  maps each state  $s$  to a distribution over actions to take at that state  $\\pi(a \\mid s)$ . We define the (normalized) return of a policy  $\\pi$  under a reward function  $R$  as\n\n\n$$\n\n$$\nJ (\\pi , R) = (1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} R (s _ {t}, a _ {t}) \\right]\n$$\n\n$$\n\n\nwhere  $\\mathbb{E}_{\\pi}$  refers to the expectation under the distribution of states and actions induced by running the policy  $\\pi$  in the environment. The normalizing factor guarantees that  $J(\\pi, R) \\in [0,1]$  always.\n\n\n![Figure 1: The MDP on the left, used in the proof of Proposition 3.1, demonstrates one drawback of using divergence between policies' action distributions for regularization. The agent stays in state  $s_1$ , where it receives 1 reward per timestep, until it takes action  $a_2$ , after which it remains in state  $s_2$  forever and receives no reward. The plot on the right shows the return  $J(\\pi, R)$  for a policy  $\\pi$  when  $\\gamma = 0.99$  as a function of the policy's action distribution at  $s_1$ . While  $\\pi_{\\mathrm{safe}}$  and  $\\pi$  (shown on the plot as dotted lines) are close in action distribution space, they achieve very different returns. Meanwhile,  $\\pi$  is far from  $\\pi_{\\mathrm{safe}}'$  in action distribution space but achieves nearly the same return. Propositions 3.2 and A.2 show that occupancy measure divergences do not have these drawbacks.](images/e46db0bd132df0f19e09b42f6dc5ed94bf4a83bfbf167ccc24ad71420437a254.jpg)\n\n\n![](images/42e30a7586f1ccf3372335b081aa04700ab7823d263b5764add183e5e6ee74d1.jpg)\n\n\nWe define the state-action occupancy measure  $\\mu_{\\pi}$  of a policy  $\\pi$  as the expected discounted number of times the agent will be in a particular state and take a specific action:\n\n\n$$\n\n$$\n\\mu_ {\\pi} (s, a) = (1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} \\mathbb {1} \\{s _ {t} = s \\wedge a _ {t} = a \\} \\right].\n$$\n\n$$\n\n\nIntuitively, the occupancy measure represents the distribution of states and actions visited by the policy over time. If  $\\pi$  spends a lot of time taking action  $a$  in state  $s$ , then  $\\mu_{\\pi}(s,a)$  will be high, whereas if  $\\pi$  never visits a state  $s$ , then  $\\mu_{\\pi}(s,a) = 0$  for all actions  $a$ .\n\n\nThe standard approach to solving an MDP is to find a policy  $\\pi$  that maximizes its return:\n\n\n$$\n\n$$\n\\text {m a x i m i z e} \\quad J (\\pi , R). \\tag {1}\n$$\n\n$$\n\n\nHowever, as we discussed in section 1, an AI system designer most likely does not have access to a reward function that perfectly encapsulates their preferences. Instead, the designer might optimize  $\\pi$  using a learned or hand-specified proxy reward function  $\\tilde{R}$  which is misaligned with the true reward function  $R$ . Blindly maximizing the proxy reward function could lead to reward hacking.\n\n\n# 3.1 ACTION DISTRIBUTION-BASED REGULARIZATION\n\n\nA widely-used approach to prevent reward hacking behaviors is to optimize the policy's return with respect to the proxy  $\\tilde{R}$  plus a regularization term that penalizes the KL divergence of the policy's action distribution from a safe policy  $\\pi_{\\mathrm{safe}}$ :\n\n\n$$\n\n$$\n\\text {m a x i m i z e} \\quad J (\\pi , \\tilde {R}) - \\lambda (1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} D _ {\\mathrm {K L}} \\left(\\pi (\\cdot \\mid s _ {t}) \\| \\pi_ {\\text {s a f e}} (\\cdot \\mid s _ {t})\\right) \\right]. \\tag {2}\n$$\n\n$$\n\n\nThe regularization term can be easily incorporated into deep RL algorithms, like PPO, since the KL divergence between action distributions can usually be calculated in closed form.\n\n\nAlthough it is simple, the action distribution-based regularization method in (2) has serious drawbacks that arise from the complex relationship between a policy's action distribution at various states and its return under the true reward function. In some cases, a very small change in action distribution space can result in a huge change in reward, and in other cases, a large change in action distribution space can result in a negligible change in reward. We formalize this in the following proposition.\n\n\nProposition 3.1. Fix  $\\epsilon >0$  and  $\\delta >0$  arbitrarily small, and  $c\\geq 0$  arbitrarily large. Then there is an MDP and true reward function  $R$  where both of the following hold:\n\n\n1. There is a pair of policies  $\\pi$  and  $\\pi_{\\text{safe}}$  where the action distribution  $KL$  divergence satisfies\n\n\n$$\n\n$$\n(1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} D _ {K L} \\left(\\pi (\\cdot \\mid s _ {t}) \\| \\pi_ {s a f e} (\\cdot \\mid s _ {t})\\right) \\right] \\leq \\epsilon\n$$\n\n$$\n\n\nbut  $|J(\\pi_{safe},R) - J(\\pi ,R)|\\geq 1 - \\delta$\n\n\n2. There is a safe policy  $\\pi_{\\text{safe}}'$  such that any other policy  $\\pi$  with\n\n\n$$\n\n$$\n(1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} D _ {K L} \\left(\\pi (\\cdot \\mid s _ {t}) \\| \\pi_ {s a f e} ^ {\\prime} (\\cdot \\mid s _ {t})\\right) \\right] \\leq c\n$$\n\n$$\n\n\nsatisfies  $|J(\\pi_{\\text{safe}}, R) - J(\\pi, R)| \\leq \\delta$ .\n\n\nThe first part of Proposition 3.1 shows that in the worst case, a KL divergence below than some arbitrarily small threshold  $\\epsilon$  from the safe policy's action distributions can induce a change in the return under true reward function  $R$  that is almost as large as the entire possible range of returns. Thus, when regularizing using action distribution KL divergence like in (2), one might have to make  $\\lambda$  extremely large to prevent drastic changes from the safe policy. However, the second part of Proposition 3.1 shows that in the same MDP, for a different safe policy, any learned policy with an arbitrarily large action distribution KL divergence from the safe policy has an extremely small difference in return. This means that one might need to set  $\\lambda$  extremely small in order to allow for the large divergence in the policies' action distributions necessary for optimization to have any effect. Since Proposition 3.1 suggests that we need to make  $\\lambda$  large for some reasons and small for others, it may be impossible to choose a good  $\\lambda$  value that both prevents undesirable behavior but still allows some optimization of the reward function. See Figure 1 for a graphical illustration of Proposition 3.1 and Appendix A.1 for a proof.\n\n\n# 3.2 OCCUPANCY MEASURE-BASED REGULARIZATION\n\n\nSince Proposition 3.1 shows that small KL divergence in action space from the safe policy can have large effects, and vice versa, it may be impossible in some environments to regularize effectively using the objective in (2). We propose instead to regularize based on the divergence between the occupancy measures of the learned and safe policies:\n\n\n$$\n\n$$\n\\text {m a x i m i z e} \\quad J (\\pi , \\tilde {R}) - \\lambda \\| \\mu_ {\\pi} - \\mu_ {\\pi_ {\\text {s a f e}}} \\| _ {1}. \\tag {3}\n$$\n\n$$\n\n\nIn (3), we use the total variation (TV) between the occupancy measures, defined as\n\n\n$$\n\n$$\n\\| \\mu_ {\\pi} - \\mu_ {\\pi_ {\\text {s a f e}}} \\| _ {1} = \\sum_ {(s, a) \\in \\mathcal {S} \\times \\mathcal {A}} | \\mu_ {\\pi} (s, a) - \\mu_ {\\pi_ {\\text {s a f e}}} (s, a) |.\n$$\n\n$$\n\n\nWhy should using the occupancy measure divergence to regularize perform better than using the divergence between action distributions? The following proposition shows that the TV distance between occupancy measures does not have the same problems as action distribution divergence: a small divergence cannot result in a large change in policy return.\n\n\nProposition 3.2. For any MDP, reward function  $R$ , and pair of policies  $\\pi$ ,  $\\pi_{safe}$ , we have\n\n\n$$\n\n$$\n\\left| J \\left(\\pi_ {s a f e}, R\\right) - J (\\pi , R) \\right| \\leq \\left\\| \\mu_ {\\pi} - \\mu_ {\\pi_ {s a f e}} \\right\\| _ {1}. \\tag {4}\n$$\n\n$$\n\n\nProposition 3.2 shows that, unlike action distribution divergences, the occupancy measure divergence between  $\\pi$  and  $\\pi_{\\mathrm{safe}}$  bounds the returns between the two policies. Specifically, a small difference between the occupancy measure of the safe and learned policies guarantees that they have similar returns. Even more, this bound is actually tight, suggesting that changes in occupancy measure TV distance will correspond closely with changes in the return of the policy under the true reward. Proposition A.2 in Appendix A shows that, in general, we cannot do any better than the bound from Proposition 3.2. See Appendix A.2 for a proof of this proposition.\n\n\nThese results suggest that the divergence between the occupancy measures of the learned and safe policies is a much better measure of how similar those policies are than the divergence between the policies' action distributions. In the following sections, we will show that these theoretical results match with intuition and empirical performance.\n\n\n# 3.3 EXAMPLE\n\n\nFigure 2 shows an intuitive example of why regularizing to a safe policy based on occupancy measure divergence is superior to regularizing using action distribution divergence. Figure 2a depicts a\n\n\n![](images/8479089310d9a8b5193015f757e18a0acb6052bfac07e291e7a58dac3fe1c44a.jpg)\n\n\n![(a) (b) Figure 2: This simple gridworld provides an intuitive example of why occupancy measure divergences are superior to action distribution divergences for regularizing to a safe policy. See Section 3.3 for the details.](images/be00f0770996ed5bddccf5ebff2a42675986967285079fedcf740b489c04f9bc.jpg)\n\n\nsimplified version of the tomato-watering AI Safety Gridworld proposed by Leike et al. (2017). The agent, a robot that starts in the lower right corner, can move up, down, left, right, or stay in place. Its objective is to water the tomatoes on the board, and it receives reward each time it moves into a square with a tomato. However, there is also a sprinkler in the upper right corner of the environment. When the robot moves into the sprinkler's square, its sensors see water everywhere, and it believes all tomatoes are watered. In this environment, the true reward function  $R$  only rewards watering tomatoes, while the proxy reward function  $\\tilde{R}$  also highly rewards reaching the sprinkler.\n\n\nThe top row of Figure 2b shows three policies for this environment: a desired policy that achieves the highest true reward, a safe policy that achieves lower true reward because it stays in place more often, and a reward hacking policy that exploits the sprinkler state to achieve high proxy reward but low true reward. The arrows between the policies on the top row of Figure 2b show the action distribution KL divergences between them as used for regularization in (2). The action distribution divergences suggest that the reward hacking policy is actually closer to the safe policy than the desired policy is. This is because the safe policy is nearly identical to the reward hacking policy in the upper right square, where the reward hacking policy spends most of its time; they both take the \"stay\" action with high probability. Thus, if we regularize to the safe policy using action distribution KL divergence, we would be more likely to find a policy that hacks the proxy reward, rather than one like the left policy, which we prefer. The problem is that the safe policy rarely reaches the sprinkler state, but the action distribution divergence doesn't account for this.\n\n\nUsing occupancy measure divergences avoids this problem. The bottom row of Figure 2b shows the occupancy measures for each policy in the top row, and the arrows between the columns show the total variation distance  $\\| \\mu - \\mu' \\|_1$  between the policies' occupancy measures. In terms of occupancy measure distance, the desired policy on the left is closer to the safe policy than the reward hacking policy is. This is because both the desired and safe policies spend most of their time actually watering tomatoes, as evidenced by the higher occupancy measure they assign to the tiles on the board with the tomatoes. In contrast, the reward hacking policy spends almost all of its time in the sprinkler square and as a result, has a very different occupancy measure. Thus, if we trained a policy regularized with\n\n\noccupancy measure divergence in this environment, we could hope to find a policy like the desired one on the left and avoid a reward hacking policy like the one on the right.\n\n\n# 3.4 OCCUPANCY-REGULARIZED POLICY OPTIMIZATION (ORPO)\n\n\nIn the previous sections, we showed strong theoretical evidence that regularizing using occupancy measure divergences is superior to regularizing using action distribution divergences. We now introduce an algorithm, Occupancy-Regularized Policy Optimization (ORPO), to feasibly approximate the occupancy measure divergence between the learned and safe policies for the purpose of regularizing deep RL agents.\n\n\nWhile our theory relies on the TV distance between occupancy measures, we find that the KL divergence is more stable to calculate in practice. Since Pinsker's inequality bounds the TV distance by the KL divergence for small KL values, and the Bretagnolle-Huber bound holds for larger KL values, our theoretical guarantees intuition remain valid in the case of OM KL (Canonne, 2022). Our objective from (3) can be reformulated with the KL divergence in place of the TV distance:\n\n\n$$\n\n$$\n\\text {m a x i m i z e} \\quad J (\\pi , \\tilde {R}) - \\lambda D _ {\\mathrm {K L}} \\left(\\mu_ {\\pi} \\| \\mu_ {\\pi_ {\\text {s a f e}}}\\right). \\tag {5}\n$$\n\n$$\n\n\nWe optimize (5) using a gradient-based method. The gradient of the first term is estimated using PPO, a popular policy gradient method (Schulman et al., 2017). However, calculating the occupancy measure divergence for the second term is intractable to do in closed form since it requires the enumeration of all possible state-action pairs, an impossible task in the case of deep RL. Thus, we approximate the KL divergence between the occupancy measures of policies by training a discriminator network, a technique that has previously been used for generative adversarial networks (GANs) (Goodfellow et al., 2014) and GAIL (Ho & Ermon, 2016).\n\n\nThe discriminator network  $d: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$  assigns a score  $d(s, a) \\in \\mathbb{R}$  to any state-action pair  $(s, a) \\in \\mathcal{S} \\times \\mathcal{A}$ , and it is trained on a mixture of data from both the learned policy  $\\pi$  and safe policy  $\\pi_{\\mathrm{safe}}$ . The objective used to train  $d$  incentives low scores for state-action pairs from  $\\pi_{\\mathrm{safe}}$  and high scores for state-action pairs from  $\\pi$ :\n\n\n$$\n\n$$\nd = \\arg \\min  _ {d} \\sum_ {t = 0} ^ {\\infty} \\left(\\mathbb {E} _ {\\pi} \\left[ \\gamma^ {t} \\log \\left(1 + e ^ {- d \\left(s _ {t}, a _ {t}\\right)}\\right) \\right] + \\mathbb {E} _ {\\pi_ {\\text {s a f e}}} \\left[ \\gamma^ {t} \\log \\left(1 + e ^ {d \\left(s _ {t}, a _ {t}\\right)}\\right) \\right]\\right). \\tag {6}\n$$\n\n$$\n\n\nHusz\u00e1r (2017) proves that if the loss function in (6) is minimized, then the expected discriminator scores for state-action pairs drawn from the learned policy distribution will approximately equal the KL divergence between the occupancy measures of the two policies:\n\n\n$$\n\n$$\nD _ {\\mathrm {K L}} \\left(\\mu_ {\\pi} (s, a) \\parallel \\mu_ {\\pi_ {\\text {s a f e}}} (s, a)\\right) \\approx (1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} d \\left(s _ {t}, a _ {t}\\right) \\right]\n$$\n\n$$\n\n\nApplying the definitions of the learned policy returns and the KL divergence between the polices' occupancy measures, we can now rewrite our ORPO objective:\n\n\n$$\n\n$$\n\\text {m a x i m i z e} \\quad \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} \\left(\\tilde {R} \\left(s _ {t}, a _ {t}\\right) - \\lambda d \\left(s _ {t}, a _ {t}\\right)\\right) \\right]. \\tag {7}\n$$\n\n$$\n\n\nThis objective dynamically changes at every step of the optimization\u2014as the policy changes online, the discriminator will also adapt and change.\n\n\nNote that (7) is identical to the normal RL objective with a reward function  $R^{\\prime}(s,a) = \\tilde{R} (s,a) - \\lambda d(s,a)$ . Thus, once the discriminator has been trained, we add the discriminator scores to the given reward function and use the combined values to update  $\\pi$  with PPO. The training process for ORPO consists of iterating between two phases: one in which data from both the safe and current policies is used to train the discriminator to minimize (6), and one in which data from the current policy is used to train the PPO agent with the augmented reward function in (7). After a policy gradient step, the process repeats.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Tomato</td><td colspan=\"3\">Environment</td></tr><tr><td colspan=\"2\">Traffic</td><td>Glucose</td></tr><tr><td>\\( \\pi_{\\text{safe}} \\)</td><td>65.3 \u00b1 0.0</td><td colspan=\"2\">-2296 \u00b1 0.0</td><td>-76199 \u00b1 0.0</td></tr><tr><td>No regularization</td><td>22.2 \u00b1 7.0</td><td colspan=\"2\">-56425 \u00b1 6877.4</td><td>-577093 \u00b1 33735.3</td></tr><tr><td>Action dist. regularization</td><td>67.1 \u00b1 3.9</td><td colspan=\"2\">-1212 \u00b1 24.2</td><td>-75955 \u00b1 2604.0</td></tr><tr><td>State occupancy regularization</td><td>66.7 \u00b1 0.8</td><td colspan=\"2\">-1244 \u00b1 242.7</td><td>-54287 \u00b1 27650.2</td></tr><tr><td>State-action occupancy regularization</td><td>69.1 \u00b1 13.1</td><td colspan=\"2\">-1096 \u00b1 36.2</td><td>-14834 \u00b1 25365.3</td></tr></table>\n\n\nTable 1: The true rewards achieved by ORPO and PPO with action distribution regularization in the three reward hacking environments. We report the best reward achieved across a range of coefficients and compare to the safe policy  $\\pi_{\\mathrm{safe}}$  and training without regularization on the proxy reward.\n\n\nWhile we have thus far considered the state-action occupancy measure of a policy  $\\mu_{\\pi}(s,a)$ , we find that in practice it sometimes makes more sense to regularize based on the state-only occupancy measure  $\\mu_{\\pi}(s) = (1 - \\gamma)\\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{1}\\{s_{t} = s\\}]$ . In particular, in many environments the reward function  $R(s)$  is only a function of the state. In this case, it is simple to establish similar guarantees to Proposition 3.2 based on the state-only occupancy measure, and so the state occupancy measure might be more effective in regularizing the behavior of the agent. We can implement this within ORPO by only providing the state as input to the discriminator rather than a state-action pair.\n\n\n# 4 EXPERIMENTS\n\n\nWe consider the empirical performance of ORPO in four environments: the tomato-watering gridworld we focused on in Section 3.3; Flow, an autonomous vehicle control environment introduced by Wu et al. (2022); SimGlucose, and a blood glucose monitoring system developed by Fox et al. (2020). We chose the first for illustrative purposes, and the following two because they were presented as reward hacking benchmark environments by Pan et al. (2022).\n\n\nTomato Environment: As before, the tomato environment contains a sprinkler state where the agent perceives all tomatoes as being watered and thus receives high proxy reward but no true reward. For our safe policy, we train a PPO agent with the true reward function, and then add a  $15\\%$  chance of taking a random action to ensure there is room to improve upon it.\n\n\nFlow Traffic Simulator: The traffic environment simulates a road network where cars on an on-ramp attempt to merge into traffic on a highway. Some vehicles are controlled by a human model and some are RL-controlled autonomous vehicles. The true objective of the self-driving agent is to ensure that there is fast traffic flow at all times in order to reduce the mean commute time, while observing the positions and velocities of nearby vehicles. The proxy reward is the average velocity of all cars in the simulation. When the traffic agent begins to reward hack, it stops cars on the on-ramp from merging into traffic. This way, the proxy reward is optimized because cars on the straightway can continue forward at a fast speed instead of having to wait for a car to merge, which increases the average velocity of all vehicles. However, the true objective is not achieved as the commute time for the cars on the on-ramp increases indefinitely. As the safe policy for the traffic environment we used the Intelligent Driver Model (IDM), a standard approximation of human driving behavior (Treiber et al., 2000). In practice, safe policies are often learned via imitation learning, so to simulate this we generate data from the IDM controller and train a behavioral cloning (BC) policy on it with some entropy added to ensure that the policy more closely resembles the quality of baseline that can be reasonably provided in practice.\n\n\nSimGlucose: This blood glucose monitoring environment is an extension of the FDA-approved glucose monitoring simulator proposed by Man et al. (2014) for Type 1 Diabetes patients. The patient eats meals, while wearing a continuous glucose monitoring (CGM) device that makes noisy readings of the patient's blood glucose. The RL agent takes the action of administering insulin to the patient in order to maintain healthy glucose levels, while observing their history of CGM readings in addition to the previous amounts of insulin that it has delivered. The true reward is the health risk that the patient encounters given different amounts of insulin they receive, but the proxy reward prioritizes the monetary cost of the treatment incurred. As the safe baseline policy, we train a BC policy based\n\n\n![Figure 3: The results of running ORPO vs. PPO with action distribution regularization across a range of coefficients  $\\lambda$  in each of the three environments. We find that generally the regularization methods succeed at reducing the action distribution or occupancy measure KL to near-0 for high enough values of  $\\lambda$ .](images/53650dd19739e534ba070f0b044c2375a13b001d5baeb724114571beb4044e12.jpg)\n\n\non actions given by a widely-used PID controller insulin pump with parameters tuned by the original designers of this system (Steil, 2013).\n\n\nWe train RL policies initialized from  $\\pi_{\\mathrm{safe}}$  in each environment using action distribution regularization and OM regularization, varying the regularization coefficient  $\\lambda$  across a wide range. In environments that we studied,  $\\lambda$  values between  $10^{-2}$  and  $10^{2}$  seemed to work best, scaled by their average per time step rewards; since the environments have vastly different magnitudes in reward, and the coefficients are applied to the per time step rewards, we scale coefficients in the range that we found worked best by multiplying by the average per time step reward. We compare the performance of the regularization techniques to the safe policies  $\\pi_{\\mathrm{safe}}$  to see if they can improve on  $\\pi_{\\mathrm{safe}}$  according to the true reward without devolving into reward hacking.\n\n\nThe results of our experiments are shown in Table 1 and Figure 3. In Table 1, we see that ORPO achieves higher true reward across all three environments compared to action distribution regularization. Figure 3 shows how the true reward, occupancy measure KL divergence, and action distribution KL divergence vary over values of the regularization coefficient  $\\lambda$ . We see that as  $\\lambda$  is increased, the KL divergence eventually falls to 0 and the policy return approaches that of  $\\pi_{\\mathrm{safe}}$ . Interestingly, we note that in general the optimal true reward is achieved at the value of  $\\lambda$  below which the occupancy measure KL suddenly increases. This observation could enable effective tuning of  $\\lambda$  in new environments, since we cannot always rely on having a true reward function to evaluate different values of  $\\lambda$  with.\n\n\n# 5 CONCLUSION\n\n\nWe have presented theoretical and empirical evidence that occupancy measure regularization can more effectively prevent reward hacking than action distribution regularization when training with a misaligned proxy reward function. To address the practical challenges of the OM-based approach, we introduced an algorithm called ORPO, which uses an adversarially trained discriminator to approximate the KL divergence between the occupancy measures of policies. In the future, we hope to experiment with learned proxy reward functions in addition to the hand-specified reward functions we considered in this paper.\n\n\n# REFERENCES\n\n- Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete Problems in AI Safety, July 2016. URL http://arxiv.org/abs/1606.06565.arXiv:1606.06565 [cs].\n\n- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, April 2022. URL http://arxiv.org/abs/2204.05862. arXiv:2204.05862 [cs].\n\n- Cl\u00e9ment L. Canonne. A short note on an inequality between KL and TV, February 2022. URL http://arxiv.org/abs/2202.07198.arXiv:2202.07198 [math, stat].\n\n- Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. Lyapunov-based Safe Policy Optimization for Continuous Control, February 2019. URL http://arxiv.org/abs/1901.10031.arXiv:1901.10031 [cs, stat].\n\n- Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, June 2017. URL http://arxiv.org/abs/1706.03741.arXiv:1706.03741 [cs, stat].\n\n- Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe Exploration in Continuous Action Spaces, January 2018. URL http://arxiv.org/abs/1801.08757. arXiv:1801.08757 [cs].\n\n- Ian Fox, Joyce Lee, Rodica Pop-Busui, and Jenna Wiens. Deep Reinforcement Learning for Closed-Loop Blood Glucose Control, September 2020. URL http://arxiv.org/abs/2009.09051.arXiv:2009.09051 [cs, stat].\n\n- Leo Gao, John Schulman, and Jacob Hilton. Scaling Laws for Reward Model Overoptimization, October 2022. URL http://arxiv.org/abs/2210.10760. arXiv:2210.10760 [cs, stat].\n\n- Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline RL Policies Should be Trained to be Adaptive, July 2022. URL http://arxiv.org/abs/2207.02200.arXiv:2207.02200 [cs, stat].\n\n- Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, So\u00f1a Mokr\u00e1, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements, September 2022. URL http://arxiv.org/abs/2209.14375.arXiv:2209.14375 [cs].\n\n- Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks, June 2014. URL http://arxiv.org/abs/1406.2661.arXiv:1406.2661 [cs, stat].\n\n- C. A. E. Goodhart. Problems of Monetary Management: The UK Experience. In C. A. E. Goodhart (ed.), Monetary Theory and Practice: The UK Experience, pp. 91-121. Macmillan Education UK, London, 1984. ISBN 978-1-349-17295-5. doi: 10.1007/978-1-349-17295-5_4. URL https://doi.org/10.1007/978-1-349-17295-5_4.\n\n- Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan First, and Nando de Freitas. Reinforced Self-Training (ReST) for Language Modeling, August 2023. URL http://arxiv.org/abs/2308.08998. arXiv:2308.08998 [cs].\n\n- Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, and Anca Dragan. Inverse Reward Design, 2017. URL http://arxiv.org/abs/1711.02827. arXiv:1711.02827 [cs].\n\n- Jonathan Ho and Stefano Ermon. Generative Adversarial Imitation Learning. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://papers.nips.cc/paper_files/paper/2016/bitstream/cc7e2b878868cbae992d1fb743995d8f-Abstract.html.\n\n- Ferenc Husz\u00e1r. Variational Inference using Implicit Distributions, February 2017. URL http://arxiv.org/abs/1702.08235. arXiv:1702.08235 [cs, stat].\n\n- Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning from human preferences and demonstrations in Atari. 2018. doi: 10.48550/ARXIV.1811.06521. URL https://arxiv.org/abs/1811.06521. Publisher: arXiv Version Number: 1.\n\n- Katie Kang, Paula Gradu, Jason Choi, Michael Janner, Claire Tomlin, and Sergey Levine. Lyapunov Density Models: Constraining Distribution Shift in Learning-Based Control, June 2022. URL http://arxiv.org/abs/2206.10524. arXiv:2206.10524 [cs, eess].\n\n- W. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward (Mis)design for Autonomous Driving, March 2022. URL http://arxiv.org/abs/2104.13906.arXiv:2104.13906 [cs].\n\n- Tomasz Korbak, Ethan Perez, and Christopher L. Buckley. RL with KL penalties is better viewed as Bayesian inference, October 2022. URL http://arxiv.org/abs/2205.11275.arXiv:2205.11275 [cs, stat].\n\n- Uesato Jonathan Mikulik Vladimir Rahtz Mathew Everitt Tom Kumar Ramana Kenton Zac Leike Jan Kraknova, Victoria and Shane Legg. Specification gaming: the flip side of AI ingenuity, 2020. URL https://www_deepmind.com/blog/ specification-gaming-the-flip-side-of-ai-ingenuity.\n\n- Victoria Krakovna. Classifying specification problems as variants of Goodhart's Law, August 2019. URL https://vkrakovna.wordpress.com/2019/08/19/classifying-specification-problems-as-variants-of-goodharts-law/.\n\n- Victoria Krakovna, Laurent Orseau, Ramana Kumar, Miljan Martic, and Shane Legg. Penalizing side effects using stepwise relative reachability, March 2019. URL http://arxiv.org/abs/1806.01186.arXiv:1806.01186 [cs, stat].\n\n- Cassidy Laidlaw, Stuart Russell, and Anca Dragan. Bridging RL Theory and Practice with the Effective Horizon, April 2023. URL http://arxiv.org/abs/2304.09853. arXiv:2304.09853 [cs, stat].\n\n- Kimin Lee, Laura Smith, and Pieter Abbeel. PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training, June 2021. URL http://arxiv.org/abs/2106.05091. arXiv:2106.05091 [cs].\n\n- Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau, and Shane Legg. AI Safety Gridworlds, November 2017. URL http://arxiv.org/abs/1711.09883. arXiv:1711.09883 [cs].\n\n- Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction, November 2018. URL http://arxiv.org/abs/1811.07871. arXiv:1811.07871 [cs, stat].\n\n- Eric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E. Gonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for Distributed Reinforcement Learning, June 2018. URL http://arxiv.org/abs/1712.09381. arXiv:1712.09381 [cs].\n\n- Chiara Dalla Man, Francesco Micheletto, Dayu Lv, Marc Breton, Boris Kovatchev, and Claudio Cobelli. The UVA/PADOVA Type 1 Diabetes Simulator. Journal of Diabetes Science and Technology, 8(1):26-34, January 2014. ISSN 1932-2968. doi: 10.1177/1932296813514502. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4454102/.\n\n- Lev McKinney, Yawen Duan, David Krueger, and Adam Gleave. On The Fragility of Learned Reward Functions, January 2023. URL http://arxiv.org/abs/2301.03652. arXiv:2301.03652 [cs].\n\n- Richard Ngo, Lawrence Chan, and Soren Mindermann. The alignment problem from a deep learning perspective, September 2023. URL http://arxiv.org/abs/2209.00626.arXiv:2209.00626 [cs].\n\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. doi: 10.48550/ARXIV.2203.02155. URL https://arxiv.org/abs/2203.02155.\n\n- Malayandi Palan, Nicholas C. Landolfi, Gleb Shevchuk, and Dorsa Sadigh. Learning Reward Functions by Integrating Human Demonstrations and Preferences. 2019. doi: 10.48550/ARXIV.1906.08928. URL https://arxiv.org/abs/1906.08928. Publisher: arXiv Version Number: 1.\n\n- Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models, February 2022. URL http://arxiv.org/abs/2201.03544.arXiv:2201.03544 [cs, stat].\n\n- Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library, December 2019. URL http://arxiv.org/abs/1912.01703. arXiv:1912.01703 [cs, stat].\n\n- Siddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, and Jan Leike. Learning human objectives by evaluating hypothetical behavior. In Hal Daum\u00e9 III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 8020-8029. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/reddy20a.html.\n\n- Julien Roy, Roger Girgis, Joshua Romoff, Pierre-Luc Bacon, and Christopher Pal. Direct Behavior Specification via Constrained Reinforcement Learning, June 2022. URL http://arxiv.org/abs/2112.12228.arXiv:2112.12228 [cs].\n\n- Stuart J. Russell, Peter Norvig, and Ernest Davis. Artificial intelligence: a modern approach. Prentice Hall series in artificial intelligence. Prentice Hall, Upper Saddle River, 3rd ed edition, 2010. ISBN 978-0-13-604259-4.\n\n- Dorsa Sadigh, Anca Dragan, Shankar Sastry, and Sanjit Seshia. Active Preference-Based Learning of Reward Functions. In Robotics: Science and Systems XIII. Robotics: Science and Systems Foundation, July 2017. ISBN 978-0-9923747-3-0. doi: 10.15607/RSS.2017.XIII.053. URL http://www. roboticsproceedings.org/rss13/p53.pdf.\n\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, August 2017. URL http://arxiv.org/abs/1707.06347.arXiv:1707.06347 [cs].\n\n- Joar Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and Characterizing Reward Hacking, September 2022. URL http://arxiv.org/abs/2209.13085.arXiv:2209.13085 [cs, stat].\n\n- Garry M. Steil. Algorithms for a Closed-Loop Artificial Pancreas: The Case for Proportional-Integral-Derivative Control. Journal of Diabetes Science and Technology, 7(6):1621-1631, November 2013. ISSN 1932-2968. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3876341/.\n\n- Nisan Stiannon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback, September 2020. URL http://arxiv.org/abs/2009.01325. arXiv:2009.01325 [cs].\n\n- Jonathan Stray, Alon Halevy, Parisa Assar, Dylan Hadfield-Menell, Craig Boutilier, Amar Ashar, Lex Beattie, Michael Ekstrand, Claire Leibowicz, Connie Moon Sehat, Sara Johansen, Lianne Kerlin, David Vickrey, Spandana Singh, Sanne Vrijenhoek, Amy Zhang, McKane Andrus, Natali Helberger, Polina Proutskova, Tanushree Mitra, and Nina Vasan. Building Human Values into Recommender Systems: An Interdisciplinary Synthesis, July 2022. URL http://arxiv.org/abs/2207.10192. arXiv:2207.10192 [cs].\n\n- Jessica Taylor. Quantilizers: A Safer Alternative to Maximizers for Limited Optimization. March 2016. URL https://wwwsemanticscholar.org/paper/Quantilizers%3A-A-Safer-Alternative-to-Maximizers-for-Taylor/4e8ff3b4069a12a00196d62925bab8add7389742.\n\n- Jessica Taylor, Eliezer Yudkowsky, Patrick LaVictoire, and Andrew Critch. Alignment for Advanced Machine Learning Systems. In Ethics of Artificial Intelligence. Oxford University Press, August 2020. ISBN 978-0-19-090505-7. Google-Books-ID: 1yT3DwAAQBAJ.\n\n- Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D. Dragan, and Daniel S. Brown. Causal Confusion and Reward Misidentification in Preference-Based Reward Learning, March 2023. URL http://arxiv.org/abs/2204.06601.arXiv:2204.06601 [cs].\n\n- Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested Traffic States in Empirical Observations and Microscopic Simulations. Physical Review E, 62(2):1805-1824, August 2000. ISSN 1063-651X, 1095-3787. doi: 10.1103/PhysRevE.62.1805. URL http://arxiv.org/abs/ cond-mat/0002177. arXiv:cond-mat/0002177.\n\n- A. M. Turner, Logan Smith, Rohin Shah, Andrew Critch, and Prasad Tadepalli. Optimal Policies Tend To Seek Power. December 2019. URL https://wwwsemanticscholar.org/paper/Optimal-Policies-Tend-To-Seek-Power-Turner-Smith/46d4452eb041e33f1e58eab64ecf5af534b6ff.\n\n- Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Jos\u00e9phine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, Sergey Levine, and Karol Hausman. Jump-Start Reinforcement Learning, July 2023. URL http://arxiv.org/abs/2204.02372.arXiv:2204.02372 [cs].\n\n- Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R\u00e9mi Munos, and Matthieu Geist. Leverage the Average: an Analysis of KL Regularization in RL, January 2021. URL http://arxiv.org/abs/2003.14089. arXiv:2003.14089 [cs, stat].\n\n- Quan Vuong, Aviral Kumar, Sergey Levine, and Yevgen Chebotar. Dual Generator Offline Reinforcement Learning, November 2022. URL http://arxiv.org/abs/2211.01471.arXiv:2211.01471 [cs].\n\n- Cathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, and Alexandre M. Bayen. Flow: A Modular Learning Framework for Mixed Autonomy Traffic. IEEE Transactions on Robotics, 38 (2):1270-1286, April 2022. ISSN 1552-3098, 1941-0468. doi: 10.1109/TRO.2021.3087314. URL http://arxiv.org/abs/1710.05465.arXiv:1710.05465 [cs].\n\n- Tian Xu, Ziniu Li, and Yang Yu. Error Bounds of Imitating Policies and Environments, October 2020. URL http://arxiv.org/abs/2010.11876. arXiv:2010.11876 [cs].\n\n- Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J. Ramadge. Accelerating Safe Reinforcement Learning with Constraint-mismatched Policies, July 2021. URL http://arxiv.org/abs/2006.11645. arXiv:2006.11645 [cs, stat].\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638533.5516593", "msecs": "551.0", "relativeCreated": "130382.15684890747", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Bayesian WeakS-to-Strong from Text Classification to Generation\n\n# Abstract\nAdvances in large language models raise the question of how alignment techniques will adapt as models become increasingly complex and humans will only be able to supervise them weakly. Weak-to-Strong mimics such a scenario where weak model supervision attempts to harness the full capabilities of a much stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by exploring an ensemble of weak models which simulate the variability in human opinions. Confidence scores are estimated using a Bayesian approach to guide the WeakS-to-Strong generalization. Furthermore, we extend the application of WeakS-to-Strong from text classification tasks to text generation tasks where more advanced strategies are investigated for supervision. Moreover, direct preference optimization is applied to advance the student model's preference learning, beyond the basic learning framework of teacher forcing. Results demonstrate the effectiveness of the proposed approach for the reliability of a strong student model, showing potential for superalignment.\n\n\n# 1 INTRODUCTION\n\n\nWith the increase in computing power and the amount of training data available, the capabilities of large language models (LLMs) have been continuously brought closer to humans in many aspects. Despite their impressive performance, the preferences and values of pre-trained LLMs do not always align with humans, and dedicated approaches are needed to tackle the problem. Based on large-scale instruction datasets, supervised finetuning (SFT) encourages LLMs to follow human instructions more strictly and respond more safely (Wei et al., 2022). Reinforcement learning (RL) is commonly applied to such alignment. By collecting model output values and the corresponding human feedback, the model can be finetuned by RL to avoid generating undesirable outputs (Ziegler et al., 2019; Bai et al., 2022a; Ouyang et al., 2022; Nakano et al., 2021; Askell et al., 2021).\n\n\nSince no current model has yet surpassed human intelligence, alignment methods, such as SFT and RL from human feedback (RLHF), remain effective. However, it is worthwhile considering future scenarios where artificial intelligence (AI) might surpass human intelligence in all aspects. Would the current alignment methods still be effective for such super AI models? How could humans supervise the super AI? To simulate this future scenario, an analogy situation is designed that downgrades both sides: using a weak model to simulate humans and a strong model to simulate future super AI (Burns et al., 2023), which is termed as superalignment. It has been demonstrated that adding a simple auxiliary loss can achieve effective Weak-to-Strong generalization, even if the weak model's supervision contains many errors, which offers hope of achieving superalignment. Nonetheless, this is just the beginning of exploring along the path of Weak-to-Strong.\n\n\nThis paper extends the discussion on Weak-to-Strong in two directions. First, given the inherent capability gap between the weak model and the strong model, we propose using an ensemble of multiple weak models to improve the quality of weak supervision, which is called WeakS-to-Strong. This also accounts for the scenario where human opinions might diverge in tasks without a commonly accepted standard. Several approaches have been studied to effectively leverage the diversity of different weak models, and we adapt a Bayesian approach referred to as evidential deep learning (EDL) (Sensoy et al., 2018) to better estimate broader human preferences by learning a prior\n\n\ndistribution over the weak labels produced by the weak models. Furthermore, the Weak-to-Strong task was primarily studied for text classification tasks (Burns et al., 2023). This paper extends the scope to text generation and compares three different approaches, Naive Multi-Weak, Joint Decoding and Bayesian Multi-Weak, as shown in Figure 1. The proposed Bayesian WeakS-to-Strong approach is demonstrated effective for both classification and generation. To better align with human preferences, a variant of direct preference optimization (DPO) (Rafailov et al., 2023) called conservative DPO (cDPO) (Eric, 2023) is used to finetune the strong model further on RL principles.\n\n\nOur main contributions are summarized as follows.\n\n- - We proposed Bayesian WeakS-to-Strong which largely improves the quality of weak supervision and recovers the performance of the strong model.\n\n- - We propose to generalize both Weak-to-Strong and WeakS-to-Strong from text classification to generation tasks, extending their scope from content regulation to content generation.\n\n- - When applied to text generation, a token-level probability estimation is proposed to achieve soft labels for strong model training. We also propose the modified DPO algorithm under the Bayesian WeakS-to-Strong framework to further improve text generation performance.\n\n\n# 2 RELATED WORK\n\n\nAI Alignment. Aligning LLMs with human preferences has been a long-standing goal. Instruction tuning uses extensive datasets to improve LLMs' adherence to human instructions (Wei et al., 2022). RL allows LLMs to learn what types of responses humans prefer or dislike, with proximal policy optimization (PPO) being an effective RL method first applied to LLMs and becoming part of the standard RLHF process (Ziegler et al., 2019; Bai et al., 2022a; Ouyang et al., 2022; Nakano et al., 2021; Askell et al., 2021). However, PPO training can be unstable, leading to the development of DPO (Rafailov et al., 2023). Given the high cost of obtaining human preference data, researchers are now exploring the use of LLMs to simulate human preferences, provide feedback, and finetune models (Lee et al., 2023; Bai et al., 2022b; Gulcehre et al., 2023).\n\n\nVariability in Human Opinions. In the process of aligning AI with human preferences, it is important to consider the inconsistency of human preferences (Liu et al., 2023), which often leads to multi-label problems. Previously, many approaches used simple methods like voting, aggregation, and averaging to handle multi-labels (Davani et al., 2022; Munos et al., 2023; Paun & Simpson, 2021; Prabhakaran et al., 2021). However, these methods do not effectively capture the preference differences of individual annotators included in the multiple labels. To better estimate the diversity of human preferences, Bayesian principles have been introduced. Deep learning models can be used to predict prior distributions, which are considered to produce the multiple available labels to estimate a broader range of human preferences (Sensoy et al., 2018; Wu et al., 2022; 2023).\n\n\nWeak-to-Strong. The goal of Weak-to-Strong is to use a weak model to better supervise a strong model. OpenAI demonstrated that adding auxiliary confidence loss from the strong model itself can significantly improve the Weak-to-Strong performance (Burns et al., 2023). Following OpenAI's work, several studies emerged to introduce multiple weak models, used either in series or parallel, to improve the quality of supervision provided by the weak models (Liu & Alahi, 2024; Sang et al., 2024). Early model ensemble methods like Adaboost (Freund & Schapire, 1995) and Bootstrap aggregating (Leo, 1996) were explored in these works. Furthermore, confidence scores are incorporated to help the strong model assess the supervision quality provided by the weak models (Guo et al., 2024) and the weak model can be directly used to modify the output of the strong model (Ji et al., 2024).\n\n\n# 3 WEAKS-TO-STRONG METHODOLOGY\n\n\n# 3.1 PRELIMINARY: WEAK-TO-STRONG\n\n\nThe Weak-to-Strong pipeline (Burns et al., 2023) involves three steps: (i) create a weak supervisor by finetuning a small pre-trained model on ground-truth labels; (ii) train a strong student model  $f_{\\Lambda}$  with weak supervision by finetuning a pre-trained LLM using \"weak labels\" generated by weak supervisors, where  $\\Lambda$  is the parameters of the strong model; (iii) finetune the large pre-trained model directly using ground-truth labels which serve as the ceiling.\n\n\nQ: How to keep healthy?\n\n\n![(a) Naive Multi-Weak](images/719f5f251c1122ea35470621f2967a2b9212c2ffa5562e49e25ca295b284de96.jpg)\n\n\n![(b) Joint Decoding](images/12fe8e993454bc48da342b6a1e14a26c610763db2c1181301495f9f34604aacb.jpg)\n\n\n![(c) Bayesian Multi-Weak Figure 1: An overview diagram of the three ensemble approaches: (a) Naive Multi-Weak: directly learn all weak labels produced by weak models, (b) Joint Decoding: weak models collaboratively determine one single target, (c) Bayesian Multi-Weak: learn a prior distribution over weak labels.](images/ddf6339aeecd7cab9d997a2d7fbceff38b2be5b6dccaff238d62e5abd5492685.jpg)\n\n\nTo leverage the superior generalization capabilities and prior knowledge of the strong model, a loss function with auxiliary confidence loss is proposed (Burns et al., 2023):\n\n\n$$\n\n$$\n\\mathcal {L} = (1 - \\gamma) \\cdot \\mathcal {L} _ {\\mathrm {C E}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\boldsymbol {y} _ {w}\\right) + \\gamma \\cdot \\mathcal {L} _ {\\mathrm {C E}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\hat {\\boldsymbol {f}} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x})\\right) \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\pmb{y}_w$  represents the weak label from weak model,  $\\hat{f}_{\\Lambda}(\\pmb{x})$  refers to the predicted class of strong model given input  $\\pmb{x}$ , and  $\\mathcal{L}_{\\mathrm{CE}}(\\cdot, \\cdot)$  denotes the cross-entropy loss. The second term is an (optional) auxiliary self-training loss designed to increase the confidence of the strong model in itself. The weight of the second loss  $\\gamma$  linearly grows up from 0 to a pre-defined hyper-parameter  $\\gamma_{\\mathrm{max}}$ , which gradually reduces the weight on the weak labels and increases the weight on self-training when the number of training steps increases.\n\n\n# 3.2 EXTENDING WEAK-TO-STRONG WITH MULTIPLE WEAK MODELS\n\n\nAlthough it has been shown that the Weak-to-Strong approach can recover part of the strong model's performance (Burns et al., 2023), the errors in weak labels limit the performance of Weak-to-Strong generalization. In response to this problem, we propose to leverage the complementarity of the error patterns of multiple weak models using an ensemble strategy, which is referred to as WeakS-to-Strong.\n\n\nA naive approach to implementing an ensemble of multiple weak models is to calculate the loss for each weak label respectively and then average these losses. An improvement of this approach is to take a weighted sum instead of a simple average:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {N a i v e}} = \\sum_ {i = 1} ^ {N} \\lambda_ {i} \\mathcal {L} _ {\\mathrm {C E}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\boldsymbol {y} _ {w} ^ {(i)}\\right)), \\tag {2}\n$$\n\n$$\n\n\nwhere  $N$  is the number of weak models,  $\\pmb{y}_w^{(i)}$  is the  $i$ th weak label produced by the  $i$ th weak model, and  $\\lambda_{i}$  is a pre-defined weight of the loss regarding the  $i$ th weak model. This approach is referred to as a Naive Multi-Weak system in the rest of the paper (as illustrated in Figure 1(a)), which is treated as one of the baselines.\n\n\n# 3.3 BAYESIAN WEAKS-TO-STRONG\n\n\nFor superalignment, multiple weak models are used to mimic the subjective preferences of multiple humans, which can be considered as observations drawn from an underlying distribution of the opinions of all humans. The naive approach described in Section 3.2 solely relies on these observations. The number of observations (human annotations or weak labels) is often very limited due to the considerable cost of hiring a new human annotator or training a new weak model. Such a limited number of observations may not result in a good approximation of the true human opinion distribution.\n\n\nHaving biased preferences or values is particularly unacceptable in the safety domain and can cause a failure of superalignment. Therefore, we propose a Bayesian WeakS-to-Strong approach based on EDL (Sensoy et al., 2018) to estimate the human opinion distribution based on the weak labels. Figure 1(c) illustrates the framework where three weak models are involved.\n\n\nFor a given input  $\\mathbf{x}$ , consider a weak label from the  $i$ th weak model  $\\mathbf{y}_w^{(i)}$ , which is a one-hot vector with  $y_w^{(i,k)}$  being one if it belongs to class  $k$  and zero otherwise.  $\\mathbf{y}_w^{(i)}$  is sampled from a categorical distribution of weak labels  $\\mathrm{Cat}(\\pi)$ , where each component  $\\pi_k$  corresponds to the probability assignment over the possible classes  $\\mathbf{y}_w^{(i)} \\sim \\mathrm{P}(\\mathbf{y}|\\pi) = \\mathrm{Cat}(\\pi)$ . EDL places a Dirichlet prior over the categorical distribution representing the probability of each categorical probability assignment, hence modelling second-order probability  $\\pi \\sim \\mathrm{p}(\\pi|\\alpha)$ , where  $\\alpha$  is the hyperparameter of the Dirichlet prior. The strong model  $f_{\\Lambda}$  is trained to predict  $\\alpha$  for each input by minimizing the negative log-likelihood of sampling  $\\mathbf{y}_w^{(i)}$  given the predicted Dirichlet prior:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {N L L}} ^ {(i)} = - \\log \\int \\mathrm {P} (\\boldsymbol {y} _ {w} ^ {(i)} | \\boldsymbol {\\pi}) \\mathrm {p} (\\boldsymbol {\\pi} | \\boldsymbol {\\alpha}) \\mathrm {d} \\boldsymbol {\\pi} = \\sum_ {k = 1} ^ {K} y _ {w} ^ {(i, k)} \\left(\\log \\left(\\alpha_ {0}\\right) - \\log \\left(\\alpha_ {k}\\right)\\right), \\tag {3}\n$$\n\n$$\n\n\nwhere  $K$  is the number of classes,  $\\alpha_0 = \\sum_{k=1}^{K} \\alpha_k$  is the Dirichlet strength, and  $y_w^{(i,k)}$  is the  $k^{th}$  value of label  $\\mathbf{y}_w^{(i)}$ . When a sample is not correctly classified, it is expected that the prior should approach non-informative prior for this sample. Following Sensoy et al. (2018), a regularization term  $\\mathcal{L}_{\\mathrm{REG}}^{(i)}$  (KL-divergence between the misleading prediction and non-informative distribution, see Appendix H for details) is added to penalize incorrect predictions and calibrate uncertainty estimation, resulting in the final EDL loss  $\\mathcal{L}_{\\mathrm{EDL}}^{(i)} = \\mathcal{L}_{\\mathrm{NLL}}^{(i)} + \\lambda_{\\mathrm{EDL}} \\mathcal{L}_{\\mathrm{REG}}^{(i)}$  where  $\\lambda_{\\mathrm{EDL}}$  is the coefficient.\n\n\nApart from the class predicted by the weak models, the confidence of weak models is also incorporated for better distribution estimation. Let  $(p_1^{(i)},\\ldots ,p_K^{(i)})$  be the probability assignment predicted by the  $i^{th}$  weak model, the EDL loss for each class is calculated based on the predicted probability assignment for each weak model and then combined in the same way as in Eqn. (2). That is,\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {E D L}} \\left( \\right.\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\left.\\left. \\boldsymbol {y} _ {w} ^ {(i)} \\right\\} _ {i = 1} ^ {N}\\right) = \\sum_ {i = 1} ^ {N} \\lambda_ {i} \\sum_ {k = 1} ^ {K} p _ {k} ^ {(i)} \\mathcal {L} _ {\\mathrm {E D L}} ^ {(i)} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\hat {\\boldsymbol {y}} _ {w} ^ {(i, k)}\\right) \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\hat{\\pmb{y}}_w^{(i,k)}$  is the predicted result, i.e., one-hot vector for class  $k$ , and  $\\lambda_{i}$ s are hyperparameters set to the same values as used in the Naive Multi-Weak approach. As a result, the auxiliary confidence loss described in Eqn. (1) is adapted for Bayesian WeakS-to-Strong as follows:\n\n\n$$\n\n$$\n\\mathcal {L} = (1 - \\gamma) \\cdot \\mathcal {L} _ {\\mathrm {E D L}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\left\\{\\boldsymbol {y} _ {w} ^ {(i)} \\right\\} _ {i = 1} ^ {N}\\right) + \\gamma \\cdot \\mathcal {L} _ {\\mathrm {E D L}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\hat {\\boldsymbol {f}} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x})\\right). \\tag {5}\n$$\n\n$$\n\n\nIn the term of  $\\mathcal{L}_{\\mathrm{EDL}}(f_{\\Lambda}(\\pmb{x}), \\hat{f}_{\\Lambda}(\\pmb{x}))$ , the class index predicted by the strong model  $\\hat{f}_{\\Lambda}(\\pmb{x})$  is used as the target. That is to say, the predictions of the strong student model are applied as part of the distribution estimation along with the weak label.\n\n\n# 4 WEAKS-TO-STRONG FOR SEQUENCE GENERATION\n\n\n# 4.1 PROBABILITY ESTIMATION FOR WEAK SEQUENCE LABELS\n\n\nTo enable the strong model to directly generate trustworthy content rather than only being trained to understand whether the content is trustworthy or not, we propose to extend the scope of Weak-to-Strong from text classification to text generation. The key challenge of directly applying the Weak-to-Strong loss to the sequence generation task is the token-level soft labelling for the target sequence. As the tokenizers are different between weak and strong models, it is infeasible to obtain a one-to-one mapping from weak model output distributions to each token in the target sequence.\n\n\nTo obtain the soft label  $\\mathbf{y}_w$  for the strong model using weak model output probabilities and bridge the gap caused by different tokenizers, we use words as an intermediary, following the equation\n\n\n$$\n\n$$\n\\mathrm {P} (W) = \\mathrm {P} \\left(w _ {2} \\mid w _ {1}\\right) \\mathrm {P} \\left(w _ {1}\\right) = \\mathrm {P} \\left(s _ {2} \\mid s _ {1}\\right) \\mathrm {P} \\left(s _ {1}\\right), \\tag {6}\n$$\n\n$$\n\n\n![Stage 1](images/b1e35976934dd183f3d420fea77a818c6696f80f5a10c5c073759afd7fbcea55.jpg)\n\n\n![Stage 3 Figure 2: The process of transforming per-token confidence scores from the sequence tokenized by the weak model to the sequence tokenized by the strong. The word \"hello\" is used as an example.](images/1faf5604f6e1b1a5af00e4a34d2252ab2df239e9257a649dba6ca1582fb1c2bb.jpg)\n\n\nwhere we use a word containing two wordpieces from both weak and strong tokenizers as an example, and  $w_{1}, w_{2}$  and  $s_{1}, s_{2}$  are both token strings that can form word  $W$ , which are generated by the tokenizers of the weak and strong models respectively. Figure 2 shows an example of the process in three stages. In stage 1, the per-token output probabilities of weak models are obtained when generating output sequences. The probabilities of wordpieces in a word are then multiplied together to obtain the score of word  $W$ , following Eqn. (6).\n\n\nIn stage 2, the word probability is used to assign probabilities to tokens from the strong model tokenizer. In the process of training the strong model via teacher-forcing, the model gives a probability to each token by applying softmax to the output logits. This probability can be seen as the model's confidence in predicting the target token, where we assume that the weak model and strong model have similar confidence for wordpiece tokens in similar positions. This allows us to obtain the actual assignment of scores to each target token  $s_i$ , instead of assigning equal probabilities to all tokens involved. Taking an example of splitting a word  $W$  into two target wordpieces  $s_1$  and  $s_2$ , the decomposition can be approximated by\n\n\n$$\n\n$$\n\\log \\mathrm {P} (s _ {1}) = \\frac {e ^ {- C _ {s} (s _ {1})}}{e ^ {- C _ {s} (s _ {1})} + e ^ {- C _ {s} (s _ {2})}} \\log \\mathrm {P} (W), \\quad \\log \\mathrm {P} (s _ {2}) = \\frac {e ^ {- C _ {s} (s _ {2})}}{e ^ {- C _ {s} (s _ {1})} + e ^ {- C _ {s} (s _ {2})}} \\log \\mathrm {P} (W), \\tag {7}\n$$\n\n$$\n\n\nwhere  $C_s(s_i)$  is strong model confidence at the step predicting wordpiece  $s_i$  that is the maximum probability in the strong model output distribution at that step in practice. In this way, lower target probabilities are allocated to tokens with lower strong model confidence, while higher probabilities are allocated to tokens with higher strong model confidence.\n\n\nAfter obtaining the probability of the target token of the strong model, the probabilities of other categories can be obtained by scaling strong prediction probabilities, which can be treated as the soft label, as shown in Figure 2. Then the obtained soft labels can be handled using methods similar to those used in classification (as described in Section 3). Notably, during the computation of EDL loss, the sparsity caused by high dimensional spaces results in a large KL (Kullback-Leibler) penalty term. To solve this problem, a coefficient is added to the KL penalty to balance it with the magnitude of the negative log-likelihood term. Additionally, clamping is applied to restrict all values within an appropriate range, preventing potentially extremely large outliers on any particular token.\n\n\n# 4.2 DPO FOR SEQUENCE GENERATION OPTIMIZATION\n\n\nDifferent from classification tasks, sequence generation tasks often benefit from sequence-level objectives that directly optimize the entire sequence jointly rather than the individual tokens separately. To further improve the strong model for sequence generation, direct preference optimization (DPO) (Rafailov et al., 2023) is investigated for WeakS-to-Strong after supervised finetuning, where we propose to use weak models to provide the preference for the strong model generation.\n\n\nAfter the strong model is pretrained by supervised finetuning, it generates  $M$  output sequences based on a given output. Then for each sequence,  $N$  scores are computed by generating it using  $N$  weak models separately (via teacher forcing) and aggregating the output (log-)probabilities. A weighted sum of the  $N$  scores is performed as the final score assigned to each sequence. The sequence with the highest final score is viewed as the preferred sequence in DPO training, as shown below\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {c} = \\underset {m, m = 1, 2, \\dots , M} {\\arg \\max } \\mathrm {P} (\\boldsymbol {y} _ {s} (m)) = \\underset {m, m = 1, 2, \\dots , M} {\\arg \\max } \\sum_ {i = 1} ^ {N} \\lambda_ {i} \\mathrm {P} (\\boldsymbol {y} _ {s} (m) | \\boldsymbol {\\theta} _ {i}), \\tag {8}\n$$\n\n$$\n\n\nwhere  $\\pmb{y}_s(m)$  is the  $m$ th output sequence generated by the strong model,  $\\mathrm{P}(\\pmb{y}_s(m)|\\pmb{\\theta}_i)$  is computed using the  $i$ th weak model with model parameters  $\\pmb{\\theta}_i$ , and  $\\lambda_i$  is the weight assigned to the  $i$ th weak model. The dispreferred sequence can be computed similarly by  $\\pmb{y}_r = \\arg \\min \\mathrm{P}(\\pmb{y}_s(m))$ .\n\n\nConsidering potential errors by weak models, a variant of DPO, conservative DPO (cDPO) (Eric, 2023) with a more conservative target distribution is applied in our work. The loss of cDPO is\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {D P O}} ^ {\\epsilon} = (1 - \\epsilon) \\mathcal {L} _ {\\mathrm {D P O}} (\\boldsymbol {\\Lambda}, \\boldsymbol {y} _ {c}, \\boldsymbol {y} _ {r}) + \\epsilon \\mathcal {L} _ {\\mathrm {D P O}} (\\boldsymbol {\\Lambda}, \\boldsymbol {y} _ {r}, \\boldsymbol {y} _ {c}), \\tag {9}\n$$\n\n$$\n\n\nwhere  $\\epsilon$  is a small constant probability that labels are flipped to make DPO more conservative, and  $\\mathcal{L}_{\\mathrm{DPO}}$  is the standard DPO loss in (Rafailov et al., 2023), which can be written as\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {D P O}} (\\boldsymbol {\\Lambda}, \\boldsymbol {y} _ {c}, \\boldsymbol {y} _ {r}) = - \\log \\sigma \\left(\\beta \\log \\frac {\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {y} _ {c})}{\\boldsymbol {f} _ {\\text {r e f}} (\\boldsymbol {y} _ {c})} - \\beta \\log \\frac {\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {y} _ {r})}{\\boldsymbol {f} _ {\\text {r e f}} (\\boldsymbol {y} _ {r})}\\right). \\tag {10}\n$$\n\n$$\n\n\n# 5 EXPERIMENTAL SETUP\n\n\n# 5.1 DATASETS\n\n\nClassification Task. The setup of the classification task follows Burns et al. (2023). The SciQ dataset (Welbl et al., 2017) is used, which contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. In our experiment, 5k data samples were extracted for training weak models and another 5k samples were reserved for generating weak labels to train the strong model. The standard test set which contains 1k data samples was used for the test. The data is restructured into a balanced binary classification task, i.e., given a question and an answer, the model is required to determine whether the answer is correct.\n\n\nSlot filling. The performance of WeakS-to-Strong on the reliability of generated content was evaluated on the slot-filling task, which is a crucial spoken language understanding task aiming at filling in the correct value for predefined slots (e.g. restaurant and hotel names). SLURP dataset (Bastianelli et al., 2020) was used which contains 16.5k sentences and 72k audio recordings of single-turn user interactions with a home assistant, annotated with scenarios, actions and entities. Only the reference transcriptions of the speech were used for training. Following Sun et al. (2023a;b), we designed the prompt with slot keys and descriptions in the same way. In our setup, 2k utterances from the train split were extracted for training the weak models, and another 2k utterances were reserved for generating weak labels and training the strong model. We report the performance of both weak and strong models on the standard SLURP test set.\n\n\n# 5.2 MODELS AND BASELINES\n\n\nModels. For classification, the Qwen-7B (Bai et al., 2023) model was applied as the strong student model. Five models were used as weak teachers: GPT2-Large (Radford et al., 2019), OPT-1.3B (Zhang et al., 2022), Pythia-1.4B (Biderman et al., 2023), BLOOM-1B1 (Le Scao et al., 2022),\n\n\nand TinyLlama_v1.1 (Zhang et al., 2024). The last linear layer which maps the embeddings to tokens is replaced with a linear classification head with two outputs to adapt language models to the classification setting.\n\n\nFor slot filling, Llama-2-7B (Touvron et al., 2023) was used as the strong model which yielded better performance than Qwen-7B in this task. The same set of weak models was used as the classification task. As before, both weak and strong models are finetuned with all model parameters.\n\n\nExperiments with different numbers of weak models were conducted. One set involved three weak models (GPT2-Large, OPT-1.3B and Pythia-1.4B) in a WeakS-to-Strong experiment, which is referred to as WeakS-to-Strong-3. Another set included all five weak models, called WeakS-to-Strong-5.\n\n\nBaselines. The proposed Bayesian WeakS-to-Strong method is compared to the following three baselines.\n\n\nNaive Multi-Weak. The Naive Multi-Weak approach introduced in Section 3.2 servers as the baseline for both classification and generation tasks. For the classification task, the loss can be computed following Eqn. (2). For generation task, Eqn. (2) is modified as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {N a i v e}} ^ {\\text {G e n}} = \\sum_ {i = 1} ^ {N} \\lambda_ {i} \\frac {1}{T _ {i}} \\sum_ {j = 1} ^ {T _ {i}} \\mathcal {L} _ {\\mathrm {C E}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} \\left(\\boldsymbol {x}, \\boldsymbol {y} _ {w} ^ {(i, 1)}, \\dots , \\boldsymbol {y} _ {w} ^ {(i, j - 1)}\\right), \\boldsymbol {y} _ {w} ^ {(i, j)}\\right)), \\tag {11}\n$$\n\n$$\n\n\nwhere  $\\{\\pmb{y}_w^{(i,1)},\\pmb{y}_w^{(i,2)},\\dots,\\pmb{y}_w^{(i,T)}\\}$  is the sequence generated by  $i$ th weak model with length  $T_{i}$ . In contrast to the classification task, where the strong model  $\\pmb{f}_{\\Lambda}$  takes only  $\\pmb{x}$  as input, in the generation task,  $\\pmb{x}$  is paired with the sequence generated by each weak model (which is the weak target) and then fed into the strong model to obtain predictions. Teacher-forcing is used during training.\n\n\nFlyingSquid. FlyingSquid (Fu et al., 2020) is a method for weak supervision, estimating the accuracies and correlations among multiple noisy label functions (different weak labels in our case) without ground-truth data. Latent variable probabilistic graphical models are used to model these dependencies, with weak labels as observed variables and unobserved ground-truth labels as hidden variables. Since FlyingSquid is designed for binary classification, this baseline is only used for classification. Through this method, we get a label model with multiple weak labels and obtain the probability for the positive category, which is then used as a soft label in strong model training.\n\n\nJoint Decoding. Joint Decoding is used as an additional baseline for text generation tasks, which is specifically designed for multiple weak model generations. In contrast to the Naive Multi-Weak scheme where each weak model provides a weak target sequence, Joint Decoding employs multiple weak models to collaboratively determine one single target, as illustrated in Figure 1(b). Specifically, we perform Joint Decoding in a re-ranking fashion. For each weak model, the top  $M$  sequences are generated by beam search in decoding. The sequences from the  $N$  weak models are gathered to form a list of  $M \\times N$  sequences. Then each sequence is scored in the same way in Section 4.2. The sequence with the highest final score is used as the target sequence for strong model training. Unless otherwise mentioned,  $M = 5$  was used in the experiments.\n\n\nEach weak-to-strong experiment was run three times with different random seeds. The average and standard deviation were reported. More implementation details can be found in Appendix C.\n\n\n# 5.3 EVALUATION METRICS\n\n\nThe classification task is evaluated by accuracy, and the SLU-F1 (Bastianelli et al., 2020) is used for slot filling, which combines both word-level and character-level F1 scores to give partial credit to non-exact match predictions. Performance gap recovered (PGR) (Burns et al., 2023) is used to measure the performance gap recovered with weak supervision, which is defined as follows:\n\n\n$$\n\n$$\n\\mathrm {P G R} := \\frac {\\mathcal {P} - \\mathcal {P} _ {w}}{\\mathcal {P} _ {s} - \\mathcal {P} _ {w}} \\tag {12}\n$$\n\n$$\n\n\nwhere  $\\mathcal{P}$  is Weak-to-Strong performance,  $\\mathcal{P}_s$  strong performance and  $\\mathcal{P}_w$  weak performance. For multiple weak model cases, the average of PGRs for each weak model is treated as the final result.\n\n\nTable 1: Performance of single model on text classification task. Trained by ground-truth labels.\n\n\n<table><tr><td></td><td>Pre-trained model</td><td>#Param</td><td>Accuracy</td></tr><tr><td>Strong Model (ceiling)</td><td>Qwen-7B</td><td>7.7B</td><td>0.898</td></tr><tr><td rowspan=\"5\">Weak Model</td><td>GPT2-Large</td><td>0.8B</td><td>0.717</td></tr><tr><td>OPT-1.3B</td><td>1.3B</td><td>0.699</td></tr><tr><td>Pythia-1.4B</td><td>1.4B</td><td>0.685</td></tr><tr><td>BLOOM-1B1</td><td>1.1B</td><td>0.729</td></tr><tr><td>TinyLlama_v1.1</td><td>1.1B</td><td>0.731</td></tr></table>\n\n\nTable 2: Weak(S)-to-Strong performance (train the strong model on the weak label) on text classification task for with (w/) and without (w/o) auxiliary loss.  $\\gamma$  in Eqn. (1) and Eqn. (5) is set to 0 if auxiliary loss is not used. Experiments with 3 weak models and 5 weak models were conducted. Each experiment was run with three different random seeds. The best results are shown in bold.\n\n\n<table><tr><td rowspan=\"2\" colspan=\"2\"></td><td colspan=\"2\">w/o aux loss</td><td colspan=\"2\">w/ aux loss</td></tr><tr><td>Accuracy</td><td>PGR</td><td>Accuracy</td><td>PGR</td></tr><tr><td rowspan=\"5\">Weak-to-Strong</td><td>GPT2-Large</td><td>0.808 \u00b1 0.007</td><td>0.503 \u00b1 0.034</td><td>0.828 \u00b1 0.006</td><td>0.614 \u00b1 0.029</td></tr><tr><td>OPT-1.3B</td><td>0.807 \u00b1 0.005</td><td>0.541 \u00b1 0.026</td><td>0.841 \u00b1 0.012</td><td>0.714 \u00b1 0.058</td></tr><tr><td>Pythia-1.4B</td><td>0.775 \u00b1 0.009</td><td>0.421 \u00b1 0.042</td><td>0.793 \u00b1 0.009</td><td>0.507 \u00b1 0.042</td></tr><tr><td>BLOOM-1B1</td><td>0.823 \u00b1 0.015</td><td>0.556 \u00b1 0.087</td><td>0.843 \u00b1 0.008</td><td>0.677 \u00b1 0.049</td></tr><tr><td>TinyLlama_v1.1</td><td>0.832 \u00b1 0.004</td><td>0.603 \u00b1 0.024</td><td>0.838 \u00b1 0.005</td><td>0.643 \u00b1 0.030</td></tr><tr><td rowspan=\"3\">WeakS-to-Strong-3</td><td>Naive Multi-Weak</td><td>0.816 \u00b1 0.002</td><td>0.586 \u00b1 0.008</td><td>0.831 \u00b1 0.013</td><td>0.661 \u00b1 0.064</td></tr><tr><td>FlyingSquid</td><td>0.809 \u00b1 0.005</td><td>0.549 \u00b1 0.026</td><td>0.825 \u00b1 0.003</td><td>0.631 \u00b1 0.013</td></tr><tr><td>Bayesian Mutli-Weak</td><td>0.819 \u00b1 0.006</td><td>0.600 \u00b1 0.033</td><td>0.850 \u00b1 0.006</td><td>0.756 \u00b1 0.028</td></tr><tr><td rowspan=\"3\">WeakS-to-Strong-5</td><td>Naive Multi-Weak</td><td>0.832 \u00b1 0.005</td><td>0.641 \u00b1 0.025</td><td>0.853 \u00b1 0.006</td><td>0.754 \u00b1 0.032</td></tr><tr><td>FlyingSquid</td><td>0.832 \u00b1 0.004</td><td>0.643 \u00b1 0.023</td><td>0.855 \u00b1 0.007</td><td>0.768 \u00b1 0.035</td></tr><tr><td>Bayesian Mutli-Weak</td><td>0.831 \u00b1 0.008</td><td>0.627 \u00b1 0.027</td><td>0.866 \u00b1 0.006</td><td>0.828 \u00b1 0.038</td></tr></table>\n\n\n# 6 RESULTS\n\n\n# 6.1 TEXT CLASSIFICATION\n\n\nThe proposed Bayesian WeakS-to-Strong approach was first evaluated on a classification task. Table 1 shows the respective performance of the strong model and the weak models trained using ground-truth labels, with the former being the ceiling of the Weak(S)-to-Strong approaches. The strong model has about 7 times the number of parameters as the weak models, which also leads to about  $28\\%$  relative improvement in the classification accuracy. Results of Weak(S)-to-Strong approaches are shown in Table 2.  $\\gamma$  in Eqn. (1) and Eqn. (5) were set to 0 if auxiliary loss was not used. It can be seen that auxiliary loss was effective for both. Comparing single Weak-to-Strong results, different weak models show significant differences in performance. For example, Pythia-1.4B recovered only  $50\\%$  of the strong performance, while OPT-1.3B recovered around  $70\\%$ .\n\n\nThe Naive Multi-Weak baseline, FlyingSquid approach and Bayesian Multi-Weak using EDL were applied to the classification task for WeakS-to-Strong. Experiments with three weak models (GPT2-Large, OPT-1.3B and Pythia-1.4B) and all five weak models were conducted. With three weak models, the Naive Multi-Weak method got an average PGR of 0.661, slightly outperforming FlyingSquid method but still lower than the single OPT-1.3B model, and Bayesian Multi-Weak boosted PGR to 0.756. This indicates that a naive ensemble approach doesn't necessarily outperform the best single model, especially when there is a certain weak model which does not perform well. The Bayesian approach can increase the fault tolerance with the usage of prior in this case, as it learns patterns from the entire dataset. With five weak models compared to three, the Bayesian Multi-Weak approach further increased average PGR to 0.828, which is  $16\\%$  relativelt higher than the best single model and  $8\\%$  relatively higher than baselines, and consistently better with all seeds. The results show the effectiveness of the Bayesian approach for distribution estimation.\n\n\nTable 3: Performance of single model on text generation task. Trained by ground-truth labels.\n\n\n<table><tr><td></td><td>Pre-trained model</td><td>#Param</td><td>SLU-F1</td></tr><tr><td>Strong Model (ceiling)</td><td>Llama-2-7B</td><td>6.7B</td><td>0.748</td></tr><tr><td rowspan=\"5\">Weak Model</td><td>GPT2-Large</td><td>0.8B</td><td>0.660</td></tr><tr><td>OPT-1.3B</td><td>1.3B</td><td>0.665</td></tr><tr><td>Pythia-1.4B</td><td>1.4B</td><td>0.680</td></tr><tr><td>BLOOM-1B1</td><td>1.1B</td><td>0.651</td></tr><tr><td>TinyLlama_v1.1</td><td>1.1B</td><td>0.676</td></tr></table>\n\n\nTable 4: Weak(S)-To-Strong performance on text generation task, training strong model on weak labels, for with (w/) and without (w/o) auxiliary loss. In cases without auxiliary loss,  $\\gamma$  in Eqn. (1) and Eqn. (5) is set to 0. Experiments were conducted using 3 and 5 weak labels, each run with three different random seeds. The best results are shown in bold.\n\n\n<table><tr><td rowspan=\"2\" colspan=\"2\"></td><td colspan=\"2\">w/o aux loss</td><td colspan=\"2\">w/ aux loss</td></tr><tr><td>SLU-F1</td><td>PGR</td><td>SLU-F1</td><td>PGR</td></tr><tr><td rowspan=\"5\">Weak-to-Strong</td><td>GPT2-Large</td><td>0.687 \u00b1 0.011</td><td>0.303 \u00b1 0.125</td><td>0.673 \u00b1 0.012</td><td>0.150 \u00b1 0.139</td></tr><tr><td>OPT-1.3B</td><td>0.660 \u00b1 0.059</td><td>-0.066 \u00b1 0.715</td><td>0.696 \u00b1 0.009</td><td>0.367 \u00b1 0.103</td></tr><tr><td>Pythia-1.4B</td><td>0.702 \u00b1 0.007</td><td>0.320 \u00b1 0.095</td><td>0.691 \u00b1 0.022</td><td>0.173 \u00b1 0.322</td></tr><tr><td>BLOOM-1B1</td><td>0.690 \u00b1 0.021</td><td>0.399 \u00b1 0.220</td><td>0.684 \u00b1 0.021</td><td>0.337 \u00b1 0.220</td></tr><tr><td>TinyLlama_v1.1</td><td>0.667 \u00b1 0.006</td><td>0.000 \u00b1 0.084</td><td>0.658 \u00b1 0.016</td><td>-0.250 \u00b1 0.224</td></tr><tr><td rowspan=\"3\">WeakS-to-Strong-3</td><td>Naive Multi-Weak</td><td>0.711 \u00b1 0.008</td><td>0.531 \u00b1 0.101</td><td>0.694 \u00b1 0.013</td><td>0.318 \u00b1 0.169</td></tr><tr><td>Joint Decoding</td><td>0.703 \u00b1 0.003</td><td>0.434 \u00b1 0.032</td><td>0.704 \u00b1 0.015</td><td>0.442 \u00b1 0.191</td></tr><tr><td>Bayesian Mutli-Weak</td><td>0.712 \u00b1 0.014</td><td>0.549 \u00b1 0.180</td><td>0.714 \u00b1 0.014</td><td>0.574 \u00b1 0.176</td></tr><tr><td rowspan=\"3\">WeakS-to-Strong-5</td><td>Naive Multi-Weak</td><td>0.716 \u00b1 0.010</td><td>0.606 \u00b1 0.123</td><td>0.694 \u00b1 0.012</td><td>0.328 \u00b1 0.144</td></tr><tr><td>Joint Decoding</td><td>0.671 \u00b1 0.010</td><td>0.037 \u00b1 0.120</td><td>0.675 \u00b1 0.005</td><td>0.091 \u00b1 0.066</td></tr><tr><td>Bayesian Mutli-Weak</td><td>0.718 \u00b1 0.014</td><td>0.627 \u00b1 0.173</td><td>0.721 \u00b1 0.013</td><td>0.668 \u00b1 0.166</td></tr></table>\n\n\n# 6.2 TEXT GENERATION\n\n\nFor the text generation task on slot filling, the performance of the student strong model and teacher weak model finetuned on ground-truth labels is presented in Table 3. The strong ceiling performance is 0.748, and the highest weak performance is 0.680.\n\n\nThe Weak(S)-to-Strong performances are reported in Table 4. For a single weak model, the Weak-to-Strong model performance didn't necessarily surpass the original weak performance (e.g. TinyLlama_v1.1), and the highest PGR is 0.399. With five weak models, our proposed Bayesian Multi-Weak approach achieved an average PGR of 0.668, which is  $26\\%$  better than a single weak model,  $6\\%$  better than the naive baseline, and consistently better across three seeds. Comparing WeakS-to-Strong performance with and without auxiliary loss, adding auxiliary loss is not effective for naive approaches, but improves the performance of Bayesian approaches. It indicates that the proposed Bayesian method, where the predictions of a strong model are applied as part of the distribution estimation as in Eqn. (5), more effectively integrates strong model predictions into the training process, showing the effectiveness of our Bayesian approach for the model ensemble. The ablation study can be found in Appendix E. A noticeable decline has been observed for Joint Decoding when the number of weak models increases from three to five. This may result from the poor performance of the TinyLlama weak model which affects the quality of the generated weak target. Recall that in Joint Decoding, multiple weak models collaboratively determine one single target. In contrast, Naive Multi-Weak and Bayesian Multi-Weak methods still benefit from the increased number of weak models, which indicates that they are more robust against the quality of a single weak model. Additional experiments on Joint Decoding can be found in Appendix G for more analysis and insights.\n\n\nBased on the strong model supervised by five weak models on the Bayesian Multi-Weak approach with auxiliary loss, a cDPO training is conducted, which further train the model in student-forcing form. Three separate cDPO experiments were conducted using different initial models obtained in SFT stage with three different seeds. The results are shown in Table 5. After cDPO, three models\n\n\nTable 5: Results before and after DPO. Based on the Bayesian Multi-Weak model using five weak models (the WeakS-to-Strong-5 setting) with auxiliary loss. Average results on three seeds are reported. Note that the initial SFT models are different for different seeds.\n\n\n<table><tr><td></td><td colspan=\"2\">seed-0</td><td colspan=\"2\">seed-1</td><td colspan=\"2\">seed-2</td><td colspan=\"2\">Average</td></tr><tr><td></td><td>SLU-F1</td><td>PGR</td><td>SLU-F1</td><td>PGR</td><td>SLU-F1</td><td>PGR</td><td>SLU-F1</td><td>PGR</td></tr><tr><td>Before cDPO</td><td>0.706</td><td>0.477</td><td>0.730</td><td>0.776</td><td>0.728</td><td>0.751</td><td>0.721 \u00b1 0.013</td><td>0.668 \u00b1 0.166</td></tr><tr><td>After cDPO</td><td>0.707</td><td>0.490</td><td>0.733</td><td>0.813</td><td>0.733</td><td>0.813</td><td>0.724 \u00b1 0.015</td><td>0.705 \u00b1 0.187</td></tr></table>\n\n\nshowed consistent performance improvements. The average PGR reached 0.705,  $6\\%$  relatively better than that before DPO, with a maximum PGR of 0.813.\n\n\n# 6.3 COMPLEMENTARITY OF WEAK MODELS\n\n\nAn experiment about the complementarity of different weak models was conducted. For classification models, the agreement is assessed by calculating the accuracy of each model's predictions on test set, treating outputs from other models as references. For generation models, the Levenshtein distance is calculated between different outputs from two models for a same input, which is obtained using the minimum edit number required to change one sequence to another. The average Levenshtein distance across all samples in the test set is used to measure the agreement between two models.\n\n\nThe results are shown in Figure 3. The agreement among different weak models is among 0.75 for classification models, while for generation task the maximum agreement between different models is 0.56. This suggests that the consistency among different weak models is low, thus they can complement each other well as the faults made by different weak models are not the same. Moreover, comparing with Naive Multi-Weak approach, our proposed Bayesian method estimates a distribution based on weak labels, learning patterns from the entire dataset, thereby increasing the tolerance for fault in weak models, as shown in results in Section 6.1 and 6.2.\n\n\n![(a) Agreement of 5 models on classification.](images/8272cf3885079e6b2e4bbe96f01d0176c4306ae74c8807ff68b987bb6fe0d084.jpg)\n\n\n![(b) Agreement of 5 models on generation. Figure 3: Agreement of weak models. The similarity between classification models was assessed by calculating the accuracy of each model's predictions against the others on the test set. For generation models, the agreement is obtained through the Levenshtein distance.](images/7cffc4c0a87ed641fdf8fb76b97330e57e9c6b442cd423f0f04371a2a2e4dba2.jpg)\n\n\n# 7 CONCLUSION\n\n\nThis paper extends the Weak-to-Strong framework to WeakS-to-Strong by leveraging an ensemble of weak models to capture the variability in human opinions. We propose a Bayesian inference method, Bayesian WeakS-to-Strong, to more accurately estimate the weak label distribution based on the outputs of multiple weak models. Additionally, while the original Weak-to-Strong method was limited to text classification tasks, this paper expands its applicability to text generation, enabling both the assessment of content trustworthiness and the generation of trustworthy content. Finally, DPO is utilized to enhance the student model's preference for learning, going beyond the traditional teacher-forcing approach. Our results demonstrate the effectiveness of Bayesian WeakS-to-Strong for both classification and generation tasks, highlighting its potential for superalignment.\n\n\n# REFERENCES\n\n- Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\n\n- Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\n\n- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.\n\n- Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022b.\n\n- Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser. SLURP: A spoken language understanding resource package. In Proc. EMNLP, 2020.\n\n- Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In Proc. ICML, 2023.\n\n- Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.\n\n- Aida Mostafazadeh Davani, Mark Diaz, and Vinodkumar Prabhakaran. Dealing with disagreements: Looking beyond the majority vote in subjective annotations. Transactions of the Association for Computational Linguistics, 10:92-110, 2022.\n\n- Mitchell Eric. A note on DPO with noisy preferences & relationship to IPO. https://ericmitchell.ai/cdpo.pdf, 2023.\n\n- Yoav Freund and Robert E Schapire. A desiccation-theoretic generalization of on-line learning and an application to boosting. In European conference on computational learning theory, pp. 23-37, 1995.\n\n- Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and Christopher R\u00e9. Fast and three-rious: Speeding up weak supervision with triplet methods. In Proc. ICML, 2020.\n\n- Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (ReST) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\n\n- Jianyuan Guo, Hanting Chen, Chengcheng Wang, Kai Han, Chang Xu, and Yunhe Wang. Vision superalignment: Weak-to-strong generalization for vision foundation models. arXiv preprint arXiv:2402.03749, 2024.\n\n- Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. In Proc. EMNLP, 2019.\n\n- Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. Aligner: Achieving efficient alignment through weak-to-strong correction. arXiv preprint arXiv:2402.02416, 2024.\n\n- Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Galle, et al. BLOOM: A 176B-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\n- Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning from human feedback with AI feedback. arXiv preprint arXiv:2309.00267, 2023.\n\n- Breiman Leo. Bagging predictors. Machine Learning, 24:123-140, 1996.\n\n- Yajiao Liu, Xin Jiang, Yichun Yin, Yasheng Wang, Fei Mi, Qun Liu, Xiang Wan, and Benyou Wang. One cannot stand for everyone! leveraging multiple user simulators to train task-oriented dialogue systems. In Proc. ACL, 2023.\n\n- Yuejiang Liu and Alexandre Alahi. Co-supervised learning: Improving weak-to-strong generalization with hierarchical mixture of experts. arXiv preprint arXiv:2402.15505, 2024.\n\n- R\u00e9mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023.\n\n- Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\n\n- Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, et al. Training language models to follow instructions with human feedback. In Proc. NeurIPS, 2022.\n\n- Silviu Paun and Edwin Simpson. Aggregating and learning from multiple annotators. In Proc. ACL, 2021.\n\n- Vinodkumar Prabhakaran, Aida Mostafazadeh Davani, and Mark Diaz. On releasing annotator-level labels and information in datasets. In Proc. ACL, 2021.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1:9, 2019.\n\n- Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Proc. NeurIPS, 2023.\n\n- Jitao Sang, Yuhang Wang, Jing Zhang, Yanxu Zhu, Chao Kong, Junhong Ye, Shuyu Wei, and Jinlin Xiao. Improving weak-to-strong generalization with scalable oversight and ensemble learning. arXiv preprint arXiv:2402.00667, 2024.\n\n- Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification uncertainty. In Proc. NeurIPS, 2018.\n\n- Guangzhi Sun, Shutong Feng, Dongcheng Jiang, Chao Zhang, Milica Ga\u0161i\u0107, and Philip C Woodland. Speech-based slot filling using large language models. arXiv preprint arXiv:2311.07418, 2023a.\n\n- Guangzhi Sun, Chao Zhang, Ivan Vulic, Pawe\u0142 Budzianowski, and Philip C Woodland. Knowledge-aware audio-grounded generative slot filling for limited annotated data. arXiv preprint arXiv:2307.01764, 2023b.\n\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\n- Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Proc. ICLR, 2022.\n\n- Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Proc. W-NUT, 2017.\n\n- Wen Wu, Chao Zhang, Xixin Wu, and Philip C Woodland. Estimating the uncertainty in emotion class labels with utterance-specific Dirichlet priors. IEEE Transactions on Affective Computing, 2022.\n\n- Wen Wu, Chao Zhang, and Philip Woodland. Estimating the uncertainty in emotion attributes using deep evidential regression. In Proc. ACL, 2023.\n\n- Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. TinyLlama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024.\n\n- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\n- Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638534.5796275", "msecs": "579.0", "relativeCreated": "131410.12501716614", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer tasked with evaluating research papers. Assess originality, methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\n\n# Learning through experience:Episodic memory representation for cognitive agents\n\n# Abstract\nAs the demand for intelligent robots and cognitive agents rises, the ability to retain and utilize past experiences through episodic memory has become crucial, especially for social companion robots that rely on previous interactions for task execution. To address this, we introduce Episodic Memory for Cognitive Agents (EMCA), a novel framework that advances knowledge representation by integrating real-world interactions. EMCA enables agents to adapt to complex environments by learning from tasks, interacting with humans, and processing multimodal data\u2014such as speech, vision, and non-verbal cues\u2014without pre-training on specific scenarios. EMCA models episodic memory through a graph-based structure , allowing for incremental storage and retrieval of experiences. Each interaction or event enriches the memory graph, supporting continuous learning and adaptation without extensive retraining. This human-like memory formation optimizes the agent\u2019s ability to retrieve relevant information for tasks like localization, planning, and reasoning based on prior experiences.Unlike conventional models relying on temporal markers or recurrent patterns, EMCA encodes data like human memory, allowing reasoning across diverse scenarios regardless of temporal patterns. The framework dynamically builds a memory graph with semantic and temporal connections based on the agent\u2019s experiences, promoting flexible temporal reasoning. It also introduces mechanisms for clustering new memories and a dynamic retrieval policy that adjusts based on context or query type, ensuring robustness even in unpredictable scenarios. Empirical tests show EMCA adapts effectively to real-world data, offering reliability and flexibility in dynamic environments.\n\n\n# 1 INTRODUCTION\n\n\nEpisodic memory, introduced by Tulving (1972) Tulving (2002), refers to the recollection of personal experiences anchored to specific times and locations. Unlike semantic memory, which contains general knowledge, episodic memory retains detailed information about events, including temporal, spatial, emotional, and contextual aspects. Tulving's framework organizes episodic memory into components: time, place, characters, and events. Each episode, as shown in Figure 1, is a synthesis of these components tied to a specific time.\n\n\nBuilding on this framework, we propose a model where episodic memories are organized as a graph, with Each episode  $i$  is represented as a node:  $\\mathbf{Episode}_i = \\{\\mathbf{C}_i,\\mathbf{T}_i,\\mathbf{L}_i,\\mathbf{e}_i\\}$\n\n\nHere,  $\\mathbf{C}_i$  denotes the characters,  $\\mathbf{T}_i$  the temporal aspects,  $\\mathbf{L}_i$  the spatial location, and  $\\mathbf{e}_i$  the events. The graph is connected by two types of edges: semantic edges  $\\mathbf{S}(v_i,v_j)$ , linking nodes with shared components, and temporal edges\n\n\n$\\mathbf{T}(v_i, v_j)$ , establishing the sequence of episodes. A dynamic clustering approach is applied to group similar episodes based on temporal and contextual similarities, optimizing retrieval efficiency.\n\n\nOur retrieval system supports three query types: \"what\" (contextual), \"when\" (temporal), and \"where\" (spatial), as outlined by Stephen et al., and Holland and Smulders (2011), enabling human-\n\n\n![Figure 1: Representation of a single episode (node) in episodic memory, integrating time, character, place, and events.](images/d287868946614c0495412b863a98de1d44a3a73418b2d7cc8adeae626933a1a5.jpg)\n\n\nlike memory recall. This is particularly valuable for applications in social companion robotics, aiding elderly or memory-impaired individuals.\n\n\nFor such a cognitive agent it is important that it possesses the ability to recall episodic memories is essential for human cognition, linking personal experiences to specific temporal and spatial contexts. Existing memory models often struggle with continuous, time-series data, which limits their ability to simulate episodic recall effectively. Many of these systems fail to store dialogues as multimodal data, which prevents them from capturing the rich, context-dependent nature of human memory. Additionally, most existing approaches store a single experience as one isolated episode and lack a mechanism for retrieving information across multiple experiences, hindering their ability to integrate knowledge over time. Furthermore, current episodic memory systems are typically restricted to performing a specific, predefined task, limiting their flexibility and adaptability. In contrast, our system is designed to be more versatile, capable of handling a variety of tasks and dynamically adapting to new scenarios, making it far more suited for real-world applications that require memory integration across different contexts and time periods. By integrating multimodal data and temporal information into the episodic memory framework, our model enables experience localization, recommendation, and episodic question answering. It provides a robust foundation for adaptable, scalable systems capable of operating without frequent retraining, applicable to real-world scenarios such as social companion robots and autonomous task planning systems.\n\n\n# CONTRIBUTIONS:\n\n- 1. Temporal connections are managed without complex pattern learning, enabling adaptive reasoning and retrieval of subgraphs from past experiences.\n\n- 2. The system incrementally stores and retrieves episodic memories, dynamically clustering them based on temporal and contextual affinities.\n\n- 3. A multi-edge graph framework optimizes path traversals for dynamic memory retrieval and personalized recommendations across subjective timescales.\n\n- 4. A new dataset is introduced to improve episodic memory question answering, enhancing the agent's ability to respond to queries based on past events.\n\n\nOur model's versatility is demonstrated through comparisons with existing systems that require retraining. It handles various dataset types, including visual, multimodal, and text-based data, and excels in temporal reasoning even without explicit timestamps, addressing complex memory retrieval tasks across diverse applications.\n\n\n# 2 RELATED WORKS\n\n\nEpisodic memory in multimodal systems has seen significant progress. Xiong et al. Xiong et al. (2016) proposed a memory framework centered on question-answer pairs, later enhanced by Han et al. Han et al. (2019) with transformer-based networks and reinforcement learning, yet constrained by predefined queries, limiting real-world use.\n\n\nTemporal Graph Networks (TGNs) by Rossi et al. Rossi et al. (2020) facilitate learning on dynamic graphs, finding applications in recommendations and social media. Associative memory models, such as Hopfield networks Ramsauer et al. (2021), allow content-based retrieval but struggle with irregular data. Sarigun Sarigun (2023) addresses dynamic temporal graphs, while TempoQR Mavromatis et al. (2021) excels in structured datasets with explicit event timing, unlike the contextual inference required by real-world agents.\n\n\nEpisodic memory is also key in temporal localization for language queries, extending beyond VideoQA Xu et al. (2021) to more complex scenarios. Techniques like 2D-TAN Zhang et al. (2020), VSLNet-L Zhang et al. (2020), and RELER Liu et al. (2022) achieve video content localization but operate independently for each video. Our model advances these by integrating audio-visual and contextual inputs for comprehensive, context-aware outputs.\n\n\nEMQA Datta et al. (2022) enriches VideoQA through episodic memory for improved responses. Memory-Augmented Neural Networks (MANNs) Santoro et al. (2016), including STM Le et al. (2020), DNC Xiong et al. (2016), and Rehearsal Memory Zhang et al. (2021), are models that make use of memory system which store memory to answer questions from videos, also B\u00e4rmann et al. develop memory graphs for long-term retentionB\u00e4rmann et al. (2024) in order to verbalize data\n\n\nfor answering questions from prior memory. These models are trained using predefined question-answer pairs and do not store audio or sound data, which makes them less suitable for supporting episodic memory. Anokhin et al. (2024) merges semantic and episodic memory and is similar to human memory but suffers from traversal inefficiencies due to complexity. Edge et al. Edge et al. (2024) enhance retrieval-augmented generation (RAG) with graph indexing in academic domains, and Mavromatis et al. Mavromatis & Karypis (2024) integrate LLMs and GNNs for multi-hop QA, limited by static clusters. Our dynamic multi-edge graph approach adapts to multimodal, time-series data by creating new clusters as the agent gains new experiences, enabling efficient retrieval and enhanced flexibility in unstructured environments.\n\n\n# 3 METHODOLOGY\n\n\nEMCA's methodology for collecting and structuring episodic experiences is inspired by cognitive psychology, specifically the 'what', 'where', and 'when' (WWW) components of episodic memory. This approach highlights the agent's ability to independently capture multimodal data\u2014visual and auditory\u2014to build a comprehensive understanding of its environment. As depicted in Figure 2, the system comprises two primary stages: Experience Memory Collection, where the agent autonomously compiles and stores experiences in a knowledge base without pretraining, and Memory Retrieval, where it recalls relevant experiences for question answering and reasoning. This method enhances adaptability by utilizing prior experiences to inform decision-making. The episodic memory framework is applicable in diverse contexts, such as memory localization and experience-based recommendation. Detailed methodology is given in Appendix B\n\n\n# 3.1 PROCESSING OF AUDIO DATA IN EPISODIC MEMORY\n\n\nAudio data, including dialogues and acoustics, is crucial for constructing episodic memory. Dialogue's provide linguistic and contextual information, while acoustics capture environmental and emotional cues. These elements are integrated as  $A(t) = D(t) + C(t)$ , where  $A(t)$  is the total audio data at time  $t$ , with  $D(t)$  representing the dialog and  $C(t)$  representing acoustics.\n\n\n# 3.1.1 EXTRACTION OF ACOUSTIC DATA USING MEL SPECTROGRAMS\n\n\nAcoustic data is transformed into Mel spectrograms, which emphasize perceptually relevant frequencies. The Mel spectrogram  $M(t,f)$  is computed as  $M(t,f) = \\log \\left(\\sum_{k}|X(t,k)|^{2}\\cdot H(f,k)\\right)$ . where  $X(t,k)$  is the magnitude of the STFT at time  $t$  and frequency  $k$ , and  $H(f,k)$  is the Mel filter bank mapping linear frequencies to the Mel scale.\n\n\n# 3.1.2 EXTRACTION OF VERBAL CUES FROM AUDIO DATA\n\n\nVerbal cues are extracted by applying the Short-Time Fourier Transform (STFT) and converting the spectrum to the Mel scale as  $M(f) = 2595\\log_{10}\\left(1 + \\frac{f}{700}\\right)$ . The Mel spectrogram is then derived as  $\\mathrm{MelSpec}(m,t) = \\log \\left(\\sum_{f_{\\mathrm{low}}}^{f_{\\mathrm{high}}}|S(f,t)|^2 M(f) + \\epsilon\\right)$ , where  $\\epsilon$  is a small constant to prevent issues with the logarithm.\n\n\n# 3.1.3 INTEGRATION OF ACOUSTIC AND VERBAL FEATURES\n\n\nThe final audio representation integrates acoustic features with transcribed dialogue:  $T_{\\mathrm{audio}}(t) = T_{\\mathrm{acoustics}}(t) + T_{\\mathrm{dialogue}}(t)$ . capturing both tonal properties and linguistic meaning. By applying tagging techniques to the processed audio data, we extract and associate place, character, and time information. These features are then saved as an embedding, forming a unified representation for episodic memory.\n\n\n# 3.2 PROCESSING OF VISUAL DATA IN EPISODIC MEMORY\n\n\nVisual data processing starts by transforming each frame  $F_{i}$  into a tensor and extracting global and local features. The scene representation is then obtained as  $V_{\\mathrm{scene}} = \\frac{1}{N}\\sum_{i = 1}^{N}V_{\\mathrm{embed}}(F_i)$ .\n\n\n![Figure 2: Methodology: The agent collects multimodal experiences through vision and audio, which are then synchronized by aligning them within specific time windows. The fused episode is stored in episodic memory which is represented in the form of a graph that, capturing details such as characters, time, place, and events and has both spatial and temporal edges. Upon receiving a query, the model retrieves the relevant episode or set of episodes from memory and generates an appropriate response.](images/62cf05bc6365554bd404ebbcc00f59d22cce8e173b477b7f77f64aa481d111df.jpg)\n\n\nwhere  $N$  represents the number of frames in the scene. To extract  $V_{\\mathrm{time}}$  and  $V_{\\mathrm{place}}$ , a convolution network processes the feature map  $F_{\\mathrm{feature}}(x,y)$ , generating probability maps for the text center lines (TCL) and text regions (TR):  $\\left(\\frac{P_{\\mathrm{TCL}}(x,y)}{P_{\\mathrm{TR}}(x,y)}\\right) = \\sigma \\left(\\left(\\frac{W_{\\mathrm{TCL}}}{W_{\\mathrm{TR}}}\\right) \\cdot F_{\\mathrm{feature}}(x,y)\\right)$ . Thresholding is applied to filter relevant text regions using the condition\n\n\n$$\n\n$$\nP _ {\\text {f i l t e r e d}} = \\left\\{\\left(x, y\\right) \\mid P _ {\\mathrm {T C L}} (x, y) \\geq T _ {\\mathrm {T C L}} \\text {a n d} P _ {\\mathrm {T R}} (x, y) \\geq T _ {\\mathrm {T R}} \\right\\}.\n$$\n\n$$\n\n\nRecognized text is processed with a softmax layer for classification:  $\\hat{y}_t = \\mathrm{Softmax}(W \\cdot h_t + b)$ . from which  $V_{\\mathrm{time}}$  and  $V_{\\mathrm{place}}$  are derived. Character information ( $V_{\\mathrm{character}}$ ) is extracted by associating text and visual features through techniques such as Named Entity Recognition (NER) or vision-language embeddings. Together,  $V_{\\mathrm{scene}}$ ,  $V_{\\mathrm{time}}$ ,  $V_{\\mathrm{place}}$ , and  $V_{\\mathrm{character}}$  form a comprehensive multimodal representation that captures contextual details for reasoning or retrieval tasks.\n\n\n# 3.2.1 MERGING AND SYNCHRONIZING DATA\n\n\nIn the final stage, processed audio and visual data are synchronized to a common timestamp, forming a unified representation:  $\\mathbf{T}_{\\mathbf{M}} = (\\mathbf{T}_{\\mathrm{audio}},\\mathbf{T}_{\\mathrm{visual}})$ . This ensures temporal alignment between the modalities. The audio and visual embeddings are then concatenated into a joint multimodal embedding:  $\\mathbf{E}_{\\mathrm{combined}} = \\mathbf{E}_{\\mathrm{audio}}\\oplus \\mathbf{E}_{\\mathrm{visual}}$ , which is stored in episodic memory, with each node representing key experience aspects such as time, location, characters, and events.\n\n\nThis integrated embedding enhances memory recall and event-based analysis. Following concatenation, the embedding is encoded to capture entities like place, time, and characters for each episode. The methodology for capturing these entities is supported by prior studies, as shown in the appendix.\n\n\n# 3.3 EPISODIC MEMORY REPRESENTATION\n\n\nEach node in the episodic memory represents a day, with subnodes capturing the activities of that day. The main node summarizes the day's events, while subnodes encode specific event details. Joint embeddings, integrating place, character, and event information, are stored at both the event and day levels. This hierarchical structure enables efficient encoding and retrieval of both events and their details. There are two types of edges: contextual edges, which con\n\n\nnect nodes based on similarities in place, character, and events, and temporal edges, which link nodes temporally to reflect the sequence of events. Figure 3 shows an episodic memory graph.\n\n\n# 3.3.1 CLUSTER DEFINITIONS\n\n\nWhen an agent receives an episodic experience, it assigns the experience to multiple clusters based on location, events, and characters: Location Cluster  $\\mathbf{C}_l$ , Character Cluster  $\\mathbf{C}_c$ , and Event Cluster  $\\mathbf{C}_e$ . Each entity within a cluster is uniquely identified, facilitating efficient memory organization and retrieval in alignment with the hierarchical structure of episodic memory, where nodes represent days and subnodes capture activities and events.\n\n\n# 3.3.2 DYNAMIC CLUSTERING\n\n\nWhen an agent gets an experience Text and visual embeddings are utilized to restore feature details and improve decision-making. Spatial and character embeddings are derived from similarities between locations and characters across episodes. For each new episode  $E_{n}$ , these embeddings are evaluated and integrated into existing clusters using an attention mechanism: attention  $X(E_{n},E_{i}) = \\frac{\\mathbf{V}_{X_{n}}\\cdot\\mathbf{V}_{X_{i}}}{\\|\\mathbf{V}_{X_{n}}\\| \\|\\mathbf{V}_{X_{i}}\\|}$ , where  $X \\in \\{\\text{location, char}\\}$ . This dynamic clustering process organizes memory based on spatial, character, and event similarities. New episodes that do not fit into existing clusters create new identifiers, ensuring continuous cluster expansion.\n\n\nEvent clusters are formed by modeling coherence among consecutive utterances within dialogues, with each dialogue  $D = (u_{1}, u_{2}, \\ldots, u_{|D|})$  consisting of  $|D|$  utterances. Instead of relying on expert annotations, we infer event structures by assum-\n\n\ning that utterances within the same event exhibit higher coherence than those spanning multiple events. A contrastive learning objective is used to maximize coherence within event-related snippets while minimizing coherence across different events. The contrastive coherence loss is defined as:  $L_{\\text{contrastive}} = \\sum_{i} [\\log p(\\text{positive}_i) - \\log p(\\text{negative}_i)]$ , where positive and negative examples are selected based on coherence metrics such as ROUGE score, facilitating unsupervised event clustering. This approach allows for the segmentation of dialogues into relevant event clusters, improving the system's ability to retrieve and reason about past experiences, enhancing context-aware decision-making. Once the new episode  $E_n$  is assigned to one or more clusters, the cluster definitions are updated as follows:  $\\mathbf{C}_X^j = \\mathbf{C}_X^j \\cup \\{E_n\\}$  (10), where  $X$  refers to location, character, or event. If  $E_n$  does not fit into any existing cluster, new identifiers are created in the cluster:  $\\mathbf{C}_X^{\\text{new}} = \\{E_n\\}$  (11), where  $X \\in \\{\\text{location}, \\text{char}, \\text{event}\\}$ .\n\n\n# 3.3.3 EDGE CONNECTION\n\n\nContextual edges between episodes are formed by analyzing shared clusters based on common characters, locations, or events. The similarity between two episodes  $E_{1}$  and  $E_{2}$  is determined by the number of overlapping clusters, with an edge created if two or more clusters are shared. The edge weight is computed by combining the location, character, and event similarities:  $\\text{Weight}(E(N_{1}, N_{2})) = \\mathbf{L}(N_{1}, N_{2}) \\| \\mathbf{C}(N_{1}, N_{2}) \\| \\mathbf{E}(N_{1}, N_{2})$ , where  $\\mathbf{L}(N_{1}, N_{2})$ ,  $\\mathbf{C}(N_{1}, N_{2})$ , and  $\\mathbf{E}(N_{1}, N_{2})$  represent location, character, and event similarities, respectively. The primary connection between episodes is established by the maximum similarity across these features:  $\\text{Link}(E(N_{1}, N_{2})) = \\arg \\max \\left( \\{\\mathbf{L}(N_{1}, N_{2}), \\mathbf{C}(N_{1}, N_{2}), \\mathbf{E}(N_{1}, N_{2})\\} \\right)$ .\n\n\nTemporal Edges are defined by the time relationship between consecutive episodes. The temporal connection between episodes  $E_{t-1}$  and  $E_t$  is represented as  $\\mathbf{T}_{\\mathrm{edge}}(E_{t-1}, E_t) = 1$ . Temporal edges help handle missing timestamps, as no traditional statistical methods are used to estimate them. Instead, each episode is treated as representing a distinct day, and the temporal indexer is updated\n\n\n![Figure 3: The diagram illustrates an episodic memory graph that organizes actions and key events hierarchically. Nodes represent memories enriched with textual, visual, and acoustic data, including Mel spectrograms. Connections reflect shared locations, characters, and events, aiding efficient recall and analysis. Clusters are formed using attention mechanisms on embeddings related to shared characters, places, and events, enabling effective grouping of related episodes.](images/a3b0f2b9751d63286705996d54dac048077355f5f46c5b34dada076579a736fa.jpg)\n\n\nbased on visual or dialogue-based date capture. This ensures that the timestamp of any episode is adjusted accordingly, with subsequent episodes indexed relative to this structure. For instance, the \"before\" node is one day prior, and the \"after\" node is one day later. Even without external timestamps, the model understands the temporal order of episodes via temporal edges, maintaining consistency in the passage of time as the agent processes the information.\n\n\n# 3.3.4 DYNAMIC EPISODE RETRIEVAL\n\n\nFigure 4 illustrates dynamic edge traversal for retrieving relevant memories using character, location, event, and temporal weights. The agent classifies the query  $q$  using language models to determine whether it is a \"what\", \"when\", or \"where\" query. \"What\" queries focus on events, \"when\" queries on temporal details, and \"where\" queries on locations. This classification allows the agent to assign the appropriate context and efficiently retrieve relevant memories. Temporal entities (e.g., weeks, months, years) are processed by subtracting fixed intervals from the current date:  $7n$  days for weeks,  $30m$  days for months, and  $365y$  days for years, where  $n$ ,  $m$ , and  $y$  are positive integers.\n\n\nThe similarity score between the query  $q$  and a set  $D_u$  of memory entries is given by  $S_s = \\sum_{e \\in D_u} \\frac{q \\cdot e}{\\|q\\| \\|e\\|}$ . For each neighbor  $v$  of node  $u$ , the weight  $W_{uv}$  is computed as  $W_{uv} = \\sum w(u, v)$ . If  $W_{uv} > \\theta$ , the query set is updated as  $Q \\gets Q \\cup (v, W_{uv})$ .\n\n\nTemporal edges  $T_{\\mathrm{edge}}(E_{t-1}, E_t)$  maintain the sequence of events without re-evaluating the entire graph, with date and time represented as separate nodes for indexing. Explicit timestamps in queries map directly to temporal nodes, while contextual queries traverse the graph based on query type: event weights for \"what,\" temporal weights for \"when,\" and location weights for \"where\" queries. This framework (Figure 4) efficiently retrieves memory clusters, providing task-specific outputs: free-form text for episodic QA and recommendations, memory nodes for experience localization, and goal directives for RL agents (AppendixE.4). Additional details of dynamic node retrieval are given in Appendix B.5\n\n\n![Figure 4: Dynamic edge traversal. Episodic graph representation of event relationships with different edge weights. The start node (E1) is highlighted in cyan, and the node with the highest similarity (E5) is in yellow. Edge colors and styles denote distinct relationship types: temporal edges (solid black), location weights (dashed blue), character weights (dotted red), and event weights (solid green). The legend clarifies the significance of the nodes and edges in the graph.](images/74618abefdcab9199f37dc03c74ba7242f87469f80e2152865a1585d8119a1f1.jpg)\n\n\n# 3.4 IMPACT OF NODE REMOVAL ON CONNECTIONS\n\n\nWhen an episode, such as  $E_{2}$ , is removed from the dynamic graph as part of the forgetting mechanism, its associated semantic links are eliminated, disrupting connections. However, the temporal edges are readjusted, allowing  $E_{3}$  to connect to the next relevant node, preserving the chronological structure. Consequently,  $E_{3}$  updates its temporal link to  $E_{1}$ , maintaining event sequence continuity. This selective removal of semantic associations, while preserving temporal coherence, eliminates the need for a full graph reevaluation. The remaining nodes retain contextual relevance, enabling the agent to reason based on prior experiences. Thus, implementing a forgetting mechanism does not compromise the integrity of the episodic memory graph. We have tested node removal mechanism by using frequency based weight decay to prune nodes as described in Appendix C. Results after node pruning are given in E.2.2\n\n\n# 4 RECOMMENDATION USING EMCA\n\n\nWhen assisting individuals with memory impairments, a cognitive agent utilizes past interactions to provide personalized support by extracting a personalized cluster  $C_p$ , representing the relevant memory subgraph for the individual. This subgraph consists of episodes, actions, and events related to the person:\n\n\n$$\n\n$$\nC _ {p} = \\left\\{G _ {p} \\mid \\text {e p i s o d e s a s s o c i a t e d w i t h p e r s o n} p \\right\\}\n$$\n\n$$\n\n\nwhere  $G_{p} = (V_{p},E_{p})$  includes episodes  $V_{p}$  and edges  $E_{p}$ , preserving temporal and contextual information.\n\n\nThe agent then identifies event clusters  $C_e$ , representing actions performed by the individual over time:\n\n\n$$\n\n$$\nC _ {e} = \\left\\{E _ {e} \\mid \\text {e v e n t s a s s o c i a t e d w i t h e p e s i o d e s i n} G _ {p} \\right\\}\n$$\n\n$$\n\n\nTo provide recommendations, the agent identifies the most recent event sequence  $S_{t} = (S_{1}, S_{2}, \\ldots, S_{t})$  and searches the memory subgraph for a matching sequence, determining the next action  $S_{t+1} = \\operatorname{argmax}_{S \\in G_{p}} \\mathbb{I}(S_{t} \\subset S)$ .\n\n\nIf multiple matches occur, the most frequent next event is selected:  $S_{t + 1} = \\mathrm{mode}\\big(\\{S_{t + 1}\\in \\mathcal{S}\\mid \\mathcal{S}_t\\subset \\mathcal{S},\\mathcal{S}\\in G_p\\} \\big)$ . This method allows the agent to generate recommendations even without recurring patterns, leveraging past episodic data to support individuals with memory impairments. Temporal graph networks and Hopfield memory networks rely on pattern recognition to predict future actions. In contrast, our approach allows the agent to make predictions and provide recommendations without depending on explicit pattern-based mechanisms, offering more flexible and adaptive support for individuals with memory impairments.\n\n\n# 5 DATASET\n\n\nWe propose a comprehensive dataset framework designed to evaluate and enhance episodic memory systems in artificial agents. This framework integrates multiple datasets, including a custom set of episodic questions based on the TV series The Big Bang Theory, spanning all nine seasons (181 episodes). The aim is to assess memory recall and narrative understanding in complex scenarios.\n\n\nWe introduce the Agent Dataset, a 10-episode time-series dataset created in Unity3D, where a virtual agent performs tasks and interacts with characters in realistic environments, simulating the role of companion robots. This dataset emphasizes the importance of multi-sensory inputs and task execution, challenging the agent to process and integrate information from dialogues and visual cues to maintain task order and achieve context-driven objectives.\n\n\nAdditionally, we adapted the Ego4D dataset, restructuring its activity sequences into simulated chronological episodes to address the original absence of time-series data\u2014portraying an agent performing a series of activities over 30 days. We also combined group activity videos designed for active speaker recognition. This transformation enables episodic queries such as \"Where did I place the agricultural tool on the last day of farming?\", enhancing the ability to localize and retrieve temporal experiences effectively.\n\n\nTogether with the PerLTQA Du et al. (2024) and LLQA Dolan & Brockett (2005) datasets, which test essential episodic memory dimensions\u2014\"what\" (context), \"when\" (time), and \"where\" (place)\u2014this framework forms a robust benchmark for evaluating advanced episodic memory capabilities in AI systems.\n\n\nData Annotation: The data was carefully annotated to tag scene information and identify characters in dialogues, ensuring that the model could recognize character presence and understand related events. This included explicitly tagging scene details for location identification and differentiating characters present in the scene versus those mentioned. Events within dialogues were also meticulously annotated to capture key details, facilitating effective memory representation beyond simple summaries. Capturing these essential details is crucial for episodic memory tasks, as it allows the agent to recall past experiences accurately. Each episode was annotated with 10 what, when, and where questions.\n\n\nData Statistics: The dataset includes a distribution of question types: temporal questions make up  $24\\%$ , spatial questions  $38\\%$ , contextual questions  $18\\%$ , multimodal questions (integrating visual and auditory information)  $10\\%$ , and dialogue-based questions  $10\\%$ . These detailed annotations enable the model to handle temporal, spatial, and contextual elements, as well as multimodal inputs, ensuring comprehensive event recognition and effective interaction.\n\n\n# 6 EXPERIMENTS AND EVALUATION\n\n\nWe evaluate our episodic memory cognitive agent (EMCA) on downstream tasks such as episodic memory question answering, benchmarking it against state-of-the-art graph-based and memory models. The agent's performance is tested on memory localization and multimodal memory-based\n\n\nvisual QA. An ablation study compares clustered and non-clustered approaches while assessing the impact of modality removal on episodic memory. These experiments leverage the episodic memory dataset, with EMCA implemented using Whisper, CLIP, and BERT backbones. Additional details are provided in the AppendixD.\n\n\n# 6.1 EVALUATION METRICS\n\n\nThe performance was assessed using recall accuracy for episodic memory question answering (QA), defined as Episodic Recall  $= \\frac{\\text{Number of Correctly Answered Questions}}{\\text{Total Questions}}$ . Additionally, the mean Intersection over Union (mIOU) score was employed to evaluate episodic memory localization.\n\n\n# 6.2 COMPARISON WITH SOTA GRAPH MODELS AND EPISODIC MEMORY QA MODELS\n\n\nWe conducted experiments comparing our approach with state-of-the-art models, including EMR, GraphRag, GNN Rag, TempoQA, and Arigraph, for retrieving relevant information from datasets based on episodic questions. The table below presents a comparative analysis across various datasets, assessing contextual, temporal, spatial, and overall performance metrics.\n\n\nEMR and Dynamic MemQA are pioneering approaches for episodic memory in multimodal QA, while Arigraph integrates semantic and episodic memory for human-like recall, and TempoQA manages temporal data for time-sensitive event comprehension. GraphRag and GNN Rag utilize graph-based memory and retrieval-augmented generation to handle complex data structures. These models were selected to benchmark EMCA against diverse memory models, graph-based RAG methods, and graph structures, including knowledge graphs and temporal graphs. Figure presents the time taken (in seconds) by different methods for various datasets, including Big Bang Theory, PerLTQA, Agent, and LLQA. Our\n\n\nmodel demonstrates significantly faster retrieval times compared to other approaches, particularly when contrasted with methods like Arigraph and TempoQA, which are slower due to their reliance on more computationally intensive processes.\n\n\n![Figure 5: Comparison of Retrieval Times for Various Methods: The chart displays the time taken (in seconds) to retrieve correct memories for four datasets: blue for Big Bang Theory, green for PerLTQA, red for Agent, and cyan for LLQA. The stacked bars represent the cumulative retrieval time for each method](images/acdfdb0cdf25e2a77b5675637b35a8fc2a0c10632ba8e4e4ac7ed33526cc6ef5.jpg)\n\n\nTable 1 demonstrates EMCA's superior performance across four datasets, highlighting its advanced multimodal capabilities and efficient storage and retrieval mechanisms. By integrating textual, visual, and acoustic data, EMCA handles diverse queries effectively, achieving high recall accuracy in contextual, temporal, and spatial questions.\n\n\nEMCA's hierarchical clustering organizes episodic memory by characters, locations, and events, enabling precise retrieval and reducing noise. This dynamic approach outperforms static graph models like GraphRag and GNN Rag, which lack adaptability. EMCA's ability to integrate multimodal data and retrieve targeted memories makes it uniquely suited for episodic memory-based question answering. Additional results with memory models are shown in Appendix E.1.\n\n\n# 6.3 EPISODIC MEMORY LOCALIZATION\n\n\nWe evaluated our model's performance in episodic memory localization using time-series-based questions. Table 2 compares our results with state-of-the-art (SOTA) models recognized for their effectiveness in multimodal data handling within the Ego4D dataset. Metrics include IOU@0.3 with Recall@1 (R@1), IOU@0.5 with Recall@5 (R@5), and mean IOU (mIOU).\n\n\nWhile SOTA models excel in multimodal data tasks, they struggle with video localization using time-series data, which is critical for maintaining accurate, context-aware recall in episodic memory systems. By saving data as a time series, our approach ensures precise temporal alignment,\n\n\n<table><tr><td>Dataset</td><td>Method</td><td>Contextual</td><td>Temporal</td><td>Spatial</td><td>Total</td></tr><tr><td rowspan=\"7\">Big Bang Theory</td><td>Ours</td><td>75</td><td>80</td><td>76</td><td>78</td></tr><tr><td>Dynamic Memory QA</td><td>5</td><td>8</td><td>7.5</td><td>10</td></tr><tr><td>EMR</td><td>14</td><td>10</td><td>13</td><td>15</td></tr><tr><td>GraphRag</td><td>32</td><td>30</td><td>30</td><td>30</td></tr><tr><td>GNN Rag</td><td>25</td><td>24</td><td>23</td><td>27</td></tr><tr><td>TempoQA</td><td>21</td><td>20</td><td>19</td><td>25</td></tr><tr><td>Arigraph</td><td>25</td><td>24</td><td>25</td><td>26</td></tr><tr><td rowspan=\"7\">Perltqa</td><td>Ours</td><td>90</td><td>95</td><td>89</td><td>90</td></tr><tr><td>Dynamic Memory QA</td><td>30</td><td>15</td><td>21</td><td>40</td></tr><tr><td>EMR</td><td>31</td><td>20</td><td>23</td><td>45</td></tr><tr><td>GraphRag</td><td>35</td><td>40</td><td>42</td><td>38</td></tr><tr><td>GNN Rag</td><td>60</td><td>51</td><td>57</td><td>55</td></tr><tr><td>TempoQA</td><td>45</td><td>50</td><td>51</td><td>52</td></tr><tr><td>Arigraph</td><td>55</td><td>75</td><td>71</td><td>79</td></tr><tr><td rowspan=\"7\">Agent Dataset</td><td>Ours</td><td>77</td><td>90</td><td>90</td><td>86</td></tr><tr><td>Dynamic Memory QA</td><td>33</td><td>30</td><td>31</td><td>37</td></tr><tr><td>EMR</td><td>36</td><td>32</td><td>33</td><td>40</td></tr><tr><td>GraphRag</td><td>31</td><td>20</td><td>29</td><td>27</td></tr><tr><td>GNN Rag</td><td>50</td><td>51</td><td>55</td><td>53</td></tr><tr><td>TempoQA</td><td>40</td><td>45</td><td>31</td><td>37</td></tr><tr><td>Arigraph</td><td>52</td><td>53</td><td>53</td><td>51</td></tr><tr><td rowspan=\"7\">LLQA</td><td>Ours</td><td>86</td><td>85</td><td>86</td><td>86</td></tr><tr><td>Dynamic Memory QA</td><td>10</td><td>20</td><td>15</td><td>20</td></tr><tr><td>EMR</td><td>15</td><td>23</td><td>20</td><td>23</td></tr><tr><td>GraphRag</td><td>24</td><td>23</td><td>22</td><td>21</td></tr><tr><td>GNN Rag</td><td>49</td><td>45</td><td>40</td><td>46</td></tr><tr><td>TempoQA</td><td>42</td><td>41</td><td>45</td><td>45</td></tr><tr><td>Arigraph</td><td>50</td><td>51</td><td>42</td><td>52</td></tr></table>\n\n\nTable 1: Comparison of Recall Accuracy for Different Question Types Across Datasets\n\n\n<table><tr><td>Method</td><td>IOU = 0.3 R@1</td><td>IOU = 0.5 R@5</td><td>mIOU</td></tr><tr><td>2D-TAN</td><td>4.32</td><td>2.60</td><td>5.62</td></tr><tr><td>VSLNet</td><td>8.09</td><td>7.03</td><td>7.65</td></tr><tr><td>CONE</td><td>10.55</td><td>7.54</td><td>9.04</td></tr><tr><td>RELER</td><td>12.89</td><td>8.14</td><td>10.51</td></tr><tr><td>SPOTEM</td><td>18.13</td><td>13.43</td><td>15.78</td></tr><tr><td>Ours</td><td>26.46</td><td>25.5</td><td>25.98</td></tr></table>\n\n\nminimizes outdated recall, and enhances context comprehension. Furthermore, integrating visual and dialogue modalities provides a comprehensive understanding of interactions and events, significantly improving episodic memory localization. Visualization of Episodic memory localization is as given in Appendix E.3\n\n\n# 6.4 ABLATION STUDIES\n\n\nWe validated our EMCA approach by systematically removing different modalities from the architecture: first the visual module, then the speech module, and finally the music module. Performance was evaluated on videos from The Big Bang Theory and the Agent Dataset by posing episodic questions. The episodic recall capacity, as detailed in 6.2, was assessed based on these crafted episodic questions. Additionally, we measured retrieval time as the number of episodes increased.\n\n\nTable 2: Performance comparison on episodic memory localization.\n\n\n<table><tr><td>Modalities</td><td>Big Bang Theory</td><td>Agent Dataset</td></tr><tr><td>Full</td><td>78</td><td>86</td></tr><tr><td>No Vision</td><td>16</td><td>15</td></tr><tr><td>No Acoustics</td><td>65</td><td>40</td></tr><tr><td>No Dialogues</td><td>36</td><td>20</td></tr></table>\n\n\nTable 3: Comparison of Modalities.\n\n\n<table><tr><td>Method</td><td>Retrieval Time (Average)</td></tr><tr><td>Dynamic Traversal</td><td>5.6</td></tr><tr><td>BFS</td><td>8.9</td></tr><tr><td>DFS</td><td>11</td></tr><tr><td>DFS + BFS</td><td>7.75</td></tr></table>\n\n\nTable 4: Graph Traversal Retrieval Times.\n\n\n<table><tr><td>Number of Episodes</td><td>Retrieval Time (ms)</td></tr><tr><td>10</td><td>9.11</td></tr><tr><td>50</td><td>11.0</td></tr><tr><td>100</td><td>11.5</td></tr><tr><td>181</td><td>12.0</td></tr></table>\n\n\nTable 5: Retrieval Times vs. Episodes.\n\n\nAs shown in Table 3, retaining all modalities is essential for agents operating in multimodal environments to make informed decisions and reason effectively based on past events.\n\n\n![Figure 6: Comparison of the Number of Edges Traversed and Retrieval Time (ms) by Query Type for Clustered and Non-Clustered Approaches](images/1ee63848f84b0867c2497852a8c6e49089838e7ffdf57a86e097ae022130ab4d.jpg)\n\n\n# ASSESSMENT OF RETRIEVAL TIME ACROSS TRAVERSAL METHODS\n\n\nWe evaluated the time complexity of our dynamic graph traversal method against traditional techniques, summarized in Table 4. The dynamic traversal method demonstrated the lowest average retrieval time, outperforming BFS, DFS, and their combined approach. When explicit timestamps are provided, queries are treated as temporal, leading to faster retrieval compared to event-based queries.\n\n\nComparing clustered and non-clustered graphs in terms of number of path traversals required and time taken to retrieval query As shown in Figure 6, the dynamic clustered graph significantly reduces query retrieval time and the number of edges traversed compared to the non-clustered graph, demonstrating the efficiency of clustering in optimizing graph traversal. Dynamic edges and clusters enhance adaptability, improving performance across various query types. Table 5 shows a modest increase in retrieval time as the number of episodes grows, highlighting the clustering mechanism's role in keeping retrieval times low. These results emphasize the scalability and efficiency of our method. Replacing an episodic memory graph with a knowledge graph introduces significant complexity, especially when incorporating temporal features. For instance, an episodic graph represents three days of experiences with just three nodes, one per day, while a temporal knowledge graph may require 15-20 nodes and edges, drastically increasing structural complexity. This added intricacy hampers memory retrieval and action prediction, making them less efficient than the streamlined architecture of episodic graphs. In visual domains, where patterns and regularities are prevalent, knowledge graphs excel by leveraging these consistencies. However, in dynamic scenarios like conversations, interactions are often unique, necessitating frequent creation of new relations. This dynamic evolution further complicates the knowledge graph and reduces practical efficiency. Additional results, including ablation studies and node pruning effects, are detailed in Appendices E.2.1 and E.2..\n\n\n# 7 CONCLUSION, LIMITATIONS, AND SOCIAL IMPACT\n\n\nThe EMCA system is designed to understand and process temporal timescales in a manner similar to human cognition. It dynamically organizes events along subjective timescales, allowing it to track and retrieve memories based on the relative importance of events rather than fixed timestamps. This enables the system to adapt to varying time-frames, understanding how past experiences may influence present contexts. By mimicking human-like temporal reasoning, EMCA can handle queries about \"when\" events occurred in both short-term and long-term memory, adjusting its responses based on the perceived significance of past events, much like human memory recalls important moments more vividly than routine occurrences.\n\n\nSocial Impact: The Episodic Memory Cognitive Agent has the potential to serve as a valuable social agent, particularly for individuals with memory-related disorders. However, without robust data protection mechanisms, there is a risk of data breaches, which could have serious privacy implications.\n\n\nLimitations: Currently, EMCA lacks a forgetting mechanism and the ability to identify key events, as humans do, based on factors such as surprise, novelty, or emotional significance. These aspects will be addressed in future work.\n\n\n# REFERENCES\n\n- Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, and Evgeny Burnaev. Arigraph: Learning knowledge graph world models with episodic memory for llm agents, 2024. URL https://arxiv.org/abs/2407.04363.\n\n- Leonard B\u00e4rmann, Chad DeChant, Joana Plewnia, Fabian Peller-Konrad, Daniel Bauer, Tamim As-four, and Alex Waibel. Episodic memory verbalization using hierarchical representations of life-long robot experience, 2024. URL https://arxiv.org/abs/2409.17702.\n\n- Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context, 2019. URL https://arxiv.org/abs/1901.02860.\n\n- Samyak Datta, Sameer Dharur, Vincent Cartillier, Ruta Desai, Mukul Khanna, Dhruv Batra, and Devi Parikh. Episodic memory question answering, 2022. URL https://arxiv.org/abs/2205.01652.\n\n- William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/I05-5002.\n\n- Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, and Kam-Fai Wong. Perl'tqa: A personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering, 2024. URL https://arxiv.org/abs/2402.16288.\n\n- Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization, 2024. URL https://arxiv.org/abs/2404.16130.\n\n- Moonsu Han, Minki Kang, Hyunwoo Jung, and Sung Ju Hwang. Episodic memory reader: Learning what to remember for question answering from streaming data, 2019. URL https://arxiv.org/abs/1903.06164.\n\n- Hung Le, Truyen Tran, and Svetha Venkatesh. Self-attentive associative memory, 2020. URL https://arxiv.org/abs/2002.03519.\n\n- Naiyuan Liu, Xiaohan Wang, Xiaobo Li, Yi Yang, and Yueting Zhuang. Refer@zju-alibaba submission to the ego4d natural language queries challenge 2022, 2022. URL https://arxiv.org/abs/2207.00383.\n\n- Costas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model reasoning, 2024. URL https://arxiv.org/abs/2405.20139.\n\n- Costas Mavromatis, Prasanna Lakkur Subramanyam, Vassilis N. Ioannidis, Soji Adeshina, Phillip R. Howard, Tetiana Grinberg, Nagib Hakim, and George Karypis. Tempoqr: Temporal question reasoning over knowledge graphs, 2021. URL https://arxiv.org/abs/2112.05785.\n\n- Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learning, 2019. URL https://arxiv.org/abs/1910.06764.\n\n- Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. Hopfield networks is all you need, 2021. URL https://arxiv.org/abs/2008.02217.\n\n- Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. Temporal graph networks for deep learning on dynamic graphs, 2020. URL https://arxiv.org/abs/2006.10637.\n\n- Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. One-shot learning with memory-augmented neural networks, 2016. URL https://arxiv.org/abs/1605.06065.\n\n- Ahmet Sarg\u00fcn. Graph mixer networks, 2023. URL https://arxiv.org/abs/2301.12493.\n\n- Endel Tulving. Episodic memory: From mind to brain. Annual review of psychology, 53:1-25, 02 2002. doi: 10.1146/annurev.psych.53.100901.135114.\n\n- Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual question answering, 2016. URL https://arxiv.org/abs/1603.01417.\n\n- Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, and Luke Zettlemoyer. VLM: Task-agnostic video-language model pretraining for video understanding. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 4227-4239, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.370. URL https://aclanthology.org/2021.findings-acl.370.\n\n- Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. Learning 2d temporal adjacent networks for moment localization with natural language, 2020. URL https://arxiv.org/abs/1912.03590.\n\n- Zhu Zhang, Chang Zhou, Jianxin Ma, Zhijie Lin, Jingren Zhou, Hongxia Yang, and Zhou Zhao. Learning to rehearse in long sequence memorization, 2021. URL https://arxiv.org/abs/2106.01096.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "TextualGradientDescent prompt for update", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "165", "funcName": "_update_prompt", "created": "1768638534.5870638", "msecs": "587.0", "relativeCreated": "131417.56129264832", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "prompt": "Here is the role of the variable you will improve: <ROLE>System prompt that sets the AI reviewer's role and approach for paper evaluation</ROLE>.\n\nThe variable is the text within the following span: <VARIABLE> You are an expert academic reviewer tasked with evaluating research (...) methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}. </VARIABLE>\n\nHere is the context and feedback we got for the variable:\n\n<CONTEXT></CONTEXT>\n\nImprove the variable (System prompt that sets the AI reviewer's role and approach for paper evaluation) using the feedback provided in <FEEDBACK> tags.\nYou must follow the following constraints:\n\n<CONSTRAINTS>Constraint 1: Keep the system prompt under 500 characters\nConstraint 2: Keep the human prefix under 800 characters\nConstraint 3: The human prefix MUST retain the output format instruction: \\boxed{Accept} or \\boxed{Reject}\nConstraint 4: The human prefix should clearly state the task is predicting paper acceptance</CONSTRAINTS>\n\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.", "message": "TextualGradientDescent prompt for update"}
{"name": "textgrad", "msg": "TextualGradientDescent optimizer response", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "179", "funcName": "step", "created": "1768638548.6798997", "msecs": "679.0", "relativeCreated": "145510.39719581604", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "optimizer.response": "<think>\nOkay, let's tackle this. The user wants to improve the system prompt for an AI reviewer that evaluates research papers. The original prompt is a bit generic. Let me check the constraints again.\n\nFirst, the system prompt needs to be under 500 characters. Let me count the original: \"You are an expert academic reviewer tasked with evaluating research (...) methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\" Hmm, the original might be over. Also, the feedback mentions the human prefix must be under 800 characters and retain the output format. The task is to predict paper acceptance, so the prompt should clearly state that.\n\nWait, the original variable is the system prompt. The user provided the role as a system prompt. The constraints mention keeping the system prompt under 500 characters. Let me check the original length. Let's see: \"You are an expert academic reviewer tasked with evaluating research methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\" That's probably around 130 characters. Wait, maybe the original was longer. Wait, the user's variable is within <VARIABLE> tags. Let me check again. The original variable is: \"You are an expert academic reviewer tasked with evaluating research (...) methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\" The ellipsis might be a typo. Let me count the exact characters. Let's assume the original is under 500, but maybe the user wants it more concise.\n\nThe feedback says to improve the variable using the given constraints. The main points are: keep the system prompt under 500, human prefix under 800, retain the output format, and clearly state the task is predicting acceptance.\n\nSo, the original system prompt is the variable. Let me rephrase it to be more concise. Maybe start with \"You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\" That's shorter. Let me check the character count. \"You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\" That's 142 characters. Well under 500. Also, the output format is retained. The task is clearly stated as predicting acceptance. The human prefix is the system prompt, so it's under 800. I think this meets all constraints. Let me make sure there's no mention of the ellipsis. The original had an ellipsis, which might be a typo. Removing that makes it clearer. So the improved variable should be this version.\n</think>\n\n<IMPROVED_VARIABLE>You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.</IMPROVED_VARIABLE>", "message": "TextualGradientDescent optimizer response"}
{"name": "textgrad", "msg": "TextualGradientDescent updated text", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "187", "funcName": "step", "created": "1768638548.6800158", "msecs": "680.0", "relativeCreated": "145510.51330566406", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "parameter.value": "You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.", "message": "TextualGradientDescent updated text"}
{"name": "textgrad", "msg": "TextualGradientDescent prompt for update", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "165", "funcName": "_update_prompt", "created": "1768638548.6801002", "msecs": "680.0", "relativeCreated": "145510.59770584106", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "prompt": "Here is the role of the variable you will improve: <ROLE>Task instruction prefix that appears before each paper, specifying the task (accept/reject prediction) and output format (\\boxed{Accept} or \\boxed{Reject})</ROLE>.\n\nThe variable is the text within the following span: <VARIABLE> Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate. </VARIABLE>\n\nHere is the context and feedback we got for the variable:\n\n<CONTEXT></CONTEXT>\n\nImprove the variable (Task instruction prefix that appears before each paper, specifying the task (accept/reject prediction) and output format (\\boxed{Accept} or \\boxed{Reject})) using the feedback provided in <FEEDBACK> tags.\nYou must follow the following constraints:\n\n<CONSTRAINTS>Constraint 1: Keep the system prompt under 500 characters\nConstraint 2: Keep the human prefix under 800 characters\nConstraint 3: The human prefix MUST retain the output format instruction: \\boxed{Accept} or \\boxed{Reject}\nConstraint 4: The human prefix should clearly state the task is predicting paper acceptance</CONSTRAINTS>\n\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.", "message": "TextualGradientDescent prompt for update"}
{"name": "textgrad", "msg": "TextualGradientDescent optimizer response", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "179", "funcName": "step", "created": "1768638561.7384462", "msecs": "738.0", "relativeCreated": "158568.94373893738", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "optimizer.response": "<think>\nOkay, let's tackle this. The user wants to improve the task instruction prefix for a paper acceptance prediction task. The original variable is: \"Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\" \n\nFirst, I need to check the constraints. The system prompt must be under 500 characters, but since the variable is the human prefix, maybe that's about the input. Wait, the constraints mention keeping the system prompt under 500 and the human prefix under 800. The original variable is the human prefix. Let me count the characters. The original is 103 characters. So that's under 800. But maybe the user wants to optimize it further.\n\nThe main points to include are: task (predict accept/reject), output format (boxed), and maybe the acceptance rate note. However, the feedback might have indicated that the note about ICLR's acceptance rate is not necessary. Wait, the original variable includes that note. But maybe the feedback said to remove it? The user hasn't provided specific feedback, but the context is empty. Wait, the user says \"using the feedback provided in <FEEDBACK> tags,\" but the context is empty. Hmm, maybe there's no feedback given, so I need to rely on the constraints and the role description.\n\nThe role is to specify the task and output format. The original includes the note about the acceptance rate, but perhaps that's not necessary. The constraints say to retain the output format instruction. So maybe the note can be removed to make it more concise. Also, the task should clearly state predicting acceptance. Let me rephrase the original.\n\nOriginal: \"Predict whether the paper should be accepted or rejected. Output: \\boxed{Accept} or \\boxed{Reject}. Note: ICLR has a ~30% acceptance rate.\"\n\nImprovements: Make it shorter, remove the note about acceptance rate, and ensure clarity. Maybe \"Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\" That's shorter and meets the constraints. Let me check the character count: \"Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\" That's 72 characters. Well under 800. Also, it clearly states the task and output format. The note about the acceptance rate might be distracting or not necessary for the task instruction. So removing it would make the instruction cleaner. The user's constraints require retaining the output format, which is done here. So the improved variable would be that.\n</think>\n\n<IMPROVED_VARIABLE>Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.</IMPROVED_VARIABLE>", "message": "TextualGradientDescent optimizer response"}
{"name": "textgrad", "msg": "TextualGradientDescent updated text", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "187", "funcName": "step", "created": "1768638561.7385528", "msecs": "738.0", "relativeCreated": "158569.05031204224", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "parameter.value": "Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.", "message": "TextualGradientDescent updated text"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638563.0466013", "msecs": "46.0", "relativeCreated": "159877.09879875183", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Debiased Graph Neural Networks with Agnostic Label Selection Bias\n\n# Abstract\nMost existing Graph Neural Networks (GNNs) are proposed without considering the selection bias in data, i.e., the inconsistent distribution between the training set with test set. In reality, the test data is not even available during the training process, making selection bias agnostic. Training GNNs with biased selected nodes leads to significant parameter estimation bias and greatly impacts the generalization ability on test nodes. In this paper, we first present an experimental investigation, which clearly shows that the selection bias drastically hinders the generalization ability of GNNs, and theoretically prove that the selection bias will cause the biased estimation on GNN parameters. Then to remove the bias in GNN estimation, we propose a novel Debiased Graph Neural Networks (DGNN) with a differentiated decorrelation regularizer. The differentiated decorrelation regularizer estimates a sample weight for each labeled node such that the spurious correlation of learned embeddings could be eliminated. We analyze the regularizer in causal view and it motivates us to differentiate the weights of the variables based on their contribution on the confounding bias. Then, these sample weights are used for reweighting GNNs to eliminate the estimation bias, thus help to improve the stability of prediction on unknown test nodes. Comprehensive experiments are conducted on several challenging graph datasets with two kinds of label selection bias. The results well verify that our proposed model outperforms the state-of-the-art methods and DGNN is a flexible framework to enhance existing GNNs.\n\n\n# 1 INTRODUCTION\n\n\nGraph Neural Networks (GNNs) are powerful deep learning algorithms on graphs with various applications (Scarselli et al., 2008; Kipf & Welling, 2016; Velickovi\u0107 et al., 2017; Hamilton et al., 2017). Existing GNNs mainly learn a node embedding through aggregating the features from its neighbors, and such message-passing framework is supervised by node label in an end-to-end manner. During this training procedure, GNNs will effectively learn the correlation between the structure pattern and node feature with node label, so that GNNs are capable of learning the embeddings of new nodes and inferring their labels.\n\n\nOne basic requirement of GNNs making precise prediction on unseen test nodes is that the distribution of labeled training and test nodes is same, i.e., the structure and feature of labeled training and test nodes follow the similar pattern, so that the learned correlation between the current graph and label can be well generalized to the new nodes. However, in reality, there are two inevitable issues. (1) Because it is difficult to control the graph collection in an unbiased environment, the relationship between the collected real-world graph and the labeled nodes is inevitably biased. Training on such graph will cause biased correlation with node label. Taking a scientist collaboration network as an example, if most scientists with \"machine learning\" (ML) label collaborate with those with \"computer vision\" (CV) label, existing GNNs may learn spurious correlation, i.e., scientists who cooperate with CV scientist are ML scientists. If a new ML scientist only connects with ML scientists or the scientists in other areas, it will be probably misclassified. (2) The test node in the real scenario is usually not available, implying that the distribution of new nodes is agnostic. Once the distribution is inconsistent with that in the training nodes, the performance of all the current GNNs will be hindered. Even transfer learning is able to solve the distribution shift problem, however, it still needs the prior of test distribution, which actually cannot be obtained beforehand. Therefore, the agnostic label selection bias greatly affects the generalization ability of GNNs on unknown test data.\n\n\n![(a) Cora](images/625d77e111858cdf5aa0539bbf223675f4234e54f25c4fa13686a5a8215d9384.jpg)\n\n\n![(b) Citeseer](images/800655a49fa14916ed955d0d673ddeb095bff8003c8a732b0fec7c1c4dd8c0eb.jpg)\n\n\n![(c) Pubmed Figure 1: Effect of selection bias on GCN and GAT.](images/e76dbe01f65bb145e6f76b2104eadc0b6e0bcbbe47361337f0f9c1ffae2cea93.jpg)\n\n\nIn order to observe selection bias in real graph data, we conduct an experimental investigation to validate the effect of selection bias on GNNs (details can be seen in Section 2.1). We select training nodes with different biased degrees for each dataset, making the distribution of training nodes and test nodes inconsistent. The results clearly show that selection bias drastically hinders the performance of GNNs on unseen test nodes. Moreover, with heavier bias, the performance drops more. Further, we theoretically analyze how the data selection bias results in the estimation bias in GNN parameters (details can be seen in Section 2.2). Based on the stable learning technique (Kuang et al., 2020), we can assume that the learned embeddings consist of two parts: stable variables and unstable variables. The data selection bias will cause the spurious correlation between these two kinds of variables. Thereby we prove that with the inevitable model misspecification, the spurious correlation will further cause the parameter estimation bias. Once the weakness of the current GNNs with selection bias is identified, one natural question is \"how to remove the estimation bias in GNNs?\"\n\n\nIn this paper, we propose a novel Debiased Graph Neural Network (DGNN) framework for stable graph learning by jointly optimizing a differentiated decorrelation regularizer and a weighted GNN model. Specifically, the differentiated decorrelation regularizer is able to learn a set of sample weights under differentiated variable weights, so that the spurious correlation between stable and unstable variables would be greatly eliminated. Based on the causal view analysis of decorrelation regularizer, we theoretically prove that the weights of variables can be differentiated by the regression weights. Moreover, to better combine the decorrelation regularizer with GNNs, we prove that adding the regularizer to the embedding learned by the second to last layer could be both theoretically sound and flexible. Then the sample weights learned by decorrelation regularizer are used to reweight GNN loss so that the parameter estimation could be unbiased.\n\n\nIn summary, the contributions of this paper are three-fold: i) We investigate a new problem of learning GNNs with agnostic label selection bias. The problem setting is general and practical for real applications. ii) We bring the idea of variable decorrelation into GNNs to relieve bias influence on model learning and propose a general framework DGNN which could be adopted to various GNNs. iii) We conduct the experiments on real-world graph benchmarks with two kinds of agnostic label selection bias, and the experimental results demonstrate the effectiveness and flexibility of our model.\n\n\n# 2 EFFECT OF LABEL SELECTION BIAS ON GNNS\n\n\nIn this section, we first formulate our target problem as follows:\n\n\nProblem 1 (Semi-supervised Learning on Graph with Agnostic Label Selection Bias). Given a training graph  $\\mathcal{G}_{train} = \\{\\mathbf{A}_{train},\\mathbf{X}_{train},\\mathbf{Y}_{train}\\}$ , where  $\\mathbf{A}_{train}\\in \\mathbb{R}^{N\\times N}$  ( $N$  nodes) represents the adjacency matrix,  $\\mathbf{X}_{train}\\in \\mathbb{R}^{N\\times D}$  ( $D$  features) refers to the node features and  $\\mathbf{Y}_{train}\\in \\mathbb{R}^{n\\times C}$  ( $n$  labeled nodes,  $C$  classes) refers to the available labels for training ( $n\\ll N$ ), the task is to learn a GNN  $g_{\\theta}(\\cdot)$  with parameter  $\\theta$  to precisely predict the label of nodes on test graph  $\\mathcal{G}_{test} = \\{\\mathbf{A}_{test},\\mathbf{X}_{test},\\mathbf{Y}_{test}\\}$ , where distribution  $\\Psi (\\mathcal{G}_{train})\\neq \\Psi (\\mathcal{G}_{test})$ .\n\n\n# 2.1 EXPERIMENTAL INVESTIGATION\n\n\nWe conduct an experimental investigation to examine whether the state-of-the-art GNNs are sensitive to the selection bias. The main idea is that we will perform two representative GNNs: GCN (Kipf\n\n\n& Welling, 2016) and GAT (Velickovi\u0107 et al., 2017) on three widely used graph datasets: Cora, Citeseer, Pubmed (Sen et al., 2008) with different degrees of bias. If the performance drops sharply in comparison with the scenarios without selection bias, this will demonstrate that GNNs cannot generalize well in selection bias setting.\n\n\nTo simulate the agnostic selection bias scenario, we first follow the inductive setting in Wu et al. (2019) that masks the validation and test nodes as the training graph  $\\mathcal{G}_{train}$  in the training phase, and then infer the labels of validation and test nodes with whole graph  $\\mathcal{G}_{test}$ . In this way, the distribution of test node can be considered agnostic. Following Zadrozny (2004), we design a biased label selection method on training graph  $\\mathcal{G}_{train}$ . The selection variable  $e$  is introduced to control whether the node will be selected as labeled nodes, where  $e = 1$  means selected and 0 otherwise. For node  $i$ , we compute its neighbor distribution ratio:  $r_i = |\\{j|j \\in \\mathcal{N}_i, y_j \\neq y_i\\}| / |\\mathcal{N}_i|$ , where  $\\mathcal{N}_i$  is neighborhood of node  $i$  in  $\\mathcal{G}_{train}$  and  $y_j \\neq y_i$  means the label of central node  $i$  is not the label of its neighborhood node  $j$ . And  $r_i$  measures the difference between the label of central node  $i$  with the labels of its neighborhood. Then we average all the nodes'  $r$  to get a threshold  $t$ . For each node, the probability to be selected is:  $P(e_i = 1|r_i) = \\left\\{ \\begin{array}{cc} \\epsilon & r_i \\geq t \\\\ 1 - \\epsilon & r_i < t \\end{array} \\right.$ , where  $\\epsilon \\in (0.5, 1)$  is used to control the degree of selection bias and the larger  $\\epsilon$  means heavier bias. We set  $\\epsilon$  as  $\\{0.7, 0.8, 0.9\\}$  to get three bias degrees for each dataset, termed as Light, Medium, Heavy, respectively. We select 20 nodes for each class for training and the validation and test nodes are same as Yang et al. (2016). Furthermore, we take the unbiased datasets as baselines, where the labeled nodes are selected randomly.\n\n\nFigure 1 is the results of GCN and GAT on biased datasets. The dashed lines mean the performances of GCN/GAT on unbiased datasets and the solid lines refer to the results on biased datasets. We can find that: i) The dashed lines are all above the corresponding coloured solid lines, indicating that selection bias greatly affects the GNNs' performance. ii) All solid lines decrease monotonically with the increase of bias degree, demonstrating that heavier bias will cause larger performance decrease.\n\n\n# 2.2 THEORETICAL ANALYSIS\n\n\nThe above experiment empirically verifies the effect of selection bias on GNNs. Here we theoretically analyze the effect of selection bias on estimating the parameters in GNNs. First, because biased labeled nodes have biased neighborhood structure, GNNs will encode this biased information into the node embeddings. Based on stable learning technique (Kuang et al., 2020), we make following assumption:\n\n\nAssumption 1. All the variables of embeddings learned by GNNs for each node can be decomposed as  $\\mathbf{H} = \\{\\mathbf{S},\\mathbf{V}\\}$ , where  $\\mathbf{S}$  represents the stable variables and  $\\mathbf{V}$  represents the unstable variables. Specifically, for both training and test environment,  $\\mathbb{E}(\\mathbf{Y}|\\mathbf{S} = s,\\mathbf{V} = v) = \\mathbb{E}(\\mathbf{Y}|\\mathbf{S} = s)$ .\n\n\nUnder Assumption 1, the distribution shift between training set and test set is mainly induced by the variation in the joint distribution over  $(\\mathbf{S},\\mathbf{V})$ , i.e.,  $\\mathbb{P}(\\mathbf{S}_{train},\\mathbf{V}_{train})\\neq \\mathbb{P}(\\mathbf{S}_{test},\\mathbf{V}_{test})$ . However, there is an invariant relationship between stable variable  $\\mathbf{S}$  and outcome  $\\mathbf{Y}$  in both training and test environments, which can be expressed as  $\\mathbb{P}(\\mathbf{Y}_{train}|\\mathbf{S}_{train}) = \\mathbb{P}(\\mathbf{Y}_{test}|\\mathbf{S}_{test})$ . Assumption 1 can be guaranteed by  $\\mathbf{Y}\\bot \\mathbf{V}|\\mathbf{S}$ . Thus, one can solve the stable prediction problem by developing a function  $f(\\cdot)$  based on  $\\mathbf{S}$ . However, one can hardly identify such variables in GNNs.\n\n\nWithout loss of generality, we take  $\\mathbf{Y}$  as continuous variable for analysis and have the following assumption:\n\n\nAssumption 2. The true generation process of target variable  $\\mathbf{Y}$  contains not only the linear combination of stable variables  $\\mathbf{S}$ , but also the nonlinear transformation of stable variables.\n\n\nBased on the above assumptions, we formalize the label generation process as follows:\n\n\n$$\n\n$$\n\\mathbf {Y} = f (\\mathbf {X}, \\mathbf {A}) + \\varepsilon = \\mathcal {G} (\\mathbf {X}, \\mathbf {A}; \\theta_ {g}) _ {S} \\beta_ {S} + \\mathcal {G} (\\mathbf {X}, \\mathbf {A}; \\theta_ {g}) _ {V} \\beta_ {V} + g (\\mathcal {G} (\\mathbf {X}, \\mathbf {A}; \\theta_ {g}) _ {S}) + \\varepsilon , \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\mathcal{G}(\\mathbf{X},\\mathbf{A};\\theta_g)\\in \\mathbb{R}^{N\\times p}$  denotes an unknown function of  $\\mathbf{X}$  and  $\\mathbf{A}$  that learns node embedding and it can be learned by a GNN, such as GCN and GAT, the output variables of  $\\mathcal{G}(\\mathbf{X},\\mathbf{A};\\theta_g)$  can be decomposed as stable variables  $\\mathcal{G}(\\mathbf{X},\\mathbf{A};\\theta_g)_S\\in \\mathbf{R}^{N\\times m}$  and unstable variables  $\\mathcal{G}(\\mathbf{X},\\mathbf{A};\\theta_g)_V\\in$ $\\mathbf{R}^{N\\times q}$ $(m + q = p)$ ,  $\\beta_{S}\\in \\mathbb{R}^{m\\times 1}$  and  $\\beta_V\\in \\mathbb{R}^{q\\times 1}$  are the linear coefficients can be learned by the last layer of GNNs,  $\\varepsilon$  is the independent random noise, and  $g(\\cdot)$  is the nonlinear transformation function\n\n\nof stable variables. According to Assumption 1, we know that coefficients of unstable variables  $\\mathcal{G}(\\mathbf{X},\\mathbf{A};\\theta_g)_V$  are actually 0 (i.e.,  $\\beta_V = 0$ ).\n\n\nFor a classical GNN model with linear regressor, its prediction function can be formulated as:\n\n\n$$\n\n$$\n\\hat {\\mathbf {Y}} = \\hat {\\mathcal {G}} (\\mathbf {X}, \\mathbf {A}; \\theta_ {g}) _ {S} \\hat {\\beta} _ {S} + \\hat {\\mathcal {G}} (\\mathbf {X}, \\mathbf {A}; \\theta_ {g}) _ {V} \\hat {\\beta} _ {V} + \\varepsilon . \\tag {2}\n$$\n\n$$\n\n\nCompared with Eq. (1), we can find that the parameters of GNN could be unbiasedly estimated if the nonlinear term  $g(\\mathcal{G}(\\mathbf{X}, \\mathbf{A}; \\theta_g)_S) = 0$ , because the GNN model will have the same label generation mechanism as Eq. (1). However, limited by the nonlinear power of GNNs (Xu et al., 2019), it is reasonable to assume that there is a nonlinear term  $g(\\mathcal{G}(\\mathbf{X}, \\mathbf{A}; \\theta_g)_S) \\neq 0$  that cannot be fitted by the GNNs. Under this assumption, next, we taking a vanilla GCN (Kipf & Welling, 2016) as an example to illustrate how the distribution shift will induce parameter estimation bias. A two-layer GCN can be formulated as  $\\hat{\\mathbf{A}}\\sigma (\\hat{\\mathbf{A}}\\mathbf{X}\\mathbf{W}^{(0)})\\mathbf{W}^{(1)}$ , where  $\\hat{\\mathbf{A}}$  is the normalized adjacency matrix,  $\\mathbf{W}$  is the transformation matrix at each layer and  $\\sigma(\\cdot)$  is the Relu activation function. We decompose GCN as two parts: one is embedding learning part  $\\hat{\\mathbf{A}}\\sigma (\\hat{\\mathbf{A}}\\mathbf{X}\\mathbf{W}^{(0)})$ , which can be decomposed as  $[\\mathbf{S}^{\\mathrm{T}}, \\mathbf{V}^{\\mathrm{T}}]$ , corresponding to  $\\hat{\\mathcal{G}}(\\mathbf{X}, \\mathbf{A}; \\theta_g)_S$  and  $\\hat{\\mathcal{G}}(\\mathbf{X}, \\mathbf{A}; \\theta_g)_V$  in Eq. (2), and the other part is  $\\mathbf{W}^{(1)}$ , where the learned parameters can be decomposed as  $[\\tilde{\\beta}_S, \\tilde{\\beta}_V]$ , corresponding to  $[\\hat{\\beta}_S, \\hat{\\beta}_V]$  in Eq. (2). We aim at minimizing the square loss:  $\\mathcal{L}_{GCN} = \\sum_{i=1}^{n} (\\mathbf{S}_i^{\\mathrm{T}}\\tilde{\\beta}_S + \\mathbf{V}_i^{\\mathrm{T}}\\tilde{\\beta}_V - \\mathbf{Y}_i)^2$ .\n\n\nAccording to the derivation rule of partitioned regression model, we have:\n\n\n$$\n\n$$\n\\tilde {\\beta} _ {V} - \\beta_ {V} = (\\frac {1}{n} \\sum_ {i = 1} ^ {n} \\mathbf {V} _ {i} ^ {\\mathrm {T}} \\mathbf {V} _ {i}) ^ {- 1} (\\frac {1}{n} \\sum_ {i = 1} ^ {n} \\mathbf {V} _ {i} ^ {\\mathrm {T}} g (\\mathbf {S} _ {i})) + (\\frac {1}{n} \\sum_ {i = 1} ^ {n} \\mathbf {V} _ {i} ^ {\\mathrm {T}} \\mathbf {V} _ {i}) ^ {- 1} (\\frac {1}{n} \\sum_ {i = 1} ^ {n} \\mathbf {V} _ {i} ^ {\\mathrm {T}} \\mathbf {S} _ {i}) (\\beta_ {S} - \\tilde {\\beta} _ {S}), (3)\n$$\n\n$$\n\n\n$$\n\n$$\n\\tilde {\\beta} _ {S} - \\beta_ {S} = \\left(\\frac {1}{n} \\sum_ {i = 1} ^ {n} \\mathbf {S} _ {i} ^ {\\mathrm {T}} \\mathbf {S} _ {i}\\right) ^ {- 1} \\left(\\frac {1}{n} \\sum_ {i = 1} ^ {n} \\mathbf {S} _ {i} ^ {\\mathrm {T}} g (\\mathbf {S} _ {i})\\right) + \\left(\\frac {1}{n} \\sum_ {i = 1} ^ {n} \\mathbf {S} _ {i} ^ {\\mathrm {T}} \\mathbf {S} _ {i}\\right) ^ {- 1} \\left(\\frac {1}{n} \\sum_ {i = 1} ^ {n} \\mathbf {S} _ {i} ^ {\\mathrm {T}} \\mathbf {V} _ {i}\\right) \\left(\\beta_ {V} - \\tilde {\\beta} _ {V}\\right), \\tag {4}\n$$\n\n$$\n\n\nwhere  $n$  is labeled node size,  $\\mathbf{S}_i$  is  $i$ -th sample of  $\\mathbf{S}$ ,  $\\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{V}_i^{\\mathrm{T}} g(\\mathbf{S}_i) = \\mathbb{E}(\\mathbf{V}^{\\mathrm{T}} g(\\mathbf{S})) + o_p(1)$ ,  $\\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{V}_i^{\\mathrm{T}} \\mathbf{S}_i = \\mathbb{E}(\\mathbf{V}^{\\mathrm{T}} \\mathbf{S}) + o_p(1)$  and  $o_p(1)$  is the error which is negligible. Ideally,  $\\tilde{\\beta}_V - \\beta_V = 0$  indicates that there is no bias between the estimated and the real parameter. However, if  $\\mathbb{E}(\\mathbf{V}^{\\mathrm{T}} \\mathbf{S}) \\neq 0$  or  $\\mathbb{E}(\\mathbf{V}^{\\mathrm{T}} g(\\mathbf{S})) \\neq 0$  in Eq. (3),  $\\tilde{\\beta}_V$  will be biased, leading to the biased estimation on  $\\tilde{\\beta}_S$  in Eq. (4) as well. Since the correlation between  $\\mathbf{V}$  and  $\\mathbf{S}$  (or  $g(\\mathbf{S})$ ) might shift in test phase, the biased parameters learned in training set is not the optimal parameters for predicting testing nodes. Therefore, to increase the stability of prediction, we need to unbiasedly estimate the parameters of  $\\tilde{\\beta}_V$  by removing the correlation between  $\\mathbf{V}$  and  $\\mathbf{S}$  (or  $g(\\mathbf{S})$ ) on training graph, making  $\\mathbb{E}(\\mathbf{V}^{\\mathrm{T}} \\mathbf{S}) = 0$  or  $\\mathbb{E}(\\mathbf{V}^{\\mathrm{T}} g(\\mathbf{S})) = 0$ . Note that  $\\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{S}_i^{\\mathrm{T}} g(\\mathbf{S}_i)$  in Eq. (4) can also cause estimation bias, but the relation between  $\\mathbf{S}$  and  $g(\\mathbf{S})$  is stable across environments, which do not influence the stability to some extent.\n\n\n# 3 PROPOSED MODEL\n\n\n# 3.1 REVISITING ON VARIABLE DECORRELATION IN CAUSAL VIEW\n\n\nTo decorrelate  $\\mathbf{V}$  and  $\\mathbf{S}$  (or  $g(\\mathbf{S})$ ), we should decorrelate the output variables of  $\\hat{\\mathcal{G}}(\\mathbf{X}, \\mathbf{A}; \\theta_g)$  (Kuang et al., 2020). They propose a Variable Decorrelation (VD) term with sample reweighting technique to eliminate the correlation between each variable pair, in which the sample weights are learned by jointly minimizing the moment discrepancy between each variable pair:\n\n\n$$\n\n$$\n\\mathcal {L} _ {V D} (\\mathbf {H}) = \\sum_ {j = 1} ^ {p} \\left| \\left| \\mathbf {H} _ {. j} ^ {\\mathrm {T}} \\Lambda_ {\\mathbf {w}} \\mathbf {H} _ {- j} / n - \\mathbf {H} _ {. j} ^ {\\mathrm {T}} \\mathbf {w} / n \\cdot \\mathbf {H} _ {- j} ^ {\\mathrm {T}} \\mathbf {w} / n \\right| \\right| _ {2} ^ {2}, \\tag {5}\n$$\n\n$$\n\n\nwhere  $\\mathbf{H} \\in \\mathbb{R}^{n \\times p}$  means the variables needed to be decorrelated,  $\\mathbf{H}_{.j}$  is  $j$ -th variable of  $\\mathbf{H}$ ,  $\\mathbf{H}_{-.j} = \\mathbf{H} \\backslash \\mathbf{H}_{.j}$  means all the remaining variables by setting the value of  $j$ -th variable in  $\\mathbf{H}$  as zero,  $\\mathbf{w} \\in \\mathbb{R}^{n \\times 1}$  are sample weights,  $\\sum_{i=1}^{n} \\mathbf{w}_i = n$  and  $\\Lambda_{\\mathbf{w}} = \\mathrm{diag}(\\mathbf{w}_1, \\dots, \\mathbf{w}_n)$  is the corresponding diagonal matrix. As we can see,  $\\mathcal{L}_{VD}(\\mathbf{H})$  can be reformulated as  $\\sum_{j \\neq k} ||\\mathbf{H}_{.j}^{\\mathrm{T}} \\Lambda_{\\mathbf{w}} \\mathbf{H}_{.k} / n - \\mathbf{H}_{.j}^{\\mathrm{T}} \\mathbf{w} / n \\cdot \\mathbf{H}_{.k}^{\\mathrm{T}} \\mathbf{w} / n||_2^2$ , and it aims to let  $\\mathbb{E}(\\mathbf{H}_{.i}^{\\mathrm{T}} \\mathbf{H}_{.j}) = \\mathbb{E}(\\mathbf{H}_{.i}^{\\mathrm{T}}) \\mathbb{E}(\\mathbf{H}_{.j})$  for each variable pair  $j$  and  $k$ .  $\\mathcal{L}_{VD}(\\mathbf{H})$  decorrelates all\n\n\nthe variable pairs equally. However, decorrelating all the variables requires sufficient samples Kuang et al. (2020), i.e.,  $n \\to \\infty$ , which is hard to be satisfied, especially in the semi-supervised setting. In this scenario, we cannot guarantee  $\\mathcal{L}_{VD}(\\mathbf{H}) = 0$ . Therefore the key challenge is how to remove the correlation influencing the unbiased estimation most when  $\\mathcal{L}_{VD}(\\mathbf{H}) \\neq 0$ .\n\n\nInspired by confounding balancing technique in observational studies (Hainmueller, 2012), we revisit the variable decorrelation regularizer in causal view and show how to differentiate each variable pair. Confounding balancing techniques are often used for causal effect estimation of treatment  $T$ , where the distributions of confounders  $\\mathbf{X}$  are different between treated ( $T = 1$ ) and control ( $T = 0$ ) groups because of non-random treatment assignment. One could balance the distribution of confounders between treatment and control groups to unbiased estimate causal treatment effects (Yao et al., 2020). Most balancing approaches exploit moments to characterize distributions, and balance them by adjusting sample weights  $\\mathbf{w}$  as follows:  $\\mathbf{w} = \\arg \\min_{\\mathbf{w}} || \\sum_{i:T_i = 1} \\mathbf{X}_i - \\sum_{i:T_i = 0} \\mathbf{w}_i \\cdot \\mathbf{X}_i ||_2^2$ . After balancing, the treatment  $T$  and confounders  $\\mathbf{X}$  tend to be independent.\n\n\nGiven one targeted variable  $j$ , under the variables only have linear relation assumption<sup>1</sup>, its decorrelation term,  $\\mathcal{L}_{VD_j} = ||\\mathbf{H}_j^{\\mathrm{T}}\\boldsymbol{\\Lambda}_{\\mathbf{w}}\\mathbf{H}_{-j} / n - \\mathbf{H}_j^{\\mathrm{T}}\\mathbf{w} / n \\cdot \\mathbf{H}_{-j}^{\\mathrm{T}}\\mathbf{w} / n||_2^2$ , is to make  $\\mathbf{H}_{.j}$  independent of  $\\mathbf{H}_{-j}$ , which is same as the confounding balancing term making treatment and confounders independent. Thereby,  $\\mathcal{L}_{VD_j}$  can also be viewed as a confounding balancing term, where  $\\mathbf{H}_{.j}$  is treatment and  $\\mathbf{H}_{-j}$  is confounders, illustrated in Fig. 2(a). Hence, our target can be explained as unbiasedly estimate causal effect of each variable which is invariant across training and test set. As different variables may contribute unequally to the confounding bias, it is necessary to differentiate the confounders. The target of differentiating confounders exactly matches our target that removes the correlation of variables influencing the unbiased estimation most.\n\n\n# 3.2 DIFFERETIATED VARIABLE DECORRELATION\n\n\nConsidering the continuous treatment, the causal effect of treatment can be measured by Marginal Treatment Effect Function (MTEF) (Kreif et al., 2015), and defined as:  $MTEF = \\frac{\\mathbb{E}[Y_i(t)] - \\mathbb{E}[Y_i(t - \\Delta t)]}{\\Delta t}$ , where  $Y_{i}(t)$  represents the potential outcome of sample  $i$  with treatment status  $T = t$ ,  $\\mathbb{E}(\\cdot)$  refers to the expectation function, and  $\\Delta t$  denotes the increasing level of treatment. With the sample weights  $w$  decorrelating treatment and confounders, we can estimate the MTEF by:\n\n\n$$\n\n$$\n\\widehat {M T E F} = \\frac {\\sum_ {i : T _ {i} = t} \\mathbf {w} _ {i} \\cdot Y _ {i} (t) - \\sum_ {j : T _ {j} = t - \\Delta t} \\mathbf {w} _ {j} \\cdot Y _ {j} (t - \\Delta t)}{\\Delta t}. \\tag {6}\n$$\n\n$$\n\n\nNext we theoretically analyze how to differentiate confounders' weights with following theorem.\n\n\nTheorem 1. In observational studies, different confounders make unequal confounding bias on Marginal Treatment Effect Function (MTEF) with their own weights, and the weights can be learned via regressing outcome  $Y$  on confounders  $\\mathbf{X}$  and treatment variable  $T$ .\n\n\nWe prove Theorem 1 with the following assumption:\n\n\nAssumption 3 (Linearity). The regression of outcome  $Y$  on confounders  $\\mathbf{X}$  and treatment variable  $T$  is linear; that is  $Y = \\sum_{k \\neq t} \\alpha_k \\mathbf{X}_{.k} + \\alpha_t T + c + \\varepsilon$ , where  $\\alpha_k \\in \\alpha$  is the linear coefficient.\n\n\nUnder Assumption 3, we can write estimator of  $\\widehat{MTEF}$  as:\n\n\n$$\n\n$$\n\\begin{array}{l} \\widehat {M T E F} = \\frac {\\sum_ {i : T _ {i} = t} \\mathbf {w} _ {i} \\cdot Y _ {i} (t) - \\sum_ {j : T _ {j} = t - \\Delta t} \\mathbf {w} _ {j} \\cdot Y _ {j} (t - \\Delta t)}{\\Delta t} \\tag {7} \\\\ = M T E F + \\sum_ {k \\neq t} \\alpha_ {k} (\\frac {\\sum_ {i : T _ {i} = t} \\mathbf {w} _ {i} \\cdot \\mathbf {X} _ {i k} - \\sum_ {j : T _ {j} = t - \\Delta t} \\mathbf {w} _ {j} \\cdot \\mathbf {X} _ {j k}}{\\Delta t}) + \\phi (\\varepsilon), \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere  $MTEF$  is the ground truth,  $\\phi (\\varepsilon)$  means the noise term, and  $\\phi (\\varepsilon)\\simeq 0$  with Gaussian noise. The detailed derivation can be found in Appendix A. To reduce the bias of  $MTEF$ , we need regulate the term  $\\sum_{k\\neq t}\\alpha_k(\\frac{\\sum_{i:T_i = t}\\mathbf{w}_i\\cdot\\mathbf{X}_{ik} - \\sum_{j:T_j = t - \\Delta t}\\mathbf{w}_j\\cdot\\mathbf{X}_{jk}}{\\Delta t})$ , where  $\\frac{\\sum_{i:T_i = t}\\mathbf{w}_i\\cdot\\mathbf{X}_{ik} - \\sum_{j:T_j = t - \\Delta t}\\mathbf{w}_j\\cdot\\mathbf{X}_{jk}}{\\Delta t}$\n\n\n![Figure 2: (a) Diagram of decorrelating node embedding with confounding balance.  $\\mathbf{H}^{(K - 1)}$  is the node embedding to be decorrelated.  $T$  is the treatment, corresponding to one target variable in  $\\mathbf{H}^{(K - 1)}$ .  $\\mathbf{X}$  is the confounders, corresponding to the remaining variables of the target variable in  $\\mathbf{H}^{(K - 1)}$ .  $Y$  is the outcome, corresponding to labels. (b) The framework of GNN-DVD. The same color in the two figures represents the same kind of variable.](images/4435e9eec4e11e6112313fc3b5468e347f73ef5b0db30a76eedab994e80194c4.jpg)\n\n\nmeans the difference of the  $k$ -th confounder between treated and control samples. The parameter  $\\alpha_{k}$  represents the confounding bias weight of the  $k$ -th confounder, and it is the coefficient of  $\\mathbf{X}_{.k}$ . Moreover, because our target is to learn the weight of each variable pair, i.e., between treatment and each confounder, we need to learn the weight  $\\alpha_{t}$  of treatment that is the coefficient of  $T$ . Hence, the confounder weights and treatment weight can be learned from the regression of observed outcome  $Y$  on confounders  $\\mathbf{X}$  and treatment  $T$  under Linearity assumption.\n\n\nDue to the connection between treatment effect estimation with variable decorrelation as analyzed in Section 3.1, we utilize Theorem 1 to reweight the variable weight in variable decorrelation term. When apply the Theorem 1 to GNNs, the confounders  $\\mathbf{X}$  should be  $\\mathbf{H}_{-j}$  and treatment is  $\\mathbf{H}_{.j}$ , where the embedding  $\\mathbf{H}$  is learned by  $\\hat{\\mathcal{G}} (\\mathbf{X},\\mathbf{A};\\theta_g)$  in Eq. (2). And the variable weights  $\\alpha$  could be computed from the regression coefficients for  $\\mathbf{H}$ , hence  $\\alpha$  is equal to  $\\hat{\\beta}$  in Eq. (2). Then the Differentiated Variable Decorrelation (DVD) term can be formulated as follows:\n\n\n$$\n\n$$\n\\begin{array}{l} \\min  _ {\\mathbf {w}} \\mathcal {L} _ {D V D} (\\mathbf {H}) = \\sum_ {j = 1} ^ {p} \\left(\\alpha^ {\\mathrm {T}} \\cdot \\operatorname {a b s} \\left(\\mathbf {H} _ {. j} ^ {\\mathrm {T}} \\Lambda_ {\\mathbf {w}} \\mathbf {H} _ {- j} / n - \\mathbf {H} _ {. j} ^ {\\mathrm {T}} \\mathbf {w} / n \\cdot \\mathbf {H} _ {- j} ^ {\\mathrm {T}} \\mathbf {w} / n\\right)\\right) ^ {2} \\\\ + \\frac {\\lambda_ {1}}{n} \\sum_ {i = 1} ^ {n} \\mathbf {w} _ {i} ^ {2} + \\lambda_ {2} \\left(\\frac {1}{n} \\sum_ {i = 1} ^ {n} \\mathbf {w} _ {i} - 1\\right) ^ {2}, s. t. \\mathbf {w} \\succeq 0 \\tag {8} \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere  $\\mathrm{abs}(\\cdot)$  means the element-wise absolute value operation, preventing positive and negative values from eliminating. Term  $\\frac{\\lambda_1}{n}\\sum_{i=1}^{n}\\mathbf{w}_i^2$  is added to reduce the variance of sample weights to achieve stability, and the formula  $\\lambda_2\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{w}_i - 1\\right)^2$  avoids all the sample weights to be 0. The term  $\\mathbf{w} \\succeq 0$  constrains each sample weight to be non-negative. After variable reweighting, the weighted decorrelation term in Eq. (8) can be rewritten as  $\\sum_{j \\neq k} \\alpha_j^2 \\alpha_k^2 ||\\mathbf{H}_{.j}^{\\mathrm{T}} \\boldsymbol{\\Lambda}_{\\mathbf{w}} \\mathbf{H}_{.k} / n - \\mathbf{H}_{.j}^{\\mathrm{T}} \\mathbf{w} / n \\cdot \\mathbf{H}_{.k}^{\\mathrm{T}} \\mathbf{w} / n||_2^2$ , and the weight for variable pair  $j$  and  $k$  would be  $\\alpha_j^2 \\alpha_k^2$ , hence it considers both the weights of treatment and confounder. We prove the uniqueness property of  $\\mathbf{w}$  in Appendix B, as follows:\n\n\nTheorem 2 (Uniqueness). If  $\\lambda_1 n \\gg p^2 + \\lambda_2$ ,  $p^2 \\gg \\max(\\lambda_1, \\lambda_2)$ ,  $|\\mathbf{H}_{i,j}| \\leq c$  and  $|\\alpha_i| \\leq c$  for some constant  $c$ , the solution  $\\hat{\\mathbf{w}} \\in \\{\\mathbf{w} : |\\mathbf{w}_i| \\leq c\\}$  to minimize Eq. (8) is unique.\n\n\n# 3.3 DEBIASED GNN FRAMEWORK\n\n\nIn this section, we describe the framework of Debiased GNN that incorporates DVD/VD term with GNNs in a seamless way. As analyzed in Section 2.2, decorrelating  $\\hat{\\mathbf{A}}\\sigma (\\hat{\\mathbf{X}}\\mathbf{W}^{(0)})$  could make GCN stable. However, most GNNs follow a layer-by-layer stacking structure, and the output embedding of each layer is more easy to obtain in implementing. Since  $\\hat{\\mathbf{A}}\\sigma (\\hat{\\mathbf{X}}\\mathbf{W}^{(0)})$  is the aggregation of\n\n\nthe first layer embedding  $\\sigma (\\hat{\\mathbf{A}}\\mathbf{X}\\mathbf{W}^{(0)})$ , decorrelating these variables may lack the flexibility that incorporates DVD/VD term with other GNN structure. Fortunately, we have the following theorem to identify a more flexible way to combine variable decorrelation with GNNs.\n\n\nTheorem 3. Given  $p$  pairwise uncorrelated variables  $\\mathbf{Z} = (\\mathbf{Z}_1, \\mathbf{Z}_2, \\dots, \\mathbf{Z}_p)$ , with a linear aggregation operator  $\\hat{\\mathbf{A}}$ , the variables of  $\\mathbf{Y} = \\hat{\\mathbf{A}}\\mathbf{Z}$  are still pairwise uncorrelated.\n\n\nProof can be found in Appendix C. The theorem indicates that if the variables of embeddings  $\\mathbf{Z}$  are uncorrelated, after any form of linear neighborhood aggregation  $\\hat{\\mathbf{A}}$ , e.g., average, attention or sum, the variables of transformed embeddings  $\\mathbf{Y}$  would be also uncorrelated. Therefore, decorrelating  $\\sigma(\\hat{\\mathbf{A}}\\mathbf{X}\\mathbf{W}^{(0)})$  can also reduce the estimation bias. For a  $K$  layers of GNN, we can directly decorrelate the output of  $(K-1)$ -th layer, i.e.,  $\\sigma(\\hat{\\mathbf{A}}\\cdots\\sigma(\\hat{\\mathbf{A}}\\mathbf{X}\\mathbf{W}^{(0)})\\cdots\\mathbf{W}^{(K-2)})$  for a  $K$  layers of GCN.\n\n\nThe previous analysis finds a flexible way to incorporate DVD/VD term with GNNs, however, recall that we analyze GNNs based on the least squares loss, and most existing GNNs are designed for classification. Therefore, in the following, we analyze that the previous conclusions are still applicable in classification. We consider the cases that softmax layer is used as the output layer of GNNs and loss is the cross-entropy error function. We use the Newton-Raphson update rule (Bishop, 2006) to bridge the gap between linear regression and multi-classification. According to the Newton-Raphson update rule, the update formula for transformation matrix  $\\mathbf{W}^{(K - 1)}$  of the last layer of GCN can be derived:\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathbf {W} _ {. j} ^ {\\text {(n e w)}} = \\mathbf {W} _ {. j} ^ {\\text {(o l d)}} - \\left(\\mathbf {H} ^ {\\mathrm {T}} \\mathbf {R} \\mathbf {H}\\right) ^ {- 1} \\mathbf {H} ^ {\\mathrm {T}} \\left(\\mathbf {H} \\mathbf {W} _ {. j} ^ {\\text {(o l d)}} - \\mathbf {Y} _ {. j}\\right) \\tag {9} \\\\ = \\left(\\mathbf {H} ^ {\\mathrm {T}} \\mathbf {R} \\mathbf {H}\\right) ^ {- 1} \\left\\{\\mathbf {H} ^ {\\mathrm {T}} \\mathbf {R} \\mathbf {H} \\mathbf {W} _ {. j} ^ {(\\mathrm {o l d})} - \\mathbf {H} ^ {\\mathrm {T}} \\left(\\mathbf {H} \\mathbf {W} _ {. j} ^ {(\\mathrm {o l d})} - \\mathbf {Y} _ {. j}\\right) \\right\\} = \\left(\\mathbf {H} ^ {\\mathrm {T}} \\mathbf {R} \\mathbf {H}\\right) ^ {- 1} \\mathbf {H} ^ {\\mathrm {T}} \\mathbf {R} \\mathbf {z}, \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere  $\\mathbf{R}_{kj} = -\\sum_{n=1}^{N} \\mathbf{H}_n \\mathbf{W}_{.k}^{(\\mathrm{old})} (\\mathbf{I}_{kj} - \\mathbf{H}_n \\mathbf{W}_{.j}^{(\\mathrm{old})})$  is a weighing matrix and  $\\mathbf{I}_{kj}$  is the element of the identity matrix, and  $\\mathbf{z} = \\mathbf{H} \\mathbf{W}_{.j}^{(\\mathrm{old})} - \\mathbf{R}^{-1} (\\mathbf{Y}_{.j} - \\mathbf{W}_{.j} \\mathbf{H})$  is an effective target value. Eq. (9) takes the form of a set of normal equations for a weighted least-squares problem. As the weighing matrix  $\\mathbf{R}$  is not constant but depends on the parameter vector  $\\mathbf{W}_{.j}^{(\\mathrm{old})}$ , we must apply the normal equations iteratively. Each iteration uses the last iteration weight vector  $\\mathbf{W}_{.j}^{(\\mathrm{old})}$  to compute a revised weighing matrix  $\\mathbf{R}$  and regresses the target value  $\\mathbf{z}$  with  $\\mathbf{H} \\mathbf{W}_{.j}^{(\\mathrm{new})}$ . Therefore, the variable decorrelation can also be applied to the GNNs with softmax classifier to reduce the estimation bias in each iteration.\n\n\nFigure 2(b) is the framework of GNN-DVD, and we input the labeled nodes' embeddings  $\\tilde{\\mathbf{H}}^{(K - 1)}$  into the regularizer  $\\mathcal{L}_{DVD}(\\tilde{\\mathbf{H}}^{(K - 1)})$ . As GCN has the formula softmax( $\\hat{\\mathbf{A}}\\mathbf{H}^{(K - 1)}\\mathbf{W}^{(K - 1)})$ , the variable weights of  $\\tilde{\\mathbf{H}}^{(K - 1)}$  used for differentiating  $\\mathcal{L}_{DVD}(\\tilde{\\mathbf{H}}^{(K - 1)})$  can be computed from  $\\alpha = \\mathrm{Var}(\\mathbf{W}^{(K - 1)}, \\mathrm{axis} = 1)$ , where  $\\mathrm{Var}(\\cdot, \\mathrm{axis} = 1)$  refers to calculating the variance of each row of some matrix and it reflects each variable's weight for classification which is similar to the regression coefficients. Note that when incorporating VD term with GNNs, we do not need compute the variable weights. Then the sample weights  $\\mathbf{w}$  learned by DVD term have the ability to remove the correlation in  $\\tilde{\\mathbf{H}}^{(K - 1)}$ . We propose to use this sample weights to reweight softmax loss:\n\n\n$$\n\n$$\n\\min  _ {\\theta} \\mathcal {L} _ {G} = \\sum_ {l \\in \\mathcal {Y} _ {L}} \\mathbf {w} _ {l} \\cdot \\ln \\left(q \\left(\\tilde {\\mathbf {H}} _ {l} ^ {(K)}\\right) \\cdot \\mathbf {Y} _ {l}\\right), \\tag {10}\n$$\n\n$$\n\n\nwhere  $q(\\cdot)$  is the softmax function,  $\\mathcal{Y}_L$  is the set of labeled node indices and  $\\theta$  is the set of parameters of GCN. The complexity analysis as well as the optimization of whole algorithm are summarized in Appendix D.\n\n\n# 4 EXPERIMENTS\n\n\nDatasets Here, we validate the effectiveness of our method on node classification with two kinds of selection biased data, i.e., label selection bias and small sample selection bias. For label selection bias, we employ three widely used graph datasets: Cora, Citeseer and Pubmed (Sen et al., 2008). As in Section 2.1, we make the inductive setting for each graph and get three biased degrees for each graph. For small sample selection bias, we conduct the experiments on NELL dataset (Carlson\n\n\nTable 1: Performance of three citation networks. The \\* indicates the best results of the baselines. Best results of all methods are indicated in bold.  $\\%$  gain over GCN/GAT' means the improvement percent of GCN/GAT-DVD against GCN/GAT, respectively.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">Cora</td><td colspan=\"3\">CiteSeer</td><td colspan=\"3\">Pubmed</td></tr><tr><td>Light</td><td>Medium</td><td>Heavy</td><td>Light</td><td>Medium</td><td>Heavy</td><td>Light</td><td>Medium</td><td>Heavy</td></tr><tr><td>MLP</td><td>0.5624</td><td>0.5197</td><td>0.5087</td><td>0.4532</td><td>0.3757</td><td>0.3893</td><td>0.6852</td><td>0.6620</td><td>0.6378</td></tr><tr><td>Planetoid (Yang et al., 2016)</td><td>0.5890</td><td>0.5240</td><td>0.5180</td><td>0.5160</td><td>0.5140</td><td>0.4880</td><td>0.7160</td><td>0.6770</td><td>0.6680</td></tr><tr><td>Chebyshev (Defferrard et al., 2016)</td><td>0.7116</td><td>0.7006</td><td>0.6809</td><td>0.6542</td><td>0.6276</td><td>0.5920</td><td>0.7358</td><td>0.6862</td><td>0.6732</td></tr><tr><td>SGC (Wu et al., 2019)</td><td>0.7800</td><td>0.7800</td><td>0.7530</td><td>0.6780</td><td>0.6730*</td><td>0.6200</td><td>0.7880*</td><td>0.7560</td><td>0.6800</td></tr><tr><td>APPNP (Klicpera et al., 2019)</td><td>0.7913</td><td>0.7689</td><td>0.7629</td><td>0.6478</td><td>0.6052</td><td>0.5903</td><td>0.7639</td><td>0.7369</td><td>0.6862</td></tr><tr><td>GNM-GCN (Zhou et al., 2019)</td><td>0.7423</td><td>0.7531</td><td>0.7196</td><td>0.5793</td><td>0.5717</td><td>0.5125</td><td>0.7552</td><td>0.7381</td><td>0.7072</td></tr><tr><td>GNM-GAT (Zhou et al., 2019)</td><td>0.7875</td><td>0.7638</td><td>0.7404</td><td>0.6524</td><td>0.6487</td><td>0.5865</td><td>0.7438</td><td>0.7568</td><td>0.6891</td></tr><tr><td>GCN (Kipf &amp; Welling, 2016)</td><td>0.7851</td><td>0.7775</td><td>0.7422</td><td>0.6786</td><td>0.5952</td><td>0.5551</td><td>0.7673</td><td>0.7545</td><td>0.7247</td></tr><tr><td>GCN-VD</td><td>0.7951</td><td>0.7855</td><td>0.7522</td><td>0.6844</td><td>0.6676</td><td>0.6408</td><td>0.7727</td><td>0.7729</td><td>0.7399</td></tr><tr><td>GCN-DVD</td><td>0.7959</td><td>0.7885</td><td>0.7555</td><td>0.6908</td><td>0.6769</td><td>0.6496</td><td>0.7741</td><td>0.7746</td><td>0.7542</td></tr><tr><td>% gain over GCN</td><td>1.38%</td><td>1.41%</td><td>1.79%</td><td>1.8%</td><td>14.2%</td><td>17.0%</td><td>0.89%</td><td>2.67%</td><td>4.07%</td></tr><tr><td>GAT (Veli\u010dkovi\u0107 et al., 2017)</td><td>0.8067*</td><td>0.8019*</td><td>0.7578</td><td>0.7033*</td><td>0.6683</td><td>0.6475*</td><td>0.7665</td><td>0.7579*</td><td>0.7068</td></tr><tr><td>GAT-VD</td><td>0.8146</td><td>0.8079</td><td>0.7708</td><td>0.7149</td><td>0.6833</td><td>0.6611</td><td>0.7783</td><td>0.7689</td><td>0.7149</td></tr><tr><td>GAT-DVD</td><td>0.8179</td><td>0.8119</td><td>0.7694</td><td>0.7172</td><td>0.6825</td><td>0.6627</td><td>0.7788</td><td>0.7723</td><td>0.7210</td></tr><tr><td>% gain over GAT</td><td>1.39%</td><td>1.26%</td><td>1.53%</td><td>1.97%</td><td>2.12%</td><td>2.34%</td><td>1.6%</td><td>1.9%</td><td>2.0%</td></tr></table>\n\n\net al., 2010) that each class only has one labeled node for training. Due to the large scale of this dataset, the test nodes are easily to have distribution shift from training nodes. The details of the datasets and experimental setup are given in Appendix E. One can download codes and datasets for all experiments from the supplementary material.\n\n\nBaselines Under our proposed framework, we incorporate the VD/DVD term with GCN and GAT called GCN-VD/DVD and GAT-VD/DVD (details in Appendix F), and thus GCN and GAT are two basic baselines. We compare with GNM-GCN/GAT (Zhou et al., 2019) that considers the label selection bias in transductive setting. Moreover, several state-of-the-art GNNs are included: Chebyshev filter (Kipf & Welling, 2016), SGC (Wu et al., 2019) and APPNP (Klicpera et al., 2019). Additionally, we compare with Planetoid (Yang et al., 2016) and MLP trained on the labeled nodes.\n\n\nResults on Label Selection Bias Dataset The results are given in Table 1, and we have the following observations. First, the proposed models (i.e., GCN/GAT with VD/DVD terms) always achieve the best performances in most cases, which well demonstrates that the effectiveness of our proposed debiased GNN framework. Second, comparing with base models, our proposed models all achieve up to  $17.0\\%$  performance improvements, and gain larger improvements under heavier bias scenarios. Since the major difference between our model with base models is the VD/DVD regularizer, we can safely attribute the significant improvements to the effective decorrelation term and its seamless joint with GNN models. Moreover, GCN/GAT-DVD achieve better results than GCN/GAT-VD in most cases. It validates the importance and effectiveness of differentiating variables' weights in semi-supervised setting. Additional experimental results about the sample weight analysis and parameter sensitivity analysis can be found in Appendix G.\n\n\nResults on Small Sample Selection Bias Dataset As NELL is a large-scale graph, we cannot run GAT on a single GPU with 16GB memory. We only perform GCN-VD/DVD and compare with representative methods which can perform on this dataset. The results are shown in Table 2. First, GCN-VD/DVD achieve significant improvements over GCN. It indicates that selection bias could be induced by a small number of labeled nodes and our proposed method can relieve the estimation bias. Moreover, GCN-DVD further improves GCN-VD with a large margin. It further validates that decorrelating all the variable pairs equally is suboptimal, and our differentiated strategy is effective when labeled nodes are scarce. The reason that GNM-GCN fails is the GNM relies on the accuracy of the IPW estimator that predicts the probability of a node to be selected, however, in this dataset, the ratio of positive and negative samples are extremely unbalanced influencing the performance of IPW.\n\n\nTable 2: Performance of NELL\n\n\n<table><tr><td>Dataset</td><td>MLP</td><td>Planetoid</td><td>SGC</td><td>GNM-GCN</td><td>GCN</td><td>GCN-VD</td><td>GCN-DVD</td></tr><tr><td>NELL</td><td>0.2385</td><td>0.3901</td><td>0.4128</td><td>0.1589</td><td>0.4416</td><td>0.4652</td><td>0.4734</td></tr></table>\n\n\n# 5 RELATED WORKS\n\n\nIn the past few years, Graph Neural Networks (GNNs) (Scarselli et al., 2008; Kipf & Welling, 2016; Velickovi\u0107 et al., 2017; Xu et al., 2019; Klicpera et al., 2019) have become the major technology to capture patterns encoded in the graph due to its powerful representation capacity. Although the current GNNs have achieved great success, when applied to inductive setting, they all assume that training nodes and test nodes follow the same distribution. However, this assumption does not always hold in real applications. GNM (Zhou et al., 2019) first pays attention on the label selection problem on graph learning, and it learns a IPW estimator to estimate the probability of each node to be selected and uses this probability to reweight the labeled nodes. However, it heavily relies on the accuracy of IPW estimator, which depends on the label assignment distribution of whole graph, hence it is more suitable for transductive setting.\n\n\nTo enhance the stability in unseen varied distributions, some literatures (Shen et al., 2020b; Kuang et al., 2020) have revealed the connection between correlation and prediction stability under model misspecification. However, these methods are built on the simple regressions, but GNNs have more complex structure and properties needed to be considered. We also notice that Shen et al. (2020a) propose a differentiated variable decorrelation term for linear regression. However, this decorrelation term requires multiple environment with different correlations between stable variable and unstable variable available in the training stage while our method do not require.\n\n\n# 6 CONCLUSION\n\n\nIn this paper, we investigate a general and practical problem: learning GNNs with agnostic selection bias. The selection bias will inevitably cause the GNNs to learn the biased correlation between aggregation mode and class label and make the prediction unstable. We then propose a novel differentiated decorrelated GNN, which combines the debiasing technique with GNNs in a unified framework. Extensive experiments well demonstrate the effectiveness and flexibility of GNN-DVD.\n\n\n# REFERENCES\n\n- Christopher M Bishop. Pattern recognition and machine learning. Springer, 2006.\n\n- Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, and Tom M Mitchell. Toward an architecture for never-ending language learning. In AAAI, 2010.\n\n- Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfitting in deep networks by decorrelating representations. In ICLR, 2016.\n\n- Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In NeurIPS, pp. 3844-3852, 2016.\n\n- Jens Hainmueller. Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political analysis, pp. 25-46, 2012.\n\n- Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeurIPS, pp. 1024-1034, 2017.\n\n- Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2016.\n\n- Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR, 2019.\n\n- No\u00e9mi Kreif, Richard Grieve, Ivan Diaz, and David Harrison. Evaluation of the effect of a continuous treatment: a machine learning approach with an application to treatment for traumatic brain injury. Health economics, 24(9):1213-1228, 2015.\n\n- Kun Kuang, Ruoxuan Xiong, Peng Cui, Susan Athey, and Bo Li. Stable prediction with model misspecification and agnostic distribution shift. In AAAI, 2020.\n\n- Yuji Nakatsukasa. Absolute and relative weyl theorems for generalized eigenvalue problems. Linear Algebra and its Applications, 432(1):242-248, 2010.\n\n- Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.\n\n- Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93-93, 2008.\n\n- Zheyan Shen, Peng Cui, Jiashuo Liu, Tong Zhang, Bo Li, and Zhitang Chen. Stable learning via differentiated variable decorrelation. In KDD, pp. 2185-2193, 2020a.\n\n- Zheyan Shen, Peng Cui, Tong Zhang, and Kun Kuang. Stable learning via sample reweighting. In AAAI, pp. 5692-5699, 2020b.\n\n- Petar Velickovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In ICLR, 2017.\n\n- Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In ICML, pp. 6861-6871. PMLR, 2019.\n\n- Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In ICLR, 2019. URL https://openreview.net/forum?id=ryGs6iA5Km.\n\n- Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In ICML, 2016.\n\n- Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao, and Aidong Zhang. A survey on causal inference. arXiv preprint arXiv:2002.02770, 2020.\n\n- Bianca Zadrozny. Learning and evaluating classifiers under sample selection bias. In ICML, pp. 114, 2004.\n\n- Fan Zhou, Tengfei Li, Haibo Zhou, Hongtu Zhu, and Ye Jieping. Graph-based semi-supervised learning with non-ignorable non-response. In NeurIPS, pp. 7015-7025, 2019.\n\n\nA DERIVATION OF MTEF\n\n\n$$\n\n$$\n\\begin{array}{l} \\widehat {M T E F} = \\frac {\\sum_ {i : T _ {i} = t} \\mathbf {w} _ {i} \\cdot Y _ {i} (t) - \\sum_ {j : T _ {j} = t - \\Delta t} \\mathbf {w} _ {j} \\cdot Y _ {j} (t - \\Delta t)}{\\Delta t} \\\\ = \\frac {\\sum_ {i : T _ {i} = t} \\mathbf {w} _ {i} \\cdot (\\sum_ {k \\neq t} \\alpha_ {k} \\mathbf {X} _ {i k} + \\alpha_ {t} t + c + \\epsilon) - \\sum_ {j : T _ {j} = t - \\Delta t} \\mathbf {w} _ {j} \\cdot (\\sum_ {k \\neq t} \\alpha_ {k} \\mathbf {X} _ {j k} + \\alpha_ {t} (t - \\Delta t) + c + \\epsilon)}{\\Delta t} \\\\ = \\frac {\\sum_ {i : T _ {i} = t} \\mathbf {w} _ {i} \\alpha_ {t} t - \\sum_ {j : T _ {j} = t - \\Delta t} \\mathbf {w} _ {j} \\alpha_ {t} (t - \\Delta t)}{\\Delta t} \\\\ + \\frac {\\left(\\sum_ {i : T _ {i} = t} \\mathbf {w} _ {i} \\sum_ {k \\neq t} \\alpha_ {k} \\mathbf {X} _ {i k} - \\sum_ {j : T _ {j} = t - \\Delta t} \\mathbf {w} _ {j} \\sum_ {k \\neq t} \\alpha_ {k} \\mathbf {X} _ {i k}\\right)}{\\Delta t} + \\phi (\\epsilon) \\\\ = M T E F + \\sum_ {k \\neq t} \\alpha_ {k} \\left(\\frac {\\sum_ {i : T _ {i} = t} \\mathbf {w} _ {i} \\cdot \\mathbf {X} _ {i k} - \\sum_ {j : T _ {j} = t - \\Delta t} \\mathbf {w} _ {j} \\cdot \\mathbf {X} _ {j k}}{\\Delta t}\\right) + \\phi (\\epsilon), \\tag {11} \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere  $\\frac{\\sum_{i:T_i = t}\\mathbf{w}_i\\alpha_t t - \\sum_{j:T_j = t - \\Delta t}\\mathbf{w}_j\\alpha_t(t - \\Delta t)}{\\Delta t}$  is the ground truth of MTEF,  $\\phi (\\epsilon)$  means the noise term, and  $\\phi (\\epsilon)\\simeq 0$  with Gaussian noise.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638564.4012053", "msecs": "401.0", "relativeCreated": "161231.70280456543", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Accelerating Safe Reinforcement Learning with Constraint-mismatched Policies\n\n# Abstract\nWe consider the problem of reinforcement learning when provided with (1) a baseline control policy and (2) a set of constraints that the controlled system must satisfy. The baseline policy can arise from a teacher agent, demonstration data or even a heuristic while the constraints might encode safety, fairness or other application-specific requirements. Importantly, the baseline policy may be sub-optimal for the task at hand, and is not guaranteed to satisfy the specified constraints. The key challenge therefore lies in effectively leveraging the baseline policy for faster learning, while still ensuring that the constraints are minimally violated. To reconcile these potentially competing aspects, we propose an iterative policy optimization algorithm that alternates between maximizing expected return on the task, minimizing distance to the baseline policy, and projecting the policy onto the constraint-satisfying set. We analyze the convergence of our algorithm theoretically and provide a finite-sample guarantee. In our empirical experiments on five different control tasks, our algorithm consistently outperforms several state-of-the-art methods, achieving 10 times fewer constraint violations and 40% higher reward on average.\n\n\n# 1 INTRODUCTION\n\n\nDeep reinforcement learning (RL) has achieved superior performance in several domains such as games (Mnih et al., 2013; Silver et al., 2016) and robotic control (Levine et al., 2016; Rajeswaran et al., 2017). However, in these complex applications, learning policies from scratch often requires tremendous amounts of time and computation power. To alleviate this issue, one would like to leverage a baseline policy available from a teacher or a previous task. However, the baseline policy may be sub-optimal for the new application and is not guaranteed to produce actions that satisfy given constraints on safety, fairness, or other costs. For instance, when you drive an unfamiliar vehicle, you do so cautiously to ensure safety, while at the same time you adapt your driving technique to the vehicle characteristics to improve your 'driving reward'. In effect, you (as the agent) gradually adapt a baseline policy (i.e., prior driving skill) to avoid violating the constraints (e.g., safety) while improving your driving reward (e.g., travel time, fuel efficiency).\n\n\nThis problem is challenging because directly leveraging the baseline policy, as in DAGGER (Ross et al., 2011) or GAIL (Ho & Ermon, 2016), may result in policies that violate the constraints since the baseline is not guaranteed to satisfy them. To ensure constraint satisfaction, prior work either adds a hyper-parameter weighted copy of the imitation learning (IL) objective (i.e., imitating the baseline policy) to the RL objective (Rajeswaran et al., 2017; Gao et al., 2018; Hester et al., 2018), or pre-trains a policy with the baseline policy and then fine-tunes it through RL (M\u00fclling et al., 2013; Chernova & Thomaz, 2014). Both these approaches incur the cost of weight tuning to satisfy the cost constraint and do not ensure constraint satisfaction during training.\n\n\nIn this work, to learn from the baseline policy while satisfying constraints, we propose an iterative algorithm that performs policy updates in three stages. The first step updates the policy to maximize expected reward using trust region policy optimization (e.g., TRPO (Schulman et al., 2015)). This can, however, result in a new intermediate policy that is too far from the baseline policy and one that may not satisfy the constraints. The second step performs a projection in policy space to control\n\n\n![(a)](images/81767c325ff7af3070470620f5359cc3dbaefc9c8eedfa364b7300fb102f6dc0.jpg)\n\n\n![(b) Figure 1: (a) Update procedures for SPACE. Step 1 (green) improves the reward in the trust region. Step 2 (blue) projects the policy onto a region around the baseline policy  $\\pi_B$ . Step 3 (red) projects the policy onto the constraint set. (b) Illustrating when  $\\pi_B$  is outside the constraint set. (c) Illustrating when  $\\pi_B$  is inside the constraint set. The highest reward is achieved at the yellow star.](images/2be668a4c9ba1d78637013a094f8a4b58c63588e02aec3827f8ac998a7f8bdbc.jpg)\n\n\n![(c)](images/feacc3e50d353f8971792545df24eace88a34ba56cdcf9116a3dbb2de94da916.jpg)\n\n\nthe distance between the current policy and the baseline policy. This distance is updated each episode depending on the reward improvement and constraint satisfaction, allowing the learning algorithm to explore without being overly restricted by the (potentially constraint-violating) baseline policy (Rajeswaran et al., 2017). This also enables the baseline policy to influence the learning without the computational burden of learning a cost function for the baseline policy (Kwon et al., 2020). The third step ensures constraint satisfaction at every iteration by performing a projection onto the set of policies that satisfy the given constraints. This ensures recovery from infeasible (i.e., constraint-violating) states (e.g., due to approximation errors), and eliminates the need for tuning weights for auxiliary cost objective functions (Tessler et al., 2018). We call our algorithm Safe Policy Adaptation with Constrained Exploration (SPACE).\n\n\nThis paper's contributions are two-fold. We first analyze our proposed SPACE algorithm and provide a finite-time guarantee for its convergence. We also provide an analysis of controlling the distance between the learned policy at iteration  $k$  and the baseline policy to ensure both feasibility of the optimization problem and safe exploration by the learning agent. Second, we empirically compare SPACE with state-of-the-art algorithms on five different control tasks, including two Mujoco environments with safety constraints from Achiam et al. (2017), two challenging traffic management tasks with fairness constraints from Vinitsky et al. (2018), and one human demonstration driving task with safety constraints from Brockman et al. (2016). In all tasks, SPACE outperforms the state-of-the-art safe RL algorithm, projection-based constrained policy optimization (PCPO) in Yang et al. (2020), averaging  $40\\%$  more reward with 10 times fewer cost constraint violations. This shows that SPACE leverages the baseline policy to achieve better learning efficiency while satisfying the cost constraint.\n\n\n# 2 RELATED WORK\n\n\nPolicy optimization with constraints. Learning constraint-satisfying policies has been explored in the context of safe RL (Garcia & Fernandez, 2015), (Hasanbeig et al., 2020; Junges et al., 2016; Jansen et al., 2020). Prior work either uses a conditional-gradient type of approach (Achiam et al., 2017), adds a weighted copy of the cost objective in the reward function (Tessler et al., 2018; Chow et al., 2019; Fujimoto et al., 2019; Stooke et al., 2020), adds a safety layer to the policy (Dalal et al., 2018) (Avni et al., 2019), or concerns about the chanced constraints (Fu & Prashanth L, 2018; Zheng & Ratliff, 2020). Perhaps the closest work to ours is Projection-based Constrained Policy Optimization (PCPO) (Yang et al., 2020), which also uses projections in policy space to ensure constraint satisfaction. However, PCPO does not have the capability to safely exploit prior information (through a baseline policy). The lack of using prior policies in PCPO makes it sample-inefficient. In addition, our SPACE algorithm's update dynamically sets distances between policies while PCPO does not. This update is important to effectively and safely learn from the baseline policy. Furthermore, we provide a safety guarantee to ensure the feasibility of the optimization problem while PCPO does not. Merely adding an IL objective in the reward objective of PCPO cannot learn efficiently, as shown in experiments. This analysis allows us to advance towards the practical use of RL in real applications, which PCPO and other algorithms have never done before.\n\n\nPolicy optimization with the initial safe set. Wachi & Sui (2020); Sui et al. (2015); Turchetta et al. (2016) assume that the initial safe set is given, and the agent explores the environment and verifies the safety function from this initial safe set. There is no baseline policy here. In contrast, our assumption is to give a baseline policy to the agent. Both assumptions are reasonable as they provide an initial understanding of the environment.\n\n\n<table><tr><td>Algorithm</td><td>Properties</td><td>Approach</td></tr><tr><td>DAGGER (Ross et al., 2011)</td><td>\u2642\u2642\u2642</td><td>Optimize IL objective</td></tr><tr><td>DAPG (Rajeswaran et al., 2017)</td><td>\u2642\u2642\u2642</td><td>Optimize RL and IL objectives jointly</td></tr><tr><td>NAC (Gao et al., 2018)</td><td>\u2642\u2642</td><td>Pre-train with IL and find-tune with RL</td></tr><tr><td>IC-GAIL (Wu et al., 2019)</td><td>\u2642\u2642</td><td>Generative adversarial IL</td></tr><tr><td>CPO (Achiam et al., 2017)</td><td>\u2642\u25b3</td><td>Safe RL solved by a line search</td></tr><tr><td>RCPO (Tessler et al., 2018)</td><td>\u2642\u25b3</td><td>Safe RL solved by a Lagrangian approach</td></tr><tr><td>PCPO (Yang et al., 2020)</td><td>\u2642\u25b3</td><td>Safe RL solved by a projection</td></tr><tr><td>PID-RL (Stooke et al., 2020)</td><td>\u2642\u25b3</td><td>Safe RL solved by PID control</td></tr><tr><td>SPACE (ours)</td><td>\u2642\u25b3\u2642</td><td>Safe RL aided by baseline policies with analysis</td></tr></table>\n\n\nLeveraging baseline policies for RL. Prior work has used baseline policies to provide initial information to RL algorithms to reduce or avoid undesirable situations. This is done by either: initializing the policy with the baseline policy (Driessens & D\u017eeroski, 2004; Smart & Kaelbling, 2000; Koppejan & Whiteson, 2011; Abbeel et al., 2010; Gao et al., 2018; Le et al., 2019; Vecerik et al., 2017; Jaques et al., 2019); or providing a teacher's advice to the agent (Garcia & Fern\u00e1ndez, 2012; Quint\u00eda Vidal et al., 2013; Abel et al., 2017; Zhang et al., 2019). However, such works often assume that the baseline policy is constraint-satisfying (Sun et al., 2018; Balakrishna et al., 2019). In contrast, our SPACE algorithm safely leverages the baseline policy without requiring it to satisfy the specified constraints.\n\n\nPathak et al. (2015); Bartocci et al. (2011) also modify the existing known models (policies) based on new conditions in the context of the formal methods. In contrast, we solve this problem using projections in the policy space.\n\n\nLearning from logged demonstration data. To effectively learn from demonstration data given by the baseline policy, Wu et al. (2019); Brown et al. (2019); Kwon et al. (2020) assess the demonstration data by either: predicting their cost in the new task using generative adversarial networks (GANs) (Goodfellow et al., 2014); or directly learning the cost function of the demonstration data. However, these approaches require a large number of training samples from the new task. In addition, the learned cost function is not guaranteed to recover the true one. This may result in driving the agent to undesirable situations. In contrast, SPACE controls the distance between the learned and baseline policies to ensure reward improvement and constraint satisfaction.\n\n\nTable 1 catalogs related work on safe RL and IL. We can differentiate these methods on several key axes: (1) whether they optimize reward and/or ensure cost satisfaction, (2) whether they use on-policy methods (i.e., query the baseline policy) or off-policy methods (i.e., learn from a batch of demonstration data), (3) whether the baseline policy or demonstration data is optimal for the agent's own objective, and (4) the technical approach of the work.\n\n\n# 3 PROBLEM FORMULATION\n\n\nWe frame our problem as a constrained Markov Decision Process (CMDP) (Altman, 1999), defined as a tuple  $< S, \\mathcal{A}, T, R, C >$ . Here  $S$  is the set of states,  $\\mathcal{A}$  is the set of actions, and  $T$  specifies the conditional probability  $T(s'|s, a)$  that the next state is  $s'$  given the current state  $s$  and action  $a$ . In addition,  $R: S \\times \\mathcal{A} \\to \\mathbb{R}$  is a reward function, and  $C: S \\times \\mathcal{A} \\to \\mathbb{R}$  is a constraint cost function. The reward function encodes the benefit of using action  $a$  in state  $s$ , while the cost function encodes the corresponding constraint violation penalty.\n\n\nA policy is a map from states to probability distributions on  $\\mathcal{A}$ . It specifies that in state  $s$  the selected action is drawn from the distribution  $\\pi(s)$ . The state then transits from  $s$  to  $s'$  according to the state transition distribution  $T(s'|s, a)$ . In doing so, a reward  $R(s, a)$  is received and a constraint cost  $C(s, a)$  is incurred, as outlined above.\n\n\nLet  $\\gamma \\in (0,1)$  denote a discount factor, and  $\\tau$  denote the trajectory  $\\tau = (s_0,a_0,s_1,\\dots)$  induced by a policy  $\\pi$ . Normally, we seek a policy  $\\pi$  that maximizes a cumulative discounted reward\n\n\n$$\n\n$$\nJ _ {R} (\\pi) \\doteq \\mathbb {E} _ {\\tau \\sim \\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} R \\left(s _ {t}, a _ {t}\\right) \\right], \\tag {1}\n$$\n\n$$\n\n\nwhile keeping the cumulative discounted cost below  $h_{C}$\n\n\n$$\n\n$$\nJ _ {C} (\\pi) \\doteq \\mathbb {E} _ {\\tau \\sim \\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} C \\left(s _ {t}, a _ {t}\\right) \\right] \\leq h _ {C}. \\tag {2}\n$$\n\n$$\n\n\nHere we consider an additional objective. We are provided with a baseline policy  $\\pi_B$  and at each state  $s$  we measure the divergence between  $\\pi(s)$  and  $\\pi_B(s)$ . For example, this could be the KL-divergence  $D(s) \\doteq D_{\\mathrm{KL}}(\\pi(s) \\| \\pi_B(s))$ . We then seek a policy that maximizes Eq. (1), satisfies Eq. (2), and ensures the discounted divergence between the learned and baseline policies is below  $h_D$ :\n\n\n$$\n\n$$\nJ _ {D} (\\pi) \\doteq \\mathbb {E} _ {\\tau \\sim \\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} D (s _ {t}) \\right] \\leq h _ {D}. \\tag {3}\n$$\n\n$$\n\n\nWe do not assume that the baseline policy satisfies the cost constraint. Hence we allow  $h_D$  to be adjusted during the learning of  $\\pi$  to allow for reward improvement and constraint satisfaction.\n\n\nLet  $\\mu_t(\\cdot|\\pi)$  denote the state distribution at time  $t$  under policy  $\\pi$ . The discounted state distribution induced by  $\\pi$  is defined to be  $d^{\\pi}(s) \\doteq (1 - \\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\mu_{t}(s|\\pi)$ . Now bring in the reward advantage function (Kakade & Langford, 2002) defined by\n\n\n$$\n\n$$\nA _ {R} ^ {\\pi} (s, a) \\doteq Q _ {R} ^ {\\pi} (s, a) - V _ {R} ^ {\\pi} (s),\n$$\n\n$$\n\n\nwhere  $V_{R}^{\\pi}(s) \\doteq \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t}, a_{t}) \\mid s_{0} = s \\right]$  is the expected reward from state  $s$  under policy  $\\pi$ , and  $Q_{R}^{\\pi}(s, a) \\doteq \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t}, a_{t}) \\mid s_{0} = s, a_{0} = a \\right]$  is the expected reward from state  $s$  and initial action  $a$ , and thereafter following policy  $\\pi$ . These definitions allow us to express the reward performance of one policy  $\\pi'$  in terms of another  $\\pi$ :\n\n\n$$\n\n$$\nJ _ {R} (\\pi^ {\\prime}) - J _ {R} (\\pi) = \\frac {1}{1 - \\gamma} \\mathbb {E} _ {s \\sim d ^ {\\pi^ {\\prime}}, a \\sim \\pi^ {\\prime}} [ A _ {R} ^ {\\pi} (s, a) ].\n$$\n\n$$\n\n\nSimilarly, we can define  $A_D^\\pi(s,a)$ ,  $Q_D^\\pi(s,a)$  and  $V_D^\\pi(s)$  for the divergence cost, and  $A_C^\\pi(s,a)$ ,  $Q_C^\\pi(s,a)$  and  $V_C^\\pi(s)$  for the constraint cost.\n\n\n# 4 SAFE POLICY ADAPTATION WITH CONSTRAINED EXPLORATION (SPACE)\n\n\nWe now describe the proposed iterative three step algorithm illustrated in Fig. 1. In what follows,  $\\pi^k$  denotes the learned policy after iteration  $k$ , and  $M$  denotes a distance measure between policies. For example,  $M$  might be the 2-norm of the difference of policy parameters or some average over the states of the KL-divergence of the action policy distributions.\n\n\nStep 1. We perform one step of trust region policy optimization (Schulman et al., 2015). This maximizes the reward advantage function  $A_R^\\pi(s, a)$  over a KL-divergence neighborhood of  $\\pi^k$ :\n\n\n$$\n\n$$\n\\pi^ {k + \\frac {1}{3}} = \\arg \\max  _ {\\pi} \\mathbb {E} _ {s \\sim d ^ {\\pi^ {k}}, a \\sim \\pi} \\left[ A _ {R} ^ {\\pi^ {k}} (s, a) \\right] \\quad \\text {s . t .} \\mathbb {E} _ {s \\sim d ^ {\\pi^ {k}}} \\left[ D _ {\\mathrm {K L}} (\\pi (s) \\| \\pi^ {k} (s)) \\right] \\leq \\delta . \\tag {4}\n$$\n\n$$\n\n\nStep 2. We project  $\\pi^{k + \\frac{1}{3}}$  onto a region around  $\\pi_B$  controlled by  $h_D^k$  to minimize  $M$ :\n\n\n$$\n\n$$\n\\pi^ {k + \\frac {2}{3}} = \\underset {\\pi} {\\arg \\min } M \\left(\\pi , \\pi^ {k + \\frac {1}{3}}\\right) \\quad \\text {s . t .} J _ {D} \\left(\\pi^ {k}\\right) + \\frac {1}{1 - \\gamma} \\mathbb {E} _ {s \\sim d ^ {\\pi^ {k}}, a \\sim \\pi} \\left[ A _ {D} ^ {\\pi^ {k}} (s) \\right] \\leq h _ {D} ^ {k}. \\tag {5}\n$$\n\n$$\n\n\nStep 3. We project  $\\pi^{k + \\frac{2}{3}}$  onto the set of policies satisfying the cost constraint to minimize  $M$ :\n\n\n$$\n\n$$\n\\pi^ {k + 1} = \\underset {\\pi} {\\arg \\min } M (\\pi , \\pi^ {k + \\frac {2}{3}}) \\quad \\mathrm {s . t .} J _ {C} (\\pi^ {k}) + \\frac {1}{1 - \\gamma} \\mathbb {E} _ {s \\sim d ^ {\\pi^ {k}}, a \\sim \\pi} [ A _ {C} ^ {\\pi^ {k}} (s, a) ] \\leq h _ {C}. \\tag {6}\n$$\n\n$$\n\n\nControl  $h_D^k$  in Step 2. We select  $h_D^0$  to be small and gradually increase  $h_D^k$  at each iteration to expand the region around  $\\pi_B$ . Specifically, we make  $h_D^{k+1} > h_D^k$  if:\n\n- (a)  $J_{C}(\\pi^{k}) > J_{C}(\\pi^{k - 1})$ : this increase is to ensure a nonempty intersection between the region around  $\\pi_B$  and the cost constraint set (feasibility). See Fig. 1(b).\n\n- (b)  $J_R(\\pi^k) < J_R(\\pi^{k-1})$ : this increase gives the next policy more freedom to improve the reward and the cost constraint performance (exploration). See Fig. 1(c).\n\n\nIt remains to determine how to set the new value of  $h_D^{k+1}$ . Let  $\\mathcal{U}_1$  denote the set of policies satisfying the cost constraint, and  $\\mathcal{U}_2^k$  denote the set of policies in the region around  $\\pi_B$  controlled by  $h_D^k$ . Then we have the following Lemma.\n\n\nLemma 4.1 (Updating  $h_D$ ). If at step  $k + 1$ :  $h_D^{k + 1} \\geq \\mathcal{O}\\big((J_C(\\pi^k) - h_C)^2\\big) + h_D^k$ , then  $\\mathcal{U}_1 \\cap \\mathcal{U}_2^{k + 1} \\neq \\emptyset$  (feasibility) and  $\\mathcal{U}_2^{k + 1} \\cap \\partial \\mathcal{U}_1 \\neq \\emptyset$  (exploration).\n\n\nProof. See the supplementary material.\n\n\n![](images/ee4c774988e1fde78323997f1daaaf8d71294905a3dfd1cafe766b7e35a08fa3.jpg)\n\n\nLemma 4.1 ensures that the boundaries of the region around  $\\pi_B$  determined by  $h_D$  and the set of policies satisfying the cost constraint intersect. Note that  $h_D$  will become large enough to guarantee feasibility during training. This allows the learning algorithm to explore policies within the cost constraint set while still learning from the baseline policy.\n\n\n# 5 A THEORETICAL ANALYSIS OF SPACE\n\n\nWe will implement a policy as a neural network with fixed architecture parameterized by  $\\theta \\in \\mathbb{R}^n$ . We then learn a policy from the achievable set  $\\{\\pi (\\cdot |\\pmb {\\theta})\\colon \\pmb {\\theta}\\in \\mathbb{R}^n\\}$  by iteratively learning  $\\pmb{\\theta}$ . Let  $\\pmb{\\theta}^{k}$  and  $\\pi^k\\doteq \\pi (\\cdot |\\pmb{\\theta}^k)$  denote the parameter value and the corresponding policy at step  $k$ . In this setting, it is impractical to solve for the policy updates in Eq. (4), (5) and (6). Hence we approximate the reward function and constraints with first order Taylor expansions, and KL-divergence with a second order Taylor expansion. We will need the following derivatives: (1)  $\\pmb {g}^{k}\\doteq \\nabla_{\\pmb{\\theta}}\\mathbb{E}_{s\\sim d^{\\pi^{k}},a\\sim \\pi}[A_{R}^{\\pi^{k}}(s,a)]$  (2)  $\\pmb {a}^{k}\\doteq \\nabla_{\\pmb{\\theta}}\\mathbb{E}_{s\\sim d^{\\pi^{k}},a\\sim \\pi}[A_{D}^{\\pi^{k}}(s)]$  (3)  $\\pmb {c}^{k}\\doteq \\nabla_{\\pmb{\\theta}}\\mathbb{E}_{s\\sim d^{\\pi^{k}},a\\sim \\pi}[A_{C}^{\\pi^{k}}(s,a)]$  and (4)  $\\pmb {F}^{k}\\doteq \\nabla_{\\pmb{\\theta}}^{2}\\mathbb{E}_{s\\sim d^{\\pi^{k}}}\\left[D_{\\mathrm{KL}}(\\pi (s)\\| \\pi^{k}(s))\\right]$ . Each of these derivatives are taken w.r.t. the neural network parameter and evaluated at  $\\pmb{\\theta}^{k}$ . We also define  $b^{k}\\doteq J_{D}(\\pi^{k}) - h_{D}^{k}$ , and  $d^{k}\\doteq J_{C}(\\pi^{k}) - h_{C}$ . Let  $u^{k}\\doteq \\sqrt{\\frac{2\\delta}{g^{kT}F^{k - 1}g^{k}}}$ , and  $\\pmb {L} = \\pmb{I}$  for the 2-norm projection and  $\\pmb {L} = \\pmb{F}^{k}$  for the KL-divergence projection.\n\n\nWe first approximate Eq. (4), Eq. (5) and Eq. (6) using the following approximations:\n\n\nStep 1. Approximating Eq. (4) yields\n\n\n$$\n\n$$\n\\boldsymbol {\\theta} ^ {k + \\frac {1}{3}} = \\underset {\\boldsymbol {\\theta}} {\\arg \\max } \\mathbf {g} ^ {k ^ {T}} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}) \\quad \\text {s . t .} \\frac {1}{2} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}) ^ {T} \\mathbf {F} ^ {k} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}) \\leq \\delta . \\tag {7}\n$$\n\n$$\n\n\nStep 2 and Step 3. Approximating Eq. (5) and (6), similarly yields\n\n\n$$\n\n$$\n\\boldsymbol {\\theta} ^ {k + \\frac {2}{3}} = \\underset {\\boldsymbol {\\theta}} {\\arg \\min } \\frac {1}{2} \\left(\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k + \\frac {1}{3}}\\right) ^ {T} \\boldsymbol {L} \\left(\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k + \\frac {1}{3}}\\right) \\quad \\text {s . t .} \\boldsymbol {a} ^ {k T} \\left(\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}\\right) + b ^ {k} \\leq 0, \\tag {8}\n$$\n\n$$\n\n\n$$\n\n$$\n\\boldsymbol {\\theta} ^ {k + 1} = \\underset {\\boldsymbol {\\theta}} {\\arg \\min } \\frac {1}{2} \\left(\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k + \\frac {2}{3}}\\right) ^ {T} \\boldsymbol {L} \\left(\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k + \\frac {2}{3}}\\right) \\quad \\text {s . t .} \\boldsymbol {c} ^ {k T} \\left(\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}\\right) + d ^ {k} \\leq 0, \\tag {9}\n$$\n\n$$\n\n\nwhere  $\\pmb{L} = \\pmb{I}$  for the 2-norm projection and  $\\pmb{L} = \\pmb{F}^k$  for the KL-divergence projection. We solve these problems using using convex programming (see the supplementary material for the derivation), then for each policy update, we have\n\n\n$$\n\n$$\n\\begin{array}{l} \\boldsymbol {\\theta} ^ {k + 1} = \\boldsymbol {\\theta} ^ {k} + u ^ {k} \\boldsymbol {F} ^ {k - 1} \\boldsymbol {g} ^ {k} - \\max  \\left(0, \\frac {u ^ {k} \\boldsymbol {a} ^ {k T} \\boldsymbol {F} ^ {k - 1} \\boldsymbol {g} ^ {k} + b ^ {k}}{\\boldsymbol {a} _ {T} ^ {k T} \\boldsymbol {L} _ {- 1} ^ {- 1} \\boldsymbol {a} ^ {k}}\\right) \\boldsymbol {L} ^ {- 1} \\boldsymbol {a} ^ {k} \\tag {10} \\\\ - \\max  (0, \\frac {u ^ {k} \\boldsymbol {c} ^ {k ^ {T}} \\boldsymbol {F} ^ {k - 1} \\boldsymbol {g} ^ {k} + d ^ {k}}{\\boldsymbol {c} ^ {k ^ {T}} \\boldsymbol {L} ^ {- 1} \\boldsymbol {c} ^ {k}}) \\boldsymbol {L} ^ {- 1} \\boldsymbol {c} ^ {k}. \\\\ \\end{array}\n$$\n\n$$\n\n\nAlgorithm 1 shows the corresponding pseudocode.\n\n\nConvergence analysis. We consider the following simplified problem to provide a finite-time guarantee of SPACE:\n\n\n$$\n\n$$\n\\min  _ {\\boldsymbol {\\theta} \\in \\mathcal {C} _ {1} \\cap \\mathcal {C} _ {2}} f (\\boldsymbol {\\theta}), \\tag {11}\n$$\n\n$$\n\n\nwhere  $f: \\mathbb{R}^n \\to \\mathbb{R}$  is a twice continuously differentiable function at every point in a open set  $\\mathcal{X} \\subseteq \\mathbb{R}^n$ , and  $\\mathcal{C}_1 \\subseteq \\mathcal{X}$  and  $\\mathcal{C}_2 \\subseteq \\mathcal{X}$  are compact convex sets with  $\\mathcal{C}_1 \\cap \\mathcal{C}_2 \\neq \\emptyset$ . The function  $f$  is the\n\n\nnegative reward function of our CMDP, and the two constraint sets represent the cost constraint set and the region around  $\\pi_B$ .\n\n\nFor a vector  $\\pmb{x}$ , let  $\\| \\pmb{x} \\|$  denote the Euclidean norm. For a matrix  $M$  let  $\\| M \\|$  denote the induced matrix 2-norm, and  $\\sigma_{i}(M)$  denote the  $i$ -th largest singular value of  $M$ .\n\n\n# ASSUMPTION 1. WE ASSUME:\n\n- (1.1) The gradient  $\\nabla f$  is  $L$ -Lipschitz continuous over a open set  $\\mathcal{X}$ .\n\n- (1.2) For some constant  $G$ ,  $\\| \\nabla f(\\pmb{\\theta}) \\| \\leq G$ .\n\n- (1.3) For some constant  $H$ ,  $\\mathrm{diam}(\\mathcal{C}_1) \\leq H$  and  $\\mathrm{diam}(\\mathcal{C}_2) \\leq H$ .\n\n\nAssumptions (1.1) and (1.2) ensure that the gradient can not change too rapidly and the norm of the gradient can not be too large. Assumption (1.3) implies that for every iteration, the diameter of the region around  $\\pi_B$  is bounded above by  $H$ .\n\n\nWe will need a concept of an  $\\epsilon$ -first order stationary point (Mokhtari et al., 2018). For  $\\epsilon > 0$ , we say that  $\\theta^{*} \\in \\mathcal{C}_{1} \\cap \\mathcal{C}_{2}$  an  $\\epsilon$ -first order stationary point ( $\\epsilon$ -FOSP) of Problem (11) under KL-divergence projection if\n\n\n$$\n\n$$\n\\nabla f \\left(\\boldsymbol {\\theta} ^ {*}\\right) ^ {T} \\left(\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {*}\\right) \\geq - \\epsilon , \\quad \\forall \\boldsymbol {\\theta} \\in \\mathcal {C} _ {1} \\cap \\mathcal {C} _ {2}. \\tag {12}\n$$\n\n$$\n\n\nSimilarly, under the 2-norm projection,  $\\pmb{\\theta}^{*}\\in \\mathcal{C}_{1}\\cap \\mathcal{C}_{2}$  an  $\\epsilon$ -FOSP of (11) if\n\n\n$$\n\n$$\n\\nabla f \\left(\\boldsymbol {\\theta} ^ {*}\\right) ^ {T} \\boldsymbol {F} ^ {*} \\left(\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {*}\\right) \\geq - \\epsilon , \\quad \\forall \\boldsymbol {\\theta} \\in \\mathcal {C} _ {1} \\cap \\mathcal {C} _ {2}, \\tag {13}\n$$\n\n$$\n\n\nwhere  $F^{*} \\doteq \\nabla_{\\theta}^{2}\\mathbb{E}_{s \\sim d^{\\pi^{*}}}[D_{\\mathrm{KL}}(\\pi(s)\\|\\pi^{*}(s))]$ . Notice that SPACE converges to distinct stationary points under the two possible projections (see the supplementary material). With these assumptions, we have the following Theorem.\n\n\nTheorem 5.1 (Finite-Time Guarantee of SPACE). Under the KL-divergence projection, there exists a sequence  $\\{\\eta^k\\}$  such that SPACE converges to an  $\\epsilon$ -FOSP in at most  $\\mathcal{O}(\\epsilon^{-2})$  iterations. Moreover, at step  $k + 1$\n\n\n$$\n\n$$\nf \\left(\\boldsymbol {\\theta} ^ {k + 1}\\right) \\leq f \\left(\\boldsymbol {\\theta} ^ {k}\\right) - \\frac {L \\epsilon^ {2}}{2 \\left(G + \\frac {H \\sigma_ {1} \\left(\\boldsymbol {F} ^ {k}\\right)}{\\eta^ {k}}\\right) ^ {2}}. \\tag {14}\n$$\n\n$$\n\n\nSimilarly, under the 2-norm projection, there exists a sequence  $\\{\\eta^k\\}$  such that SPACE converges to an  $\\epsilon$ -FOSP in at most  $\\mathcal{O}(\\epsilon^{-2})$  iterations. Moreover, at step  $k + 1$\n\n\n$$\n\n$$\nf \\left(\\boldsymbol {\\theta} ^ {k + 1}\\right) \\leq f \\left(\\boldsymbol {\\theta} ^ {k}\\right) - \\frac {L \\epsilon^ {2}}{2 \\left(G \\sigma_ {1} \\left(\\boldsymbol {F} ^ {k - 1}\\right) + \\frac {H}{\\eta^ {k}}\\right) ^ {2}}. \\tag {15}\n$$\n\n$$\n\n\nProof. The proof and the sequence  $\\{\\eta^k\\}$  are given in the supplementary material.\n\n\nWe now make several observations for Theorem 5.1.\n\n- (1) The smaller  $H$  is, the greater the decrease in the objective. This observation supports the idea of starting with a small value for  $h_D$  and increasing it only when needed.\n\n- (2) Under the KL-divergence projection, the effect of  $\\sigma_{1}(\\pmb{F}^{k})$  is negligible. This is because in this case  $\\eta^k$\n\n\n# ALGORITHM 1 SPACE\n\n\nInitialize a policy  $\\pi^0 = \\pi (\\cdot |\\pmb{\\theta}^0)$  and a trajectory buffer  $\\mathcal{B}$  for  $k = 0,1,2,\\dots$  do\n\n\nRun  $\\pi^k = \\pi (\\cdot |\\pmb {\\theta}^k)$  and store trajectories in  $\\mathcal{B}$\n\n\nCompute  $g, a, c, F, b$  and  $d$  using  $\\mathcal{B}$\n\n\nObtain  $\\pmb{\\theta}^{k + 1}$  using the update in Eq. (10)\n\n\nif  $J_{C}(\\pi^{k}) > J_{C}(\\pi^{k - 1})$  or  $J_{R}(\\pi^{k}) <   J_{R}(\\pi^{k - 1})$  then\n\n\nUpdate  $h_D^{k+1}$  using Lemma 4.1\n\n\nEmpty  $\\mathcal{B}$\n\n\nis determined by the KL-divergence between two consecutive updated policies (see the supplementary material). This implies that  $\\eta^k$  is proportional to  $\\sigma_1(\\pmb{F}^k)$ . Hence  $\\sigma_{1}(\\pmb{F}^{k})$  does not play a major role in decreasing the objective value.\n\n\n(3) Under the 2-norm projection, the smaller  $\\sigma_1(F^{k-1})$  (i.e., larger  $\\sigma_n(F^k)$ ) is, the greater the decrease in the objective. This is because a large  $\\sigma_n(F^k)$  means a large curvature of  $f$  in all directions. This implies that the 2-norm distance between the pre-projection and post-projection points is small, leading to a small deviation from the reward improvement direction after doing projections (see the supplementary material for a visualization).\n\n\n![(a) Gather](images/5c452235b82b930a71348bd0cd3a484bc9724c99b0c0f35e5a92e525a0d3191c.jpg)\n\n\n![(b) Circle](images/bec64ee33395951ae2fe71ec137ead6bd62daa4a8d754083cd38f8cb1e69fac1.jpg)\n\n\n![(c) Grid](images/69dd719e7d2d2bf2e0df0846c40b7736fa1b8ef5c58e9374a3a0183173bb4420.jpg)\n\n\n![(d) Bottleneck](images/37e5cd8b23b58708fabf33ced1633ed973ac0b6aa6dde4894f81965ec7cc7f96.jpg)\n\n\n![(e) Car-racing](images/e4480bc594e5da3789afa062459b29e6799c34b17783a731b5fa02d31dbc7312.jpg)\n\n\n![(f) Demo.](images/ba1b02c1af0f6fb677fb599fac8bb77b651238f09e7a0747010df0c7c0c7d459.jpg)\n\n\nFigure 2: (a) Gather task: the agent is rewarded for gathering green apples, but is constrained to collect a limited number of red apples (Achiam et al., 2017). (b) Circle task: the agent is rewarded for moving in a specified wide circle, but is constrained to stay within a safe region smaller than the radius of the circle (Achiam et al., 2017). (c) Grid task: the agent controls the traffic lights in a grid road network and is rewarded for high throughput, but is constrained to let lights stay red for at most 7 consecutive seconds (Vinitsky et al., 2018). (d) Bottleneck task: the agent controls a set of autonomous vehicles (shown in red) in a traffic merge situation and is rewarded for achieving high throughput, but constrained to ensure that human-driven vehicles (shown in white) have low speed for no more than 10 seconds (Vinitsky et al., 2018). (e) Car-racing task: the agent controls an autonomous vehicle on a race track and is rewarded for driving through as many tiles as possible, but is constrained to use the brakes at most 5 times to encourage a smooth ride (Brockman et al., 2016). (f) A human player plays car-racing with demonstration data logged.\n\n\n# 6 EXPERIMENTS\n\n\nTasks. We compare the proposed algorithm with existing approaches on five control tasks: three tasks with safety constraints ((a), (b) and (e) in Fig. 2), and two tasks with fairness constraints ((c) and (d) in Fig. 2). These tasks are briefly described in the caption of Fig. 2. We chose the traffic management tasks since a good control policy can benefit millions of drivers. In addition, we chose the car-racing task since a good algorithm should safely learn from baseline human policies. For all the algorithms, we use neural networks to represent Gaussian policies. We use the KL-divergence projection in the Mujoco and car-racing tasks, and the 2-norm projection in the traffic management task since it achieves better performance. We use a grid-search to select for the hyper-parameters. See Appendix for more experimental details.\n\n\nBaseline algorithms. We compare SPACE with five baseline methods.\n\n\n(1) Fixed-point Constrained Policy Optimization (f-CPO). In f-CPO, the weight  $\\lambda$  is fixed followed by a CPO update (Achiam et al., 2017). The f-CPO policy update solves:\n\n\n$$\n\n$$\n\\boldsymbol {\\theta} ^ {k + 1} = \\underset {\\boldsymbol {\\theta}} {\\arg \\max } (\\boldsymbol {g} ^ {k} + \\lambda \\boldsymbol {a} ^ {k}) ^ {T} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}) \\text {s . t .} \\frac {1}{2} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}) ^ {T} \\boldsymbol {F} ^ {k} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}) \\leq \\delta , \\boldsymbol {c} ^ {k T} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}) + d ^ {k} \\leq 0.\n$$\n\n$$\n\n\nf-CPO adds the divergence objective in the reward function. The lack of weight-adjustment makes it susceptible to suboptimal  $\\pi_B$ .\n\n\n(2) Fixed-point PCPO (f-PCPO). In f-PCPO, the weight  $\\lambda$  is fixed followed by a PCPO update (Yang et al., 2020). The f-PCPO policy update solves:\n\n\n$$\n\n$$\n\\boldsymbol {\\theta} ^ {k + \\frac {1}{2}} = \\underset {\\boldsymbol {\\theta}} {\\arg \\max } \\left(\\boldsymbol {g} ^ {k} + \\lambda \\boldsymbol {a} ^ {k}\\right) ^ {T} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}) \\quad \\text {s . t .} \\frac {1}{2} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}) ^ {T} \\boldsymbol {F} ^ {k} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}) \\leq \\delta ,\n$$\n\n$$\n\n\n$$\n\n$$\n\\boldsymbol {\\theta} ^ {k + 1} = \\underset {\\boldsymbol {\\theta}} {\\arg \\min } \\frac {1}{2} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k + \\frac {1}{2}}) ^ {T} \\boldsymbol {L} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k + \\frac {1}{2}}) \\quad \\text {s . t .} \\boldsymbol {c} ^ {k T} (\\boldsymbol {\\theta} - \\boldsymbol {\\theta} ^ {k}) + d ^ {k} \\leq 0.\n$$\n\n$$\n\n\nf-PCPO also adds the divergence objective in the reward function, but followed by a PCPO update\n\n- (3) Dynamic-point Constrained Policy Optimization (d-CPO). The d-CPO update solves f-CPO problem with a stateful  $\\lambda^{k + 1} = (\\lambda^k)^\\beta$ , where  $0 < \\beta < 1$ . This is inspired by Rajeswaran et al. (2017), in which they have the same weight-scheduling method to adjust  $\\lambda^k$ .\n\n- (4) Dynamic-point PCPO (d-PCPO). The d-PCPO update solves f-PCPO problem with a stateful  $\\lambda^{k + 1} = (\\lambda^k)^\\beta$ , where  $0 < \\beta < 1$ . d-PCPO also uses the same method to learn from  $\\pi_B$  as in d-CPO. For all the experiments and the algorithms, the weight is fixed and it is set to 1.\n\n\nBaseline policies  $\\pi_B$ . To test whether SPACE can safely and efficiently leverage the baseline policy, we used three variants of the baseline policies: (1) suboptimal  $\\pi_B^{\\mathrm{cost}}$  with  $J_C(\\pi_B^{\\mathrm{cost}}) \\approx 0$ , (2) suboptimal  $\\pi_B^{\\mathrm{reward}}$  with  $J_C(\\pi_B^{\\mathrm{reward}}) > h_C$ , and (3)  $\\pi_B^{\\mathrm{near}}$  with  $J_C(\\pi_B^{\\mathrm{near}}) \\approx h_C$  (i.e., the baseline policy has the same cost constraint as the agent). Note that these  $\\pi_B$  have different degrees of constraint satisfaction. This is to examine (1) whether SPACE can achieve better learning efficiency given the constraint-satisfying  $\\pi_B$  (i.e.,  $\\pi_B^{\\mathrm{cost}}$  and  $\\pi_B^{\\mathrm{near}}$ ), and (2) whether SPACE can safely learn from constraint-violating  $\\pi_B$  (i.e.,  $\\pi_B^{\\mathrm{reward}}$ ). In addition, in the car-racing task we pre-train a  $\\pi_B$  using an off-policy algorithm (DDPG (Lillicrap et al., 2016)), which directly learns from human demonstration data (Fig. 2(f)). This suboptimal human baseline policy is denoted by  $\\pi_B^{\\mathrm{human}}$ . This is\n\n\n![](images/f24391079a02c4bd654562f01f4b01c3f3950c24734440dc47bc99d4c8c66db2.jpg)\n\n\n![](images/ff014a048d05ed4cf68340bde1f07efc312fdaaa037920ee532605ce41be7e81.jpg)\n\n\n![](images/79f71ec4c28b9dffc45f9cc3259c1d12a4cf4be9b1f0276650f02466578d5956.jpg)\n\n\n![](images/6ba53928e5f0e3069d900d184ccf63fa59c42bcda5c5cc61648773cd6e7871a8.jpg)\n\n\n![](images/513673ac15785cb76bb97773ebf367f34c955169263fe01f1e3155fe5fde80d1.jpg)\n\n\n![](images/ec2892c87fd03de131027a84a26e214b7ac5fccf8ee179c125d0679c471a9898.jpg)\n\n\n![Figure 3: The discounted reward, and the undiscounted constraint cost, the undiscounted divergence cost over policy updates for the tested algorithms and tasks. The solid line is the mean and the shaded area is the standard deviation over 5 runs. The baseline policies in the grid and bottleneck tasks are  $\\pi_B^{\\mathrm{near}}$ , and the baseline policy in the car-racing task is  $\\pi_B^{\\mathrm{human}}$ . The black dashed line is the cost constraint threshold  $h_C$ . Overall, we observe that SPACE is the only algorithm that satisfies the constraints while achieving superior reward performance in all cases. Although  $\\pi_B^{\\mathrm{human}}$  has substantially low reward, SPACE still can learn to improve the reward. (We only show the results in these tasks as representative cases since these tasks are more challenging. Please read Appendix for more results. Best viewed in color.)](images/ad3342db5638cc2a0adffdfd467faffee6ee8390a3135863491af3e9ff0e9034.jpg)\n\n\n![](images/b2907cc841b991c21b8cda5a86b17406ea826233048c4352cc47087ebba0ef85.jpg)\n\n\n![](images/a6900bd0abbbdcf25d29d2e2692177af7016514da7f87179ffecc6c347050b16.jpg)\n\n\nto simulate that  $\\pi_B$  comes from a heuristic or human demonstration data, which are far from optimal. The detailed implementation of updating  $h_D$  can be found in Appendix E.1.\n\n\nOverall performance. The learning curves of the discounted reward  $(J_{R}(\\pi))$ , the undiscounted constraint cost  $(J_{C}(\\pi))$ , and the undiscounted divergence cost  $(J_{D}(\\pi))$  over policy updates are shown for all tested algorithms and tasks in Fig. 3. We use  $\\pi_{B}^{\\mathrm{near}}$  in bottleneck and grid tasks, and  $\\pi_{B}^{\\mathrm{human}}$  in car-racing task. Note that  $\\pi_{B}^{\\mathrm{human}}$  is highly suboptimal to the agent (i.e.,  $J_{R}(\\pi_{B}^{\\mathrm{human}})$  is small). The value of the reward is only around 5 as shown in the plot. It does not solve the task at hand. Overall, we observe that (1) SPACE achieves at least 2 times faster cost constraint satisfaction in all cases even learning from  $\\pi_{B}^{\\mathrm{human}}$ . (2) SPACE achieves at least  $10\\%$  more reward in the bottleneck and car-racing tasks compared to the best baseline, and (3) SPACE is the only algorithm that satisfies the cost constraints in all cases. In contrast, PCPO agent cannot robustly satisfy the cost constraints. This is because using the baseline policy can provide additional information about the safety constraints. For example, when using  $\\pi_{B}^{\\mathrm{cost}}$  that is only optimized for the safety constraints specified by the environment, we can observe the agent trained with SPACE satisfies the constraints more. This shows that using the baseline policy could also help to improve the cost performance.\n\n\nFor example, in the car-racing task we observe that  $J_{D}(\\pi)$  in SPACE decreases at the initial iteration, but increases in the end. This implies that the learned policy is guided by the baseline policy  $\\pi_B^{\\mathrm{human}}$  in the beginning, but use less supervision in the end. In addition, in the grid task we observe that the final reward of SPACE is lower than the baseline algorithm. This is because that SPACE converges to a policy in the cost constraint set, whereas the baseline algorithms do not find constraint-satisfying policies. These observations show that SPACE can robustly ensure constraint satisfaction while achieving fast learning aided by the potentially suboptimal baseline policy.\n\n\nf-CPO and f-PCPO. f-CPO and f-PCPO fail to improve the reward and have more cost violations. Most likely this is due to persistent supervision from the baseline policy which need not satisfy the cost constraints nor have high reward. For example, in the car-racing task we observe that the value\n\n\nof the divergence cost decreases throughout the training. This implies that the learned policy overly evolves to the sub-optimal baseline policy and hence degrades the reward performance.\n\n\nd-CPO and d-PCPO. d-CPO and d-PCPO improve the reward slowly and have more cost violations. They do not use projection to quickly learn from  $\\pi_B$ . For example, in the car-racing task  $J_{D}(\\pi)$  in d-CPO and d-PCPO are high compared to SPACE throughout the training. This suggests that simply adding an IL objective in the reward objective is susceptible to a suboptimal  $\\pi_B$ . Hence these approaches require a good selection of the initial value of  $\\lambda^k$  and  $\\beta$ .\n\n\nSuboptimal  $\\pi_B^{\\mathrm{cost}}$  and  $\\pi_B^{\\mathrm{reward}}$ . The learning curves of  $J_{C}(\\pi)$  and the  $J_{R}(\\pi)$  over policy updates are shown for the point gather task in Fig. 4. We use two suboptimal baseline policies:  $\\pi_B^{\\mathrm{cost}}$  and  $\\pi_B^{\\mathrm{reward}}$ , and  $h_C$  is set to 0.5. They do not solve the task at hand.\n\n\nWe observe that SPACE robustly satisfies the cost constraints in all cases even when learning from  $\\pi_B^{\\mathrm{reward}}$ . In addition, we observe that learning guided by  $\\pi_B^{\\mathrm{reward}}$  achieves faster reward learning efficiency at the initial iteration. This is because  $J_{R}(\\pi_{B}^{\\mathrm{reward}}) > J_{R}(\\pi_{B}^{\\mathrm{cost}})$  as seen in the reward plot. Furthermore, we observe that learning guided by  $\\pi_B^{\\mathrm{cost}}$  achieves faster reward learning efficiency at the later iteration. This is because that by starting in the interior of the cost constraint set (i.e.,  $J_{C}(\\pi_{B}^{\\mathrm{cost}})\\approx 0\\leq h_{C}$ ), the agent can safely exploit the baseline policy. The results show that SPACE enables fast\n\n\n![Figure 4: Learning from suboptimal  $\\pi_B$ . The undiscounted constraint cost and the discounted reward over policy updates for the point gather task. The solid line is the mean and the shaded area is the standard deviation over 5 runs. The black dashed line is the cost constraint threshold  $h_C$ . We observe that SPACE satisfies the cost constraints even when learning from the suboptimal  $\\pi_B$ . (Best viewed in color.)](images/b06cc0bcdcb43b4c738f4673eae02e21d74fd1358d73d9927b8d03abba980092.jpg)\n\n\n![](images/5738b8773b85d768dd349181c96dde0c036e9582fcc26cb8a42f83b92c46811c.jpg)\n\n\nconvergence to a constraint-satisfying policy, even if  $\\pi_B$  does not meet the constraints itself or does not optimize the reward.\n\n\n# 7 CONCLUSION\n\n\nWe address the problem of learning constraint-satisfying policies given a baseline policy from either a teacher agent or demonstration data. The proposed algorithm effectively learns from the potentially suboptimal baseline policy without violating the constraints. SPACE achieves superior reward and cost performance compared with state-of-the-art approaches (i.e., PCPO). We further analyze the convergence of SPACE and provide an effective approach to controlling the distance between the learned and baseline policies. Future work will consider learning system dynamics to account for the uncertainty of the environment and hence enable safe learning in real applications.\n\n\n# REFERENCES\n\n- Pieter Abbeel, Adam Coates, and Andrew Y. Ng. Autonomous helicopter aerobatics through apprenticeship learning. The International Journal of Robotics Research, 29(13):1608-1639, 2010.\n\n- David Abel, John Salvatier, Andreas Stuhlmuller, and Owain Evans. Agent-agnostic human-in-the-loop reinforcement learning. arXiv preprint arXiv:1701.04079, 2017.\n\n- Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In Proceedings of the International Conference on Machine Learning, pp. 22-31, 2017.\n\n- Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.\n\n- Guy Avni, Roderick Bloem, Krishnendu Chatterjee, Thomas A. Henzinger, Bettina K\u00f6nighofer, and Stefan Pranger. Run-time optimization for learned controllers through quantitative games. In International Conference on Computer Aided Verification, pp. 630-649. Springer, 2019.\n\n- Ashwin Balakrishna, Brijen Thananjeyan, Jonathan Lee, Arsh Zahed, Felix Li, Joseph E. Gonzalez, and Ken Goldberg. On-policy robot imitation learning from a converging supervisor. In Proceedings of the Conference on Robot Learning, 2019.\n\n- Ezio Bartocci, Radu Grosu, Panagiotis Katsaros, CR Ramakrishnan, and Scott A. Smolka. Model repair for probabilistic systems. In International Conference on Tools and Algorithms for the Construction and Analysis of Systems, pp. 326-340. Springer, 2011.\n\n- Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n\n- Daniel S. Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations. In Proceedings of the International Conference on Machine Learning, pp. 783-792, 2019.\n\n- Gong Chen and Marc Teboulle. Convergence analysis of a proximal-like minimization algorithm using bregman functions. SIAM Journal on Optimization, 3(3):538-543, 1993.\n\n- Sonia Chernova and Andrea L. Thomaz. Robot learning from human teachers. Synthesis Lectures on Artificial Intelligence and Machine Learning, 8(3):1-121, 2014.\n\n- Yinlam Chow, Ofir Nachum, Aleksandra Faust, Mohammad Ghavamzadeh, and Edgar Duenez-Guzman. Lyapunov-based safe policy optimization for continuous control. arXiv preprint arXiv:1901.10031, 2019.\n\n- Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.\n\n- Kurt Driessens and Sa\u0161o D\u017eeroski. Integrating guidance into relational reinforcement learning. Machine Learning, 57(3):271-304, 2004.\n\n- Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In Proceedings of the International Conference on Machine Learning, pp. 1329-1338, 2016.\n\n- Michael Fu and A. Prashanth L. Risk-sensitive reinforcement learning: A constrained optimization viewpoint. arXiv preprint arXiv:1810.09126, 2018.\n\n- Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch deep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019.\n\n- Yang Gao, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. Reinforcement learning from imperfect demonstrations. In Proceedings of the International Conference on Machine Learning, 2018.\n\n- Javier Garcia and Fernando Fern\u00e1ndez. Safe exploration of state and action spaces in reinforcement learning. Journal of Artificial Intelligence Research, 45:515-564, 2012.\n\n- Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437-1480, 2015.\n\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672-2680, 2014.\n\n- Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. Cautious reinforcement learning with logical constraints. arXiv preprint arXiv:2002.12156, 2020.\n\n- Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.\n\n- Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565-4573, 2016.\n\n- Nils Jansen, Bettina K\u00f6nighofer, Sebastian Junges, Alex Serban, and Roderick Bloem. Safe Reinforcement Learning Using Probabilistic Shields (Invited Paper). In Igor Konnov and Laura Kov\u00e1cs (eds.), 31st International Conference on Concurrency Theory (CONCUR 2020), volume 171 of Leibniz International Proceedings in Informatics (LIPics), pp. 3:1-3:16, Dagstuhl, Germany, 2020. Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik. ISBN 978-3-95977-160-3. doi: 10.4230/LIPics.CONCUR.2020.3. URL https://drops.dagstuhl.de/opus/volltexte/2020/12815.\n\n- Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.\n\n- Sebastian Junges, Nils Jansen, Christian Dehnert, Ufuk Topcu, and Joost-Pieter Katoen. Safety-constrained reinforcement learning for mdps. In International Conference on Tools and Algorithms for the Construction and Analysis of Systems, pp. 130-146. Springer, 2016.\n\n- Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the International Conference on Machine Learning, pp. 267-274, 2002.\n\n- Rogier Koppejan and Shimon Whiteson. Neuroevolutionary reinforcement learning for generalized control of simulated helicopters. Evolutionary intelligence, 4(4):219-241, 2011.\n\n- Minae Kwon, Erdem Biyik, Aditi Talati, Karan Bhasin, Dylan P. Losey, and Dorsa Sadigh. When humans aren't optimal: Robots that collaborate with risk-aware humans. In Proceedings of ACM/IEEE International Conference on Human-Robot Interaction, 2020.\n\n- Hoang M. Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Proceedings of the International Conference on Machine Learning, pp. 3703-3712, 2019.\n\n- Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.\n\n- Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Proceedings of the International Conference on Learning Representations, 2016.\n\n- Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\n- Aryan Mokhtari, Asuman Ozdaglar, and Ali Jabbabaie. Escaping saddle points in constrained optimization. In Advances in Neural Information Processing Systems, pp. 3629-3639, 2018.\n\n- Katharina Mulling, Jens Kober, Oliver Kroemer, and Jan Peters. Learning to select and generalize striking movements in robot table tennis. The International Journal of Robotics Research, 32(3): 263-279, 2013.\n\n- Shashank Pathak, Erika \u00c1br\u00e1ham, Nils Jansen, Armando Tacchella, and Joost-Pieter Katoen. A greedy approach for the efficient repair of stochastic models. In NASA Formal Methods Symposium, pp. 295-309. Springer, 2015.\n\n- Pablo Quint\u00eda Vidal, Roberto Iglesias Rodr\u00edguez, Miguel \u00c1ngel Rodr\u00edguez Gonz\u00e1lez, and Carlos V\u00e1zquez Regueiro. Learning on real robots from experience and simple user feedback. 2013.\n\n- Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. In Proceedings of Robotics: Science and Systems, 2017.\n\n- St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the International Conference on Artificial Intelligence and Statistics, pp. 627-635, 2011.\n\n- John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the International Conference on Machine Learning, pp. 1889-1897, 2015.\n\n- John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. In Proceedings of the International Conference on Learning Representations, 2016.\n\n- David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016.\n\n- William D. Smart and Leslie Pack Kaelbling. Practical reinforcement learning in continuous spaces. In Proceedings of the International Conference on Machine Learning, pp. 903-910. CiteSeer, 2000.\n\n- Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by pid lagrangian methods. arXiv preprint arXiv:2007.03964, 2020.\n\n- Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause. Safe exploration for optimization with gaussian processes. In International Conference on Machine Learning, pp. 997-1005. PMLR, 2015.\n\n- Wen Sun, Geoffrey J. Gordon, Byron Boots, and Andrew J. Bagnell. Dual policy iteration. In Advances in Neural Information Processing Systems, pp. 7059-7069, 2018.\n\n- Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. In Proceedings of the International Conference on Learning Representations, 2018.\n\n- Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration in finite markov decision processes with gaussian processes. In Advances in Neural Information Processing Systems, pp. 4312-4320, 2016.\n\n- Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth\u00f6rl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017.\n\n- Eugene Vinitsky, Aboudy Kreidieh, Luc Le Flem, Nishant Kheterpal, Kathy Jang, Cathy Wu, Fangyu Wu, Richard Liaw, Eric Liang, and Alexandre M. Bayen. Benchmarks for reinforcement learning in mixed-autonomy traffic. In Proceedings of Conference on Robot Learning, pp. 399-409, 2018.\n\n- Akifumi Wachi and Yanan Sui. Safe reinforcement learning in constrained markov decision processes. arXiv preprint arXiv:2008.06626, 2020.\n\n- Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Imitation learning from imperfect demonstration. In Proceedings of the International Conference on Machine Learning, pp. 6818-6827, 2019.\n\n- Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J. Ramadge. Projection-based constrained policy optimization. In Proceedings of the International Conference on Learning Representations, 2020.\n\n- Ruohan Zhang, Faraz Torabi, Lin Guan, Dana H. Ballard, and Peter Stone. Leveraging human guidance for deep reinforcement learning tasks. In Proceedings of the International Joint Conference on Artificial Intelligence, pp. 6339-6346, 2019.\n\n- Liyuan Zheng and Lillian J. Ratliff. Constrained upper confidence reinforcement learning. arXiv preprint arXiv:2001.09377, 2020.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638565.2714267", "msecs": "271.0", "relativeCreated": "162101.9241809845", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Hyperparameter Transfer Across Developer Adjustments\n\n# Abstract\nAfter developer adjustments to a machine learning (ML) algorithm, how can the results of an old hyperparameter optimization (HPO) automatically be used to speedup a new HPO? This question poses a challenging problem, as developer adjustments can change which hyperparameter settings perform well, or even the hyperparameter search space itself. While many approaches exist that leverage knowledge obtained on previous tasks, so far, knowledge from previous development steps remains entirely untapped. In this work, we remedy this situation and propose a new research framework: hyperparameter transfer across adjustments (HT-AA). To lay a solid foundation for this research framework, we provide four simple HT-AA baseline algorithms and eight benchmarks changing various aspects of ML algorithms, their hyperparameter search spaces, and the neural architectures used. The best baseline, on average and depending on the budgets for the old and new HPO, reaches a given performance 1.2-3.6x faster than a prominent HPO algorithm without transfer. As HPO is a crucial step in ML development but requires extensive computational resources, this speedup would lead to faster development cycles, lower costs, and reduced environmental impacts. To make these benefits available to ML developers off-the-shelf and to facilitate future research on HT-AA, we provide python packages for our baselines and benchmarks.\n\n\n![](images/0b383a6c0235da4a9c1e71b8cf8966c07396ac0b8059214d65589a0fc3cd12e0.jpg)\n\n\nGraphical Abstract: Hyperparameter optimization (HPO) across adjustments to the algorithm or hyperparameter search space. A common practice is to perform HPO from scratch after each adjustment or to somehow manually transfer knowledge. In contrast, we propose a new research framework about automatic knowledge transfers across adjustments for HPO.\n\n\n# 1 INTRODUCTION: A NEW HYPERPARAMETER TRANSFER FRAMEWORK\n\n\nThe machine learning (ML) community arrived at the current generation of ML algorithms by performing many iterative adjustments. Likely, the way to artificial general intelligence requires many more adjustments. Each algorithm adjustment could change which settings of the algorithm's hyperparameters perform well, or even the hyperparameter search space itself (Chen et al., 2018; Li et al., 2020). For example, when deep learning developers change the optimizer, the learning rate's optimal value\n\n\nlikely changes, and the new optimizer may also introduce new hyperparameters. Since ML algorithms are known to be very sensitive to their hyperparameters (Chen et al., 2018; Feurer & Hutter, 2019), developers are faced with the question of how to adjust their hyperparameters after changing their code. Assuming that the developers have results of one or several hyperparameter optimizations (HPOs) that were performed before the adjustments, they have two options:\n\n\n1. Somehow manually transfer knowledge from old HPOs.\n\n\nThis is the option chosen by many researchers and developers, explicitly disclosed, e.g., in the seminal work on AlphaGo (Chen et al., 2018). However, this is not a satisfying option since manual decision making is time-consuming, often individually designed, and has already lead to reproducibility problems (Musgrave et al., 2020).\n\n\n2. Start the new HPO from scratch.\n\n\nLeaving previous knowledge unutilized can lead to higher computational demands and worse performance (demonstrated empirically in Section 5). This is especially bad as the energy consumption of ML algorithms is already recognized as an environmental problem. For example, deep learning pipelines can have  $\\mathrm{CO}_{2}$  emissions on the order of magnitude of the emissions of multiple cars for a lifetime (Strubell et al., 2019), and their energy demands are growing furiously: Schwartz et al. (2019) cite a \"300,000x increase from 2012 to 2018\". Therefore, reducing the number of evaluated hyperparameter settings should be a general goal of the community.\n\n\nThe main contribution of this work is the introduction of a new research framework: Hyperparameter transfer across adjustments (HT-AA), which empowers developers with a third option:\n\n\n3. Automatically transfer knowledge from previous HPOs.\n\n\nThis option leads to advantages in two aspects: The automation of decision making and the utilization of previous knowledge. On the one hand, the automation allows to benchmark strategies, replaces expensive manual decision making, and enables reproducible and comparable experiments; on the other hand, utilizing previous knowledge leads to faster development cycles, lower costs, and reduced environmental impacts.\n\n\nTo lay a solid foundation for the new HT-AA framework, our individual contributions are as follows:\n\n- - We formally introduce a basic version of the HT-AA problem (Section 2).\n\n- - We provide four simple baseline algorithms for our basic HT-AA problem (Section 3).\n\n- - We provide a comprehensive set of eight novel benchmarks for our basic HT-AA problem (Section 4).\n\n- - We show the advantage of transferring across developer adjustments: some of our simple baseline algorithms outperform HPO from scratch up to 1.2\u20133.6x on average depending on the budgets (Section 5).\n\n- - We empirically demonstrate the need for well-vetted algorithms for HT-AA: two baselines modelled after actually-practiced manual strategies perform horribly on our benchmarks (Section 5).\n\n- - We relate the HT-AA framework to existing research efforts and discuss the research opportunities it opens up (Section 6).\n\n- - To facilitate future research on HT-AA, we provide open-source code for our experiments and benchmarks and provide a python package with an out-of-the-box usable implementation of our HT-AA algorithms.\n\n\n# 2 HYPERPARAMETER TRANSFER ACROSS ADJUSTMENTS\n\n\nAfter presenting a broad introduction to the topic, we now provide a detailed description of hyperparameter transfer across developer adjustments (HT-AA). We first introduce hyperparameter optimization, then discuss the types of developer adjustments, and finally describe the transfer across these adjustments.\n\n\n![Figure 1: Developer adjustments from the perspective of hyperparameter optimization.](images/2f9edf255f9f619c029dff76a9f49f67e722106c60a42316f03bcc64cc2166b4.jpg)\n\n\n![](images/d97b4d6e4bef8aeeb02b9019f27778e1c3f253b8eee5f22c80a8e2da0a91bb92.jpg)\n\n\n![](images/648f5714defe24ebcd50974be92ea93d349c81d3c70daca64b912aa00f18b26b.jpg)\n\n\nHyperparameter optimization (HPO) The HPO formulation we utilize in this work is as follows:\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {x} \\in \\mathcal {X}} {\\text {m i n i m i z e}} f _ {\\mathcal {A}} (\\boldsymbol {x}) \\quad \\text {w i t h} b \\tag {1}\n$$\n\n$$\n\n\nwhere  $f_{\\mathcal{A}}(\\pmb{x})$  is the objective function for ML algorithm  $\\mathcal{A}$  with hyperparameter setting  $\\pmb{x}, b$  is the number of available evaluations, and  $\\mathcal{X}$  is the search space. We allow the search space  $\\mathcal{X}$  to contain categorical and numerical dimensions alike and consider only sequential evaluations. We refer to a specific HPO problem with the 3-tuple  $(\\mathcal{X}, f_{\\mathcal{A}}, b)$ . For a discussion on potential extensions of our framework to different HPO formulations, we refer the reader to Section 6.\n\n\nDeveloper adjustments We now put developer adjustments on concrete terms and introduce a taxonomy of developer adjustments. We consider two main categories of developer adjustments: ones that do not change the search space  $\\mathcal{X}$  (homogeneous adjustments) and ones that do (heterogenous adjustments). Homogeneous adjustments could either change the algorithm's implementation or the hardware that the algorithm is run on. Heterogeneous adjustments can be further categorized into adjustments that add or remove a hyperparameter (hyperparameter adjustments) and adjustments that change the search space for a specific hyperparameter (range adjustments). Figure 1 shows an illustration of the adjustment types.\n\n\nKnowledge transfer across adjustments In general, a continuous stream of developer adjustments could be accompanied by multiple HPOs. We simplify the problem in this fundamental work and only consider the transfer between two HPO problems; we discuss a potential extension in Section 6. The two HPO problems arise from adjustments  $\\Psi$  to a ML algorithm  $\\mathcal{A}_{\\mathrm{old}}$  and its search space  $\\mathcal{X}_{\\mathrm{old}}$ , which lead to  $\\mathcal{A}_{\\mathrm{new}}, \\mathcal{X}_{\\mathrm{new}} \\coloneqq \\Psi(\\mathcal{A}_{\\mathrm{old}}, \\mathcal{X}_{\\mathrm{old}})$ . Specifically, the hyperparameter transfer across adjustments problem is to solve the HPO problem ( $\\mathcal{X}_{\\mathrm{new}}, f_{\\mathcal{A}_{\\mathrm{new}}}, b_{\\mathrm{new}}$ ), given the results for ( $\\mathcal{X}_{\\mathrm{old}}, f_{\\mathcal{A}_{\\mathrm{old}}}, b_{\\mathrm{old}}$ ). Compared to HPO from scratch, developers can choose a lower budget  $b_{\\mathrm{new}}$ , given evidence for a transfer algorithm achieving the same performance faster.\n\n\nRelation to hyperparameter transfer across tasks (HT-AT) There exists an extensive research field that studies the transfer across tasks for HPOs (Vanschoren, 2018). The main difference to hyperparameter transfer across adjustments is that HT-AT assumes that there are no changes to hardware, ML algorithm, and search space, whereas HT-AA is characterized by these changes. Another difference is that most work on HT-AT considers large amounts of meta-data (up to more than a thousand previous HPO runs and function evaluations (Wang et al., 2018; Metz et al., 2020)), whereas in the basic HT-AA problem described above, only one previous HPO run is available. Compared to HT-AT approaches, the main technical challenge for HT-AA approaches are heterogeneous developer adjustments, as homogeneous HT-AA problems, where none of the adjustments changes the search space, are syntactically equivalent to HT-AT problems with only one prior HPO run. For this homogeneous HT-AA, existing approaches for HT-AT could, in principle, be applied without modification; this includes, for example the transfer acquisition function (Wistuba et al., 2018), multi-task bayesian optimization (Swersky et al., 2013), multi-task adaptive bayesian linear regression (Perrone et al., 2018), ranking-weighted gaussian process ensemble (Feurer et al., 2018), and difference-modelling bayesian optimisation (Shilton et al., 2017). However, the low amount of meta-data in (homogeneous) HT-AA poses additional challenges.\n\n\n# 3 BASELINE ALGORITHMS FOR HT-AA\n\n\nIn this section we present four baselines for the specific instantiation of the hyperparameter transfer across adjustments (HT-AA) framework discussed in Section 2. We resist the temptation to introduce complex approaches alongside a new research framework and instead focus on a solid foundation. Specifically, we focus on approaches that do not use any knowledge from the new HPO for the transfer and include two baselines that are modelled after actually-practiced manual strategies. We first introduce the two basic HPO algorithm that the transfer approaches build upon, then introduce two decompositions of HPO search spaces across adjustments, and finally, we present the four baselines themselves.\n\n\n# 3.1 BACKGROUND: HYPERPARAMETER OPTIMIZATION ALGORITHMS\n\n\nWe instantiate our transfer algorithms with two different basic hyperparameter optimization algorithms, and also evaluate against these two algorithms (Section 5). Both algorithms are based on Bayesian optimization (BO): Given observations  $\\mathcal{D} = \\{(x_k, f_A(x_k))\\}_{k=1}^n$ , BO fits a probabilistic model  $p(f_A \\mid \\mathcal{D})$  for the objective function  $f_A$ ; then, to decide which hyperparameter setting to evaluate next, BO maximizes an acquisition function  $\\alpha(\\boldsymbol{x} \\mid \\mathcal{D})$ , which uses this probabilistic model. For both approaches we follow standard practice and sample configurations randomly in a fraction of cases (here  $1/3$ ). For a recent review of BO we refer to Shahriari et al. (2016). In the following, we provide details for the two basic HPO algorithms which we use.\n\n\nTree-Structured Parzen Estimator One of the two algorithms that we use is the Tree-Structured Parzen Estimator (TPE) (Bergstra et al., 2011), which is the default algorithm in the popular HyperOpt package (Bergstra et al., 2013). TPE uses kernel density estimators to model the densities  $l(\\pmb{x})$  and  $g(\\pmb{x})$ , for the probability of a given hyperparameter configuration  $\\pmb{x}$  being worse ( $l$ ), or better ( $g$ ), than the best already evaluated configuration. To decide which configuration to evaluate, TPE then maximizes the acquisition function  $\\pmb{x}^* \\in \\arg \\max_{\\pmb{x} \\in \\mathcal{X}} g(\\pmb{x}) / b(\\pmb{x})$  approximately. In our experiments, we use the TPE implementation and hyperparameter settings from Falkner et al. (2018), which requires  $2(\\dim(\\mathcal{X}_{\\mathrm{new}}) + 1)$  evaluations to build a model.\n\n\nBayesian optimization with Gaussian processes Bayesian optimization with Gaussian processes uses Gaussian process regression (Rasmussen & Williams, 2006) to fit a probabilistic model  $p(f_{\\mathcal{A}} \\mid \\mathcal{D})$ . To generate a sample, we optimize the widely used expected improvement (Jones et al., 1998) acquisition function. If there are less than  $\\dim(\\mathcal{X})$  evaluations, we sample randomly. In our experiments, we use the implementation from the SMAC3 package (Lindauer et al., 2017) with default kernels, hyperparameters, and optimization procedures. In the following we will refer to BO with Gaussian processes with GP.\n\n\n# 3.2 PRELIMINARIES: SEARCH SPACE DECOMPOSITIONS\n\n\nHyperparameter adjustments For hyperparameter adjustments the new search space  $\\mathcal{X}_{\\mathrm{new}}$  and the old search space  $\\mathcal{X}_{\\mathrm{old}}$  only differ in hyperparameters, not in hyperparameter ranges, so we can decompose the search spaces as  $\\mathcal{X}_{\\mathrm{new}} = \\mathcal{X}_{\\mathrm{only - new}}\\times \\mathcal{X}_{\\mathrm{both}}$  and  $\\mathcal{X}_{\\mathrm{old}} = \\mathcal{X}_{\\mathrm{both}}\\times \\mathcal{X}_{\\mathrm{only - old}}$ , where  $\\mathcal{X}_{\\mathrm{both}}$  is the part of the search space that remains unchanged across adjustments (see Figure 2 for reference). All baselines use this decomposition and project the hyperparameter settings that were evaluated in the old HPO from  $\\mathcal{X}_{\\mathrm{old}}$  to  $\\mathcal{X}_{\\mathrm{both}}$ .\n\n\nRange adjustments A range adjustment can remove values from the hyperparameter range or add values. For an adjustment of hyperparameter range  $\\mathcal{X}_{old}^{H_i}$  to  $\\mathcal{X}_{\\mathrm{new}}^{H_i}$  this can be expressed as  $\\mathcal{X}_{\\mathrm{new}}^{H_i} = \\mathcal{X}_{\\mathrm{both}}^{H_i} \\cup \\mathcal{X}_{\\mathrm{both,range-only-new}}^{H_i}$  with  $\\mathcal{X}_{\\mathrm{both}}^{H_i} = \\mathcal{X}_{old}^{H_i} \\setminus \\mathcal{X}_{\\mathrm{both,range-only-old}}^{H_i}$ .\n\n\n# 3.3 BASELINE ALGORITHMS\n\n\nOnly Optimize New Hyperparameters A common developer strategy to manually transfer across adjustments is to set previous hyperparameters in  $\\mathcal{X}_{\\mathrm{both}}$  to the best setting of the previous HPO and only optimize hyperparameters in  $\\mathcal{X}_{\\mathrm{only-new}}$  (Agostinelli et al., 2014; Huang et al., 2017; Wu & He, 2018).\n\n\n![Figure 2: Example search space decomposition for a hyperparameter addition and removal.](images/ea5c832688a7f9bc66bfd5831bbd2d7c7e80932d9ed86ed9d7b70a4ae4a7345e.jpg)\n\n\nIf the previous best setting is not valid anymore, i.e., it has values in  $\\mathcal{X}_{\\mathrm{both},\\mathrm{range - only - old}}^{H_i}$  for a hyperparameter  $H_{i}$  still in  $\\mathcal{X}_{\\mathrm{both}}$ , this strategy uses the best setting that still is a valid configuration. In the following, we refer to this strategy as only-optimize-new.\n\n\nDrop Unimportant Hyperparameters A strategy inspired by manual HT-AA efforts is to only optimize important hyperparameters. The utilization of importance statistics was, for example, explicitly disclosed in the seminal work on AlphaGo (Chen et al., 2018). Here, we determine the importance of each individual hyperparameter with functional analysis of variance (fANOVA) (Hutter et al., 2014) and do not tune hyperparameters with below mean importance. Therefore, this strategy only optimizes hyperparameters in  $\\mathcal{X}_{\\mathrm{only - new}}$  and hyperparameters in  $\\mathcal{X}_{\\mathrm{both}}$  with above mean importance. In the following, we refer to this strategy as drop-unimportant.\n\n\nFirst Evaluate Best The best-first strategy uses only-optimize-new for the first evaluation, and uses standard GP for the remaining evaluations. This strategy has a large potential speedup and low risk as it falls back to standard GP.\n\n\nTransfer GP (TGP) / Transfer TPE (T2PE) The main idea of Transfer GP (TGP) / Transfer TPE (T2PE) is to fit a GP / TPE model on the observations of the previous HPO  $\\mathcal{D}_{\\mathrm{old}} = \\{(x_k, f_{\\mathcal{A}_{\\mathrm{old}}}(\\boldsymbol{x}_k))_{k=1}^{b_{\\mathrm{old}}}\\}$  for the part of the search space that remained unchanged (i.e.,  $\\mathcal{X}_{\\mathrm{both}}$ , for reference see Figure 2). To this end, TGP/T2PE first discards now invalid hyperparameter settings and projects the remaining settings  $\\boldsymbol{x}_i$  in  $\\mathcal{D}_{\\mathrm{old}}$  from  $\\mathcal{X}_{\\mathrm{old}}$  to  $\\mathcal{X}_{\\mathrm{both}}$  to yield  $\\mathcal{D}_{\\mathrm{old,both}}$ . Then, either a GP or TPE model  $M_{\\mathrm{both}}$  is fit on these transformed previous results  $\\mathcal{D}_{\\mathrm{old,both}}$ , and to generate a sample  $\\boldsymbol{x} \\in \\mathcal{X}_{\\mathrm{new}}$  for the new HPO, a sample from  $M_{\\mathrm{both}}$  over  $\\mathcal{X}_{\\mathrm{both}}$  is combined with a random sample over  $\\mathcal{X}_{\\mathrm{only,new}}$ . Further, as in the basic HPO algorithms, TGP/T2PE uses a random sample over  $\\mathcal{X}_{\\mathrm{new}}$  in  $\\frac{1}{3}$  of cases, and after  $\\dim(\\mathcal{X}_{\\mathrm{new}})$  (TGP) or  $2(\\dim(\\mathcal{X}_{\\mathrm{new}}) + 1)$  (T2PE) evaluations, TGP/T2PE uses a model fitted on the new observations. We provide pseudocode for TGP/T2PE in Algorithm 1.\n\n\n```\nInput: Current search space  $\\mathcal{X}_{\\mathrm{new}}$  , previous search space  $\\mathcal{X}_{\\mathrm{old}}$  , previous results  $\\mathcal{D}_{\\mathrm{old}}$  , budget  $b_{\\mathrm{new}}$    \n1: Decompose  $\\mathcal{X}_{\\mathrm{new}} = (\\mathcal{X}_{\\mathrm{both}}\\cup \\mathcal{X}_{\\mathrm{both,range - only - new}})\\times \\mathcal{X}_{\\mathrm{only - new}}$  See Section 3.2   \n2:Discard hyperparameter settings in  $\\mathcal{D}_{\\mathrm{old}}$  that have values in  $\\mathcal{X}_{\\mathrm{both,range - only - new}}$    \n3: Project configis in  $\\mathcal{D}_{\\mathrm{old}}$  to space  $\\mathcal{X}_{\\mathrm{both}}$  , to yield  $\\mathcal{D}_{\\mathrm{old,both}}$    \n4: Fit TPE or GP model  $M_{\\mathrm{both}}$  for  $\\mathcal{X}_{\\mathrm{both}}$  on  $\\mathcal{D}_{\\mathrm{old,both}}$    \n5: for  $t$  in 1,...,  $b_{\\mathrm{new}}$  do   \n6: if is random fraction then See Section 3.1   \n7: Sample  $\\mathbf{x}_{\\mathrm{new}}$  from prior on  $\\mathcal{X}_{\\mathrm{new}}$    \n8: else if no model for  $\\mathcal{X}_{\\mathrm{new}}$  then Differs for TPE and GP, see Section 3.1   \n9: Sample  $\\mathbf{x}_{\\mathrm{both}}$  from  $\\mathcal{X}_{\\mathrm{both}}$  using  $M_{\\mathrm{both}}$    \n10: Sample  $\\mathbf{x}_{\\mathrm{only - new}}$  from prior on  $\\mathcal{X}_{\\mathrm{only - new}}$    \n11: Combine  $\\mathbf{x}_{\\mathrm{both}}$  with  $\\mathbf{x}_{\\mathrm{only - new}}$  to yield sample  $\\mathbf{x}_{\\mathrm{new}}$    \n12: else   \n13: Fit TPE or GP model  $M_{\\mathrm{new}}$  for  $\\mathcal{X}_{\\mathrm{new}}$  on current observations   \n14: Sample  $\\mathbf{x}_{\\mathrm{new}}$  from  $\\mathcal{X}_{\\mathrm{new}}$  using  $M_{\\mathrm{new}}$\n```\n\n\n# 4 BENCHMARKS FOR HT-AA\n\n\nWe introduce eight novel benchmarks for the basic hyperparameter transfer across adjustments (HT-AA) problem discussed in Section 2. As is common in hyperparameter optimization research, we employ tabular and surrogate benchmarks to allow cheap and reproducible benchmarking (Perrone et al., 2018; Falkner et al., 2018). Tabular benchmarks achieve this with a lookup table for all possible hyperparameter settings. In contrast, surrogate benchmarks fit a surrogate model for the objective function (Eggensperger et al., 2014). We base our surrogate models and lookup tables on code and data from existing hyperparameter optimization (HPO) benchmarks (detailed in Appendix A) that, together, cover four different machine learning algorithms: a fully connected neural network (FCN), neural architecture search for a convolutional neural network (NAS), a support vector machine (SVM), and XGBoost (XGB). While the NAS and FCN benchmarks are based on lookup tables, the SVM and XGB based benchmarks use surrogate models. For each of these base benchmarks, we simulate two different types of developer adjustments (Table 1), to arrive at a total of eight benchmarks. Additionally, for each algorithm and adjustment, we consider multiple tasks in our benchmarks. Further, we provide a python package with all our benchmarks and refer the reader to Appendix A for additional details on the benchmarks.\n\n\nTable 1: Developer adjustments and types of adjustments in the benchmarks.\n\n\n<table><tr><td>Benchmark</td><td>Adjustments</td><td>Adjustment Type</td></tr><tr><td>FCN-A</td><td>Increase #units-per-layer 16\u00d7Double #epochsFix batch size hyperparameter</td><td>HomogeneousHomogeneousHeterogeneous</td></tr><tr><td>FCN-B</td><td>Add per-layer choice of activation functionChange learning rate schedule</td><td>HeterogeneousHomogeneous</td></tr><tr><td>NAS-A</td><td>Add 3x3 average pooling as choice of operation to each edge</td><td>Heterogeneous</td></tr><tr><td>NAS-B</td><td>Add node to cell template (adds 3 hyperparameters)</td><td>Heterogeneous</td></tr><tr><td>XGB-A</td><td>Expose four booster hyperparameters</td><td>Heterogeneous</td></tr><tr><td>XGB-B</td><td>Change four unexposed booster hyperparameter values</td><td>Homogeneous</td></tr><tr><td>SVM-A</td><td>Change kernelRemove hyperparameter for old kernelAdd hyperparameter for new kernel</td><td>HomogeneousHeterogeneousHeterogeneous</td></tr><tr><td>SVM-B</td><td>Increase range for cost hyperparameter</td><td>Heterogeneous</td></tr></table>\n\n\n# 5 EXPERIMENTS AND RESULTS\n\n\nIn this section, we empirically evaluate the four baseline algorithms presented in Section 3 as solutions for the hyperparameter transfer across adjustments problem. We first describe the evaluation protocol used through all studies and then present the results.\n\n\nEvaluation protocol We use the benchmarks introduced in Section 4 and focus on the speedup of transfer strategies over either Bayesian optimization with Gaussian processes (GP), or over Tree Parzen Estimator (TPE). Specifically, we measured how much faster a transfer algorithm reaches a given objective value compared to GP or TPE in terms of the number of evaluations. We repeated all measurements across 25 different random seeds and report results for validation objectives, as not all benchmarks provide test objectives, and to reduce noise in our evaluation. We terminate runs after 400 evaluations and report ratio of means. To aggregate these ratios across tasks and benchmarks, we use the geometric mean. We use the geometric mean, as, intuitively, two speedups of e.g.,  $0.1\\mathrm{x}$  and  $10\\mathrm{x}$  average to  $1\\mathrm{x}$ , and not  $5.05\\mathrm{x}$ . We want to note that the standard mean is an upper bound for the geometric mean, so using the geometric mean in fact makes the speedups slightly less impressive than had we used the standard arithmetic mean. To determine the target objective values, we measured GP's average performance for 10, 20, and 40 evaluations. We chose this range of evaluations as a\n\n\nTable 2: Average speedup across benchmarks for different #evaluations for the old and new HPO. For the GP based (left) and TPE based (right) evaluation.\n\n\n<table><tr><td>#Evals Old</td><td>#Evals New</td><td>Best First</td><td>Transfer GP/TPE</td><td>Best First + Transfer GP/TPE</td></tr><tr><td rowspan=\"3\">10</td><td>10</td><td>1.6 / 1.5</td><td>1.3 / 1.0</td><td>1.7 / 1.7</td></tr><tr><td>20</td><td>1.4 / 1.4</td><td>1.2 / 1.0</td><td>1.4 / 1.4</td></tr><tr><td>40</td><td>1.1 / 1.2</td><td>1.1 / 1.1</td><td>1.2 / 1.2</td></tr><tr><td rowspan=\"3\">20</td><td>10</td><td>2.3 / 1.8</td><td>1.7 / 1.3</td><td>2.6 / 2.1</td></tr><tr><td>20</td><td>1.7 / 1.4</td><td>1.5 / 1.2</td><td>2.0 / 1.8</td></tr><tr><td>40</td><td>1.3 / 1.1</td><td>1.2 / 1.1</td><td>1.4 / 1.5</td></tr><tr><td rowspan=\"3\">40</td><td>10</td><td>3.3 / 2.6</td><td>2.1 / 1.5</td><td>3.6 / 2.6</td></tr><tr><td>20</td><td>2.6 / 2.0</td><td>2.0 / 1.4</td><td>2.9 / 2.0</td></tr><tr><td>40</td><td>1.8 / 1.4</td><td>1.5 / 1.2</td><td>2.1 / 1.5</td></tr></table>\n\n\nsurvey among NeurIPS2019 and ICLR2020 authors indicates that most hyperparameter optimizations (HPOs) do not consider more than 50 evaluations (Bouthillier & Varoquaux, 2020). Further, for transfer approaches, we perform this experiment for different evaluation budgets for the HPO before the adjustments (also for 10, 20, and 40 evaluations).\n\n\nResults The transfer GP (TGP) / transfer TPE (T2PE) and best-first strategy lead to large speedups, while drop-unimportant and only-optimize-new perform poorly. Here, in this paragraph we focus on the GP based evaluation. On average and depending on the budgets for the old and new HPO, TGP reaches the given objective values 1.1\u20132.1x faster than GP, and best-first 1.1\u20133.3x faster. The combination of TGP/T2PE and best-first leads to further speedups over best-first if the budget for the old HPO was 20 or 40, and when the old budget and new budget both equal 10. These additional average speedups when combining TGP with best-first are between 0.1 and 0.3. We visualize these results with violin plots (Figure 3), as they take into account the multi modality of the results, and also provide a table with exact averages (Table 2). There are two main trends visible: (1) The more optimal the target objective, the smaller the speedup, and (2) the higher the budget for the previous HPO, the higher the speedup. For a more fine-grained visualization that shows violin plots over task averages for each benchmark, we refer to Appendix B. The approaches inspired by actually-practiced manual strategies, drop-unimportant and only-optimize-new, do not reach the performance of GP in a large percentage of  $(20\\% -70\\%)$  on average), even while given at least  $10\\mathrm{x}$  the budget compared to GP. These high failure rates makes an evaluation for the speedup unfeasible and indicates that actually-practiced manual strategies perform worse than starting from scratch. For a visualization of the failure rates of drop-unimportant and only-optimize-new, and for GP, TGP, and best-first  $(0 - 0.8\\%)$  we refer the reader to Appendix C.\n\n\nAdditionally, we provide a study on the improvement in objective in Appendix D; in Appendix E we show the results of a control study that compares GP and TPE with different ranges of random seeds; and in Appendix F we compare GP and TPE to random search to validate their reliability.\n\n\n# 6 RELATED WORK AND RESEARCH OPPORTUNITIES\n\n\nIn this section, we discuss work related to hyperparameter transfer across adjustments (HT-AA) and present several research opportunities in combining existing ideas with HT-AA.\n\n\nTransfer learning Transfer learning studies how to use observations from one or multiple source tasks to improve learning on one or multiple target tasks (Zhuang et al., 2019). If we view the HPO problems before and after specific developer adjustments as tasks, we can consider HT-AA as a specific transfer learning problem. As developer adjustments may change the search space, HT-AA would then be categorized as a heterogeneous transfer learning problem (Day & Khoshgoftaar, 2017).\n\n\nTransfer learning across adjustments Recently, Berner et al. (2019) transferred knowledge between deep reinforcement learning agents across developer adjustments. They crafted techniques to preserve, or approximately preserve, the neural network policy for each type of adjustment they\n\n\n![Figure 3: Speedup to reach a given reference objective value compared to GP/TPE for best-first, best-first combined with transfer GP/TPE, and transfer GP/TPE (from left to right in each violin triplet) across 8 benchmarks. The violins estimate densities of benchmark geometric means. The horizontal line in each violin shows the geometric mean across these benchmark means. #Evaluations for the old HPO increases from left to right. The x-axis shows the budget for the GP and TPE reference. Note that the GP and TPE runs we used to determine the reference objective are different to the ones that we show the speedups over.](images/04187543515fa93fe7c7bc1c099d5f05570bd8780cf81b9baf1536bfb041e5cf.jpg)\n\n\nencountered. Their transfer strategies are inspired by Net2Net knowledge transfer (Chen et al., 2015), and they use the term surgery to refer to this practice. Their work indicates that transfer learning across adjustments is not limited to knowledge about hyperparameters, but extends to a more general setting, leaving room for many research opportunities.\n\n\nContinuous knowledge transfer In this paper, we focus on transferring knowledge from the last HPO performed, but future work could investigate a continuous transfer of knowledge across many cycles of adjustments and HPOs. Transferring knowledge from HPO runs on multiple previous versions could lead to further performance gains, as information from each version could be useful for the current HPO. Such continuous HT-AA would then be related to the field of continual learning (Thrun & Mitchell, 1995; Lange et al., 2020).\n\n\nHyperparameter transfer across tasks (HT-AT) While we have related HT-AA to HT-AT in Section 2, here, we want to complement this discussion with the research opportunities we see in HT-AA based on HT-AT. First, an adaptation of across-task strategies, e.g., the ones cites in Section 2, to the across-adjustments setting could lead to more powerful HT-AA approaches in the future. Second, the combination of across-task and across-adjustments hyperparameter transfer could provide even larger speedups than either transfer strategy on its own.\n\n\n![Figure 4: Percent of runs that do not reach the reference objective for the GP based drop-unimport and only-optimize-new. Each data point for the violins represents the mean percentage of failures for a benchmark. The line in each violin shows the mean across these benchmark means. #Evaluations for the old HPO increases from left to right. The x-axis shows the budget for the GP reference.](images/ea79e32f6bf8d352f99d693e492ce202c4f0c9321d7b1367b336efbf2f6deabf.jpg)\n\n\nAdvanced hyperparameter optimization HT-AA can be combined with the many extensions to hyperparameter optimization (HPO). One such extension is multi-fidelity HPO, which allows the use of cheap-to-evaluate approximations to the actual objective (Li et al., 2017; Falkner et al., 2018). Similarly, cost-aware HPO adds a cost to each hyperparameter setting, so the evaluation of cheap settings can be prioritized over expensive ones (Snoek et al., 2012). Yet another extension is to take evaluation noise into account (Kersting et al., 2007), or to consider not one, but multiple objectives to optimize for (Khan et al., 2002). All these HPO formulations can be studied in conjunction with HT-AA, to either provide further speedups or deal with more general optimization problems.\n\n\nGuided machine learning The field of guided machine learning (gML) studies the design of interfaces that enables humans to guide ML processes (Westphal et al., 2019). An HT-AA algorithm could be viewed as a ML algorithm that receives incremental guidance in the form of arbitrary developer adjustments; the interface would then be the programming language(s) the ML algorithm is implemented in. On a different note, gML could provide HT-AA algorithms with additional information about the adjustments to the ML algorithm. For example, when adding a hyperparameter, there are two distinctions we can make: Either an existing hyperparameter is exposed, or a new component is added to the algorithm that introduces a new hyperparameter From the HPO problem itself, we cannot know which case it is, and neither which fixed value an exposed hyperparameter had. Guided HT-AA algorithms could ask for user input to fill this knowledge gap, or HT-AA algorithms with code analysis could extract this knowledge from the source code.\n\n\nProgramming by optimization The programming by optimization (PbO) framework (Hoos, 2012) proposes the automatic construction of a search space of algorithms based on code annotations, and the subsequent search in this search space. While PbO considers changing search spaces over developer actions, each task and development step restarts the search from scratch. This is in contrast to HT-AA, which alleviates the need to restart from scratch after each developer adjustment.\n\n\n# 7 CONCLUSION\n\n\nIn this work, we introduced hyperparameter transfer across developer adjustments to improve efficiency during the development of ML algorithms. In light of rising energy demands of ML algorithms and rising global temperatures, more efficient ML development practices are an important issue now and will become more important in the future. As already some of the simple baseline algorithm considered in this work lead to large empirical speedups, our new framework represents a promising step towards efficient ML development.\n\n\n# REFERENCES\n\n- Forest Agostinelli, Matthew Hoffman, Peter Sadowski, and Pierre Baldi. Learning activation functions to improve deep neural networks. arXiv preprint arXiv:1412.6830, 2014.\n\n- James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In International conference on machine learning, pp. 115-123, 2013.\n\n- James S Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for hyper-parameter optimization. In Advances in neural information processing systems, pp. 2546-2554, 2011.\n\n- Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n\n- Xavier Bouthillier and Ga\u00ebl Varoquaux. Survey of machine-learning experimental methods at NeurIPS2019 and ICLR2020. Research report, Inria Saclay Ile de France, January 2020. URL https://hal.archives-ouvertes.fr/hal-02447823.\n\n- Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641, 2015.\n\n- Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in alphago. arXiv preprint arXiv:1812.06855, 2018.\n\n- Oscar Day and Taghi M Khoshgoftaar. A survey on heterogeneous transfer learning. Journal of Big Data, 4(1):29, 2017.\n\n- Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In International Conference on Learning Representations, 2019.\n\n- K. Eggensperger, F. Hutter, H. Hoos, and K. Leyton-Brown. Surrogate benchmarks for hyperparameter optimization. In ECAI workshop on Metalearning and Algorithm Selection (MetaSel'14), 2014.\n\n- Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: Robust and efficient hyperparameter optimization at scale. In Proceedings of the 35th International Conference on Machine Learning (ICML 2018), pp. 1436-1445, July 2018.\n\n- Matthias Feurer and Frank Hutter. Hyperparameter optimization. In Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (eds.), Automatic Machine Learning: Methods, Systems, Challenges, pp. 3-38. Springer, 2019. Available at http://automl.org/book.\n\n- Matthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable meta-learning for bayesian optimization. arXiv preprint arXiv:1802.02219, 2018.\n\n- Holger H Hoos. Programming by optimization. Communications of the ACM, 55(2):70-80, 2012.\n\n- Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708, 2017.\n\n- F. Hutter, H. Hoos, and K. Leyton-Brown. An efficient approach for assessing hyperparameter importance. In E. Xing and T. Jebara (eds.), Proceedings of the 31th International Conference on Machine Learning, (ICML'14), pp. 754-762. Omnipress, 2014.\n\n- Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455-492, 1998.\n\n- Kristian Kersting, Christian Plagemann, Patrick Pfaff, and Wolfram Burgard. Most likely heteroscedastic gaussian process regression. In Proceedings of the 24th international conference on Machine learning, pp. 393-400, 2007.\n\n- Nazan Khan, David E Goldberg, and Martin Pelikan. Multi-objective bayesian optimization algorithm. In Proceedings of the 4th Annual Conference on Genetic and Evolutionary Computation, pp. 684-684. CiteSeer, 2002.\n\n- Aaron Klein and Frank Hutter. Tabular benchmarks for joint architecture and hyperparameter optimization. arXiv preprint arXiv:1905.04970, 2019.\n\n- Daniel K\u00fchn, Philipp Probst, Janek Thomas, and Bernd Bischl. Automatic exploration of machine learning experiments on openml. arXiv preprint arXiv:1806.10961, 2018.\n\n- Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks, 2020.\n\n- Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Rethinking the hyperparameters for fine-tuning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=B1g8VkHFPH.\n\n- L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: Bandit-based configuration evaluation for hyperparameter optimization. In OpenReview.net (ed.), Proceedings of the International Conference on Learning Representations (ICLR'17), 2017.\n\n- M. Lindauer, K. Eggensperger, M. Feurer, S. Falkner, A. Biedenkapp, and Hutter F. Smac v3: Algorithm configuration in python. https://github.com/automl/SMAC3, 2017.\n\n- Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha Sohl-Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies, 2020.\n\n- Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. A metric learning reality check. arXiv preprint arXiv:2003.08505, 2020.\n\n- Valerio Perrone, Rodolphe Jenatton, Matthias W Seeger, and Cedric Archambeau. Scalable hyperparameter transfer learning. In Advances in Neural Information Processing Systems, pp. 6845-6855, 2018.\n\n- C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.\n\n- Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. arXiv preprint arXiv:1907.10597, 2019.\n\n- B. Shahriari, K. Swersky, Z. Wang, R. Adams, and N. de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1):148-175, 2016.\n\n- Alistair Shilton, Sunil Gupta, Santu Rana, and Svetha Venkatesh. Regret Bounds for Transfer Learning in Bayesian Optimisation. In Aarti Singh and Jerry Zhu (eds.),., volume 54 of Proceedings of Machine Learning Research, pp. 307-315, Fort Lauderdale, FL, USA, 20-22 Apr 2017. PMLR. URL http://proceedings.mlr.press/v54/shilton17a.html.\n\n- J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger (eds.), Proceedings of the 26th International Conference on Advances in Neural Information Processing Systems (NIPS'12), pp. 2960-2968, 2012.\n\n- Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3645-3650, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1355. URL https://www.aclweb.org/anthology/P19-1355.\n\n- Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization. In Advances in neural information processing systems, pp. 2004-2012, 2013.\n\n- Sebastian Thrun and Tom M. Mitchell. Lifelong robot learning. Robotics and Autonomous Systems, 15(1):25 - 46, 1995. ISSN 0921-8890. doi: https://doi.org/10.1016/0921-8890(95)00004-Y. URL http://www.sciencedirect.com/science/article/pii/092188909500004Y. The Biology and Technology of Intelligent Autonomous Agents.\n\n- Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018.\n\n- Zi Wang, Beomjoon Kim, and Leslie Pack Kaelbling. Regret bounds for meta bayesian optimization with an unknown gaussian process prior. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 10477-10488. Curran Associates, Inc., 2018.\n\n- Florian Westphal, Niklas Lavesson, and H\u00e5kan Grahn. A case for guided machine learning. In Andreas Holzinger, Peter Kieseberg, A Min Tjoa, and Edgar Weippl (eds.), Machine Learning and Knowledge Extraction, pp. 353-361, Cham, 2019. Springer International Publishing. ISBN 978-3-030-29726-8.\n\n- Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Scalable gaussian process-based transfer surrogates for hyperparameter optimization. Machine Learning, 107(1):43-78, 2018.\n\n- Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3-19, 2018.\n\n- Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. arXiv preprint arXiv:1911.02685, 2019.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638566.8769312", "msecs": "876.0", "relativeCreated": "163707.42869377136", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Uniform Localized Convergence and Sharper Generalization Bounds for Minimax Problems\n\n# Abstract\nMinimax problems have achieved widely success in machine learning such as adversarial training, robust optimization, reinforcement learning. Existing studies focus on minimax problems with specific \\text{algorithms} in stochastic optimization, with only a few work on generalization performance. Current generalization bounds almost all depend on stability, which need case-by-case analyses for specific \\text{algorithms}. Additionally, recent work provides the $O(\\sqrt{d/n})$ generalization bound in expectation based on uniform convergence. In this paper, we study the generalization bounds measured by the gradients of primal functions using the uniform localized convergence. We relax the Lipschitz continuity assumption and give a sharper high probability generalization bound for nonconvex-strongly-concave (NC-SC) stochastic minimax problems considering the localized information. Furthermore, we provide dimension-independent results under Polyak-Lojasiewicz condition for the outer layer. Based on the uniform localized convergence, we analyze some popular \\text{algorithms} such as the empirical saddle point (ESP), gradient descent ascent (GDA) and stochastic gradient descent ascent (SGDA) and improve the generalization bounds for primal functions. We can even gain approximate $O(1/n^2)$ excess primal risk bounds with further assumptions that the optimal population risks are small, which, to the best of our knowledge, are the sharpest results in minimax problems.\n\n\n# 1 INTRODUCTION\n\n\nModern machine learning settings such as reinforcement learning (Du et al., 2017; Dai et al., 2018), adversarial learning (Goodfellow et al., 2016), robust optimization (Chen et al., 2017; Namkoong & Duchi, 2017) often need to solve minimax problems, which divide the training process into two groups: one for minimization and one for maximization. To solve the problems, various efficient optimization algorithms such as gradient descent ascent (GDA), stochastic gradient descent ascent (SGDA) have been proposed. Most of them were focused on the iteration complexity, which only considered the optimization error. In contrast, the generalization performance analysis is less considered, which is an important measure to foresee their prediction behavior after training.\n\n\nRecently, Zhang et al. (2022) introduced an expectation generalization error for primal functions in minimax problems using complexity. Naturally, we want to create a high-probability version, preferably using local methods to introduce variance information and obtain a tighter upper bound. A straightforward idea is that we can continue with the traditional localized approach and solve the problem with covering numbers Bartlett et al. (2002). However, these technologies require additional bounded assumptions (Assumption 2), or need certain distributional assumptions for unbounded condition. For example, Mei et al. (2018) introduced the \"Hessian statistical noise\" assumption when using covering numbers. Fortunately, Xu & Zeevi (2020) developed a novel \"uniform localized convergence\" framework using generic chaining for the minimization problems and Li & Liu (2021b) extended it to analyze stochastic algorithms.\n\n\nThis novel framework can not only relax the bounded (or specific distribution) assumptions but also impose fewer restrictions on the surrogate function for the localized method, enabling us to design the measurement functional to achieve a sharper bound. Consequently, we introduce this remarkable framework into minimax problems. Our generalization bound uses weaker assumptions\n\n\ncomparing with Zhang et al. (2022) and is sharper in some conditions due to our utilization of variance information.\n\n\nIntroducing this new framework into minimax problems is not straightforward. Zhang et al. (2022) indeed established a connection between inner and outer layers with the loss of primal functions, but we need do this with a new generic chaining approach. Furthermore, while Zhang et al. (2022) only needed to bound the error caused by the connection between inner and outer layer with  $O(1 / \\sqrt{n})$ . We need to introduce the variance for a sharper bound, to bound the error involved by the two layers.\n\n\nNext, we turn to applications. Firstly, for a sharper excess risk bound, we need to introduce the PL-SC condition to establish a connection between excess risk and the gradient of primal functions, leading to our results for ESP. Unfortunately, for GDA and SGDA algorithms, we found that all the related optimization papers in minimax problems focused on the iteration complexity (or gradient complexity). As a result, they only require  $\\mathbb{E}[\\|\\nabla\\Phi(\\mathbf{x}_T)\\|]$  for average of round  $T$  with an expected outcome. We were compelled to derive the high probability empirical optimization bound ourselves using classical optimization methods under SC-SC conditions.\n\n\nNotice that even under SC-SC settings, achieving this for SGDA remains difficult. Drawing inspiration from Lei et al. (2021)'s proof of Primal-Dual Risk optimization bound, we eventually derive the optimization bound for primal risk. The proofs for excess risks in applications differ from minimization problems Li & Liu (2021b) and pose challenges. These challenges stem from errors in the inner and outer layers of minimax problems. Consequently, we can only achieve a result close to  $O(1 / n^2)$ . Our contributions are summarized as follows:\n\n- 1. We introduce local uniform convergence using new generic chaining techniques. Comparing with traditional uniform convergence results in Zhang et al. (2022), we derive sharper generalization bounds measured by the gradients of primal functions for NC-SC minimax problems. It provides problem independent results that can be used in various minimax algorithms.\n\n- 2. Under the Polyak-Lojasiewicz condition for the outer layer, we provide dimension-independent results and remove the dimension of parameters  $d$  from our generalization bound when the sample size  $n$  is large enough, which is, to our knowledge, the first result in minimax problems.\n\n- 3. We extend our main theorems into various algorithms such as ESP, GDA, SGDA. We establish faster  $O(1/n)$  order bounds for excess primal risk. We can even gain approximate  $O(1/n^2)$  bounds with further assumptions that the optimal population risk is small. To our best knowledge, it is the first time to gain approximate  $O(1/n^2)$  for NC-SC minimax problems in expectation and the first result nearly to  $O(1/n^2)$  high probability bound for SC-SC settings.\n\n\nThis paper is organized as follows. In Section 2 we review the related work. In Section 3 we introduce the notations and assumptions about the problems. Section 4 presents our main results. Then we apply our main theorems into various algorithms and give the sharper bounds for different settings in Section 5. Section 6 concludes our paper. All the proofs in our paper are given in Appendix.\n\n\n# 2 RELATED WORK\n\n\nMinimax optimization. Minimax optimization analysis has been widely studied in different settings. For example, one of the most popular SGDA algorithm and its variants have been analyzed in several recent works including Palaniappan & Bach (2016); Hsieh et al. (2019) for SC-SC cases, Nedic & Ozdaglar (2009); Nemirovski et al. (2009) for convex-concave (C-C) cases, Lin et al. (2020); Luo et al. (2020); Yan et al. (2020); Rafique et al. (2022) for NC-SC problems, Thekumparampil et al. (2019); Yan et al. (2020) for nonconvex-concave (NC-C) cases and Loizou et al. (2020); Liu et al. (2021); Yang et al. (2020) for nonconvex-nonconcave (NC-NC) minimax optimization problems. All these works focus on the iteration complexity (or the gradient complexity) of the algorithms, which only proved the optimization error bounds for the sum of  $T$  iteration's gradient of primal empirical function in expectation. Recently Li & Liu (2021a); Lei et al. (2021) gave optimization bounds with high probability for Primal-Dual risk. We notice that the optimization error of the gradients of primal functions with high probability haven't been studied yet.\n\n\nAlgorithmic stability. Algorithmic stability is a classical approach, which was presented by Rogers & Wagner (1978). It gives the generalization bound by analyzing the sensitivity of a particular\n\n\nlearning algorithm when changing one data point in the dataset. Modern framework of stability analysis was established by Bousquet & Elisseeff (2002), where they present an important concept called uniform stability. Since then, a lot of works based on uniform stability have emerged. On the one hand, the generalization bound with algorithmic stability have been significantly improved by Bousquet et al. (2020); Feldman & Vondrak (2018; 2019); Klochkov & Zhivotovsky (2021). On the other hand, different algorithmic stability measures such as uniform argument stability (Liu et al., 2017; Bassily et al., 2020), on average stability (Shalev-Shwartz et al., 2010; Kuzborskij & Lampert, 2018), collective stability (London et al., 2016) have been developed. For minimax problems, many useful stability measures have also been extended, for example, weak stability (Lei et al., 2021), argument stability (Lei et al., 2021; Li & Liu, 2021a), and uniform stability (Lei et al., 2021; Li & Liu, 2021a; Zhang et al., 2021; Farnia & Ozdaglar, 2021; Ozdaglar et al., 2022). Most of them focused on the expectation generalization bounds and only Lei et al. (2021); Li & Liu (2021a) established some high probability bounds.\n\n\nUniform convergence. Uniform convergence is another popular approach in statistical learning theory to study generalization bounds (Fisher, 1922; Vapnik, 1999; Van der Vaart, 2000). The main idea is to bound the generalization gap by its supremum over the whole (or a subset) of the hypothesis space via some space complexity measures, such as VC dimension, covering number and Rademacher complexity. For finite-dimensional problems, Kleywegt et al. (2002) provided that the generalization error is  $O(\\sqrt{d / n})$  depended on the sample size  $n$  and the dimension of parameters  $d$  in high probability. For nonconvex settings, Mei et al. (2018); Davis & Drusvyatskiy (2022) showed that the empirical of generalization error is  $O(\\sqrt{d / n})$ . Xu & Zeevi (2020) developed a novel \"uniform localized convergence\" framework using generic chaining for the minimization problems and Li & Liu (2021b) extended it to analyze stochastic algorithms. In minimax problems, Zhang et al. (2022) established the first uniform convergence and showed that the empirical generalization error of the gradients for primal functions is  $O(\\sqrt{d / n})$  under NC-SC settings.\n\n\n# 3 PRELIMINARIES\n\n\nLet  $\\mathcal{X} \\in \\mathbb{R}^d$  and  $\\mathcal{Y} \\in \\mathbb{R}^{d'}$  be two nonempty closed convex parameters spaces. Let  $\\mathbb{P}$  be a probability measure defined on a sample space  $\\mathcal{Z}$ . We consider the following minimax optimization problem\n\n\n$$\n\n$$\n\\min  _ {\\mathbf {x} \\in \\mathcal {X}} \\max  _ {\\mathbf {y} \\in \\mathcal {Y}} F (\\mathbf {x}, \\mathbf {y}) := \\mathbb {E} _ {\\mathbf {z} \\sim \\mathbb {P}} [ f (\\mathbf {x}, \\mathbf {y}; \\mathbf {z}) ], \\tag {1}\n$$\n\n$$\n\n\nwhere  $f: \\mathcal{X} \\times \\mathcal{Y} \\times \\mathcal{Z} \\to \\mathbb{R}$  is continuously differentiable and Lipschitz smooth jointly in  $\\mathbf{x}$  and  $\\mathbf{y}$  for any  $\\mathbf{z}$ . This above minimax objective called as the population minimax problem represents an expectation of a cost function  $f(\\mathbf{x}, \\mathbf{y}; \\mathbf{z})$  for minimization variable  $\\mathbf{x}$ , maximization variable  $\\mathbf{y}$  and data variable  $\\mathbf{z}$ . In this paper, we focus on the NC-SC problem which means that  $f$  is nonconvex in  $\\mathbf{x}$  and strongly concave in  $\\mathbf{y}$ . Obviously, our goal is to gain the optimal solution  $(\\mathbf{x}^*, \\mathbf{y}^*)$  to (1). Since the distribution  $\\mathbb{P}$  is unavailable, we can only gain a dataset  $S = \\{\\mathbf{z}_1, \\dots, \\mathbf{z}_n\\}$  drawn  $n$  times independently according to  $\\mathbb{P}$ . Therefore, we solve the following empirical minimax problem instead\n\n\n$$\n\n$$\n\\min  _ {\\mathbf {x} \\in \\mathcal {X}} \\max  _ {\\mathbf {y} \\in \\mathcal {Y}} F _ {S} (\\mathbf {x}, \\mathbf {y}) := \\frac {1}{n} \\sum_ {i = 1} ^ {n} f (\\mathbf {x}, \\mathbf {y}; \\mathbf {z} _ {i}). \\tag {2}\n$$\n\n$$\n\n\nNext we introduce one of the common measures in minimax problems called primal functions.\n\n\nDefinition 1 (primal function (empirical/population)). The primal population function and the primal empirical function are given by\n\n\n$$\n\n$$\n\\Phi (\\mathbf {x}) := \\max  _ {\\mathbf {y} \\in \\mathcal {Y}} F (\\mathbf {x}, \\mathbf {y}) \\quad a n d \\quad \\Phi_ {S} (\\mathbf {x}) := \\max  _ {\\mathbf {y} \\in \\mathcal {Y}} F _ {S} (\\mathbf {x}, \\mathbf {y}).\n$$\n\n$$\n\n\nSince  $F$  and  $F_{S}$  are nonconvex in  $\\mathbf{x}$ , it is difficult to find the global optimal solution in general. In practice, we design an algorithm  $\\mathcal{A}$  that finds an  $\\epsilon$ -stationary point\n\n\n$$\n\n$$\n\\left\\| \\nabla \\Phi \\left(\\mathcal {A} _ {\\mathbf {x}} (S)\\right) \\right\\| \\leq \\epsilon , \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\mathcal{A}_{\\mathbf{x}}(S)$  is the  $x$ -component of the output using any algorithm  $\\mathcal{A}(S) = (\\mathcal{A}_{\\mathbf{x}}(S), \\mathcal{A}_{\\mathbf{y}}(S))$  for solving (2). Then the optimization error for solve the population minimax problem (1) can be decomposed into two terms:\n\n\n$$\n\n$$\n\\left\\| \\nabla \\Phi \\left(\\mathcal {A} _ {\\mathbf {x}} (S)\\right) \\right\\| \\leq \\left\\| \\nabla \\Phi_ {S} \\left(\\mathcal {A} _ {\\mathbf {x}} (S)\\right) \\right\\| + \\left\\| \\nabla \\Phi \\left(\\mathcal {A} _ {\\mathbf {x}} (S)\\right) - \\nabla \\Phi_ {S} \\left(\\mathcal {A} _ {\\mathbf {x}} (S)\\right) \\right\\|, \\tag {4}\n$$\n\n$$\n\n\nwhere the first term on the right-hand-side corresponds to the optimization error of solving the empirical minimax problem (2) and the second term corresponds to the generalization error of the gradients of primal function. The above inequality satisfies from the triangle inequality.\n\n\nLet  $\\| \\cdot \\|$  be the Euclidean norm for simplicity and  $B(\\mathbf{x}_0,R)\\coloneqq \\{\\mathbf{x}\\in \\mathbb{R}^d:\\| \\mathbf{x} - \\mathbf{x}_0\\| \\leq R\\}$  denote a ball with center  $\\mathbf{x}_0\\in \\mathbb{R}^d$  and radius  $R$ . For the closed convex set  $\\mathcal{X}$ , we assume that there is a radius  $R_{1}$  such that  $\\mathcal{X}\\in B(\\mathbf{x}^{*},R_{1})$ . Let  $\\mathcal{A}(S)\\coloneqq (\\mathcal{A}_{\\mathbf{x}}(S),\\mathcal{A}_{\\mathbf{y}}(S))$  denote the output of an algorithm  $\\mathcal{A}$  for solving the empirical minimax problem (2) with dataset  $S$  and  $\\nabla f = (\\nabla_{\\mathbf{x}}f,\\nabla_{\\mathbf{y}}f)$  denote the gradient of a function  $f$ .\n\n\nDefinition 2 (Strongly convex function). Let  $\\mu_{\\mathbf{y}} > 0$ . A differentiable function  $g:\\mathcal{W}\\to \\mathbb{R}$  is called  $\\mu$ -strongly-convex in  $\\mathbf{w}$  if the following inequality holds for every  $\\mathbf{w}_1,\\mathbf{w}_2$ :\n\n\n$$\n\n$$\ng (\\mathbf {w} _ {1}) - g (\\mathbf {w} _ {2}) \\geq \\left\\langle \\nabla g (\\mathbf {w} _ {2}), \\mathbf {w} _ {1} - \\mathbf {w} _ {2} \\right\\rangle + \\frac {\\mu}{2} \\| \\mathbf {w} _ {1} - \\mathbf {w} _ {2} \\| ^ {2},\n$$\n\n$$\n\n\nwe say  $g$  is  $\\mu$ -strongly-concave if  $-g$  is  $\\mu$ -strongly-convex.\n\n\nDefinition 3 (Smooth function). Let  $\\beta > 0$ . A function  $f: \\mathcal{X} \\times \\mathcal{Y} \\times \\mathcal{Z} \\to \\mathbb{R}$  is  $\\beta$ -smooth in  $(\\mathbf{x}, \\mathbf{y})$  if the function is continuous differentiable and for any  $\\mathbf{x}_1, \\mathbf{x}_2 \\in \\mathcal{X}$ ,  $\\mathbf{y}_1, \\mathbf{y}_2 \\in \\mathcal{Y}$  and  $\\mathbf{z} \\in \\mathcal{Z}$ ,  $f(\\mathbf{x}, \\mathbf{y}; \\mathbf{z})$  satisfies\n\n\n$$\n\n$$\n\\left\\| \\left( \\begin{array}{c} \\nabla_ {\\mathbf {x}} f (\\mathbf {x} _ {1}, \\mathbf {y} _ {1}; \\mathbf {z}) - \\nabla_ {\\mathbf {x}} f (\\mathbf {x} _ {2}, \\mathbf {y} _ {2}; \\mathbf {z}) \\\\ \\nabla_ {\\mathbf {y}} f (\\mathbf {x} _ {1}, \\mathbf {y} _ {1}; \\mathbf {z}) - \\nabla_ {\\mathbf {y}} f (\\mathbf {x} _ {2}, \\mathbf {y} _ {2}; \\mathbf {z}) \\end{array} \\right) \\right\\| \\leq \\beta \\left\\| \\left( \\begin{array}{c} \\mathbf {x} _ {1} - \\mathbf {x} _ {2} \\\\ \\mathbf {y} _ {1} - \\mathbf {y} _ {2} \\end{array} \\right) \\right\\|.\n$$\n\n$$\n\n\nAssumption 1 (Nonconvex-strongly-concave minimax problem). In order to obtain meaningful conclusions, we make the following assumptions:\n\n- - Let  $\\mu_{\\mathbf{y}} > 0$ . The function  $f(\\mathbf{x},\\mathbf{y};\\mathbf{z})$  is  $\\mu_{\\mathbf{y}}$ -strongly concave in  $\\mathbf{y} \\in \\mathcal{V}$  for any  $\\mathbf{x} \\in \\mathcal{X}$  and  $\\mathbf{z} \\in \\mathcal{Z}$ .\n\n- - The function  $f(\\mathbf{x}, \\mathbf{y}; \\mathbf{z})$  is  $\\beta$ -smooth in  $(\\mathbf{x}, \\mathbf{y}) \\in \\mathcal{X} \\times \\mathcal{Y}$  for any  $\\mathbf{z}$ .\n\n- -  $\\mathcal{X}$  and  $\\mathcal{Y}$  are compact convex sets, which means that there exist constants  $D_{\\mathcal{X}}, D_{\\mathcal{Y}} > 0$  such that for any  $\\mathbf{x} \\in \\mathcal{X}$ ,  $\\|\\mathbf{x}\\|^2 \\leq D_{\\mathcal{X}}$  and for any  $\\mathbf{y} \\in \\mathcal{Y}$ ,  $\\|\\mathbf{y}\\|^2 \\leq D_{\\mathcal{Y}}$ .\n\n\nThe first two assumptions in Assumption I are standard in NC-SC minimax problems (Zhang et al., 2021; Farnia & Ozdaglar, 2021; Lei et al., 2021; Li & Liu, 2021a) and the last one in Assumption II is widely used in uniform convergence analysis (Kleywegt et al., 2002; Davis & Drusvyatskiy, 2022; Zhang et al., 2022).\n\n\nAssumption 2 (Lipschitz continuity). Let  $L > 0$ , assume that for any  $\\mathbf{x} \\in \\mathcal{X}$  and any  $\\mathbf{y} \\in \\mathcal{Y}$  respectively for any  $\\mathbf{z}$ , the function  $f(\\mathbf{x},\\mathbf{y};\\mathbf{z})$  satisfies\n\n\n$$\n\n$$\n\\left\\| \\nabla_ {\\mathbf {x}} f (\\mathbf {x}, \\mathbf {y}; \\mathbf {z}) \\right\\| \\leq L \\quad a n d \\quad \\left\\| \\nabla_ {\\mathbf {y}} f (\\mathbf {x}, \\mathbf {y}; \\mathbf {z}) \\right\\| \\leq L.\n$$\n\n$$\n\n\nLipschitz assumption is also the standard assumption and widely used in literature such as Zhang et al. (2021); Farnia & Ozdaglar (2021); Lei et al. (2021); Li & Liu (2021a). But we need to emphasize that our main Theorem and Theorem do not require the Lipschitz assumption. Instead, we introduce a weaker assumption called Bernstein condition in minimax problems.\n\n\nDefinition 4 (Bernstein condition). Given a random variable  $X$  with mean  $\\mu = \\mathbb{E}[X]$  and variance  $\\sigma^2 = \\mathbb{E}[X^2] - \\mu^2$ , we say that Bernstein's condition holds if there exists  $B > 0$  such that for all  $2 \\leq k \\leq n$ ,\n\n\n$$\n\n$$\n\\left| \\mathbb {E} \\left[ (X - \\mu) ^ {k} \\right] \\right| \\leq \\frac {1}{2} k! \\sigma^ {2} B ^ {k - 2}. \\tag {5}\n$$\n\n$$\n\n\nRemark 1. Bernstein condition has been widely used to obtain tail bounds that may be tighter than the Hoeffding bounds. It is easy to verify that any bounded variable satisfies the Bernstein condition. Moreover, the Bernstein condition is milder than the bounded assumption of random variables and is also satisfied by various unbounded variables. For example, a random variable is sub-exponential if it satisfies the Bernstein condition (Wainwright, 2019). Please refer to Wainwright (2019) for more discussions. Next, we introduce a straightforward generalization of the Bernstein condition to minimax problems. We formally state these extension in the following assumptions.\n\n\nAssumption 3. In minimax problems, the function  $f(\\mathbf{x}, \\mathbf{y}; \\mathbf{z})$  satisfies Bernstein condition in  $\\mathbf{x}^*$  for  $\\mathbf{y}^*$ : there exists  $B_{\\mathbf{x}^*} > 0$  such that for all  $2 \\leq k \\leq n$ ,\n\n\n$$\n\n$$\n\\mathbb {E} \\left[ \\| \\nabla_ {\\mathbf {x}} f \\left(\\mathbf {x} ^ {*}, \\mathbf {y} ^ {*}; \\mathbf {z}\\right) \\| ^ {k} \\right] \\leq \\frac {1}{2} k! \\mathbb {E} \\left[ \\| \\nabla_ {\\mathbf {x}} f \\left(\\mathbf {x} ^ {*}, \\mathbf {y} ^ {*}; \\mathbf {z}\\right) \\| ^ {2} \\right] B _ {\\mathbf {x} ^ {*}} ^ {k - 2}. \\tag {6}\n$$\n\n$$\n\n\nAnd the function  $f(\\mathbf{x}, \\mathbf{y}; \\mathbf{z})$  satisfies Bernstein condition in  $\\mathbf{y}^*(\\mathbf{x})$  for any fixed  $\\mathbf{x}$ : there exists  $B_{\\mathbf{y}^*} > 0$  such that for all  $2 \\leq k \\leq n$ ,\n\n\n$$\n\n$$\n\\mathbb {E} [ \\| \\nabla_ {\\mathbf {y}} f (\\mathbf {x}, \\mathbf {y} ^ {*} (\\mathbf {x}); \\mathbf {z}) \\| ^ {k} ] \\leq \\frac {1}{2} k! \\mathbb {E} [ \\| \\nabla_ {\\mathbf {y}} f (\\mathbf {x}, \\mathbf {y} ^ {*} (\\mathbf {x}); \\mathbf {z}) \\| ^ {2} ] B _ {\\mathbf {y} ^ {*}} ^ {k - 2}, \\tag {7}\n$$\n\n$$\n\n\nwhere  $\\mathbf{y}^{*}(\\mathbf{x})\\coloneqq \\arg \\max_{\\mathbf{y}\\in \\mathcal{Y}}F(\\mathbf{x},\\mathbf{y})$\n\n\nRemark 2. We can easily obtain that Assumption  $\\boxed{2}$  can derive Assumption  $\\boxed{3}$  For example, if function  $f$  is  $L$ -Lipschitz continuous, then  $\\| \\nabla_{\\mathbf{x}} f(\\mathbf{x}, \\mathbf{y}; \\mathbf{z}) \\| \\leq L$ . Thus for any  $\\mathbf{x} \\in \\mathcal{X}, \\mathbf{y} \\in \\mathcal{Y}$  and for all  $2 \\leq k \\leq n$ , we have  $\\mathbb{E}[\\| \\nabla_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y}; \\mathbf{z}) \\|^{k}] \\leq \\frac{1}{2} k! \\mathbb{E}[\\| \\nabla_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y}; \\mathbf{z}) \\|^{2}] L^{k-2}$ , which means that the function  $f$  satisfies Bernstein condition for any  $\\mathbf{x}, \\mathbf{y}$ . Similarly,  $\\mathbb{E}[\\| \\nabla_{\\mathbf{x}} f(\\mathbf{x}, \\mathbf{y}; \\mathbf{z}) \\|^{k}] \\leq \\frac{1}{2} k! \\mathbb{E}[\\| \\nabla_{\\mathbf{x}} f(\\mathbf{x}, \\mathbf{y}; \\mathbf{z}) \\|^{2}] L^{k-2}$  can be easily derived. Furthermore, Bernstein condition assumption is pretty mild since  $B_{\\mathbf{y}^*}$  only depends on gradients at  $(\\mathbf{x}, \\mathbf{y}^*(\\mathbf{x}))$  for any  $\\mathbf{x} \\in \\mathcal{X}$  and  $B_{\\mathbf{x}^*}$  only depends on gradients at  $(\\mathbf{x}^*, \\mathbf{y}^*)$ .\n\n\n# 4 UNIFORM LOCALIZED CONVERGENCE AND GENERALIZATION BOUNDS\n\n\nUniform convergence of the gradients for the primal functions measures the deviation between the gradients of the primal population function  $\\nabla \\Phi(\\mathbf{x})$  and the gradients of the primal empirical function  $\\nabla \\Phi_S(\\mathbf{x})$ . In this section, we provide the sharper uniform convergence of the gradients for the primal functions comparing with Zhang et al. (2022).\n\n\nTheorem 1. Under Assumption [1] and [3] we have the following inequality that for any  $\\delta \\in (0,1)$  and for all  $\\mathbf{x} \\in \\mathcal{X}$ , with probability at least  $1 - \\delta$ ,\n\n\n$$\n\n$$\n\\begin{array}{l} \\| \\nabla \\Phi (\\mathbf {x}) - \\nabla \\Phi_ {S} (\\mathbf {x}) \\| \\leq \\frac {\\beta}{\\mu_ {\\mathbf {y}}} \\left(\\sqrt {\\frac {2 \\mathbb {E} \\| \\nabla_ {\\mathbf {y}} f (\\mathbf {x} , \\mathbf {y} ^ {*} (\\mathbf {x}) ; \\mathbf {z}) \\| ^ {2} \\log \\frac {4}{\\delta}}{n}} + \\frac {B _ {\\mathbf {y} ^ {*}} \\log \\frac {4}{\\delta}}{n}\\right) \\\\ + \\sqrt {\\frac {2 \\mathbb {E} \\| \\nabla_ {\\mathbf {x}} f (\\mathbf {x} ^ {*} , \\mathbf {y} ^ {*} ; \\mathbf {z}) \\| ^ {2} \\log \\frac {8}{\\delta}}{n}} + \\frac {B _ {\\mathbf {x} ^ {*}} \\log \\frac {8}{\\delta}}{n} + \\frac {C \\beta (\\mu_ {\\mathbf {y}} + \\beta)}{\\mu_ {\\mathbf {y}}} \\max \\left\\{\\| \\mathbf {x} - \\mathbf {x} ^ {*} \\|, \\frac {1}{n} \\right\\} \\\\ \\times \\left(\\sqrt {\\frac {d + \\log \\frac {1 6 \\log_ {2} (\\sqrt {2} R _ {1} n + 1)}{\\delta}}{n}} + \\frac {d + \\log \\frac {1 6 \\log_ {2} (\\sqrt {2} R _ {1} n + 1)}{\\delta}}{n}\\right), \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere  $C$  is a absolute constant.\n\n\nThere is only one uniform convergence of gradients for primal functions in minimax problems given in Zhang et al. (2022). Here is their main theorem in NC-SC settings.\n\n\nTheorem 2 (Theorem in (Zhang et al., 2022)). Under Assumption I and 2 we have\n\n\n$$\n\n$$\n\\mathbb {E} \\left[ \\max _ {\\mathbf {x} \\in \\mathcal {X}} \\| \\nabla \\Phi (\\mathbf {x}) - \\nabla \\Phi_ {S} (\\mathbf {x}) \\| \\right] = \\tilde {O} \\left(\\frac {L (\\mu_ {\\mathbf {y}} + \\beta)}{\\mu_ {\\mathbf {y}}} \\sqrt {\\frac {d}{n}}\\right),\n$$\n\n$$\n\n\nwhere  $\\tilde{O} (\\cdot)$  hides logarithmic factors.\n\n\nRemark 3. We now compare our uniform convergence of gradient for primal functions with Zhang et al. (2022). Firstly, our result is the only one with high-probability format. Besides, we successfully relax the assumptions. Theorem \u2461 requires the Lipschitz continuity assumption, while our result only needs Bernstein condition assumption. Please refer to Remark \u2460 Remark \u2461 for the detailed comparison between these assumptions. Then, the factor in Theorem \u2461 is  $\\frac{L(\\mu_{\\mathbf{y}} + \\beta)}{\\mu_{\\mathbf{y}}}$ , while our result in Theorem \u2460 is  $\\frac{C\\beta(\\mu_{\\mathbf{y}} + \\beta)}{\\mu_{\\mathbf{y}}}$  max  $\\left\\{\\| \\mathbf{x} - \\mathbf{x}^{*}\\| ,\\frac{1}{n}\\right\\}$ , not involving the term  $L$ , which may be very large and even infinite without Lipschitz continuity assumption. Finally, while Zhang et al. (2022) studied\n\n\nthe worst-case upper bounds on the parameters, results based on generic chaining yield upper bound related to the parameters. As shown Theorem we have the term  $\\max \\{\\| \\mathbf{x} - \\mathbf{x}^*\\| ,\\frac{1}{n}\\}$  before the term  $O(\\sqrt{d / n})$ , indicating that our results improve as the calculated parameters of algorithms approach the optimal solution. In the optimal scenario, when  $\\| \\mathbf{x} - \\mathbf{x}^*\\| \\leq \\frac{1}{n}$ , we can attain the best (sharper) results.\n\n\nNext, we provide a dimension-free uniform convergence of gradients for the primal functions when the PL condition is satisfied. Firstly, we introduce the extension of the PL condition to the minimax problem used in Guo et al. (2020); Yang et al. (2020).\n\n\nAssumption 4 (x-side  $\\mu_{\\mathbf{x}}$ -Polyak-Lojasiewicz condition). For any  $\\mathbf{y} \\in \\mathcal{V}$ , the function  $F(\\mathbf{x}, \\mathbf{y})$  satisfies the x-side  $\\mu_{\\mathbf{x}}$ -Polyak-Lojasiewicz (PL) condition with parameter  $\\mu_{\\mathbf{x}} > 0$  on all  $\\mathbf{x} \\in \\mathcal{X}$  if\n\n\n$$\n\n$$\nF (\\mathbf {x}, \\mathbf {y}) - \\inf _ {\\mathbf {x} ^ {\\prime}} F (\\mathbf {x} ^ {\\prime}, \\mathbf {y}) \\leq \\frac {1}{2 \\mu_ {\\mathbf {x}}} \\| \\nabla_ {\\mathbf {x}} F (\\mathbf {x}, \\mathbf {y}) \\| ^ {2}.\n$$\n\n$$\n\n\nRemark 4. Numerous studies have been conducted on deep learning to provide evidence for the validity of the PL condition in risk minimization problems. This condition has been demonstrated to hold either globally or locally in certain networks with specific structural, activation, or loss function characteristics (Hardt & Ma 2016; Li & Yuan 2017; Zhou & Liang 2017; Li & Liang 2018; Arora et al. 2018; Charles & Papailopoulos 2018; Du et al. 2018; Allen-Zhu et al. 2019). For instance, Du et al. (2018) has exhibited that if a two-layer neural network possesses a sufficiently wide width, the PL condition is upheld within a ball centered at the initial solution, and the global optimum is situated within this same ball. Additionally, Allen-Zhu et al. (2019) have further demonstrated that in overparameterized deep neural networks utilizing ReLU activation, the PL condition is applicable to a global optimum located in the vicinity of a random initial solution.\n\n\nTheorem 3. Under Assumption  $\\boxed{7}$  and  $\\boxed{3}$  assume that the population risk  $F(\\mathbf{x},\\mathbf{y})$  satisfies Assumption  $\\boxed{4}$  with parameter  $\\mu_{\\mathbf{x}}$  and let  $c = \\max \\{16C^2,1\\}$ . We have that for all  $\\mathbf{x} \\in \\mathcal{X}$ , when  $n \\geq \\frac{c\\beta^2(\\mu_{\\mathbf{y}} + \\beta)^2(d + \\log\\frac{8\\log_2\\sqrt{2}R_1n + 1}{\\delta})}{\\mu_{\\mathbf{y}}^2\\mu_{\\mathbf{x}}^2}$  with probability at least  $1 - \\delta$\n\n\n$$\n\n$$\n\\begin{array}{l} \\| \\nabla \\Phi (\\mathbf {x}) - \\nabla \\Phi_ {S} (\\mathbf {x}) \\| \\leq \\| \\nabla \\Phi_ {S} (\\mathbf {x}) \\| + 2 \\sqrt {\\frac {2 \\mathbb {E} \\| \\nabla_ {\\mathbf {x}} f (\\mathbf {x} ^ {*} , \\mathbf {y} ^ {*} ; \\mathbf {z}) \\| ^ {2} \\log \\frac {8}{\\delta}}{n}} \\\\ + \\frac {2 B _ {\\mathbf {x} ^ {*}} \\log \\frac {8}{\\delta}}{n} + \\frac {\\mu_ {\\mathbf {x}}}{n} + \\frac {2 \\beta}{\\mu_ {\\mathbf {y}}} \\left(\\sqrt {\\frac {2 \\mathbb {E} \\| \\nabla_ {\\mathbf {y}} f (\\mathbf {x} , \\mathbf {y} ^ {*} (\\mathbf {x}) ; \\mathbf {z}) \\| ^ {2} \\log \\frac {4}{\\delta}}{n}} + \\frac {B _ {\\mathbf {y} ^ {*}} \\log \\frac {4}{\\delta}}{n}\\right). \\\\ \\end{array}\n$$\n\n$$\n\n\nRemark 5. The following inequality can be easily derived using the norm triangle inequality and Cauchy-Bunyakovsky-Schwarz inequality.\n\n\n$$\n\n$$\n\\begin{array}{l} \\Phi (\\mathbf {x}) - \\Phi (\\mathbf {x} ^ {*}) \\leq \\frac {8 \\| \\nabla \\Phi_ {S} (\\mathbf {x}) \\| ^ {2}}{\\mu_ {\\mathbf {x}}} + \\frac {1 6 \\beta^ {2} \\mathbb {E} \\| \\nabla_ {\\mathbf {y}} f (\\mathbf {x} , \\mathbf {y} ^ {*} (\\mathbf {x}) ; \\mathbf {z}) \\| ^ {2} \\log \\frac {4}{\\delta}}{\\mu_ {\\mathbf {x}} \\mu_ {\\mathbf {y}} ^ {2} n} \\\\ + \\frac {1 6 \\mathbb {E} \\| \\nabla_ {\\mathbf {x}} f (\\mathbf {x} ^ {*} , \\mathbf {y} ^ {*} ; \\mathbf {z}) \\| ^ {2} \\log \\frac {8}{\\delta}}{\\mu_ {\\mathbf {x}} n} + \\frac {2 \\left(\\frac {2 \\beta B _ {\\mathbf {y} ^ {*}}}{\\mu_ {\\mathbf {y}}} \\log \\frac {4}{\\delta} + 2 B _ {\\mathbf {x} ^ {*}} \\log \\frac {8}{\\delta} + \\mu_ {\\mathbf {x}}\\right) ^ {2}}{\\mu_ {\\mathbf {x}} n ^ {2}}. \\tag {8} \\\\ \\end{array}\n$$\n\n$$\n\n\nWe can easily derive (8) from Theorem 3 to gain the excess primal risk bound, where  $\\|\\nabla\\Phi_S(\\mathbf{x})\\|$  is the empirical optimization error of the primal function. In Theorem 3 and (8),  $\\|\\nabla\\Phi_S(\\mathbf{x})\\|$  can be very tiny since most famous optimization algorithms such as GDA and SGDA, can optimize it small enough. The term  $\\mathbb{E}\\|\\nabla_{\\mathbf{x}}f(\\mathbf{x}^*,\\mathbf{y}^*;\\mathbf{z})\\|^2$  and  $\\mathbb{E}\\|\\nabla_{\\mathbf{y}}f(\\mathbf{x},\\mathbf{y}^*(\\mathbf{x});\\mathbf{z})\\|^2$  can be also tiny since they only depend on the gradient of the optima  $\\mathbf{x}^*$  w.r.t  $\\mathbf{x}$  and the gradient of the optima  $\\mathbf{y}^*(\\mathbf{x})$  w.r.t.  $\\mathbf{y}$ . Thus, comparing with Theorem 2 in Zhang et al. (2022), this uniform localized convergence bound is clearly tighter when relaxing Lipschitz continuity (Assumption 2) and considering PL condition (Assumption 4). We further analyze these two terms  $\\mathbb{E}\\|\\nabla_{\\mathbf{x}}f(\\mathbf{x}^*,\\mathbf{y}^*;\\mathbf{z})\\|^2$  and  $\\mathbb{E}\\|\\nabla_{\\mathbf{y}}f(\\mathbf{x},\\mathbf{y}^*(\\mathbf{x});\\mathbf{z})\\|^2$  using \"Self-bounding\" property for smooth function (Srebro et al. 2010) and considering specific algorithms in Section 5 which can derive to almost  $O(1/n^2)$  bounds. Additionally, uniform convergence often implies results with a square-root dependence on the dimension  $d$  such as Theorem 7 and Zhang et al. (2022). Another distinctive improvement of Theorem 3 is that we remove the dimension  $d$  when the population risk  $F(\\mathbf{x},\\mathbf{y})$  satisfies the  $\\mathbf{x}$ -side PL condition and the sample size  $n$  is large enough.\n\n\nRemark 6. There are two mainly challenges in our work for minimax problems. On one hand, comparing with uniform convergence in Zhang et al. (2022), we use a novel uniform localized convergence techniques (Xu & Zeevi 2020) to construct a functional w.r.t. loss functions on minimax problems. This two layer structure involves difficulties. On the other hand, it is noteworthy that the optimal point  $\\mathbf{y}^{*}(\\mathbf{x}) \\coloneqq \\arg \\max_{\\mathbf{y} \\in \\mathcal{Y}} F(\\mathbf{x}, \\mathbf{y})$  for a given  $\\mathbf{x}$  differs from  $\\mathbf{y}_S^*(\\mathbf{x}) \\coloneqq \\arg \\max_{\\mathbf{y} \\in \\mathcal{Y}} F_S(\\mathbf{x}, \\mathbf{y})$ , thus introducing an additional error term  $\\| \\mathbf{y}^{*}(\\mathbf{x}) - \\mathbf{y}_S^*(\\mathbf{x}) \\|$ . Compared to Zhang et al. (2022), they only need to bound this term with  $O\\left(\\frac{1}{\\sqrt{n}}\\right)$ . But we need to reach the upper bound of order  $O\\left(\\frac{1}{n}\\right)$  under certain assumptions.\n\n\n# 5 APPLICATION\n\n\n# 5.1 EMPIRICAL SADDLE POINT\n\n\nEmpirical saddle point (ESP) problem, which is also known as sample average approximation (SAA) (Zhang et al., 2021) refers to [2]. We denote  $(\\hat{\\mathbf{x}}^{*},\\hat{\\mathbf{y}}^{*})$  as one of the ESP solution to [2]. Then we can provide some important theorems in this subsection.\n\n\nTheorem 4. Suppose the empirical saddle point  $(\\hat{\\mathbf{x}}^*, \\hat{\\mathbf{y}}^*)$  exists and Assumption [1] and [3] hold, for any  $\\delta \\in (0,1)$ , with probability at least  $1 - \\delta$ , we have\n\n\n$$\n\n$$\n\\| \\nabla \\Phi (\\hat {\\mathbf {x}} ^ {*}) \\| = O \\left(\\sqrt {\\frac {d + \\log \\frac {\\log n}{\\delta}}{n}}\\right)\n$$\n\n$$\n\n\nRemark 7. When Assumption I and 3 hold, Theorem 4 gives that the population optimization error  $\\|\\nabla\\Phi(\\hat{\\mathbf{x}}^{*})\\|$  is of order  $O\\left(\\sqrt{\\frac{d + \\log\\frac{1}{\\delta}}{n}}\\right)$  ( $\\log n$  is small and can be ignored typically). Note that this result doesn't require the Lipschitz continuity assumption (Assumption 2). Although it may be hard to find  $(\\hat{\\mathbf{x}}^{*},\\hat{\\mathbf{y}}^{*})$  in NC-SC minimax problems, it is still meaningful when assuming the ESP  $(\\hat{\\mathbf{x}}^{*},\\hat{\\mathbf{y}}^{*})$  has been found.\n\n\nTheorem 5. Suppose Assumption  $\\square$  and  $\\boxed{3}$  hold. Assume that the population risk  $F(\\mathbf{x},\\mathbf{y})$  satisfies Assumption  $\\boxed{4}$  with parameter  $\\mu_{\\mathbf{x}}$ . For any  $\\delta \\in (0,1)$ , with probability at least  $1 - \\delta$ , when  $n \\geq \\frac{c\\beta^2(\\mu_{\\mathbf{y}} + \\beta)^2(d + \\log\\frac{8\\log_2\\sqrt{2}R_1n + 1}{\\delta})}{\\mu_{\\mathbf{y}}^2\\mu_{\\mathbf{x}}^2}$ , where  $c$  is an absolute constant, we have\n\n\n$$\n\n$$\n\\begin{array}{l} \\Phi (\\hat {\\mathbf {x}} ^ {*}) - \\Phi (\\mathbf {x} ^ {*}) \\leq \\frac {1 2 \\beta^ {2} \\mathbb {E} \\| \\nabla_ {\\mathbf {y}} f (\\hat {\\mathbf {x}} ^ {*} , \\mathbf {y} ^ {*} (\\hat {\\mathbf {x}} ^ {*}) ; \\mathbf {z}) \\| ^ {2} \\log \\frac {4}{\\delta}}{\\mu_ {\\mathbf {x}} \\mu_ {\\mathbf {y}} ^ {2} n} + \\frac {1 2 \\mathbb {E} \\| \\nabla_ {\\mathbf {x}} f (\\mathbf {x} ^ {*} , \\mathbf {y} ^ {*} ; \\mathbf {z}) \\| ^ {2} \\log \\frac {8}{\\delta}}{\\mu_ {\\mathbf {x}} n} \\\\ + \\frac {3 \\left(\\frac {2 \\beta B _ {\\mathbf {y} ^ {*}}}{\\mu_ {\\mathbf {y}}} \\log \\frac {4}{\\delta} + 2 B _ {\\mathbf {x} ^ {*}} \\log \\frac {8}{\\delta} + \\mu_ {\\mathbf {x}}\\right) ^ {2}}{2 \\mu_ {\\mathbf {x}} n ^ {2}}. \\\\ \\end{array}\n$$\n\n$$\n\n\nFurthermore, if we let  $n \\geq \\max \\left\\{\\frac{c\\beta^2(\\mu_{\\mathbf{y}} + \\beta)^2(d + \\log\\frac{8\\log_2\\sqrt{2}R_1n + 1}{\\delta})}{\\mu_{\\mathbf{y}}^2\\mu_{\\mathbf{x}}^2}, \\frac{48\\beta^3\\log\\frac{4}{\\delta}}{\\mu_{\\mathbf{x}}\\mu_{\\mathbf{y}}^2}\\right\\}$  and assume the function  $f(\\mathbf{x},\\mathbf{y};\\mathbf{z})$  is non-negative and  $\\Phi (\\mathbf{x}^{*}) = O\\left(\\frac{1}{n}\\right)$ , we have\n\n\n$$\n\n$$\n\\Phi (\\hat {\\mathbf {x}} ^ {*}) - \\Phi (\\mathbf {x} ^ {*}) = O \\left(\\frac {\\log^ {2} \\frac {1}{\\delta}}{n \\left(n - \\frac {4 8 \\beta^ {3} \\log \\frac {4}{\\delta}}{\\mu_ {\\mathbf {x}} \\mu_ {\\mathbf {y}} ^ {2}}\\right)}\\right).\n$$\n\n$$\n\n\nRemark 8. Theorem  $\\boxed{5}$  shows that when the population minimax risk  $F(\\mathbf{x},\\mathbf{y})$  satisfies  $\\mathbf{x}$ -side PL condition, we can provide a sharper excess risk bound for primal function, which can be almost  $O(1 / n^2)$ . Note that the optimal population primal function  $\\Phi (\\mathbf{x}^{*}) = O(1 / n)$  is a very common assumption in many researches such as Srebro et al. (2010); Zhang et al. (2017); Liu et al. (2018); Zhang & Zhou (2019); Lei & Ying (2020), which is natural because  $F(\\mathbf{x}^{*},\\mathbf{y}^{*})$  is the minimal population risk. Now we compare our results with recent related work (Li & Liu 2021b), which studied the general machine learning settings for  $f(\\mathbf{w})$  under PL condition. Their empirical risk minimizer (ERM) excess risk bounds provided  $O\\left(1 / n^{2}\\right)$  order rates. We analyze the excess risk with primal functions which involve an additional error term. In consequence, our result for ESP just approximate  $O\\left(1 / n^{2}\\right)$  order rate.\n\n\n# ALGORITHM 1 TWO-TIMESCALE GDA FOR MINIMAX PROBLEM\n\n- 1: Input:  $(\\mathbf{x}_1, \\mathbf{y}_1)$ , step sizes  $\\eta_{\\mathbf{x}} > 0$ ,  $\\eta_{\\mathbf{y}} > 0$  and dataset  $S = \\{\\mathbf{z}_1, \\dots, \\mathbf{z}_n\\}$\n\n- 2: for  $t = 1, \\dots, T$  do\n\n- 3: update  $\\mathbf{x}_{t + 1} = \\mathbf{x}_t - \\eta_{\\mathbf{x}}\\nabla_{\\mathbf{x}}F_S(\\mathbf{x}_t,\\mathbf{y}_t)$\n\n- 4: update  $\\mathbf{y}_{t + 1} = \\mathbf{y}_t + \\eta_{\\mathbf{y}}\\nabla_{\\mathbf{y}}F_S(\\mathbf{x}_t,\\mathbf{y}_t)$\n\n\n# ALGORITHM 2 TWO-TIMESCALE SGDA FOR MINIMAX PROBLEM\n\n- 1: Input:  $(\\mathbf{x}_1, \\mathbf{y}_1) = (0, 0)$ , step sizes  $\\{\\eta_{\\mathbf{x}_t}\\}_t > 0$ ,  $\\{\\eta_{\\mathbf{y}_t}\\}_t > 0$  and dataset  $S = \\{\\mathbf{z}_1, \\dots, \\mathbf{z}_n\\}$\n\n- 2: for  $t = 1, \\dots, T$  do\n\n- 3: update  $\\mathbf{x}_{t + 1} = \\mathbf{x}_t - \\eta_{\\mathbf{x}_t}\\nabla_{\\mathbf{x}}f(\\mathbf{x}_t,\\mathbf{y}_t;\\mathbf{z}_{i_t})$\n\n- 4: update  $\\mathbf{y}_{t + 1} = \\mathbf{y}_t + \\eta_{\\mathbf{y}_t}\\nabla_{\\mathbf{y}}f(\\mathbf{x}_t,\\mathbf{y}_t;\\mathbf{z}_{i_t})$\n\n\n# 5.2 GRADIENT DESCENT ASCENT\n\n\nGradient descent ascent (GDA) presented in Algorithm [1] is one of the most popular algorithms and has been widely used in minimax problems. In this subsection, we provide the generalization and excess risk bounds of primal functions with the two-timescale GDA Algorithm which is harder to analyze compared to GDMax and multistep GDA (Lin et al., 2020).\n\n\nTheorem 6. Suppose Assumption I and 2 hold. Let  $\\{\\mathbf{x}_t\\}_{t}$  be the sequence produced by Algorithm I with the step sizes chosen as  $\\eta_{\\mathbf{x}} = \\frac{1}{16(\\frac{\\beta}{\\mu} + 1)^2\\beta}$  and  $\\eta_{\\mathbf{y}} = \\frac{1}{\\beta}$ , for any  $\\delta \\in (0,1)$ , with probability at least  $1 - \\delta$ , we have\n\n\n$$\n\n$$\n\\frac {1}{T} \\sum_ {t = 1} ^ {T} \\| \\nabla \\Phi (\\mathbf {x} _ {t}) \\| ^ {2} \\leq O \\left(\\frac {1}{T}\\right) + O \\left(\\frac {d + \\log \\frac {1 6 \\log_ {2} (\\sqrt {2} R _ {1} n + 1)}{\\delta} T}{n}\\right).\n$$\n\n$$\n\n\nFurthermore, when  $T \\asymp O\\left(\\sqrt{\\frac{n}{d}}\\right)$ , we have\n\n\n$$\n\n$$\n\\frac {1}{T} \\sum_ {t = 1} ^ {T} \\| \\nabla \\Phi (\\mathbf {x} _ {t}) \\| ^ {2} \\leq O \\left(\\frac {d + \\log \\frac {\\log n}{\\delta}}{\\sqrt {n d}}\\right).\n$$\n\n$$\n\n\nRemark 9. Theorem reveals that we need to balance the optimization error and the generalization error for GDA. According to the results, the iterative complexity of Algorithm should be chosen as  $T \\asymp O\\left(\\sqrt{\\frac{n}{d}}\\right)$ , which achieves the optimal population optimization error of primal function.\n\n\nIn comparison to Theorem 4, Theorem 6 derives into population optimization error w.r.t SGD, which is much more difficult. To establish population optimization error, we need to bound the empirical optimization error, an area where no research has been conducted in NC-SC settings with high probability. One possible approach is to construct the martingale difference sequence of step  $T$  for primal functions, yet this constitutes a separate topic warranting further exploration. Theorem 6 aims to directly apply Theorem 1 to SGD. Comparing with Theorem 3, Theorem 6 only necessitates smooth and Lipschitz conditions (Assumption 7 and 2) and doesn't require PL conditions. In fact, Theorem 7 constitutes a further extension of Theorem 3\n\n\nNext, we provide the excess risk of primal functions  $\\Phi (\\bar{\\mathbf{x}}_T) - \\Phi (\\mathbf{x}^*)$  for Algorithm 1 where  $\\bar{\\mathbf{x}}_T = \\frac{1}{T}\\sum_{t = 1}^{T}\\mathbf{x}_t$ . We need to know the empirical optimization error  $\\| \\nabla \\Phi_S(\\bar{\\mathbf{x}}_T)\\|$ . Unfortunately, although the generalization bounds we proved are in NC-SC settings, we require the SC-SC assumptions to derive the empirical optimization error bound of primal functions, to gain the high probability bound. We relax this SC-SC assumption in Appendix E using existing optimization error bound with expectation format.\n\n\nDefinition 5. A function  $g: \\mathcal{X} \\times \\mathcal{Y} \\to \\mathbb{R}$  is  $\\mu_{\\mathbf{x}}$ -strongly-convex-  $\\mu_{\\mathbf{y}}$ -strongly-concave if  $g(\\cdot, \\mathbf{y})$  is  $\\mu_{\\mathbf{x}}$ -strongly-convex for any  $\\mathbf{y} \\in \\mathcal{Y}$  and  $g(\\mathbf{x}, \\cdot)$  is  $\\mu_{\\mathbf{y}}$ -strongly-concave for any  $\\mathbf{x} \\in \\mathcal{X}$ .\n\n\nAssumption 5 (Strongly-convex-strongly-concave minimax problem). Assume Assumption [I] holds and let  $\\mu_{\\mathbf{x}} > 0, \\mu_{\\mathbf{y}} > 0$ . The function  $f(\\mathbf{x}, \\mathbf{y}; \\mathbf{z})$  is  $\\mu_{\\mathbf{x}}$ -strongly-convex- $\\mu_{\\mathbf{y}}$ -strongly-concave in  $\\mathbf{y} \\in \\mathcal{V}$  for any  $\\mathbf{x} \\in \\mathcal{X}$  and  $\\mathbf{z} \\in \\mathcal{Z}$ .\n\n\nRemark 10. Assumption 5 is commonly used in SC-SC problems (Zhang et al. 2021; Li & Liu, 2021a). We require this assumption to derive the empirical optimization error bound of primal functions. The detailed proofs of the optimization error bound  $\\|\\nabla \\Phi_S(\\bar{\\mathbf{x}}_T)\\|$  are given in Section D.2 for GDA and in Section D.3 for SGDA.\n\n\nTheorem 7. Suppose Assumption  $\\boxed{3}$  and  $\\boxed{5}$  hold. Let  $\\{\\mathbf{x}_t\\}_t$  be the sequence produced by Algorithm  $\\boxed{7}$  and  $\\bar{\\mathbf{x}}_T = \\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{x}_t$  with the step sizes chosen as  $\\eta_{\\mathbf{x}} = \\frac{1}{16(\\frac{\\beta}{\\mu} + 1)^2\\beta}$  and  $\\eta_{\\mathbf{y}} = \\frac{1}{\\beta}$ , for any  $\\delta \\in (0,1)$ , with probability at least  $1 - \\delta$ , when  $T \\asymp n$  and  $n \\geq \\frac{c\\beta^2(\\mu_{\\mathbf{y}} + \\beta)^2(d + \\log\\frac{8\\log_2\\sqrt{2}R_1n + 1}{\\delta})}{\\mu_{\\mathbf{y}}^2\\mu_{\\mathbf{x}}^2}$ , where  $c$  is an absolute constant, we have\n\n\n$$\n\n$$\n\\Phi (\\bar {\\mathbf {x}} _ {T}) - \\Phi (\\mathbf {x} ^ {*}) = O \\bigg (\\frac {\\mathbb {E} \\| \\nabla_ {\\mathbf {x}} f (\\mathbf {x} ^ {*} , \\mathbf {y} ^ {*} ; \\mathbf {z}) \\| ^ {2} \\log \\frac {1}{\\delta}}{n} + \\frac {\\mathbb {E} \\| \\nabla_ {\\mathbf {y}} f (\\bar {\\mathbf {x}} _ {T} , \\mathbf {y} ^ {*} (\\bar {\\mathbf {x}} _ {T}) ; \\mathbf {z}) \\| ^ {2} \\log \\frac {1}{\\delta}}{n} + \\frac {\\log^ {2} \\frac {1}{\\delta}}{n ^ {2}} \\bigg).\n$$\n\n$$\n\n\nFurthermore, Let  $T \\asymp n^2$  and  $n \\geq \\max \\left\\{ \\frac{c\\beta^2(\\mu_{\\mathbf{y}} + \\beta)^2(d + \\log\\frac{8\\log_2\\sqrt{2}R_1n + 1}{\\delta})}{\\mu_{\\mathbf{y}}^2\\mu_{\\mathbf{x}}^2}, \\frac{128\\beta^3\\log\\frac{4}{\\delta}}{\\mu_{\\mathbf{x}}\\mu_{\\mathbf{y}}^2} \\right\\}$ . Assume the function  $f(\\mathbf{x},\\mathbf{y};\\mathbf{z})$  is non-negative and  $\\Phi (\\mathbf{x}^{*}) = O\\left(\\frac{1}{n}\\right)$ , we have\n\n\n$$\n\n$$\n\\Phi (\\bar {\\mathbf {x}} _ {T}) - \\Phi (\\mathbf {x} ^ {*}) = O \\left(\\frac {\\log^ {2} \\frac {1}{\\delta}}{n \\left(n - \\frac {1 2 8 \\beta^ {3} \\log \\frac {4}{\\delta}}{\\mu_ {\\mathbf {x}} \\mu_ {\\mathbf {y}} ^ {2}}\\right)}\\right).\n$$\n\n$$\n\n\nRemark 11. Theorem  $\\boxed{7}$  shows that the excess risk for primal functions can be bound almost to  $O\\left(1 / n^{2}\\right)$  comparing with the optimal result  $O(1 / n)$  given in Li & Liu (2021a) when  $n$  is large enough. Note that we require the SC-SC assumption to derive the empirical optimization error. If we give this bound in expectation, we can relax the SC-SC assumption with x-side PL-strongly-concave assumption instead.\n\n\n# 5.3 STOCHASTIC GRADIENT DESCENT ASCENT\n\n\nWe now analyze the excess risk bound of primal functions for stochastic gradient descent ascent (SGDA). The algorithmic scheme that we study is two-timescale SGDA  $(\\eta_{\\mathbf{x}} \\neq \\eta_{\\mathbf{y}})$  with variable step sizes, presented in Algorithm 2 which is more nature in the real problems.\n\n\nTheorem 8. Suppose Assumption  $\\boxed{2}$  and  $\\boxed{5}$  hold, let  $\\{\\mathbf{x}_t\\}_t$  be the sequence produced by Algorithm  $\\boxed{2}$  and  $\\bar{\\mathbf{x}}_T = \\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{x}_t$  with the step sizes chosen as  $\\eta_{\\mathbf{x}_t} = \\frac{1}{\\mu_{\\mathbf{x}}(t + t_0)}$  and  $\\eta_{\\mathbf{y}_t} = \\frac{1}{\\mu_{\\mathbf{y}}(t + t_0)}$ , for any  $\\delta \\in (0,1)$ , with probability at least  $1 - \\delta$ , when  $T \\asymp n^2$  and  $n \\geq \\frac{c\\beta^2(\\mu_{\\mathbf{y}} + \\beta)^2(d + \\log\\frac{8\\log_2\\sqrt{2}R_1n + 1}{\\delta})}{\\mu_{\\mathbf{y}}^2\\mu_{\\mathbf{x}}^2}$ , where  $c$  is an absolute constant, we have\n\n\n$$\n\n$$\n\\Phi (\\bar {\\mathbf {x}} _ {T}) - \\Phi (\\mathbf {x} ^ {*}) = O \\left(\\frac {\\mathbb {E} \\| \\nabla_ {\\mathbf {x}} f (\\mathbf {x} ^ {*} , \\mathbf {y} ^ {*} ; \\mathbf {z}) \\| ^ {2} \\log \\frac {1}{\\delta}}{n} + \\frac {\\mathbb {E} \\| \\nabla_ {\\mathbf {y}} f (\\bar {\\mathbf {x}} _ {T} , \\mathbf {y} ^ {*} (\\bar {\\mathbf {x}} _ {T}) ; \\mathbf {z}) \\| ^ {2} \\log \\frac {1}{\\delta}}{n} + \\frac {\\log^ {2} \\frac {1}{\\delta}}{n ^ {2}}\\right).\n$$\n\n$$\n\n\nFurthermore, let  $T \\asymp n^4$  and  $n \\geq \\max \\left\\{\\frac{c\\beta^2(\\mu_{\\mathbf{y}} + \\beta)^2(d + \\log\\frac{8\\log_2\\sqrt{2}R_1n + 1}{\\delta})}{\\mu_{\\mathbf{y}}^2\\mu_{\\mathbf{x}}^2}, \\frac{128\\beta^3\\log\\frac{8}{\\delta}}{\\mu_{\\mathbf{x}}\\mu_{\\mathbf{y}}^2}\\right\\}$ . Assume the function  $f(\\mathbf{x},\\mathbf{y};\\mathbf{z})$  is non-negative and  $\\Phi (\\mathbf{x}^{*}) = O\\left(\\frac{1}{n}\\right)$ , we have\n\n\n$$\n\n$$\n\\Phi (\\bar {\\mathbf {x}} _ {T}) - \\Phi (\\mathbf {x} ^ {*}) = O \\left(\\frac {\\log^ {2} \\frac {1}{\\delta}}{n \\left(n - \\frac {1 2 8 \\beta^ {3} \\log \\frac {8}{\\delta}}{\\mu_ {\\mathbf {x}} \\mu_ {\\mathbf {y}} ^ {2}}\\right)}\\right).\n$$\n\n$$\n\n\nRemark 12. Theorem reveals that under the SC-SC settings, the excess risk bound can be approximate  $O(1/n^2)$  comparing with the optimal result  $O(1/n)$  given in [Li & Liu, 2021a]. Similarly, since the SC-SC assumption is required to derive the empirical optimization bound, we can relax the assumptions when we only need expectation bounds instead of high probability bounds.\n\n\n# 6 CONCLUSION\n\n\nIn this paper, we provide the improved generalization bounds for minimax problems with uniform localized convergence. We firstly provide a sharper bound measured by the gradients of primal functions with weaker assumptions. Then we provide dimension-independent results under PL condition. Finally we extend our main theorems into various algorithms to reach the optimal excess primal risk bounds. We notice that most optimization works focused on the gradient complexity with expectation results. It would be interesting to give the optimization error of  $\\bar{\\mathbf{x}}_T$  or even  $\\mathbf{x}_T$  with high probability under weaker conditions. Combining with our generalization work, we can get a tighter excess primal risk bound with weaker conditions.\n\n\n# REFERENCES\n\n- Zeyuan Allen-Zhu, Yanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.\n\n- Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018.\n\n- Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized rademacher complexities. In International Conference on Computational Learning Theory, pp. 44-58. Springer, 2002.\n\n- Raef Bassily, Vitaly Feldman, Crist\u00f3bal Guzm\u00e1n, and Kunal Talwar. Stability of stochastic gradient descent on nonsmooth convex losses. In Proceedings of the 34th International Conference on Neural Information Processing Systems (NeurIPS), volume 33, pp. 4381-4391, 2020.\n\n- St\u00e9phane Boucheron, G\u00e1bor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013.\n\n- Olivier Bousquet and Andr\u00e9 Elisseeff. Stability and generalization. The Journal of Machine Learning Research, 2:499-526, 2002.\n\n- Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovsky. Sharper bounds for uniformly stable algorithms. In Conference on Learning Theory, pp. 610-626. PMLR, 2020.\n\n- Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. In International Conference on Machine Learning, pp. 745-754. PMLR, 2018.\n\n- Robert S Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for nonconvex objectives. Advances in Neural Information Processing Systems, 30, 2017.\n\n- Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed: Convergent reinforcement learning with nonlinear function approximation. In International Conference on Machine Learning, pp. 1125-1134. PMLR, 2018.\n\n- Damek Davis and Dmitriy Drusvyatskiy. Graphical convergence of subgradients in nonconvex optimization and learning. Mathematics of Operations Research, 47(1):209-231, 2022.\n\n- Sjoerd Dirksen. Tail bounds via generic chaining. Electronic Journal of Probability, 20:1-29, 2015.\n\n- Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction methods for policy evaluation. In International Conference on Machine Learning, pp. 1049-1058. PMLR, 2017.\n\n- Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.\n\n- Farzan Farnia and Asuman Ozdaglar. Train simultaneously, generalize better: Stability of gradient-based minimax learners. In International Conference on Machine Learning, pp. 3174-3185. PMLR, 2021.\n\n- Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. Proceedings of the 32nd International Conference on Neural Information Processing Systems (NeurIPS), 31, 2018.\n\n- Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Conference on Learning Theory, pp. 1270-1279. PMLR, 2019.\n\n- Ronald A Fisher. On the mathematical foundations of theoretical statistics. Philosophical transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character, 222(594-604):309-368, 1922.\n\n- Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT Press, 2016.\n\n- Zhishuai Guo, Yan Yan, Zhuoning Yuan, and Tianbao Yang. Fast objective & duality gap convergence for nonconvex-strongly-concave min-max problems. arXiv preprint arXiv:2006.06889, 2020.\n\n- Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.\n\n- Yu-Guan Hsieh, Franck Iutzeler, J\u00e9r\u00f4me Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-lojasiewicz condition. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16, pp. 795-811. Springer, 2016.\n\n- Anton J Kleywegt, Alexander Shapiro, and Tito Homem-de Mello. The sample average approximation method for stochastic discrete optimization. SIAM Journal on optimization, 12(2):479-502, 2002.\n\n- Yegor Klochkov and Nikita Zhivotovsky. Stability and deviation optimal risk bounds with convergence rate  $o(1/n)$ . Proceedings of the 35th International Conference on Neural Information Processing Systems (NeurIPS), 34:5065-5076, 2021.\n\n- Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In Proceedings of the 35th International Conference on Machine Learning (ICML), pp. 2815-2824. PMLR, 2018.\n\n- Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic gradient descent. In International Conference on Machine Learning, pp. 5809-5819. PMLR, 2020.\n\n- Yunwen Lei, Zhenhuan Yang, Tianbao Yang, and Yiming Ying. Stability and generalization of stochastic gradient methods for minimax problems. In International Conference on Machine Learning, pp. 6175-6186. PMLR, 2021.\n\n- Shaojie Li and Yong Liu. High probability generalization bounds with fast rates for minimax problems. In International Conference on Learning Representations, 2021a.\n\n- Shaojie Li and Yong Liu. Improved learning rates for stochastic optimization: Two theoretical viewpoints. arXiv preprint arXiv:2107.08686, 2021b.\n\n- Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. Advances in neural information processing systems, 31, 2018.\n\n- Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. Advances in neural information processing systems, 30, 2017.\n\n- Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In International Conference on Machine Learning, pp. 6083-6093. PMLR, 2020.\n\n- Mingrui Liu, Xiaoxuan Zhang, Lijun Zhang, Rong Jin, and Tianbao Yang. Fast rates of erm and stochastic approximation: Adaptive to error bound conditions. Advances in Neural Information Processing Systems, 31, 2018.\n\n- Mingrui Liu, Hassan Rafique, Qihang Lin, and Tianbao Yang. First-order convergence theory for weakly-convex-weakly-concave min-max problems. The Journal of Machine Learning Research, 22(1):7651-7684, 2021.\n\n- Tongliang Liu, G\u00e1bor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis complexity. In International Conference on Machine Learning, pp. 2159-2167. PMLR, 2017.\n\n- Nicolas Loizou, Hugo Berard, Alexia Jolicoeur-Martineau, Pascal Vincent, Simon Lacoste-Julien, and Ioannis Mitliagkas. Stochastic hamiltonian gradient methods for smooth games. In International Conference on Machine Learning, pp. 6370-6381. PMLR, 2020.\n\n- Ben London, Bert Huang, and Lise Getoor. Stability and generalization in structured prediction. The Journal of Machine Learning Research, 17(1):7808-7859, 2016.\n\n- Luo Luo, Haishan Ye, Zhichao Huang, and Tong Zhang. Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems. Advances in Neural Information Processing Systems, 33:20566-20577, 2020.\n\n- Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for nonconvex losses. The Annals of Statistics, 46(6A):2747-2774, 2018.\n\n- Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. Advances in neural information processing systems, 30, 2017.\n\n- Angelia Nedic and Asuman Ozdaglar. Subgradient methods for saddle-point problems. Journal of optimization theory and applications, 142:205-228, 2009.\n\n- Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574-1609, 2009.\n\n- Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2003.\n\n- Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving a class of non-convex min-max games using iterative first order methods. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Asuman Ozdaglar, Sarath Pattathil, Jiawei Zhang, and Kaiqing Zhang. What is a good metric to study generalization of minimax learners? arXiv preprint arXiv:2206.04502, 2022.\n\n- Balamurugan Palaniappan and Francis Bach. Stochastic variance reduction methods for saddle-point problems. Advances in Neural Information Processing Systems, 29, 2016.\n\n- Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of Probability, pp. 1679-1706, 1994.\n\n- Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Weakly-convex-concave min-max optimization: provable algorithms and applications in machine learning. Optimization Methods and Software, 37(3):1087-1121, 2022.\n\n- William H Rogers and Terry J Wagner. A finite sample distribution-free performance bound for local discrimination rules. The Annals of Statistics, pp. 506-514, 1978.\n\n- Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635-2670, 2010.\n\n- Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their approximations. Constructive approximation, 26(2):153-172, 2007.\n\n- Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth loss. arXiv preprint arXiv:1009.3896, 2010.\n\n- Michel Talagrand. Majorizing measures: the generic chaining. The Annals of Probability, 24(3): 1049-1103, 1996.\n\n- Pierre Tarres and Yuan Yao. Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence. IEEE Transactions on Information Theory, 60(9):5716-5735, 2014.\n\n- Kiran K Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient algorithms for smooth minimax optimization. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.\n\n- Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, 10(5):988-999, 1999.\n\n- Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.\n\n- Yunbei Xu and Assaf Zeevi. Towards optimal problem dependent generalization error bounds in statistical learning theory. arXiv preprint arXiv:2011.06186, 2020.\n\n- Yan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Optimal epoch stochastic gradient descent ascent methods for min-max optimization. Advances in Neural Information Processing Systems, 33:5789-5800, 2020.\n\n- Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems. Advances in Neural Information Processing Systems, 33:1153-1165, 2020.\n\n- Junyu Zhang, Mingyi Hong, Mengdi Wang, and Shuzhong Zhang. Generalization bounds for stochastic saddle point problems. In International Conference on Artificial Intelligence and Statistics, pp. 568-576. PMLR, 2021.\n\n- Lijun Zhang and Zhi-Hua Zhou. Stochastic approximation of smooth and strongly convex functions: Beyond the  $o(1/t)$  convergence rate. In Conference on Learning Theory, pp. 3160-3179. PMLR, 2019.\n\n- Lijun Zhang, Tianbao Yang, and Rong Jin. Empirical risk minimization for stochastic convex optimization:  $o(1/n)$ -and  $o(1/n^2)$ -type of risk bounds. In Conference on Learning Theory, pp. 1954-1979. PMLR, 2017.\n\n- Siqi Zhang, Yifan Hu, Liang Zhang, and Niao He. Uniform convergence and generalization for nonconvex stochastic minimax problems. arXiv preprint arXiv:2205.14278, 2022.\n\n- Yi Zhou and Yingbin Liang. Characterization of gradient dominance and regularity conditions for neural networks. arXiv preprint arXiv:1710.06910, 2017.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638567.9150243", "msecs": "915.0", "relativeCreated": "164745.52178382874", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Learning Value Functions from Undirected State-only Experience\n\n# Abstract\nThis paper tackles the problem of learning value functions from undirected state-only experience (state transitions without action labels i.e. (s,s',r) tuples). We first theoretically characterize the applicability of Q-learning in this setting. We show that tabular Q-learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-learning or LAQ, an offline RL method that can learn effective value functions from state-only experience. Latent Action Q-learning (LAQ) learns value functions using Q-learning on discrete latent actions obtained through a latent-variable future prediction model. We show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions. Value functions learned using LAQ lead to sample efficient acquisition of goal-directed behavior, can be used with domain-specific low-level controllers, and facilitate transfer across embodiments. Our experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives, imitation learning oracles, and competing methods.\n\n\n# 1 INTRODUCTION\n\n\nOffline or batch reinforcement learning focuses on learning goal-directed behavior from prerecorded data of undirected experience in the form of  $(s_t, a_t, s_{t+1}, r_t)$  quadruples. However, in many realistic applications, action information is not naturally available (e.g. when learning from video demonstrations), or worse still, isn't even well-defined (e.g. when learning from the experience of an agent with a different embodiment). Motivated by such use cases, this paper studies if and how, intelligent behavior can be derived from undirected streams of observations:  $(s_t, s_{t+1}, r_t)$ .<sup>1</sup>\n\n\nOur key conceptual insight is that while an observation-only dataset doesn't tell us the precise action to execute, i.e. the policy  $\\pi(a|s)$ ; it may still tell us which states are more likely to lead us to the goal than not, i.e. the value function  $V(s)$ . For example, simply by looking at someone working in the kitchen, we can infer that approaching the microwave handle is more useful (i.e. has higher value) for opening the microwave than to approach the buttons. Thus, we can still make use of observation-only data, if we focused on learning value functions as opposed to directly learning goal-directed policies. Once we have learned a good value function, it can be used to quickly acquire or infer behavior. Using learned value functions as dense rewards can lead to quick policy learning through some small amount of interaction in the environment. Alternatively, they could be used to directly guide the behavior of low-level controllers that may already be available for the agent (as is often the case in robotics) without any further training. Furthermore, decoupling the learning of value functions from policy learning enables deriving behavior for agents with a different embodiment as long as the overall strategy to solve the task remains similar.\n\n\nThus, the central technical question is how to learn a good value function from undirected observation streams. Is it even possible? If so, under what conditions? This paper tackles these questions from a theoretical and practical perspective.\n\n\nWe start out by characterizing the behavior of tabular Q-learning from Watkins (1989) under missing action labels. We note that Q-learning with naively imputed action labels is equivalent to the TD(0)\n\n\npolicy evaluation, which serves as a simple baseline method for deriving a value function. However, depending on the policy that generated the data, the learned values (without any action grounding) can differ from the optimal values. Furthermore, it is possible to construct simple environments where the behavior implied by the learned value function is also sub-optimal.\n\n\nNext, we present a more optimistic result. There are settings in which Q-learning can recover the optimal value function even in the absence of the knowledge of underlying actions. Concretely, we prove that if we are able to obtain an action space which is a strict refinement of the original action space, then Q-learning in this refined action space recovers the optimal value function.\n\n\nThis motivates a practical algorithm for learning value functions from the given undirected observation-only experience. We design a latent-variable future prediction model that seeks to obtain a refined action space. It operates by predicting  $s_{t + 1}$  from  $s_t$  and a discrete latent variable  $\\hat{a}$  from a set of actions  $\\hat{\\mathbf{A}}$  (Section 4.1). Training this latent variable model assigns a discrete action  $\\hat{a}_t$  to each  $(s_t, s_{t + 1})$  tuple. This allows us to employ Q-learning to learn good value functions (Section 4.2). The learned value function is used to derive behavior (Section 4.3) either through some online interaction with the environment, or through the use of domain specific low-level controllers.\n\n\nThe use of a latent action space for Q-learning allows us to exceed the performance of methods based on policy evaluation (Edwards & Isbell, 2019), which will learn the value of the demonstration policy, not the optimal value function. Additionally, it side-steps the problem of reconstructing high-dimensional images faced by other state-only value learning methods (Edwards et al., 2020). Other approaches for learning from state-only data rely on imitating the demonstration data, which renders them unable to improve on sub-optimal demonstration data. See Section 7 for more discussion.\n\n\nOur experiments in five environments (2D grid world, 2D continuous control, Atari game Freeway, robotic manipulation, and visual navigation in realistic 3D environments) test our proposed ideas. Our method approximates a refinement of the latent space better than clustering alternatives, and in turn, learns value functions highly correlated with ground truth. Good value functions in-turn lead to sample efficient acquisition of behavior, leading to significant improvement over learning with only environment rewards. Our method compares well against existing methods that learn from undirected observation-only data, while being also applicable to the case of high-dimensional observation spaces in the form of RGB images. We are also able to outperform imitation learning methods, even when these imitation learning methods have access to privileged ground-truth action information. Furthermore, our method is able to use observation-only experience from one agent to speed up learning for another agent with a different embodiment. Code, models, simulation environments will be released.\n\n\n# 2 PRELIMINARIES\n\n\nFollowing the notation from Sutton & Barto (2018), our Markov decision process (MDP) is specified by  $(\\mathbf{S},\\mathbf{A},p,\\gamma)$ , where  $\\mathbf{S}$  is a state space,  $\\mathbf{A}$  is an action space,  $\\gamma$  is the discount factor, and  $p(s',r|s,a)$  is the state/reward joint dynamics function. It specifies the probability distribution that the agent ends up in state  $s'$ , receives a reward of  $r$  on executing action  $a$  from state  $s$ .\n\n\nOffline or batch RL (Lange et al., 2012; Levine et al., 2020) studies the problem of deriving high reward behavior when only given a dataset of experience in an MDP, in the form of a collection of quadruples  $(s, a, s', r)$ . In this paper, we tackle a harder version of this problem where instead we are only given a collection of triplets  $(s, s', r)$ , i.e. experience without information about intervening actions. In general, this dataset could contain any quality of behavior, from optimal, to actively adversarial. In contrast to some methods (see Section 7), we will not assume that demonstrations in the dataset are of high quality, and design our method to be robust to sub-optimal data.\n\n\nIn this paper, we will focus on methods based on Q-learning (Watkins, 1989) for tackling this problem. Q-learning has the advantage of being off-policy, i.e., experience from another policy (or task) can be used to learn or improve a different policy for a different task. Q-learning seeks to learn the optimal Q-function  $Q^{*}(s,a)$  by iteratively updating  $Q(s_{t},a_{t})$  to the Bellman equation. This process converges to the  $Q^{*}$  under mild conditions in many settings (Watkins, 1989), and gives the optimal state-value function as  $V^{*}(s) = \\max_{a}Q^{*}(s,a)$ .\n\n\n![](images/8db76b66f99759d570c58f88c1117d158d7fedb39bf326ebc88b276b3daed6c0.jpg)\n\n\n![](images/362ee09ba8ebcf87cbc01b75c745178308d722ccea0d284d1442b6196b9e7a46.jpg)\n\n\n![](images/f0e1540cbe9aa0924c6a5dd596f04db9533b9ab63bd5ab6b3609fc64b43755c4.jpg)\n\n\n![Figure 1: We visualize the learned value function when using different action labels for Q-learning. See Section 3.2 for more details.](images/1b252a603b1a19934110ec5d754f301aa904dceba1e912a4175614715064a368.jpg)\n\n\n# 3 CHARACTERIZING Q-LEARNING WITHOUT TRUE ACTION LABELS\n\n\nWe characterize the outcome of Q-learning in settings where we don't have ground truth intervening actions in the offline dataset being used for Q-learning. We first consider the case of ignoring the action altogether, which amounts to TD(0) policy evaluation. Next, we study if labeling  $(s,s^{\\prime},r)$  samples with actions from a different action space  $\\hat{\\mathbf{A}}$  to construct a new MDP could aid learning. More specifically, can the optimal Q-function for this new MDP, as obtained through Q-learning on samples  $(s,s^{\\prime},r)$  labeled with actions from  $\\hat{\\mathbf{A}}$ , be useful in the original MDP? We show that under the right conditions the value function learned under the altered action space  $\\hat{\\mathbf{A}}$  is identical to the value function learned for the original MDP.\n\n\nQ-learning naive action labels (single action): Without action labels, one could simply assign all transitions the same label. In this case, Q-Learning becomes TD(0) policy evaluation. The induced value function isn't the optimal value function for the MDP, but rather the value according to the policy that generated the dataset. Depending on the dataset, this could be sub-optimal.\n\n\n# 3.1 OPTIMALITY OF ACTION REFINEMENT\n\n\nAssume we have a Markov Decision Process (MDP)  $M$  specified by  $(\\mathbf{S},\\mathbf{A},p,\\gamma)$ . Let the action space  $\\mathbf{A}$  be composed of actions  $a_1,a_2,a_3,\\ldots ,a_n\\in \\mathbf{A}$ . We are interested in the value learned under a modified MDP,  $\\hat{M}$  composed of  $(\\mathbf{S},\\hat{\\mathbf{A}},\\hat{p},\\gamma)$ . We will show that if the actions and transitions  $\\hat{\\mathbf{A}}$  and  $\\hat{p}$  are a refinement of  $\\mathbf{A}$  and  $p$ , then the value function learned on  $\\hat{M}$ ,  $V_{\\hat{M}}$  is identical to the value function learned on  $M$ ,  $V_{M}$ . We define actions and transitions in  $\\hat{M}$  to be a refinement of those in  $M$  when, a) in each state, for every action in  $\\hat{\\mathbf{A}}$ , there is at least one action in  $\\mathbf{A}$  which is functionally identical in the same state, and b) in each state, for each action in  $\\mathbf{A}$  is represented by at least one action in  $\\hat{\\mathbf{A}}$  in that state.\n\n\nDefinition 3.1 Given a discrete finite MDP,  $M$  specified by  $(\\mathbf{S},\\mathbf{A},p)$  and MDP,  $\\hat{M}$  specified by  $(\\mathbf{S},\\hat{\\mathbf{A}},\\hat{p})$ ,  $\\hat{M}$  is a refinement of  $M$  when\n\n\n$$\n\n$$\n\\underset {\\hat {a} \\in \\hat {\\mathbf {A}}, s \\in \\mathbf {S}} {\\forall} \\underset {a \\in \\mathbf {A}} {\\exists} \\underset {s ^ {\\prime}, r} {\\forall} \\hat {p} (s ^ {\\prime}, r | s, \\hat {a}) = p (s ^ {\\prime}, r | s, a), \\text {a n d} \\underset {a \\in \\mathbf {A}, s \\in \\mathbf {S}} {\\forall} \\underset {\\hat {a} \\in \\hat {\\mathbf {A}}} {\\exists} \\underset {s ^ {\\prime}, r} {\\forall} \\hat {p} (s ^ {\\prime}, r | s, \\hat {a}) = p (s ^ {\\prime}, r | s, a),\n$$\n\n$$\n\n\nNote that this definition of refinement requires a state conditioned correspondence between action behavior. Actions do not need to have to correspond across states.\n\n\nTheorem 3.1. Given a discrete finite MDP,  $\\hat{M}$  which is a refinement of  $M$  (Definition 3.1) then both MDPs induce the same optimal value function, i.e.  $\\forall_{s}V_{\\hat{M}}^{*}(s) = V_{M}^{*}(s)$ .\n\n\nWe prove this by showing that optimal policies under both MDPs induce the same value function.\n\n\nLemma 3.2. For any policy  $\\pi_M$  on  $M$ , there exists a policy  $\\pi_{\\hat{M}}$  on  $\\hat{M}$  such that  $V_{\\hat{M}}^{\\pi_{\\hat{M}}}(s) = V_{M}^{\\pi_{M}}(s)$ ,  $\\forall s$ , and for any policy  $\\pi_{\\hat{M}}$  on  $\\hat{M}$  there exists a policy  $\\pi_M$  on  $M$  such that  $V_{\\hat{M}}^{\\pi_{\\hat{M}}}(s) = V_{M}^{\\pi_{M}}(s)$ $\\forall s$ .\n\n\nFor this lemma we introduce the notion of fundamental actions, which are actions which correspond to sets of actions which have the same state and reward transition distributions in a given state. We\n\n\n![Figure 2: Approach Overview. Our proposed approach Latent Action Q-Learning (LAQ) starts with a dataset of  $(s, s', r)$  triples. Using the latent action learning process, each sample is assigned a latent action  $\\hat{a}$ . Q-learning on the dataset of quadruples produces a value function,  $V(s)$ . Behaviors are derived from the value function through densified RL, or by guiding low-level controllers.](images/d303a71ed1a85a6f2e44677b980f3cd6eae365305065f1caa64337d6c40dce2c.jpg)\n\n\nutilize the equivalence of fundamental actions between MDPs to construct a policy in the new MDP which induces the same value function as a given policy in the original MDP. We provide proofs for Theorem 3.1 and Lemma 3.2 in Section A.1.\n\n\n# 3.2 GRIDWORLD CASE STUDY\n\n\nWe validate these results in a tabular grid world setting. In particular, we measure the error in learned value functions and the induced behavior, when conducting Q-learning with datasets with different qualities of intervening actions. The agent needs to navigate from the top left of a  $6 \\times 6$  grid to the bottom right with sparse reward. We generate data from a fixed, sub-optimal policy to evaluate all methods in an offline fashion (additional details in Section A.5). We generate 20K episodes with this policy, and obtain value functions using Q-learning under the following 4 choices for the intervening actions: (1) Ground truth actions ( $V_{\\mathrm{gt}}$ ), (2) One action ( $V_{\\mathrm{one - act}}$ , ammounts to TD(0) policy evaluation), (3)  $4 \\times$  refinement of original action space ( $V_{4 \\times - \\mathrm{gt}}$ ). We modify the data so that each sample for a particular action in the original action space is randomly mapped to one of 4 actions in the augmented space. (4) Obfuscated actions ( $V_{\\mathrm{impure - act}}$ ). Original action with probability 0.5, and a random action with probability 0.5.\n\n\nFigure 1 shows the learned value functions under these different action labels, and reports the MSE from the true value function, along with induced behavior. In line with our expectations,  $V_{4 \\times \\mathrm{gt}}$  which uses a refinement of the actions is able to recover the optimal value function.  $V_{\\mathrm{one - act}}$  fails to recover the optimal value function, and recovers the value corresponding to the policy that generated the data.  $V_{\\mathrm{impure - act}}$ , under noise in action labels (non-refinement) also fails to recover the optimal value function. Furthermore, the behavior implied by  $V_{\\mathrm{impure - act}}$  and  $V_{\\mathrm{one - act}}$  is sub-optimal. We also analyze the effect of the action impurity on learned values and implied behavior. Behavior becomes increasingly inaccurate as action impurity increases. More details in Section A.3.\n\n\n# 4 LATENT ACTION Q-LEARNING\n\n\nOur analysis in Section 3 motivates the design of our approach for learning behaviors from state-only experience. Our proposed approach decouples learning into three steps: mining latent actions from state-only trajectories, using these latent actions for Q-learning to obtain value functions, and learning a policy to act according to the learned value function. As per our analysis, if learned latent actions are a state-conditioned refinement of the original actions, Q-learning will result in good value functions, that will lead to good behaviors. Refer to Algorithm 1 for details.\n\n\n# 4.1 LATENT ACTIONS FROM FUTURE PREDICTION\n\n\nGiven a dataset  $\\mathbf{D}$  of observations streams  $\\dots, o_t, o_{t+1}, \\dots$ , the goal in this step is to learn latent actions that are a refinement of the actual actions that the agent executed. We learn these latent actions through future prediction. We train a future prediction model  $f_\\theta$ , that maps the observation  $o_t$  at time  $t$ , and a latent action  $\\hat{a}$  (from a set  $\\hat{\\mathbf{A}}$  of discrete latent actions) to the observation  $o_{t+1}$  at time  $t+1$ , i.e.  $f_\\theta(o_t, \\hat{a})$ .  $f$  is trained to minimize a loss  $l$  between the prediction  $f_\\theta(o_t, \\hat{a})$  and the ground truth observation  $o_{t+1}$ .  $\\hat{a}$  is treated as a latent variable during learning. Consequently,  $f_\\theta$  is trained using a form of expectation maximization (Bishop, 2006). Each training sample  $(o_t, o_{t+1})$  is assigned to the action that leads to the lowest loss under the current forward model. The function  $f_\\theta$  is optimized to minimize the loss under the current latent action assignment. More formally,\n\n\nthe loss for each sample  $(o_t, o_{t+1})$  is:  $L(o_t, o_{t+1}) \\coloneqq \\min_{\\hat{a} \\in \\hat{\\mathbf{A}}} l(f_\\theta(o_t, \\hat{a}), o_{t+1})$ . We minimize  $\\sum_{(o_t, o_{t+1}) \\in \\mathbf{D}} L(o_t, o_{t+1})$  over the dataset to learn  $f_\\theta$ .\n\n\nLatent action  $\\hat{a}_t$  for observation pairs  $(o_t, o_{t+1})$  are obtained from the learned function  $f_\\theta$  as:  $\\arg \\min_{\\hat{a} \\in \\hat{\\mathbf{A}}} l(f_\\theta(o_t, \\hat{a}), o_{t+1})$ . Choice of the function  $f_\\theta$  and loss  $l$  vary depending on the problem. We use L2 loss in the observation space (low-dimensional states, or images).\n\n\n# 4.2 Q-LEARNING WITH LATENT ACTIONS\n\n\nLatent actions mined from Section 4.1 allow us to complete the given  $(o_{t}, o_{t+1}, r_{t})$  tuples into  $(o_{t}, \\hat{a}_{t}, o_{t+1}, r_{t})$  quadruples for use in Q-learning Watkins (1989). As our actions are discrete we can easily adopt any of the existing Q-learning methods for discrete action spaces (e.g. Mnih et al. (2013)). Though, we note that this Q-learning still needs to be done in an offline manner from pre-recorded state-only experience. While we adopt the most basic Q-learning in our experiments, more sophisticated versions that are designed for offline Q-learning (e.g. Kumar et al. (2020); Fujimoto et al. (2019)) can be directly adopted, and should improve performance further. Value functions are obtained from the Q-functions as  $V(s) = \\max_{\\hat{a} \\in \\hat{\\mathbf{A}}} Q(s, \\hat{a})$ .\n\n\n# 4.3 BEHAVIORS FROM VALUE FUNCTIONS\n\n\nGiven a value function, our next goal is to derive behaviors from the learned value function. In general, this requires access to the transition function of the underlying MDP. Depending on what assumptions we make, this will be done in the following two ways.\n\n\nDensified Reinforcement Learning. Learning a value function from state-only experience can be extremely valuable when a dense reward function for the underlying task is not readily available. In this case, using the learned value function can densify sparse reward functions, making previously intractable RL problems solvable. Specifically, we use the value function to create a potential-based shaping function  $F(s,s^{\\prime}) = V(s^{\\prime}) - V(s)$ , based on Ng et al. (1999), and construct an augmented reward function  $r^{\\prime}(s,a,s^{\\prime}) = r(s,a,s^{\\prime}) + F(s,s^{\\prime})$ . Our experiments show that using this densified reward function speeds up behavior acquisition.\n\n\nDomain Specific Low-level Controllers. In more specific scenarios, it may be possible to employ hand designed low-level controllers in conjunction with a model that can predict the next state  $s'$  on executing any of low-level controllers. In such a situation, behavior can directly be obtained by picking the low-level controller that conveys the agent to the state  $s'$  that has the highest value under the learned  $V(s)$ . Such a technique was used by Chang et al. (2020). We show results in their setup.\n\n\n# 5 EXPERIMENTS\n\n\nWe design experiments to assess the quality of value functions learned by LAQ from undirected state-only experience. We do this in two ways. First, we measure the extent to which value functions learned with LAQ without ground truth information agree with value functions learned with Q-learning with ground truth action information. This provides a direct quality measure and allows us to compare different ways of arriving at the value function: other methods in the literature (D3G (Edwards et al., 2020)), and simpler alternatives of arriving at latent actions. Our second evaluation measures the effectiveness of LAQ-learned value functions for deriving effective behavior in different settings: when using it as a dense reward, when using it to guide low-level controllers, and when transferring behavior across embodiments. Where possible, we compare to behavior cloning (BC) with privileged ground truth actions. BC with ground truth actions serves as an upper bound on the performance of state-only imitation learning methods (BCO from Torabi et al. (2018a), ILPO from Edwards et al. (2019), etc.) and allows us to indirectly compare with these methods.\n\n\nTest Environments. Our experiments are conducted in five varied environments: the grid world environment from Section 3, the Atari game Freeway from Bellemare et al. (2013), 3D visual navigation in realistic environments from Chang et al. (2020); Savva et al. (2019), and two continuous control tasks from Fu et al. (2020)'s D4RL: Maze2D (2D continuous control navigation), and FrankaKitchen (dexterous manipulation in a kitchen). For Maze2D and FrankaKitchen environments, we also consider embodiment transfer, where we seek to learn policies for an ant and a hook\n\n\n![2D Grid World Figure 3: We experiment with five environments: 2D Grid World, Freeway (Atari), 3D Visual Navigation, Maze2D (2D Continuous Control), and FrankaKitchen. Top right corner of Maze2D and FrankaKitchen, shows the embodiments for cross-embodiment transfer (ant and hook, respectively).](images/6b40b0699a1ad04be9e602899b3715522bae403879488ff9c51a64891948ab30.jpg)\n\n\n![Freeway](images/f975d9fe888a7b6bf27d7c0c86f46869b4735d844018d02e23869e312bcb2b6c.jpg)\n\n\n![3D Visual Nav.](images/5d840d5657c00505f9070cfc3ad3336cc27b3dd9ebf17fd098187d9c037f9186.jpg)\n\n\n![2D Maze](images/a318ea9379c330deac310df5105f9545367809ebcb26917d0b217bbe80d2682a.jpg)\n\n\n![Kitchen](images/25bc14fb87f235502eb9a81af19715e22fee2c18c51d2f4e21c603dd9f36fb22.jpg)\n\n\nTable 1: We report Spearman's correlation coefficients for value functions learned using various methods with DQN, against a value function learned offline using ground-truth actions (DQN for discrete action environments, and DDPG for continuous action environments). The Ground Truth Actions column shows Spearman's correlation coefficients between two different runs of offline learning with ground-truth actions. See Section 5.1. Details on model selection in Section A.8.\n\n\n<table><tr><td>Environment</td><td>D3G</td><td>Single Action</td><td>Clustering</td><td>Clustering (Diff)</td><td>Latent Actions</td><td>Ground Truth Actions</td></tr><tr><td>2D Grid World</td><td>0.959</td><td>0.093</td><td>0.430</td><td>1.000</td><td>0.985</td><td>1.000</td></tr><tr><td>Freeway</td><td>- (image input)</td><td>0.886</td><td>0.945</td><td>0.902</td><td>0.961</td><td>0.970</td></tr><tr><td>3D Visual Navigation</td><td>- (image input)</td><td>0.641</td><td>0.722</td><td>0.827</td><td>0.927</td><td>0.991</td></tr><tr><td>2D Continuous Control</td><td>0.673</td><td>0.673</td><td>0.613</td><td>0.490</td><td>0.844</td><td>0.851</td></tr><tr><td>Kitchen Manipulation</td><td>0.854</td><td>0.858</td><td>0.818</td><td>0.815</td><td>0.905</td><td>0.901</td></tr></table>\n\n\nrespectively from the observation-only experience of a point mass and the Franka arm. Together, these environments test our approach on factors that make policy learning hard: continuous control, high-dimensional observations and control, complex real world appearance, 3D geometric reasoning, and learning across embodiments. See visualizations in Figure 3, details in Section A.4.\n\n\nExperimental Setup For each setting, we work with a pre-collected dataset of experience in the form of state, next state and reward triplets,  $(o_{t}, o_{t+1}, r_{t})$ . We use our latent-variable forward model (Section 4.1) and label triplets with latent actions to obtain quadruples  $(o_{t}, \\hat{a}_{t}, o_{t+1}, r)$ . We perform Q-learning on these quadruples to obtain value functions  $V(s)$ , which are used to acquire behaviors either through densified RL by interacting with the environment, or through use of domains-specific low-level controllers. We use the ACME codebase (Hoffman et al., 2020) for experiments.\n\n\nLatent Action Quality. In line with the theory developed in Section 3, we want to establish how well our method learns a refinement of the underlying action space. To assess this, we study the state-conditioned purity of the partition induced by the learned latent actions. It computes the proportion of the most frequent action of all ground truth actions mapped to a latent action (for any given state). Overall, our method is effective at finding refinement of the original action space. It achieves higher state-conditioned purity than a single action and clustering. In high-dimensional image observation settings, it surpasses baselines by a wide margin. More details in Section A.6.\n\n\n# 5.1 QUALITY OF LEARNED VALUE FUNCTIONS\n\n\nWe evaluate the quality of the value functions learned through LAQ. We use as reference the value function  $V_{\\mathrm{gt - act}}$ , obtained through offline Q-learning (DDPG for continuous action cases) with true ground truth actions i.e.  $(o_t, a_t, o_{t + 1}, r_t)$ . For downstream decision making, we only care about the relative ordering of state values. Thus, we measure the Spearman's rank correlation coefficient between the different value functions. Table 1 reports the Spearman's coefficients of value functions obtained using different action labels: single action, clustering, latent actions (ours), and with ground truth actions. We also report Spearman's correlations of value functions produced using\n\n\n![](images/3c1ed100aa60c2b1bac1464854503a57ac6db864ba918d3be049b4be8c2e0ad6.jpg)\n\n\n![](images/1da2f0a70c03beb05d75c477b9158bcae39281ce2f866b2fb4fdba83998a9f1b.jpg)\n\n\n![](images/60d3d9eed5787f8a3e42c84eb04b4c083cabfcae1504e74e7ba0a62db985754d.jpg)\n\n\n![Figure 4: We show learning curves for acquiring behavior using learned value functions. We compare densified RL (Section 4.3) with sparse RL and BC/BC+RL. See Section 5.2 for more details. Results are averaged over 5 seeds and show  $\\pm$  standard error.](images/93559af5c98a6201ca88e619dc12dd6689428efdfb26240ee336592f5167be1d.jpg)\n\n\nD3G (Edwards et al., 2020). In all settings we do Q-learning over the top 8 dominant actions, except for Freeway, where using the top three actions stabilized training.\n\n\nOur method out performs all baselines in settings with high-dimensional image observations (3D Visual Navigation, Freeway). In state based settings, where clustering state differences is a helpful inductive bias, we see that our method is still on-par with, or superior to clustering state differences and even D3G, which predicts state differences.\n\n\n# 5.2 USING VALUE FUNCTIONS FOR DOWNSSTREAM TASKS\n\n\nOur next experiments test the utility of LAQ-learned value functions for acquiring goal-driven behavior. We first describe the 3 settings that we use to assess this, and then summarize our takeaways.\n\n- - Using value functions as dense reward functions. We combine sparse task reward with the learned value function as a potential function (Section 4.3). We scale up the sparse task rewards by a factor of 5 so that behavior is dominated by the task reward once policy starts solving the task. Figure 4 measures the learning sample efficiency. We compare to only using the sparse reward, behavior cloning (BC) with ground truth actions, and BC followed by spare reward RL.\n\n- - Using value functions to learn behavior of an agent with a different embodiment. Decoupling the learning of value function and the policy has the advantage that learned value functions can be used to improve learning across embodiment. We demonstrate this, we keep the same task, but change the embodiment of the agent in Maze2D and FrankaKitchen environments. Note that we do not assume access to ground truth actions in these experiments either. For Maze2D, the point-mass is replaced with a 8-DOF quadrupedal ant. For FrankaKitchen, the Franka arm is replaced with a position-controlled hook. We may need to define how we query the value function when the embodiment (and the underlying state space) changes. For the ant in Maze2D, the location (with 0 velocity) of the ant body is used to query the value function learned with the point-mass. For the hook in FrankaKitchen, the value function is able to transfer directly as both settings observe end-effector position and environment state. We report results in Figure 5.\n\n- - Using value functions to guide low-level controllers. Learned value functions also have the advantage that they can be used directly at test time to guide the behavior of low-level controllers. We do this experiment in context of 3D visual navigation in a scan of a real building and use the branching environment from Chang et al. (2020). We follow their setup and replace their value functions with ones learned using LAQ in their hierarchical policy, and compare the efficiency of behavior encoded by the different value functions.\n\n\nLAQ value functions speed up downstream learning. Learning plots in Figure 4 show that LAQ-learned value functions speed up learning in the different settings over learning simply with sparse rewards (orange line vs. blue line). In all settings except Freeway, our method not only learns more quickly than sparse reward, but converges to a higher mean performance.\n\n\nLAQ discovers stronger behavior than imitation learning when faced with undirected experience. An advantage of LAQ over other imitation-learning based methods such as BCO (Torabi et al., 2018a) and ILPO (Edwards et al., 2019) is LAQ's ability to learn from sub-optimal or undirected experience. To showcase this, we compare the performance of LAQ with behavior cloning (BC) with ground truth actions. Since BCO and ILPO recover ground truth actions to perform behavior\n\n\nTable 2: We report Spearman's correlation coefficients for value functions learned using either DQN or BCQ, against a value function learned offline using BCQ with ground-truth actions. The Ground Truth Actions column shows Spearman's correlation coefficients between two different runs of offline learning with ground-truth actions. See Section 5.1.\n\n\n<table><tr><td>Environment</td><td>Single Action</td><td>Clustering</td><td>Clustering (Diff)</td><td>Latent Actions</td><td>Ground Truth Actions</td></tr><tr><td>2D Continuous Control (DQN)</td><td>0.664</td><td>0.431</td><td>0.312</td><td>0.807</td><td>0.765</td></tr><tr><td>2D Continuous Control (BCQ)</td><td>0.710</td><td>0.876</td><td>0.719</td><td>0.927</td><td>0.990</td></tr></table>\n\n\n![Figure 5: Learning curves for acquiring behavior with value functions across embodiment. Results averaged over 50 seeds and show  $\\pm$  standard error.](images/087143d13b455d86beef4691470e3194123003c3b8fa7ec288caabfdb8470626.jpg)\n\n\n![](images/f4e65eac12d70ce9f13815ceda10f052af731c9f83331b79228ef552adb2155d.jpg)\n\n\n![](images/89a1d2537be34fe1ff7b68a1ece5f4c6268c9e91a6c45e3df0d57c3d7f4dbb2e.jpg)\n\n\n<table><tr><td></td><td colspan=\"2\">Interaction</td></tr><tr><td></td><td>Samples</td><td>SPL</td></tr><tr><td>Vone-act (Chang et al., 2020)</td><td>0</td><td>0.53</td></tr><tr><td>Vcluster-act</td><td>0</td><td>0.57</td></tr><tr><td>VIatent-act</td><td>0</td><td>0.82</td></tr><tr><td>VInverse-act (Chang et al., 2020)</td><td>40K</td><td>0.95</td></tr></table>\n\n\nFigure 6: Visualization of trajectories and SPL numbers in the 3D visual navigation environment.\n\n\ncloning (BC), BC with ground truth actions serves as an upper bound on the performance of all methods in this class. Learning plots in Figure 4 shows the effectiveness of LAQ over BC and BC followed by fine-tuning with sparse rewards for environments where the experience is undirected (Maze2D, and GridWorld). For Freeway, the experience is fairly goal-directed, thus BC already works well. A similar trend can be seen in the higher Spearman's coefficient for LAQ vs.  $V_{\\text{one-act}}$  in Table 1. LAQ discovers stronger behavior than imitation learning when faced with undirected data.\n\n\nLAQ is compatible with other advances in batch RL. Although LAQ uses the most basic Q-Learning as our offline value learning method, it is compatible with recent more advanced offline RL value-learning methods (such as CQL (Kumar et al., 2020) and BCQ (Fujimoto et al., 2019)). We validate by simply swapping to using (discrete) BCQ with our latent actions. Figures. 4 and 5 show that LAQ with BCQ is the strongest method, outperforming ours with DQN, and D3G, on Maze2D and embodiment transfer environments. Analysis of Spearman's correlations in Table 2 shows the same trend as before with latent actions: better than single actions, and clustering variants. Note also that use of BCQ leads to value functions with better Spearman's correlations than DQN.\n\n\nLAQ value functions allow transfer across embodiments. Figure 5 shows learning plots of agents trained with cross-embodiment value functions. LAQ-densified rewards functions, speed-up learning and consistently guide to higher reward solutions than sparse task rewards, or D3G.\n\n\nLAQ compares favorably to D3G. We compare LAQ and D3G (a competing state-only method) in different ways. D3G relies on generating potential future states. This is particularly challenging for image observations, and D3G doesn't show results with image observations. In contrast, LAQ maps state transitions to discrete actions, and hence works with image observations as our experiments show. Even in scenarios with low-dimensional state inputs, LAQ learns better value functions than D3G, as evidences by Spearman's correlations in Table 1, and learning plots in Figure 4 and Figure 5.\n\n\nLAQ value functions can guide low-level controllers for zero-shot control: We report the SPL for 3D navigation using value functions combined with low-level controllers in Figure 6. We report the efficiency of behavior induced by LAQ learned value functions as measured by the SPL metric from Anderson et al. (2018) (higher is better). The branching environment has two goal states, one optimal and one sub-optimal. The demonstrations there-in were specifically designed to emphasize the utility of knowing the intervening actions. Simple policy evaluation leads sub-optimal behavior (SPL of 0.53) and past work relied on using an inverse model to label actions (Chang et al., 2020) to derive better behavior. This inverse model itself required  $40K$  interactions with the environment\n\n\nfor training, and boosted the SPL to 0.95. LAQ is able to navigate to the optimal goal (w/ SPL 0.82) but without the  $40K$  online interaction samples necessary to acquire the inverse model. It also performs better than clustering transitions, doing which achieves an SPL of 0.57. The improvement is borne out in visualizations in Figure 6. LAQ correctly learns to go to the nearer goal, even when the underlying experience came from a policy that preferred the further away goal.\n\n\n# 6 DISCUSSION\n\n\nOur theoretical characterization and experiments in five representative environments showcase the possibility and potential of deriving goal-directed signal from undirected state-only experience. Here we discuss some scenarios which are fundamentally hard, and some avenues for future research.\n\n\nNon-deterministic MDPs. Our theoretical result relies on a refinement where state-action transition probabilities are matched. However, the latent action mining procedure in LAQ results in deterministic actions. Thus, for non-deterministic MDPs (where executing the same action in the same state takes the agent to different next state), LAQ will be unable to achieve a strict refinement, leading to sub-optimal value functions. However, note that this limitation isn't specific to our method, but applies to any deterministic algorithm that seeks to learn from observation only data. We formalize this concept and provide a proof in Section A.2.\n\n\nConstraining evaluation of  $V(s)$  to within its domain. LAQ learns a value function  $V(s)$  over the set of states that were available in the experience dataset, and as such its estimates are only accurate within this set. In situations where the experience dataset doesn't span the entire state space, we may need to assess where the predictions of  $V(s)$  are valid to avoid degenerate solutions due to OOD observations. We discuss a density based model solution we used for this problem in Section A.4.\n\n\nOffline RL Validation. Validation (e.g. when to stop training) is a known issue in offline RL (Gulcehre et al., 2020). Like other offline RL methods, LAQ suffers from it too. LAQ's use of Q-learning makes it compatible to recent advances (Kumar et al., 2021) that tackle this validation problem.\n\n\n# 7 RELATED WORK\n\n\nOur work focuses on batch (or offline) RL with state-only data using a latent-variable future prediction model. We survey works on batch RL, state-only learning, and future prediction.\n\n\nBatch Reinforcement Learning. As the field of reinforcement learning has matured, batch RL (Lange et al., 2012; Levine et al., 2020) has gained attention as a component of practical systems. A large body of work examines solutions the problem of extrapolation error in batch RL settings. Advances in these works are complementary to our approach, as substantiated by our experiments with BCQ. A more detailed discussion of batch RL methods can be found in Section A.0.\n\n\nState-only Learning. Past works have explored approaches for dealing with the lack of actions in offline RL when given goal-directed or undirected state-only experience. Works in the former category rely on high quality behavior in the data, and suffer on sub-optimal data. Past work on state-only learning from undirected experience relies on either domain knowledge or state reconstruction and only show results with low dimensional states. See Section A.0 for continued discussion.\n\n\nFuture Prediction Models. Past work from Oh et al. (2015); Agrawal et al. (2016); Finn et al. (2016) (among many others) has focused on building action conditioned forward models in pixel and latent spaces. Yet other work in computer vision studies video prediction problems (Xue et al., 2016; Castrejon et al., 2019). Given the uncertainty in future prediction, these past works have pursued variational (or latent variable) approaches to make better predictions. Our latent variable future model is inspired from these works, but we explore its applications in a novel context.\n\n\nLatent MDP Learning One way to interpret our method is that of learning an approximate MDP homomorphism (Taylor et al., 2008; Ravindran & Barto, 2004). Other works have explored learning latent homorphic MDPs. These methods tend to focus on learning equivalent latent state spaces (Li et al., 2006; Givan et al., 2003). Most similarly to our work (van der Pol et al., 2020) also learns a latent action space, but relies on ground truth action data to do so.\n\n\n# REFERENCES\n\n- Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pp. 104-114. PMLR, 2020. 18, 19\n\n- Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. In NeurIPS, 2016. 9\n\n- Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, and Amir Zamir. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018. 8\n\n- Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning, 2021. 13\n\n- M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, jun 2013. 5\n\n- Christopher M Bishop. Pattern recognition and machine learning. Springer, 2006. 4\n\n- Lluis Castrejon, Nicolas Ballas, and Aaron Courville. Improved conditional vrnns for video prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7608-7617, 2019. 9\n\n- Matthew Chang, Arjun Gupta, and Saurabh Gupta. Semantic visual navigation by watching youtube videos. In NeurIPS, 2020. 5, 7, 8, 13, 18, 19, 22\n\n- Ashley D. Edwards and Charles L. Isbell. Perceptual values from observation. CoRR, abs/1905.07861, 2019. 2, 13\n\n- Ashley D Edwards, Himanshu Sahni, Yannick Schroecker, and Charles L Isbell. Imitating latent policies from observation. In ICML, 2019. 5, 7, 13\n\n- Ashley D. Edwards, Himanshu Sahni, Rosanne Liu, Jane Hung, Ankit Jain, Rui Wang, Adrien Ecoffet, Thomas Miconi, Charles Isbell, and Jason Yosinski. Estimating  $q(s,s')$  with deep deterministic dynamics gradients. In ICML, 2020. 2, 5, 7, 13\n\n- Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. arXiv preprint arXiv:1605.07157, 2016. 9\n\n- Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. 5, 13, 18\n\n- Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In ICML, 2019. 5, 8, 13\n\n- Robert Givan, Thomas Dean, and Matthew Greig. Equivalence notions and model minimization in markov decision processes. Artificial Intelligence, 147(1-2):163-223, 2003. 9\n\n- Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gomez Colmenarejo, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, Gabriel Dulac-Arnold, Jerry Li, Mohammad Norouzi, Matt Hoffman, Ofir Nachum, George Tucker, Nicolas Heess, and Nando deFreitas. Rl unplugged: Benchmarks for offline reinforcement learning, 2020. 9, 13\n\n- Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas Abdelmaleki, Albin Cassirer, Fan Yang, Kate Baumli, et al. Acme: A research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020. 6\n\n- Ashish Kumar, Saurabh Gupta, and Jitendra Malik. Learning navigation subroutines by watching videos. In CoRL, 2019a. 13\n\n- Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. In NeurIPS, 2019b. 13\n\n- Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In NeurIPS, 2020. 5, 8, 13\n\n- Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for offline model-free robotic reinforcement learning. In CoRL, 2021. 9\n\n- Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement learning, pp. 45-73. Springer, 2012. 2, 9\n\n- Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. 2, 9\n\n- Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a unified theory of state abstraction for mdps. ISAIM, 4:5, 2006. 9\n\n- Ajay Mandlekar, Fabio Ramos, Byron Boots, Silvio Savarese, Fei-Fei Li, Animesh Garg, and Dieter Fox. IRIS: implicit reinforcement without interaction at scale for learning control from offline robot manipulation data. In ICRA, 2020. 13\n\n- Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. 5\n\n- Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, pp. 278-287. Morgan Kaufmann, 1999. 5\n\n- Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. arXiv preprint arXiv:1507.08750, 2015. 9\n\n- Ilija Radosavovic, Xiaolong Wang, Lerrel Pinto, and Jitendra Malik. State-only imitation learning for dexterous manipulation. CoRR, abs/2004.04650, 2020. 13\n\n- Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Offline reinforcement learning from images with latent space models. CoRR, abs/2012.11547, 2020. 13\n\n- Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit gradients. arXiv preprint arXiv:1909.04630, 2019. 13\n\n- Balaraman Ravindran and Andrew G Barto. Approximate homomorphisms: A framework for non-exact minimization in markov decision processes. 2004. 9\n\n- Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A platform for embodied AI research. In ICCV, 2019. 5, 18, 19\n\n- Pierre Sermanet, Kelvin Xu, and Sergey Levine. Unsupervised perceptual rewards for imitation learning. In RSS, 2017. 13\n\n- Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. Concept2robot: Learning manipulation concepts from instructions and human demonstrations. In RSS, 2020. 13\n\n- Avi Singh, Larry Yang, Chelsea Finn, and Sergey Levine. End-to-end robotic reinforcement learning without reward engineering. In RSS, 2019. 13\n\n- Shuran Song, Andy Zeng, Johnny Lee, and Thomas Funkhouser. Grasping in the wild: Learning 6 dof closed-loop grasping from low-cost demonstrations. Robotics and Automation Letters, 2020. 13\n\n- Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249. 2, 14\n\n- Jonathan Taylor, Doina Precup, and Prakash Panagaden. Bounding performance loss in approximate mdp homomorphisms. Advances in Neural Information Processing Systems, 2008. 9\n\n- Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In *IJCAI*, 2018a. 5, 7, 13\n\n- Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation. CoRR, abs/1807.06158, 2018b. 13\n\n- Faraz Torabi, Garrett Warnell, and Peter Stone. Adversarial imitation learning from state-only demonstrations. In AAMAS, pp. 2229-2231. International Foundation for Autonomous Agents and Multiagent Systems, 2019a. 13\n\n- Faraz Torabi, Garrett Warnell, and Peter Stone. Imitation learning from video by leveraging proprioception. arXiv preprint arXiv:1905.09335, 2019b. 13\n\n- Elise van der Pol, Thomas Kipf, Frans A. Oliehoek, and Max Welling. Plannable approximations to MDP homomorphisms: Equivariance under actions. In International Conference on Autonomous Agents and Multiagent Systems, AAMAS, 2020. 9\n\n- Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989. 1, 2, 5, 15\n\n- Annie Xie, Avi Singh, Sergey Levine, and Chelsea Finn. Few-shot goal inference for visuomotor learning and planning. In Conference on Robot Learning, pp. 40-52. PMLR, 2018. 13\n\n- Tianfan Xue, Jiajun Wu, Katherine L Bouman, and William T Freeman. Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks. arXiv preprint arXiv:1607.02586, 2016.9\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638569.0839453", "msecs": "83.0", "relativeCreated": "165914.44277763367", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Logical Languages Accepted by Transformer Encoders with Hard Attention\n\n# Abstract\nWe contribute to the study of formal languages that can be recognized by transformer encoders. We focus on two self-attention mechanisms: (1) UHAT (Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention Transformers). UHAT encoders are known to recognize only languages inside the circuit complexity class ${\\sf AC}^0$, i.e., accepted by a family of poly-sized and depth-bounded boolean circuits with unbounded fan-ins. On the other hand, AHAT encoders can recognize languages outside ${\\sf AC}^0$), but their expressive power still lies within the bigger circuit complexity class ${\\sf TC}^0$, i.e., ${\\sf AC}^0$-circuits extended by majority gates. We first show a negative result that there is an ${\\sf AC}^0$-language that cannot be recognized by an UHAT encoder. On the positive side, we show that UHAT encoders can recognize a rich fragment of ${\\sf AC}^0$-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates. This logic, includes, for example, all regular languages from ${\\sf AC}^0$. We then show that AHAT encoders can recognize all languages of our logic even when we enrich it with counting terms. Using these results, we obtain a characterization of which counting properties are expressible by UHAT and AHAT, in relation to regular languages.\n\n\n# 1 INTRODUCTION\n\n\nTransformers have revolutionized natural language processing by facilitating the efficient and effective modeling of intricate contextual relationships within text (Vaswani et al., 2017). This remarkable capability has sparked numerous investigations into the potential boundaries of transformers' power (Hahn, 2020; Yao et al., 2021; P\u00e9rez et al., 2021; Weiss et al., 2021; Hao et al., 2022; Chiang & Cholak, 2022; Bhattamishra et al., 2020; Chiang et al., 2023; Merrill et al., 2022; Merrill & Sabharwal, 2023; Strobl, 2023). One natural method for addressing this question is to explore the classes of formal languages that these architectures can recognize. This approach provides an insight into their strengths and limitations. The response to this question naturally relies on the specific features allowed within transformer encoders. These encompass the interplay between encoders and decoders, the kind of functions used for positional encodings and attention mechanisms, and considerations of fixed or unbounded precision, among other factors.\n\n\nWhile the capacity of transformers that incorporate both encoders and decoders to recognize languages is well understood today (indeed, such architectures are Turing-complete and can thus recognize any computable language (P\u00e9rez et al., 2021)), a precise characterization of the languages accepted by transformer encoders is lacking. Unique Hard Attention Transformers (UHAT) are a class of transformer encoders that has been a subject of many recent papers. As was shown by Hao et al. (2022), UHATs recognize only languages in  $\\mathsf{AC}^0$ , i.e., recognized by families of Boolean circuits of unbounded fan-in that have constant depth and polynomial size. Intuitively, this means that\n\n\nUHATs are weak at \"counting\" (more precisely, reasoning about the number of occurrences of various letters in the input word). For example, consider the following languages: majority and parity. The first one corresponds to the set of words over alphabet  $\\{a, b\\}$  for which the majority of positions are labeled by  $a$ , while the second checks if the number of positions labeled  $a$  is even. That these languages are not in  $\\mathsf{AC}^0$  follows from a groundbreaking result in circuit complexity theory (Furst et al., 1981; Ajtai, 1983)). Hence, they are neither accepted by UHATs. However, which fragment of the  $\\mathsf{AC}^0$  languages can actually be recognized by UHATs remains an unresolved question.\n\n\nWe start by showing that not all  $\\mathsf{AC}^0$  languages can be accepted by UHATs. This is obtained by combining results of Ajtai (1983) and Hahn (2020). Based on the previous observation, we focus on identifying a rich fragment of  $\\mathsf{AC}^0$  that can in fact be embedded into the class of UHATs. To achieve this, we use the characterization of  $\\mathsf{AC}^0$  as the class of languages expressible in FO(All), the extension of first-order logic (FO) with all numerical predicates defined in relation to the linear order of a word (Immerman, 1999). We show that UHATs recognize all languages definable in FO(Mon), the restriction of FO(All) with unary numerical predicates only (Barrington et al., 2005). The logic FO(Mon) is highly expressive. Unlike FO, it can express non-regular languages like  $\\{a^n b^n \\mid n > 0\\}$ . Remarkably, it contains all regular languages within  $\\mathsf{AC}^0$ , which includes examples like  $(aa)^*$  \u2014 a language not definable in FO. Additionally, our result subsumes the result of Yao et al. (2021), where it is shown that Dyck languages of bounded nested depth can be recognized by UHATs. These languages are regular and belong to  $\\mathsf{AC}^0$ , hence they are expressible in FO(Mon).\n\n\nTo establish the result that UHATs recognize all languages definable in  $\\mathrm{FO(Mon)}$ , we take a slightly circuitous route: rather than directly formulating  $\\mathrm{FO(Mon)}$  sentences as UHATs, we show that each formula in LTL(Mon), the extension of linear temporal logic (LTL) (Clarke et al., 2018) with arbitrary unary numerical predicates, can be equivalently represented as an UHAT. The proof for  $\\mathrm{FO(Mon)}$  then derives from Kamp's seminal theorem (Kamp, 1968), which establishes the equivalence between languages definable in FO and LTL. The advantage of dealing with LTL, in contrast to FO, lies in the fact that all LTL formulas are unary in nature, i.e., they are interpreted as sets of positions on a word, unlike FO formulas which possess arbitrary arity. This property aligns well with the expressive capabilities of UHATs, facilitating a proof through structural induction.\n\n\nWhile the fact that UHAT is in  $\\mathsf{AC}^0$  implies limited counting abilities of such encoders, recent work has shown that a slight extension of the hard attention mechanism can help in recognizing languages outside  $\\mathsf{AC}^0$  (Hao et al., 2022). Instead of using unique hard attention, this model uses average hard attention (AHAT), which refers to the idea that the attention mechanism returns the uniform average value among all positions that maximize the attention. To what extent does AHAT enrich the counting ability of UHAT? In answering this question, we introduce a logic named LTL( $\\mathbf{C}, +$ ), which is an extension of LTL(Mon) that naturally incorporates counting features. We show that any language that can be defined within LTL( $\\mathbf{C}, +$ ) can also be identified by an AHAT. The logic LTL( $\\mathbf{C}, +$ ) can express interesting languages lying outside  $\\mathsf{AC}^0$  including majority and parity. More generally, our result implies that AHATs are equipped with a powerful counting ability: all permutation-closed languages over a binary alphabet and all permutation closures of regular languages (which are in general not context-free) can be recognized by AHATs.\n\n\nAs a corollary, we provide a characterization of the \"counting properties\" of regular languages which can be captured by UHAT and AHAT. Two approaches for understanding counting properties of regular languages can be found in formal language theory: (1) Parikh-equivalence, and (2) taking letter-permutation closure. Both approaches \"remove\" the ordering from the input string, and as a result only the letter-count (more popularly called Parikh images) of an input string matters. In the setting of (1) (e.g. see (Parikh, 1966; Kozen, 1997)), a machine is allowed to try all letter reordering of the input string  $w$ , and accepts iff the machine accepts some letter reordering of  $w$ . According to the well-known Parikh's Theorem (Parikh, 1966), each context-free language can in this way be captured by a regular language, e.g.,  $\\{0^n 1^n : n \\geq 0\\}$  can be captured by  $(01)^*$ . We show in this paper that each regular language is Parikh-equivalent to an UHAT language, despite the fact that PARITY is not in UHAT. In the setting of (2), a machine must accept all letter permutations of an input string  $w$ . The letter-permutation closure of a regular language is not necessarily regular, e.g., such a closure language of  $(abc)^*$  consists of all strings with the same number of  $a$ 's,  $b$ 's, and  $c$ 's, which is not even context-free. In this setting, although UHAT cannot capture regular languages like PARITY, AHAT can surprisingly capture all regular languages.\n\n\nRelated work. There has been very little research on identifying logical languages that can be accepted by transformers. The only example we are aware of is the recent work by Chiang et al. (2023), in which a variant of first-order logic with counting quantifiers is demonstrated to be embeddable into transformer encoders with a soft attention mechanism. The primary distinction between their work and our results is the choice of the attention mechanism. Additionally, the logic examined in their paper does not have access to the underlying word order being considered. This implies that some simple languages, such as  $a^*b^*$ , which are definable in FO, are not definable in their logic.\n\n\nDue to the space constraints, some of the proofs are omitted and can be found in the online version of this paper (Barcel\u00f3 et al., 2023).\n\n\n# 2 BACKGROUND NOTIONS AND RESULTS\n\n\n# 2.1 TRANSFORMER ENCODERS\n\n\nAn encoder layer is a function that takes a sequence of vectors,  $\\mathbf{v}_0,\\dots ,\\mathbf{v}_{n - 1}$ , in  $\\mathbb{R}^d$  as input, where  $d\\geq 0$ . It produces an output sequence of vectors of the same length,  $\\mathbf{v}_0^{\\prime},\\ldots ,\\mathbf{v}_{n - 1}^{\\prime}$ , in  $\\mathbb{R}^e$ , with  $e\\geq 0$ . The length of the sequence,  $n$ , can be arbitrary, but input and output dimensions,  $d$  and  $e$ , are fixed for an encoder layer. For the first part of the paper we employ a unique hard attention mechanism, meaning that a position only attends to the element with the highest attention score.\n\n\nFormally, an encoder layer with unique hard attention is given by two affinte transformations  $A, B \\colon \\mathbb{R}^d \\to \\mathbb{R}^d$  and one feed-forward neural network  $\\mathcal{N} \\colon \\mathbb{R}^{2d} \\to \\mathbb{R}^e$  with ReLU activation function. For  $i \\in \\{0, \\dots, n-1\\}$ , we set\n\n\n$$\n\n$$\n\\mathbf {a} _ {i} \\leftarrow \\mathbf {v} _ {j _ {i}},\n$$\n\n$$\n\n\nwhere  $j_{i}\\in \\{0,\\dots ,n - 1\\}$  is the minimum element that maximizes the attention score  $\\langle A\\mathbf{v}_i,B\\mathbf{v}_j\\rangle$  over  $j\\in \\{0,\\ldots ,n - 1\\}$ . The  $a_{i}$ s are often known as attention vectors. After that, we set\n\n\n$$\n\n$$\n\\mathbf {v} _ {i} ^ {\\prime} \\leftarrow \\mathcal {N} (\\mathbf {v} _ {i}, \\mathbf {a} _ {i}), \\quad i = 0, \\dots , n - 1.\n$$\n\n$$\n\n\nIt is well-known that feed-forward neural networks with ReLU activation can express the function  $\\max \\{x,y\\}$ . Thus, we may assume that  $\\mathcal{N}$  can be an arbitrary composition of affine functions with max.\n\n\nTransformer encoder. A unique hard attention transformer encoder (UHAT) is defined simply as the repeated application of encoder layers with unique hard attention.\n\n\n# 2.2 LANGUAGES ACCEPTED BY TRANSFORMER ENCODERS\n\n\nNext, we define how a transformer can be used to accept languages over a finite alphabet. This requires extending transformer encoders with three features: a function for representing alphabet symbols as vectors (which, for the purposes of this paper, we represent as one-hot encodings), another function that provides information about the absolute positions of these symbols within the input word, and a vector that is used for checking whether the word should be accepted or not. The function that provides information about positions is often referred to as a positional encoding, and it is essential for recognizing properties of ordered sequences of vectors. In fact, without positional encoding, encoders treat input sequences as invariant to permutations (P\u00e9rez et al., 2021).\n\n\nConsider a finite alphabet  $\\Sigma$  and let  $T$  be an UHAT that takes a sequence of vectors over  $\\mathbb{R}^d$  as input and converts it into a sequence of vectors over  $\\mathbb{R}^e$ . A language  $L \\subseteq \\Sigma^+$  is accepted by  $T$ , if there is an embedding function  $f \\colon \\Sigma \\to \\mathbb{R}^d$ , a positional encoding function  $p \\colon \\mathbb{N} \\times \\mathbb{N} \\to \\mathbb{R}^d$ , and a vector  $\\mathbf{t} \\in \\mathbb{R}^e$ , such that for every  $\\bar{w} \\in L$  we have  $T'(\\bar{w}) > 0$ , and for every  $w \\in \\Sigma^+ \\setminus L$  we have  $T'(\\bar{w}) < 0$ . Here,  $T': \\Sigma^+ \\to \\mathbb{R}$  is defined as follows. Let  $\\bar{w} = a_0 \\dots a_{n-1} \\in \\Sigma^n$ , and suppose the output of  $T$  when given the input sequence  $f(a_0) + p(0,n)$ , ...,  $f(a_{n-1}) + p(n-1,n)$  is the sequence  $\\mathbf{v}_0, \\dots, \\mathbf{v}_{n-1}$ . Then we set  $T'(\\bar{w}) = \\langle \\mathbf{t}, \\mathbf{v}_0 \\rangle$ .\n\n\n# 2.3 FIRST ORDER LOGIC ON WORDS\n\n\nWe assume familiarity with first-order logic (FO). Let  $\\Sigma$  be a finite alphabet. A word  $\\bar{w} = a_0 \\cdots a_{n-1}$  in  $\\Sigma^+$  is represented as a structure  $S_{\\bar{w}}$  whose domain is  $\\{0, \\ldots, n-1\\}$ . This structure includes a binary relation  $<$  that is interpreted as the linear order on the domain, and for each symbol  $a \\in \\Sigma$ , there is a unary relation  $P_a$  containing positions  $i = 0, \\ldots, n-1$  where  $a_i = a$ . Given an FO sentence over words, that is, an FO formula without free variables, we denote the language of all words  $\\bar{w} \\in \\Sigma^+$  satisfying  $S_{\\bar{w}} \\models \\phi$  as  $L(\\phi)$ . If an  $L \\subseteq \\Sigma^+$  satisfies  $L = L(\\phi)$ , for some FO sentence  $\\phi$ , then we say that  $L$  is definable in FO.\n\n\nExample 1. First-order logic (FO) enables us to define certain languages of interest. Here, we present an illustrative example. Initially, we recognize that we can employ FO to define a relation  $\\operatorname{first}(x) := \\neg \\exists y(y < x)$  that exclusively holds true at the first position of a word. Correspondingly, we can define a relation  $\\operatorname{last}(x) := \\neg \\exists y(x < y)$  that holds solely at the last position of the word. Moreover, it is possible to define a binary relation  $\\operatorname{succ}(x,y) := x < y \\land \\neg \\exists z(x < z \\land z < y)$ , which defines the successor relation within the domain. With these expressions, we can show that FO is capable of defining the language  $(ab)^{+}$ :\n\n\n$$\n\n$$\n\\exists x \\left(\\operatorname {f i r s t} (x) \\wedge P _ {a} (x)\\right) \\wedge \\exists x \\left(\\operatorname {l a s t} (x) \\wedge P _ {b} (x)\\right) \\wedge \\forall x \\forall y \\left(\\operatorname {s u c c} (x, y) \\rightarrow \\left(P _ {a} (x) \\leftrightarrow P _ {b} (y)\\right)\\right).\n$$\n\n$$\n\n\nThat is, the first symbol of the word is an  $a$ , the last one is a  $b$ , every  $a$  is followed by a  $b$ , and every  $b$  is preceded by an  $a$ .\n\n\n# 2.4 UNARY NUMERICAL PREDICATES\n\n\nIt is known that FO sentences can only define regular languages. In turn, there are regular languages that are not definable in FO. An example is the language  $(aa)^*$ , which contains those words formed solely by the symbol  $a$  that are of even length. However, there is a straightforward extension of FO that can define this language: all we need to do is add unary predicate  $\\mathrm{even}(x)$ , which holds true at position  $i$  in a word if and only if  $i$  is even. In fact, extending FO with the predicate  $\\mathrm{even}(x)$  allows us to define the language  $(aa)^*$  using the following formula, which indicates that the last symbol in the word satisfies the unary predicate even:  $\\forall x P_a(x) \\land \\forall y (\\mathrm{last}(y) \\to \\mathrm{even}(y))$ .\n\n\nThe extension of FO with unary numerical predicates can then be useful for defining languages. We define a unary numerical predicate  $\\Theta$  as an infinite family of functions\n\n\n$$\n\n$$\n\\theta_ {n}: \\{0, \\dots , n \\} \\rightarrow \\{0, 1 \\}, \\qquad n > 0.\n$$\n\n$$\n\n\nGiven a word  $\\bar{w}$  in  $\\Sigma^{+}$  of length  $n$ , for  $n > 0$ , we have that the predicate  $\\Theta(x)$  holds in position  $i$  in  $\\bar{w}$  if and only if  $\\theta_{n}(i) = 1$  (so far, we do not use the value of  $\\theta_{n}$  at  $n$  as positions are numbered from 0 to  $n - 1$ . We will use this value in Section 4). Notice that under our definition, the truth of a unary numerical predicate at position  $i$  in the word  $\\bar{w}$  depends not only on  $i$  but also on the length of the word  $\\bar{w}$ . As we will explore further, this characteristic is advantageous for defining interesting languages in FO extended with arbitrary unary numerical predicates. Following the literature, we write FO(Mon) for such an extension (Barrington et al., 2005).\n\n\nExample 2. Consider, for example, the non-regular language  $\\{a^n b^n \\mid n > 0\\}$ . We show that it can be expressed in  $\\mathrm{FO(Mon)}$  with the help of a unary numerical predicate  $\\Theta(x)$  such that  $\\theta_n(i) = 1$  iff  $n$  is even and  $i = n/2 - 1$ . In fact, it suffices to use the formula:  $\\exists x (\\Theta(x) \\land P_a(x) \\land \\forall y (y < x \\rightarrow P_a(y)) \\land \\forall y (x < y \\rightarrow P_b(y)))$ . This formula expresses that the middle point  $i$  of  $\\bar{w}$  exists, is labeled as  $a$ , and all positions smaller than  $i$  are also labeled  $a$ , while all positions larger than  $i$  are labeled as  $b$ . This example illustrates the significance of unary numerical predicates depending on both the position and the length of the word over which the formula is evaluated.\n\n\nThe definition of the language  $L(\\phi) \\subseteq \\Sigma^{+}$  defined by an FO(Mon) sentence  $\\phi$  is analogous to the one we provided for FO.\n\n\n# 3  $AC^0$  LANGUAGES ACCEPTED BY UHATS\n\n\n# 3.1 NOT ALL LANGUAGES IN  $\\MATHSF{AC}^0$  ARE ACCEPTED BY UHATS.\n\n\nHao et al. (2022) proved that languages accepted by UHATs belong to the circuit complexity class  $\\mathsf{AC}^0$ , i.e., the class of languages accepted by families of Boolean circuits of unbounded fan-in,\n\n\nconstant depth, and polynomial size. We combine results by Ajtai (1983) and Hahn (2020) to show that the opposite is not the case, i.e., there are  $AC^0$  languages that are not accepted by UHATs.\n\n\nAs shown in Ajtai (1983), there is an  $\\mathsf{AC}^0$ -family of circuits  $\\{C_n\\colon \\{0,1\\} ^n\\to \\{0,1\\}\\}_{n\\in \\mathbb{N}}$  such that for all  $n$ , the circuit  $C_n$  accepts all strings with at least  $2n / 3$  ones and rejects all strings with at most  $n / 3$ . Consider a language approximate majority, consisting of strings accepted by circuits from  $\\{C_n\\}$ . This language is in  $\\mathsf{AC}^0$  by construction. However, as we state next, it cannot be recognized by an UHAT. This result is proved by using a property of UHATs established in Hahn (2020).\n\n\nProposition 1. There is no UHAT that accepts the language approximate majority.\n\n\nViola (2009) shows that  $\\{C_n\\}$  can be made polynomial-time computable, which implies the existence of a polynomial-time computable language from  $\\mathsf{AC}^0$  that cannot be accepted by an UHAT.\n\n\n# 3.2 MAIN RESULT: FO(MON) LANGUAGES ARE ACCEPTED BY UHATS\n\n\nProposition 1 tells us that not all  $\\mathsf{AC}^0$  languages are accepted by UHATs. In this section, we identify a significant subset of  $\\mathsf{AC}^0$  languages that can be accepted by UHATs. To accomplish this, we rely on the characterization of the class  $\\mathsf{AC}^0$  as those languages that can be defined in FO extended with arbitrary numerical predicates. Our main result establishes that as long as we restrict ourselves to unary numerical predicates, translation into UHATs is possible.\n\n\nTheorem 1. Let  $\\Sigma$  be a finite alphabet and  $\\phi$  an FO(Mon) sentence over words from the alphabet  $\\Sigma$ . There is an UHAT that accepts  $L(\\phi)$ .\n\n\nProving this result by induction on  $\\mathrm{FO(Mon)}$  formulas, which would be the most natural approach to tackle the problem, turns out to be difficult. The challenge arises because the  $\\mathrm{FO(Mon)}$  formulas obtained by induction can have arbitrary arity, and transformer encoders do not seem capable of handling the requirements imposed by such formulas. To address this issue, we take a different approach. We employ Kamp's Theorem, which establishes that the languages definable in FO are precisely those that are definable in linear temporal logic (LTL) (Kamp, 1968).\n\n\n# 3.3 USING LTL(MON) TO PROVE OUR MAIN RESULT\n\n\nWe first explain how LTL is defined, as this is crucial to understanding the remainder of the paper. Let  $\\Sigma$  be a finite alphabet. LTL formulas over  $\\Sigma$  are defined as follows: if  $a \\in \\Sigma$ , then  $a$  is an LTL formula. Additionally, LTL formulas are closed under Boolean combinations. Finally, if  $\\phi$  and  $\\psi$  are LTL formulas, then  $\\mathbf{X}\\phi$  and  $\\phi \\mathbf{U}\\psi$  are also LTL formulas. Here,  $\\mathbf{X}$  is referred to as the next operator, and  $\\mathbf{U}$  as the until operator.\n\n\nLTL formulas are unary, i.e., they are evaluated over positions within a word. Let  $\\bar{w} = a_0\\cdots a_{n-1}$  be a word in  $\\Sigma^{+}$ , and let  $i = 0,\\dots,n-1$ . We define the satisfaction of an LTL formula  $\\phi$  over  $\\bar{w}$  at position  $i$ , written as  $(\\bar{w},i)\\models \\phi$ , inductively as follows (omitting Boolean combinations):\n\n- -  $(\\bar{w}, i) \\models a$  if and only if  $a = a_i$ , for  $a \\in \\Sigma$ .\n\n- -  $(\\bar{w}, i) \\models \\mathbf{X}\\phi$  if and only if  $i < n - 1$  and  $(\\bar{w}, i + 1) \\models \\phi$ . In other words,  $\\phi$  holds in the next position after  $i$  (if such a position exists).\n\n- -  $(\\bar{w}, i) \\models \\phi \\mathbf{U} \\psi$  if and only if there exists a position  $j = i, \\dots, n - 1$  for which  $(\\bar{w}, j) \\models \\psi$  and such that  $(\\bar{w}, k) \\models \\phi$  for every  $k$  with  $i \\leq k < j$ . That is,  $\\phi$  holds starting from position  $i$  until the first position where  $\\psi$  holds (and a position where  $\\psi$  holds must exist).\n\n\nWe can extend LTL with unary numerical predicates in the same way we did it for FO. Formally, we define  $\\mathrm{LTL}(\\mathrm{Mon})$  as the extension of LTL with every formula of the form  $\\Theta$ , for  $\\Theta$  a unary numerical predicate. We write  $(\\bar{w},i)\\models \\Theta$  to denote that  $\\theta_{n}(i) = 1$ , where  $n$  is the length of  $\\bar{w}$ . If  $\\phi$  is an LTL(Mon) formula over  $\\Sigma$ , we write  $L(\\phi)$  for the set of words  $\\bar{w}\\in \\Sigma^{+}$  with  $(\\bar{w},0)\\models \\phi$ .\n\n\nKamp's Theorem establishes that for every FO sentence  $\\phi$  there exists an LTL formula  $\\psi$  such that  $L(\\phi) = L(\\psi)$ , and vice-versa. It is straightforward to see that this property extends to the logics FO(Mon) and LTL(Mon).\n\n\nProposition 2. (Kamp, 1968) For every FO(Mon) sentence  $\\phi$  there exists an LTL(Mon) formula  $\\psi$  such that  $L(\\phi) = L(\\psi)$ , and vice-versa.\n\n\nOur proof of Theorem 1 is then derived directly from Proposition 2 and the following result.\n\n\nProposition 3. Let  $\\Sigma$  be a finite alphabet and  $\\phi$  an LTL(Mon) formula defined over words from the alphabet  $\\Sigma$ . There is an UHAT  $T$  that accepts  $L(\\phi)$ .\n\n\nBefore proving this result, we make the following important remark regarding the positional encoding  $p$  used by  $T$  to accept  $L(\\phi)$ . On a pair  $(i,n) \\in \\mathbb{N} \\times \\mathbb{N}$  with  $i < n$ , we have that  $p(i,n)$  is composed of elements  $i, 1/(i+1), (-1)^i, \\cos\\left(\\pi(1-2^{-i})/10\\right), \\sin\\left(\\pi(1-2^{-i})/10\\right)$ , and  $\\theta_n(i)$ , for every unary numerical predicate  $\\Theta$  mentioned in  $\\phi$ .\n\n\nProof of Proposition 3. Let  $\\phi$  be a formula of LTL(Mon). We say that a UHAT realizes  $\\phi$  position-wise if, given a word  $\\bar{w} = a_0\\ldots a_{n - 1}\\in \\Sigma^+$ , the UHAT outputs a sequence:\n\n\n$$\n\n$$\n\\mathbb {I} \\left\\{\\left(\\bar {w}, 0\\right) \\vDash \\phi \\right\\}, \\mathbb {I} \\left\\{\\left(\\bar {w}, 1\\right) \\vDash \\phi \\right\\}, \\dots , \\mathbb {I} \\left\\{\\left(\\bar {w}, n - 1\\right) \\vDash \\phi \\right\\};\n$$\n\n$$\n\n\nthat is, a binary word indicating for which positions  $\\phi$  is true on  $\\bar{w}$  and for which is false. We show by structural induction that every LTL(Mon) formula is realizable position-wise by some UHAT.\n\n\nLet us consider first the base cases. If  $\\phi = a$ , for some  $a \\in \\Sigma$ , our goal is to obtain a sequence:\n\n\n$$\n\n$$\n\\mathbb {I} \\{a _ {0} = a \\}, \\mathbb {I} \\{a _ {1} = a \\}, \\dots , \\mathbb {I} \\{a _ {n - 1} = a \\}.\n$$\n\n$$\n\n\nThis can easily be achieved by using a one-hot encoding as the embedding function. In turn, if  $\\phi = \\Theta$ , for  $\\Theta$  a unary numerical predicate, then  $\\phi$  can be realized position-wise using the corresponding positional encoding  $p(i,n) = \\theta_n(i)$ .\n\n\nWe continue with Boolean combinations. They can be implemented position-wise by compositions of affine transformation and max as follows:  $\\neg x = 1 - x^{\\prime}$  and  $x\\vee y = \\frac{\\max\\{2x - 1,2y - 1\\} + 1}{2}$ .\n\n\nFor the cases when our formula is of the form  $\\mathbf{X}\\phi$  or  $\\phi \\mathbf{U}\\psi$ , we need the following lemma.\n\n\nLemma 1. There is an UHAT that transforms each  $x_0, \\ldots, x_{n-1} \\in \\{0,1\\}$  as follows:\n\n\n$$\n\n$$\nx _ {0}, \\dots , x _ {n - 2}, x _ {n - 1} \\mapsto x _ {0}, \\dots , x _ {n - 2}, 0.\n$$\n\n$$\n\n\nLet us assume now that our formula is of the form  $\\mathbf{X}\\phi$ . It is enough to design a unique hard attention layer in which attention is always maximized at the next position. More precisely, we construct an UHAT that outputs a sequence of vectors  $\\mathbf{v}_1,\\ldots ,\\mathbf{v}_n\\in \\mathbb{R}^3$ , and a linear transformation  $A\\colon \\mathbb{R}^3\\to \\mathbb{R}^3$ , such that  $\\arg \\max_{j\\in \\mathbb{N}}\\langle A\\mathbf{v}_i,\\mathbf{v}_j\\rangle = \\{i + 1\\}$ , for  $i = 0,\\dots ,n - 2$ . This will allow us to \"send\"  $\\mathbb{I}\\{(\\bar{w},i + 1)\\models \\phi \\} = \\mathbb{I}\\{(\\bar{w},i)\\models \\mathbf{X}\\phi \\}$  to the  $i$ th position, for  $i = 0,\\dots ,n - 2$ . It only remains then to apply Lemma 1 to obtain  $0 = \\mathbb{I}\\{(\\bar{w},n - 1)\\models \\mathbf{X}\\phi \\}$  at the last position.\n\n\nUsing our positional encoding and an affine position-wise transformation, we can obtain:\n\n\n$$\n\n$$\n\\mathbf {v} _ {i} = \\left(\\cos \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), \\sin \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), (- 1) ^ {i} \\cdot 1 0\\right).\n$$\n\n$$\n\n\nLet  $A$  be a linear transformation that inverts the sign of the third coordinate. Observe that:\n\n\n$$\n\n$$\n\\langle A \\mathbf {v} _ {i}, \\mathbf {v} _ {j} \\rangle = \\cos \\left(\\frac {\\pi (2 ^ {- i} - 2 ^ {- j})}{1 0}\\right) + (- 1) ^ {i + j + 1} \\cdot 1 0.\n$$\n\n$$\n\n\nWe claim that, for a fixed  $i$ , this quantity is maximized at  $j = i + 1$ . First, those  $js$  that have the same parity as  $i$  (in particular,  $j = i$ ) cannot achieve the maximum because the second term is  $-10$ . For  $js$  with a different parity, we have  $\\langle A\\mathbf{v}_i,\\mathbf{v}_j\\rangle = \\cos \\left(\\pi (2^{-i} - 2^{-j}) / 10\\right) + 10$ . Since all angles are in  $[- \\pi /10,\\pi /10]$ , this quantity is maximized when  $|2^{-i} - 2^{-j}|$  is minimized. For  $j < i$ , the last quantity is at least  $2^{-i}$ , and for  $j > i$ , the minimum of this quantity is  $2^{-i - 1}$ , achieved at  $j = i + 1$ .\n\n\nLet us finally assume that our formula is of the form  $\\phi \\mathbf{U}\\psi$ . Observe that the value of  $\\phi \\mathbf{U}\\psi$  at position  $i$  can be computed as follows: we go to the right, starting from the  $i$ th position, until we see a position  $j_{i}$  where either  $\\phi$  is false, or  $\\psi$  is true, or this is the last position. Then  $\\phi \\mathbf{U}\\psi$  holds at  $i$  if and only if  $\\psi$  holds at  $j_{i}$ . That is,  $(\\bar{w}, i) \\models$  if and only if  $(\\bar{w}, j_{i}) \\models \\psi$ , where  $j_{i} \\in \\{i, \\dots, n-1\\}$  is the minimal position with  $\\tau(j) = 1$ , where, in turn, the sequence  $\\tau$  is defined by  $\\tau(i) = \\mathbb{I}\\{(\\bar{w}, i) \\models \\neg \\phi \\vee \\psi\\} \\vee \\mathbb{I}\\{i = n-1\\}$ , for  $i = 0, \\dots, n-1$ . To compute the sequence  $\\tau$ , we first compute  $\\phi \\wedge \\neg \\psi$\n\n\nposition-wise (we can do that because we already have  $\\phi$  and  $\\psi$  at our disposal), then we add the conjunction with  $\\mathbb{I}\\{i\\neq n - 1\\}$  by Lemma 1, and then we take the negation of the resulting sequence.\n\n\nTo show the lemma, it is enough to create a unique hard attention layer, where for every position  $i$  the attention is maximized at  $j_{i}$ . Using our positional encoding and the induction hypothesis, we can obtain a sequence of vectors  $\\mathbf{v}_1,\\ldots ,\\mathbf{v}_n\\in \\mathbb{R}^4$  such that:\n\n\n$$\n\n$$\n\\mathbf {v} _ {i} = \\Big (\\cos \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), \\sin \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), 1, \\tau (i) \\Big).\n$$\n\n$$\n\n\nConsider a linear transformation  $B\\colon \\mathbb{R}^4\\to \\mathbb{R}^4$  such that\n\n\n$$\n\n$$\nB \\mathbf {v} _ {i} = \\Big (\\cos \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), \\sin \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), 1 0 \\tau (i), 0 \\Big).\n$$\n\n$$\n\n\nObserve that  $\\langle \\mathbf{v}_i, B\\mathbf{v}_j \\rangle = \\cos \\left( \\frac{\\pi (2^{-i} - 2^{-j})}{10} \\right) + 10\\tau(j)$ . We claim that this expression is maximized at  $j = j_i$ . First, because of the last term in it, it cannot be maximized at  $j$  with  $\\tau(j) = 0$ . It remains to show that among the  $j$ 's with  $(\\bar{w}, j) \\neq \\phi$ , this quantity is minimized on the minimal  $j$  which is at least  $i$ . In fact, in this case we have  $\\langle \\mathbf{v}_i, B\\mathbf{v}_j \\rangle = \\cos \\left( \\frac{\\pi (2^{-i} - 2^{-j})}{10} \\right) + 10$ . All the angles in question are in  $[-\\pi / 10, \\pi / 10]$ , so the cosine is maximized when  $|2^{-i} - 2^{-j}|$  is minimized. Now, this absolute value is at least  $2^{-i}$  when  $j < i$ . In turn, this absolute value is smaller than  $2^{-i}$  for  $j \\geq i$ , and it is the smaller the smaller is  $j$ , as required.\n\n\n# 3.4 APPLICATIONS OF OUR MAIN RESULT\n\n\nWe show two applications of our main result. First, UHATs accept all regular languages in  $\\mathsf{AC}^0$ . Second, UHATs are strictly more expressive than regular and context-free languages in terms of the acceptance of languages up to letter-permutation.\n\n\nRegular languages in  $\\mathsf{AC}^0$ . There is an important fragment of  $\\mathrm{FO(Mon)}$  which is interesting in its own right. This is the logic  $\\mathrm{FO(Mod)}$ , i.e., the extension of FO with unary numerical predicates of the form  $\\mathsf{Mod}_p^r$ , for  $p > 1$  and  $0 \\leq r \\leq p - 1$ . We have that  $\\mathsf{Mod}_p^r(i) = 1$  if and only if  $i \\equiv r \\pmod{p}$ . In fact, by using a characterization given in Barrington et al. (1992), one can show that the languages definable in  $\\mathrm{FO(Mod)}$  are precisely the regular languages within  $\\mathsf{AC}^0$ . Then:\n\n\nCorollary 1. Let  $L \\subseteq \\Sigma^{+}$  be a regular language in  $\\mathsf{AC}^0$ . There is an UHAT that accepts  $L$ .\n\n\nRecognizing regular languages up to letter-permutation. Although not all regular languages are accepted by UHATs (e.g. parity), we can use Theorem 1 to show that, up to letter-permutation, UHAT is in fact strictly more powerful than regular and context-free languages.\n\n\nTo formalize our result, we recall the notion of semilinear sets and the Parikh image of a language. A linear set  $S$  is a subset of  $\\mathbb{N}^d$  (for some positive integer  $d$ , called dimension) of the form  $\\mathbf{v}_0 + \\sum_{i=1}^{r} \\mathbf{v}_i \\mathbb{N} := \\{\\mathbf{v}_0 + \\sum_{i=1}^{r} k_i \\mathbf{v}_i : k_1, \\ldots, k_r \\in \\mathbb{N}\\}$  for some vectors  $\\mathbf{v}_0, \\ldots, \\mathbf{v}_r \\in \\mathbb{N}^d$ . A semilinear set  $S$  over  $\\mathbb{N}^d$  is a finite union of linear sets over  $\\mathbb{N}^d$ . Semilinear sets have a very tight connection to formal languages through the notion of the Parikh image a language  $L$  (Parikh, 1966), which intuitively corresponds to the set of \"letter-counts\" of  $L$ . More precisely, consider the alphabet  $\\Sigma = \\{a_1, \\ldots, a_d\\}$  and a language  $L$  over  $\\Sigma$ . For a word  $w \\in \\Sigma$ , let  $|w|_{a_i}$  denotes the number of occurrences of  $a_i$  in  $w$ . The Parikh image  $\\mathcal{P}(L)$  of  $L$  is defined to be the set of tuples  $\\mathbf{v} = (|w|_{a_1}, \\ldots, |w|_{a_d}) \\in \\mathbb{N}^d$  for some word  $w \\in L$ . For example, if  $L = \\{a^n b^n : n \\geq 0\\}$  and  $L' = (ab)^*$ , then  $\\mathcal{P}(L) = \\mathcal{P}(L')$ . In this case, we say that  $L$  and  $L'$  are Parikh-equivalent. Note that  $L'$  is regular, while  $L$  is context-free but not regular. This is not a coincidence based on the celebrated Parikh's Theorem (cf. Parikh (1966), also see Kozen (1997)).\n\n\nProposition 4 (Parikh (1966)). The Parikh images of both regular and context-free languages coincide with semilinear sets.\n\n\nIn other words, although context-free languages are strict superset of regular languages, they are in fact equally powerful up to letter-permutation. What about UHATs? We have that they are strictly more powerful than regular and context-free languages up to letter-permutation.\n\n\nProposition 5. Each regular language has a Parikh-equivalent language accepted by an UHAT. In turn, there is an UHAT language with no Parikh-equivalent regular language.\n\n\n# 4 L A N G U A G E S BEYOND AC\n\n\nTransformer encoders with unique hard attention can only recognize languages in  $\\mathsf{AC}^0$ , but a slight extension of the attention mechanism allows to recognize languages lying outside such a class (Hao et al., 2022). In this section, we show that in fact such an extended model can recognize all languages definable in a powerful logic that extends LTL with counting features. This logic can express interesting languages outside  $\\mathsf{AC}^0$ , such as majority and parity.\n\n\n# 4.1 AVERAGE HARD ATTENTION\n\n\nFor the results in this section, we consider an extended version of transformer encoders that utilize an average hard attention mechanism (P\u00e9rez et al., 2021; Hao et al., 2022). Following the literature, we call these AHAT.\n\n\nEncoder layer with average hard attention. As before, these layers are defined by two affine transformations,  $A,B\\colon \\mathbb{R}^d\\to \\mathbb{R}^d$  and one feed-forward neural network  $\\mathcal{N}\\colon \\mathbb{R}^{2d}\\to \\mathbb{R}^e$  with ReLU activation function. For every  $i\\in \\{0,\\dots ,n - 1\\}$ , we define  $S_{i}$  as the set of positions  $j\\in \\{0,\\dots ,n - 1\\}$  that maximize  $\\langle A\\mathbf{v}_i,B\\mathbf{v}_j\\rangle$ . We then set\n\n\n$$\n\n$$\n\\mathbf {a} _ {i} \\leftarrow \\left(\\sum_ {j \\in S _ {i}} \\mathbf {v} _ {j}\\right) / | S _ {i} |.\n$$\n\n$$\n\n\nAfter that, we set  $\\mathbf{v}_i^{\\prime}\\gets \\mathcal{N}(\\mathbf{v}_i,\\mathbf{a}_i)$ , for each  $i = 0,\\dots ,n - 1$ . That is, attention scores under average hard attention return the uniform average value among all positions that maximize attention.\n\n\nWe also use future positional masking that allows us to take into account only positions up to  $i$ . If the future positional masking is used, the sets  $S_{i}$  are defined as sets of positions  $j \\in \\{0,1,\\dots,i\\}$  that maximize  $\\langle A\\mathbf{v}_i,B\\mathbf{v}_j\\rangle$ . Positional masks have been employed on several occasions in theoretical papers (Yao et al., 2021; Bhattamishra et al., 2020; Hao et al., 2022) as well as in practice, for example, for training GPT-2 (Radford et al., 2019).\n\n\n# 4.2 LTL EXTENDED WITH COUNTING TERMS\n\n\nWe present here  $\\mathrm{LTL}(\\mathbf{C}, +)$ , an extension of  $\\mathrm{LTL}(\\mathbf{Mon})$  that allows us to define counting properties over words in a simple manner. This requires the introduction of counting terms as defined next.\n\n\nCounting terms. Suppose  $\\phi$  is a unary formula. Then  $\\overleftarrow{\\# \\phi}$  and  $\\overrightarrow{\\# \\phi}$  are counting terms. The interpretation of these terms in position  $i$  of a word  $\\bar{w}$  of length  $n$  is defined as follows:\n\n\n$$\n\n$$\n\\overleftarrow {\\# \\phi} (\\bar {w}, i) = | \\{j \\in \\{0, \\dots , i \\} \\mid (\\bar {w}, j) \\models \\phi \\} |, \\overrightarrow {\\# \\phi} (\\bar {w}, i) = | \\{j \\in \\{i, \\dots , n - 1 \\} \\mid (\\bar {w}, j) \\models \\phi \\} |.\n$$\n\n$$\n\n\nThat is,  $\\overleftarrow{\\# \\phi}(\\bar{w}, i)$  is the number of positions to the left of  $i$  (including  $i$ ) that satisfy  $\\phi$ , while  $\\overrightarrow{\\# \\phi}(\\bar{w}, i)$  is the number of positions to the right of  $i$  (including  $i$ ) that satisfy  $\\phi$ . Notice that, for words of length  $n$ , counting terms take values in  $\\{0, 1, \\ldots, n\\}$ .\n\n\nCounting formulas. With counting terms and unary numerical predicates we can create new formulas in the following way. Let  $\\phi$  be a unary formula and  $\\Theta$  a unary numerical predicate. We define new formulas  $\\Theta (\\overleftarrow{\\# \\phi})$  and  $\\Theta (\\overrightarrow{\\# \\phi})$ . The interpretation of such formulas on position  $i$  of a word  $\\bar{w}$  of length  $n$  is as follows:\n\n\n$$\n\n$$\n(\\bar {w}, i) \\models \\Theta (\\overleftarrow {\\# \\phi}) \\Leftrightarrow \\theta_ {n} (\\overleftarrow {\\# \\phi} (\\bar {w}, i)) = 1 \\qquad (\\bar {w}, i) \\models \\Theta (\\overrightarrow {\\# \\phi}) \\Leftrightarrow \\theta_ {n} (\\overrightarrow {\\# \\phi} (\\bar {w}, i)) = 1.\n$$\n\n$$\n\n\nThat is, the number of positions to the left (resp., right) of  $i$  (including  $i$ ) that satisfy  $\\phi$  satisfies the predicate  $\\Theta$ . As counting terms can take value  $n$ , the value of  $\\theta_{n}$  on  $n$  becomes useful.\n\n\nWe also incorporate into our logic the possibility of checking linear inequalities with integer coefficients over counting terms. More specifically, for any finite set of unary formulas  $\\phi_1,\\ldots ,\\phi_{k_j},\\psi_1,\\ldots ,\\psi_k$  , and for any coefficients  $c_{1},\\ldots ,c_{k},d_{1},\\ldots ,d_{k}\\in \\mathbb{Z}$  we can create a formula:  $\\sum_{j = 1}^{k}c_{j}\\cdot \\overline{\\# \\phi_{j}} +\\sum_{j = 1}^{k}d_{j}\\cdot \\overrightarrow{\\# \\psi_{j}}\\geq 0$  , which is interpreted as follows:\n\n\n$$\n\n$$\n(\\bar {w}, i) \\vDash \\sum_ {j = 1} ^ {k} c _ {j} \\cdot \\overleftarrow {\\# \\phi_ {j}} + \\sum_ {j = 1} ^ {k} d _ {j} \\cdot \\overrightarrow {\\# \\psi_ {j}} \\geq 0 \\iff \\sum_ {j = 1} ^ {k} c _ {j} \\cdot \\overleftarrow {\\# \\phi_ {j}} (\\bar {w}, i) + \\sum_ {j = 1} ^ {k} d _ {j} \\cdot \\overrightarrow {\\# \\psi_ {j}} (\\bar {w}, i) \\geq 0.\n$$\n\n$$\n\n\nThe logic  $\\mathrm{LTL}(\\mathbf{C}, +)$ . We denote by  $\\mathrm{LTL}(\\mathbf{C}, +)$  the logic that is recursively defined as follows:\n\n- - Every formula  $\\mathrm{LTL}(\\mathrm{Mon})$  is also an  $\\mathrm{LTL}(\\mathbf{C}, +)$  formula.\n\n- - Boolean combinations of  $\\mathrm{LTL}(\\mathbf{C}, +)$  formulas are  $\\mathrm{LTL}(\\mathbf{C}, +)$  formulas.\n\n- - If  $\\phi$  and  $\\psi$  are  $\\mathrm{LTL}(\\mathbf{C}, +)$  formulas, then so are  $\\mathbf{X}\\phi$  and  $\\phi \\mathbf{U}\\psi$ .\n\n- - If  $\\phi$  is an LTL $(\\mathbf{C}, +)$  formula and  $\\Theta$  is a unary numerical predicate, then  $\\Theta(\\overleftarrow{\\# \\phi})$  and  $\\Theta(\\overrightarrow{\\# \\phi})$  are LTL $(\\mathbf{C}, +)$  formulas.\n\n- - If  $\\phi_1, \\ldots, \\phi_k, \\psi_1, \\ldots, \\psi_k$  are formulas of  $\\mathrm{LTL}(\\mathbf{C}, +)$ , then  $\\sum_{j=1}^{k} c_j \\cdot \\overleftarrow{\\# \\phi_j} + \\sum_{j=1}^{k} d_j \\cdot \\overrightarrow{\\# \\psi_j} \\geq 0$ , is a formula of  $\\mathrm{LTL}(\\mathbf{C}, +)$ .\n\n\n# 4.3 LTL(C) DEFINABLE LANGUAGES ARE ACCEPTED BY ENCODERS\n\n\nNext, we state the main result of this section: languages definable by  $\\mathrm{LTL}(\\mathbf{C}, + )$  formulas are accepted by transformer encoders with average hard attention.\n\n\nTheorem 2. Let  $\\Sigma$  be a finite alphabet and  $\\phi$  an LTL $(\\mathbf{C}, +)$  formula defined over words from the alphabet  $\\Sigma$ . There is an AHAT  $T$  that accepts  $L(\\phi)$ .\n\n\nAs a corollary to Theorem 2, we show that AHATs are rather powerful in counting. To make this claim more formal, we study permutation-closed languages, i.e., languages  $L$  such that  $\\bar{v} \\in L$  iff any letter-permutation of  $\\bar{v}$  is in  $L$ . For a language  $L$ , we write  $\\text{perm}(L)$  to be the permutation-closure of  $L$ , i.e.,  $\\text{perm}(L) = \\{\\bar{w} : \\mathcal{P}(\\bar{w}) = \\mathcal{P}(\\bar{v}) \\text{ for some } \\bar{v} \\in L\\}$ . Observe that  $\\text{perm}((abc)^*)$  consists of all strings with the same number of occurrences of  $a, b,$  and  $c$ ; this is not even context-free. Owing to Parikh's Theorem, to recognize  $\\text{perm}(L)$ , where  $L$  is a regular language, an ability to perform letter-counting and linear arithmetic reasoning (i.e. semilinear set reasoning) is necessary. AHATs possess such an ability, as shown by the following corollary.\n\n\nCorollary 2. The permutation closure  $\\operatorname{perm}(L)$  of any regular language  $L$  is accepted by an AHAT. Moreover, any permutation-closed language over a binary alphabet is accepted by an AHAT.\n\n\nBoth majority and parity are permutation-closed and are over a binary alphabet. Hence, by the previous result, they are both accepted by AHATs. While for majority this was known (Hao et al., 2022), the result for parity is new.\n\n\n# 5 CONCLUSIONS AND FUTURE WORK\n\n\nWe have conducted an investigation of the problem of which languages can be accepted by transformer encoders with hard attention. For UHATs, we have demonstrated that while they cannot accept all languages in  $\\mathsf{AC}^0$ , they can still accept all languages in a 'monadic' version of it defined by the logic FO(Mon). Crucial to the proof of this result is the equivalence between FO and LTL, as provided by Kamp's Theorem. In turn, we have shown that AHATs are capable of expressing any language definable in a powerful counting logic,  $\\mathrm{LTL}(\\mathbf{C}, +)$ , that can express properties beyond  $\\mathsf{AC}^0$ . This implies, among other things, that the parity language can be accepted by an AHAT.\n\n\nIn a work of Angluin et al. (2023), contemporaneous to ours, it was shown that the logic FO(Mon) exactly captures languages, accepted by UHATs with positional masking and finite-valued positional encoding. At the same time, with our technique, it can be shown that there are languages beyond FO(Mon) that can be accepted by UHATs with infinite-valued positional encoding, for instance, the language of palindromes. The idea is that since we do not use positional masking, we can use arbitrary order on positions, in particular, one that puts the language of palindromes into FO(Mon). Due to the space constraints, we omit a more detailed discussion of this.\n\n\n# REFERENCES\n\n- Mikl\u00f3s Ajtai.  $\\sum_{1}^{1}$ -formulae on finite structures. Ann. Pure Appl. Log., 24(1):1-48, 1983.\n\n- Dana Angluin, David Chiang, and Andy Yang. Masked hard-attention transformers and boolean rasp recognize exactly the star-free languages, 2023.\n\n- Pablo Barcel\u00f3, Alexander Kozachinskiy, Anthony Widjaja Lin, and Vladimir Podolskii. Logical languages accepted by transformer encoders with hard attention. arXiv preprint arXiv:2310.03817, 2023.\n\n- David A. Mix Barrington, Kevin J. Compton, Howard Straubing, and Denis Th\u00e9rien. Regular languages in nc $^1$ . J. Comput. Syst. Sci., 44(3):478-499, 1992.\n\n- David A. Mix Barrington, Neil Immerman, Clemens Lautemann, Nicole Schweikardt, and Denis Th\u00e9rien. First-order expressibility of languages with neutral letters or: The crane beach conjecture. J. Comput. Syst. Sci., 70(2):101-127, 2005.\n\n- Satwik Bhattachamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to recognize formal languages. In EMNLP, pp. 7096-7116, 2020.\n\n- David Chiang and Peter Cholak. Overcoming a theoretical limitation of self-attention. In ACL, pp. 7654-7664, 2022.\n\n- David Chiang, Peter Cholak, and Anand Pillay. Tighter bounds on the expressivity of transformer encoders. In ICML, volume 202, pp. 5544-5562. PMLR, 2023.\n\n- Edmund M. Clarke, Orna Grumberg, Daniel Kroening, Doron A. Peled, and Helmut Veith. Model checking, 2nd Edition. MIT Press, 2018.\n\n- Merrick L. Furst, James B. Saxe, and Michael Sipser. Parity, circuits, and the polynomial-time hierarchy. In *FOCS*, pp. 260-270. IEEE Computer Society, 1981.\n\n- Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Trans. Assoc. Comput. Linguistics, 8:156-171, 2020.\n\n- Yiding Hao, Dana Angluin, and Robert Frank. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. Trans. Assoc. Comput. Linguistics, 10:800-810, 2022.\n\n- Neil Immerman. Descriptive complexity. Graduate texts in computer science. Springer, 1999.\n\n- H. W. Kamp. Tense Logic and the Theory of Linear Order. PhD thesis, University of California, Los Angeles, 1968.\n\n- Dexter C. Kozen. Automata and Computability. Springer, 1997.\n\n- William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers, 2023.\n\n- William Merrill, Ashish Sabharwal, and Noah A. Smith. Saturated transformers are constant-depth threshold circuits. Trans. Assoc. Comput. Linguistics, 10:843-856, 2022. URL https://transacl.org/ojs/index.php/tacl/article/view/3465.\n\n- Rohit Parikh. On context-free languages. J. ACM, 13(4):570-581, 1966. doi: 10.1145/321356.321364. URL https://doi.org/10.1145/321356.321364.\n\n- Jorge P\u00e9rez, Pablo Barcel\u00f3, and Javier Marinkovic. Attention is turing-complete. J. Mach. Learn. Res., 22:75:1-75:35, 2021.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\n- Lena Strobl. Average-hard attention transformers are constant-depth uniform threshold circuits, 2023.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998-6008, 2017.\n\n- Emanuele Viola. On approximate majority and probabilistic time. Computational Complexity, 18: 337-375, 2009.\n\n- Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In ICML, volume 139, pp. 11080-11090, 2021.\n\n- Shunyu Yao, Binghui Peng, Christos H. Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. In ACLIJCNLP, pp. 3770-3785, 2021.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638570.7087684", "msecs": "708.0", "relativeCreated": "167539.26587104797", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Concept-based Explanations for Out-of-Distribution Detectors\n\n# Abstract\nOut-of-distribution (OOD) detection plays a crucial role in ensuring the safe deployment of deep neural network (DNN) classifiers. While a myriad of methods have focused on improving the performance of OOD detectors, a critical gap remains in interpreting their decisions. We help bridge this gap by providing explanations for OOD detectors based on learned high-level concepts. We first propose two new metrics for assessing the effectiveness of a particular set of concepts for explaining OOD detectors: 1) $\\textit{detection completeness}$, which quantifies the sufficiency of concepts for explaining an OOD-detector's decisions, and 2) $\\textit{concept separability}$, which captures the distributional separation between in-distribution and OOD data in the concept space. Based on these metrics, we propose a framework for learning a set of concepts that satisfy the desired properties of detection completeness and concept separability, and demonstrate the framework's effectiveness in providing concept-based explanations for diverse OOD detection techniques. We also show how to identify prominent concepts that contribute to the detection results via a modified Shapley value-based importance score.\n\n\n# 1 INTRODUCTION\n\n\nIt is well known that machine learning (ML) models can yield uncertain and unreliable predictions on out-of-distribution (OOD) inputs, i.e., inputs from outside the training distribution (Amodei et al., 2016; Goodfellow et al., 2015; Nguyen & O'Connor, 2015). The most common line of defense in this situation is to augment the ML model (e.g., a DNN classifier) with a detector that can identify and flag such inputs as OOD. The ML model can then abstain from making predictions on such inputs (Hendrycks et al., 2019; Lin et al., 2021; Mohseni et al., 2020).\n\n\nCurrently, the problem of explaining the decisions of an OOD detector remains largely unexplored. Much of the focus in learning OOD detectors has been on improving their detection performance (Liu et al., 2020; Mohseni et al., 2020; Lin et al., 2021; Chen et al., 2021; Sun et al., 2021; Cao & Zhang, 2022), but not on improving their explainability. A potential approach would be to run an existing interpretation method for DNN classifiers with ID and OOD data separately, and then inspect the difference between the generated explanations. However, it is not known if an explanation method that is effective for explaining in-distribution class predictions will also be effective for OOD detectors. For instance, feature attributions, the most popular type of explanation (Sundararajan et al., 2017; Ribeiro et al., 2016), may not capture visual differences in the generated explanations between ID and OOD inputs (Adebayo et al., 2020). Moreover, their explanations based on pixel-level activations may not provide the most intuitive form of explanations for humans.\n\n\nThis paper addresses the above research gap by proposing the first method (to our knowledge) to help interpret the decisions of an OOD detector in a human-understandable way. We build upon recent advances in concept-based explanations for DNN classifiers (Ghorbani et al., 2019; Zhou et al., 2018a; Bouchacourt & Denoyer, 2019; Yeh et al., 2020), which offer an advantage of providing explanations in terms of high-level concepts for classification tasks. We make the first effort at extending their utility to the problem of OOD detection. Consider Figure 1 which illustrates our concept-based explanations given inputs which are all classified as \"Dolphin\" by a DNN classifier, but detected as either ID or OOD by an OOD detector. We observe that the OOD detector predicts a certain input as ID when its concept-score patterns are similar to that of ID images from the Dolphin class. Likewise, the detector predicts an input as OOD when its concept-score patterns are very different from that of\n\n\n![(a) Correct detection: ID (or OOD) dolphin image correctly detected as ID (or OOD).](images/22d06372cf2391a4f26ccbc4fbc756fef43003e4abed7b7900ee2f172210b020.jpg)\n\n\n![(b) Wrong detection: ID (or OOD) dolphin image falsely detected as OOD (or ID). Figure 1: Our concept-based explanation for the Energy OOD detector (Liu et al., 2020). On the x-axis, we present the top-5 important concepts that describe the detector's behavior given images classified as \"Dolphin\". Concepts C90 and C1 represent \"dolphin-like skin\" and \"wavy surface of the sea\" respectively (see Figure 12b). ID profile shows the concept score patterns for ID images predicted as Dolphin.](images/8301239a7b63b8db6acebf5217f7b104edaeddbac066a57c3540833fe79d2378.jpg)\n\n\nID inputs. A user can verify whether the OOD detector makes decisions based on the concepts that are aligned with human intuition (e.g., C90 and C1), and that the incorrect detection (as in Figure 1b) is an understandable mistake, not a misbehavior of the OOD detector. Such explanations can help the user evaluate the reliability of the OOD detector and decide upon its adoption in practice.\n\n\nWe aim to provide a general interpretability framework that is applicable to the wide range of OOD detectors in the world. Accordingly, a research question we ask is: without relying on the internal mechanism of an OOD detector, can we determine a good set of concepts that are appropriate for understanding why the OOD detector predicts a certain input to be ID/OOD? A key contribution of this paper is to show that this can be done in an unsupervised manner without any human annotations.\n\n\nWe make the following contributions in this paper:\n\n- - We motivate and propose new metrics to quantify the effectiveness of concept-based explanations for a black-box OOD detector, namely detection completeness and concept separability (\u00a7 2.2, \u00a7 3.1, and \u00a7 3.2).\n\n- - We propose a concept-learning objective with suitable regularization terms that, given an OOD detector for a DNN classifier, learns a set of concepts with good detection completeness and concept separability (\u00a7 3.3);\n\n- - By treating a given OOD detector as black-box, we show that our method can be applied to explain a variety of existing OOD detection methods. By identifying prominent concepts that contribute to an OOD detector's decisions via a modified Shapley value score based on the detection completeness, we demonstrate how the discovered concepts can be used to understand the OOD detector. (\u00a7 4).\n\n\nRelated Work. In the literature of OOD detection, recent studies have designed various scoring functions based on the representation from the final or penultimate layers (Liang et al., 2018; DeVries & Taylor, 2018), or a combination of different internal layers of DNN model (Lee et al., 2018; Lin et al., 2021; Raghuram et al., 2021). A recent survey on generalized OOD detection can be found in Yang et al. (2021). Our work aims to provide post-hoc explanations applicable to a wide range of black-box OOD detectors without modifying their internals. Among different interpretability approaches, concept-based explanation (Koh et al., 2020; Alvarez Melis & Jaakkola, 2018) has gained popularity as it is designed to be better-aligned with human reasoning (Armstrong et al., 1983; Tenenbaum, 1999) and intuition (Ghorbani et al., 2019; Zhou et al., 2018a; Bouchacourt & Denoyer, 2019; Yeh et al., 2020). There have been limited attempts to assess the use of concept-based explanations under data distribution changes such as adversarial manipulation (Kim et al., 2018) or spurious correlations (Adebayo et al., 2020). Designing concept-based explanations for OOD detection requires further exploration and is the focus of our work.\n\n\n# 2 PROBLEM SETUP AND BACKGROUND\n\n\nNotations. Let  $\\mathcal{X} \\subseteq \\mathbb{R}^{a_0 \\times b_0 \\times d_0}$  denote the space of inputs  $^1\\mathbf{x}$ , where  $d_0$  is the number of channels and  $a_0$  and  $b_0$  are the image size along each channel. Let  $\\mathcal{Y} \\coloneqq \\{1, \\dots, L\\}$  denote the space of\n\n\noutput class labels  $y$ . Let  $\\Delta_L$  denote the set of all probabilities over  $\\mathcal{V}$  (the simplex in  $L$ -dimensions). We assume that natural inputs to the DNN classifier are sampled from an unknown probability distribution  $P_{\\mathrm{in}}$  over the space  $\\mathcal{X} \\times \\mathcal{Y}$ . The compact notation  $[n]$  denotes  $\\{1, \\dots, n\\}$  for a positive integer  $n$ . Boldface symbols are used to denote both vectors and tensors.  $\\langle \\mathbf{x}, \\mathbf{x}' \\rangle$  denotes the standard inner-product between a pair of vectors. The indicator function  $\\mathbb{1}[c]$  takes value 1 (0) when the condition  $c$  is true (false).\n\n\nID and OOD Datasets. Consider a labeled ID training dataset  $D_{\\mathrm{in}}^{\\mathrm{tr}} = \\{(\\mathbf{x}_i,y_i), i = 1,\\dots ,N_{\\mathrm{in}}^{\\mathrm{tr}}\\}$  sampled from the distribution  $P_{\\mathrm{in}}$ . We assume the availability of an unlabeled training dataset  $D_{\\mathrm{out}}^{\\mathrm{tr}} = \\{\\widetilde{\\mathbf{x}}_i, i = 1,\\dots ,N_{\\mathrm{out}}^{\\mathrm{tr}}\\}$  from a different distribution, referred to as the auxiliary OOD dataset. Similarly, we define the ID test dataset (sampled from  $P_{\\mathrm{in}}$ ) as  $D_{\\mathrm{in}}^{\\mathrm{te}}$ , and the OOD test dataset as  $D_{\\mathrm{out}}^{\\mathrm{te}}$ . Note that the auxiliary OOD dataset  $D_{\\mathrm{in}}^{\\mathrm{tr}}$  and the test OOD dataset  $D_{\\mathrm{out}}^{\\mathrm{te}}$  are from different distributions. All the OOD datasets are unlabeled since their label space is usually different from  $\\mathcal{V}$ .\n\n\nOD Detector. The goal of an OOD detector is to determine if a test input to the classifier is ID (i.e., from the distribution  $P_{\\mathrm{in}}$ ); otherwise the input is declared to be OOD (Yang et al., 2021). Given a trained classifier  $\\mathbf{f} : \\mathcal{X} \\mapsto \\Delta_L$ , the decision function of an OOD detector can be generally defined as  $\\mathcal{D}_{\\gamma}(\\mathbf{x}, \\mathbf{f}) = \\mathbb{1}[S(\\mathbf{x}, \\mathbf{f}) \\geq \\gamma]$ , where  $S(\\mathbf{x}, \\mathbf{f}) \\in \\mathbb{R}$  is the score function of the detector for an input  $\\mathbf{x}$  and  $\\gamma$  is the threshold. We follow the convention that larger scores correspond to ID inputs, and the detector outputs of 1 and 0 correspond to ID and OOD respectively. We assume the availability of pre-trained DNN classifier and a paired OOD detector that is trained to detect inputs for the classifier.\n\n\n# 2.1 PROJECTION INTO CONCEPT SPACE\n\n\nConsider a pre-trained DNN classifier  $\\mathbf{f}:\\mathcal{X}\\mapsto$ $\\Delta_L$  that maps an input  $\\mathbf{x}$  to its corresponding predicted class probabilities. Without loss of generality, we can partition the DNN at a convolutional layer  $\\ell$  into two parts, i.e.,  $\\mathbf{f} = \\mathbf{h}\\circ \\phi$  where: 1)  $\\phi :\\mathcal{X}\\mapsto \\mathcal{Z}:= \\mathbb{R}^{a_{\\ell}b_{\\ell}\\times d_{\\ell}}$  is the first half of  $\\mathbf{f}$  that maps an input  $\\mathbf{x}$  to the intermediate feature representation  ${}^2\\phi (\\mathbf{x})$  , and 2)  $\\mathbf{h}:\\mathcal{Z}\\mapsto$ $\\Delta_L$  is the second half of  $\\mathbf{f}$  that maps  $\\phi (\\mathbf{x})$  to the predicted class probabilities  $\\mathbf{h}(\\phi (\\mathbf{x}))$  . We denote the predicted probability of a class  $y$  by  $f_{y}(\\mathbf{x}) = h_{y}(\\phi (\\mathbf{x}))$  , and the prediction of the classifier by  $\\widehat{y} (\\mathbf{x}) = \\mathrm{argmax}_y f_y(\\mathbf{x})$\n\n\nOur work is based on the common implicit assumption of linear interpretability in the concept-based explanation literature, i.e., high-level concepts lie in a linearly-projected subspace of the feature representation space  $\\mathcal{Z}$  of the classifier (Kim et al., 2018). Consider a projection matrix  $\\mathbf{C} = [\\mathbf{c}_1,\\dots ,\\mathbf{c}_m]\\in \\mathbb{R}^{d_\\ell \\times m}$  (with  $m\\ll d_{\\ell}$ ) that maps from the space  $\\mathcal{Z}$  into a reduced-dimension concept space.  $\\mathbf{C}$  consists\n\n\nof  $m$  unit vectors, where  $\\mathbf{c}_i\\in \\mathbb{R}^{d_\\ell}$  is referred to as the concept vector representing the  $i$ -th concept (e.g., \"stripe\" or \"oval face\"), and  $m$  is the number of concepts. We define the concept score for  $\\mathbf{x}$  as the linear projection of the high-dimensional layer representation  $\\phi (\\mathbf{x})\\in \\mathbb{R}^{a_{\\ell}b_{\\ell}\\times d_{\\ell}}$  into the concept space (Yeh et al., 2020), i.e.  $\\mathbf{v}_{\\mathbf{C}}(\\mathbf{x})\\coloneqq \\phi (\\mathbf{x})\\mathbf{C}\\in \\mathbb{R}^{a_{\\ell}b_{\\ell}\\times m}$ . We also define a mapping from the projected concept space back to the feature space by a non-linear function  $\\mathbf{g}:\\mathbb{R}^{a_{\\ell}b_{\\ell}\\times m}\\mapsto \\mathbb{R}^{a_{\\ell}b_{\\ell}\\times d_{\\ell}}$ . The reconstructed feature representation at layer  $\\ell$  is then defined as  $\\widehat{\\phi}_{\\mathbf{g},\\mathbf{C}}(\\mathbf{x})\\coloneqq \\mathbf{g}(\\mathbf{v}_{\\mathbf{C}}(\\mathbf{x}))$ .\n\n\n![Figure 2: Our two-world view of the classifier and OOD detector.](images/7f8cbfbdde14d8310a66c2cf4beff2716b1d0a8824c7e82511de69732682b744.jpg)\n\n\n# 2.2 CANONICAL WORLD AND CONCEPT WORLD\n\n\nAs shown in Fig. 2, we consider a \"two-world\" view of the classifier and OOD detector consisting of the canonical world and the concept world, which are defined as follows:\n\n\nCanonical World. In this case, both the classifier and OOD detector use the original layer representation  $\\phi (\\mathbf{x})$  for their predictions. The prediction of the classifier is  $\\mathbf{f}(\\mathbf{x}) = \\mathbf{h}(\\phi (\\mathbf{x}))$ , and the decision function of the detector is  $\\mathcal{D}_{\\gamma}(\\mathbf{x},\\mathbf{h}\\circ \\phi)$  with a score function  $S(\\mathbf{x},\\mathbf{h}\\circ \\phi)$ .\n\n\nConcept World. We use the following observation in constructing the concept-world formulation: both the classifier and the OOD detector can be modified to make predictions based on the reconstructed feature representation, i.e., using  $\\hat{\\phi}_{\\mathbf{g},\\mathbf{C}}(\\mathbf{x})$  instead of  $\\phi (\\mathbf{x})$ . Accordingly, we define the corresponding classifier, detector, and score function in the concept world as follows:\n\n\n$$\n\n$$\n\\mathbf {f} ^ {\\operatorname {c o n}} (\\mathbf {x}) := \\mathbf {h} \\left(\\widehat {\\phi} _ {\\mathbf {g}, \\mathbf {C}} (\\mathbf {x})\\right) = \\mathbf {h} (\\mathbf {g} (\\mathbf {v} _ {\\mathbf {C}} (\\mathbf {x})))\n$$\n\n$$\n\n\n$$\n\n$$\n\\mathcal {D} _ {\\gamma} ^ {\\mathrm {c o n}} (\\mathbf {x}, \\mathbf {f}) := \\mathcal {D} _ {\\gamma} (\\mathbf {x}, \\mathbf {h} \\circ \\widehat {\\phi} _ {\\mathbf {g}, \\mathbf {C}}) = \\mathcal {D} _ {\\gamma} (\\mathbf {x}, \\mathbf {h} \\circ \\mathbf {g} \\circ \\mathbf {v} _ {\\mathbf {C}})\n$$\n\n$$\n\n\n$$\n\n$$\nS ^ {\\mathrm {c o n}} (\\mathbf {x}, \\mathbf {f}) := S (\\mathbf {x}, \\mathbf {h} \\circ \\widehat {\\phi} _ {\\mathbf {g}, \\mathbf {C}}) = S (\\mathbf {x}, \\mathbf {h} \\circ \\mathbf {g} \\circ \\mathbf {v} _ {\\mathbf {C}}). \\tag {1}\n$$\n\n$$\n\n\nWe further elaborate on this two-world view and introduce the following two desirable properties.\n\n\nDetection Completeness. Given a fixed algorithmic approach for learning the classifier and OOD detector, and with fixed internal parameters of  $\\mathbf{f}$ , we would ideally like the classifier prediction and the detection score to be indistinguishable between the two worlds. In other words, for the concepts to sufficiently explain the OOD detector, we require  $\\mathcal{D}_{\\gamma}^{\\mathrm{con}}(\\mathbf{x},\\mathbf{f})$  to closely mimic  $\\mathcal{D}_{\\gamma}(\\mathbf{x},\\mathbf{f})$ . Likewise, we require  $\\mathbf{f}^{\\mathrm{con}}(\\mathbf{x})$  to closely mimic  $\\mathbf{f}(\\mathbf{x})$  since the detection mechanism of  $\\mathcal{D}_{\\gamma}$  is closely paired to the classifier. We refer to this property as the completeness of a set of concepts with respect to the OOD detector and its paired classifier. As discussed in \u00a7 3.1, this extends the notion of classification completeness introduced by Yeh et al. (2020) to an OOD detector and its paired classifier.\n\n\nConcept Separability. To improve the interpretability of the resulting explanations for the OOD detector, we require another desirable property from the learned concepts: data detected as ID by  $\\mathcal{D}_{\\gamma}$  (henceforth referred to as detected-ID data) and data detected as OOD by  $\\mathcal{D}_{\\gamma}$  (henceforth referred to as detected-OOD data) should be well-separated in the concept-score space. Since our goal is to help an analyst understand which concepts distinguish the detected-ID data from detected-OOD data, we would like to learn a set of concepts that have a well-separated concept score pattern for inputs from these two groups (e.g., the concepts \"stripe\" and \"oval face\" in Fig. ?? have distinct concept scores).\n\n\n# 3 PROPOSED APPROACH\n\n\nGiven a trained DNN classifier  $\\mathbf{f}$ , a paired OOD detector  $\\mathcal{D}_{\\gamma}$ , and a set of concepts  $\\mathbf{C}$ , we address the following questions: 1) Are the concepts sufficient to capture the prediction behavior of both the classifier and OOD detector? (see \u00a7 3.1); 2) Do the concepts show clear distinctions in their scores between detected-ID data and detected-OD data? (see \u00a7 3.2). We first propose new metrics for quantifying the set of learned concepts, followed by a general framework for learning concepts that possess these properties (see \u00a7 3.3).\n\n\n# 3.1 METRICS FOR DETECTION COMPLETENESS\n\n\nDefinition 1. Given a trained DNN classifier  $\\mathbf{f} = \\mathbf{h} \\circ \\phi$  and a set of concept vectors  $\\mathbf{C}$ , the classification completeness with respect to  $P_{\\mathrm{in}}(\\mathbf{x}, y)$  is defined as (Yeh et al., 2020):\n\n\n$$\n\n$$\n\\eta_ {\\mathbf {f}} (\\mathbf {C}) := \\frac {\\operatorname * {s u p} _ {\\mathbf {g}} \\mathbb {E} _ {(\\mathbf {x} , y) \\sim P _ {\\mathrm {i n}}} \\left[ \\mathbb {1} [ y = \\operatorname * {a r g m a x} _ {y ^ {\\prime}} h _ {y ^ {\\prime}} (\\widehat {\\phi} _ {\\mathbf {g} , \\mathbf {C}} (\\mathbf {x})) ] - a _ {r} \\right.}{\\mathbb {E} _ {(\\mathbf {x} , y) \\sim P _ {\\mathrm {i n}}} \\left[ \\mathbb {1} [ y = \\operatorname * {a r g m a x} _ {y ^ {\\prime}} h _ {y ^ {\\prime}} (\\phi (\\mathbf {x})) ] \\right] - a _ {r}}\n$$\n\n$$\n\n\nwhere  $a_{r} = 1 / L$  is the accuracy of a random  $L$ -class classifier.\n\n\nThe denominator of  $\\eta_{\\mathbf{f}}(\\mathbf{C})$  is the accuracy of the original classifier  $\\mathbf{f}$ , while the numerator is the maximum accuracy that can be achieved in the concept world using the feature representation reconstructed from the concept scores. The maximization is over the parameters of the neural network  $\\mathbf{g}$  that reconstructs the feature representation from the vector of concept scores.\n\n\nDefinition 2. Given a trained DNN classifier  $\\mathbf{f} = \\mathbf{h} \\circ \\phi$ , a trained OOD detector with score function  $S(\\mathbf{x}, \\mathbf{f})$ , and a set of concept vectors  $\\mathbf{C}$ , we define the detection completeness score with respect to the ID distribution  $P_{\\mathrm{in}}(\\mathbf{x}, y)$  and OOD distribution  $P_{\\mathrm{out}}(\\mathbf{x})$  as follows:\n\n\n$$\n\n$$\n\\eta_ {\\mathbf {f}, S} (\\mathbf {C}) := \\frac {\\sup  _ {\\mathbf {g}} \\operatorname {A U C} \\left(\\mathbf {h} \\circ \\widehat {\\phi} _ {\\mathbf {g} , \\mathbf {C}}\\right) - b _ {r}}{\\operatorname {A U C} \\left(\\mathbf {h} \\circ \\phi\\right) - b _ {r}}, \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\mathrm{AUC}(\\mathbf{f})$  is the area under the ROC curve of an OOD detector based on  $\\mathbf{f}$ , defined as  $\\mathrm{AUC}(\\mathbf{f}) := \\mathbb{E}_{(\\mathbf{x},y) \\sim P_{\\mathrm{in}}} \\mathbb{E}_{\\mathbf{x}' \\sim P_{\\mathrm{out}}} \\mathbb{1}\\left[S(\\mathbf{x},\\mathbf{f}) > S(\\mathbf{x}',\\mathbf{f})\\right]$ , and  $b_r = 0.5$  is the AUROC of a random detector.\n\n\nThe numerator term is the maximum achievable AUROC in the concept world via reconstructed features from concept scores. In practice,  $\\mathrm{AUC}(\\mathbf{f})$  is estimated using the test datasets  $D_{\\mathrm{in}}^{\\mathrm{te}}$  and  $D_{\\mathrm{out}}^{\\mathrm{te}}$ . Both the classification completeness and detection completeness scores are designed to be in the range [0, 1]. However, this is not strictly guaranteed since the classifier or OOD detector in the concept world may empirically have a better (corresponding) metric on a given ID/OOD dataset. A completeness score close to 1 indicates that the set of concepts  $\\mathbf{C}$  are close to complete in characterizing the behavior of the classifier and/or the OOD detector.\n\n\n# 3.2 CONCEPT SEPARABILITY SCORE\n\n\nConcept Scores. In Section 2.1, we introduced a projection matrix  $\\mathbf{C} \\in \\mathbb{R}^{d_{\\ell} \\times m}$  that maps  $\\phi(\\mathbf{x})$  to  $\\mathbf{v}_{\\mathbf{C}}(\\mathbf{x})$ , and consists of  $m$  unit concept vectors  $\\mathbf{C} = [\\mathbf{c}_1 \\cdots \\mathbf{c}_m]$ . The inner product between the feature representation and a concept vector is referred to as the concept score, and it quantifies how close an input is to the given concept (Kim et al., 2018; Ghorbani et al., 2019). Specifically, the concept score corresponding to concept  $i$  is defined as  $\\mathbf{v}_{\\mathbf{c}_i}(\\mathbf{x}) \\coloneqq \\langle \\phi(\\mathbf{x}), \\mathbf{c}_i \\rangle = \\phi(\\mathbf{x}) \\mathbf{c}_i \\in \\mathbb{R}^{a_{\\ell} b_{\\ell}}$ . The matrix of concept scores from all the concepts is simply the concatenation of the individual concept scores, i.e.,  $\\mathbf{v}_{\\mathbf{C}}(\\mathbf{x}) = \\phi(\\mathbf{x}) \\mathbf{C} = [\\mathbf{v}_{\\mathbf{c}_1}(\\mathbf{x}) \\cdots \\mathbf{v}_{\\mathbf{c}_m}(\\mathbf{x})] \\in \\mathbb{R}^{a_{\\ell} b_{\\ell} \\times m}$ . We also define a dimension-reduced version of the concept scores that takes the maximum of the inner-product over each  $a_{\\ell} \\times b_{\\ell}$  patch as follows:  $\\widetilde{\\mathbf{v}}_{\\mathbf{C}}(\\mathbf{x})^T = [\\widetilde{v}_{\\mathbf{c}_1}(\\mathbf{x}), \\dots, \\widetilde{v}_{\\mathbf{c}_m}(\\mathbf{x})] \\in \\mathbb{R}^m$ , where  $\\widetilde{v}_{\\mathbf{c}_i}(\\mathbf{x}) = \\max_{p,q} |\\langle \\phi^{p,q}(\\mathbf{x}), \\mathbf{c}_i \\rangle| \\in \\mathbb{R}$ . Here  $\\phi^{p,q}(\\mathbf{x})$  is the feature representation corresponding to the  $(p,q)$ -th patch of input  $\\mathbf{x}$  (i.e., receptive field (Araujo et al., 2019)). This reduction operation is done to capture the most important correlations from each patch, and the  $m$ -dimensional concept score will be used to define our concept separability metric as follows.\n\n\nWe would like the set of concept-score vectors from the detected-ID class  $V_{\\mathrm{in}}(\\mathbf{C}) \\coloneqq \\{\\widetilde{\\mathbf{v}}_{\\mathbf{C}}(\\mathbf{x}), \\mathbf{x} \\in D_{\\mathrm{in}}^{\\mathrm{tr}} \\cup D_{\\mathrm{out}}^{\\mathrm{tr}} : \\mathcal{D}_{\\gamma}(\\mathbf{x}, \\mathbf{f}) = 1\\}$ , and the set of concept-score vectors from the detected-OOD class  $V_{\\mathrm{out}}(\\mathbf{C}) \\coloneqq \\{\\widetilde{\\mathbf{v}}_{\\mathbf{C}}(\\mathbf{x}), \\mathbf{x} \\in D_{\\mathrm{in}}^{\\mathrm{tr}} \\cup D_{\\mathrm{out}}^{\\mathrm{tr}} : \\mathcal{D}_{\\gamma}(\\mathbf{x}, \\mathbf{f}) = 0\\}$  to be well separated. Let  $J_{\\mathrm{sep}}(V_{\\mathrm{in}}(\\mathbf{C}), V_{\\mathrm{out}}(\\mathbf{C})) \\in \\mathbb{R}$  define a general measure of separability between the two subsets, such that a larger value corresponds to higher separability. We discuss a specific choice for  $J_{\\mathrm{sep}}$  for which it is possible to tractably optimize concept separability as part of the learning objective in Section 3.3.\n\n\nGlobal Concept Separability. Class separability metrics have been well studied in the pattern recognition literature, particularly for the two-class case (Fukunaga, 1990b) $^3$ . Motivated by Fisher's linear discriminant analysis (LDA), we explore the use of class-separability measures based on the within-class and between-class scatter matrices (Murphy, 2012). The goal of LDA is to find a projection vector (direction) such that data from the two classes are maximally separated and form compact clusters upon projection. Rather than finding an optimal projection direction, we are more interested in ensuring that the concept-score vectors from the detected-ID and detected-OOD data have high separability. Consider the within-class and between-class scatter matrices based on  $V_{\\mathrm{in}}(\\mathbf{C})$  and  $V_{\\mathrm{out}}(\\mathbf{C})$ , given by\n\n\n$$\n\n$$\n\\mathbf {S} _ {w} = \\sum_ {\\mathbf {v} \\in V _ {\\mathrm {i n}} (\\mathbf {C})} \\left(\\mathbf {v} - \\boldsymbol {\\mu} _ {\\mathrm {i n}}\\right) \\left(\\mathbf {v} - \\boldsymbol {\\mu} _ {\\mathrm {i n}}\\right) ^ {T} + \\sum_ {\\mathbf {v} \\in V _ {\\mathrm {o u t}} (\\mathbf {C})} \\left(\\mathbf {v} - \\boldsymbol {\\mu} _ {\\mathrm {o u t}}\\right) \\left(\\mathbf {v} - \\boldsymbol {\\mu} _ {\\mathrm {o u t}}\\right) ^ {T}, \\tag {3}\n$$\n\n$$\n\n\n$$\n\n$$\n\\mathbf {S} _ {b} = \\left(\\boldsymbol {\\mu} _ {\\text {o u t}} - \\boldsymbol {\\mu} _ {\\text {i n}}\\right) \\left(\\boldsymbol {\\mu} _ {\\text {o u t}} - \\boldsymbol {\\mu} _ {\\text {i n}}\\right) ^ {T}, \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\pmb{\\mu}_{\\mathrm{in}}$  and  $\\pmb{\\mu}_{\\mathrm{out}}$  are the mean concept-score vectors from  $V_{\\mathrm{in}}(\\mathbf{C})$  and  $V_{\\mathrm{out}}(\\mathbf{C})$  respectively. We define the following separability metric based on the generalized eigenvalue equation solved by Fisher's LDA (Fukunaga, 1990b):  $J_{\\mathrm{sep}}(\\mathbf{C}) := J_{\\mathrm{sep}}(V_{\\mathrm{in}}(\\mathbf{C}), V_{\\mathrm{out}}(\\mathbf{C})) = \\operatorname{tr}\\left[\\mathbf{S}_w^{-1} \\mathbf{S}_b\\right]$ . Maximizing the above metric is equivalent to maximizing the sum of eigenvalues of the matrix  $\\mathbf{S}_w^{-1} \\mathbf{S}_b$ , which\n\n\nin-turn ensures a large between-class separability and a small within-class separability for the detected-ID and detected-OOD concept scores. We refer to this as a global concept separability metric because it does not analyze the separability on a per-class level<sup>4</sup>. The separability metric is closely related to the Bhattacharya distance, which is an upper bound on the Bayes error rate (see Appendix A.1).\n\n\n# 3.3 PROPOSED CONCEPT LEARNING - KEY IDEAS\n\n\nPrior Approaches and Limitations. Among post-hoc concept-discovery methods for a DNN classifier with ID data, unlike Kim et al. and Ghorbani et al., that do not support imposing required conditions into the concept discovery, Yeh et al. devised a learning-based approach where classification completeness and the saliency of concepts are optimized via a regularized objective given by\n\n\n$$\n\n$$\n\\underset {\\mathbf {C}, \\mathbf {g}} {\\operatorname {a r g m a x}} \\underset {(\\mathbf {x}, y) \\sim P _ {\\mathrm {i n}}} {\\mathbb {E}} \\left[ \\log h _ {y} \\left(\\mathbf {g} \\left(\\mathbf {v} _ {\\mathbf {C}} (\\mathbf {x})\\right)\\right) \\right] + \\lambda_ {\\exp} R _ {\\exp} (\\mathbf {C}). \\tag {5}\n$$\n\n$$\n\n\nHere  $\\mathbf{C}$  and  $\\mathbf{g}$  (parameterized by a neural network) are jointly optimized, and  $R_{\\mathrm{exp}}(\\mathbf{C})$  is a regularization term used to ensure that the learned concept vectors have high spatial coherency and low redundancy among themselves (see (Yeh et al., 2020) for the definition).\n\n\nWhile the objective (5) of Yeh et al. can learn a set of sufficient concepts that have a high classification completeness score, we find that it does not necessarily replicate the per-instance prediction behavior of the classifier in the concept world. Specifically, there can be discrepancies in the reconstructed feature representation, whose effect propagates through the remaining part of the classifier. Since many widely-used OOD detectors rely on the feature representations and/or the classifier's predictions, this discrepancy in the existing concept learning approaches makes it hard to closely replicate the OOD detector in the concept world (see Fig. 3). Furthermore, the scope of Yeh et al. is confined to concept learning for explaining the classifier's predictions based on ID data, and there is no guarantee that the learned concepts would be useful for explaining an OOD detector. To address these gaps, we propose a general method for concept learning that complements prior work by imposing additional instance-level constraints on the concepts, and by considering both the OOD detector and OOD data.\n\n\nConcept Learning Objective. We define a concept learning objective that aims to find a set of concepts C and a mapping g that have the following properties: 1) high detection completeness w.r.t the OOD detector; 2) high classification completeness w.r.t the DNN classifier; and 3) high separability in the concept-score space between detected-ID data and detected-OOD data.\n\n\nInspired by recent works on transferring feature information from a teacher model to a student model (Hinton et al., 2015; Zhou et al., 2018b), we encourage accurate reconstruction of  $\\hat{\\mathcal{Z}}$  based on the concept scores by adding a regularization term that is the squared  $\\ell_2$  distance between the original and reconstructed representations  $J_{\\mathrm{norm}}(\\mathbf{C},\\mathbf{g}) = \\mathbb{E}_{\\mathbf{x}\\sim P_{\\mathrm{in}}}\\| \\phi (\\mathbf{x}) - \\widehat{\\phi}_{\\mathbf{g},\\mathbf{C}}(\\mathbf{x})\\| ^2$ . In order to close the gap between the scores of the OOD detector in the concept world and canonical world on a per-sample level, we introduce the following mean-squared-error (MSE) based regularization:\n\n\n$$\n\n$$\nJ _ {\\mathrm {m s e}} (\\mathbf {C}, \\mathbf {g}) = \\underset {\\mathbf {x} \\sim P _ {\\mathrm {i n}}} {\\mathbb {E}} \\left(S (\\mathbf {x}, \\mathbf {h} \\circ \\widehat {\\phi} _ {\\mathbf {g}, \\mathbf {C}}) - S (\\mathbf {x}, \\mathbf {f})\\right) ^ {2} + \\underset {\\mathbf {x} \\sim P _ {\\mathrm {o u t}}} {\\mathbb {E}} \\left(S (\\mathbf {x}, \\mathbf {h} \\circ \\widehat {\\phi} _ {\\mathbf {g}, \\mathbf {C}}) - S (\\mathbf {x}, \\mathbf {f})\\right) ^ {2}. \\tag {6}\n$$\n\n$$\n\n\nMSE terms are computed with both the ID and OOD data because we want to ensure that the ROC curve corresponding to both the score functions are close to each other (which requires OOD data). Finally, we include a regularization term to maximize the separability metric between the detected-ID and detected-OOD data in the concept-score space, resulting in our final concept learning objective:\n\n\n$$\n\n$$\n\\begin{array}{l} \\underset {\\mathbf {C}, \\mathbf {g}} {\\operatorname {a r g m a x}} \\underset {(\\mathbf {x}, y) \\sim P _ {\\mathrm {i n}}} {\\mathbb {E}} \\left[ \\log h _ {y} (\\mathbf {g} (\\mathbf {v} _ {\\mathbf {C}} (\\mathbf {x}))) \\right] + \\lambda_ {\\exp} R _ {\\exp} (\\mathbf {C}) \\\\ - \\lambda_ {\\mathrm {m s e}} J _ {\\mathrm {m s e}} (\\mathbf {C}, \\mathbf {g}) - \\lambda_ {\\mathrm {n o r m}} J _ {\\mathrm {n o r m}} (\\mathbf {C}, \\mathbf {g}) + \\lambda_ {\\mathrm {s e p}} J _ {\\mathrm {s e p}} (\\mathbf {C}). \\tag {7} \\\\ \\end{array}\n$$\n\n$$\n\n\nThe  $\\lambda$  coefficients are non-negative hyper-parameters that are further discussed in Section 4. We note that both  $J_{\\mathrm{mse}}(\\mathbf{C},\\mathbf{g})$  and  $J_{\\mathrm{sep}}(\\mathbf{C})$  depend on the OOD detector<sup>5</sup>. We use the SGD-based Adam optimizer (Kingma & Ba, 2014)) to solve the learning objective. The expectations involved in the objective terms are calculated using sample estimates from the training ID and OOD datasets. Specifically,  $D_{\\mathrm{in}}^{\\mathrm{tr}}$  and  $D_{\\mathrm{out}}^{\\mathrm{tr}}$  are used to compute the expectations over  $P_{\\mathrm{in}}$  and  $P_{\\mathrm{out}}$ , respectively. Our complete concept learning is summarized in Algorithm 1 (Appendix A.4).\n\n\n# 4 EXPERIMENTS\n\n\nWe briefly describe the experimental setup here and provide additional details in Appendix B.\n\n\nDatasets. For the ID dataset, we use Animals with Attributes (AwA) (Xian et al., 2018) with 50 animal classes, and split it into a train set (29841 images), validation set (3709 images), and test set (3772 images). We use the MSCOCO dataset (Lin et al., 2014) as the auxiliary OOD dataset  $D_{\\mathrm{out}}^{\\mathrm{tr}}$  for training and validation. For the OOD test dataset  $D_{\\mathrm{out}}^{\\mathrm{te}}$ , we follow a common setting in the literature of large-scale OOD detection (Huang & Li, 2021) and use three different image datasets: Places365 (Zhou et al., 2017), SUN (Xiao et al., 2010), and Textures (Cimpoi et al., 2014).\n\n\nModels. We apply our framework to interpret five prominent OOD detectors from the literature: MSP (Hendrycks & Gimpel, 2016), ODIN (Liang et al., 2018), Generalized-ODIN (Hsu et al., 2020), Energy (Liu et al., 2020) and Mahalanobis (Lee et al., 2018). The OOD detectors are paired with the widely-used Inception-V3 model (Szegedy et al., 2016) (following the setup in prior works (Yeh et al., 2020; Ghorbani et al., 2019; Kim et al., 2018)) trained on the Animals-with-Attributes (AwA) dataset (Xian et al., 2018), which has a test accuracy of  $92.10\\%$ .\n\n\nMetrics. For each set of concepts learned with different OOD detectors and hyperparameters, we report the classification completeness  $\\eta_{\\mathbf{f}}(\\mathbf{C})$ , detection completeness  $\\eta_{\\mathbf{f},S}(\\mathbf{C})$ , and the relative concept separability metric (defined below). In contrast to the completeness scores that are almost always bounded to the range [0, 1], it is hard to gauge the possible range of the separability score  $J_{\\mathrm{sep}}(\\mathbf{C})$  (or  $J_{\\mathrm{sep}}^y (\\mathbf{C}))$  across different settings (datasets, classification models, and OOD detectors), and whether the value represents a significant improvement in separability. Hence, we define the relative concept separability, which captures the relative improvement in concept separability using concepts  $\\mathbf{C}$  compared to a different set of concepts  $\\mathbf{C}'$ , as follows\n\n\n$$\n\n$$\nJ _ {\\operatorname {s e p}} \\left(\\mathbf {C}, \\mathbf {C} ^ {\\prime}\\right) = \\operatorname {M e d i a n} \\left(\\left\\{\\frac {J _ {\\operatorname {s e p}} ^ {y} (\\mathbf {C}) - J _ {\\operatorname {s e p}} ^ {y} \\left(\\mathbf {C} ^ {\\prime}\\right)}{J _ {\\operatorname {s e p}} ^ {y} \\left(\\mathbf {C} ^ {\\prime}\\right)} \\right\\} _ {y = 1} ^ {L}\\right). \\tag {8}\n$$\n\n$$\n\n\nWe choose  $\\mathbf{C}'$  to be the set of concepts learned by the baseline (Yeh et al., 2020), which is a special case of our learning objective when  $\\lambda_{\\mathrm{mse}} = \\lambda_{\\mathrm{norm}} = \\lambda_{\\mathrm{sep}} = 0$ . The set of concept  $\\mathbf{C}$  are obtained via our concept learning objective, with various combinations of hyperparameter values.\n\n\n<table><tr><td rowspan=\"3\">OOD detector</td><td rowspan=\"3\">Hyper-parameters</td><td rowspan=\"3\">\u03b7F(C)\u2191</td><td colspan=\"6\">Test OOD dataset</td></tr><tr><td colspan=\"2\">Places</td><td colspan=\"2\">SUN</td><td colspan=\"2\">Textures</td></tr><tr><td>\u03b7F,S(C)\u2191</td><td>Jsep(C,C&#x27;)\u2191</td><td>\u03b7F,S(C)\u2191</td><td>Jsep(C,C&#x27;)\u2191</td><td>\u03b7F,S(C)\u2191</td><td>Jsep(C,C&#x27;)\u2191</td></tr><tr><td rowspan=\"4\">MSP</td><td>(0,0,0)</td><td>0.977 \u00b1 0.0006</td><td>0.774 \u00b1 0.0010</td><td>0.694 \u00b1 0.0153</td><td>0.782 \u00b1 0.0010</td><td>1.088 \u00b1 0.0175</td><td>0.593 \u00b1 0.0013</td><td>0.765 \u00b1 0.0157</td></tr><tr><td>(10,0.1,0)</td><td>0.994 \u00b1 0.0004</td><td>0.947 \u00b1 0.0004</td><td>1.892 \u00b1 0.0393</td><td>0.946 \u00b1 0.0004</td><td>3.074 \u00b1 0.0531</td><td>0.920 \u00b1 0.0005</td><td>3.577 \u00b1 0.1292</td></tr><tr><td>(0,0,50)</td><td>0.980 \u00b1 0.0005</td><td>0.814 \u00b1 0.0008</td><td>2.533 \u00b1 0.0714</td><td>0.816 \u00b1 0.0009</td><td>4.295 \u00b1 0.1048</td><td>0.773 \u00b1 0.0010</td><td>3.147 \u00b1 0.2076</td></tr><tr><td>(10,0.1,50)</td><td>0.984 \u00b1 0.0004</td><td>0.960 \u00b1 0.0004</td><td>2.756 \u00b1 0.0854</td><td>0.961 \u00b1 0.0005</td><td>4.442 \u00b1 0.0830</td><td>0.937 \u00b1 0.0004</td><td>3.587 \u00b1 0.2145</td></tr><tr><td rowspan=\"4\">ODIN</td><td>(0,0,0)</td><td>0.977 \u00b1 0.0006</td><td>0.742 \u00b1 0.0011</td><td>0.444 \u00b1 0.0119</td><td>0.745 \u00b1 0.0010</td><td>0.710 \u00b1 0.0156</td><td>0.618 \u00b1 0.0013</td><td>0.501 \u00b1 0.0121</td></tr><tr><td>(108,0.1,0)</td><td>0.994 \u00b1 0.0004</td><td>0.951 \u00b1 0.0004</td><td>1.166 \u00b1 0.0303</td><td>0.958 \u00b1 0.0004</td><td>2.135 \u00b1 0.0450</td><td>0.934 \u00b1 0.0004</td><td>2.793 \u00b1 0.0865</td></tr><tr><td>(0,0,50)</td><td>0.987 \u00b1 0.0004</td><td>0.899 \u00b1 0.0007</td><td>1.785 \u00b1 0.0669</td><td>0.911 \u00b1 0.0006</td><td>3.814 \u00b1 0.0768</td><td>0.793 \u00b1 0.0008</td><td>3.046 \u00b1 0.2845</td></tr><tr><td>(108,0.1,50)</td><td>0.991 \u00b1 0.0005</td><td>0.973 \u00b1 0.0009</td><td>1.813 \u00b1 0.0268</td><td>0.969 \u00b1 0.0010</td><td>4.000 \u00b1 0.0094</td><td>0.945 \u00b1 0.0006</td><td>3.662 \u00b1 0.1005</td></tr><tr><td rowspan=\"4\">General-ODIN</td><td>(0,0,0)</td><td>0.988 \u00b1 0.0004</td><td>0.769 \u00b1 0.0004</td><td>0.506 \u00b1 0.0165</td><td>0.719 \u00b1 0.0014</td><td>0.816 \u00b1 0.0192</td><td>0.605 \u00b1 0.0013</td><td>0.558 \u00b1 0.1683</td></tr><tr><td>(106,0.1,0)</td><td>0.995 \u00b1 0.0004</td><td>0.951 \u00b1 0.0006</td><td>1.461 \u00b1 0.0321</td><td>0.960 \u00b1 0.0005</td><td>3.007 \u00b1 0.0316</td><td>0.940 \u00b1 0.0008</td><td>2.619 \u00b1 0.1077</td></tr><tr><td>(0,0,50)</td><td>0.981 \u00b1 0.0004</td><td>0.859 \u00b1 0.0007</td><td>1.814 \u00b1 0.0685</td><td>0.803 \u00b1 0.0006</td><td>4.204 \u00b1 0.0159</td><td>0.826 \u00b1 0.0008</td><td>4.014 \u00b1 0.2246</td></tr><tr><td>(106,0.1,50)</td><td>0.990 \u00b1 0.0005</td><td>0.971 \u00b1 0.0010</td><td>1.835 \u00b1 0.0669</td><td>0.963 \u00b1 0.0004</td><td>4.287 \u00b1 0.0284</td><td>0.951 \u00b1 0.0005</td><td>3.695 \u00b1 0.1921</td></tr><tr><td rowspan=\"4\">Energy</td><td>(0,0,0)</td><td>0.977 \u00b1 0.0006</td><td>0.671 \u00b1 0.0012</td><td>0.453 \u00b1 0.0121</td><td>0.682 \u00b1 0.0012</td><td>0.675 \u00b1 0.0148</td><td>0.557 \u00b1 0.0014</td><td>0.521 \u00b1 0.0131</td></tr><tr><td>(1.0,1,0)</td><td>0.993 \u00b1 0.0005</td><td>0.965 \u00b1 0.0004</td><td>1.266 \u00b1 0.0319</td><td>0.963 \u00b1 0.0004</td><td>2.125 \u00b1 0.0413</td><td>0.960 \u00b1 0.0003</td><td>2.648 \u00b1 0.0596</td></tr><tr><td>(0,0,50)</td><td>0.987 \u00b1 0.0005</td><td>0.779 \u00b1 0.0010</td><td>1.920 \u00b1 0.0725</td><td>0.793 \u00b1 0.0009</td><td>3.659 \u00b1 0.0659</td><td>0.767 \u00b1 0.0010</td><td>4.397 \u00b1 0.2165</td></tr><tr><td>(1,0,1,50)</td><td>0.980 \u00b1 0.0005</td><td>0.943 \u00b1 0.0005</td><td>1.839 \u00b1 0.0662</td><td>0.941 \u00b1 0.0005</td><td>3.421 \u00b1 0.0619</td><td>0.936 \u00b1 0.0005</td><td>3.917 \u00b1 0.1691</td></tr><tr><td rowspan=\"4\">Mahala-nobis</td><td>(0,0,0)</td><td>0.990 \u00b1 0.0007</td><td>0.715 \u00b1 0.0011</td><td>0.571 \u00b1 0.0110</td><td>0.736 \u00b1 0.0011</td><td>0.822 \u00b1 0.0165</td><td>0.591 \u00b1 0.0011</td><td>0.564 \u00b1 0.0203</td></tr><tr><td>(0.1,0.1,0)</td><td>0.994 \u00b1 0.0004</td><td>0.950 \u00b1 0.0009</td><td>1.532 \u00b1 0.0351</td><td>0.960 \u00b1 0.0010</td><td>2.276 \u00b1 0.0466</td><td>0.938 \u00b1 0.0004</td><td>2.915 \u00b1 0.1132</td></tr><tr><td>(0,0,50)</td><td>0.985 \u00b1 0.0004</td><td>0.880 \u00b1 0.0005</td><td>2.550 \u00b1 0.0681</td><td>0.883 \u00b1 0.0006</td><td>4.091 \u00b1 0.1013</td><td>0.774 \u00b1 0.0007</td><td>4.274 \u00b1 0.2305</td></tr><tr><td>(0.1,0.1,50)</td><td>0.992 \u00b1 0.0006</td><td>0.961 \u00b1 0.0005</td><td>2.616 \u00b1 0.0857</td><td>0.966 \u00b1 0.0005</td><td>4.325 \u00b1 0.0055</td><td>0.949 \u00b1 0.0003</td><td>4.308 \u00b1 0.2011</td></tr></table>\n\n\nTable 1: Results of concept learning with different parameter settings across various OOD detectors and test OOD datasets. The ID dataset is AwA for both training and test, and the auxiliary OOD dataset is MSCOCO. Hyperparameters are in the order of  $(\\lambda_{\\mathrm{mse}},\\lambda_{\\mathrm{norm}},\\lambda_{\\mathrm{sep}})$ , and their values are set based on the scale of corresponding regularization terms in Eqn. (7), for a specific choice of the OOD detector. Further investigation, including an ablation study on each regularization term can be found in Appendix B.2. Across the rows (for a given OOD detector and OOD dataset), the best value is boldfaced, and second best value is underscored. The  $95\\%$  confidence intervals are estimated by bootstrapping the test set over 200 trials.\n\n\n![(a) Target distribution of  $S(\\mathbf{x}, \\mathbf{f})$  in the canonical world.](images/23bd7b755ba63ec62133e1ca4d208485899f74a88668b7afaa105bd50fdffdfe.jpg)\n\n\n![Figure 3: Detection completeness and estimated density of OOD score  $S(\\mathbf{x}, \\mathbf{f})$  from MSP detector. Concepts by ours are learned using  $\\lambda_{\\mathrm{mse}} = 10$ ,  $\\lambda_{\\mathrm{norm}} = 0.1$ ,  $\\lambda_{\\mathrm{sep}} = 50$ . Comparison is made between AwA test set (ID; blue) vs. SUN (OOD; red).](images/41b3455e97d631bc4b009209304d00929f497f64ef333ab18adc564c0760b574.jpg)\n\n\n![(b) Reconstructed distribution of  $S^{\\mathrm{con}}(\\mathbf{x},\\mathbf{f})$  in the concept world, using concepts by (Yeh et al., 2020). (c) Reconstructed distribution of  $S^{\\mathrm{con}}(\\mathbf{x},\\mathbf{f})$  in the concept world, using concepts by ours.](images/67cd5a14cfc4a4728b8ba8b228cb7cb805de7d7b1b92ac33378b997557708211.jpg)\n\n\n# 4.1 EFFECTIVENESS OF OUR METHOD\n\n\nIn this subsection, we carry out experiments to answer the following question: does our concept learning objective effectively encourage concepts to have the desired properties of detection completeness and concept separability? Table 1 summarizes the results of concept learning with various combinations of hyperparameters for the proposed regularization terms in Eqn. (7): i) all the hyperparameters are set to 0 (first row); ii) only the terms directly relevant to detection completeness (i.e., reconstruction error  $J_{\\mathrm{norm}}(\\mathbf{C}, \\mathbf{g})$  and mean-squared error  $J_{\\mathrm{mse}}(\\mathbf{C}, \\mathbf{g})$ ) are included (second row); iii) only the term responsible for concept separability  $J_{\\mathrm{sep}}(\\mathbf{C})$  is included (third row); iv) all the regularization terms are included (fourth row).\n\n\nIn all cases, we observe that the baseline achieves a high enough classification completeness score, but the lowest detection completeness and concept separability. This indicates that the concepts discovered by (Yeh et al., 2020) would be sufficient to describe the DNN classifier, but using the same set of concepts would be inappropriate for the OOD detector. In contrast, our method significantly improves the detection completeness (and even the classification completeness) by having  $\\lambda_{\\mathrm{mse}} > 0$ ,  $\\lambda_{\\mathrm{norm}} > 0$  and concept separability by having  $\\lambda_{\\mathrm{sep}} > 0$ , compared to the baseline. Moreover, the three terms make the best synergy together in almost all cases.\n\n\nDetection completeness and accurate reconstruction of  $\\mathcal{Z}$ . Additionally, we observe whether the proposed evaluation metrics are well-aligned with the interpretability of the resulting concept-based explanations. In Fig. 3, we observe that concepts by (Yeh et al., 2020) with low detection completeness  $(\\eta_{\\mathbf{f},S}(\\mathbf{C}) = 0.782$  for MSP and  $\\eta_{\\mathbf{f},S}(\\mathbf{C}) = 0.682$  for Energy) lead to a strong mismatch between the score distributions on both ID data and OOD data. In contrast, concepts learned by our method with high detection completeness  $(\\eta_{\\mathbf{f},S}(\\mathbf{C}) = 0.961$  for the MSP detector, and  $\\eta_{\\mathbf{f},S}(\\mathbf{C}) = 0.941$  for the Energy detector) approximate the target score distributions more closely on both ID data and OOD data. By reducing the performance gap of OOD detector between canonical world and concept world, it leads to more accurate explanations for OOD detectors.\n\n\n# 4.2 CONCEPT-BASED EXPLANATIONS FOR OOD DETECTORS\n\n\nContribution of each concept to detection. The proposed concept learning algorithm learns concepts for both the classifier and OOD detector considering all the classes, and we address the following question: how much does each concept contribute to the detection results for inputs predicted to a particular class?. Recent works have adopted the Shapley value from Coalitional Game theory literature (Shapley, 1953; Fujimoto et al., 2006) for scoring the importance of a feature subset towards the predictions of a model (Chen et al., 2018; Lundberg & Lee, 2017; Sundararajan & Najmi, 2020). Extending this idea, we modify the characteristic function of the Shapley value using our per-class detection completeness metric (Eqn. (11) in Appendix A.2). The modified Shapley value of a concept  $\\mathbf{c}_i \\in \\mathbf{C}$  with respect to the predicted class  $j \\in [L]$  is defined as\n\n\n$$\n\n$$\n\\operatorname {S H A P} \\left(\\eta_ {\\mathbf {f}, S} ^ {j}, \\mathbf {c} _ {i}\\right) := \\sum_ {\\mathbf {C} ^ {\\prime} \\subseteq \\mathbf {C} \\backslash \\left\\{\\mathbf {c} _ {i} \\right\\}} \\frac {(m - | \\mathbf {C} ^ {\\prime} | - 1) ! | \\mathbf {C} ^ {\\prime} | !}{m !} \\left(\\eta_ {\\mathbf {f}, S} ^ {j} \\left(\\mathbf {C} ^ {\\prime} \\cup \\left\\{\\mathbf {c} _ {i} \\right\\}\\right) - \\eta_ {\\mathbf {f}, S} ^ {j} \\left(\\mathbf {C} ^ {\\prime}\\right)\\right), \\tag {9}\n$$\n\n$$\n\n\n![](images/dd3326b28c26ee119637137780e7f6601521b83fffe4496d0c296f087c2b016d.jpg)\n\n\n![](images/099355153ab051d4f29f141432465ce12d46d543dde3c0b02232fd81074c3cda.jpg)\n\n\n![](images/492f725ff9bcb42a375427befe960743ee7d77dcf5968dd40eeaf2d450986477.jpg)\n\n\n![](images/f97457a6a40fa6d7a16e5503f408dd4ea37e15ef579975fc9814205ce2b34ca6.jpg)\n\n\n![](images/fa67ae1d6459789692268dfb3e02efed9cebb7162a560af7300eb94bbdf355ab.jpg)\n\n\n![](images/cdae7f2b8783e5a6efa12c27ab4c5a0ca9a93e69e34a560137d36e72488b127b.jpg)\n\n\n![Figure 4: Concept-based explanations for Energy OOD detector using concepts by Yeh et al. (2020) vs. ours. Images are randomly selected from AwA test set (ID) and Places (OOD), and all predicted to class \"Collie\". ID profile shows the normal concept-score patterns for ID Collie images.](images/05cf143d28d17bbab525b9b3f90184d5ca115cf952db330b0ddcf9a4beb25f92.jpg)\n\n\n![](images/30e77ba0dd7c5f87081ad96c498ba2bc53c2f52efb2eca7f8e94f7770d44418f.jpg)\n\n\n![](images/206b4f15229cefcba2ff4fb1e8ef182b912bc66f2c7a363f11eb3e8ebc403f4a.jpg)\n\n\nwhere  $\\mathbf{C}'$  is a subset of  $\\mathbf{C}$  excluding concept  $\\mathbf{c}_i$ , and  $\\eta_{\\mathbf{f},S}^j$  is the per-class detection completeness with respect to class  $j$ . This Shapley value captures the average marginal contribution of concept  $\\mathbf{c}_i$  towards explaining the decisions of the OOD detector for inputs that are predicted into class  $j$ .\n\n\nEventually, we interpret the behavior of the given OOD detector by plotting the concept score patterns with respect to the concepts ranked by the above Shapley importance score. Figure 4 illustrates the generated explanations given correctly-detected inputs (ID/OOD input detected as ID/OOD; first row of figure), and incorrectly-detected inputs (ID/OOD input detected as OOD/ID; second row of figure). Overall, we observe that the OOD detector predicts an input as ID when the concept scores show a similar pattern to the ID profile, or predicts an input as OOD when the concept score pattern is far from the ID profile. For instance, the fourth input is OOD image from Places dataset but detected as ID, since its score for C54 (furry dog skin) is as high as usual ID Collie images (which is true in the image). Thus, we conclude this to be an understandable mistake by the OOD detector.\n\n\nWe also provide qualitative comparison between Yeh et al. and our method in the resulting explanations for OOD detector. We observe that Yeh et al. fails to generate visually-distinguishable explanations between detected-ID and detected-OOD inputs. The separation between the solid green bars and the orange bars in each figure becomes more visible in our explanations, which enables more intuitive interpretation for human users, and this reflects our design goal for concept separability. It is also noteworthy that our concepts that are most important to distinguish ID Collie from OOD Collie (i.e., C54 and C30) are more specific, and finer-grained characteristics of Collie, while Yeh et al. finds concepts that are vaguely similar to the features of dog, but rather generic (i.e., C43 and C29). This is the reason we require more number of concepts to achieve detection completeness and concept separability, compared to solely considering the classification completeness<sup>6</sup>.\n\n\n# 5 CONCLUSION\n\n\nIn this work, we make a first attempt at developing an unsupervised and human-interpretable explanation method for black-box OOD detectors based on high-level concepts derived from the internal layer representations of a (paired) DNN classifier. We propose novel metrics viz. detection completeness and concept separability to evaluate the completeness (sufficiency) and quality of the learned concepts for OOD detection. The proposed concept learning method is quite general and applies to a broad class of off-the-shelf OOD detectors. Through extensive experiments and qualitative examples, we demonstrate the practical utility of our method for understanding and debugging an OOD detector. We discuss additional aspects of our method such as the choice of auxiliary OOD dataset, human subject study, and societal impact in Appendix E.\n\n\n# REFERENCES\n\n- Julius Adebayo, Michael Muelly, Ilaria Liccardi, and Been Kim. Debugging tests for model explanations. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/2020/bit/075b051ec3d22dac7b33f788da631fd4-Abstract.html.\n\n- David Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf.\n\n- Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.\n\n- Andre Araujo, Wade Norris, and Jack Sim. Computing receptive fields of convolutional neural networks. Distill, 2019. doi: 10.23915/distill.00021. https://distill.pub/2019/computing-receptive-fields.\n\n- Sharon Lee Armstrong, Lila R Gleitman, and Henry Gleitman. What some concepts might not be. Cognition, 13 (3):263-308, 1983.\n\n- Anil Bhattacharyya. On a measure of divergence between two statistical populations defined by their probability distributions. Bull. Calcutta Math. Soc., 35:99-109, 1943.\n\n- Diane Bouchacourt and Ludovic Denoyer. Educe: Explaining model decisions through unsupervised concepts extraction. arXiv preprint arXiv:1905.11852, 2019.\n\n- Senqi Cao and Zhongfei Zhang. Deep hybrid models for out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4733-4743, 2022.\n\n- Jianbo Chen, Le Song, Martin J Wainwright, and Michael I Jordan. L-shapley and c-shapley: Efficient model interpretation for structured data. In International Conference on Learning Representations, 2018.\n\n- Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-of-distribution detection using outlier mining. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 430-445. Springer, 2021.\n\n- Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3606-3613, 2014.\n\n- Terrance DeVries and Graham W Taylor. Learning confidence for out-of-distribution detection in neural networks. arXiv preprint arXiv:1802.04865, 2018.\n\n- Katsushige Fujimoto, Ivan Kojadinovic, and Jean-Luc Marichal. Axiomatic characterizations of probabilistic and cardinal-probabilistic interaction indices. Games and Economic Behavior, 55(1):72-99, 2006.\n\n- Keinosuke Fukunaga. Introduction to Statistical Pattern Recognition, chapter 3, pp. 97-103. Academic Press, 2 edition, 1990a.\n\n- Keinosuke Fukunaga. Introduction to Statistical Pattern Recognition, chapter 10, pp. 446-451. Academic Press, 2 edition, 1990b.\n\n- Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based explanations. Advances in Neural Information Processing Systems, 32:9277-9286, 2019.\n\n- Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.\n\n- Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.\n\n- Dan Hendrycks, Mantas Mazeika, and Thomas G. Dietterich. Deep anomaly detection with outlier exposure. In 7th International Conference on Learning Representations (ICLR). OpenReview.net, 2019. URL https://openreview.net/forum?id=HyxxCxhRcY7.\n\n- Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang, Bo Li, Dawn Song, and Jacob Steinhardt. Pixmix: Dreamlike pictures comprehensively improve safety measures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16783-16792, 2022.\n\n- Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\n- Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10951-10960, 2020.\n\n- Rui Huang and Yixuan Li. MOS: Towards scaling out-of-distribution detection for large semantic space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8710-8719, June 2021.\n\n- Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In International conference on machine learning, pp. 2668-2677. PMLR, 2018.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\n- Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In International Conference on Machine Learning, pp. 5338-5348. PMLR, 2020.\n\n- Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.\n\n- Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In International Conference on Learning Representations, 2018.\n\n- Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr D\u00f3lar, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In European Conference on Computer Vision - ECCV, volume 8693 of Lecture Notes in Computer Science, pp. 740-755. Springer, 2014. doi: 10. 1007/978-3-319-10602-1\\48. URL https://doi.org/10.1007/978-3-319-10602-1\\48.\n\n- Ziqian Lin, Sreya Dutta Roy, and Yixuan Li. MOOD: Multi-level out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15313-15323, 2021.\n\n- Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems, 33, 2020.\n\n- Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Proceedings of the 31st international conference on neural information processing systems, pp. 4768-4777, 2017.\n\n- Sina Mohseni, Mandar Pitale, JBS Yadawa, and Zhangyang Wang. Self-supervised learning for generalizable out-of-distribution detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 5216-5223, 2020.\n\n- Kevin P Murphy. Machine Learning: A Probabilistic Perspective, chapter 8, pp. 271-274. MIT press, 2012.\n\n- Khanh Xuan Nguyen and Brendan O'Connor. Posterior calibration and exploratory analysis for natural language processing models. In EMNLP, 2015.\n\n- Jayaram Raghuram, Varun Chandrasekaran, Somesh Jha, and Suman Banerjee. A general framework for detecting anomalous inputs to DNN classifiers. In International Conference on Machine Learning, pp. 8764-8775. PMLR, 2021.\n\n- Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135-1144, 2016.\n\n- LS Shapley. A value for n-person games. Contributions to the Theory of Games, (28):307-317, 1953.\n\n- Yiyou Sun, Chuan Guo, and Yixuan Li. ReAct: Out-of-distribution detection with rectified activations. In Advances in Neural Information Processing Systems (NeurIPS), pp. 144-157, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html.\n\n- Mukund Sundararajan and Amir Najmi. The many shapley values for model explanation. In International Conference on Machine Learning, pp. 9269-9278. PMLR, 2020.\n\n- Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International Conference on Machine Learning, pp. 3319-3328. PMLR, 2017.\n\n- Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826, 2016.\n\n- Joshua Brett Tenenbaum. A Bayesian framework for concept learning. PhD thesis, Massachusetts Institute of Technology, 1999.\n\n- Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning\u2014a comprehensive evaluation of the good, the bad and the ugly. IEEE transactions on pattern analysis and machine intelligence, 41(9):2251-2265, 2018.\n\n- Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 3485-3492. IEEE, 2010.\n\n- Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. CoRR, abs/2110.11334, 2021. URL https://arxiv.org/abs/2110.11334.\n\n- Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. On completeness-aware concept-based explanations in deep neural networks. Advances in Neural Information Processing Systems, 33:20554-20565, 2020.\n\n- Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6): 1452-1464, 2017.\n\n- Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. Interpretable basis decomposition for visual explanation. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 119-134, 2018a.\n\n- Guorui Zhou, Ying Fan, Runpeng Cui, Weijie Bian, Xiaogiang Zhu, and Kun Gai. Rocket launching: A universal and efficient framework for training well-performing light net. In Thirty-second AAAI conference on artificial intelligence, 2018b.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638572.1189122", "msecs": "118.0", "relativeCreated": "168949.40972328186", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code\n\n# Abstract\nCode has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline.\n\n\n# 1 INTRODUCTION\n\n\nVarious studies (Azerbayev et al., 2024; Shao et al., 2024) have shown that training on code enhances the mathematical reasoning abilities of large language models (LLMs). Previous research in continued mathematical pretraining often includes code that utilizes math-related packages (Azerbayev et al., 2024). This code is typically sourced from GitHub and is primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than focusing directly on mathematics. Recent models (Zhou et al., 2024; Yang et al., 2024b; Ying et al., 2024; Shao et al., 2024; Wang et al., 2023a) have adopted Tool-Integrated Reasoning (TIR) in fine-tuning. They utilize integrated natural language reasoning steps and Python code to improve performance on mathematical reasoning tasks. Reasoning with the help of code is particularly effective for more challenging problems, likely due to its precision and accuracy.\n\n\nAlthough utilizing existing open-source code in the pretraining phase can enhance the mathematical reasoning abilities of LLMs, such code often lacks accompanying natural language explanations or context. This might hinder the model's ability to fully understand them. In this paper, we propose a novel method for generating large amounts of mathematical code accompanied by corresponding natural language reasoning steps, which are extracted from math-related pretraining texts. Different from the existing math-related code, our generated code is paired with natural language reasoning steps, making the code more comprehensible. Also, as our code is generated based on math-related texts, they are all highly related to mathematical reasoning. When used in pretraining, the mathe\n\n\nmatical code paired with reasoning steps facilitates LLMs' understanding of math-related pretraining texts, as it effectively captures the underlying reasoning process. Furthermore, this data enhances the model's potential to be finetuned for TIR reasoning.\n\n\nOur data processing pipeline consists of two key steps: (1) carefully curating a robust basic dataset for pretraining, and (2) generating paired reasoning steps and mathematical code by extracting Latex expressions and their context, translating the extracted information into Python code snippets, executing the generated code snippets, and verifying their correctness.\n\n\nFirst, we gather and carefully filter a wide variety of math-related data sources, including web pages, model-generated data, math-related code, and textbooks. Through an advanced filtering process, we ensure the dataset is both large and highly relevant, minimizing irrelevant content while preserving the mathematical texts necessary for training. This results in a 16.5B-token dataset that forms the foundation of our pretraining efforts. By conducting experiments with smaller models, we show that this careful curation leads to more efficient training without sacrificing model performance.\n\n\nSecond, we propose a novel method for generating large amounts of paired mathematical reasoning steps and their corresponding Python code. Given a piece of text from the pretraining corpus collected above, we wrap it in a carefully designed prompt that instructs a Llama-3.1-70B-Instruct model to extract LaTeX expressions along with their relevant context, including the conditions for each expression and the result of its computation. This results in a list of comprehensive mathematical reasoning steps, complete with the necessary conditions, the computations taken, and the results. Then, we prompt the model to translate each reasoning step into a Python code snippet that captures the underlying reasoning process. The generated Python snippets are executed, and only those that run successfully and produce outputs matching the expected results are retained. By pairing the code with the corresponding reasoning step, we create the final data. The process is demonstrated in the lower half of Fig. 1. This process yields a 2.7B-token corpus of mathematical code snippets accompanied with their corresponding reasoning steps, which we combine with the data generated in the first step to create a 19.2B-token pretraining dataset, named MathCode-Pile.\n\n\nWe validate the effectiveness of MathCode-Pile on four popular base models: Llama-3-8B, DeepSeekMath-7B, Mistral-7B, and Code-Llama-7B, significantly improving their performance on five representative mathematical benchmarks. We name the resulting family of pretrained models MathCoder2. In particular, MathCoder2-Llama-3-8B achieves 4-shot accuracies of  $38.4\\%$  on MATH and  $69.9\\%$  on GSM8K, outperforming the baseline of training only on the basic data generated in the first step by  $3.1\\%$  and  $4.1\\%$ , respectively. This demonstrates that the data of mathematical code accompanied with reasoning steps effectively enhances LLMs' reasoning abilities.\n\n\nDifferent from recent works, such as DeepSeekMath (Shao et al., 2024), InternLM-Math (Ying et al., 2024), and Qwen2.5-Math (Yang et al., 2024b), which only release their model weights, we offer a detailed, open-source framework for data processing and training that achieves performance competitive with these models, fostering further progress in mathematical reasoning for LLMs.\n\n\nOur contributions include:\n\n- - A novel and effective method for generating large amounts of mathematical code with corresponding natural language reasoning steps, significantly enhancing pretraining outcomes.\n\n- - The creation of MathCode-Pile, a meticulously curated 19.2B-token dataset for continued mathematical pretraining. This dataset includes math-related web data, synthetic data, code, textbooks, and model-translated mathematical code.\n\n- - Full open-sourcing of all data processing and training code, ensuring transparency and reproducibility to support future research.\n\n\n# 2 CURATION OF MATHCODE-PILE\n\n\nWe curate our mathematical pretraining dataset, MathCode-Pile, in two steps: first, we collect the basic data in Sec. 2.1, and then we use it to generate mathematical code snippets with their corresponding natural language reasoning steps in Sec. 2.2.\n\n\n![Figure 1: The data processing pipeline. (a) shows the pipeline of prior works. (b) demonstrates our method. We first use a fastText classifier to filter the Common Crawl corpus, resulting in the initial filtered math texts. Then, we annotate part of the filtered texts to train a new fastText classifier, and conduct a second filtering, resulting in the finer filtered math texts. Then we use an instruction-tuned LLM to extract reasoning steps from these math-related texts, and translate the reasoning steps into corresponding code snippets. We execute the code snippets and compare the output with the expected result. If the code executes successfully and the result is as expected, the code is retained.](images/d9702e8efdfee905cc47e77b78e3b4c586dd9d0fe136c552c6913e5f0cee72b9.jpg)\n\n\n# 2.1 BASIC DATA\n\n\nWe collect and carefully filter a diverse range of mathematical data to ensure relevance and quality for continued pretraining of LLMs. The data includes math-related web content, synthetic data, code utilizing mathematical packages, and mathematical textbooks.\n\n\nMath-related Web Data. Web data offers a broad range of real-world mathematical examples. We start with the OpenWebMath (Paster et al., 2023) dataset, which contains mathematical web pages sourced from Common CWEl. Observing that a significant portion of these documents are unrelated to mathematics, we instruct the Mixtral-8x7B-Instruct model with a carefully designed prompt (detailed in Appendix A) to filter out irrelevant texts. Examples of irrelevant texts are shown in Appendix D. This reduces the dataset from 13.7B tokens to 4.8B tokens (measured using the Llama-3 tokenizer). We call this filtered version filtered-OpenWebMath.\n\n\nTo further expand the dataset, we train a fastText classifier (Joulin et al., 2016) using filtered-OpenWebMath as positive samples and random Common Crawl data as negative samples (training details are explained in Appendix. B). This model helps identify additional math-related documents within the Common Crawl data from Matrix (Zhang et al., 2024), a general pretraining dataset. A second round of filtering is performed, where Mixtral-8x7B-Instruct annotates a portion of these documents, and a new fastText classifier trained based on these annotations further refines the data. This produces 6.4B tokens, which we label as filtered-CC-En-math. Finally, we combine filtered-OpenWebMath and filtered-CC-En-math, resulting in a comprehensive 11.2B-token math-related web dataset.\n\n\nSynthetic Data. Synthetic data offers structured mathematical texts that complement the web data. We collect synthetic data from various open-source repositories on Hugging Face, including datasets like Education-College-Students<sup>1</sup>, Maths-College<sup>2</sup>, and synthetic math books from Matrix (Zhang\n\n\nPrompt: You will be presented with a text related to math. I need you to identify all the complex computations in it. For each complex computation, find out the conditions needed for the computation, the LaTeX expression that conducts the computation, and the result of the computation. Then generate a Python code snippet for each computation that demonstrates how the result is reached. Output each computation in the following format:\n\n\nConditions Needed:\n\n- 1. [Condition 1]\n\n- 2. [Condition 2]\n\n\n\u00b7\u00b7\u00b7\n\n\nComputation Expression:\n\n\n[ \\text{[LaTeX Expression]} ]\n\n\nComputation Result:\n\n\n[Computation Result]\n\n\nPython Code Snippets:\n\n\n``python\n\n\n[Python Code]\n\n\n1\n\n\nThere can be more than one complex computation in the text. Output only the computations that require calculation. Do not include mathematical statements or definitions as a computation. Make sure each snippet can be executed individually. The text is as follows: {TEXT}\n\n\nThe computations are:\n\n\nFigure 2: The prompt for extracting reasoning steps from texts in the pretraining corpus and generating the corresponding Python snippets. {TEXT} is replaced with the text from the dataset collected in Sec. 2.1.\n\n\net al., 2024). To ensure relevance, we apply a fastText classifier to filter out non-mathematical documents, refining the dataset to 2.2B tokens of high-quality synthetic math content.\n\n\nCode Utilizing Mathematical Packages. Code data offers practical examples of how mathematical libraries are used in programming. We collect code from Python and Jupyter files within the StarCoderData dataset (Li et al., 2023), retaining only programs that import math-related packages such as sympy, fractions, cmath, scipy, or statistics. The widely used numpy package is not used to filter the documents, as it appears frequently in non-mathematical contexts. After filtering, this collection process results in 1.7B tokens of code related to mathematical computations.\n\n\nMathematical Textbooks. Textbooks provide formal, structured presentations of mathematical concepts, making them a valuable source of math knowledge. We gather 8K PDFs of textbooks from online resources by identifying those with titles containing math-related keywords such as algebra, geometry, probability, etc. These PDF files are then converted into markdown format using the Nougat tool for easier integration into our training pipeline.\n\n\n# 2.2 MODEL-TRANSLATED MATHEMATICAL CODE\n\n\nIn this section, we propose a novel approach for extracting reasoning steps from the basic pretraining data and translating them into corresponding Python code snippets that capture the underlying reasoning processes. This extraction and translation process is performed using a strong instruction-tuned model, which is Llama-3.1-70B-Instruct in this paper.\n\n\nOur method begins with taking a piece of text from the basic pretraining data and wrapping it in a carefully designed prompt, as shown in Fig. 2. This prompt instructs the model to identify LaTeX expressions denoting complex computations, along with the necessary context, including the conditions required for the computation and the expected result. By explicitly extracting the conditions of the LaTeX expression, we enhance the model's ability to comprehend the underlying mathematical reasoning behind the usage of the expression. The expected result of the computation can later serve as a basis for verifying the correctness of the generated code. A mathematical reasoning step is con\n\n\nTable 1: The components and data statistics of MathCode-Pile.\n\n\n<table><tr><td>Components</td><td>Size (MB)</td><td>Documents</td><td>Tokens</td><td>Average (Tokens)</td></tr><tr><td>Filtered-OpenWebMath</td><td>16,999</td><td>2,824,705</td><td>4,826,902,621</td><td>1,709</td></tr><tr><td>Filtered-CC-En-math</td><td>23,465</td><td>7,597,718</td><td>6,341,745,645</td><td>835</td></tr><tr><td>Synthetic data</td><td>8,855</td><td>2,195,974</td><td>2,193,189,314</td><td>999</td></tr><tr><td>Code using math packages</td><td>6,120</td><td>513,059</td><td>1,703,226,005</td><td>3,320</td></tr><tr><td>Mathematical textbooks</td><td>4,431</td><td>8,373</td><td>1,390,268,773</td><td>166,042</td></tr><tr><td>Translated mathematical code</td><td>8,235</td><td>6,347,823</td><td>2,728,740,985</td><td>430</td></tr><tr><td>Total</td><td>68,105</td><td>19,487,652</td><td>19,184,073,343</td><td>984</td></tr></table>\n\n\nstructured by combining the conditions, expression and result. The prompt then directs the model to produce a Python code snippet that accurately reflects the underlying reasoning process behind the extracted reasoning step. The model is asked to present the conditions, LaTeX expression, result, and Python code snippet in a structured format, ensuring that each part can be easily extracted from the generated text. Examples of generated texts are shown in Appendix C.\n\n\nAfter the Python code snippets are generated, they are executed, and outputs of the execution are compared with the expected results extracted from the generated text. Only the Python code snippets that execute without errors and produce correct outputs are retained. This filtering process ensures a higher quality of generated code, making the resulting dataset more reliable for mathematical pretraining compared to approaches that rely on unverified and general-purpose code.\n\n\nLeveraging the Llama-3.1-70B-Instruct model, we initially generated 3.1B tokens of the data. After applying the filtering process, we obtain a total of 2.7B tokens of high-quality data of mathematical code snippets accompanied with their corresponding reasoning steps. This newly generated data significantly enriches our original pretraining corpus. By combining this data with the basic pretraining data, we create a comprehensive pretraining dataset totaling 19.2B tokens, which we refer to as MathCode-Pile. Detailed statistics of MathCode-Pile are presented in Tab. 1. This dataset is tailored specifically for enhancing the mathematical and coding abilities of LLMs. To avoid benchmark contamination, we follow Yang et al. (2024b) to filter out samples that have significant overlaps with any of the questions from benchmark datasets used in evaluation. We use exact match to remove the identical samples and further apply 13-gram dedduplication (with a condition that the Jaccard similarity should be larger than 0.6) to remove more samples that might cause contamination.\n\n\nIn comparison to traditional methods of curating math-related code, which often draw on general-purpose repositories, our method ensures that the code is not only syntactically correct but also mathematically sound, reflecting a deeper understanding of mathematical reasoning. Our mathematical code is accompanied with corresponding natural language reasoning steps, which makes understanding the reasoning process easier. This makes MathCode-Pile a superior resource for models aimed at performing advanced mathematical reasoning tasks.\n\n\n# 3 EXPERIMENTS\n\n\nTo demonstrate the effectiveness of our method, we first train several base models ranging from 7B to 8B parameters using MathCode-Pile and compare them to other best-performing models of the same size. The group of models resulting from the continued mathematical pretraining is named MathCoder2. Next, we train and compare various other open-source math pretraining datasets against MathCode-Pile using a smaller model, DeepSeekCoder-1.3B. To showcase the potential of the MathCoder2 models, we further perform supervised fine-tuning on them. Finally, we conduct ablation studies to analyze the impact of each component of the dataset.\n\n\n# 3.1 MAIN RESULTS\n\n\nBenchmark datasets. We evaluate the MathCoder2 models on five representative datasets: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), SAT-Math (Azerbayev et al., 2024), OCW (Lewkowycz et al., 2022), and MMLU-Math (Hendrycks et al., 2021a). GSM8K and MATH\n\n\nTable 2: Performance of various pretrained models on five representative mathematical datasets. All results reported are based on greedy decoding. \"Code-open\" shows whether the code for data-processing and model-training is open-sourced. The red numbers show the improvements compared to the base model from which each MathCoder2 model is trained.\n\n\n<table><tr><td>Model</td><td>Size</td><td>Code-open</td><td>MATH</td><td>GSM8K</td><td>SAT</td><td>OCW</td><td>MMLU-MATH</td></tr><tr><td>Qwen2-Math</td><td>7B</td><td>X</td><td>50.4</td><td>80.4</td><td>87.5</td><td>14.0</td><td>57.9</td></tr><tr><td>Qwen2.5-Math</td><td>7B</td><td>X</td><td>55.4</td><td>91.6</td><td>-</td><td>-</td><td>-</td></tr><tr><td>InternLM2.5</td><td>7B</td><td>X</td><td>34.0</td><td>74.8</td><td>65.6</td><td>8.1</td><td>49.6</td></tr><tr><td>InternLM2-Math-Base</td><td>7B</td><td>X</td><td>21.5</td><td>49.2</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Llemma</td><td>7B</td><td>\u2713</td><td>18.0</td><td>36.4</td><td>53.1</td><td>7.7</td><td>-</td></tr><tr><td>Llama-2</td><td>7B</td><td>X</td><td>3.2</td><td>11.8</td><td>25.0</td><td>3.7</td><td>-</td></tr><tr><td>Llama-3</td><td>8B</td><td>X</td><td>21.4</td><td>54.8</td><td>56.3</td><td>10.3</td><td>42.8</td></tr><tr><td>MathCoder2-Llama-3</td><td>8B</td><td>\u2713</td><td>38.4(+17.0)</td><td>69.9(+15.1)</td><td>84.4(+28.1)</td><td>18.0(+7.7)</td><td>46.5(+3.7)</td></tr><tr><td>DeepSeekMath</td><td>7B</td><td>X</td><td>36.2</td><td>64.2</td><td>84.4</td><td>15.4</td><td>47.4</td></tr><tr><td>MathCoder2-DeepSeekMath</td><td>7B</td><td>\u2713</td><td>38.6(+2.4)</td><td>68.8(+4.6)</td><td>90.6(+6.2)</td><td>16.9(+1.5)</td><td>48.3(+0.9)</td></tr><tr><td>Mistral</td><td>7B</td><td>X</td><td>13.1</td><td>52.2</td><td>75.0</td><td>8.5</td><td>38.3</td></tr><tr><td>MathCoder2-Mistral</td><td>7B</td><td>\u2713</td><td>36.7(+23.6)</td><td>68.2(+16.0)</td><td>81.3(+6.3)</td><td>13.2(+4.7)</td><td>42.2(+3.9)</td></tr><tr><td>Code-Llama</td><td>7B</td><td>X</td><td>6.7</td><td>14.6</td><td>25.0</td><td>3.7</td><td>26.4</td></tr><tr><td>MathCoder2-Code-Llama</td><td>7B</td><td>\u2713</td><td>28.8(+22.1)</td><td>52.3(+37.7)</td><td>71.9(+46.9)</td><td>8.5(+4.8)</td><td>33.7(+7.3)</td></tr></table>\n\n\nare tested using a 4-shot prompt with MAmmoTH's evaluation framework (Yue et al., 2023). SATMath and OCW are tested using a 4-shot prompt with DeepSeekMath's evaluation framework (Shao et al., 2024). MMLU-Math is tested using the lm-evaluation-harness's (Gao et al., 2024) default zero-shot setting for MMLU. These datasets cover a wide range of mathematical problems across various types and difficulty levels, from primary school math word problems to college-level challenges, providing a comprehensive evaluation of the models.\n\n\nBase models and training settings. To demonstrate that our pretraining corpus is effective across different base models, we continue pretraining four base models with MathCode-Pile: Llama-3-8B (Dubey et al., 2024), DeepSeekMath-7B (Shao et al., 2024), Mistral-7B (Jiang et al., 2023), and Code-Llama-7B (Rozi\u00e8re et al., 2024). MathCoder2-Llama-3-8B is trained for 3 epochs with a global batch size of 4 million tokens and an 8192 token context length. MathCoder2-DeepSeekMath, MathCoder2-Mistral, and MathCoder2-CodeLlama are each trained for 3 epochs with a global batch size of 4 million tokens and a 4096 token context length.\n\n\nBaselines. We compare our method with various other base models that possess strong mathematical abilities and are of similar sizes, including Qwen2-Math 7B (Yang et al., 2024a), Qwen2.5-Math 7B (Yang et al., 2024b), InternLM2-Math-Base 7B (Ying et al., 2024), InternLM2.5 7B (Cai et al., 2024), DeepSeekMath 7B (Shao et al., 2024), Llemma 7B (Azerbayev et al., 2024), Mistral 7B (Jiang et al., 2023), Llama2 7B (Touvron et al., 2023), Llama3 8B (Dubey et al., 2024) and Code-Llama 7B (Rozi\u00e8re et al., 2024).\n\n\nResults: As demonstrated in Tab. 2, continued pretraining on MathCode-Pile consistently improves performance across all five benchmark datasets. MathCoder2 models rival the performance of top models like InternLM2-Math-Base, InternLM2.5, and DeepSeekMath. In particular, MathCoder2-DeepSeekMath demonstrates that our method continues to enhance the performance of DeepSeekMath, a model that has already been extensively trained on large amounts of math-related data. However, there remains a performance gap between MathCoder2 and the Qwen2-Math and Qwen2.5-Math models. This gap might be attributed to their superior computational, manual, and financial resources, which enable the scaling of data size and the further improvement of data quality, reporting a mathemtcaical dataset of 700B tokens (Yang et al., 2024b).\n\n\nIn contrast to models like Qwen2-Math, which only open-source their model weights, with much of their data processing and training details undisclosed, MathCoder2 is fully open-sourced, including all data processing pipelines and training code. This openness facilitates transparency, reproducibil-\n\n\nTable 3: Performance of various finetuned models on five representative mathematical datasets. All results reported are based on greedy decoding.\n\n\n<table><tr><td>Model</td><td>Size</td><td>MATH</td><td>GSM8K</td><td>OCW</td><td>Olympiad Bench</td><td>SVAMP</td></tr><tr><td>Qwen2-Math-Instruct</td><td>7B</td><td>75.1</td><td>89.9</td><td>34.6</td><td>38.2</td><td>-</td></tr><tr><td>Qwen2.5-Math-Instruct</td><td>7B</td><td>83.6</td><td>95.2</td><td>37.1</td><td>41.6</td><td>-</td></tr><tr><td>DeepSeekMath-Instruct-CoT</td><td>7B</td><td>46.8</td><td>82.9</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DeepSeekMath-Instruct-TIR</td><td>7B</td><td>57.4</td><td>83.7</td><td>-</td><td>-</td><td>-</td></tr><tr><td>InternLM2-math-plus</td><td>7B</td><td>54.4</td><td>84.0</td><td>17.3</td><td>18.8</td><td>-</td></tr><tr><td>NuminaMath-7B-CoT</td><td>7B</td><td>55.2</td><td>75.4</td><td>19.1</td><td>19.9</td><td>-</td></tr><tr><td>NuminaMath-7B-TIR</td><td>7B</td><td>68.1</td><td>84.6</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ToRA-Code</td><td>7B</td><td>44.6</td><td>72.6</td><td>-</td><td>-</td><td>70.4</td></tr><tr><td>MathCoder</td><td>7B</td><td>30.2</td><td>67.8</td><td>-</td><td>-</td><td>70.7</td></tr><tr><td>MAmmoTH2-Plus</td><td>8B</td><td>42.8</td><td>84.1</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Llama-3.1-Instruct</td><td>8B</td><td>47.2</td><td>76.6</td><td>21.7</td><td>15.4</td><td>-</td></tr><tr><td>MathCoder2-Llama-3-Instruct-CoT</td><td>8B</td><td>58.5</td><td>83.9</td><td>29.4</td><td>25.8</td><td>92.7</td></tr><tr><td>MathCoder2-Llama-3-Instruct-TIR</td><td>8B</td><td>69.7</td><td>85.8</td><td>37.6</td><td>37.6</td><td>94.9</td></tr><tr><td>MathCoder2-DeepSeekMath-Instruct-CoT</td><td>7B</td><td>55.2</td><td>80.3</td><td>30.9</td><td>23.0</td><td>92.1</td></tr><tr><td>MathCoder2-DeepSeekMath-Instruct-TIR</td><td>7B</td><td>69.6</td><td>86.5</td><td>41.9</td><td>37.9</td><td>92.8</td></tr></table>\n\n\nity, and further research, which is crucial for advancing the field. Compared to Llemma, which also open-sources its code, our method achieves better performance on the five datasets. Particularly, when trained on the same base model, Code-Llama, our method performs significantly better, which demonstrates the effectiveness of the MathCode-Pile pretraining corpus.\n\n\n# 3.2 POST-TRAINING\n\n\nTo further demonstrate the potential of the MathCoder2 models in aligning to mathematical problem-solving tasks, we select the MathCoder2-Llama-3-8B model and MathCoder2-DeepSeekMath-7B for finetuning on mathematical problem-solution pairs. We first train the base model on general mathematical instructions following Yue et al. (2024) for two epochs. Subsequently, we finetune the model on NuminaMath-CoT $^3$ , and NuminaMath-TIR $^4$  datasets for three epochs.\n\n\nThe results are shown in Tab. 3. MathCoder2-Instruct-TIR achieves high performance on all five datasets, reaching  $69.7\\%$  on MATH and  $86.5\\%$  on GSM8K, outperforming many of the best open-source models of similar size and demonstrating our method's potential to improve performance on downstream mathematical reasoning tasks. As this paper focuses on continued mathematical pretraining, the post-training serves only as a validation of the potential of our models. We conducted only simple supervised fine-tuning, without performing reinforcement learning or direct preference optimization, which could further improve performance on downstream tasks.\n\n\n# 3.3 ABLATION STUDIES\n\n\nIn this session, we first analyze the impact of various components of the training data. Next, we compare MathCode-Pile to other open-source mathematical pretraining corpora.\n\n\nAnalysis of the impact of the mathematical code. We analyze the impact of the mathematical code on continued pretraining by comparing the results of adding and not adding the mathematical code. As shown in Tab. 4, the addition of the mathematical code in the pretraining corpus significantly improves performance across all five datasets. Note that the mathematical code only constitutes  $14.1\\%$  of the 19.2B tokens in the MathCode-Pile dataset, yet the improvement in accuracy it brings about compared to the total improvement in accuracy  $(\\frac{acc_{MathCode - Pile} - acc_{basic}}{acc_{MathCodePile} - acc_{orig}})$  is  $21.8\\%$ ,  $27.1\\%$ ,  $44.5\\%$ ,  $66.2\\%$ , and  $35.1\\%$  on the five benchmark datasets, respectively, demonstrating the effectiveness of the mathematical code. Comparison across different training steps is shown in Appendix F.\n\n\nTable 4: Analysis of the impact of the mathematical code. The upper half presents the results of using and not using the mathematical code data. The lower half analyzes design of concatenating the reasoning steps and code snippets. \"Basic + Reasoning-step-only\" represents only adding the conditions, expressions, and results, while \"Basic + Trans-code-only\" represents only adding the translated code. \"Basic + Separated Text&Code\" represents seperating corresponding code and text. \"Reasoning-Step&Code\" represents the concatenated data combining both. \"Basic + No-code-prompt\" represents using a prompt that simply instruct Llama-3.1-70B-Instruct to rewrite texts to improve their quality.\n\n\n<table><tr><td>Data Composition</td><td>Base Model</td><td>MATH</td><td>GSM8K</td><td>SAT</td><td>OCW</td><td>MMLU-MATH</td></tr><tr><td>Basic</td><td>Llama-3-8B</td><td>34.7</td><td>65.8</td><td>71.9</td><td>12.9</td><td>45.2</td></tr><tr><td>Basic + Reasoning-Step&amp;Code</td><td>Llama-3-8B</td><td>38.4(+3.7)</td><td>69.9(+4.1)</td><td>84.4(+12.5)</td><td>18.0(+5.1)</td><td>46.5(+1.3)</td></tr><tr><td>Basic + Reasoning-step-only</td><td>DeepSeekCoder-1.3B</td><td>16.7</td><td>22.7</td><td>40.6</td><td>4.8</td><td>25.9</td></tr><tr><td>Basic + Trans-code-only</td><td>DeepSeekCoder-1.3B</td><td>14.6</td><td>22.1</td><td>43.8</td><td>5.5</td><td>25.5</td></tr><tr><td>Basic + No-code-prompt</td><td>DeepSeekCoder-1.3B</td><td>15.7</td><td>21.3</td><td>37.5</td><td>4.8</td><td>24.4</td></tr><tr><td>Basic + Separated Text&amp;Code</td><td>DeepSeekCoder-1.3B</td><td>17.0</td><td>22.0</td><td>46.9</td><td>4.8</td><td>25.3</td></tr><tr><td>Basic + Reasoning-Step&amp;Code</td><td>DeepSeekCoder-1.3B</td><td>17.8</td><td>25.5</td><td>59.4</td><td>5.9</td><td>26.1</td></tr></table>\n\n\nTable 5: Analysis of the effect of different components in MathCoder2-Pile. The base model is DeepSeekCoder-1.3B.\n\n\n<table><tr><td>Data Composition</td><td>MATH</td><td>GSM8K</td><td>SAT</td><td>OCW</td><td>MMLU-MATH</td></tr><tr><td>No Math Training</td><td>4.8</td><td>4.3</td><td>18.8</td><td>2.6</td><td>24.8</td></tr><tr><td>filtered-OpenWebMath (4.8B)</td><td>9.0</td><td>11.4</td><td>34.4</td><td>3.7</td><td>25.4</td></tr><tr><td>OpenWebMath (12.9B)</td><td>9.4</td><td>11.2</td><td>31.3</td><td>2.6</td><td>24.4</td></tr><tr><td>filtered-CC-En-math (6.4B)</td><td>9.1</td><td>12.1</td><td>31.3</td><td>3.7</td><td>25.2</td></tr><tr><td>CC-En-math (22.1B)</td><td>8.4</td><td>13.0</td><td>25.0</td><td>2.9</td><td>25.0</td></tr><tr><td>filtered-OpenWebMath + textbooks</td><td>9.4</td><td>12.7</td><td>50.0</td><td>4.0</td><td>25.4</td></tr><tr><td>filtered-OpenWebMath + synthetic data</td><td>10.8</td><td>12.6</td><td>50.0</td><td>4.0</td><td>25.6</td></tr><tr><td>filtered-OpenWebMath + code</td><td>9.4</td><td>12.1</td><td>46.9</td><td>4.0</td><td>25.4</td></tr><tr><td>MathCoder2-Pile</td><td>17.8</td><td>25.5</td><td>59.4</td><td>5.9</td><td>26.1</td></tr></table>\n\n\nWe also analyze the design choice of concatenating the natural language reasoning step with the mathematical code for pretraining. This analysis is conducted by studying the results of adding only the natural language reasoning steps, and separately adding only the translated code. As shown in Tab. 4, Basic + Reasoning-step-only represents adding only the natural language reasoning steps; Basic + Trans-code-only represents adding only the translated code; Basic + Separated Text&Code represents seperating code and text; and Basic + Reasoning-Step&Code represents adding the concatenated data that combines both. The Basic + Reasoning-Step&Code configuration results in the best performance, demonstrating the importance of including both the natural language reasoning step and the translated mathematical code.\n\n\nTo rule out the possibility that the improvement comes from the higher quality of texts generated by Llama-3.1-70B-Instruct, we use a prompt that asks Llama-3.1-70B-Instruct to rewrite the given text. The details of this prompt are provided in Appendix E. We present the results of replacing the mathematical code with texts generated using this prompt in Tab. 4, labeled as \"Basic + No-code-prompt\". Our method of generating mathematical code accompanied with corresponding reasoning steps outperforms this baseline, demonstrating the effectiveness of our approach.\n\n\nAnalysis of the impact of various parts of the basic data. We perform experiments on a smaller model, DeepSeekCoder-1.3B, using different parts of the basic data. As demonstrated in Tab. 5, filtered-OpenWebMath and filtered-CC-En-math significantly improve the performance of the model. In comparison, textbooks, synthetic data, and code are smaller in data size and play a less important role. As each of these parts of data is too small for individual pretraining, we combine\n\n\nTable 6: Comparison between MathCode-Pile and other Mathematical Pretrain datasets.\n\n\n<table><tr><td>Pretrain Dataset</td><td>Base Model</td><td>MATH</td><td>GSM8K</td><td>SAT</td><td>OCW</td><td>MMLU-MATH</td></tr><tr><td>No Math Training</td><td>DeepSeekCoder-1.3B</td><td>4.8</td><td>4.3</td><td>18.8</td><td>2.6</td><td>24.8</td></tr><tr><td>OpenWebMath</td><td>DeepSeekCoder-1.3B</td><td>9.4</td><td>11.2</td><td>31.3</td><td>2.6</td><td>24.4</td></tr><tr><td>Proof-Pile-2</td><td>DeepSeekCoder-1.3B</td><td>9.2</td><td>11.2</td><td>50.0</td><td>4.4</td><td>25.8</td></tr><tr><td>MathPile</td><td>DeepSeekCoder-1.3B</td><td>5.3</td><td>3.4</td><td>21.9</td><td>2.2</td><td>24.9</td></tr><tr><td>DeepSeekMath Corpus</td><td>DeepSeekLLM-1.3B</td><td>13.6</td><td>23.8</td><td>56.3</td><td>4.8</td><td>-</td></tr><tr><td>MathCoder2-Pile</td><td>DeepSeekCoder-1.3B</td><td>17.8</td><td>25.5</td><td>59.4</td><td>5.9</td><td>26.1</td></tr></table>\n\n\nTable 7: Comparison between finetuning the original Llama-3-8B, MathCoder2-Basic-Llama-3-8B, and MathCoder2-Llama-3-8B on NuminaMath-TIR. MathCoder2-Basic-Llama-3-8B is the model resulting from continued pretraining on the basic data without adding the model-translated mathematical code.\n\n\n<table><tr><td>Base Model</td><td>MATH</td><td>GSM8K</td><td>OCW</td><td>Olympiad Bench</td><td>SVAMP</td></tr><tr><td>Llama-3-8B</td><td>56.1</td><td>80.1</td><td>24.6</td><td>28.4</td><td>83.8</td></tr><tr><td>MathCoder2-Basic-Llama-3-8B</td><td>62.9</td><td>81.3</td><td>26.8</td><td>32.9</td><td>86.7</td></tr><tr><td>MathCoder2-Llama-3-8B</td><td>65.1</td><td>84.5</td><td>34.6</td><td>34.4</td><td>87.9</td></tr></table>\n\n\nthem with OpenWebMath-filtered to show that they each bring a small yet noticeable improvement compared to using only OpenWebMath-filtered. Since we performed filtering on OpenWebMath and the initially filtered CC-En to remove irrelevant data, we also compare the performance before and after filtering. We observe that there is no obvious degradation in performance after removing irrelevant content, showing the effectiveness of the filtering.\n\n\nComparison with other open-source mathematical pretraining corpora. We compare MathCode-Pile with various other open-source mathematical pretraining corpora. We train each corpus for 3 epochs with a global batch size of 2 million tokens and a 4096 token context length, since we observe that the model's performance usually saturates around 3 epochs. As shown in Tab. 6, MathCode-Pile significantly outperforms OpenWebMath, Proof-Pile-2, and MathPile when trained on DeepSeekCoder-1.3B. The DeepSeekMath Corpus is not open-source, and its performance on DeepSeekLLM-1.3B is taken from Shao et al. (2024), which is trained for 150B tokens, more than our MathCode-Pile's training of approximately 60B tokens. The 1.3B model trained with MathCode-Pile outperforms the 1.3B model trained with DeepSeekMath Corpus.\n\n\nAnalysis of the improvement on the potential of being finetuned for TIR reasoning. To analyze the effect of the model-translated mathematical code on LLMs' potential to be finetuned for TIR reasoning, we finetune the original Llama-3-8B, MathCoder2-Basic-Llama-3-8B, and MathCoder2-Llama-3-8B on NuminaMath-TIR for three epochs, respectively. As shown in Tab. 7, the results of finetuning on MathCoder2-Basic-Llama-3-8B are higher than the results of finetuning on Llama-3-8B. Finetuning on MathCoder2-Llama-3-8B results in even higher performance than finetuning on MathCoder2-Basic-Llama-3-8B, showing that the addition of mathematical code effectively enhances the models' potential of being finetuned for TIR reasoning.\n\n\n# 4 RELATED WORK\n\n\nContinued mathematical pretraining. Several works (Shao et al., 2024; Azerbayev et al., 2024; Ying et al., 2024; Yang et al., 2024b) have explored the continued pretraining of LLMs on mathematical data, such as mathematical web content, synthetic data, and code. InternLM-Math (Ying et al., 2024) and Query of CC Fei et al. (2024) use BM25 for data retrieval, while other works such as DeepSeekMath (Shao et al., 2024) and Qwen2-Math (Yang et al., 2024b) employ fastText (Joulin et al., 2016) and other meta-information to retrieve texts from Common Crawl. Our approach follows these methods by using fastText for data filtering, and we introduce a second iteration of finer filtering to retain more relevant data. MathPile (Wang et al., 2023b) and phi (Gunasekar et al.,\n\n\n2023) utilize real or synthesized textbooks, while Llemma (Azerbayev et al., 2024) and Qwen2-Math (Yang et al., 2024b) incorporate math-related code in their datasets. However, unlike our method of generating mathematical code with accompanied natural language reasoning, their code mostly has no natural language explanations or context. Our work builds on these prior efforts by collecting and expanding upon these sources of math-related text. Unlike works that only open-source their model weights, we take a more transparent approach by open-sourcing both our data processing and model training code, thereby ensuring reproducibility and facilitating future research in this field. Compared to Llemma (Azerbayev et al., 2024), which also open-source their data and training code, our method results in better performance on mathematical reasoning tasks.\n\n\nSynthetic data. Numerous finetuning (Yu et al., 2024; Wang et al., 2023a; Lu et al., 2024a) and pretraining Gunasekar et al. (2023); Wang et al. (2023b); Yang et al. (2024b) studies have explored training on synthetic data generated using language models or predefined templates. MathGLM (Yang et al., 2023) and InternLM-Math (Ying et al., 2024) use templates to generate synthetic numerical operation data, while phi (Gunasekar et al., 2023) produces textbook-quality data with models. EntiGraph (Yang et al., 2024c) generates diverse text by drawing connections between sampled entities. Our work proposes a novel method for extracting mathematical reasoning steps and generating synthetic code snippets that captures the underlying reasoning processes.\n\n\nPost-training. There are many methods for further improving the mathematical problem-solving abilities of LLMs. Supervised finetuning adjusts pretrained models using math problems and solutions in various formats, such as Chain-of-Thought (Yu et al., 2024; Yuan et al., 2023), Program-of-Thought (Yue et al., 2023), and Tool-Integrated Reasoning (Gou et al., 2024; Wang et al., 2023a; Liao et al., 2024). Reinforcement learning Lightman et al. (2023); Wang et al. (2024) and Direct Preference Optimization Rafailov et al. (2024); Xu et al. (2024); Lu et al. (2024b) utilize mathematical preference data to adjust the models' outputs. These methods are diverse and reveal the potential of pretrained models. Their performance is often influenced by the quality of the training data used in the pretraining stage. To explore the potential of finetuning our pretrained models for downstream tasks, we conduct supervised finetuning with existing open-source data.\n\n\n# 5 LIMITATIONS AND FUTURE WORK\n\n\nOne limitation of our work is that our continued pretraining corpus focuses primarily on mathematics and does not intentionally include other STEM subjects. Additionally, our pretraining data consists entirely of English texts, without incorporating math-related content in other languages, like Chinese. Due to limitations in computational resources, we only trained models ranging from 1.3B to 8B parameters. Future work could address these limitations by expanding the dataset to include other subjects and languages and by training on larger language models. Also, this paper primarily focuses on continued mathematical pretraining, so we did not apply reinforcement learning methods like PPO and GRPO, or Direct Preference Optimization in our post-training phase, which can further improve performance on mathematical reasoning tasks. In the future, we could explore these methods on our finetuned models. Also, this work did not discuss theorem proving with formal languages such as Lean and Coq, which is worth investigating in future works.\n\n\n# 6 CONCLUSION\n\n\nIn this paper, we present an effective open-source continued mathematical pretraining pipeline for enhancing mathematical reasoning of LLMs. Through the meticulous collection and filtering of diverse math-related texts, such as mathematical web content, synthetic data, code that uses mathematical packages, and math textbooks, we curate a basic dataset for continued mathematical pretraining. We then propose a novel method for extracting mathematical reasoning steps from the previously collected dataset and translating them to code snippets reflecting the underlying reasoning processes. By combining the basic data with the model-generated mathematical code accompanied with their corresponding reasoning steps, we produce a 19.2B-token mathematical pretraining corpus named MathCode-Pile, which significantly improves the performance of four different base models across five representative mathematical benchmarks. By open-sourcing the entire data processing pipeline and model training code, we actively promote transparency, reproducibility, and collaboration within the research community, facilitating future research in this area.\n\n\n# REFERENCES\n\n- Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics, 2024. URL https://arxiv.org/abs/2310.10631.\n\n- Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piaq: Reasoning about physical commonsense in natural language, 2019. URL https://arxiv.org/abs/1911.11641.\n\n- Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zhang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internl m2 technical report, 2024.\n\n- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168.\n\n- Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\n- Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Conghui He, Hang Yan, Dahua Lin, and Xipeng Qiu. Query of cc: Unearthing large scale domain-specific knowledge from public corpora, 2024. URL https://arxiv.org/abs/2401.14624.\n\n- Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602.\n\n- Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving, 2024. URL https://arxiv.org/abs/2309.17452.\n\n- Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2023. URL https://arxiv.org/abs/2306.11644.\n\n- Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021a. URL https://arxiv.org/abs/2009.03300.\n\n- Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021b. URL https://arxiv.org/abs/2103.03874.\n\n- Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825.\n\n- Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification, 2016. URL https://arxiv.org/abs/1607.01759.\n\n- Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858.\n\n- Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023. URL https://arxiv.org/abs/2305.06161.\n\n- Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. Mario: Math reasoning with code interpreter output - a reproducible pipeline, 2024. URL https://arxiv.org/abs/2401.08190.\n\n- Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step, 2023. URL https://arxiv.org/abs/2305.20050.\n\n- Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation, 2023. URL https://arxiv.org/abs/2305.01210.\n\n- Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms, 2024a. URL https://arxiv.org/abs/2402.16352.\n\n- Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Step-controlled dpo: Leveraging stepwise error for enhanced mathematical reasoning, 2024b. URL https://arxiv.org/abs/2407.00782.\n\n- Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text, 2023. URL https://arxiv.org/abs/2310.06786.\n\n- Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model, 2024. URL https://arxiv.org/abs/2305.18290.\n\n- Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950.\n\n- Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. URL https://arxiv.org/abs/1907.10641.\n\n- Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300.\n\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288.\n\n- Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning, 2023a. URL https://arxiv.org/abs/2310.03731.\n\n- Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024. URL https://arxiv.org/abs/2312.08935.\n\n- Zengzhi Wang, Rui Xia, and Pengfei Liu. Generative ai for math: Part i - mathpile: A billion-token-scale pretraining corpus for math, 2023b. URL https://arxiv.org/abs/2312.17120.\n\n- Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, Jie Tang, and Yuxiao Dong. Chatglm-math: Improving math problem-solving in large language models with a self-critique pipeline, 2024. URL https://arxiv.org/abs/2404.02893.\n\n- An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a.\n\n- An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024b. URL https://arxiv.org/abs/2409.12122.\n\n- Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang. Gpt can solve mathematical problems without a calculator, 2023. URL https://arxiv.org/abs/2309.03241.\n\n- Zitong Yang, Neil Band, Shuangping Li, Emmanuel Cand\u00e8s, and Tatsunori Hashimoto. Synthetic continued pretraining, 2024c. URL https://arxiv.org/abs/2409.07431.\n\n- Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, and Dahua Lin. Internl m- math: Open math large language models toward verifiable reasoning, 2024.\n\n\nPrompt: You will be provided with a block of text. I need you to classify the text into one of the following types:\n\n- 1. The text describes a mathematical problem and its solution.\n\n- 2. The text explains a mathematical concept or mathematical theory.\n\n- 3. The text explains a scientific or engineering concept that requires mathematical knowledge.\n\n- 4. The text describes a programming problem and its solution.\n\n- 5. The text explains a concept or theory related to programming.\n\n- 6. The text explains the usage of a programming language or software tool.\n\n- 7. The text does not belong to any of the types above.\n\n- Here's the text I've provided. Kindly analyze and classify it into type 1, 2, 3, 4, 5, 6 or 7. Put your choice behind \"The type is:\" Please do not generate any unrelated additional comments! The type number must match the type description. Here's one of the texts that needs to be classified: {TEXT} The type is:\n\n\nFigure 3: The prompt for annotation of OpenWebMath and the initially filtered CC-En documents. {TEXT} is replaced with the content of the document.\n\n- Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2024. URL https://arxiv.org/abs/2309.12284.\n\n- Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models, 2023. URL https://arxiv.org/abs/2308.01825.\n\n- Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning, 2023. URL https://arxiv.org/abs/2309.05653.\n\n- Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web, 2024. URL https://arxiv.org/abs/2405.03548.\n\n- Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830.\n\n- Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. Map-neo: Highly capable and transparent bilingual large language model series, 2024. URL https://arxiv.org/abs/2405.19327.\n\n- Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=c8McWs4Av0.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638573.4067974", "msecs": "406.0", "relativeCreated": "170237.29491233826", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Supervised Contrastive Regression\n\n# Abstract\nDeep regression models typically learn in an end-to-end fashion and do not explicitly try to learn a regression-aware representation. Their representations tend to be fragmented and fail to capture the continuous nature of regression tasks. In this paper, we propose Supervised Contrastive Regression (SupCR), a framework that learns a regression-aware representation by contrasting samples against each other based on their target distance. SupCR is orthogonal to existing regression models, and can be used in combination with such models to improve performance. Extensive experiments using five real-world regression datasets that span computer vision, human-computer interaction, and healthcare show that using SupCR achieves the state-of-the-art performance and consistently improves prior regression baselines on all datasets, tasks, and input modalities. SupCR also improves robustness to data corruptions, resilience to reduced training data, performance on transfer learning, and generalization to unseen targets.\n\n\n# 1 INTRODUCTION\n\n\nRegression problems are ubiquitous and fundamental in the real world. They include estimating age from human appearance (Rothe et al., 2015), predicting health scores from physiological signals (Engemann et al., 2022), and detecting gaze directions from webcam images (Zhang et al., 2017b). Since regression targets are continuous, the most widely used approach to train a regression model is to have the model predict the target value, and use the distance (e.g., L1 or L2 distance) between the prediction and the ground-truth target as the loss function (Zhang et al., 2017a;b; Schrumpf et al., 2021; Engemann et al., 2022). There are also works that control the relationship between predictions and targets by converting the regression task into a classification task and training the model with the cross-entropy loss (Rothe et al., 2015; Niu et al., 2016; Shi et al., 2021).\n\n\nHowever, all previous methods focus on imposing constraints on the final predictions in an end-to-end fashion, but do not explicitly consider the representations learned by the model. Their representations tend to be fragmented and fail to capture the continuous relationships underlying regression tasks. For example, Figure 1(a) shows the representation learned by the L1 loss in the task of predicting weather temperature from webcam outdoor images (Chu et al., 2018), where the images are captured by 44 outdoor webcams at different locations. The representation learned by the L1 model does not exhibit the continuous ground-truth temperatures; rather it is grouped by different webcams in a fragmented manner. Such unordered and fragmented representation is sub-optimal for the regression task and can even hamper performance since it contains distracting information (e.g., the capturing webcam).\n\n\nWhile there is a rich literature on representation learning, past methods are focused on classification problems. In particular, contrastive learning (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b) and supervised contrastive learning (SupCon) (Khosla et al., 2020) have been proven highly effective in representation learning. However, as shown in Figure 1(b), which plots the representation learned by SupCon for the visual temperature prediction task mentioned above, such method produces a sub-optimal representation for regression problems because it ignores the continuous order between the samples in a regression task. Besides, there are several recent works (Wang et al., 2022; Dufumier et al., 2021a;b; Schneider et al., 2022) adopting contrastive learning in the context of continuous labels, but they are not doing regression learning tasks.\n\n\nIn this paper, we introduce Supervised Contrastive Regression (SupCR), a new framework for deep regression learning, where we first learn a representation that ensures the distances in the embedding\n\n\n![(a) L1](images/a81b8e39f8bb941eecb45768e99bcd7ca157b504cd225ca2a6c96df5b05fb79a.jpg)\n\n\n![(b) SupCon Figure 1: UMAP (McInnes et al., 2018) visualization of the representation learned by L1, SupCon (Khosla et al., 2020), and the proposed SupCR for the task of predicting the temperature from webcam outdoor images (Chu et al., 2018). The representation of each image is visualized as a dot and the color indicates the ground-truth temperature. SupCR can learn a representation that captures the intrinsic ordered relationships between the samples while L1 and SupCon fail to do so.](images/f0c876486cd367d6f48e8d5c211b57b9dfccaaae76d0620764b37a98db8ebb8a.jpg)\n\n\n![(c) SupCR](images/34cda3a85be98e06ec8959b65694db7d93346d8ed71140f782fc9724dbec4a8c.jpg)\n\n\nspace are ordered according to the target values. We then use this representation to predict the targets. To learn such a regression-aware representation, we contrast samples against each other based on their label/target distances. Our method explicitly leverages the ordered relationships between the samples to optimize the representation for the downstream regression task. As shown in Figure 1(c), SupCR leads to a representation that captures the intrinsic ordered relationships between the samples. Moreover, our framework is orthogonal to existing regression methods as one can use any kind of regression method to map our learned representation to the prediction values.\n\n\nExtensive experiments using five real-world regression datasets that span computer vision, human-computer interaction, and healthcare verify the superior performance of our framework. The results show that SupCR achieves the state-of-the-art performance and consistently improves the performance of previous regression methods by  $8.7\\%$  on average across all datasets, tasks, and input modalities. Furthermore, our experimental results also show that SupCR delivers multiple desirable properties: 1) improved robustness to data corruptions; 2) enhanced resilience to reduced training data; 3) higher performance on transfer learning; and 4) better generalization to unseen targets.\n\n\n# 2 RELATED WORK\n\n\nRegression Learning. Deep learning has achieved a great success in addressing regression tasks (Rothe et al., 2015; Zhang et al., 2017a;b; Schrumpf et al., 2021; Engemann et al., 2022). The most straightforward and widely used way to train a regression model is to have the model predict the target value, and use the distance between the prediction and the ground-truth target as the loss function. Common distance metrics used as the loss function in regression tasks are the L1 loss, the MSE loss and the Huber loss (Huber, 1992).\n\n\nPast work has also proposed variants to the basic methods described above. One branch of prior works (Rothe et al., 2015; Gao et al., 2017; 2018; Pan et al., 2018) divide the regression range into small bins to convert the problem into a classification problem, and use classification loss, e.g., the cross-entropy loss, to train the model. Another branch of prior work (Niu et al., 2016; Fu et al., 2018; Cao et al., 2020; Shi et al., 2021) casts regression as an ordinal classification problem. They design multiple ordered thresholds and use multiple binary classifiers, one for each threshold, to learn whether the target is larger than each threshold.\n\n\nPast work however focuses on forcing the final predictions of the model to be close to the target and does so in an end-to-end manner. In contrast, this paper focuses on ordering the samples in the embedding space in accordance with the label order, and it does so by first learning a representation that contrasts samples against each other, and then learning a predictor to map the representation to the prediction values. Therefore, our method is orthogonal to prior methods for regression learning as those prior methods can be applied to training our predictor and learning the final predictions with the help of our ordered representation.\n\n\nContrastive Learning. Contrastive learning, which aligns positive pairs and repulses negative pairs in the representation space, has demonstrated improved performance for self-supervised learn\n\n\ning (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Chuang et al., 2020). Its supervised version, named supervised contrastive learning (SupCon) (Khosla et al., 2020), which defines samples from the same class as positive pairs and samples from different classes as negative pairs, was also shown to outperform the cross-entropy loss on image classification tasks (Khosla et al., 2020). It is also beneficial for learning in challenging settings such as noisy labels (Li et al., 2022a), long-tailed classification (Kang et al., 2020; Li et al., 2022b) and out-of-domain detection (Zeng et al., 2021).\n\n\nA couple of recent papers have used the concept of \"contrastive\" or contrastive learning in the context of continuous labels. In particular, Yu et al. (2021) learns a model for action quality assessment by regressing the relative scores between two input videos. This work differs from ours in that it does not use the standard contrastive learning framework. Wang et al. (2022) proposes to improve domain adaptation for gaze estimation by adding a contrastive loss term to the L1 loss. They show that their approach is beneficial for adapting a gaze estimation model from one dataset (i.e., one domain) to another, but the approach produces no benefits and even reduces performance for the source dataset. In contrast, our approach improves performance for the source dataset as opposed to being specific to domain adaptation. Besides, Dufumier et al. (2021a;b) use a contrastive loss-weighted by continuous meta-data for classification. Schneider et al. (2022) learns low-dimensional and interpretable embeddings to encode behavioral and neural data using a generalized contrastive loss which samples positives and negatives according to continuous behavior or time labels. However, the continuous meta-data in (Dufumier et al., 2021a;b) and the continuous labels in (Schneider et al., 2022) are not the targets in their tasks and they are not working on regression problems.\n\n\n# 3 METHOD\n\n\nFor a regression task, we aim to train a neural network composed of a feature encoder  $f(\\cdot): X \\to \\mathbb{R}^{d_e}$  and a predictor  $p(\\cdot): \\mathbb{R}^{d_e} \\to \\mathbb{R}^{d_t}$  to predict the target  $\\pmb{y} \\in \\mathbb{R}^{d_t}$  based on the input data  $\\pmb{x} \\in X$ .\n\n\nGiven an input batch of data, similar to contrastive learning methods (Chen et al., 2020a; Khosla et al., 2020), we first apply data augmentation twice to obtain two views of the batch. The two views are fed into the encoder  $f(\\cdot)$  to obtain a  $d_{e}$ -dimensional feature embedding for each augmented input data. Our supervised contrastive regression loss,  $\\mathcal{L}_{\\mathrm{supCR}}$ , is computed on the feature embeddings. To use the learned representation for regression, we freeze the encoder  $f(\\cdot)$  and train the predictor on top of it using a regression loss (e.g., L1 loss).\n\n\n# 3.1 SUPERVISED CONTRASTIVE REGRESSION LOSS\n\n\nWe would like our loss function to ensure distances in the embedding space are ordered according to distances in the label space. But, how can we design a contrastive loss that delivers this property? Below, we first define our loss function, then explain it in the context of positive and negative contrastive samples, and finally provide a theoretical analysis to prove that as our loss approaches its minimum, the learned representation will be ordered according to label distances.\n\n\nFor a positive integer  $I$ , let  $[I]$  denote the set  $\\{1,2,\\dots,I\\}$ . Given a randomly sampled batch of  $N$  input and label pairs  $\\{(\\pmb{x}_n,\\pmb{y}_n)\\}_{n\\in [N]}$ , we apply data augmentation to the batch and obtain a two-view batch  $\\{(\\tilde{\\pmb{x}}_{\\ell},\\tilde{\\pmb{y}}_{\\ell})\\}_{\\ell \\in [2N]}$ , where  $\\tilde{\\pmb{x}}_{2n} = t(\\pmb {x}_n)$ ,  $\\tilde{\\pmb{x}}_{2n - 1} = t^{\\prime}(\\pmb {x}_n)$  ( $t$  and  $t^\\prime$  are independently sampled data augmentations operations) and  $\\tilde{\\pmb{y}}_{2n} = \\tilde{\\pmb{y}}_{2n - 1} = \\pmb {y}_n$ ,  $\\forall n\\in [N]$ . The augmented batch is fed into the encoder  $f(\\cdot)$  to obtain  $\\pmb{v}_l = f(\\tilde{\\pmb{x}}_l)\\in \\mathbb{R}^{d_e},\\forall l\\in [2N]$ . Our supervised contrastive regression loss is then computed over the feature embeddings  $\\{\\pmb {v}_l\\}_{l\\in [2N]}$  as:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\operatorname {S u p C R}} = - \\frac {1}{2 N} \\sum_ {i = 1} ^ {2 N} \\frac {1}{2 N - 1} \\sum_ {j = 1, j \\neq i} ^ {2 N} \\log \\frac {\\exp (\\sin (\\boldsymbol {v} _ {i} , \\boldsymbol {v} _ {j}) / \\tau)}{\\sum_ {k = 1} ^ {2 N} \\mathbb {1} _ {[ k \\neq i , d (\\tilde {\\boldsymbol {y}} _ {i} , \\tilde {\\boldsymbol {y}} _ {k}) \\geq d (\\tilde {\\boldsymbol {y}} _ {i} , \\tilde {\\boldsymbol {y}} _ {j}) ]} \\exp (\\sin (\\boldsymbol {v} _ {i}, \\boldsymbol {v} _ {k}) / \\tau)}, \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\mathrm{sim}(\\cdot ,\\cdot)$  is the function measuring the similarity between two feature embeddings (e.g., negative L2 norm),  $d(\\cdot ,\\cdot)$  is the function measuring the distance between two labels (e.g., L1 distance),  $\\mathbb{1}_{[\\cdot ]}\\in \\{0,1\\}$  is an indicator function evaluating to 1 iff the conditions in the square brackets are satisfied, and  $\\tau$  is the temperature parameter.\n\n\nAt a high level, our loss can be explained in the context of positive/negative pairs, commonly used in contrastive learning. Contrastive learning and SupCon focus on classification. Hence, positive\n\n\n![(a)](images/60018adbd6e1d0020e76f433acba9eaac3e92b6aaa30b594dcc8ed2921111120.jpg)\n\n\n![(b) Figure 2: Illustration of  $\\mathcal{L}_{\\mathrm{supCR}}$ . (a) An example batch of input data and their age labels. (b) Two example positive pairs and the corresponding negative pair(s) when the anchor is the 20-year old man (shown in gray shading). When the 30-year old man creates a positive pair with the anchor, their label distance is 10, so the corresponding negative samples are the 0-year old baby and the 70-year old man, whose label distances to the anchor are 20 and 50 respectively. When the 0-year old baby creates a positive pair with the anchor, their label distance is 20. Only one sample in the batch has a larger label distance to the anchor, namely the 70-year old man, and it acts as a negative sample.](images/1c1edba30507604bf56e08aea699ea87c28a203428b6c892b1676492727d546e.jpg)\n\n\nsamples are those that come from the same class or the same input image, and all other samples are negative samples. In regression problems, there are no classes but continuous labels. Any two samples can be thought of as a positive pair or a negative pair, depending on context. To exploit the inherent continuity underlying the labels, we define positive and negative samples in a relative way, based on their label distance to the anchor. In particular, we rank samples with respect to their label distance to the anchor. We then contrast the anchor to the sample in the batch that is closest in the label space, which will be treated as the positive sample. All other samples are farther from the anchor and should be negative samples. We also contrast the anchor to the second closest sample in the batch. In this case, only samples in the batch that are farther from the positive sample (i.e., samples whose ranks is three or higher) can act as negative samples. We continue this process for higher rank samples (the third closest, fourth closest, etc.), and all anchors in a batch.\n\n\nFigure 2(a) shows an example batch, and Figure 2(b) shows two positive pairs and their corresponding negative pair(s). Specifically, for any anchor sample  $i$ , any other sample  $j$  in the same batch can be used to create a positive pair with corresponding negative samples set to all samples in the batch whose labels differ from  $i$ 's label by more than the label of  $j$ . In other words,  $\\mathcal{L}_{\\mathrm{SupCR}}$  aims to force the feature similarity between  $i$  and  $j$  greater than the feature similarity between  $i$  and any other sample  $k$  in the batch if the label distance between  $i$  and  $k$  is greater than  $i$  and  $j$ . Therefore, optimizing  $\\mathcal{L}_{\\mathrm{SupCR}}$  will make the feature embedding ordered according to the order in the label space. The design of  $\\mathcal{L}_{\\mathrm{SupCR}}$  sufficiently leverages the ordered relationships between samples reflected in the continuous label space of regression problems, which is also the intrinsic difference between regression problems and classification problems.\n\n\n# 3.2 THEORETICAL ANALYSIS\n\n\nIn this section, we analytically show that optimizing  $\\mathcal{L}_{\\mathrm{SupCR}}$  will make the feature embedding ordered according to the order in the label space. To show this, we first derive a lower bound of  $\\mathcal{L}_{\\mathrm{SupCR}}$ , and show  $\\mathcal{L}_{\\mathrm{SupCR}}$  can be arbitrarily close to it. Next, we formalize the concept of  $\\delta$ -ordering which refers to the feature embeddings being ordered according to the order in the label space, and show that as  $\\mathcal{L}_{\\mathrm{SupCR}}$  gets sufficiently close to the lower bound, the feature embeddings will be  $\\delta$ -ordered. All proofs are in Appendix A.\n\n\nNotations. Let  $s_{i,j} \\coloneqq \\mathrm{sim}(\\pmb{v}_i, \\pmb{v}_j) / \\tau, \\forall i, j \\in [2N]$ ,  $d_{i,j} \\coloneqq d(\\tilde{\\pmb{y}}_i, \\tilde{\\pmb{y}}_j)$ ,  $\\forall i, j \\in [2N]$ ,  $D_{i,1} < D_{i,2} < \\dots < D_{i,M_i}$  be the sorted label distances starting from the  $i$ -th sample, i.e., sort  $(\\{d_{i,j} | j \\in [2N] \\setminus \\{i\\}\\}), \\forall i \\in [2N]$ , and  $n_{i,m} \\coloneqq |\\{j | d_{i,j} = D_{i,m}, j \\in [2N] \\setminus \\{i\\}\\}|$  be the number of samples whose distance from the  $i$ -th sample equals  $D_{i,m}, \\forall i \\in [2N], m \\in [M_i]$ .\n\n\nFirst, we show that  $L^{\\star} \\coloneqq \\frac{1}{2N(2N - 1)}\\sum_{i = 1}^{2N}\\sum_{m = 1}^{M_i}n_{i,m}\\log n_{i,m}$  is a lower bound of  $\\mathcal{L}_{\\mathrm{SupCR}}$ .\n\n\nTheorem 1 (Lower bound of  $\\mathcal{L}_{\\mathrm{supCR}}$ ).  $L^{\\star}$  is a lower bound of  $\\mathcal{L}_{\\mathrm{supCR}}$ , i.e.,  $\\mathcal{L}_{\\mathrm{supCR}} > L^{\\star}$ .\n\n\nNext, we show that  $\\mathcal{L}_{\\mathrm{SupCR}}$  can be arbitrarily close to its lower bound  $L^{\\star}$ .\n\n\nTheorem 2 (Lower bound tightness). For any  $\\epsilon >0$ , there exists a set of feature embeddings such that  $\\mathcal{L}_{\\mathrm{supCR}} < L^{\\star} + \\epsilon$ .\n\n\nThen, we define a property called  $\\delta$ -ordered for the feature embeddings  $\\{\\pmb{v}_l\\}_{l \\in [2N]}$ .\n\n\nDefinition 1 ( $\\delta$ -ordered feature embeddings). For any  $0 < \\delta < 1$ , the feature embeddings  $\\{\\pmb{v}_l\\}_{l \\in [2N]}$  are  $\\delta$ -ordered if  $\\forall i \\in [2N], j, k \\in [2N] \\setminus \\{i\\}$ ,\n\n\n$$\n\n$$\n\\left\\{ \\begin{array}{l} s _ {i, j} > s _ {i, k} + \\frac {1}{\\delta} i f d _ {i, j} <   d _ {i, k} \\\\ | s _ {i, j} - s _ {i, k} | <   \\delta i f d _ {i, j} = d _ {i, k} \\\\ s _ {i, j} <   s _ {i, k} - \\frac {1}{\\delta} i f d _ {i, j} > d _ {i, k} \\end{array} \\right..\n$$\n\n$$\n\n\nIt says that a  $\\delta$ -ordered set of feature embeddings satisfies the following properties: First, for all  $j$  and  $k$  such that  $d_{i,j} = d_{i,k}$ , the gap between  $s_{i,j}$  and  $s_{i,k}$  should be smaller than  $\\delta$ ; Second, for all  $j$  and  $k$  such that  $d_{i,j} < d_{i,k}$ ,  $s_{i,j}$  should be larger than  $s_{i,k}$  by at least  $\\frac{1}{\\delta}$ . Notice that  $\\frac{1}{\\delta} > \\delta$ , which means the feature similarity gap between a sample pair with different label distance to the anchor is always larger than the feature similarity gap between a sample pair with equal label distance to the anchor.\n\n\nFinally, we show that for any  $0 < \\delta < 1$ , when  $\\mathcal{L}_{\\mathrm{supCR}}$  is close enough to its lower bound  $L^{\\star}$ , the feature embeddings will be  $\\delta$ -ordered.\n\n\nTheorem 3 (Main theorem). For any  $0 < \\delta < 1$ , there exist  $\\epsilon > 0$ , such that if  $\\mathcal{L}_{\\mathrm{sup}CR} < L^{\\star} + \\epsilon$ , then the feature embeddings are  $\\delta$ -ordered.\n\n\n# 4 EXPERIMENTS\n\n\nIn this section, we evaluate our proposed method empirically. We first benchmark our method and compare it with the state-of-the-art regression baselines. Then, we evaluate the desirable properties of our learned representations, including the robustness to data corruptions, the resilience to reduced training data, the performance on transfer learning, and the generalization ability to unseen targets. Finally, we analyze and evaluate the design variants for our method. Refer to Appendix E for additional experiments and analysis.\n\n\nDatasets. We benchmark our method on five regression datasets for common real-world tasks in computer vision, human-computer interaction, and healthcare. Ethics statements of the datasets are included in Appendix B.\n\n- 1) AgeDB (Moschoglou et al., 2017) is used for predicting age from face images. It contains 16,488 in-the-wild images of celebrities and the corresponding age labels. The age range is between 0 and 101. It is split into a 12,208-image training set, a 2140-image validation set and a 2140-image test set.\n\n- 2) TUAB (Obeid & Picone, 2016; Engemann et al., 2022) is used for brain-age estimation from EEG resting-state signals. The dataset comes from EEG exams at the Temple University Hospital in Philadelphia. Following Engemann et al. (2022), we use only the non-pathological subjects, so that we may consider their chronological age as their brain-age label. The dataset includes 1,385 21-channel EEG signals sampled at  $200\\mathrm{Hz}$  from individuals whose age ranges from 0 to 95. It is split into a 1,246-subject training set and a 139-subject test set.\n\n- 3) MPIIFaceGaze (Zhang et al., 2017a;b) is used for gaze direction estimation from face images. It contains 213,659 face images collected from 15 participants during their natural everyday laptop use. We subsample and split it into a 33,000-image training set, a 6,000-image validation set and a 6000-image test set with no overlapping participants. The gaze direction is described as a 2-dimensional vector with the pitch angle in the first dimension and the yaw angle in the second dimension. The range of the pitch angle is  $-40^{\\circ}$  to  $10^{\\circ}$  and the range of the yaw angle is  $-45^{\\circ}$  to  $45^{\\circ}$ .\n\n- 4) SkyFinder (Mihail et al., 2016; Chu et al., 2018) is used for temperature prediction from outdoor webcam images. It contains 35,417 images captured by 44 cameras around 11am on each day under a wide range of weather and illumination conditions. The temperature range is  $-20^{\\circ}\\mathrm{C}$  to  $49^{\\circ}\\mathrm{C}$ . It is split into a 28,373-image training set, a 3,522-image validation set and a 3,522-image test set.\n\n\nTable 1: Evaluation results on AgeDB.\n\n\n<table><tr><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td></tr><tr><td>L1</td><td>6.63</td><td>0.828</td><td>SUPCR(L1)</td><td>6.14 (+0.49)</td><td>0.850 (+0.022)</td></tr><tr><td>MSE</td><td>6.57</td><td>0.828</td><td>SUPCR(MSE)</td><td>6.19 (+0.38)</td><td>0.849 (+0.021)</td></tr><tr><td>HUBER</td><td>6.54</td><td>0.828</td><td>SUPCR(HUBER)</td><td>6.15 (+0.39)</td><td>0.850 (+0.022)</td></tr><tr><td>DEX (Rothe et al., 2015)</td><td>7.29</td><td>0.787</td><td>SUPCR(DEX)</td><td>6.43 (+0.86)</td><td>0.836 (+0.049)</td></tr><tr><td>DLDL-v2 (Gao et al., 2018)</td><td>6.60</td><td>0.827</td><td>SUPCR(DLDL-v2)</td><td>6.32 (+0.28)</td><td>0.844 (+0.017)</td></tr><tr><td>OR (Niu et al., 2016)</td><td>6.40</td><td>0.830</td><td>SUPCR(OR)</td><td>6.34 (+0.06)</td><td>0.843 (+0.013)</td></tr><tr><td>CORN (Shi et al., 2021)</td><td>6.72</td><td>0.811</td><td>SUPCR(CORN)</td><td>6.44 (+0.28)</td><td>0.838 (+0.027)</td></tr></table>\n\n\nTable 2: Evaluation results on TUAB.\n\n\n<table><tr><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td></tr><tr><td>L1</td><td>7.96</td><td>0.599</td><td>SUPCR(L1)</td><td>6.97 (+0.99)</td><td>0.697 (+0.098)</td></tr><tr><td>MSE</td><td>8.06</td><td>0.585</td><td>SUPCR(MSE)</td><td>7.05 (+1.01)</td><td>0.692 (+0.107)</td></tr><tr><td>HUBER</td><td>7.59</td><td>0.637</td><td>SUPCR(HUBER)</td><td>6.99 (+0.60)</td><td>0.696 (+0.059)</td></tr><tr><td>DEX (Rothe et al., 2015)</td><td>8.01</td><td>0.537</td><td>SUPCR(DEX)</td><td>7.23 (+0.78)</td><td>0.646 (+0.109)</td></tr><tr><td>DLDL-v2 (Gao et al., 2018)</td><td>7.91</td><td>0.560</td><td>SUPCR(DLDL-v2)</td><td>6.91 (+1.00)</td><td>0.697 (+0.137)</td></tr><tr><td>OR (Niu et al., 2016)</td><td>7.36</td><td>0.646</td><td>SUPCR(OR)</td><td>7.01 (+0.35)</td><td>0.688 (+0.042)</td></tr><tr><td>CORN (Shi et al., 2021)</td><td>8.11</td><td>0.597</td><td>SUPCR(CORN)</td><td>7.22 (+0.89)</td><td>0.663 (+0.066)</td></tr></table>\n\n\nTable 3: Evaluation results on MPIIFaceGaze.\n\n\n<table><tr><td>Method</td><td>Angular\u2193</td><td>R2\u2191</td><td>Method</td><td>Angular\u2193</td><td>R2\u2191</td></tr><tr><td>L1</td><td>5.97</td><td>0.744</td><td>SUPCR(L1)</td><td>5.27 (+0.70)</td><td>0.815 (+0.071)</td></tr><tr><td>MSE</td><td>6.02</td><td>0.747</td><td>SUPCR(MSE)</td><td>5.35 (+0.67)</td><td>0.802 (+0.055)</td></tr><tr><td>HUBER</td><td>6.34</td><td>0.709</td><td>SUPCR(HUBER)</td><td>5.15 (+1.19)</td><td>0.830 (+0.121)</td></tr><tr><td>DEX (Rothe et al., 2015)</td><td>5.72</td><td>0.776</td><td>SUPCR(DEX)</td><td>5.14 (+0.58)</td><td>0.805 (+0.029)</td></tr><tr><td>DLDL-v2 (Gao et al., 2018)</td><td>5.47</td><td>0.799</td><td>SUPCR(DLDL-v2)</td><td>5.16 (+0.31)</td><td>0.802 (+0.003)</td></tr><tr><td>OR (Niu et al., 2016)</td><td>5.86</td><td>0.770</td><td>SUPCR(OR)</td><td>5.13 (+0.73)</td><td>0.825 (+0.055)</td></tr><tr><td>CORN (Shi et al., 2021)</td><td>5.88</td><td>0.762</td><td>SUPCR(CORN)</td><td>5.18 (+0.70)</td><td>0.820 (+0.058)</td></tr></table>\n\n\nTable 4: Evaluation results on SkyFinder.\n\n\n<table><tr><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td></tr><tr><td>L1</td><td>2.95</td><td>0.860</td><td>SUPCR(L1)</td><td>2.86 (+0.09)</td><td>0.869 (+0.009)</td></tr><tr><td>MSE</td><td>3.08</td><td>0.851</td><td>SUPCR(MSE)</td><td>2.86 (+0.22)</td><td>0.869 (+0.018)</td></tr><tr><td>HUBER</td><td>2.92</td><td>0.860</td><td>SUPCR(HUBER)</td><td>2.86 (+0.06)</td><td>0.869 (+0.009)</td></tr><tr><td>DEX (Rothe et al., 2015)</td><td>3.58</td><td>0.778</td><td>SUPCR(DEX)</td><td>2.88 (+0.70)</td><td>0.865 (+0.087)</td></tr><tr><td>DLDL-v2 (Gao et al., 2018)</td><td>2.99</td><td>0.856</td><td>SUPCR(DLDL-v2)</td><td>2.85 (+0.14)</td><td>0.869 (+0.013)</td></tr><tr><td>OR (Niu et al., 2016)</td><td>2.92</td><td>0.861</td><td>SUPCR(OR)</td><td>2.86 (+0.06)</td><td>0.867 (+0.006)</td></tr><tr><td>CORN (Shi et al., 2021)</td><td>3.24</td><td>0.819</td><td>SUPCR(CORN)</td><td>2.89 (+0.35)</td><td>0.862 (+0.043)</td></tr></table>\n\n\n5) IMDB-WIKI (Rothe et al., 2015) is a large dataset for predicting age from face images, which contains 523,051 celebrities images and the corresponding age labels. The age range is between 0 and 186 (some images are unlabeled). We use this dataset to test our method's resilience to reduced training data, performance on transfer learning, and ability to generalize to unseen targets. We subsample the dataset to create a variable size training set, and keep the validation set and test set unchanged with 11,022 images in each.\n\n\nMetrics. We report two kinds of metrics: the prediction error and coefficient of determination  $(\\mathbb{R}^2)$ . Prediction errors have practical meaning and are convenient for interpretation, while  $\\mathbb{R}^2$  quantifies how much the model outperforms a dummy regressor that always predicts the mean value of the training labels. For age, brain-age, and temperature, the mean absolute error (MAE) is reported as the prediction error. For gaze direction, the angular error is reported as the prediction error.\n\n\nBaselines. We implemented seven typical regression methods as baselines. L1, MSE and HUBER have the model directly predict the target value and train the model with an error-based loss function, where L1 uses the mean absolute error, MSE uses the mean squared error and HUBER uses an MSE\n\n\nterm when the error is below a threshold and an L1 term otherwise. DEX (Rothe et al., 2015) and DLDL-v2 (Gao et al., 2018) divide the regression range of each label dimension into several bins and learn the probability distribution over the bins. DEX (Rothe et al., 2015) optimizes a cross-entropy loss between the predicted distribution and the one-hot ground-truth labels, while DLDL-v2 (Gao et al., 2018) jointly optimizes a KL loss between the predicted distribution and a normal distribution centered at the ground-truth value, as well as an L1 loss between the expectation of the predicted distribution and the ground-truth value. During inference, they output the expectation of the predicted distribution for each label dimension. OR (Niu et al., 2016) and CORN (Shi et al., 2021) design multiple ordered threshold for each label dimension, and learn a binary classifier for each threshold. OR (Niu et al., 2016) optimizes a binary cross-entropy loss for each binary classifier to learn whether the target value is larger than each threshold, while CORN (Shi et al., 2021) learns whether the target value is larger than each threshold conditioning on it is larger than the previous threshold. During inference, they aggregate all binary classification results to produce the final results.\n\n\nExperiment Settings. We use ResNet-18 (He et al., 2016) as the backbone model for AgeDB, IMDB-WIKI, MPIIFaceGaze and SkyFinder datasets. For TUAB, we use a 24-layer 1D ResNet (He et al., 2016) as the backbone model to process the EEG signals. We use the linear regressor as the predictor. We use the SGD optimizer and cosine learning rate annealing (Loshchilov & Hutter, 2016) for training. The batch size is set to 256. We pick the best learning rates and weight decays by grid search for both our method and the baselines. We train all the baselines and the encoder of our method for 400 epochs, and the linear regressor of our method for 100 epochs. The same data augmentations are adopted for all baselines and our method: random crop and resize (with random flip), color distortions for AgeDB, IMDB-WIKI, SkyFinder; random crop and resize (without random flip), color distortions for MPIIFaceGaze; random crop for TUAB. Appendix C shows augmentation examples for each dataset. Negative L2 norm, i.e.,  $\\mathrm{sim}(\\pmb {v}_i,\\pmb {v}_j) = -\\| \\pmb {v}_i - \\pmb {v}_j\\| _2$  is used as the feature similarity measure in  $\\mathcal{L}_{\\mathrm{supCR}}$ . L1 distance is used as the label distance measure in  $\\mathcal{L}_{\\mathrm{supCR}}$  for AgeDB, IMDB-WIKI, SkyFinder and TUAB, while angular distance is used as the label distance measure for MPIIFaceGaze. The temperature  $\\tau$  is set to 2.0. Refer to Appendix D for more details.\n\n\n# 4.1 MAIN RESULTS\n\n\nAs explained earlier, our model learns a regression-suitable representation that can be used by any of the baselines. Thus, in our comparison, we first train the encoder with the proposed  $\\mathcal{L}_{\\mathrm{SupCR}}$ , and then freeze the encoder and train a predictor on top of it using each of the baseline methods. We then compare the original baseline without our representation, to the same baseline with our representation. For example, L1 means training the encoder and predictor end-to-end with L1 loss and  $\\mathrm{SUPCR(L1)}$  means training the encoder with  $\\mathcal{L}_{\\mathrm{SupCR}}$  and then training the predictor with L1 loss.\n\n\nTable 1, Table 2, Table 3 and Table 4 show the evaluation results on AgeDB, TUAB, MPIIfaceGaze and SkyFinder, respectively. Green numbers highlight the performance gains brought by using our representation and the best numbers are shown in bold. As all tables indicate, SupCR consistently achieves the best performance on both metrics across all datasets. Moreover, incorporating SupCR to learn the representation consistently reduces the prediction error of all baselines by  $5.8\\%$ ,  $10.1\\%$ ,  $11.7\\%$  and  $7.0\\%$  on average on the AgeDB, TUAB, MPIIfaceGaze and SkyFinder, respectively.\n\n\n# 4.2 ROBUSTNESS TO DATA CORRUPTIONS\n\n\nIt is well known that deep neural networks are highly vulnerable to out of distribution data and different forms of corruptions (Hendrycks & Dietterich, 2019), such as noise, blur and color distortions. Using the corruption generation process in ImageNet-C benchmark (Hendrycks & Dietterich, 2019), we corrupt the AgeDB test set with 19 diverse corruption types under different severity levels. In Figure 3, we compare SupCR(L1) with L1 by testing the model (trained on clean AgeDB training set) on corrupted test set and reporting the average mean absolute error over all types of corruptions for each severity level. The re\n\n\n![Figure 3: SupCR is more robust to data corruptions. Figure shows MAE $\\downarrow$  as a function of corruption severity.](images/61489af599e6b6fc1fca95443058ff0943da2fb314257c56f4a3cd6d471f221f.jpg)\n\n\nsults show that the representation learned by SupCR is more robust to unforeseen data corruptions, and SupCR suffers from lesser performance degradation as corruption severity increases.\n\n\n# 4.3 RESILIENCE TO REDUCED TRAINING DATA\n\n\nMuch of the success of modern deep learning is due to the availability of massive training datasets. However, in many real-world settings, collecting a large training set is often infeasible due to the time and cost of labeling. Thus, it is desirable to improve model resilience to reduced training data. In Figure 4, we subsample IMDB-WIKI to produce varying size training sets, and compare SUPCR(L1) with L1. The results show that SUPCR is more resilient to reduced training data, and exhibits lesser performance degradation as the number of training samples decreases.\n\n\n# 4.4 TRANSFER LEARNING\n\n\nWe evaluate the transfer learning performance by first pre-training the feature encoder on a large dataset and then using either linear probing (fixed encoder) or fine-tuning to learn a predictor on a small dataset. We investigate two scenarios: transferring from AgeDB which contains  $\\sim 12\\mathrm{k}$  samples to a subsampled IMDB-WIKI of 2k samples and transferring from another subsampled IMDB-WIKI of 32k samples to AgeDB. As shown in Table 5, SupCR(L1) outperforms L1 in both the linear probing and fine-tuning settings in both two scenarios.\n\n\n![Figure 4: SupCR is more resilient to reduced training data. Figure shows  $\\mathrm{MAE}^{\\downarrow}$  as a function of the number of training samples.](images/0967fa25160e206c670752d75d80203f2a9544d77daa649406ef4ebb9e3ed843.jpg)\n\n\nTable 5: Transfer learning results.\n\n\n<table><tr><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td></tr><tr><td colspan=\"6\">AgeDB \u2192 IMDB-WIKI (subsampled, 2k):</td></tr><tr><td colspan=\"3\">Linear Probing:</td><td colspan=\"3\">Fine-tuning:</td></tr><tr><td>L1</td><td>12.25</td><td>0.496</td><td>L1</td><td>11.57</td><td>0.528</td></tr><tr><td>SUPCR(L1)</td><td>11.12</td><td>0.556</td><td>SUPCR(L1)</td><td>11.09</td><td>0.546</td></tr><tr><td></td><td>(+1.13)</td><td>(+0.060)</td><td></td><td>(+0.48)</td><td>(+0.018)</td></tr><tr><td colspan=\"6\">IMDB-WIKI (subsampled, 32k) \u2192 AgeDB:</td></tr><tr><td colspan=\"3\">Linear Probing:</td><td colspan=\"3\">Fine-tuning:</td></tr><tr><td>L1</td><td>7.36</td><td>0.801</td><td>L1</td><td>6.36</td><td>0.848</td></tr><tr><td>SUPCR(L1)</td><td>7.06</td><td>0.812</td><td>SUPCR(L1)</td><td>6.13</td><td>0.850</td></tr><tr><td></td><td>(+0.30)</td><td>(+0.011)</td><td></td><td>(+0.23)</td><td>(+0.002)</td></tr></table>\n\n\n# 4.5 GENERALIZATION TO UNSEEN TARGETS\n\n\nIn real-world regression tasks, it is common that some targets are unseen during training. As in Yang et al. (2021), we curate two subsets of IMDB-WIKI that contain unseen targets. The test set is a uniform distribution across the whole target range. Table 6 shows the label distributions of the two training set, where pink shading indicates regions of unseen targets, and the blue shading represents the distribution of seen targets. The first training set consists of 3,781 training samples with a bi-modal Gaussian dis\n\n\nTable 6: Generalization performance to unseen targets on two curated subsets of IMDB-WIKI. MAE $^{\\downarrow}$  is used as the metric.\n\n\n<table><tr><td>Label Distribution</td><td>Method</td><td>All</td><td>Seen</td><td>Unseen</td></tr><tr><td></td><td>L1</td><td>12.53</td><td>10.82</td><td>18.40</td></tr><tr><td></td><td>SUPCR(L1)</td><td>11.69</td><td>10.46</td><td>15.92</td></tr><tr><td></td><td></td><td>(+0.84)</td><td>(+0.36)</td><td>(+2.48)</td></tr><tr><td></td><td>L1</td><td>11.94</td><td>10.43</td><td>14.98</td></tr><tr><td></td><td>SUPCR(L1)</td><td>10.88</td><td>9.78</td><td>13.08</td></tr><tr><td></td><td></td><td>(+1.06)</td><td>(+0.64)</td><td>(+1.90)</td></tr></table>\n\n\ndistribution over the target space, and the second training set consists of 3,662 training samples with a tri-modal Gaussian distribution over the target space. We report the prediction error on the seen and unseen targets separately. The results show that SupCR (L1) outperforms L1 by a larger margin on the unseen targets without sacrificing the performance on the seen targets.\n\n\n# 4.6 DESIGN VARIANTS\n\n\nSimilarity Measure and Projection Head. We explore potential variants of our method to pick the best design. Table 7 compares the performance of our method ( $\\mathcal{L}_{\\mathrm{SupCR}}$  with similarity measure using the negative L2 norm) with alternative designs that use a different similarity measure and potentially support a projection head. The results are on the AgeDB dataset and using SupCR(L1).\n\n\nSpecifically, the table shows the performance of using different feature similarity measures  $\\sin (\\cdot ,\\cdot)$ . We see that among cosine similarity, i.e.,  $\\begin{array}{rlr}{\\mathrm{sim}(\\pmb {v}_i,\\pmb {v}_j)} & = & {\\frac{\\pmb{v}_i\\cdot\\pmb{v}_j}{\\|\\pmb{v}_i\\|_2\\|\\pmb{v}_j\\|_2}} \\end{array}$  negative L1 norm, i.e.,\n\n\nTable 7: Comparison to potential variants of  ${\\mathcal{L}}_{\\text{SupCR }}$  and their performance on AgeDB.\n\n\n<table><tr><td>Loss Term</td><td>Similarity Measure</td><td>Projection Head</td><td>MAE\u2193</td><td>R2\u2191</td></tr><tr><td>LSimCLR</td><td>cosine</td><td>\u2713</td><td>9.59</td><td>0.666</td></tr><tr><td>LSimCLR</td><td>cosine</td><td>X</td><td>10.67</td><td>0.600</td></tr><tr><td>LSupCon</td><td>cosine</td><td>\u2713</td><td>8.13</td><td>0.753</td></tr><tr><td>LSupCon</td><td>cosine</td><td>X</td><td>7.89</td><td>0.760</td></tr><tr><td>LSupCR</td><td>cosine</td><td>\u2713</td><td>7.34</td><td>0.793</td></tr><tr><td>LSupCR</td><td>cosine</td><td>X</td><td>6.51</td><td>0.836</td></tr><tr><td>LSupCR</td><td>negative L1 norm</td><td>X</td><td>6.25</td><td>0.842</td></tr><tr><td>LSupCR</td><td>negative L2 norm</td><td>X</td><td>6.14</td><td>0.850</td></tr></table>\n\n\n$\\mathrm{sim}(\\pmb {v}_i,\\pmb {v}_j) = -\\| \\pmb {v}_i - \\pmb {v}_j\\| _1$  and negative L2 norm, i.e.,  $\\mathrm{sim}(\\pmb {v}_i,\\pmb {v}_j) = -\\| \\pmb {v}_i - \\pmb {v}_j\\| _2$  , negative L2 norm delivers the best performance.\n\n\nThe table also shows the performance with and without a projection head. To train the encoder network  $f(\\cdot):X\\to \\mathbb{R}^{d_e}$ , one can involve a projection head  $h(\\cdot):\\mathbb{R}^{d_e}\\rightarrow \\mathbb{R}^{d_p}$  during training to calculate the loss on  $\\{h(f(\\tilde{x}_i))\\}_{i = 1\\in [2N]}$ , and then discard the projection head to use  $f(x_{i})$  for inference. One can also directly calculate the loss on  $\\{f(\\tilde{x}_i)\\}_{i\\in [2N]}$  and then use  $f(x_{i})$  for inference. SIMCLR (Chen et al., 2020a) and SUPCON (Khosla et al., 2020) both use the former approach.\n\n\nFor  $\\mathcal{L}_{\\mathrm{SimCLR}}$ , the results show that adding the projection head can benefit the regression performance. This is because  $\\mathcal{L}_{\\mathrm{SimCLR}}$  aims to extract features that are invariant to data augmentations and can remove information that may be useful for the downstream regression task. For  $\\mathcal{L}_{\\mathrm{SupCon}}$  and  $\\mathcal{L}_{\\mathrm{SupCR}}$ , however, it is better to train the encoder without the projection head, since both  $\\mathcal{L}_{\\mathrm{SupCon}}$  and  $\\mathcal{L}_{\\mathrm{SupCR}}$  leverage the label information to extract features that directly target on the downstream task.\n\n\nMoreover, we see that using  $\\mathcal{L}_{\\mathrm{SupCon}}$  or  $\\mathcal{L}_{\\mathrm{supCR}}$  as the loss function delivers better performance than using  $\\mathcal{L}_{\\mathrm{SimCLR}}$ , verifying that it is helpful to utilize label information during encoder training. We also see that  $\\mathcal{L}_{\\mathrm{supCR}}$  outperforms  $\\mathcal{L}_{\\mathrm{supCon}}$  by a large margin, highlighting the superiority of  $\\mathcal{L}_{\\mathrm{supCR}}$  which explicitly leverages the ordered relationships between samples for regression problems.\n\n\nTraining Scheme. There are usually three schemes to train the feature encoder: (1) Linear probing: first trains the feature encoder using the representation learning loss, then freezes the encoder and trains a linear regressor on top of it using a regression loss. (2) Fine-tuning: first trains\n\n\nTable 8: Performance of alternative training schemes for the feature encoder on AgeDB.\n\n\n<table><tr><td>Scheme</td><td>Linear Probing</td><td>Fine-tuning</td><td>Regularization</td><td>L1</td></tr><tr><td>MAE\u2193</td><td>6.14</td><td>6.36</td><td>6.42</td><td>6.63</td></tr><tr><td>R2\u2191</td><td>0.850</td><td>0.844</td><td>0.833</td><td>0.828</td></tr></table>\n\n\nthe feature encoder using the representation learning loss, then fine-tunes the whole model using a regression loss. (3) Regularization: trains the whole model while jointly optimizing the representation learning loss and the regression loss.\n\n\nTable 8 shows the performance on AgeDB for the three schemes using  $\\mathcal{L}_{\\mathrm{SupCR}}$  as the representation learning loss and L1 as the regression loss. We see that all of the three schemes can improve performance over using the regression loss L1 alone to train a model. Further, unlike classification problems where fine-tuning often delivers the best performance, freezing the feature encoder performs the best. This is because in the case of regression, back-propagating the L1 loss to the representation can destroy the order in the embedding space learned by  $\\mathcal{L}_{\\mathrm{SupCR}}$ , which leads to poorer performance.\n\n\n# 5 CONCLUSION\n\n\nIn this paper, we propose Supervised Contrastive Regression (SupCR), a framework that learns a regression-aware representation by contrasting samples against each other according to their target distance. The proposed framework is orthogonal to existing regression models, and can be used in combination with such models to improve performance. It achieves the state-of-the-art performance and consistently improves prior regression baselines across different datasets, tasks, and input modalities. It also improves the robustness to data corruptions, resilience to reduced training data, performance on transfer learning, and generalization to unseen targets.\n\n\n# REFERENCES\n\n- Ahmed A Abdelrahman, Thorsten Hempel, Aly Khalifa, and Ayoub Al-Hamadi. L2cs-net: Fine-grained gaze estimation in unconstrained environments. arXiv preprint arXiv:2203.03339, 2022.\n\n- Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka. Rank consistent ordinal regression for neural networks with application to age estimation. Pattern Recognition Letters, 140:325-331, 2020.\n\n- Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9650-9660, 2021.\n\n- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. PMLR, 2020a.\n\n- Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.\n\n- Wei-Ta Chu, Kai-Chia Ho, and Ali Borji. Visual weather temperature prediction. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 234-241. IEEE, 2018.\n\n- Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. Advances in neural information processing systems, 33:8765-8775, 2020.\n\n- Benoit Dufumier, Pietro Gori, Julie Victor, Antoine Grigis, and Edouard Duchesnay. Conditional alignment and uniformity for contrastive learning with continuous proxy labels. arXiv preprint arXiv:2111.05643, 2021a.\n\n- Benoit Dufumier, Pietro Gori, Julie Victor, Antoine Grigis, Michele Wessa, Paolo Brambilla, Pauline Favre, Mircea Polosan, Colm Mcdonald, Camille Marie Piguet, et al. Contrastive learning with continuous proxy meta-data for 3d mri classification. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 58-68. Springer, 2021b.\n\n- Denis A Engemann, Apolline Mellot, Richard H\u00f6chenberger, Hubert Banville, David Sabbagh, Lukas Gemein, Tonio Ball, and Alexandre Gramfort. A reusable benchmark of brain-age prediction from m/eeg resting-state signals. NeuroImage, 262:119521, 2022.\n\n- Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2002-2011, 2018.\n\n- Bin-Bin Gao, Chao Xing, Chen-Wei Xie, Jianxin Wu, and Xin Geng. Deep label distribution learning with label ambiguity. IEEE Transactions on Image Processing, 26(6):2825-2838, 2017.\n\n- Bin-Bin Gao, Hong-Yu Zhou, Jianxin Wu, and Xin Geng. Age estimation using expectation of label distribution learning. In *IJCAI*, pp. 712-718, 2018.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.\n\n- Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019.\n\n- Peter J Huber. Robust estimation of a location parameter. In *Breakthroughs in statistics*, pp. 492-518. Springer, 1992.\n\n- Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi Feng. Exploring balanced feature spaces for representation learning. In International Conference on Learning Representations, 2020.\n\n- Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661-18673, 2020.\n\n- Shikun Li, Xiaobo Xia, Shiming Ge, and Tongliang Liu. Selective-supervised contrastive learning with noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 316-325, 2022a.\n\n- Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe Yang, Rogerio S Feris, Piotr Indyk, and Dina Katabi. Targeted supervised contrastive learning for long-tailed recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6918-6928, 2022b.\n\n- Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.\n\n- Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.\n\n- Radu P Mihail, Scott Workman, Zach Bessinger, and Nathan Jacobs. Sky segmentation in the wild: An empirical study. In 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1-6. IEEE, 2016.\n\n- Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop, volume 2, pp. 5, 2017.\n\n- Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. Ordinal regression with multiple output cnn for age estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4920-4928, 2016.\n\n- Iyad Obeid and Joseph Picone. The temple university hospital eeg data corpus. Frontiers in neuroscience, 10:196, 2016.\n\n- Hongyu Pan, Hu Han, Shiguang Shan, and Xilin Chen. Mean-variance loss for deep age estimation from a face. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5285-5294, 2018.\n\n- Rasmus Rothe, Radu Timofte, and Luc Van Gool. Dex: Deep expectation of apparent age from a single image. In Proceedings of the IEEE international conference on computer vision workshops, pp. 10-15, 2015.\n\n- Steffen Schneider, Jin Hwa Lee, and Mackenzie Weygandt Mathis. Learnable latent embeddings for joint behavioral and neural analysis. arXiv preprint arXiv:2204.00673, 2022.\n\n- Fabian Schrumpf, Patrick Frenzel, Christoph Aust, Georg Osterhoff, and Mirco Fuchs. Assessment of non-invasive blood pressure prediction from ppg and rppg signals using deep learning. Sensors, 21(18):6022, 2021.\n\n- Xintong Shi, Wenzhi Cao, and Sebastian Raschka. Deep neural networks for rank-consistent ordinal regression based on conditional probabilities, 2021.\n\n- Yaoming Wang, Yangzhou Jiang, Jin Li, Bingbing Ni, Wenrui Dai, Chenglin Li, Hongkai Xiong, and Teng Li. Contrastive regression for domain adaptation on gaze estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19376-19385, 2022.\n\n- Yuzhe Yang, Kaiwen Zha, Yingcong Chen, Hao Wang, and Dina Katabi. Delving into deep imbalanced regression. In International Conference on Machine Learning, pp. 11842-11851. PMLR, 2021.\n\n- Xumin Yu, Yongming Rao, Wenliang Zhao, Jiwen Lu, and Jie Zhou. Group-aware contrastive regression for action quality assessment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7919-7928, 2021.\n\n- Zhiyuan Zeng, Keqing He, Yuanmeng Yan, Zijun Liu, Yanan Wu, Hong Xu, Huixing Jiang, and Weiran Xu. Modeling discriminative representations for out-of-domain detection with supervised contrastive learning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 870-878, 2021.\n\n- Xuong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. Mpiigaze: Real-world dataset and deep appearance-based gaze estimation. IEEE transactions on pattern analysis and machine intelligence, 41(1):162-175, 2017a.\n\n- Xuong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. It's written all over your face: Full-face appearance-based gaze estimation. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on, pp. 2299-2308. IEEE, 2017b.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638574.3458197", "msecs": "345.0", "relativeCreated": "171176.31721496582", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Limitations of measure-first protocols in quantum machine learning\n\n# Abstract\nIn recent times, there have been major developments in two distinct yet connected domains of quantum information. On the one hand, substantial progress has been made in so-called randomized measurement protocols. Here, a number of properties of unknown quantum states can be deduced from surprisingly few measurement outcomes, using schemes such as classical shadows. On the other hand, significant progress has been made in quantum machine learning. For example, exponential advantages have been proven when the data consists of quantum states and quantum algorithms can coherently measure multiple copies of input states. In this work, we aim to understand the implications and limitations of combining randomized measurement protocols with quantum machine learning, although the implications are broader. Specifically, we investigate quantum machine learning algorithms that, when dealing with quantum data, can either process it entirely using quantum methods or measure the input data through a fixed measurement scheme and utilize the resulting classical information. We prove limitations for quantum machine learning algorithms that use fixed measurement schemes on the input quantum states. Our results have several implications. From the perspective of randomized measurement procedures, we show limitations of measure-first protocols in the average case, improving on the state-of-the-art which only focuses on worst-case scenarios. Additionally, previous lower bounds were only known for physically unrealizable states. We improve upon this by employing quantum pseudorandom functions to prove that a learning separation also exists when dealing with physically realizable states, which may be encountered in experiments. From a machine learning perspective, our results are crucial for defining a physically meaningful task that shows fully quantum machine learning processing is not only more efficient but also necessary for solving certain problems. The tasks at hand are also realistic, as the algorithms and proven separations hold when working with efficiently preparable states and remain robust in the presence of measurement and preparation errors.\n\n\n# 1 INTRODUCTION\n\n\nA central question in quantum machine learning revolves around understanding the various types of advantages one can achieve by exploiting quantum effects. Some of the most interesting scenarios arise when the dataset itself comprises quantum states, which can then be processed fully coherently, or through elaborate measurement strategies. In this context, exponential advantages have been identified when coherent measurements of multiple copies of a given quantum state are allowed Chen et al. (2022b); Huang et al. (2022; 2020b). In a parallel related line, there have been significant breakthroughs in extracting useful classical information from quantum states using the versatile toolkit of randomized measurements Elben et al. (2023). This toolkit includes the groundbreaking concept of classical shadows Huang et al. (2020a; 2022), which can extract an efficient classical description of quantum states that allows one to compute various physical properties. These two distinct research lines cast doubt on the advantages of quantum machine learning protocols that process quantum data directly compared to those that use a fixed measurement procedure, which can also allow for a coherent manipulation of quantum states before the fixed measurement, in order to extract valuable classical data<sup>1</sup>. In particular we address the question: is it possible that\n\n\na \"measure-first\" protocol can be universally used as a substitute for any \"fully-quantum\" protocol in the general context of machine learning?\n\n\nWe formally define what we mean by a \"measure-first\" or a \"fully-quantum\" protocol in Section 2 (see Definition 3 and Definition 5), and we provide an overview in Figure 1. Importantly, we focus on physically relevant scenarios where the quantum states in the dataset are efficiently preparable, rather than arbitrary unphysical states that do not allow a polynomial description in terms of quantum gates.\n\n\nOur main conceptual contribution is to resolve the above question negatively by presenting a concrete machine learning scenario that clearly exhibits the limitations of any measure-first protocol. In a machine learning setup, we construct a task that requires reproducing an undisclosed measurement from the data. This task shows an exponential difference in the number of data needed between fully-quantum protocols and measure-first protocols.\n\n\nOutside of machine learning, certain limitations of measure-first protocols are implied by other works in various tasks such as distributed sampling Montanaro (2019) and relational problems Aaronson et al. (2023). However, these results primarily address separations in worst-case scenarios, which contrasts with the typical focus in machine learning settings where average-case performance is usually sufficient. In this paper, we advance these findings by identifying the limitations of measure-first protocols that need only achieve average-case correctness for a set of efficiently preparable quantum states, i.e. physical states. On a technical level, establishing limitations for measure-first protocols in a general quantum machine learning setting necessitates novel techniques beyond those utilized in previous studies. In particular, limitations for the task of predicting properties of quantum states were also recently derived for worst-case settings Grier et al. (2022) exploiting lower bounds for classical one-way communication complexity. However, this approach inherently limits the focus to states that are not efficiently preparable. This limitation arises because, if states were efficiently preparable, there would be a concise classical description of their preparation procedure, enabling the transmission of valid classical messages to the recipient. Our main technical contribution lies in exploiting that, from the more general machine learning standpoint, it makes sense to constrain the computational power of the learning protocol. This allows us to additionally impose the requirement that states must be efficiently preparable and still achieve a separation by utilizing a novel construction that combines results on one-way communication complexity and pseudorandom quantum states Brakerski & Shmueli (2019). The task we consider is also experimentally robust, in the sense that we allow for errors in the preparation of the input quantum states and on the measurement outcomes which label them.\n\n\n# 1.1 HIGH-LEVEL OVERVIEW OF LEARNING SETTING AND MAIN RESULT\n\n\nThe machine learning problem concerns learning an unknown measurement acting on a set of input quantum states. Specifically, the data that the learning protocol gets consists of pairs of copies of  $n$ -qubit quantum states  $\\rho$  drawn from some distribution  $\\mathcal{D}$  together with a corresponding label  $z \\in \\{0,1\\}^{2n + 1}$ . The first  $n$  bits of  $z$  encode a  $2^n$ -outcome POVM measurement  $\\Lambda_{x} = \\{E_{j}^{x} | j \\in \\{0,1\\}^{n}\\}$  from some set  $\\Lambda = \\{\\Lambda_{x} | x \\in \\{0,1\\}^{n}\\}$ . The remaining  $n + 1$  bits are determined by the outcome of  $\\Lambda_{x}$  on the quantum state  $\\rho$ . The goal of the machine learning protocol is to \"learn\" how to reproduce the measurement  $\\Lambda_{x}$ . More precisely, the trained learning protocol has to receive as input an unseen quantum state  $\\rho'$  and output a sample  $z'$  in agreement with the probability distribution  $\\pi_{x}(\\rho)$ , where  $\\pi_{x}(\\rho)$  denotes the distribution of the measurement outcomes of  $\\Lambda_{x}$ .\n\n\nThis paper explores whether solving learning problems such as the above requires a quantum computer capable of adaptive measurements on the training data, or if a fixed measurement strategy that produces classical representations of the quantum states is sufficient. Specifically, we consider so-called \"measure-first protocols\" that are forced to measure the input states and use the obtained classical description to train the machine learning model to produce samples from the target distribution. Importantly, in a measure-first protocol the measurements are not allowed to depend on the training data, but are unconstrained otherwise. In particular, the measure-first protocol can depend on the set of measurements that is to be reproduced from the data, but not on the specific individual measurement hidden within some given training data. On the other hand, we consider so-called \"fully-quantum protocols\" that can coherently process the quantum states and adjust the measurements based on the training data. Our main result is the existence of a learning problem where the\n\n\n![Figure 1: An overview of the quantum machine learning protocols explored in our paper. In both protocols, the quantum algorithm  $A$  is tasked with processing training data  $T_{x}$  to output a classical description of a function  $h$  that correctly produces samples  $z^{*}$  for new input states  $\\rho^{*}$  (i.e.,  $h$  should implement the quantum measurement). The difference between the two protocols is that, in the measure-first protocol, both the input quantum states and those in the training set  $T_{x}$  are subjected to a randomized measurement strategy  $M$ . Consequently, these quantum states  $\\rho^{*}$  are transformed into a classical representation  $\\hat{\\rho}^{*}$ . Importantly, this strategy is allowed to depend only on the concept class  $\\mathcal{C}$  (which captures the general learning problem), not on the specific target concept  $\\pi_{x} \\in \\mathcal{C}$ . In other words, the same randomized measurement strategy  $M$  is used to preprocess the quantum data for all concepts  $\\pi_{x} \\in \\mathcal{C}$ .](images/6da32fa5cf8e2628e4fada6af45c1312f421dae9d434c2b3fa522a43e07dbb40.jpg)\n\n\nquantum data is efficiently generatable on a quantum computer for which a \"measure-first protocol\" requires an exponential amount of data to be able to reproduce measurement outcomes on new input states, whereas a fully-quantum protocol only requires a polynomial amount of data. Additionally, we require that the protocols must be efficient in the sense that they run in time polynomial in  $n$ . Using the notion of learning we presented above, we now give an informal description of the main result of this paper.\n\n\nTheorem 1. (Informal) There exist a concept class  $\\mathcal{C}$ , where each concept is defined by a measurement and a distribution  $\\mathcal{D}$  over efficiently preparable quantum states such that no \"measure-first\" protocol can learn to reproduce  $\\mathcal{C}$  correctly on average with a polynomial amount of training data. On the other hand, there exists a \"fully-quantum\" protocol which can learn  $\\mathcal{C}$  efficiently with respect to both sample and time complexity.\n\n\nThe specific learning problem which exhibits a learning separation is carefully outlined in Section 3 and the precise proofs of Theorem 1 can be found in the Appendix.\n\n\n# 1.2 RELATED WORK\n\n\nIn this section we discuss related works and highlight their relationships to our learning setting. Firstly, in Huang et al. (2020a), the authors introduced a randomized measurement technique tailored to extract a classical description of a quantum state  $\\rho$ . This description enables the computation of the expectation values of any set of observables  $\\{O_k\\}_{k=1}^M$  - provided they have a low \"shadow norm\", such as when the observables are local - up to a precision of  $\\epsilon$ . Notably, they showed that a number of copies of  $\\rho$ , scaling logarithmically with the number of observables  $M$  and inverse-polynomially in the precision  $\\epsilon$ , suffices for this task. As we want to learn the distribution associated of a  $2^n$ -outcome POVM with an  $\\epsilon$  error in total variation, directly applying these techniques to estimate each of the  $2^n$  outcomes would require an exponential precision for each. In their upper bound estimates, this results in the requirement of an exponential number of samples.\n\n\nThe concept of shadow tomography, introduced in Aaronson (2018), revolves around the problem of computing the expectation values of any set of  $M$  two-outcome measurements on an  $n$ -qubit state  $\\rho$  up to precision  $\\epsilon$ . It has been shown that this can be done using a number of copies of  $\\rho$  that scale\n\n\npolylogarithmically in  $M$ , linearly in  $n$ , and inverse-polynomially in  $\\epsilon$  Aaronson (2018). In contrast to the methods in Huang et al. (2020a), the approach in Aaronson (2018) requires coherent measurements on multiple copies of  $\\rho$ . Additionally, as demonstrated in Huang et al. (2022); Chen et al. (2022a) for specific tasks, the capacity to coherently measure multiple copies of quantum states provides an exponential advantage in sample complexity over sequential measurements. Considering our framework, where measurements can act coherently on multiple copies of each input state, one might question whether such strategies enable a measure-first protocol to solve our learning task. However, it is important to note that coherent measurements do not improve the scaling with respect to the precision  $\\epsilon$ , leaving room for the possibility of a separation since the learning task studied in this paper would require at least exponential precision to be solved.\n\n\nIn Gong & Aaronson (2023), the authors extended the concept of shadow tomography to the scenario of learning a  $K$ -outcome POVMs (for  $K \\geq 2$ ) selected from a set of  $M$  unknown quantum measurements. In contrast to the binary outcome case, the goal now is to approximate an unknown distribution up to precision  $\\epsilon$  in total variation distance, rather than focusing on expectation values. Their procedure requires a number of copies of the quantum state scaling linearly with  $K$  and  $n$ , polylogarithmically with  $M$ , and inverse-polynomially with the precision  $\\epsilon$ . Moreover, they establish the optimality of this scaling with respect to the dependence on the number of outcomes  $K$ . This result prompts the question of whether our separation between measure-first and fully-quantum protocols can be directly inferred from it. However, in establishing their lower bound, it is important to note that no assumptions were made regarding the complexity of the unknown quantum state. The crux of our study lies in demonstrating that measure-first protocols fall short of reproducing the unknown measurement, even on quantum states that are efficiently preparable. Moreover, it is important to highlight that while their shadow tomography procedures can be employed to construct a measure-first protocol by \"shadowfying\" input states to approximate the expected values of each measurement outcome, this is not strictly necessary for our task. Specifically, understanding the probability of each outcome allows for the creation of an \"evaluator\" that can compute the correct probability for every outcome. However, to resolve our learning problem, a \"generator\" (i.e., an algorithm generating samples with the correct probabilities) already suffices, and it does not necessarily require computing the output probabilities Sweke et al. (2021).\n\n\nNext, in Grier et al. (2022) the authors explore lower bounds on the number of copies of a quantum state required for solving what they term the \"classical shadow\" task. This task involves estimating the expected values of observables using a measure-first approach. While they establish lower bounds on the number of copies of a quantum state required for this task, shedding light on the inherent limitations of measure-first protocols, it is crucial to note two significant bottlenecks of their approach when viewed from our more general machine learning perspective. Firstly, their analysis only focuses on worst-case scenarios, whereas in our machine learning scenarios we are concerned with average-case performance. Secondly, their investigation centers on protocols required to operate effectively even for unphysical quantum states that are not efficiently preparable.\n\n\nThe authors of Cheng et al. (2015) provide upper bounds for the dual problem of shadow tomography, or more specifically the problem of learning a measurement. In particular, they studied the task of learning an unknown two-outcome POVM denoted  $\\{E, I - E\\}$ , from data of the form  $\\{(\\rho_i, \\mathrm{Tr}(E\\rho_i))\\}_{i=1}^m$ . They showed that to approximate the unknown  $E$  up to a precision of  $\\epsilon$  on new  $n$ -qubit input states,  $\\mathcal{O}(2^n / \\epsilon^2)$  training samples are sufficient. Nonetheless, the number of required samples scales exponentially with the number of qubits.\n\n\nFinally, in Jerbi et al. (2023) the authors study quantum process learning, where the task is to learn a unitary  $U$ , from data of the form  $\\{| \\psi_i \\rangle, U | \\psi_i \\rangle$ . In particular, they study the limitations of what they call incoherent learning, where the learner is constrained to first measure multiple copies of the data  $U | \\psi_i \\rangle$ . While they therefore also study the problem of extracting classical information from quantum data and utilizing it in the learning process, the setting in their work differs from ours. Namely, the quantum states in our scenario are labeled by a sample obtained from the unknown measurement process, whereas in Jerbi et al. (2023) the labels are the input quantum states when evolved under some unknown target unitary.\n\n\n# 2 BACKGROUND\n\n\nIn this section, we introduce the notations and definitions needed to frame our main result, which involves a learning problem that demonstrate the separation between any measure-first protocol and a fully quantum protocol.\n\n\nThe learning problem we study is the learning of a measurement. In particular, it involves generating samples from a distribution induced by measuring an (unknown) POVM measurement  $\\Lambda_{x}$  on  $n$ -qubit quantum states. The (unknown) target measurement  $\\Lambda_{x}$  is drawn from a set of POVMs  $\\{\\Lambda_{x} \\mid x \\in \\{0,1\\}^{n}\\}$ , and each measurement  $\\Lambda_{x}$  is a computational basis measurement preceded by an  $n$ -qubit unitary  $U_{x}$ , i.e.,\n\n\n$$\n\n$$\n\\Lambda_ {x} = \\left\\{E _ {j} ^ {x} | j = 1, \\dots , 2 ^ {n} \\right\\}, \\text {w h e r e} E _ {j} ^ {x} = U _ {x} | j \\rangle \\langle j | U _ {x} ^ {\\dagger}.\n$$\n\n$$\n\n\nDuring training the learner is given a set of examples  $T_{x}$ , where each example consists of a polynomial number of copies of a phase state  $|\\psi_f\\rangle$  together with a sample from the associated POVM-induced distribution. This is a special case of labeled quantum data, which was introduced in Aimeur et al. (2006), where we are additionally allowed to have access to polynomially many copies of the input quantum state. We formalize our learning problem by generalizing the standard PAC learning framework Valiant (1984). In our generalization, a concept corresponds to a quantum randomized function, i.e., a function that on each quantum input state outputs a sample from a random variable (which in our case corresponds to the outcomes of a POVM on the input quantum state). Before we define the concept class studied throughout this paper, we first setup some auxiliary definitions.\n\n\nDefinition 1 (Auxiliary definitions/notation).\n\n- - Let  $N = 2^n$ , or equivalently  $n = \\log_2 N$ .\n\n- - We identify a function  $f: \\{0,1\\}^n \\to \\{0,1\\}$  with its truth table  $f \\in \\{0,1\\}^N$ , and we denote its corresponding phase state with\n\n\n$$\n\n$$\n\\left| \\psi_ {f} \\right\\rangle = \\frac {1}{\\sqrt {N}} \\sum_ {i = 1} ^ {N} (- 1) ^ {f _ {i}} | i \\rangle . \\tag {1}\n$$\n\n$$\n\n- - Let  $\\mathcal{S}_{\\mathrm{phase}} = \\{\\left|\\psi_f\\right\\rangle^{\\otimes \\ell} \\mid f \\in \\{0,1\\}^N\\}$  denote the set of  $\\ell = \\mathrm{poly}(n)$  copies of  $n$ -qubit phase states.\n\n- - We write  $x \\sim \\pi$  to denote that  $x$  was drawn according to a distribution  $\\pi$ .\n\n- - We write  $\\mathcal{U}(\\mathcal{X})$  for the uniform distribution over a set  $\\mathcal{X}$ .\n\n- - We write  $\\Delta(\\mathcal{X})$  for the set of all distributions over a set  $\\mathcal{X}$ .\n\n- - For  $p, q \\in \\mathbb{R}_{\\geq 0}^{2^n}$  probability distributions over  $\\{0,1\\}^n$  we define\n\n\n$$\n\n$$\n\\left| \\left| p - q \\right| \\right| _ {T V} = \\frac {1}{2} \\sum_ {x \\in \\{0, 1 \\} ^ {n}} \\left| p _ {x} - q _ {x} \\right|\n$$\n\n$$\n\n\nto be the total variation distance.\n\n\nDefinition 2 (Concept class). We define our concept class as  $\\mathcal{C} = \\{\\pi_x\\mid x\\in \\{0,1\\} ^n\\}$ , where\n\n\n$$\n\n$$\n\\pi_ {x}: \\mathcal {S} _ {\\text {p h a s e}} \\rightarrow \\Delta (\\{0, 1 \\} ^ {2 n + 1}) \\tag {2}\n$$\n\n$$\n\n\n$$\n\n$$\n\\left| \\psi_ {f} \\right\\rangle^ {\\otimes \\ell} \\mapsto \\pi_ {x} (f)\n$$\n\n$$\n\n\nwhere  $\\pi_x(f)$  is a distribution over samples  $(x,y,b) \\in \\{0,1\\}^{2n + 1}$ , where  $(y,b) \\in \\{0,1\\}^{n + 1}$  are variables sampled from a problem-specific distribution characterizing the specific learning task.\n\n\nIn particular,  $\\pi_x$  is a randomized function which takes as input a polynomial number of copies of a phase state and outputs a sample  $z$  consisting of  $x \\in \\{0,1\\}^n$  together with some  $(y,b) \\in \\{0,1\\}^{n+1}$  sampled from some problem-specific probability distribution. As we will see in the next section,\n\n\na learning separation for this kind of learning problem is achieved by considering  $(y, b)$  sampled uniformly from the set of variables which satisfy the Hidden Matching relation defined by the input functions  $f$  and the bitstring  $x \\in \\{0, 1\\}^n$ .\n\n\nA learner then is given several evaluations of the randomized function  $\\pi_{x}$  in the form of training data and its objective is to implement a randomized function  $\\widetilde{\\pi}_{x}$  that closely approximates  $\\pi_{x}$  on most input states.\n\n\nIn this paper, we compare two categories of machine learning systems that can tackle problems of this type. First, we introduce what we call a \"fully-quantum protocol\".\n\n\nDefinition 3 (Fully-quantum protocol). A fully-quantum protocol for the concept class  $\\mathcal{C}$  in Definition 7 is a polynomial-time quantum algorithm  $A$  that takes as input training data of the form\n\n\n$$\n\n$$\nT _ {x} = \\left\\{\\left(\\left| \\psi_ {f ^ {(i)}} \\right\\rangle^ {\\otimes \\ell}, (x, y, b)\\right) \\mid (x, y, b) \\sim \\pi_ {x} (f ^ {(i)}), \\right.\n$$\n\n$$\n\n\n$$\n\n$$\n\\left. \\text {a n d} f ^ {(i)} \\sim \\mathcal {U} \\left(\\{0, 1 \\} ^ {N}\\right) \\right\\} _ {i = 1} ^ {\\operatorname {p o l y} (n)}, \\tag {3}\n$$\n\n$$\n\n\nand outputs a classical description of a polynomial-time quantum algorithm that on input  $|\\psi_f\\rangle^{\\otimes \\ell} \\in S_{\\mathrm{phase}}$  generates a sample from a distribution  $\\widetilde{\\pi}_x(f) \\in \\Delta(\\{0,1\\}^{2n+1})$ .\n\n\nWe emphasize that for a \"fully quantum\" protocol, the learning algorithm must produce a classical description of the quantum algorithm generating samples from  $\\widetilde{\\pi}_x$ . Consequently, we do not store any quantum states from the training data in quantum memory, which would be more general but not studied in this paper. Ultimately, the goal of the protocol is to implement a randomized function  $\\widetilde{\\pi}_x$  that closely approximates the actual data-generating randomized function  $\\pi_x$  for most of the input quantum states.\n\n\nDefinition 4  $((\\epsilon, \\delta, p_{\\mathrm{suc}})$ -fully-quantum learnable). We say that  $\\mathcal{C}$  is  $(\\epsilon, \\delta, p_{\\mathrm{suc}})$ -fully-quantum learnable if there exists a fully-quantum protocol  $A$  such that for every  $\\pi_x \\in \\mathcal{C}$ , with probability at least  $p_{\\mathrm{suc}}$  we have\n\n\n$$\n\n$$\n\\Pr_ {f \\sim \\mathcal {U} (\\{0, 1 \\} ^ {N})} \\left(\\left| \\left| \\widetilde {\\pi} _ {x} (f) - \\pi_ {x} (f) \\right| \\right| _ {T V} \\leq \\epsilon\\right) \\geq 1 - \\delta , \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\widetilde{\\pi}_x(f) \\in \\Delta(\\{0,1\\}^{2n+1})$  denotes the distribution that the polynomial-time quantum algorithm obtained from the learning algorithm  $A$  generates samples from on input  $|\\psi_f\\rangle^{\\otimes \\ell} \\in S_{\\mathrm{phase}}$ .\n\n\nNext, we introduce a \"measure-first protocol\" which consists of two components: (i) a randomized measurement strategy  $M$ , and (ii) a learning algorithm  $A$ . The main difference between a measure-first protocol and a fully-quantum protocol is that the former involves a randomized measurement procedure that first measures the quantum states before putting it into a learning algorithm. Importantly, the measure-first protocol is allowed to perform arbitrary coherent measurements on all input quantum states (i.e., the polynomially-many copies of the phase states). The only constraint is that the measurement strategy cannot depend on the specific target concept of the learning problem, although it is allowed to depend on the concept class (i.e., the set of all possible target concepts). In short, a randomized measurement strategy is a polynomial-time algorithms that maps a polynomial number of copies of a phase state  $|\\psi_f\\rangle$  to some classical description  $\\widehat{\\psi}_f\\in \\{0,1\\} ^m$  for some  $m = \\mathrm{poly}(n)$ . These classical descriptions  $\\widehat{\\psi}_f$  are then used as input for the learning algorithm, that is tasked with implementing a randomized function close to  $\\pi_{x}$ .\n\n\nDefinition 5 (Measure-first protocol). A measure-first protocol is a tuple  $(M, A)$  where\n\n- -  $M$  is a measurement strategy that in time  $\\mathcal{O}(\\mathrm{poly}(n))$  maps  $|\\psi_f\\rangle^{\\otimes \\ell}\\in S_{\\mathrm{phase}}^2$  to some  $\\widehat{\\psi}_f\\in \\{0,1\\} ^m$ , where  $m = \\mathrm{poly}(n)$ .\n\n- -  $A$  is a polynomial-time quantum algorithm that takes input of the form\n\n\n$$\n\n$$\nT _ {x} ^ {M} = \\left\\{\\left(\\widehat {\\psi} _ {f ^ {(i)}}, (x, y, b)\\right) \\mid (x, y, b) \\sim \\pi_ {x} (f ^ {(i)}) \\right.\n$$\n\n$$\n\n\n$$\n\n$$\n\\left. \\text {a n d} f ^ {(i)} \\sim \\mathcal {U} (\\{0, 1 \\} ^ {N}) \\right\\} _ {i = 1} ^ {\\text {p o l y} (n)}, \\tag {5}\n$$\n\n$$\n\n\nand outputs a description of a polynomial-time quantum algorithm that on input  $\\widehat{\\psi}_f = M(|\\psi_f\\rangle^{\\otimes \\ell})$  generates a sample from a distribution  $\\widetilde{\\pi}_x(f)\\in \\Delta (\\{0,1\\}^{2n + 1})$\n\n\nNote that the distinction between measure-first and fully-quantum protocols lies in Eq. 5, where the data is measured to give  $\\hat{\\psi_f}$  instead of remaining quantum states  $|\\psi_f\\rangle$ . Nonetheless, the measurements employed by the measurement strategy  $M$  are completely arbitrary and entirely unrestricted. We emphasize, however, that the sole constraint on the measurement strategy  $M$  is that it must not rely on the specific concepts it will be applied to later, although it can depend on the concept class it is intended for. Recall that the objective of the protocol is to implement a randomized function  $\\widetilde{\\pi}_x$  that closely approximates the actual data-generating randomized function  $\\pi_x$  on most inputs.\n\n\nDefinition 6  $((\\epsilon, \\delta, p_{\\mathrm{suc}})$ -measure-first learnable). We say that  $\\mathcal{C}$  is  $(\\epsilon, \\delta, p_{\\mathrm{suc}})$ -measure-first learnable if there exists a measure-first protocol  $(M, A)$  such that for every  $\\pi_x \\in \\mathcal{C}$ , with probability at least  $p_{\\mathrm{suc}}$  we have\n\n\n$$\n\n$$\n\\Pr_ {f \\sim_ {U} \\{0, 1 \\} ^ {N}} \\left(\\left| \\left| \\widetilde {\\pi} _ {x} (f) - \\pi_ {x} (f) \\right| \\right| _ {T V} \\leq \\epsilon\\right) \\geq 1 - \\delta , \\tag {6}\n$$\n\n$$\n\n\nwhere  $\\widetilde{\\pi}_x(f) \\in \\Delta(\\{0,1\\}^{2n+1})$  denotes the distribution that the polynomial-time quantum algorithm obtained from the learning algorithm  $A$  generates samples from on input  $\\widehat{\\psi}_f = M(|\\psi_f\\rangle^{\\otimes \\ell})$ .\n\n\nAlthough we defined the two protocols in an idealized case, we will show in Appendix A.1 that our separation results are robust. In particular, they still hold even in the experimental setting where the input states  $|\\psi_f\\rangle$  are affected by preparation errors and they are just close in trace distance to the ones defined in Eq.(1). Furthermore, we also allow for measurement errors on the labels  $(y,b)$\n\n\n# 3 MAIN RESULT\n\n\nIn this section, we present the key findings of our paper. In Section 3.1, we define the specific learning problem considered and show how the fully-quantum machine learning model can efficiently solve it. In Section 3.2, we present our first main result showing that no measure-first quantum machine learning model can solve our learning problem efficiently. Finally, in Section 3.3, we show that this separation between the models still holds if the quantum states in the data are efficiently preparable.\n\n\n# 3.1 THE LEARNING PROBLEM\n\n\nIn this section, we concretely define the learning problem for which we prove a learning separation. Then, we initiate the proof by showing that it is learnable using a fully-quantum protocol.\n\n\nThe specific concept class we developed to prove our main result is a particular instance of the one defined in Def. 2, with the condition that the variables  $(y,b)$  are sampled from the set  $R_{x}(f)$  defined as follows:\n\n\nDefinition 7 (Concept class).  $\\mathcal{C} = \\{\\pi_x\\mid x\\in \\{0,1\\} ^n\\}$ , where\n\n\n$$\n\n$$\n\\pi_ {x}: \\mathcal {S} _ {\\text {p h a s e}} \\rightarrow \\Delta (\\{0, 1 \\} ^ {2 n + 1}) \\tag {7}\n$$\n\n$$\n\n\n$$\n\n$$\n\\left| \\psi_ {f} \\right\\rangle^ {\\otimes \\ell} \\mapsto \\pi_ {x} (f)\n$$\n\n$$\n\n\nwhere  $\\pi_x(f)$  is a distribution over samples  $(x,y,b)$ , where  $(y,b) \\sim \\mathcal{U}(R_f(x))$  and\n\n\n$$\n\n$$\n\\begin{array}{c} R _ {f} (x) = \\{(y, b) \\mid y \\in \\{0, 1 \\} ^ {n}, b \\in \\{0, 1 \\}, \\\\ f (y) \\oplus f (y \\oplus x) = b \\}. \\end{array} \\tag {8}\n$$\n\n$$\n\n\nImportantly, in Aaronson et al. (2023) the authors showed that for each  $x \\in \\{0,1\\}^n$  there exist a POVM measurement  $\\Lambda_x$  which when applied to a phase state  $|\\psi_f\\rangle$  outputs a pair exactly satisfying the relation  $R_{f}(x)$ . With regards to our learning problem, the task of the learning protocols is to learn this measurement.\n\n\nWe now describe how the concept class in Definition 7 is fully-quantum learnable.\n\n\nProposition 1. The concept class in Definition 7 is  $(0,0,1)$ -fully-quantum learnable.\n\n\nThe proof of Proposition 1 can be found in Appendix A, and we provide a high-level overview of the fully-quantum protocol here. Firstly, the fully-quantum protocol reads out  $x$  from one of the samples generated by  $\\pi_x$  in the training data. Next, a quantum circuit denoted as  $U_x$  is constructed following the procedure outlined in Aaronson et al. (2023). Measuring  $U_x | \\psi_f \\rangle$  in the computational basis results in a sample from the distribution  $\\mathcal{U}(R_f(x))$ . Crucially, it is worth noting that these quantum circuits  $U_x$  are of size  $\\mathcal{O}(\\mathrm{poly}(n))$  and can be constructed in time  $\\mathcal{O}(\\mathrm{poly}(n))$ . We remark here that the concept class in Definition 7 remains fully quantum learnable even when errors are present in the training data, see Appendix A.1.\n\n\nIt might seem that little genuine learning occurs when  $x$  can be readily read out from a single example in  $T_{x}$ . However, we can introduce various levels of learning by providing only partial information about  $x$  within the examples. This partial information should allow the recovery of  $x$  from a polynomial number of examples. Several examples illustrating this are discussed in more detail in Appendix A.\n\n\n# 3.2 LIMITATIONS OF MEASURE-FIRST PROTOCOLS WITH GENERAL QUANTUM DATA\n\n\nIn the last section, we discussed how the concept class in Definition 7 is fully-quantum learnable. Conversely, in this section we discuss the first part of our main result which states that this concept class is not measure-first learnable.\n\n\nTheorem 2. The concept class in Definition 7 is not  $(\\epsilon, \\delta, p_{\\mathrm{succ}})$ -measure-first learnable for  $(1 - \\epsilon) \\cdot (1 - \\delta) > 7/8$  and any  $p_{\\mathrm{succ}} > 0$ .\n\n\nThe full proof of Theorem 2 is provided in Appendix B, we present a concise overview of the proof here. At its core, the proof hinges on the notion that the existence of a measure-first protocol for the concept class described in Definition 7 implies the existence of an efficient classical one-way communication protocol for the Hidden Matching (HM) problem Bar-Yossef et al. (2004). Notably, in Bar-Yossef et al. (2004), it has been shown that the HM problem cannot be solved with a communication cost of  $\\mathcal{O}(\\mathrm{poly}(n))$  bits, even on a  $7/8$  fraction of possible inputs. In essence, if the concept class in Definition 7 was learnable, one of the two parties can employ the measurement protocol to encode their input for the HM problem, transmit it to the other party, who can then utilize the learning algorithm  $A$  to successfully solve the HM problem. Intuitively, the reason behind why measure-first learning fails is that due to Bar-Yossef et al. (2004) it is not possible to compress a phase state  $|\\psi_f\\rangle$  into a polynomially-sized classical representation  $\\widehat{\\psi}_f$  that contains enough information to allow one to generate samples from the distributions  $\\pi_x(f)$  for all possible  $x$ . Importantly, within the machine learning context, it is crucial to also consider the possibility of protocols making errors on a fraction of inputs and the requirement to gather the necessary data. To overcome this limitation, we use Yao's principle to show that the existence of a protocol which succeeds in the machine learning context would still violate the classical hardness of the HM problem. In particular, by applying Yao's principle, we can ensure the existence of a deterministic algorithm that solves the HM problem on a sufficiently large fraction of inputs, provided that there exists a machine learning measure-first protocol capable of solving the learning task defined in Definition 6.\n\n\nSince in our machine learning setting we are concerned with physical states appearing in the realworld, in the next section we show that our findings also apply to settings where states are efficiently preparable, which we achieve by using pseudorandom states Brakerski & Shmueli (2019) (see Section 3.3). Finally, it is crucial to note that even when the data in Eq.(5) is derived from a polynomial number  $\\ell$  of copies of each input state  $|\\psi_f\\rangle$ , measure-first protocols remain incapable of solving the task. This holds true even when considering inefficient (i.e., superpolynomial-time) algorithms or an exponential number of training points, as the communication complexity bounds still apply regardless of the time resources utilized by the algorithms (since the communication protocol assumes both parties have unbounded resources).\n\n\n# 3.3 LEARNING SEPARATION WITH EFFICIENTLY PREPARABLE QUANTUM DATA\n\n\nFrom a pragmatic perspective, a crucial limitation of the learning problem outlined in the previous section is that preparing a general phase state is intractable (i.e., not realized by polynomial-time processes). In particular, this raises the question of whether separations could persist for states that are prepared by (natural or artificial) polynomial-time processes. To address this limitation, we show\n\n\nthat the concept class in Definition 7 remains not measure-first learnable, even when we constrain the input of the random functions  $\\pi_{x}$  to phase states of so-called pseudorandom functions. Notably, phase states corresponding to appropriately chosen pseudorandom functions can be efficiently prepared. Because of this additional consideration, our separations are notably more general than those in previous works Aaronson et al. (2023); Bar-Yossef et al. (2004). Our definition of pseudorandom functions is as follows.\n\n\nDefinition 8 (Quantum-secure pseudorandom function (QPRF) Brakerski & Shmueli (2019)). Let  $\\mathcal{K} = \\{\\mathcal{K}_n\\}_{n\\geq 1}$  be an efficiently samplable key distribution, and let  $\\mathrm{PRF} = \\{\\mathrm{PRF}_n\\}_{n\\geq 1}$ ,  $\\mathrm{PRF}_n:\\mathcal{K}_n\\times \\{0,1\\} ^n\\to \\{0,1\\}$  be an efficiently computable function. We say PRF is a quantum-secure pseudorandom function if for every efficient non-uniform quantum algorithm  $A$  that can make quantum queries there exists a negligible function  $\\mathrm{negl}(.)$  such that for every  $n\\geq 1$ :\n\n\n$$\n\n$$\n\\left| \\Pr_ {k \\sim \\mathcal {U} \\left(\\mathcal {K} _ {n}\\right)} \\left[ A ^ {\\underset {\\sim \\infty} {\\operatorname {P R F}} _ {n} (k)} () = 1 \\right] - \\Pr_ {f \\sim \\mathcal {U} \\left(\\{0, 1 \\} ^ {n}\\right)} \\left[ A ^ {f} () = 1 \\right] \\right| \\leq \\operatorname {n e g l} (n) \\tag {9}\n$$\n\n$$\n\n\nWhere the notation  $A^{f}()$  stands for the non-uniform quantum algorithm which can make queries to the function  $f$  and can take any quantum state as input. We remark that if every function  $\\mathrm{PRF}_n$  admits a classical circuit of size  $s(n)$  and depth  $d(n)$ , then one can prepare the corresponding phase states using a quantum circuit of size  $\\mathcal{O}(s(n))$  and depth  $d(n) + 1$  Brakerski & Shmueli (2019). Moreover, the existence of such  $\\mathrm{PRF}_n$  is implied by the existence of quantum secure one-way functions Zhandry (2012).\n\n\n# 3.3.1 FULLY-QUANTUM LEARNABILITY WITH PSEUDORANDOM PHASE STATES\n\n\nNote that when we constrain the inputs of  $\\pi_x$  to phase states of pseudorandom functions, we essentially modify the distribution over input states in Eq. 4 and Eq. 6. This new distribution now only has support on phase states that are efficiently preparable. While Proposition 1 examines general quantum phase states as input states (which are not typically efficiently preparable), we note that the fully-quantum learnability directly extends the setting where we limit ourselves to efficiently preparable phase states as well. We summarize this observation in the following proposition (whose proof is the same as that of Proposition 1).\n\n\nProposition 2. Let  $S_{\\mathrm{pr}} = \\{|\\psi_{f^{(k)}}\\rangle^{\\otimes \\ell} \\mid f^{(k)}(.) = \\mathsf{PRF}_n(k,.)$ ,  $k \\in \\mathcal{K} \\}$ , where PRF is a quantum-secure pseudorandom function with keys  $\\mathcal{K}$ . The concept class in Definition 7 is  $(0,0,1)$ -fully-quantum learnable when the distribution over input states is uniform over  $S_{\\mathrm{pr}}$ .\n\n\n# 3.3.2 LIMITATIONS OF MEASURE-FIRST PROTOCOLS WITH PSEUDORANDOM PHASE STATES\n\n\nIn the last section, we discussed how the concept class in Definition 7 remains fully-quantum learnable when restricted to phase states of pseudorandom functions. Conversely, in this section we show that this concept class also remains not measure-first learnable when restricted to phase states of pseudorandom functions.\n\n\nTheorem 3. Let  $S_{\\mathrm{pr}} = \\{\\left|\\psi_{f^{(k)}}\\right\\rangle^{\\otimes \\ell} \\mid f^{(k)}(.) = \\mathsf{PRF}_n(k,.)$ ,  $k \\in \\mathcal{K}\\}$ , where PRF is a quantum-secure pseudorandom function with keys  $\\mathcal{K}$ . The concept class in Definition 7 is not  $(\\epsilon, \\delta, p_{\\mathrm{suc}})$ -measure-first learnable for  $(1 - \\epsilon) \\cdot (1 - \\delta) \\cdot p_{\\mathrm{suc}} > c$  for any constant  $c > 7/8$  when the distribution over input states is uniform over  $S_{\\mathrm{pr}}$ .\n\n\nThe main results of this section directly follows from combining Theorem 3 and Proposition 2.\n\n\nCorollary 1 (informal). If there exist quantum-secure pseudorandom functions, then there exists a quantum supervised learning problem with efficiently generatable quantum data, which cannot be learned by any measure-first protocol according to Def. 6 while there exist a fully-quantum protocol which satisfies the learning condition of Def. 4\n\n\nThe proof of Theorem 3 is provided in Appendix C, and we first present a concise overview of the proof here. The main idea behind the proof is to illustrate that if the concepts are measure-first learnable when restricted to pseudorandom phase states, then the corresponding measure-first learning protocol can be harnessed to create a non-uniform quantum algorithm that is able to distinguish between truly random functions and pseudorandom functions. More precisely, this \"distinguisher\" algorithm employs the measure-first learning protocol and evaluates its performance\n\n\nwhen applied to the phase state corresponding to the function it has been given oracular access to. In the proof of Theorem 2, we established an upper bound on the generalization performance of any measure-first protocol for truly random phase states. If, however, the measure-first protocol performs well on pseudorandom phase states, then the outcomes of the \"distinguisher\" algorithm would differ significantly based on whether it is given oracular access to a truly random or a pseudorandom function, thereby contradicting the pseudorandomness assumption. In other words, if the measure-first protocols were effective on pseudorandom states, there would be a clear difference in the performance of the distinguisher: poor accuracy when dealing with truly random functions and strong accuracy when dealing with pseudorandom functions. However, since this would violate Eq. 9, measure-first protocols have to fail when applied to pseudorandom functions.\n\n\n# 4 CONCLUSION\n\n\nIn our study, we explored the constraints and capabilities of learning from quantum data. We established a formal machine learning framework that contrasts two protocols: \"fully quantum\", which adjusts measurements based on data, and \"measure-first\" restricted by fixed initial (though arbitrarily powerful) measurements. In particular, we provided an example of a learning problem efficiently solved by a fully-quantum protocol but beyond the capabilities of measure-first protocols. Moreover, we showed that this persists even when we limit from universal quantum states, which include also those intractable to prepare, to efficiently preparable quantum states. These findings underscore the crucial role of processing unmeasured quantum data in machine learning, presenting a setting where quantum advantages arise. In particular, they imply that certain learning tasks inherently require the \"exponential capacity\" of quantum states, distinct from classical data. In other words, the number of bits needed to store  $n$  qubits, in a way that allows a learner to successfully solve the learning problem, is exponential in  $n$ . Such a conclusion is analogous to what Montanaro refers as \"anti-Holevo\" theorems Montanaro (2019). While our proof relies on both separations in one-way communication complexity and pseudorandom states, we highlight the potential for more general constructions. Instead of considering states demonstrating a one-way communication complexity separation, any quantum advice state used in quantum advice complexity classes that cannot be classically simulated with polynomial overhead could suffice, leveraging the separation between the classes FBQP/qpoly and FBQP/poly showed in Aaronson et al. (2023). In particular, by the result in Aaronson & Drucker (2010) any problem that can be efficiently solved with a polynomial-sized quantum advice state can also be solved with an advice state that is the ground state of a local Hamiltonian. This suggests that learning problems where the input quantum data consists of ground states of sufficiently complex local Hamiltonians are promising candidates for demonstrating a separation between measure-first and fully-quantum protocols. Finally, we note that beyond pseudorandom states one could use so-called \"computationally indistinguishable\" states, which are known to exist assuming the intractability of the graph isomorphism problem Kawachi et al. (2012), or various other complexity theoretic assumptions Brakerski et al. (2022).\n\n\n# REFERENCES\n\n- Scott Aaronson. Shadow tomography of quantum states. In Proceedings of the 50th annual ACM SIGACT symposium on theory of computing, pp. 325-338, 2018.\n\n- Scott Aaronson and Andrew Drucker. A full characterization of quantum advice. In Proceedings of the forty-second ACM symposium on Theory of computing, pp. 131-140, 2010.\n\n- Scott Aaronson, Harry Buhrman, and William Kretschmer. A qubit, a coin, and an advice string walk into a relational problem. arXiv:2302.10332, 2023.\n\n- Esma Aimeur, Gilles Brassard, and S\u00e9bastien Gambs. Machine learning in a quantum world. In Proceedings of the Advances in Artificial Intelligence: 19th Conference of the Canadian Society for Computational Studies of Intelligence, pp. 431-442. Springer, 2006.\n\n- Ziv Bar-Yossef, Thathachar S Jayram, and Jordanis Kerenidis. Exponential separation of quantum and classical one-way communication complexity. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, 2004.\n\n- Zvika Brakerski and Omri Shmueli. (pseudo) random quantum states with binary phase. In Theory of Cryptography Conference. Springer, 2019.\n\n- Zvika Brakerski, Ran Canetti, and Luowen Qian. On the computational hardness needed for quantum cryptography. arXiv preprint arXiv:2209.04101, 2022.\n\n- Sitan Chen, Jordan Cotler, Hsin-Yuan Huang, and Jerry Li. Exponential separations between learning with and without quantum memory. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pp. 574-585. IEEE, 2022a.\n\n- Sitan Chen, Jordan Cotler, Hsin-Yuan Huang, and Jerry Li. Exponential separations between learning with and without quantum memory. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), 2022b.\n\n- Hao-Chung Cheng, Min-Hsiu Hsieh, and Ping-Cheng Yeh. The learnability of unknown quantum measurements. arXiv:1501.00559, 2015.\n\n- Andreas Elben, Steven T Flammia, Hsin-Yuan Huang, Richard Kueng, John Preskill, Benoit Vermersch, and Peter Zoller. The randomized measurement toolbox. Nature Reviews Physics, 5, 2023.\n\n- Weiyuan Gong and Scott Aaronson. Learning distributions over quantum measurement outcomes. In International Conference on Machine Learning, pp. 11598-11613. PMLR, 2023.\n\n- Daniel Grier, Hakop Pashayan, and Luke Schaeffer. Sample-optimal classical shadows for pure states. arXiv:2211.11810, 2022.\n\n- Hsin-Yuan Huang, Richard Kueng, and John Preskill. Predicting many properties of a quantum system from very few measurements. Nature Physics, 16(10):1050-1057, 2020a.\n\n- Hsin-Yuan Huang, Richard Kueng, and John Preskill. Predicting many properties of a quantum system from very few measurements. Nature Physics, 16, 2020b.\n\n- Hsin-Yuan Huang, Richard Kueng, Giacomo Torlai, Victor V Albert, and John Preskill. Provably efficient machine learning for quantum many-body problems. Science, 377, 2022.\n\n- Sofiene Jerbi, Joe Gibbs, Manuel S Rudolph, Matthias C Caro, Patrick J Coles, Hsin-Yuan Huang, and Zo\u00eb Holmes. The power and limitations of learning quantum dynamics incoherently. arXiv:2303.12834, 2023.\n\n- Akinori Kawachi, Takeshi Koshiba, Harumichi Nishimura, and Tomoyuki Yamakami. Computational indistinguishability between quantum states and its cryptographic application. Journal of cryptology, 25:528-555, 2012.\n\n- Yunchao Liu, Srinivasan Arunachalam, and Kristan Temme. A rigorous and robust quantum speed-up in supervised machine learning. Nature Physics, 2021.\n\n- Ashley Montanaro. Quantum states cannot be transmitted efficiently classically. Quantum, 3, 2019.\n\n- Michael A Nielsen and Isaac L Chuang. Quantum computation and quantum information. Cambridge university press, 2010.\n\n- Ryan Sweke, Jean-Pierre Seifert, Dominik Hangleiter, and Jens Eisert. On the quantum versus classical learnability of discrete distributions. Quantum, 5:417, 2021.\n\n- Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.\n\n- Mark Zhandry. How to construct quantum random functions. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science. IEEE, 2012.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638575.8498187", "msecs": "849.0", "relativeCreated": "172680.3162097931", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Stochastic AUC Maximization with Deep Neural Networks\n\n# Abstract\nStochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.\n\n\n# 1 INTRODUCTION\n\n\nDeep learning has been witnessed with tremendous success for various tasks, including computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2016; Ren et al., 2015), speech recognition (Hinton et al., 2012; Mohamed et al., 2012; Graves, 2013), natural language processing (Bahdanau et al., 2014; Sutskever et al., 2014; Devlin et al., 2018), etc. From an optimization perspective, all of them are solving an empirical risk minimization problem in which the objective function is a surrogate loss of the prediction error made by a deep neural network in comparison with the ground-truth label. For example, for image classification task, the objective function is often chosen as the cross entropy between the probability distribution calculated by forward propagation of a convolutional neural network and the vector encoding true label information (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2016), where the cross entropy is a surrogate loss of the misclassification rate. However, when the data is imbalanced, this formulation is not reasonable since the data coming from minor class have little effect in this case and the model is almost determined by the data from the majority class.\n\n\nTo address this issue, AUC maximization has been proposed as a new learning paradigm (Zhao et al., 2011). Statistically, AUC (short for Area Under the ROC curve) is defined as the probability that the prediction score of a positive example is higher than that of a negative example (Hanley & McNeil, 1982; 1983). Compared with misclassification rate and its corresponding surrogate loss, AUC is more suitable for imbalanced data setting (Elkan, 2001). Several online or stochastic algorithms for AUC maximization have been developed based on a convex surrogate loss (Zhao et al., 2011; Gao et al., 2013; Ying et al., 2016; Liu et al., 2018; Natole et al., 2018). However, all of these works only consider learning a linear predictive model. This naturally motivates the following question:\n\n\n# HOW TO DESIGN STOCHASTIC ALGORITHMS WITH PROVABLE GUARANTEES TO SOLVE THE AUC MAXIMIZATION PROBLEM WITH A DEEP NEURAL NETWORK AS THE PREDICTIVE MODEL?\n\n\nIn this paper, we make some efforts to answer this question. We design two algorithms with state-of-the-art complexities for this problem. Based on a surrogated loss of AUC and inspired by the min-max reformulation in (Ying et al., 2016), we cast the problem into a non-convex concave min-max stochastic optimization problem, where it is nonconvex in the primal variable and concave\n\n\nin the dual variable. This allows us to leverage the inexact proximal point algorithmic framework proposed in (Rafique et al., 2018) to solve stochastic AUC maximization with a deep neural network. However, their algorithms are limited for stochastic AUC maximization with a deep neural network due to three reasons. First, their algorithms are general and do not utilize the underlying favorable property of the the objective function induced by an overparameterized deep neural network, which prevents them from designing better algorithms with faster convergence. Second, these algorithms use a polynomially decaying step size scheme instead of the widely used geometrically decaying step size scheme in deep neural network training. Third, the algorithm in (Rafique et al., 2018) with the best attainable complexity only applies to the finite-sum setting, which needs to go through all data at the end of each stage and are not applicable to the pure stochastic setting.\n\n\nTo address these limitations, we propose to leverage the Polyak-Lojasiewicz (PL) condition of the objective function for AUC maximization with a deep neural network. The PL condition (or its equivalent condition) has been proved for a class of linear and non-linear neural networks (Hardt & Ma, 2016; Charles & Papailiopoulos, 2017; Zhou & Liang, 2017). It is the key to recent developments that prove that (stochastic) gradient descent can find a global minimum for an overparameterized deep neural network (Allen-Zhu et al., 2018; Du et al., 2018b). It is also observed in practice for learning deep neural networks (Li & Yuan, 2017; Kleinberg et al., 2018). From an optimization perspective, the PL condition has been considered extensively for designing faster optimization algorithms in the literature (Karimi et al., 2016; Reddi et al., 2016; Lei et al., 2017). However, there still remains a big gap between existing algorithms that focus on solving a minimization problem and the considered min-max problem of AUC maximization. It is a non-trivial task to leverage the PL condition of a non-convex minimization objective for developing faster primal-dual stochastic algorithms to solve its equivalent non-convex concave min-max problem. The main theoretical contributions in this paper are to solve this issue. Our contributions are:\n\n- - We propose a stochastic algorithm named Proximal Primal-Dual Stochastic Gradient (PPD-SG) for solving a min-max formulation of AUC maximization under the PL condition of the surrogated AUC objective with a deep neural network. We establish a convergence rate in the order of  $O(1 / \\epsilon)$ , which is faster than that achieved by simply applying the result in (Rafique et al., 2018) to the considered problem under the PL condition, i.e.,  $O(1 / \\epsilon^3)$  and  $O(n / \\epsilon)$  with  $n$  being the size of training set.\n\n- - In addition, we propose an AdaGrad-style primal-dual algorithm named Proximal Primal-Dual Adagrad (PPD-Adagrad), and show that it enjoys better adaptive complexity when the growth of cumulative stochastic gradient is slow. This is the first time an adaptive convergence of a stochastic AdaGrad-style algorithm is established for solving non-convex concave min-max problems.\n\n- - We evaluate the proposed algorithms on several large-scale benchmark datasets. The experimental results show that our algorithms have superior performance than other baselines.\n\n\nTo the best of our knowledge, this is the first work incorporating PL condition into stochastic AUC maximization with a deep neural network as the predictive model, and more generally into solving a non-convex concave min-max problem. Our results achieve the state-of-the-art iteration complexity for non-convex concave min-max problems.\n\n\n# 2 RELATED WORK\n\n\nStochastic AUC Maximization. Stochastic AUC maximization in the classical online setting is challenging due to its pairwise nature. There are several studies trying to update the model each time based on a new sampled/received training data. Instead of storing all examples in the memory, Zhao et al. (2011) employ reservoir sampling technique to maintain representative samples in a buffer, based on which their algorithms update the model. To get optimal regret bound, their buffer size needs to be  $O(\\sqrt{n})$ , where  $n$  is the number of received training examples. Gao et al. (2013) design a new algorithm which is not buffer-based. Instead, their algorithm needs to maintain the first-order and second-order statistics of the received data to compute the stochastic gradient, which is prohibitive for high dimensional data. Based on a novel saddle-point reformulation of a surrogate loss of AUC proposed by (Ying et al., 2016), there are several studies (Ying et al., 2016; Liu et al., 2018; Natole et al., 2018) trying to design stochastic primal-dual algorithms. Ying et al. (2016) employ the classical primal-dual stochastic gradient (Nemirovski et al., 2009) and obtain  $\\widetilde{O}(1/\\sqrt{t})$  convergence\n\n\nrate. Natole et al. (2018) add a strongly convex regularizer, invoke composite mirror descent (Duchi et al., 2010) and achieve  $\\widetilde{O}(1/t)$  convergence rate. Liu et al. (2018) leverage the structure of the formulation, design a multi-stage algorithm and achieve  $\\widetilde{O}(1/t)$  convergence rate without strong convexity assumptions. However, all of them only consider learning a linear model, which results in a convex objective function.\n\n\nNon-Convex Min-max Optimization. Stochastic optimization of non-convex min-max problems have received increasing interests recently (Rafique et al., 2018; Lin et al., 2018; Sanjabi et al., 2018; Lu et al., 2019; Jin et al., 2019). When the objective function is weakly convex in the primal variable and is concave in the dual variable, Rafique et al. (2018) design a proximal guided algorithm in spirit of the inexact proximal point method (Rockafellar, 1976), which solves a sequence of convex-concave subproblems constructed by adding a quadratic proximal term in the primal variable with a periodically updated reference point. Due to the potential non-smoothness of objective function, they show the convergence to a nearly-stationary point for the equivalent minimization problem. In the same vein as (Rafique et al., 2018), Lu et al. (2019) design an algorithm by adopting the block alternating minimization/maximization strategy and show the convergence in terms of the proximal gradient. When the objective is weakly convex and weakly concave, Lin et al. (2018) propose a proximal algorithm which solves a strongly monotone variational inequality in each epoch and establish its convergence to stationary point. Sanjabi et al. (2018) consider non-convex non-concave min-max games where the inner maximization problem satisfies a PL condition, based on which they design a multi-step deterministic gradient descent ascent with convergence to a stationary point. It is notable that our work is different in that (i) we explore the PL condition for the outer minimization problem instead of the inner maximization problem; (ii) we focus on designing stochastic algorithms instead of deterministic algorithms.\n\n\nLeveraging PL Condition for Minimization. PL condition is first introduced by Polyak (Polyak, 1963), which shows that gradient descent is able to enjoy linear convergence to a global minimum under this condition. Karimi et al. (2016) show that stochastic gradient descent, randomized coordinate descent, greedy coordinate descent are able to converge to a global minimum with faster rates under the PL condition. If the objective function has a finite-sum structure and satisfies PL condition, there are several non-convex SVRG-style algorithms (Reddi et al., 2016; Lei et al., 2017; Nguyen et al., 2017; Zhou et al., 2018; Li & Li, 2018; Wang et al., 2018), which are guaranteed to converge to a global minimum with a linear convergence rate. However, the stochastic algorithms in these works are developed for a minimization problem, and hence is not applicable to the min-max formulation for stochastic AUC maximization. To the best of our knowledge, Liu et al. (2018) is the only work that leverages an equivalent condition to the PL condition (namely quadratic growth condition) to develop a stochastic primal-dual algorithm for AUC maximization with a fast rate. However, as mentioned before their algorithm and analysis rely on the convexity of the objective function, which does not hold for AUC maximization with a deep neural network.\n\n\nFinally, we notice that PL condition is the key to many recent works in deep learning for showing there is no spurious local minima or for showing global convergence of gradient descent and stochastic gradient descent methods (Hardt & Ma, 2016; Li & Yuan, 2017; Arora et al., 2018; Allen-Zhu et al., 2018; Du et al., 2018b;a; Li & Liang, 2018; Allen-Zhu et al., 2018; Zou et al., 2018; Zou & Gu, 2019). Using the square loss, it has also been proved that the PL condition holds globally or locally for deep linear residual network (Hardt & Ma, 2016), deep linear network, one hidden layer neural network with Leaky ReLU activation (Charles & Papailiopoulos, 2017; Zhou & Liang, 2017). Several studies (Li & Yuan, 2017; Arora et al., 2018; Allen-Zhu et al., 2018; Du et al., 2018b; Li & Liang, 2018) consider the trajectory of (stochastic) gradient descent on learning neural networks, and their analysis imply the PL condition in a certain form. For example, Du et al. (2018b) show that when the width of a two layer neural network is sufficiently large, a global optimum would lie in the ball centered at the initial solution, in which PL condition holds. Allen-Zhu et al. (2018) extends this insight further to overparameterized deep neural networks with ReLU activation, and show that the PL condition holds for a global minimum around a random initial solution.\n\n\n# 3 PRELIMINARIES AND NOTATIONS\n\n\nLet  $\\| \\cdot \\|$  denote the Euclidean norm. A function  $f(\\mathbf{x})$  is  $\\rho$ -weakly convex if  $f(\\mathbf{x}) + \\frac{\\rho}{2} \\| \\mathbf{x} \\|^2$  is convex, where  $\\rho$  is the so-called weak-convexity parameter. A function  $f(\\mathbf{x})$  satisfies PL condition with parameter  $\\mu > 0$  if  $f(\\mathbf{x}) - f(\\mathbf{x}_*) \\leq \\frac{1}{2\\mu} \\| \\nabla f(\\mathbf{x}) \\|^2$ , where  $\\mathbf{x}_*$  stands for the optimal solution of\n\n\n$f$ . Let  $\\mathbf{z} = (\\mathbf{x},y)\\sim \\mathbb{P}$  denote a random data following an unknown distribution  $\\mathbb{P}$ , where  $\\mathbf{x}\\in \\mathcal{X}$  represents the feature vector and  $y\\in \\mathcal{V} = \\{-1, + 1\\}$  represents the label. Denote by  $\\mathcal{Z} = \\mathcal{X}\\times \\mathcal{V}$  and by  $p = \\operatorname *{Pr}(y = 1) = \\mathbb{E}_y\\left[\\mathbb{I}_{[y = 1]}\\right]$ , where  $\\mathbb{I}(\\cdot)$  is the indicator function.\n\n\nThe area under the curve (AUC) on a population level for a scoring function  $h: \\mathcal{X} \\to \\mathbb{R}$  is defined as\n\n\n$$\n\n$$\n\\operatorname {A U C} (h) = \\Pr \\left(h (\\mathbf {x}) \\geq h \\left(\\mathbf {x} ^ {\\prime}\\right) \\mid y = 1, y ^ {\\prime} = - 1\\right),\n$$\n\n$$\n\n\nwhere  $\\mathbf{z} = (\\mathbf{x},y)$  and  $\\mathbf{z}' = (\\mathbf{x}',y')$  are drawn independently from  $\\mathbb{P}$ . By employing the squared loss as the surrogate for the indicator function that is a common choice used by previous studies (Ying et al., 2016; Gao et al., 2013), the AUC maximization problem can be formulated as\n\n\n$$\n\n$$\n\\min _ {h \\in \\mathcal {H}} \\mathbb {E} _ {\\mathbf {z}, \\mathbf {z} ^ {\\prime}} \\left[ (1 - h (\\mathbf {x}) + h (\\mathbf {x} ^ {\\prime})) ^ {2} \\big | y = 1, y ^ {\\prime} = - 1 \\right],\n$$\n\n$$\n\n\nwhere  $\\mathcal{H}$  denotes a hypothesis class. All previous works of AUC maximization assume  $h(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$  for simplicity. Instead, we consider learning a general nonlinear model parameterized by  $\\mathbf{w}$ , i.e.  $h(\\mathbf{w};\\mathbf{x})$ , which is not necessarily linear or convex in terms of  $\\mathbf{w}$  (e.g.,  $h(\\mathbf{w};\\mathbf{x})$  can be a score function defined by a neural network with weights denoted by  $\\mathbf{w}$ ). Hence, the corresponding optimization problem becomes\n\n\n$$\n\n$$\n\\min  _ {\\mathbf {w} \\in \\mathbb {R} ^ {d}} P (\\mathbf {w}) := \\mathbb {E} _ {\\mathbf {z}, \\mathbf {z} ^ {\\prime}} \\left[ (1 - h (\\mathbf {w}; \\mathbf {x}) + h (\\mathbf {w}; \\mathbf {x} ^ {\\prime})) ^ {2} | y = 1, y ^ {\\prime} = - 1 \\right] \\tag {1}\n$$\n\n$$\n\n\nThe following proposition converts the original optimization problem (1) into a saddle-point problem, which is similar to Theorem 1 in (Ying et al., 2016). For completeness, the proof is included in the supplement.\n\n\nProposition 1. The optimization problem (1) is equivalent to\n\n\n$$\n\n$$\n\\min  _ {\\mathbf {w} \\in \\mathbb {R} ^ {d}, (a, b) \\in \\mathbb {R} ^ {2}} \\max  _ {\\alpha \\in \\mathbb {R}} f (\\mathbf {w}, a, b, \\alpha) := \\mathbb {E} _ {\\mathbf {z}} [ F (\\mathbf {w}, a, b, \\alpha ; \\mathbf {z}) ], \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\mathbf{z} = (\\mathbf{x},y)\\sim \\mathbb{P}$  , and\n\n\n$$\n\n$$\n\\begin{array}{l} F (\\mathbf {w}, a, b, \\alpha , \\mathbf {z}) = (1 - p) \\left(h (\\mathbf {w}; \\mathbf {x}) - a\\right) ^ {2} \\mathbb {I} _ {[ y = 1 ]} + p (h (\\mathbf {w}; \\mathbf {x}) - b) ^ {2} \\mathbb {I} _ {[ y = - 1 ]} \\\\ + 2 \\left(1 + \\alpha\\right) \\left(p h (\\mathbf {w}; \\mathbf {x}) \\mathbb {I} _ {[ y = - 1 ]} - (1 - p) h (\\mathbf {w}; \\mathbf {x}) \\mathbb {I} _ {[ y = 1 ]}\\right) - p (1 - p) \\alpha^ {2} \\\\ \\end{array}\n$$\n\n$$\n\n\nRemark: It is notable that the min-max formulation (2) is more favorable than the original formulation (1) for developing a stochastic algorithm that updates the model parameters based on one example or a mini-batch of samples. For stochastic optimization of (1), one has to carefully sample both positive and negative examples, which is not allowed in an online setting. It is notable that in the classical batch-learning setting,  $p$  becomes the ratio of positive training examples and the expectation in (2) becomes average over  $n$  individual functions. However, our algorithms are applicable to both batch-learning setting and online learning setting.\n\n\nDefine  $\\mathbf{v} = (\\mathbf{w}^{\\top},a,b)^{\\top}$ ,  $\\phi (\\mathbf{v}) = \\max_{\\alpha}f(\\mathbf{v},\\alpha)$ . It is clear that  $\\min_{\\mathbf{w}}P(\\mathbf{w}) = \\min_{\\mathbf{v}}\\phi (\\mathbf{v})$  and  $P(\\mathbf{w})\\leq \\phi (\\mathbf{v})$  for any  $\\mathbf{v} = (\\mathbf{w}^{\\top},a,b)^{\\top}$ . The following assumption is made throughout the paper.\n\n\nAssumption 1. (1)  $\\mu (\\phi (\\mathbf{v}) - \\phi (\\mathbf{v}_{*}))\\leq \\frac{1}{2}\\| \\nabla \\phi (\\mathbf{v})\\|^{2}$ , where  $\\mu >0$  and  $\\mathbf{v}_*$  is the optimal solution of  $\\phi$ . (2)  $h(\\mathbf{w};\\mathbf{x})$  is  $\\tilde{L}$ -Lipschitz continuous in terms of  $\\mathbf{w}$  for all  $\\mathbf{x}$ . (3)  $\\phi (\\mathbf{v})$  is  $L$ -smooth. (4) Var  $[h(\\mathbf{w};\\mathbf{x})|y = -1] + \\operatorname {Var}[h(\\mathbf{w};\\mathbf{x})|y = 1]\\leq \\sigma^2$ . (5) Given a initial solution  $\\bar{\\mathbf{v}}_0$ , there exists  $\\Delta_0 > 0$  such that  $\\phi (\\bar{\\mathbf{v}}_0) - \\phi (\\mathbf{v}_*)\\leq \\Delta_0$ , where  $\\mathbf{v}_*$  is the global minimum of  $\\phi$ .\n\n\nRemark: The first condition is inspired by a PL condition on the objective function  $P(\\mathbf{w})$  for learning a deep neural network. The following lemma establishes the connection.\n\n\nLemma 1. Suppose  $\\| \\nabla_{\\mathbf{w}}h(\\mathbf{w};\\mathbf{x})\\| \\leq \\tilde{L}$  for all  $\\mathbf{w}$  and  $\\mathbf{x}$ . If  $P(\\mathbf{w})$  satisfies PL condition, i.e. there exists  $\\mu' > 0$ , such that  $\\mu'(P(\\mathbf{w}) - \\min_{\\mathbf{w}} P(\\mathbf{w})) \\leq \\frac{1}{2} \\| \\nabla_{\\mathbf{w}}P(\\mathbf{w})\\|^2$ , then we have  $\\mu(\\phi(\\mathbf{v}) - \\phi(\\mathbf{v}_*)) \\leq \\frac{1}{2} \\| \\nabla \\phi(\\mathbf{v})\\|^2$ , where  $\\mu = \\frac{1}{\\max\\left(\\frac{1}{2\\min(p,1-p)} + \\frac{2\\tilde{L}^2}{\\mu'\\min(p^2,(1-p)^2)}, \\frac{2}{\\mu'}\\right)}$ .\n\n\nRemark: The PL condition of  $P(\\mathbf{w})$  could be proved for learning a neural network similar to existing studies, which is not the main focus of this paper. Nevertheless, In Appendix A.7, we provide an example for AUC maximization with one-hidden layer neural network.\n\n\nWarmup. We first discuss the algorithms and their convergence results of (Rafique et al., 2018) applied to the considered min-max problem. They have algorithms for problems in batch-learning setting and online learning setting. Since the algorithms for the batch-learning setting have complexities scaling with  $n$ , we will concentrate on the algorithm for the online learning setting. The algorithm\n\n\n```\n1: Initialize  $\\bar{\\mathbf{v}}_0 = \\mathbf{0} \\in \\mathbb{R}^{d+2}$ ,  $\\bar{\\alpha}_0 = 0$ , the global index  $j = 0$   \n2: for  $k = 1, \\ldots, K$  do  \n3:  $\\mathbf{v}_0^k = \\bar{\\mathbf{v}}_{k-1}$ ,  $\\alpha_0^k = \\bar{\\alpha}_{k-1}$ ,  $\\eta_k = \\eta_0 / k$ ,  $T_k = T_0 \\cdot k^2$   \n4: for  $t = 1, \\ldots, T_k$  do  \n5: Receive  $\\mathbf{z}_j = (\\mathbf{x}_j, y_j)$  from  $\\mathbb{P}$ ,  $\\hat{\\mathbf{g}}_{\\mathbf{v}} = \\nabla_{\\mathbf{v}} F(\\mathbf{v}_{t-1}^k, \\alpha_{t-1}^k; \\mathbf{z}_j)$ ,  $\\hat{\\mathbf{g}}_{\\alpha} = \\nabla_{\\alpha} F(\\mathbf{v}_{t-1}^k, \\alpha_{t-1}^k; \\mathbf{z}_j)$   \n6:  $\\mathbf{v}_t^k = \\Pi_{\\Omega_1} \\left[ \\mathbf{v}_{t-1}^k - \\eta_k \\left( \\hat{\\mathbf{g}}_{\\mathbf{v}} + \\frac{1}{\\gamma} (\\mathbf{v}_{t-1}^k - \\mathbf{v}_0^k) \\right) \\right]$ , where  $\\Omega_1 = \\{\\mathbf{v} : \\| \\mathbf{v} \\| \\leq R_1\\}$   \n7:  $\\alpha_t^k = \\Pi_{\\Omega_2} \\left[ \\alpha_{t-1}^k + \\eta_k \\hat{\\mathbf{g}}_{\\alpha} \\right]$ , where  $\\Omega_2 = \\{\\alpha : |\\alpha| \\leq R_2\\}$   \n8: end for  \n9:  $\\bar{\\mathbf{v}}_k = \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{v}_t^k$ ,  $\\bar{\\alpha}_k = \\frac{1}{T} \\sum_{t=1}^{T} \\alpha_t^k$   \n10: end for  \n11: Sample  $\\tau$  uniformly randomly from  $\\{1, \\ldots, K\\}$   \n12: return  $\\bar{\\mathbf{v}}_\\tau, \\bar{\\alpha}_\\tau$\n```\n\n\nis presented in Algorithm 1, which is a direct application of Algorithm 2 of (Rafique et al., 2018) to an online setting. Since their analysis requires the domain of the primal and the dual variable to be bounded, hence we add a ball constraint on the primal variable and the dual variable as well. As long as  $R_{1}$  and  $R_{2}$  is sufficiently large, they should not affect the solution. The convergence result of Algorithm 1 is stated below.\n\n\nTheorem 1. (Rafique et al., 2018) Suppose  $f(\\mathbf{v},\\alpha)$  is  $\\rho$ -weakly convex in  $\\mathbf{v}$  and concave in  $\\alpha$ . Let  $\\gamma = 1 / 2\\rho$ , and define  $\\hat{\\mathbf{v}}_{\\tau} = \\arg \\min_{\\mathbf{v}}\\phi (\\mathbf{v}) + \\frac{1}{2\\gamma}\\| \\mathbf{v} - \\bar{\\mathbf{v}}_{\\tau}\\|^{2}$ . Algorithm 1 with  $T_{k} = ck^{2}$  and  $K = \\widetilde{O} (\\epsilon^{-2})$  ensures that  $\\mathbb{E}\\left[\\mathrm{dist}^2 (0,\\partial \\phi (\\hat{\\mathbf{v}}_\\tau))\\right]\\leq \\frac{1}{\\gamma^2}\\mathbb{E}\\| \\hat{\\mathbf{v}}_\\tau -\\bar{\\mathbf{v}}_\\tau \\| ^2\\leq \\epsilon^2$ . The total iteration complexity is  $\\widetilde{O} (\\epsilon^{-6})$ .\n\n\nRemark: Under the condition  $\\phi (\\mathbf{v})$  is smooth and the returned solution is within the added bounded ball constraint, the above result implies  $\\mathbb{E}[\\| \\nabla \\phi (\\bar{\\mathbf{v}}_{\\tau})\\|^{2}\\leq \\epsilon ]$  with a complexity of  $\\widetilde{O}(1 / \\epsilon^3)$ . It further implies that with a complexity of  $\\widetilde{O}(1 / (\\mu^3\\epsilon^3))$  we have  $\\mathbb{E}[\\phi (\\bar{\\mathbf{v}}_{\\tau}) - \\min_{\\mathbf{v}}\\phi (\\mathbf{v})]\\leq \\epsilon$  under the assumed PL condition.\n\n\nWe can see that this complexity result under the PL condition of  $\\phi (\\mathbf{v})$  is worse than the typical complexity result of stochastic gradient descent method under the PL condition (i.e.,  $O(1 / \\epsilon)$ ) (Karimi et al., 2016). It remains an open problem how to design a stochastic primal-dual algorithm for solving  $\\min_{\\mathbf{v}}\\max_{\\alpha}F(\\mathbf{v},\\alpha)$  in order to achieve a complexity of  $O(1 / \\epsilon)$  in terms of minimizing  $\\phi (\\mathbf{v})$ . A naive idea is to solve the inner maximization problem of  $\\alpha$  first and the use SGD on the primal variable  $\\mathbf{v}$ . However, this is not viable since exact maximization over  $\\alpha$  is a non-trivial task.\n\n\n# 4 ALGORITHMS AND THEORETICAL ANALYSIS\n\n\nIn this section, we present two primal-dual algorithms for solving the min-max optimization problem (2) with corresponding theoretical convergence results. For simplicity, we first assume the positive ratio  $p$  is known in advance, which is true in the batch-learning setting. Handling the unknown  $p$  in an online learning setting is a simple extension, which will be discussed in Section 4.3. The proposed algorithms follow the same proximal point framework proposed in (Rafique et al., 2018), i.e., we solve the following convex-concave problems approximately and iteratively:\n\n\n$$\n\n$$\n\\min  _ {\\mathbf {v}} \\max  _ {\\alpha \\in \\mathbb {R}} \\left\\{f (\\mathbf {v}, \\alpha) + \\frac {1}{2 \\gamma} \\| \\mathbf {v} - \\mathbf {v} _ {0} \\| ^ {2} \\right\\} \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\gamma < 1 / L$  to ensure that the new objective function becomes convex and concave, and  $\\mathbf{v}_0$  is periodically updated.\n\n\n# 4.1 PROXIMAL PRIMAL-DUAL STOCHASTIC GRADIENT\n\n\nOur first algorithm named Proximal Primal-Dual Stochastic Gradient (PPD-SG) is presented in Algorithm 2. Similar to Algorithm 1, it has a nested loop, where the inner loop is to approximately solve a regularized min-max optimization problem (3) using stochastic primal-dual gradient method, and the outer loop updates the reference point and learning rate. One key difference is that PPD-SG uses a geometrically decaying step size scheme, while Algorithm 1 uses a polynomially decaying step size scheme. Another key difference is that at the end of  $k$ -th outer loop, we update the dual variable  $\\bar{\\alpha}_k$  in Step 12, which is motivated by its closed-form solution given  $\\bar{\\mathbf{v}}_k$ . In particular, the\n\n\n```\n1: Initialize  $\\bar{\\mathbf{v}}_0 = \\mathbf{0} \\in \\mathbb{R}^{d+2}$ ,  $\\bar{\\alpha}_0 = 0$ , the global index  $j = 0$   \n2: for  $k = 1, \\ldots, K$  do  \n3:  $\\mathbf{v}_0^k = \\bar{\\mathbf{v}}_{k-1}, \\alpha_0^k = \\bar{\\alpha}_{k-1}, \\eta_k = \\eta_0 \\exp\\left(-\\left(k - 1\\right)\\frac{\\mu / L}{5 + \\mu / L}\\right)$   \n4: for  $t = 1, \\ldots, T_k$  do  \n5: Receive  $\\mathbf{z}_j = (\\mathbf{x}_j, y_j)$  from  $\\mathbb{P}$ ,  $\\hat{\\mathbf{g}}_{\\mathbf{v}} = \\nabla_{\\mathbf{v}} F(\\mathbf{v}_{t-1}^k, \\alpha_{t-1}^k; \\mathbf{z}_j)$ ,  $\\hat{\\mathbf{g}}_{\\alpha} = \\nabla_{\\alpha} F(\\mathbf{v}_{t-1}^k, \\alpha_{t-1}^k; \\mathbf{z}_j)$   \n6:  $\\mathbf{v}_t^k = \\mathbf{v}_{t-1}^k - \\eta_k\\left(\\hat{\\mathbf{g}}_{\\mathbf{v}} + \\frac{1}{\\gamma}(\\mathbf{v}_{t-1}^k - \\mathbf{v}_0^k)\\right)$   \n7:  $\\alpha_t^k = \\alpha_{t-1}^k + \\eta_k\\hat{\\mathbf{g}}_{\\alpha}$   \n8:  $j = j + 1$   \n9: end for  \n10:  $\\bar{\\mathbf{v}}_k = \\frac{1}{T_k}\\sum_{t=1}^{T_k}\\mathbf{v}_t^k$   \n11: Draw a minibatch  $\\{\\mathbf{z}_j, \\ldots, \\mathbf{z}_{j+m_k-1}\\}$  of size  $mk$   \n12:  $\\bar{\\alpha}_k = \\frac{\\sum_{i=j}^{j+m_k-1}h(\\bar{\\mathbf{w}}_k;\\mathbf{x}_j)\\mathbb{I}_{y_j=-1}}{\\sum_{i=j}^{j+m_k-1}\\mathbb{I}_{y_j=-1}} - \\frac{\\sum_{i=j}^{j+m_k-1}h(\\bar{\\mathbf{w}}_k;\\mathbf{x}_j)\\mathbb{I}_{y_j=1}}{\\sum_{i=j}^{j+m_k-1}\\mathbb{I}_{y_j=1}}$   \n13:  $j = j + mk$   \n14: end for  \n15: return  $\\bar{\\mathbf{v}}_K, \\bar{\\alpha}_K$\n```\n\n\ngiven  $\\bar{\\mathbf{v}}_k$ , the dual solution that optimizes the inner maximization problem is given by:\n\n\n$$\n\n$$\n\\alpha = \\frac {\\mathbb {E} [ h (\\bar {\\mathbf {w}} _ {k} ; \\mathbf {x}) I _ {y = - 1} ]}{1 - p} - \\frac {\\mathbb {E} [ h (\\bar {\\mathbf {w}} _ {k} ; \\mathbf {x}) I _ {y = 1} ]}{p} = \\mathbb {E} _ {\\mathbf {x}} [ h (\\bar {\\mathbf {w}} _ {k}; \\mathbf {x}) | y = - 1 ] - \\mathbb {E} _ {\\mathbf {x}} [ h (\\bar {\\mathbf {w}} _ {k}; \\mathbf {x}) | y = 1 ].\n$$\n\n$$\n\n\nIn the algorithm, we only use a small number of samples in Step 11 to compute an estimation of the optimal  $\\alpha$  given  $\\bar{\\mathbf{v}}_k$ . These differences are important for us to achieve lower iteration complexity of PPD-SG. Next, we present our convergence results of PPD-SG.\n\n\nLemma 2 (One Epoch Analysis of Algorithm 2). Suppose Assumption 1 and there exists  $G > 0$  such that  $\\| \\hat{\\mathbf{g}}_t^k\\| _2\\leq G$  where  $\\hat{\\mathbf{g}}_t^k = \\left(\\nabla_{\\mathbf{v}}F(\\mathbf{v}_t^k,\\alpha_t^k;\\mathbf{z})^\\top +\\frac{1}{\\gamma}\\left(\\mathbf{v}_t^k -\\mathbf{v}_0^k\\right)^\\top , - \\nabla_\\alpha F(\\mathbf{v}_t^k,\\alpha_t^k;\\mathbf{z})^\\top\\right)^\\top$  Define  $\\phi_k(\\mathbf{v}) = \\phi (\\mathbf{v}) + \\frac{1}{2\\gamma}\\| \\mathbf{v} - \\bar{\\mathbf{v}}_{k - 1}\\| ^2$ $\\mathbf{s}_k = \\arg \\min_{\\mathbf{v}\\in \\mathbb{R}^{d + 2}}\\phi_k(\\mathbf{v})$  Choosing  $m_{k}\\geq \\frac{\\sigma^{2}}{\\eta_{k}^{2}G^{2}T_{k}}$  then we have\n\n\n$$\n\n$$\n\\mathbb {E} _ {k - 1} \\left[ \\phi_ {k} (\\bar {\\mathbf {v}} _ {k}) - \\underset {\\mathbf {v}} {\\min} \\phi_ {k} (\\mathbf {v}) \\right] \\leq \\frac {\\| \\bar {\\mathbf {v}} _ {k - 1} - \\mathbf {s} _ {k} \\| ^ {2} + 8 \\tilde {L} ^ {2} \\mathbb {E} _ {k - 1} \\| \\bar {\\mathbf {v}} _ {k - 1} - \\bar {\\mathbf {v}} _ {k} \\| ^ {2}}{2 \\eta_ {k} T _ {k}} + 2 \\eta_ {k} G ^ {2}.\n$$\n\n$$\n\n\nwhere  $\\mathbb{E}_{k - 1}$  stands for the conditional expectation conditioning on all the stochastic events until  $\\bar{\\mathbf{v}}_{k - 1}$  is generated.\n\n\nTheorem 2. Suppose the same conditions in Lemma 2 hold. Set  $\\eta_{k} = \\eta_{0}\\exp \\left(-\\left(k - 1\\right)\\frac{\\mu / L}{5 + \\mu / L}\\right)$  and  $T_{k} = \\frac{\\max(2,8\\tilde{L}^{2})}{L\\eta_{0}}\\exp \\left((k - 1)\\frac{\\mu / L}{5 + \\mu / L}\\right),m_{k} = \\frac{\\sigma^{2}L}{G^{2}\\eta_{0}\\max(2,8\\tilde{L}^{2})}\\exp \\left((k - 1)\\frac{\\mu / L}{5 + \\mu / L}\\right),\\gamma = \\frac{1}{2L}$  in Algorithm 2. To return  $\\bar{\\mathbf{v}}_K$  such that  $\\mathbb{E}\\left[\\phi (\\bar{\\mathbf{v}}_K) - \\phi (\\mathbf{v}_*)\\right]\\leq \\epsilon$  , it suffices to choose  $K\\ge$ $\\left(\\frac{5L}{\\mu} +1\\right)\\max \\left(\\log \\frac{2\\Delta_0}{\\epsilon},\\log K + \\log \\frac{24G^2\\eta_0}{5\\epsilon}\\right)$  . As a result, the number of iterations is at most  $\\widetilde{O}\\left(\\frac{LG^2}{\\mu^2\\epsilon}\\right)$  , and the required number of samples is at most  $\\widetilde{O}\\left(\\frac{L^3\\sigma^2}{\\mu^2\\epsilon}\\right)$  , where  $\\widetilde{O} (\\cdot)$  hides logarithmic factor of  $L,\\mu ,\\epsilon ,G,\\sigma$\n\n\nRemark: The above complexity result is similar to that of (Karimi et al., 2016) for solving nonconvex minimization problem under the PL condition up to a logarithmic factor. Compared with the complexity result of Algorithm 1 discussed earlier, i.e.,  $\\widetilde{O}(1 / (\\mu^3\\epsilon^3))$ , the above complexity in the order of  $\\widetilde{O}(1 / (\\mu^2\\epsilon))$  is much better - it not only improves the dependence on  $\\epsilon$  but also improves the dependence on  $\\mu$ .\n\n\n# 4.2 PROXIMAL PRIMAL-DUAL ADAGRAD\n\n\nOur second algorithm named Proximal Primal-Dual Adagrad (PPD-Adagrad) is a AdaGrad-style algorithm. Since it only differs from PPD-SG in the updates of the inner loop, we only present the inner loop in Algorithm 3. The updates in the inner loop are similar to the adaptive updates of traditional AdaGrad (Duchi et al., 2011). We aim to achieve an adaptive convergence by using PPD-AdaGrad. The analysis of PPD-AdaGrad is inspired by the analysis of AdaGrad for non-convex minimization problems (Chen et al., 2019). The key difference is that we have to carefully deal with\n\n\n```\n1: for  $t = 1,\\dots ,T_{k}$  do   \n2:Receive  $\\mathbf{z}_j = (\\mathbf{x}_j,y_j)$  from  $\\mathbb{P},\\hat{\\mathbf{g}}_{\\mathbf{v}} = \\nabla_{\\mathbf{v}}F(\\mathbf{v}_{t}^{k},\\alpha_{t}^{k};\\mathbf{z}_{j}),\\hat{\\mathbf{g}}_{\\alpha} = \\nabla_{\\alpha}F(\\mathbf{v}_{t}^{k},\\alpha_{t}^{k};\\mathbf{z}_{j})$    \n3:  $\\hat{\\mathbf{g}}_t^k = [\\hat{\\mathbf{g}}_{\\mathbf{v}} + \\frac{1}{\\gamma} (\\mathbf{v}_t^k -\\mathbf{v}_0^k); - \\hat{\\mathbf{g}}_\\alpha ]\\in \\mathbb{R}^{d + 3},\\hat{\\mathbf{g}}_{1:t}^k = [\\hat{\\mathbf{g}}_{1:t - 1}^k,\\hat{\\mathbf{g}}_t^k ],s_{t,i}^k = ||\\hat{\\mathbf{g}}_{1:t,i}^k ||_2,$    \n4:  $H_{t}^{k} = \\delta I + \\mathrm{diag}(s_{t}^{k}),\\psi_{t}^{k}(\\mathbf{u}) = \\frac{1}{2}\\langle \\mathbf{u} - \\mathbf{u}_{0}^{k},H_{t}^{k}(\\mathbf{u} - \\mathbf{u}_{0}^{k})\\rangle ,$  where  $\\mathbf{u}_0^k = [\\mathbf{v}_0^k;\\alpha_0^k ]\\in \\mathbb{R}^{d + 3}$    \n5:  $\\mathbf{u}_{t + 1}^k = \\arg \\min_{\\mathbf{u}}\\left\\{\\eta_k\\langle \\frac{1}{t}\\sum_{\\tau = 1}^t\\hat{\\mathbf{g}}_\\tau^k,\\mathbf{u}\\rangle +\\frac{1}{t}\\psi_t^k (\\mathbf{u})\\right\\}$    \n6: end for   \nAlgorithm 4 Update  $T_{+},T_{-},\\widehat{p},p(1 - p),\\bar{y}$  given data  $\\{\\mathbf{z}_j,\\ldots ,\\mathbf{z}_{j + m - 1}\\}$    \n1: Update  $T_{-} = T_{-} + \\sum_{i = j}^{j + m - 1}\\mathbb{I}_{[y_j = -1]},T_{+} = T_{+} + \\sum_{i = j}^{j + m - 1}\\mathbb{I}_{[y_j = 1]}$    \n2:  $\\widehat{p} = T_{+} / (T_{+} + T_{-}),\\bar{y} = \\frac{(j + 2)\\bar{y} + \\sum_{i = j}^{j + m - 1}\\mathbb{I}_{[y_j = 1]}}{j + m + 2},p(1 - p) = \\frac{(j + 1)p(1 - p) + \\sum_{i = j}^{j + m - 1}(\\mathbb{I}_{[y_j = 1]} - \\bar{y})^2}{j + m + 1}$\n```\n\n\nthe primal-dual updates for the non-convex min-max problem. We summarize the convergence results of PPD-AdaGrad below.\n\n\nLemma 3 (One Epoch Analysis of Algorithm 3). Suppose Assumption 1 and  $\\| \\hat{\\mathbf{g}}_t^k\\|_\\infty \\leq \\delta$  hold. Define  $\\phi_k(\\mathbf{v}) = \\phi (\\mathbf{v}) + \\frac{1}{2\\gamma}\\| \\mathbf{v} - \\bar{\\mathbf{v}}_{k - 1}\\|^2,\\mathbf{s}_k = \\arg \\min_{\\mathbf{v}\\in \\mathbb{R}^{d + 2}}\\phi_k(\\mathbf{v}).$  Choosing  $m_{k}\\geq \\frac{\\sigma^{2}}{(d + 3)\\eta_{k}^{2}}$\n\n\n$$\n\n$$\nT _ {k} = \\inf  \\left\\{\\tau : \\tau \\geq M _ {k} \\max  \\left(\\frac {\\left(\\delta + \\max  _ {i} \\| \\hat {\\mathbf {g}} _ {1 : \\tau , i} ^ {k} \\| _ {2}\\right) \\max  (1 , 8 \\tilde {L} ^ {2})}{2 c}, c (\\sum_ {i = 1} ^ {d + 3} \\| \\hat {\\mathbf {g}} _ {1: \\tau , i} ^ {k} \\| _ {2} + (d + 3) (\\delta + \\max  _ {i} \\| \\hat {\\mathbf {g}} _ {1: \\tau , i} ^ {k} \\| _ {2}))\\right) \\right\\}\n$$\n\n$$\n\n\nwith  $M_{k} > 0$ ,  $c > 0$ , then we have\n\n\n$$\n\n$$\n\\mathbb {E} _ {k - 1} \\left[ \\phi_ {k} (\\bar {\\mathbf {v}} _ {k}) - \\underset {\\mathbf {v}} {\\min } \\phi_ {k} (\\mathbf {v}) \\right] \\leq \\frac {c \\left(\\| \\bar {\\mathbf {v}} _ {k - 1} - \\mathbf {s} _ {k} \\| _ {2} ^ {2} + \\mathbb {E} _ {k - 1} \\| \\bar {\\mathbf {v}} _ {k - 1} - \\bar {\\mathbf {v}} _ {k} \\| _ {2} ^ {2}\\right)}{\\eta_ {k} M _ {k}} + \\frac {\\eta_ {k}}{c M _ {k}}.\n$$\n\n$$\n\n\nwhere  $\\mathbb{E}_{k - 1}$  stands for the conditional expectation conditioning on all the stochastic events until  $\\bar{\\mathbf{v}}_{k - 1}$  is generated.\n\n\nTheorem 3. Suppose the same conditions as in Lemma 3 hold. Set  $\\eta_{k} = \\eta_{0}\\exp \\left(-\\frac{(k - 1)}{2}\\frac{\\mu / L}{5 + \\mu / L}\\right)$ ,  $M_{k} = \\frac{4c}{L\\eta_{0}}\\exp \\left(\\frac{(k - 1)}{2}\\frac{\\mu / L}{5 + \\mu / L}\\right)$ ,  $m_{k} = \\frac{\\sigma^{2}}{\\eta_{0}^{2}(d + 3)}\\exp \\left((k - 1)\\frac{\\mu / L}{5 + \\mu / L}\\right)$ ,  $\\gamma = \\frac{1}{2L}$  and  $T_{k}$  as in Lemma 3 where  $c = \\frac{1}{\\sqrt{d + 3}}$ . Suppose  $\\|\\hat{\\mathbf{g}}_{1:T_k,i}^k\\|_2 \\leq \\delta \\cdot T_k^\\alpha$  for  $\\forall k$ , where  $0 \\leq \\alpha \\leq \\frac{1}{2}$ . To return  $\\bar{\\mathbf{v}}_K$  such that  $\\mathbb{E}\\left[\\phi (\\bar{\\mathbf{v}}_K) - \\phi (\\mathbf{v}_*)\\right] \\leq \\epsilon$ , it suffices to choose  $K \\geq \\left(\\frac{5L}{\\mu} + 1\\right)\\max \\left(\\log \\frac{2\\Delta_0}{\\epsilon}, \\log K + \\log \\frac{\\eta_0^2L}{5c^2\\epsilon}\\right)$ . The number of iterations is at most  $\\widetilde{O}\\left(\\left(\\frac{L\\delta^2d}{\\mu^2\\epsilon}\\right)^{\\frac{1}{2(1 - \\alpha)}}\\right)$ , and the required number of samples is at most  $\\widetilde{O}\\left(\\frac{L^3\\sigma^2}{\\mu^2\\epsilon}\\right)$ , where  $\\widetilde{O}(\\cdot)$  hides logarithmic factors of  $L, \\mu, \\epsilon, \\delta$ .\n\n\nRemark: When the cumulative growth of stochastic gradient is slow, i.e.,  $\\alpha < 1/2$ , the number of iterations is less than that in Theorem 2, which exhibits adaptive iteration complexity.\n\n\n# 4.3 EXTENSIONS\n\n\nSetting  $\\eta_k, T_k, m_k$ . It is notable that the setting of  $\\eta_k, T_k, m_k$  depends on unknown parameters  $\\mu, L$ , etc., which are typically unknown. One heuristic to address this issue is that we can decrease  $\\eta_k$  by a constant factor larger than 1 (e.g., 2 or 5 or 10), and similarly increase  $T_k$  and  $m_k$  by a constant factor. Another heuristic is to decrease the step size by a constant factor when the performance on a validation data saturates (Krizhevsky et al., 2012).\n\n\nVariants when  $p$  is unknown. In the online learning setting when  $p$  is unknown, the stochastic gradients of  $f$  in both  $\\mathbf{v}$  and  $\\alpha$  are not directly available. To address this issue, we can keep unbiased estimators for both  $p$  and  $p(1 - p)$  which are independent of the new arrived data, and update these estimators during the optimization procedure. All values depending on  $p$  and  $p(1 - p)$  (i.e.,  $F, \\mathbf{g}_{\\mathbf{v}}, \\mathbf{g}_{\\alpha}$ ) are estimated by substituting  $p$  and  $p(1 - p)$  by  $\\widehat{p}$  and  $p(1 - p)$  (i.e.,  $\\hat{F}, \\hat{\\mathbf{g}}_{\\mathbf{v}}, \\hat{\\mathbf{g}}_{\\alpha}$ ) respectively. The approach for keeping unbiased estimator  $\\hat{p}$  and  $p(1 - p)$  during the optimization is described in Algorithm 4, where  $j$  is the global index, and  $m$  is the number of examples received.\n\n\nExtensions to multi-class problems. In the previous analysis, we only consider the binary classification problem. We can extend it to the multi-class setting. To this end, we first introduce the definition of AUC in this setting according to (Hand & Till, 2001). Suppose there are  $c$  classes, we have  $c$\n\n\nscoring functions for each class, namely  $h(\\mathbf{w}_1;\\mathbf{x}),\\ldots ,h(\\mathbf{w}_c;\\mathbf{x})$ . We assume that these scores are normalized such that  $\\sum_{k = 1}^{c}h(\\mathbf{w}_c;\\mathbf{x}) = 1$ . Note that if these functions are implemented by a deep neural network, they can share the lower layers and have individual last layer of connections. The AUC is defined as\n\n\n$$\n\n$$\n\\operatorname {A U C} (h) = \\frac {1}{c (c - 1)} \\sum_ {i = 1} ^ {c} \\sum_ {j \\neq i} \\Pr \\left(h \\left(\\mathbf {w} _ {i}, \\mathbf {x}\\right) \\geq h \\left(\\mathbf {w} _ {i}; \\mathbf {x} ^ {\\prime}\\right) | y = i, y ^ {\\prime} = j\\right), \\tag {4}\n$$\n\n$$\n\n\nSimilar to Proposition 1, we can cast the problem into\n\n\n$$\n\n$$\n\\min  _ {\\mathbf {w}, \\mathbf {a}, \\mathbf {b}} \\max  _ {\\alpha} \\frac {1}{c (c - 1)} \\sum_ {i = 1} ^ {c} \\sum_ {j \\neq i} \\mathbb {E} _ {\\mathbf {z}} \\left[ F _ {i j} \\left(\\mathbf {w} _ {i}, a _ {i j}, b _ {i j}, \\alpha_ {i j}; \\mathbf {z}\\right) \\right], \\tag {5}\n$$\n\n$$\n\n\nwhere  $\\mathbf{w} = [\\mathbf{w}_1,\\dots ,\\mathbf{w}_c]$  ,a,  $\\mathbf{b},\\alpha \\in \\mathbb{R}^{c\\times c}$ $i,j = 1,\\ldots ,c$ $\\mathbf{z} = (\\mathbf{x},y)\\sim \\mathbb{P},p_i = \\operatorname *{Pr}(y = i)$  , and\n\n\n$$\n\n$$\n\\begin{array}{l} F _ {i j} (\\mathbf {w} _ {i}, a _ {i j}, b _ {i j}, \\alpha_ {i j}, \\mathbf {z}) = p _ {j} \\left(h (\\mathbf {w} _ {i}; \\mathbf {x}) - a _ {i j}\\right) ^ {2} \\mathbb {I} _ {[ y = i ]} + p _ {i} (h (\\mathbf {w} _ {i}; \\mathbf {x}) - b _ {i j}) ^ {2} \\mathbb {I} _ {[ y = j ]} \\\\ + 2 \\left(1 + \\alpha_ {i j}\\right) \\left(p _ {i} h (\\mathbf {w} _ {i}; \\mathbf {x}) \\mathbb {I} _ {[ y = j ]} - p _ {j} h (\\mathbf {w} _ {i}; \\mathbf {x}) \\mathbb {I} _ {[ y = i ]}\\right) - p _ {i} p _ {j} \\alpha_ {i j} ^ {2}. \\\\ \\end{array}\n$$\n\n$$\n\n\nThen we can modify our algorithms to accommodate the multiple class pairs. We can also add another level of sampling of class pairs into computing the stochastic gradients.\n\n\n![](images/9abce1b077d53e1c15bcecb3a831b4eb3e316b421b635af4f200fad91bab903e.jpg)\n\n\n![](images/55a2681efd84c20cfdd7b777837923dd787d5975eb30cd0cd9bbf76f9e5ecb45.jpg)\n\n\n![](images/b78b3af2c5941342c44e760a7bcc84e7a93ce7d711a4a54578fb756fbb4749db.jpg)\n\n\n![](images/a31523adf67843656cd72c5d5ca19330d53d573a49dc51c4eaf6958b4fb4d276.jpg)\n\n\n![](images/e003be0482525208d58e813863284a459e4ce086ba4c6b3eeea2b6a4095d9a27.jpg)\n\n\n![](images/f18618448ff6fdb5a77c6b0429a68f44aa2f8494f1429d58d207350bf9fec934.jpg)\n\n\n![](images/a06c6750def9babddbdd55a317ad750bc6d2682bdc8af47badd101708d356343.jpg)\n\n\n![](images/bcbee0c0cfc71e22f53872d3536656a9c54bf72564c3e9aabc9356adaa7cbf2d.jpg)\n\n\n![](images/10782a43ef80f41acdede72947a5367c5a6097fb9c613ec6ebfc7332bf5eed54.jpg)\n\n\n![](images/7e050f3375349790f934cd0c7f1bb557dea413a223189aece76745beffd9313a.jpg)\n\n\n![](images/2ae4947cb33b8e565918e2f0360e39ab3039612c2fa3fc427dac4525f310cdab.jpg)\n\n\n![](images/f40ab450b345326af03ce7dce2173c27ba5efadfcbed3b242a147830a3daf985.jpg)\n\n\n![Figure 1: Comparison of testing AUC on Cat&Dog, CIFAR10, CIFAR100 and STL10.](images/eb253d41f4c7ee05d2ca295c426b70e1620b7a7b4025388066ac6fb7dfa9f91e.jpg)\n\n\n![](images/9370f5808702cb41210eea59277d5ae5b5547eb1e608e795308b015202a633f6.jpg)\n\n\n![](images/849e00124a1a2e37a9e9c0b9396fc59f4c953e5fe902351d41b3e2fe98c5b8b1.jpg)\n\n\n![](images/4f4be7468b71654f2369f9b6b65796eebd273eaedbf764621ea191c2a147efb5.jpg)\n\n\n# 5 EXPERIMENTAL RESULTS\n\n\nIn this section, we present some empirical results to verify the effectiveness of the proposed algorithms. We compare our algorithms (PPD-SG and PPD-AdaGrad) with three baseline methods including PGA (Algorithm 1), Online AUC method (Ying et al., 2016) (OAUC) that directly employs the standard primal-dual stochastic gradient method with a decreasing step size for solving the min-max formulation, and the standard stochastic gradient descent (SGD) for minimizing cross-entropy loss. Comparing with PGA and OAUC allows us to verify the effectiveness of the proposed algorithms for solving the same formulation, and comparing with SGD allows us to verify the effectiveness of maximizing AUC for imbalanced data. We use a residual network with 20 layers (ResNet-20) to implement the deep neural network for all algorithms.\n\n\nWe use the stagewise step size strategy as in (He et al., 2016) for SGD, i.e. the step size is decreased by 10 times at  $40\\mathrm{K}$ ,  $60\\mathrm{K}$ . We use  $19\\mathrm{k} / 1\\mathrm{k}$ ,  $45\\mathrm{k} / 5\\mathrm{k}$ ,  $45\\mathrm{k} / 5\\mathrm{k}$ ,  $4\\mathrm{k} / 1\\mathrm{k}$  training/validation split on C2, C10, C100, and STL10 respectively. For PPD-SG and PPD-AdaGrad, we set  $T_{s} = T_{0}3^{k}$ ,  $\\eta_{s} = \\eta_{0} / 3^{k}$ .  $T_{0}, \\eta_{0}$  are tuned on a validation data. The value of  $\\gamma$  is tuned for PGA and the same value is used for PPD-SG and PPD-AdaGrad. The initial step size is tuned in [0.1, 0.05, 0.01, 0.008, 0.005] and  $T_{0}$  is tuned in  $[200 \\sim 2000]$  for each algorithm separately. The batch size is set to 128. For STL10, we use a smaller batch size 32 due to the limited training data.\n\n\nWe conduct the comparisons on four benchmark datasets, i.e., Cat&Dog (C2), CIFAR10 (C10), CIFAR100 (C100), STL10. STL10 is an extension of CIFAR10 and the images are acquired from ImageNet. Cat&Dog is from Kaggle containing 25,000 images of dogs and cats and we choose an 80:20 split to construct training and testing set. For each data sets, we construct multiple binary classification tasks with varying imbalanced ratio of number negative examples to number of positive examples. For details of construction of binary classification tasks, please refer to the Appendix A.8.\n\n\nWe report the convergence of AUC on testing data in Figure 1, where the title shows the ratio of the majority class to the minority class. The results about the convergence of AUC versus the time in seconds are also presented in Figure 3. From the results we can see that for the balanced settings with ratio equal to  $50\\%$ , SGD performs consistently better than other methods on C2 and CIFAR10 data. However, it is worse than AUC optimization based methods on CIFAR100 and STL10. For imbalanced settings, AUC maximization based methods are more advantageous than SGD in most cases. In addition, PPD-SG and PPD-AdaGrad are mostly better than other baseline algorithms. In certain cases, PPD-AdaGrad can be faster than PPD-SG. Finally, we observe even better performance (in Appendix) by a mixed strategy that pre-trains the model with SGD and then switches to PPD-SG.\n\n\n# 6 CONCLUSION\n\n\nIn this paper, we consider stochastic AUC maximization problem when the predictive model is a deep neural network. By abuilding on the saddle point reformulation and exploring Polyak-Lojasiewicz condition in deep learning, we have proposed two algorithms with state-of-the-art complexities for stochastic AUC maximization problem. We have also demonstrated the efficiency of our proposed algorithms on several benchmark datasets, and the experimental results indicate that our algorithms converge faster than other baselines. One may consider to extend the analysis techniques to other problems with the min-max formulation.\n\n\n# REFERENCES\n\n- Zeyuan Allen-Zhu, Yanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. arXiv preprint arXiv:1811.03962, 2018.\n\n- Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018.\n\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n\n- Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. arXiv preprint arXiv:1710.08402, 2017.\n\n- Zaiyi Chen, Zhuoning Yuan, Jinfeng Yi, Bowen Zhou, Enhong Chen, and Tianbao Yang. Universal stagewise learning for non-convex problems with convergence on averaged solutions. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Syx5V2CcFm.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\n- Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.\n\n- Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.\n\n- John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.\n\n- John C Duchi, Shai Shalev-Shwartz, Yoram Singer, and Ambuj Tewari. Composite objective mirror descent. In  $COLT$ , pp. 14-26, 2010.\n\n- Charles Elkan. The foundations of cost-sensitive learning. In International joint conference on artificial intelligence, volume 17, pp. 973-978. Lawrence Erlbaum Associates Ltd, 2001.\n\n- Wei Gao, Rong Jin, Shenghuo Zhu, and Zhi-Hua Zhou. One-pass auc optimization. In ICML (3), pp. 906-914, 2013.\n\n- Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\n\n- David J Hand and Robert J Till. A simple generalisation of the area under the roc curve for multiple class classification problems. Machine learning, 45(2):171-186, 2001.\n\n- James A Hanley and Barbara J McNeil. The meaning and use of the area under a receiver operating characteristic (roc) curve. Radiology, 143(1):29-36, 1982.\n\n- James A Hanley and Barbara J McNeil. A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3):839-843, 1983.\n\n- Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Brian Kingsbury, et al. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal processing magazine, 29, 2012.\n\n- Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Minmax optimization: Stable limit points of gradient descent ascent are locally optimal. arXiv preprint arXiv:1902.00618, 2019.\n\n- Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-\u0142ojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 795\u2013811. Springer, 2016.\n\n- Robert Kleinberg, Yanzhi Li, and Yang Yuan. An alternative view: When does sgd escape local minima? arXiv preprint arXiv:1802.06175, 2018.\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097-1105, 2012.\n\n- Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pp. 2348-2358, 2017.\n\n- Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-8166, 2018.\n\n- Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597-607, 2017.\n\n- Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 5564-5574, 2018.\n\n- Qihang Lin, Mingrui Liu, Hassan Rafique, and Tianbao Yang. Solving weakly-convex-weakly-concave saddle-point problems as weakly-monotone variational inequality. arXiv preprint arXiv:1810.10207, 2018.\n\n- Mingrui Liu, Xiaoxuan Zhang, Zaiyi Chen, Xiaoyu Wang, and Tianbao Yang. Fast stochastic auc maximization with o (1/n)-convergence rate. In International Conference on Machine Learning, pp. 3195-3203, 2018.\n\n- Songtao Lu, Ioannis Tsaknakis, Mingyi Hong, and Yongxin Chen. Hybrid block successive approximation for one-sided non-convex min-max problems: Algorithms and applications. arXiv preprint arXiv:1902.08294, 2019.\n\n- Abdel-rahman Mohamed, George E Dahl, and Geoffrey Hinton. Acoustic modeling using deep belief networks. IEEE Transactions on Audio, Speech, and Language Processing, 20(1):14\u201322, 2012.\n\n- Michael Natole, Yiming Ying, and Siwei Lyu. Stochastic proximal algorithms for auc maximization. In International Conference on Machine Learning, pp. 3707-3716, 2018.\n\n- Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574-1609, 2009.\n\n- Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.\n\n- Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Stochastic recursive gradient algorithm for nonconvex optimization. arXiv preprint arXiv:1705.07261, 2017.\n\n- Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.\n\n- Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization: Provable algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060, 2018.\n\n- Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In International conference on machine learning, pp. 314-323, 2016.\n\n- Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pp. 91-99, 2015.\n\n- R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on control and optimization, 14(5):877-898, 1976.\n\n- Maziar Sanjabi, Meisam Razaviyayn, and Jason D Lee. Solving non-convex non-concave min-max games under polyak-1 ojasiewicz condition. arXiv preprint arXiv:1812.02878, 2018.\n\n- Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\n- Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104-3112, 2014.\n\n- Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost: A class of faster variance-reduced algorithms for nonconvex optimization. arXiv preprint arXiv:1810.10690, 2018.\n\n- Yiming Ying, Longyin Wen, and Siwei Lyu. Stochastic online auc maximization. In Advances in Neural Information Processing Systems, pp. 451-459, 2016.\n\n- Peilin Zhao, Rong Jin, Tianbao Yang, and Steven C Hoi. Online auc maximization. In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 233-240, 2011.\n\n\nDongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduced gradient descent for nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 3921-3932, 2018.\n\n\nYi Zhou and Yingbin Liang. Characterization of gradient dominance and regularity conditions for neural networks. arXiv preprint arXiv:1710.06910, 2017.\n\n\nDifan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. arXiv preprint arXiv:1906.04688, 2019.\n\n\nDifan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638577.5094957", "msecs": "509.0", "relativeCreated": "174339.9932384491", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Theoretical Aspects of Bias and Diversity in Minimum Bayes Risk Decoding\n\n# Abstract\nText generation commonly relies on greedy and beam decoding that limit the search space and degrade output quality. Minimum Bayes Risk (MBR) decoding can mitigate this problem by utilizing automatic evaluation metrics and model-generated pseudo-references. Previous studies have conducted empirical analyses to reveal the improvement by MBR decoding, and reported various observations. However, despite these observations, the theoretical relationship between them remains uncertain. To address this, we present a novel theoretical interpretation of MBR decoding from the perspective of bias-diversity decomposition. We decompose errors in the estimated quality of generated hypotheses in MBR decoding into two key factors: *bias*, which reflects the closeness between utility functions and human evaluations, and *diversity*, which represents the variation in the estimated quality of utility functions. Our theoretical analysis reveals the difficulty in simultaneously improving both bias and diversity, and highlights the effectiveness of increasing diversity to enhance MBR decoding performance. This analysis verifies the alignment between our theoretical insights and the empirical results reported in previous work. Furthermore, to support our theoretical findings, we propose a new metric, pseudo-bias, which approximates the bias term using gold references. We also introduce a new MBR approach, Metric-augmented MBR (MAMBR), which increases diversity by adjusting the behavior of utility functions without altering the pseudo-references. Experimental results across multiple NLP tasks show that the decomposed terms in the bias-diversity decomposition correlate well with performance, and that MAMBR improves text generation quality by modifying utility function behavior.\n\n\n# 1 INTRODUCTION\n\n\nAs demonstrated by the success of large language models (LLMs) (Brown et al., 2020; OpenAI et al., 2024), text generation is one of the most fundamental tasks in Natural Language Processing (NLP). Text generation commonly relies on greedy and beam searches, which heavily restrict the search space when decoding texts from a model. This procedure, which only considers the model's predictions within a limited search space, can sometimes degrade the quality of the generated text.\n\n\nMinimum Bayes Risk (MBR) decoding (Goel & Byrne, 2000) can mitigate this problem by using a utility function, essentially an automatic evaluation metric, along with pseudo-references generated by the model. MBR decoding was initially applied to speech recognition (Goel & Byrne, 2000) and later to statistical machine translation (SMT) (Kumar & Byrne, 2002; Duan et al., 2011). Following these successes, MBR decoding has been expanded to various text generation tasks, including neural machine translation (NMT) (Stahlberg et al., 2017), text summarization (Bertsch et al., 2023), and image captioning (Borgeaud & Emerson, 2020).\n\n\nSince MBR decoding has become an important inference technique in text generation, various empirical studies have explored its characteristics. M\u00fcller & Sennrich (2021); Freitag et al. (2022a); Fernandes et al. (2022); Amrhein & Sennrich (2022) highlight the importance of using high quality evaluation metrics that is robust and correlate well with human evaluations as utility functions. Jinnai et al. (2024a); Heineman et al. (2024) emphasize the importance of high-quality pseudo-references that closely resemble human-created ones, while also stressing the significance of pseudo-reference\n\n\ndiversity. Although these empirical findings cover various aspects in detail, a unified interpretation remains challenging due to the lack of theoretical frameworks explaining the relationships behind them.\n\n\nTo address this gap, we provide theoretical interpretations of MBR decoding through bias-diversity decomposition (Krogh & Vedelsby, 1994; Wood et al., 2024). Our theoretical interpretation focuses on errors in the estimated quality of hypotheses in MBR decoding. These errors are decomposed into two critical factors: bias and diversity. The bias term represents the closeness between the estimated quality produced by utility functions and human evaluations. The diversity term reflects the variance in the estimated quality across different utility functions. Based on this interpretation, we theoretically demonstrate the difficulty in improving both the bias and diversity terms simultaneously, and we highlight the effectiveness of increasing diversity in MBR decoding, verifying the correspondence with empirically induced results from previous work.\n\n\nTo empirically verify our theoretical findings, we propose pseudo-bias, which approximates the bias term using gold references. Furthermore, to explore the potential for increasing the diversity term by adjusting the behavior of utility functions without varying the pseudo-references, we introduce a new MBR approach: Metric-augmented MBR (MAMBR).\n\n\nOur empirical analysis on machine translation, text summarization, and image captioning\u2014using pseudo-references generated by five different sampling methods\u2014shows that the decomposed bias and diversity terms correlate with performance, consistent with our theoretical analysis. Moreover, using MAMBR demonstrates performance improvements by simply modifying the behavior of utility functions.\n\n\n# 2 MINIMUM BAYES RISK (MBR) DECODING\n\n\nMinimum Bayes Risk (MBR) decoding (Goel & Byrne, 2000) estimates the quality of each hypothesis  $h$  in a set  $\\mathcal{H}$  by comparing it with each pseudo-reference (evidence sample)  $y$  in a set of all sequences  $\\Omega$  and its model predicted probability  $P(y|x)$  for a given input sequence  $x$ . This process uses an evaluation metric, treated as a utility function  $f_{\\theta}(h,y)$ , which calculates the similarity between  $h$  and  $y$  to choose the best hypothesis  $\\hat{h}_{best}$  in  $\\mathcal{H}$  as follows:\n\n\n$$\n\n$$\n\\hat {h} _ {\\text {b e s t}} = \\underset {h \\in \\mathcal {H}} {\\arg \\max } \\sum_ {y \\in \\Omega} f _ {\\theta} (h, y) P (y | x), \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\theta$  represents the parameters of the evaluation metric used as the utility function  $f_{\\theta}(h,y)$ . Since calculating  $\\Omega$  is intractable, Eikema & Aziz (2020; 2022) replace  $\\Omega$  with  $|\\mathcal{V}|$ , a set of sampled  $y$  as follows:\n\n\n$$\n\n$$\n\\hat {h} _ {m b r} = \\underset {h \\in \\mathcal {H}} {\\arg \\max } \\frac {1}{| \\mathcal {Y} |} \\sum_ {y \\in \\mathcal {Y}} f _ {\\theta} (h, y), \\quad y \\sim P (y | x). \\tag {2}\n$$\n\n$$\n\n\nHere, instead of using the utility function  $f_{\\theta}(h,y)$ , we can assume human-estimated quality (Naskar et al., 2023; Suzgun et al., 2023; Jinnai et al., 2024a; Ohashi et al., 2024) denoted as  $\\hat{f}_{\\hat{\\theta}}(h)$ . Under this assumption, the ideal decoding, which fully relies on human-estimated results, is represented as follows:\n\n\n$$\n\n$$\n\\hat {h} _ {\\text {h u m a n}} = \\underset {h \\in \\mathcal {H}} {\\arg \\max } \\hat {f} _ {\\hat {\\theta}} (h). \\tag {3}\n$$\n\n$$\n\n\nIn this paper, we focus on analyzing the differences between the internally estimated qualities for each hypothesis by MBR decoding and those estimated by humans to better understand the characteristics of MBR decoding (\u00a73).\n\n\n# 3 THEORETICAL ANALYSIS BASED ON BIAS-DIVERSITY DECOMPOSITION\n\n\n# 3.1 EVALUATION DISCREPANCY\n\n\nTo measure the discrepancy between the human estimated quality,  $\\hat{f}_{\\hat{\\theta}}(h)$  and the MBR decoding estimated quality,  $\\frac{1}{|\\mathcal{V}|}\\sum_{y\\in \\mathcal{Y}}f_{\\theta}(h,y)$ , we define a  $|\\mathcal{H}|$ -dimensional vector  $\\mathbf{u}^j$  that represents estimated\n\n\nquality for each hypothesis based on the  $j$ -th pseudo-reference and also define  $\\bar{\\mathbf{u}}$ , the average vector of all  $\\mathbf{u}^j$  as follows:\n\n\n$$\n\n$$\n\\mathbf {u} ^ {j} = \\left[ \\begin{array}{l} u _ {1} ^ {j} \\\\ \\dots \\\\ u _ {| \\mathcal {H} |} ^ {j} \\end{array} \\right], \\quad u _ {i} ^ {j} = f _ {\\theta} \\left(h _ {i}, y _ {j}\\right), \\quad \\bar {\\mathbf {u}} = \\frac {1}{| \\mathcal {Y} |} \\sum_ {j = 1} ^ {| \\mathcal {Y} |} \\mathbf {u} ^ {j}. \\tag {4}\n$$\n\n$$\n\n\nSimilarly, we can define a  $|\\mathcal{H}|$ -dimensional vector,  $\\hat{\\mathbf{u}}$  that represents the human estimated quality for each hypothesis as follows:\n\n\n$$\n\n$$\n\\hat {\\mathbf {u}} = \\left[ \\begin{array}{l} \\hat {u} _ {1} \\\\ \\dots \\\\ \\hat {u} _ {| \\mathcal {H} |} \\end{array} \\right], \\quad \\hat {u} _ {i} = \\hat {f} _ {\\hat {\\theta}} (h _ {i}). \\tag {5}\n$$\n\n$$\n\n\nHere, by using Equations 4 and 5, we can reformulate MBR decoding in Equation 2 and the ideal decoding in Equation 3 as follows:\n\n\n$$\n\n$$\n(2) \\equiv \\hat {h} _ {m b r} = \\underset {h _ {i}} {\\arg \\max } \\bar {u} _ {i}, \\quad (3) \\equiv \\hat {h} _ {h u m a n} = \\underset {h _ {i}} {\\arg \\max } \\hat {u} _ {i}. \\tag {6}\n$$\n\n$$\n\n\nTherefore, based on Equation 6, we can investigate the discrepancy between the estimated quality by MBR decoding and human through the comparison of  $\\bar{\\mathbf{u}}$  and  $\\hat{\\mathbf{u}}$ . In our work, to estimate the discrepancy, we consider the prediction error of  $\\bar{\\mathbf{u}}$  to  $\\hat{\\mathbf{u}}$  by using Mean Squared Error (MSE) as follows:\n\n\n$$\n\n$$\nM S E (\\hat {\\mathbf {u}}, \\bar {\\mathbf {u}}) = \\frac {1}{| \\mathcal {H} |} \\sum_ {i = 1} ^ {| \\mathcal {H} |} \\left(\\hat {u} _ {i} - \\bar {u} _ {i}\\right) ^ {2}. \\tag {7}\n$$\n\n$$\n\n\n# 3.2 BIAS-DIVERSITY DECOMPOSITION\n\n\nOur goal is to reveal the characteristics of MBR decoding through theoretical analysis. To achieve this, we focus on the bias and diversity underlying Equation 7. Based on this approach, we can induce the following decomposition:\n\n\nTheorem 1. The quality estimation error for each hypothesis in MBR decoding,  $(\\hat{u}_i - \\bar{u}_i)^2$ , can be decomposed to bias and diversity (ambiguity) terms (Krogh & Vedelsby, 1994) as follows:\n\n\n$$\n\n$$\n\\left(\\hat {u} _ {i} - \\bar {u} _ {i}\\right) ^ {2} = \\underbrace {\\frac {1}{| \\mathcal {Y} |} \\sum_ {j = 1} ^ {| \\mathcal {Y} |} \\left(\\hat {u} _ {i} - f _ {\\theta} \\left(h _ {i} , y _ {j}\\right)\\right) ^ {2}} _ {\\text {B i a s}} - \\underbrace {\\frac {1}{| \\mathcal {Y} |} \\sum_ {j = 1} ^ {| \\mathcal {Y} |} \\left(\\bar {u} _ {i} - f _ {\\theta} \\left(h _ {i} , y _ {j}\\right)\\right) ^ {2}} _ {\\text {D i v e r s i t y}}. \\tag {8}\n$$\n\n$$\n\n\nProof. See Appendix A.\n\n\nIn Equation 8, two terms represent bias and diversity. Unlike the well-known bias-variance decomposition (Geman et al., 1992) that targets a single estimator which is  $\\mathbf{u}$  in our case<sup>1</sup>, the second term is negative, which is why it is referred to as diversity rather than variance (Wood et al., 2024). The bias term indicates how closely the utility function's estimated quality for a hypothesis matches human estimation. The diversity term reflects how different the utility function's estimated qualities are for each other. This decomposition emphasizes the importance of increasing the diversity term while reducing the bias term to improve the quality estimation error,  $(\\hat{u}_i - \\bar{u}_i)^2$ , for each hypothesis.\n\n\nWhile MBR decoding considers all hypotheses to rank and select the best one, Theorem 1 addresses only the quality estimation for each hypothesis. To bridge this gap, we decompose  $MSE(\\hat{\\mathbf{u}},\\bar{\\mathbf{u}})$  that accounts for all hypotheses. The following theorem addresses this broader perspective:\n\n\nTheorem 2. The quality estimation error for all hypotheses in MBR decoding,  $MSE(\\hat{u},\\bar{u})$ , can be decomposed into bias and diversity terms for all hypotheses as follows:\n\n\n$$\n\n$$\nM S E (\\hat {\\boldsymbol {u}}, \\bar {\\boldsymbol {u}}) = \\underbrace {\\frac {1}{| \\mathcal {H} |} \\sum_ {i = 1} ^ {| \\mathcal {H} |} \\frac {1}{| \\mathcal {Y} |} \\sum_ {j = 1} ^ {| \\mathcal {Y} |} \\left(\\hat {u} _ {i} - f _ {\\theta} \\left(h _ {i} , y _ {j}\\right)\\right) ^ {2}} _ {\\text {B i a s f o r a l l h y p o t h e s e s}} - \\underbrace {\\frac {1}{| \\mathcal {H} |} \\sum_ {i = 1} ^ {| \\mathcal {H} |} \\frac {1}{| \\mathcal {Y} |} \\sum_ {j = 1} ^ {| \\mathcal {Y} |} \\left(\\bar {u} _ {i} - f _ {\\theta} \\left(h _ {i} , y _ {j}\\right)\\right) ^ {2}} _ {\\text {D i v e r s i t y f o r a l l h y p o t h e s e s}}. \\tag {9}\n$$\n\n$$\n\n\nProof. See Appendix B.\n\n\n![](images/9bbb26c7c49f5442386516690c12b2035b51b383897c3dd33c748a699d28c21a.jpg)\n\n\nAs in Theorem 1, Theorem 2 highlights the importance of increasing the diversity term while decreasing the bias term to improve the quality estimation in MBR decoding.\n\n\n# 3.3 INTERPRETATION\n\n\nThe decompositions presented in Theorems 1 and 2 allow us to provide theoretical interpretations for the empirically analyzed characteristics of MBR decoding and its extensions discussed in prior studies.\n\n\n# 3.3.1 CORRELATION TO HUMAN EVALUATION RESULTS\n\n\nThe bias term of the decomposition,  $(\\hat{u}_i - f_\\theta (h_i,y_j))^2$ , highlights the importance of considering the closeness between the human-estimated quality,  $\\hat{u}_i$ , and the quality estimated by the utility function,  $f_{\\theta}(h_i,y_j)$ , for improving the performance of MBR decoding. Specifically, since the utility function,  $f_{\\theta}(h_i,y_j)$ , is influenced by the pseudo-reference  $y_{j}$ , the bias term underscores the significance of considering the utility function's correlation to human evaluation and the closeness between pseudo-references and human-created references. Therefore, it emphasizes the importance of examining both utility functions and sampling strategies for generating pseudo-references.\n\n\nQuality of Evaluation Metrics. Our theoretical insight is supported by empirical findings (M\u00fcller & Sennrich, 2021; Freitag et al., 2022a; Fernandes et al., 2022; Amrhein & Sennrich, 2022), in which the quality of evaluation metrics used as utility functions is crucial for performance improvement.\n\n\nQuality of Pseudo-References. Ohashi et al. (2024); Jinnai et al. (2024a) empirically show the importance of selecting appropriate pseudo-references. Thus, our findings theoretically support these empirically driven insights of these previous studies.\n\n\nChallenges in the Real World. Our theoretical findings emphasize the necessity of directly reducing the bias term. However, this requires human evaluation of the combination of pseudo-references and evaluation metrics, used as utility functions, for each hypothesis. This task is clearly challenging due to the high cost of human evaluation. As a solution, we propose a method to approximate this in \u00a74.1 and evaluated its correlation with task-specific performance in \u00a75.3.\n\n\n# 3.3.2 DIVERSITY OF AUTOMATIC EVALUATION RESULTS\n\n\nBased on the diversity term of the decomposition, increasing diversity can contribute to performance improvements by reducing each prediction error  $(\\hat{u}_i - \\bar{u}_i)^2$  of  $MSE(\\hat{\\mathbf{u}},\\bar{\\mathbf{u}})$  in MBR decoding. A key insight here is that the diversity expressed by  $(\\bar{u}_i - f_\\theta (h_i,y_j))^2$  stems from the different estimated qualities produced by each utility function  $f_{\\theta}(h_i,y_j)$ . Thus, this diversity can be influenced by the pseudo-reference  $y_{j}$  and/or the model parameters  $\\theta$  of the evaluation metric.\n\n\nDiversity of Pseudo-references. This finding supports the previous studies (Freitag et al., 2023a; Jinnai et al., 2024a; Heineman et al., 2024) that conclude the diversity of sampling methods is essential for performance improvement of MBR decoding considering that the diversity of the pseudoreferences can indirectly contribute to increasing the diversity of  $f_{\\theta}(h_i, y_j)$  by each  $y_j$ . Basically, as Jinnai et al. (2024a) introduce, reranking algorithms are more effective when the candidates are diverse (Gimpel et al., 2013; Li & Jurafsky, 2016; Li et al., 2016) owing to their diverse information to make a consensus.\n\n\nDiversity of Evaluation Metrics. We anticipate performance improvements by combining multiple different evaluation metrics as utility functions to increase diversity. While this approach has been shown to improve the quality estimation of generated texts (Glushkova et al., 2023), to the best of our knowledge, it has not yet been applied to MBR decoding.\n\n\nUnexplored Aspect. Furthermore, the effect of increasing the diversity of estimated qualities from utility functions by varying the evaluation metric's model parameters  $\\theta$  remains uncertain. To investigate this, we propose a method to adjust the diversity of estimated qualities by modifying  $\\theta$  in \u00a74.2 and compare its behavior with that of varying pseudo-references in \u00a75.4.\n\n\n# 3.3.3 MBR DECODING AS ENSEMBLE LEARNING\n\n\nOur decomposition of MBR decoding aligns with ensemble learning, which is induced by Krogh & Vedelsby (1994). Thus, we can understand that the quality estimation by MBR decoding is a kind of ensemble learning.\n\n\nQuality Estimation. Our decomposition starts from the definition  $MSE(\\hat{\\mathbf{u}}, \\bar{\\mathbf{u}})$  in Eq. 7, the error between the estimated qualities from human evaluation and MBR decoding. We can actually observe the reduction of errors as the improvement in quality score estimation of (Naskar et al., 2023; Cheng & Vlachos, 2024) by ensembling utility functions that are similar to MBR decoding.\n\n\nWeighted-voting. Furthermore, this viewpoint supports the validity of the previous work (Suzgun et al., 2023; Bertsch et al., 2023) that shows the interpretation of MBR decoding as soft-weighted voting, a variant of ensemble learning. Different from our setting, soft-weighted voting restricts the value range of voters (utility functions) from 0 to 1. Wood et al. (2024) shows that soft-weighted voting can be converted to the decomposition of Krogh & Vedelsby (1994), equivalent to our decomposition in Eq. 8. Therefore, weighted voting-based MBR decoding is similarly explained in our decomposition.\n\n\nNumber of Pseudo-references. Generally, increasing the number of pseudo-references improves performance but demands additional computational cost. DeNero et al. (2009); Eikema & Aziz (2022); Cheng & Vlachos (2023); Deguchi et al. (2024b); Vamvas & Sennrich (2024); Trabelsi et al. (2024) prune samples to speedup inference and maintain the original quality similar to the case of pruning estimators in ensemble learning (Liu et al., 2004; Bonab & Can, 2016; 2019).\n\n\nConsidering an ensemble learning method, such as the Bayes optimal classifier (Mitchell, 1997), and assuming that Eq. 2 approximates the expectation by sampling  $y_{j}$ , we can explain the performance improvement of increased pseudo-references by the law of large numbers and the success of the pruning and weighted utility functions (Jinnai et al., 2024b) through importance sampling (Kloek & Van Dijk, 1978). (See Appendix C for more details.)\n\n\n# 3.3.4 BIAS AND DIVERSITY TRADE-OFF\n\n\nAt first glance, based on the interpretation in \u00a73.3.1 and \u00a73.3.2, decreasing bias while increasing diversity seems to be the best strategy to improve quality estimation performance in MBR decoding, which was investigated by Jinnai et al. (2024a). To understand the validity of this strategy, we need to focus on the bias-diversity trade-off (Krogh & Vedelsby, 1994).\n\n\nLimitation of MBR Decoding. The bias-diversity trade-off highlights the difficulty of decreasing bias while increasing diversity. In Eqs. 8 and 9, when the bias term approaches zero, i.e., each  $f_{\\theta}(h_i, y_j)$  approaching to  $\\hat{u}_i$ , the diversity term also approaches zero. This theoretical fact indicates that even if we can prepare high-quality evaluation metrics and high-quality pseudo-references that correlate well with human behavior, there may be no performance improvement due to diminished diversity.\n\n\nDiversity Assists Inferior Methods. Conversely, when the evaluation metrics and pseudo-references are inferior, we can expect performance improvements through increased diversity at the cost of increased bias. This phenomenon can explain the sometimes competitive performance of BLEU (Papineni et al., 2002) against COMET (Rei et al., 2020) in Freitag et al. (2022b), and that of ancestral sampling (Robert, 1999) against other sampling methods in Freitag et al. (2023a); Ohashi et al. (2024) using MBR decoding. However, unlike the case where decreased bias leads to decreased diversity, increased bias does not guarantee increased diversity. Therefore, we must carefully assess their diversity when using low-quality evaluation metrics and pseudo-references in MBR decoding.\n\n\n# 4 REMAINING PROBLEMS & SOLUTIONS\n\n\nOur theoretical analysis covers various aspects of MBR decoding. However, for a comprehensive analysis, we should investigate empirical results not addressed in previous work and bridge the gap between theory and real-world applications. To this end, we provide the following solutions.\n\n\n# 4.1 PSEUDO-BIAS\n\n\nAs discussed in \u00a73.3.1, the bias term suggests the importance of considering the correlation between the results of human evaluation and the evaluation metric's decisions based on pseudo-references to improve the performance of MBR decoding. However, calculating the bias term requires human evaluation, and conducting human evaluations for each setting is unrealistic and difficult. To address this issue, we introduce pseudo-bias, an approximation of the bias term in our decomposition. By using  $|\\hat{Y}|$ , the number of gold references  $\\hat{y}$ , pseudo-bias is defined as follows:\n\n\n$$\n\n$$\n\\underbrace {\\frac {1}{| \\mathcal {Y} |} \\sum_ {j = 1} ^ {| \\mathcal {Y} |} \\left(\\hat {u} _ {i} - f _ {\\theta} \\left(h _ {i} , y _ {j}\\right)\\right) ^ {2}} _ {\\text {B i a s}} \\approx \\underbrace {\\frac {1}{| \\mathcal {Y} |} \\sum_ {j = 1} ^ {| \\mathcal {Y} |} \\left(\\tilde {u} _ {i} - f _ {\\theta} \\left(h _ {i} , y _ {j}\\right)\\right) ^ {2}} _ {\\text {P s e u d o - b i a s}}, \\quad \\tilde {u} _ {i} = \\frac {1}{| \\hat {\\mathcal {Y}} |} \\sum_ {j = 1} ^ {| \\hat {\\mathcal {Y}} |} f _ {\\theta} \\left(h _ {i}, \\hat {y} _ {j}\\right). \\tag {10}\n$$\n\n$$\n\n\nThis formulation is based on the premise that automatic evaluation metrics correlate to human evaluation when receiving human-created references. Since we can calculate the diversity term without any approximation, we compare pseudo-bias with diversity in terms of how they correlate with performance.\n\n\n# 4.2 METRIC-AUGMENTED MBR\n\n\nThe discussion in \u00a73.3.2 shows the possibility of increasing the diversity of the utility function,  $f_{\\theta}(h_i,y_j)$ , by changing the evaluation metric's model parameters,  $\\theta$ , as well as by introducing diversity through pseudo-references. To this end, we propose a new method called Metric-augmented Minimum Bayes Risk (MAMBR) decoding. In MAMBR, we employ different parameters for the evaluation metric to enhance the diversity of utility functions. Letting  $\\Theta$  be a set of model parameters, MAMBR is defined as follows:\n\n\n$$\n\n$$\n\\hat {h} _ {\\text {m a m b r}} = \\arg \\max  _ {h \\in \\mathcal {H}} \\frac {1}{| \\mathcal {Y} | | \\Theta |} \\sum_ {\\theta \\in \\Theta} \\sum_ {y \\in \\mathcal {Y}} f _ {\\theta} (h, y). \\tag {11}\n$$\n\n$$\n\n\nWhen using MAMBR, we train evaluation metrics with different initial random seeds to generate  $\\Theta$  as a set of diverse model parameters.\n\n\n# 5 EMPIRICAL ANALYSIS\n\n\nWe conduct empirical analysis corresponding to our theoretical analysis through experiments for a comprehensive understanding of MBR decoding.\n\n\n# 5.1 OVERALL SETTINGS\n\n\nWe target three different text generation tasks, machine translation, text summarization, and image captioning to investigate the general performance of MBR decoding. In all tasks, we followed the settings of Jinnai et al. (2024b) for generating samples. We used epsilon sampling (Hewitt et al., 2022) to generate hypotheses. $^3$  For the generation of pseudo-references, we used various sampling approaches: beam decoding, nucleus sampling (Holtzman et al., 2020) with  $p = 0.9$ , ancestral sampling, top- $k$  sampling (Fan et al., 2018) with  $k = 10$ , and epsilon sampling with  $\\epsilon = 0.02$ . We set the sampling size for hypotheses to 64. We chose the sampling size for pseudo-references from  $\\{4, 8, 16, 32, 64\\}$ . We used the following datasets, models $^4$ , and evaluation metrics for each task:\n\n\nMachine Translation We used the WMT19 English to German (En-De) and WMT19 English to Russian (En-Ru) datasets (Barrault et al., 2019). We used facebook/wmt19-en-de for En-De\n\n\n![Figure 1: Correlation between measures in our decomposition and performance for each dataset. The underlined scores indicate statistically significant results  $(p < 0.05)$ .<sup>7</sup> Note that the italic scores at Avg. are not the target of the significance test.](images/b3535ed22a682c79a9d30cd805cf9c29073924b1e7d1d9a1118c60a3a169f81f.jpg)\n\n\nand facebook/wmt19-en-ru for En-Ru, respectively. As the utility function and evaluation metric, we used COMET with the model Unbabel/wmt22-comet-da.\n\n\nText Summarization We used the SAMSum (Gliwa et al., 2019) and XSum (Narayan et al., 2018) datasets, and used philschmid/bart-large-cnn-samsum and facebook/bart-large-xsum for generation in SAMSum and XSum, respectively. As the utility function and evaluation metric, we used BERTScore (Zhang* et al., 2020) with the model microsoft/deberta-xlarge-mnli.\n\n\nImage Captioning We used the MSCOCO dataset (Lin et al., 2014) with the split of Karpathy & Fei-Fei (2015) and the NoCaps dataset (Agrawal et al., 2019). We used Salesforce/blip2-flan-t5-x1-coco and Salesforce/blip2-flan-t5-x1 for generation in MSCOCO and NoCaps, respectively. As the utility function and evaluation metric, we used BERTScore with the model microsoft/deberta-xlarge-mnli. Since both datasets have multiple references, we report their average scores.\n\n\nOur implementation of the generation part is based on the released code of Jinnai et al. (2024b) and the MBR decoding part is based on the toolkit, mbrs by Deguchi et al. (2024a). We generate samples on NVIDIA GeForce RTX 3090 and perform MBR decoding on an NVIDIA RTX A6000.\n\n\n# 5.2 CORRELATION OF BIAS AND DIVERSITY TO PERFORMANCE\n\n\nTo verify our theoretical decomposition, we investigate the correlation of bias and diversity to performance on each dataset. For this purpose, we approximately compute the bias term by using our pseudo-bias in \u00a74.1. Furthermore, we investigate the importance of whether to consider the entire candidate (Eq. 9) or one best candidate (Eq. 8).\n\n\nSettings We compared the following measures in our decomposition: OVERALL BIAS, the first term of Eq. 9 for all hypotheses; ONE BEST BIAS, the first term of Eq. 8 for the one best result by MBR decoding; OVERALL DIVERSITY, the second term of Eq. 9 for all hypotheses; ONE BEST DIVERSITY, the second term of Eq. 8 for the one best result by MBR decoding; OVERALL MSE, indicating errors for all hypotheses based on Eq. 9; and ONE BEST MSE, indicating errors for the one best result by MBR decoding based on Eq. 8. For the comparison, we calculated Spearman's rank correlation and Pearson correlation between these measures and the performance based on the results of five different sampling methods with five different sampling sizes on each dataset (see \u00a75.1). Since lower bias and MSE are better for performance, we took their negative values in the correlation calculation. Moreover, we report averaged correlation across all datasets by Fisher z-transformation (Corey et al., 1998).\n\n\nResults Figure 1 shows the correlation between the measures and performance for each dataset. These results show that MSE for both overall and one best results correlates well with the performance for each dataset in Spearman's rank correlation, indicating the importance of considering quality estimation in MBR decoding, as in Eqs. 8 and 9. On the other hand, the decomposed bias\n\n\n![Figure 2: The relationship between bias, diversity, and performance in MBR decoding. The x-axis shows the number of used pseudo-references.  $(\\uparrow)$  indicates higher scores are better whereas  $(\\downarrow)$  indicates lower scores are better.](images/4c65ad5dbe0f66fe43117697e86226354be6370fc932a3f4a62258090cb95c3f.jpg)\n\n\nand diversity show different tendencies. ONE BEST BIAS, which considers the one best result, is important for bias, whereas OVERALL DIVERSITY, which considers overall results, is important for diversity. This result is reasonable given an assumption that MBR decoding aims to select texts that are close to human-created ones. Based on this assumption, we can say that diversity supports the selection by considering the importance of all hypotheses not covered by One Best Bias. In contrast to the results in Spearman's rank correlation, the coefficients of Pearson's correlation decrease. Based on these results, we can conclude that the measures, i.e., ONE BEST BIAS, OVERALL DIVERSITY, ONE BEST MSE and OVERALL MSE, correlate well with the rank in performance, but they are difficult to precisely capture subtle differences of values. (See Appendix D for further details.)\n\n\n# 5.3 BIAS AND DIVERSITY TRADE-OFF\n\n\nTo investigate the bias-diversity trade-off in more detail, we followed the setup described in \u00a75.2. We plotted the results for each dataset using different sampling methods in Figure 2. The results show that while ancestral sampling exhibits the highest bias, except in the case of the SAMSum\n\n\nTable 1: Results of MAMBR with ancestral sampling. Bold font indicates the best result.\n\n\n<table><tr><td colspan=\"2\"></td><td colspan=\"5\">WMT19 En-De</td><td colspan=\"5\">WMT19 En-Ru</td></tr><tr><td colspan=\"2\">Num. of Samples</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td></tr><tr><td rowspan=\"4\">Num. of Models</td><td>1</td><td>85.7</td><td>85.9</td><td>85.9</td><td>85.9</td><td>85.9</td><td>87.4</td><td>87.4</td><td>87.5</td><td>87.5</td><td>87.5</td></tr><tr><td>2</td><td>85.7</td><td>86.0</td><td>86.0</td><td>85.9</td><td>85.9</td><td>87.4</td><td>87.4</td><td>87.5</td><td>87.5</td><td>87.6</td></tr><tr><td>4</td><td>85.8</td><td>86.0</td><td>86.0</td><td>86.0</td><td>86.0</td><td>87.4</td><td>87.4</td><td>87.5</td><td>87.5</td><td>87.6</td></tr><tr><td>8</td><td>85.8</td><td>86.0</td><td>86.0</td><td>86.0</td><td>86.1</td><td>87.4</td><td>87.5</td><td>87.6</td><td>87.6</td><td>87.6</td></tr><tr><td colspan=\"2\"></td><td colspan=\"5\">SAMSum</td><td colspan=\"5\">XSum</td></tr><tr><td colspan=\"2\">Num. of Samples</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td></tr><tr><td rowspan=\"4\">Num. of Models</td><td>1</td><td>28.6</td><td>29.1</td><td>29.5</td><td>29.5</td><td>29.7</td><td>54.2</td><td>55.2</td><td>55.7</td><td>56.0</td><td>56.1</td></tr><tr><td>2</td><td>28.8</td><td>29.6</td><td>29.9</td><td>29.9</td><td>30.1</td><td>54.2</td><td>55.2</td><td>55.7</td><td>56.1</td><td>56.2</td></tr><tr><td>4</td><td>28.7</td><td>29.5</td><td>29.9</td><td>29.8</td><td>30.2</td><td>54.2</td><td>55.2</td><td>55.8</td><td>56.1</td><td>56.2</td></tr><tr><td>8</td><td>28.7</td><td>29.5</td><td>29.8</td><td>29.9</td><td>30.1</td><td>54.3</td><td>55.3</td><td>55.8</td><td>56.1</td><td>56.2</td></tr><tr><td colspan=\"2\"></td><td colspan=\"5\">MSCOCO</td><td colspan=\"5\">NoCaps</td></tr><tr><td colspan=\"2\">Num. of Samples</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td></tr><tr><td rowspan=\"4\">Num. of Models</td><td>1</td><td>54.9</td><td>55.8</td><td>56.3</td><td>56.5</td><td>56.8</td><td>42.9</td><td>45.3</td><td>46.8</td><td>47.8</td><td>48.6</td></tr><tr><td>2</td><td>54.9</td><td>55.8</td><td>56.4</td><td>56.6</td><td>56.8</td><td>43.2</td><td>45.6</td><td>47.2</td><td>48.3</td><td>48.9</td></tr><tr><td>4</td><td>54.9</td><td>56.0</td><td>56.4</td><td>56.7</td><td>56.9</td><td>43.3</td><td>45.6</td><td>47.3</td><td>48.4</td><td>49.0</td></tr><tr><td>8</td><td>54.9</td><td>56.0</td><td>56.5</td><td>56.8</td><td>56.9</td><td>43.5</td><td>45.7</td><td>47.4</td><td>48.5</td><td>49.0</td></tr></table>\n\n\nTable 2: Results of MAMBR with epsilon sampling. Notations are the same as Table 1.\n\n\n<table><tr><td colspan=\"2\"></td><td colspan=\"5\">WMT19 En-De</td><td colspan=\"5\">WMT19 En-Ru</td></tr><tr><td colspan=\"2\">Num. of Samples</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td></tr><tr><td rowspan=\"4\">Num. of Models</td><td>1</td><td>85.9</td><td>86.1</td><td>86.2</td><td>86.2</td><td>86.2</td><td>87.3</td><td>87.6</td><td>87.7</td><td>87.7</td><td>87.7</td></tr><tr><td>2</td><td>86.0</td><td>86.1</td><td>86.1</td><td>86.2</td><td>86.3</td><td>87.3</td><td>87.6</td><td>87.7</td><td>87.7</td><td>87.7</td></tr><tr><td>4</td><td>86.0</td><td>86.1</td><td>86.2</td><td>86.2</td><td>86.3</td><td>87.4</td><td>87.6</td><td>87.7</td><td>87.7</td><td>87.7</td></tr><tr><td>8</td><td>86.0</td><td>86.2</td><td>86.3</td><td>86.3</td><td>86.4</td><td>87.4</td><td>87.7</td><td>87.7</td><td>87.7</td><td>87.8</td></tr><tr><td colspan=\"2\"></td><td colspan=\"5\">SAMSum</td><td colspan=\"5\">XSum</td></tr><tr><td colspan=\"2\">Num. of Samples</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td></tr><tr><td rowspan=\"4\">Num. of Models</td><td>1</td><td>27.5</td><td>27.9</td><td>28.3</td><td>28.4</td><td>28.5</td><td>54.9</td><td>55.7</td><td>56.1</td><td>56.3</td><td>56.4</td></tr><tr><td>2</td><td>27.7</td><td>28.1</td><td>28.5</td><td>28.6</td><td>28.7</td><td>54.9</td><td>55.7</td><td>56.1</td><td>56.3</td><td>56.5</td></tr><tr><td>4</td><td>27.7</td><td>28.2</td><td>28.5</td><td>28.6</td><td>28.6</td><td>54.9</td><td>55.7</td><td>56.1</td><td>56.4</td><td>56.5</td></tr><tr><td>8</td><td>27.6</td><td>28.2</td><td>28.6</td><td>28.6</td><td>28.7</td><td>54.9</td><td>55.8</td><td>56.2</td><td>56.4</td><td>56.5</td></tr><tr><td colspan=\"2\"></td><td colspan=\"5\">MSCOCO</td><td colspan=\"5\">NoCaps</td></tr><tr><td colspan=\"2\">Num. of Samples</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td></tr><tr><td rowspan=\"4\">Num. of Models</td><td>1</td><td>55.2</td><td>55.9</td><td>56.3</td><td>56.5</td><td>56.7</td><td>44.4</td><td>46.7</td><td>48.5</td><td>49.1</td><td>49.5</td></tr><tr><td>2</td><td>55.2</td><td>55.9</td><td>56.3</td><td>56.5</td><td>56.7</td><td>44.4</td><td>46.8</td><td>48.6</td><td>49.2</td><td>49.6</td></tr><tr><td>4</td><td>55.2</td><td>56.0</td><td>56.3</td><td>56.6</td><td>56.8</td><td>44.5</td><td>46.9</td><td>48.7</td><td>49.3</td><td>49.7</td></tr><tr><td>8</td><td>55.3</td><td>56.1</td><td>56.3</td><td>56.6</td><td>56.8</td><td>44.6</td><td>47.0</td><td>48.7</td><td>49.4</td><td>49.7</td></tr></table>\n\n\ndataset, it sometimes outperforms other sampling methods owing to its greater diversity. Focusing on top-k sampling, which has the lowest bias, again excluding the SAMSum dataset, we can observe that the reduction in bias tends to limit the increase in diversity. This finding supports our previously noted bias-diversity trade-off in MBR decoding. However, as evidenced by the performance of beam decoding, which has the lowest diversity, the importance of bias and diversity varies depending on the target dataset. Therefore, while our theoretical analysis effectively explains the performance tendencies in MBR decoding, it remains essential to consider task-specific features carefully to achieve further performance improvements. (See Appendix E for further details.)\n\n\n# 5.4 EFFECTIVENESS OF METRIC-AUGMENTED MBR\n\n\nWe investigate the possibility of improving performance of MAMBR in Eq. 11 by changing automatic evaluation metric's model parameters.\n\n\nTable 3: Results of MAMBR with beam decoding. Notations are the same as Table 1.\n\n\n<table><tr><td colspan=\"2\"></td><td colspan=\"5\">WMT19 En-De</td><td colspan=\"5\">WMT19 En-Ru</td></tr><tr><td colspan=\"2\">Num. of Samples</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td></tr><tr><td rowspan=\"4\">Num. of Models</td><td>1</td><td>85.2</td><td>85.4</td><td>85.6</td><td>85.7</td><td>85.8</td><td>86.5</td><td>86.8</td><td>87.0</td><td>87.1</td><td>87.1</td></tr><tr><td>2</td><td>85.3</td><td>85.5</td><td>85.7</td><td>85.8</td><td>85.8</td><td>86.5</td><td>86.8</td><td>86.9</td><td>87.1</td><td>87.1</td></tr><tr><td>4</td><td>85.3</td><td>85.5</td><td>85.7</td><td>85.8</td><td>85.8</td><td>86.6</td><td>86.9</td><td>87.0</td><td>87.1</td><td>87.2</td></tr><tr><td>8</td><td>85.3</td><td>85.5</td><td>85.7</td><td>85.8</td><td>85.9</td><td>86.5</td><td>86.8</td><td>87.0</td><td>87.1</td><td>87.2</td></tr><tr><td colspan=\"2\"></td><td colspan=\"5\">SAMSum</td><td colspan=\"5\">XSum</td></tr><tr><td colspan=\"2\">Num. of Samples</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td></tr><tr><td rowspan=\"4\">Num. of Models</td><td>1</td><td>27.6</td><td>28.7</td><td>29.2</td><td>29.3</td><td>29.7</td><td>53.8</td><td>54.0</td><td>54.2</td><td>54.2</td><td>54.4</td></tr><tr><td>2</td><td>27.8</td><td>28.9</td><td>29.2</td><td>29.4</td><td>29.7</td><td>53.8</td><td>53.9</td><td>54.1</td><td>54.2</td><td>54.4</td></tr><tr><td>4</td><td>27.8</td><td>28.9</td><td>29.2</td><td>29.4</td><td>29.7</td><td>53.8</td><td>54.0</td><td>54.2</td><td>54.2</td><td>54.4</td></tr><tr><td>8</td><td>27.8</td><td>28.9</td><td>29.2</td><td>29.4</td><td>29.7</td><td>53.8</td><td>54.0</td><td>54.2</td><td>54.2</td><td>54.4</td></tr><tr><td colspan=\"2\"></td><td colspan=\"5\">MSCOCO</td><td colspan=\"5\">NoCaps</td></tr><tr><td colspan=\"2\">Num. of Samples</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td><td>4</td><td>8</td><td>16</td><td>32</td><td>64</td></tr><tr><td rowspan=\"4\">Num. of Models</td><td>1</td><td>55.4</td><td>55.7</td><td>55.9</td><td>56.1</td><td>56.3</td><td>48.2</td><td>48.8</td><td>49.4</td><td>49.9</td><td>50.2</td></tr><tr><td>2</td><td>55.4</td><td>55.8</td><td>55.9</td><td>56.1</td><td>56.3</td><td>48.2</td><td>48.8</td><td>49.4</td><td>49.9</td><td>50.3</td></tr><tr><td>4</td><td>55.5</td><td>55.7</td><td>55.9</td><td>56.1</td><td>56.3</td><td>48.2</td><td>48.8</td><td>49.5</td><td>49.9</td><td>50.2</td></tr><tr><td>8</td><td>55.5</td><td>55.7</td><td>55.9</td><td>56.1</td><td>56.3</td><td>48.2</td><td>48.8</td><td>49.5</td><td>49.9</td><td>50.2</td></tr></table>\n\n\nSettings To prepare the set of model parameters, we trained eight models by varying their initial seeds. We trained Unbabel/wmt22-comet-da on the Direct Assessments (DA) task (Graham et al., 2013), using the WMT 2017 to 2020 datasets (Bojar et al., 2017; 2018; Barrault et al., 2019; 2020) for training and the WMT 2021 dataset (Akhbardeh et al., 2021) for validation in COMET. Additionally, we trained microsoft/deberta-large on the MNLI dataset from GLUE (Wang et al., 2018) for BERTScore. During inference, to control model diversity, we selected the top- $n$  models based on their proximity to the median validation scores, with  $n$  chosen from 1, 2, 4, 8. We aimed to explore the relationship between the diversity caused by pseudo-references and model parameters, focusing on sampling strategies with varying levels of diversity: ancestral sampling, epsilon sampling, and beam decoding.\n\n\nResults Tables 1, 2, and 3 respectively show the MAMBR results with pseudo-references from ancestral sampling, epsilon sampling, and beam decoding. In ancestral and epsilon sampling, the best and moderately diversified sampling strategies (as shown in Figure 2), we observe performance improvement as the number of models increases. On the other hand, in the lowest diversity method, beam decoding, performance improvement is limited. These results suggest that MAMBR can improve performance by enhancing the diversity of evaluation metrics, although the diversity of the sampling strategy itself remains important. (See Appendix F for further details.)\n\n\n# 6 CONCLUSION\n\n\nThis work provides a unified theoretical interpretation of Minimum Bayes Risk (MBR) decoding through the lens of bias-diversity decomposition. By decomposing the errors in quality estimation in MBR decoding into bias and diversity, we highlight the trade-off between improving these two factors, with an emphasis on the benefits of increasing diversity. Our theoretical insights align with previous empirical results, and we further investigate aspects not covered by these empirical findings through the introduction of the pseudo-bias metric and the development of the Metric-augmented MBR (MAMBR) approach. Experimental results across multiple tasks demonstrate the validity of our theoretical findings and the effectiveness of our approach in improving text generation quality. These findings bridge the gap between empirical observations and theoretical understanding of MBR decoding, offering new insights for optimizing text generation. (See Appendix H for the limitations of our work.)\n\n\n# REFERENCES\n\n- Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In Proceedings of the IEEE International Conference on Computer Vision, pp. 8948-8957, 2019.\n\n- Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ondrej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussa, Cristina Espana-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. Findings of the 2021 conference on machine translation (WMT21). In Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi, Andre Martins, Makoto Morishita, and Christof Monz (eds.), Proceedings of the Sixth Conference on Machine Translation, pp. 1-88, Online, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.wmt-1.1.\n\n- Chantal Amrhein and Rico Senrich. Identifying weaknesses in machine translation metrics through minimum Bayes risk decoding: A case study for COMET. In Yulan He, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang (eds.), Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1125-1141, Online only, November 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.aacl-main.83.\n\n- Loic Barrault, Ondrej Bojar, Marta R. Costa-juss\u00e0, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Santanu Pal, Matt Post, and Marcos Zampieri. Findings of the 2019 conference on machine translation (WMT19). In Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Andr\u00e9 Martins, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9veol, Mariana Neves, Matt Post, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pp. 1-61, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5301. URL https://aclanthology.org/W19-5301.\n\n- Loic Barrault, Magdalena Biesialska, Ondrej Bojar, Marta R. Costa-jussa, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubesic, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. Findings of the 2020 conference on machine translation (WMT20). In Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Yvette Graham, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes,\n\n- Philipp Koehn, Andr\u00e9 Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, and Matteo Negri (eds.), Proceedings of the Fifth Conference on Machine Translation, pp. 1-55, Online, November 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.wmt-1.1.\n\n- Amanda Bertsch, Alex Xie, Graham Neubig, and Matthew Gormley. It's MBR all the way down: Modern generation techniques through the lens of minimum Bayes risk. In Yanai Elazar, Allyson Ettinger, Nora Kassner, Sebastian Ruder, and Noah A. Smith (eds.), Proceedings of the Big Picture Workshop, pp. 108-122, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023/bigpicture-1.9. URL https://aclanthology.org/2023/bigpicture-1.9.\n\n- Ondrej Bojar, Yvette Graham, Amir Kamran, and Milo\u0161 Stanojevic. Results of the WMT16 metrics shared task. In Ondrej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Liane Guillou, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Aur\u00e9lie N\u00e9veol, Mariana Neves, Pavel Pecina, Martin Popel, Philipp Koehn, Christof Monz, Matteo Negri, Matt Post, Lucia Specia, Karin Verspoor, J\u00f6rg Tiedemann, and Marco Turchi (eds.), Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pp. 199-231, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2302. URL https://aclanthology.org/W16-2302.\n\n- Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. Findings of the 2017 conference on machine translation (WMT17). In Ondrej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, and Julia Kreutzer (eds.), Proceedings of the Second Conference on Machine Translation, pp. 169-214, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4717. URL https://aclanthology.org/W17-4717.\n\n- Ondrej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. Findings of the 2018 conference on machine translation (WMT18). In Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9vel, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pp. 272-303, Belgium, Brussels, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6401. URL https://aclanthology.org/W18-6401.\n\n- Hamed Bonab and Fazli Can. Less is more: A comprehensive framework for the number of components of ensemble classifiers. IEEE Transactions on Neural Networks and Learning Systems, 30 (9):2735-2745, 2019. doi: 10.1109/TNNLS.2018.2886341.\n\n- Hamed R. Bonab and Fazli Can. A theoretical framework on the ideal number of classifiers for online ensembles in data streams. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM '16, pp. 2053-2056, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450340731. doi: 10.1145/2983323.2983907. URL https://doi.org/10.1145/2983323.2983907.\n\n- Sebastian Borgeaud and Guy Emerson. Leveraging sentence similarity in natural language generation: Improving beam search using range voting. In Alexandra Birch, Andrew Finch, Hiroaki Hayashi, Kenneth Heafield, Marcin Junczys-Dowmunt, Ioannis Konstas, Xian Li, Graham Neubig, and Yusuke Oda (eds.), Proceedings of the Fourth Workshop on Neural Generation and Translation, pp. 97-109, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.ngt-1.11. URL https://aclanthology.org/2020.ngt-1.11.\n\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\n\n- Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfbcb4967418bf8ac142f64a-Paper.pdf.\n\n- Julius Cheng and Andreas Vlachos. Faster minimum Bayes risk decoding with confidence-based pruning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 12473-12480, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.767. URL https://aclanthology.org/2023.emnlp-main.767.\n\n- Julius Cheng and Andreas Vlachos. Measuring uncertainty in neural machine translation with similarity-sensitive entropy. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2115-2128, St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-long.129.\n\n- David M Corey, William P Dunlap, and Michael J Burke. Averaging correlations: Expected values and bias in combined pearson rs and fisher's z transformations. The Journal of general psychology, 125(3):245-261, 1998.\n\n- Hiroyuki Deguchi, Yusuke Sakai, Hidetaka Kamigaito, and Taro Watanabe. mbrs: A library for minimum bayes risk decoding, 2024a. URL https://arxiv.org/abs/2408.04167.\n\n- Hiroyuki Deguchi, Yusuke Sakai, Hidetakam Kamigaito, Taro Watanabe, Hideki Tanaka, and Masao Utiyama. Centroid-based efficient minimum Bayes risk decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 11009\u201311018, Bangkok, Thailand and virtual meeting, August 2024b. Association for Computational Linguistics. URL https://aclanthology.org/2024-findings-acl.654.\n\n- John DeNero, David Chiang, and Kevin Knight. Fast consensus decoding over translation forests. In Keh-Yih Su, Jian Su, Janyce Wiebe, and Haizhou Li (eds.), Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp. 567-575, Suntec, Singapore, August 2009. Association for Computational Linguistics. URL https://aclanthology.org/P09-1064.\n\n- Nan Duan, Mu Li, Ming Zhou, and Lei Cui. Improving phrase extraction via MBR phrase scoring and pruning. In Proceedings of Machine Translation Summit XIII: Papers, Xi-amen, China, September 19-23 2011. URL https://aclanthology.org/2011.mtsummit-papers.20.\n\n- Bryan Eikema and Wilker Aziz. Is MAP decoding all you need? the inadequacy of the mode in neural machine translation. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 4506-4520, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.398. URL https://aclanthology.org/2020.coling-main.398.\n\n- Bryan Eikema and Wilker Aziz. Sampling-based approximations to minimum Bayes risk decoding for neural machine translation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 10978-10993, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.754. URL https://aclanthology.org/2022.emnlp-main.754.\n\n- Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 889-898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://aclanthology.org/P18-1082.\n\n- Patrick Fernandes, Antonio Farinhas, Ricardo Rei, Jos\u00e9 G. C. de Souza, Perez Ogayo, Graham Neubig, and Andre Martins. Quality-aware decoding for neural machine translation. In Marine Carpuat, Marie-Catherine de Marneff\u00e9, and Ivan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1396-1412, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.100. URL https://aclanthology.org/2022.naacl-main.100.\n\n- Markus Freitag, David Grangier, Qijun Tan, and Bowen Liang. High Quality Rather than High Model Probability: Minimum Bayes Risk Decoding with Neural Metrics. Transactions of the Association for Computational Linguistics, 10:811-825, 07 2022a. ISSN 2307-387X. doi: 10.1162/tacl_a_00491. URL https://doi.org/10.1162/tacl_a_00491.\n\n- Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and Andr\u00e9 F. T. Martins. Results of WMT22 metrics shared task: Stop using BLEU \u2013 neural metrics are better and more robust. In Philipp Koehn, Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, Andre Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, Matteo Negri, Aur\u00e9lie N\u00e9veol, Mariana Neves, Martin Popel, Marco Turchi, and Marcos Zampieri (eds.), Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 46\u201368, Abu Dhabi, United Arab Emirates (Hybrid), December 2022b. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.2.\n\n- Markus Freitag, Behrooz Ghorbani, and Patrick Fernandes. Epsilon sampling rocks: Investigating sampling strategies for minimum Bayes risk decoding for machine translation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 9198-9209, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023-findings-emnlp.617. URL https://aclanthology.org/2023-findings-emnlp.617.\n\n- Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz (eds.), Proceedings of the Eighth Conference on Machine Translation, pp. 578-628, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.wmt-1.51. URL https://aclanthology.org/2023.wmt-1.51.\n\n- Stuart Geman, Elie Bienenstock, and Ren\u00e9 Doursat. Neural networks and the bias/variance dilemma. Neural computation, 4(1):1-58, 1992.\n\n- Kevin Gimpel, Dhruv Batra, Chris Dyer, and Gregory Shakhnarovich. A systematic exploration of diversity in machine translation. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard (eds.), Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1100-1111, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1111.\n\n- Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization. In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini, and Fei Liu (eds.), Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 70\u201379, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409.\n\n- Taisiya Glushkova, Chrysoula Zerva, and Andr\u00e9 F. T. Martins. BLEU meets COMET: Combining lexical and neural metrics towards robust machine translation evaluation. In Mary Nurmiinen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Sergi Alvarez Vidal, Nora Aranberri, Mara Nunziatini, Carla Parra Escartin, Mikel Forcada, Maja Popovic, Carolina Scarton, and Helena Moniz (eds.), Proceedings of the 24th Annual Conference of the European Association for Machine\n\n- Translation, pp. 47-58, Tampere, Finland, June 2023. European Association for Machine Translation. URL https://aclanthology.org/2023.eamt-1.6.\n\n- Vaibhava Goel and William J Byrne. Minimum bayes-risk automatic speech recognition. Computer Speech & Language, 14(2):115-135, 2000. ISSN 0885-2308. doi: https://doi.org/10.1006/csla.2000.0138. URL https://www.sciencedirect.com/science/article/pii/S0885230800901384.\n\n- Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. Continuous measurement scales in human evaluation of machine translation. In Antonio Pareja-Lora, Maria Liakata, and Stefanie Dipper (eds.), Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pp. 33-41, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL https://aclanthology.org/W13-2305.\n\n- David Heineman, Yao Dou, and Wei Xu. Improving minimum bayes risk decoding with multiprompt, 2024. URL https://arxiv.org/abs/2407.15343.\n\n- John Hewitt, Christopher Manning, and Percy Liang. Truncation sampling as language model desmoothing. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 3414-3427, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.249. URL https://aclanthology.org/2022-findings-emnlp.249.\n\n- Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rygGQyrFvH.\n\n- Yuu Jinnai, Ukyo Honda, Tetsuro Morimura, and Peinan Zhang. Generating diverse and high-quality texts by minimum Bayes risk decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 8494-8525, Bangkok, Thailand and virtual meeting, August 2024a. Association for Computational Linguistics. URL https://aclanthology.org/2024-findings-acl.503.\n\n- Yuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, and Kenshi Abe. Model-based minimum Bayes risk decoding for text generation. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 22326-22347. PMLR, 21-27 Jul 2024b. URL https://proceedings.mlr.press/v235/jinnai24a.html.\n\n- Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3128-3137, 2015.\n\n- Teun Kloek and Herman K Van Dijk. Bayesian estimates of equation system parameters: an application of integration by monte carlo. *Econometrica: Journal of the Econometric Society*, pp. 1-19, 1978.\n\n- Anders Krogh and Jesper Vedelsby. Neural network ensembles, cross validation, and active learning. In G. Tesauro, D. Touretzky, and T. Leen (eds.), Advances in Neural Information Processing Systems, volume 7. MIT Press, 1994. URL https://proceedings.neurips.cc/paper_files/paper/1994/file/b8c37e33defde51cf91e1e03e51657da-Paper.pdf.\n\n- Shankar Kumar and William Byrne. Minimum Bayes-risk word alignments of bilingual texts. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pp. 140-147. Association for Computational Linguistics, July 2002. doi: 10.3115/1118693.1118712. URL https://aclanthology.org/W02-1019.\n\n- Shankar Kumar and William Byrne. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pp. 169-176, Boston, Massachusetts, USA, May 2 - May 7 2004. Association for Computational Linguistics. URL https://aclanthology.org/N04-1022.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638579.134567", "msecs": "134.0", "relativeCreated": "175965.06452560425", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Benign Overfitting in Adversarially Robust Linear Classification\n\n# Abstract\n``Benign overfitting'', where classifiers memorize noisy training data yet still achieve a good generalization performance, has drawn great attention in the machine learning community. To explain this surprising phenomenon, a series of works have provided theoretical justification in over-parameterized linear regression, classification, and kernel methods. However, it is not clear if benign overfitting still occurs in the presence of adversarial examples, i.e., examples with tiny and intentional perturbations to fool the classifiers. In this paper, we show that benign overfitting indeed occurs in adversarial training, a principled approach to defend against adversarial examples. In detail, we prove the risk bounds of the adversarially trained linear classifier on the mixture of sub-Gaussian data under $\\ell_p$ adversarial perturbations. Our result suggests that under moderate perturbations, adversarially trained linear classifiers can achieve the near-optimal standard and adversarial risks, despite overfitting the noisy training data. Numerical experiments validate our theoretical findings.\n\n\n# 1 INTRODUCTION\n\n\nModern machine learning methods such as deep learning have made many breakthroughs in a variety of application domains, including image classification (He et al., 2016; Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012) and etc. These models are typically over-parameterized: the number of model parameters far exceeds the size of the training samples. One mystery is that, these over-parameterized models can memorize noisy training data and yet still achieve quite good generalization performances on the test data (Zhang et al., 2017). Many efforts have been made to explain this striking phenomenon, which against what the classical notion of overfitting might suggest. A line of research works (Soudry et al., 2018; Ji & Telgarsky, 2019b; Nacson et al., 2019; Gunasekar et al., 2018b;a) shows that there exists the so-called implicit bias (Neyshabur, 2017): the training algorithms tend to converge to certain kinds of solutions even with no explicit regularization. Specifically, Soudry et al. (2018); Ji & Telgarsky (2019b); Nacson et al. (2019) demonstrate that gradient descent trained linear classifiers on logistic or exponential loss with no regularization asymptotically converge to the maximum  $L_{2}$  margin classifier. Recent works (Bartlett et al., 2020; Chatterji & Long, 2020; Cao et al., 2021; Wang & Thrampoulidis, 2021; Tsigler & Bartlett, 2020) further shows that over-parameterized and implicitly regularized interpolators can indeed achieve small test error, and formulate this phenomenon as \"benign overfitting\". More concretely, suppose the classification model  $f$  is parameterized by  $\\theta \\in \\Theta$  and the loss is denoted as  $\\ell(\\cdot)$ . The population risk is defined as\n\n\n$$\n\n$$\n\\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} [ f _ {\\pmb {\\theta}} (\\mathbf {x}) \\neq y ],\n$$\n\n$$\n\n\nwhere data pair  $(\\mathbf{x},y)$  is generated from certain data generation model. Chatterji & Long (2020) shows that with sufficient over-parameterization, gradient descent trained maximum  $L_{2}$  margin classifier can achieve nearly optimal population risk on noisy data for data generated from a sub-Gaussian mixture model. This suggests that the overfitting can be \"benign\" in the overparameterized setting.\n\n\nBesides these studies on the benign overfitting phenomenon, another well-known feature of modern machine learning methods is that they are vulnerable to adversarial examples. Recent studies (Szegedy et al., 2013; Goodfellow et al., 2015) show that modern machine learning systems are brittle: slight input perturbation that is imperceptible to human eyes could mislead a well-trained classifier into wrong classification result. These malicious inputs are also known as the adversarial\n\n\nexamples (Szegedy et al., 2013; Goodfellow et al., 2015). Adversarial examples raise severe trustworthy issues and security concerns on the current machine learning systems especially in security-critical applications. Various methods (Kurakin et al., 2016; Madry et al., 2018; Zhang et al., 2019; Wang et al., 2019; 2020) have been proposed to defend against the threats posed by adversarial examples. One of the notable approaches is adversarial training (Madry et al., 2018). Specifically, adversarial training solves the following min-max optimization problem,\n\n\n$$\n\n$$\n\\min _ {\\pmb {\\theta} \\in \\Theta} \\frac {1}{n} \\sum_ {i = 1} ^ {n} \\max _ {\\mathbf {x} _ {i} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x} _ {i})} \\ell (f _ {\\pmb {\\theta}} (\\mathbf {x} _ {i} ^ {\\prime}), y _ {i}),\n$$\n\n$$\n\n\nwhere  $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$  is the training set and  $\\mathcal{B}_{\\epsilon}^p(\\mathbf{x}_i) = \\{\\mathbf{x} : \\| \\mathbf{x} - \\mathbf{x}_i \\|_p \\leq \\epsilon\\}$  denotes the  $\\epsilon$ -ball around  $\\mathbf{x}_i$  in  $\\ell_p$  norm ( $p \\geq 1$ ). Many empirical or theoretical studies have been conducted trying to analyze or further improve adversarial training robustness (Zhang et al., 2019; Rice et al., 2020; Wang et al., 2020; Carmon et al., 2019; Wang et al., 2019; Raghunathan et al., 2020). A recent work (Sanyal et al., 2021) also pointed out that normally trained interpolators with the presence of label noise are unlikely to be adversarily robust, while adversially robust classifiers cannot overfit noisy labels under certain conditions. However, it is still not clear whether the benign overfitting phenomenon occurs for extremely over-parameterized models in the presence of adversarial examples.\n\n\nIn this paper, we show that benign overfitting indeed occurs in adversarial training. In order to properly characterize the benign overfitting phenomenon on adversarial training, we also define the population adversarial risk, which is the counterpart for population risk in standard training scenario:\n\n\n$$\n\n$$\n\\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\exists \\mathbf {x} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x}) s. t., f _ {\\boldsymbol {\\theta}} (\\mathbf {x} ^ {\\prime}) \\neq y \\right].\n$$\n\n$$\n\n\nThe adversarial risk measures the misclassification rate of the target classifier under the presence of  $\\ell_p$ -norm adversarial perturbations. It is easy to observe that the adversarial risk is always larger than standard risk as it requires the classifier to correctly classify the data examples within the entire local  $\\ell_p$  norm ball.\n\n\nWe summarize our contributions of this paper in the following\n\n- - We show that the benign overfitting phenomenon can occur in adversarially robust linear classifiers with sufficient over-parameterization. Specifically, under moderate  $\\ell_p$  norm perturbations, adversarially trained linear classifiers can achieve the near-optimal standard and adversarial risks, in spite of overfitting the noisy training data.\n\n- - When the perturbation strength  $\\epsilon$  is set to be 0, our adversarial risk bound reduces to the standard one. The resulting standard risk bound extends Chatterji & Long (2020)'s risk bound to further characterize the behavior of the linear classifier trained by  $t$ -step gradient descent.\n\n- - We show that depending on the value of  $p$  (perturbation norm), the adversarial risk bound can be different. The higher value of  $p$  (typically for  $p \\geq 2$  case) actually leads to a larger gap between the adversarial risk and the standard risk with the same  $\\epsilon$ .\n\n\nNotation. We use lower case letters to denote scalars and lower case bold face letters to denote vectors. For a vector  $\\mathbf{x} \\in \\mathbb{R}^d$ , we denote its  $\\ell_p$  norm ( $p \\geq 1$ ) of  $\\mathbf{x}$  by  $\\| \\mathbf{x} \\|_p = \\left( \\sum_{i=1}^d |x_i|^p \\right)^{1/p}$ , the  $\\ell_\\infty$  norm of  $\\mathbf{x}$  by  $\\| \\mathbf{x} \\|_\\infty = \\max_{i=1}^d |x_i|$ . We denote  $\\mathbf{x}^{\\circ p}$  as the element-wise  $p$ -power of  $\\mathbf{x}$ . For  $p \\geq 1$ , we denote  $\\mathcal{B}_r^p(\\mathbf{x})$  as the  $\\ell_p$  norm ball of radius  $r$  centered at  $\\mathbf{x}$ . Given two sequences  $\\{a_n\\}$  and  $\\{b_n\\}$ , we write  $a_n = O(b_n)$  if there exists a constant  $0 < C < +\\infty$  such that  $a_n \\leq C b_n$ . We denote  $a_n = \\Omega(b_n)$  if  $b_n = O(a_n)$ . We denote  $a_n = \\Theta(b_n)$  if  $a_n = O(b_n)$  and  $a_n = \\Omega(b_n)$ .\n\n\n# 2 RELATED WORK\n\n\nThere exists a large body of works on adversarial training, implicit bias and benign overfitting. In this section, we review the most relevant works with ours.\n\n\nAdversarial Training. Adversarial training (Madry et al., 2018) and its variants (Zhang et al., 2019; Wang et al., 2019; 2020) are currently the most effective type of approaches to empirically defend against adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2015). And many attempts have been made to understand its empirical success. Charles et al. (2019); Li et al. (2020) showed that the adversarially trained linear classifier directionally converges to the maximum margin classifier. Gao et al. (2019); Zhang et al. (2020b) showed that adversarial training with neural networks can achieve low robust training loss. Yet these conclusions cannot explain the test (population) performances. Another line of research focuses on the generalization performance of adversarial training\n\n\nand the number of training samples. Schmidt et al. (2018) showed that adversarial models require more data than standard models to achieve certain test accuracy. Chen et al. (2020) showed that more data may actually increase the gap between the generalization error of adversarially-trained models and standard models. Yin et al. (2019); Cullina et al. (2018) studied the adversarial Rademacher complexity and VC-dimensions. Some other works focus on the trade-off between robustness and natural accuracy (Zhang et al., 2019; Tsipras et al., 2019; Wu et al., 2020; Raghunathan et al., 2020; Yang et al., 2020; Dobriban et al., 2020; Javanmard & Soltanolkotabi, 2020), adversarial model complexity lower bound (Allen-Zhu & Li, 2020), as well as the provable robustness upper bound (Fawzi et al., 2018; Zhang et al., 2020a).\n\n\nRecently, some works also focus on studying the learning of robust halfspaces and linear models. Montasser et al. (2020) studied the conditions on the adversarial perturbation sets under which half-spaces are robustly learnable in the presence of random label noise. Diakonikolas et al. (2020) studied the computational complexity of adversarially robust halfspaces under  $\\ell_p$  norm perturbations. Zou et al. (2021) showed that adversarially trained halfspaces are provably robust with low robust classification error in the presence of noise. Dan et al. (2020) proposed an adversarial signal to noise ratio and studied the excess risk lower/upper bounds for learning Gaussian mixture models. Taheri et al. (2020); Javanmard & Soltanolkotabi (2020) studied adversarial learning of linear models on Gaussian mixture data where the data dimension and the number of training data points have a fixed ratio.\n\n\nImplicit Bias. Several recent works studied the implicit bias of various training algorithms in overparameterized models. Soudry et al. (2018) studied the implicit bias of gradient descent trained on linearly separable data while Ji & Telgarsky (2019b) studied the non-separable case. Gunasekar et al. (2018a) studied the implicit bias of various optimization methods in linear regression and classification problems. Ji & Telgarsky (2019a) studied the implicit bias for deep linear networks and Arora et al. (2019); Gunasekar et al. (2018b) studied the implicit bias for matrix factorization. Lyu & Li (2020) studied the implicit regularization of homogeneous neural networks with exponential loss and logistic loss.\n\n\nBenign Overfitting and Double Descent. A series of recent works have studied the \"benign overfitting\" phenomenon Bartlett et al. (2020) that when training over-parameterized models, classifiers can still achieve good population risk even when overfitting the noisy training data. Bartlett et al. (2020); Tsigler & Bartlett (2020) studied the risk bounds for over-parameterized linear (ridge) regression and showed that under certain settings, the interpolating linear model with minimum parameter norm can have asymptotically optimal risk. Chatterji & Long (2020); Cao et al. (2021); Wang & Thampoulidis (2021) studied the risk bounds in linear logistic regression and linear support vector machines. Belkin et al. (2018; 2019a;b); Hastie et al. (2019); Wu & Xu (2020) further quantified the dependency curve between the population risk and the degree of over-parameterization and showed that the curve has a double-descent shape.\n\n\n# 3 PROBLEM SETTING AND PRELIMINARIES\n\n\nWe consider a sub-Gaussian mixture data generation model in our work. Specifically, the clean data  $(\\widetilde{\\mathbf{x}},\\widetilde{y})\\sim \\widetilde{\\mathcal{D}}$  is generated such that, for each data point  $(\\widetilde{\\mathbf{x}},\\widetilde{y})\\in \\mathbb{R}^{d}\\times \\{\\pm 1\\}$ , we have  $\\widetilde{y}\\sim \\mathrm{Unif}(\\{\\pm 1\\})$  and  $\\widetilde{\\mathbf{x}} = \\widetilde{y}\\pmb {\\mu} + \\pmb{\\xi}$  where  $\\pmb {\\xi}\\in \\mathbb{R}^d$  and  $\\xi_1,\\xi_2,\\ldots ,\\xi_d$  are i.i.d. zero-mean sub-Gaussian variables with sub-Gaussian norm at most 1. The actual data examples are sampled from a noisy distribution  $\\mathcal{D}$  which is close to the clean distribution  $\\widetilde{\\mathcal{D}}$ . Specifically,  $\\mathcal{D}$  can be any distribution over  $\\mathbb{R}^d\\times \\{\\pm 1\\}$  who has the same marginal distribution on  $\\mathbb{R}^d$  and the total variation distance  $d_{\\mathrm{TV}}(\\mathcal{D},\\widetilde{\\mathcal{D}})\\leq \\eta$  where  $\\eta$  denotes the noise level.\n\n\nNote that our data generation model is standard for studying the population risk of overparameterized linear classification. In fact, it is exactly the same as the one studied in Chatterji & Long (2020). In this model, following standard coupling lemma (Lindvall, 2002), there always exists a joint distribution on original data and noisy data  $(\\widetilde{\\mathbf{x}},\\widetilde{y}),(x,y))$  such that the marginal distribution for  $(\\widetilde{\\mathbf{x}},\\widetilde{y})$  is  $\\widetilde{\\mathcal{D}}$ , the marginal distribution for  $(\\mathbf{x},y)$  is  $\\mathcal{D}$ ,  $\\mathbb{P}[\\mathbf{x} = \\widetilde{\\mathbf{x}}] = 1$  and  $\\mathbb{P}[y\\neq \\widetilde{y}]\\leq \\eta$ .\n\n\nIn this paper, we study the problem of robust binary classification with training data  $\\{(\\mathbf{x}_i,y_i)\\}_{i = 1}^n$  drawn i.i.d. from the distribution  $\\mathcal{D}$ . Let's denote the \"clean\" sample index as  $\\mathcal{C} \\coloneqq \\{k:y_k = \\widetilde{y}_k\\}$  and the \"noisy\" sample index as  $\\mathcal{N} \\coloneqq \\{k:y_k \\neq \\widetilde{y}_k\\}$ . We consider the adversarially trained linear\n\n\n```\n1: input: Training data  $\\{\\mathbf{x}_i, y_i\\}_{i=1}^n$ , number of training iterations  $T$ , maximum perturbation strength  $\\epsilon$ , training step sizes  $\\alpha_t$ ;  \n2: initialize model parameter  $\\theta_0 = 0$   \n3: for  $t = 1, \\dots, T$  do  \n4: for each  $\\{\\mathbf{x}_i, y_i\\}$  do  \n5:  $\\mathbf{x}_i' = \\operatorname{argmax}_{\\mathbf{x}_i' \\in B_\\epsilon^p(\\mathbf{x}_i)} \\exp(-y_i \\boldsymbol{\\theta}_{t-1}^\\top \\mathbf{x}_i')$   \n6: end for  \n7:  $\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\alpha_t \\cdot \\nabla_\\boldsymbol{\\theta} L(\\boldsymbol{\\theta}_{t-1})$   \n8: end for\n```\n\n\nclassifier under exponential loss. In such case, the adversarial loss can be explicitly written as\n\n\n$$\n\n$$\nL (\\boldsymbol {\\theta}) = \\sum_ {i = 1} ^ {n} \\max  _ {\\mathbf {x} _ {i} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} \\left(\\mathbf {x} _ {i}\\right)} \\exp \\left(- y _ {i} \\boldsymbol {\\theta} ^ {\\top} \\mathbf {x} _ {i} ^ {\\prime}\\right). \\tag {3.1}\n$$\n\n$$\n\n\nIn gradient descent adversarial training algorithm, the adversarial loss  $L(\\theta)$  is minimized by first solving the inner maximization problem in (3.1) with respect to the current model parameter  $\\theta_{t-1}$  and then update the model parameter  $\\theta_t$  by performing gradient descent to minimize the adversarial loss in each iteration. We summarized the training procedure for gradient descent adversarial training<sup>1</sup> in Algorithm 1. Note that in the linear classifier setting, the inner maximization problem in (3.1) has the following property\n\n\n$$\n\n$$\n\\underset {\\mathbf {x} _ {i} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x} _ {i})} {\\operatorname {a r g m a x}} \\exp (- y _ {i} \\boldsymbol {\\theta} ^ {\\top} \\mathbf {x} _ {i} ^ {\\prime}) = \\underset {\\mathbf {u} _ {i} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {0})} {\\operatorname {a r g m a x}} \\exp (- y _ {i} \\boldsymbol {\\theta} ^ {\\top} (\\mathbf {x} _ {i} + \\mathbf {u} _ {i})) = \\underset {\\| \\mathbf {u} _ {i} \\| _ {p} \\leq \\epsilon} {\\operatorname {a r g m i n}} y _ {i} \\boldsymbol {\\theta} ^ {\\top} \\mathbf {u} _ {i}. \\tag {3.2}\n$$\n\n$$\n\n\nBy H\u00f6lders' inequality it is easy to observe that the optimal adversarial loss and the corresponding gradient can be written as\n\n\n$$\n\n$$\nL (\\boldsymbol {\\theta}) = \\sum_ {i = 1} ^ {n} \\exp (- y _ {i} \\boldsymbol {\\theta} ^ {\\top} \\mathbf {x} _ {i} + \\epsilon \\| \\boldsymbol {\\theta} \\| _ {q}), \\nabla_ {\\boldsymbol {\\theta}} L (\\boldsymbol {\\theta}) = - \\sum_ {i = 1} ^ {n} (y _ {i} \\mathbf {x} _ {i} - \\epsilon \\cdot \\partial \\| \\boldsymbol {\\theta} \\| _ {q}) \\exp (- y _ {i} \\boldsymbol {\\theta} ^ {\\top} \\mathbf {x} _ {i} + \\epsilon \\| \\boldsymbol {\\theta} \\| _ {q}),\n$$\n\n$$\n\n\nwhere  $1 / p + 1 / q = 1$ . Also note that in the over-parameterized settings, training examples draw from our data generation model are linearly separable with high probability (See Lemma 5.1 in Section 5). Linearly separable property ensures that the training samples have a positive margin (with high probability). Following Li et al. (2020), we also define the standard and adversarial margin as\n\n\n$$\n\n$$\n\\bar {\\gamma} := \\max  _ {\\| \\boldsymbol {\\theta} \\| _ {q} = 1} \\min  _ {i \\in [ n ]} y _ {i} \\boldsymbol {\\theta} ^ {\\top} \\mathbf {x} _ {i}, \\quad \\gamma := \\max  _ {\\| \\boldsymbol {\\theta} \\| _ {2} = 1} \\min  _ {i \\in [ n ]} \\min  _ {\\mathbf {x} _ {i} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x} _ {i})} y _ {i} \\boldsymbol {\\theta} ^ {\\top} \\mathbf {x} _ {i} ^ {\\prime}, \\tag {3.3}\n$$\n\n$$\n\n\nwhich are useful in our later analysis. We also define the unique linear classifier  $\\theta$  that achieves adversarial margin  $\\gamma$  defined above as  $\\mathbf{w}$ .\n\n\n# 4 MAIN RESULTS\n\n\nIn this section, we study both the behavior of the population risk and the population adversarial risk for adversarially trained linear classifiers.\n\n\nAssumption 4.1. The adversarial perturbation radius  $\\epsilon$  is upper bounded by a constant  $R$  and is smaller than the  $\\ell_p$  data margin  $\\bar{\\gamma}$ , i.e.,  $\\epsilon \\leq \\min\\{R, \\bar{\\gamma}\\}$ .\n\n\nThe goal of adversarial training is to obtain high-accuracy classifiers that are also robust to small input perturbations which can be ignored by human beings (e.g., small  $\\ell_{\\infty}$ -norm perturbations that are invisible to human eyes). Therefore, Assumption 4.1 is reasonable by constraining the maximum allowable perturbation magnitude.\n\n\nAssumption 4.2. The noise  $\\xi$  in the data generation model satisfies that  $\\mathbb{E}[\\|\\xi\\|_2^2] \\geq \\kappa d$  for some constant  $\\kappa$ .\n\n\nAssumption 4.2 is a common condition that has also been considered in Chatterji & Long (2020). It ensures that the summation of the variances of the data input increases in the order of  $\\Theta(d)$ . Clearly, this assumption covers the most common setting where the entries of  $\\xi$  are i.i.d. and have a variance larger than or equal to  $\\kappa$ .\n\n\nAssumption 4.3. The gradient descent starts at  $\\mathbf{0}$ , and the step sizes are set as  $\\alpha_0 = 1 / (Gdn)$ ,  $\\alpha_{t} = \\alpha \\leq 1 / (GdnM)$  for  $M = \\max \\{[2d + \\epsilon (q - 1)d^{\\frac{3q - 2}{2q - 2}} / \\gamma ]\\exp (-\\gamma^{2} / (Gd) + \\epsilon /G),1\\}$  and a constant  $G$ .\n\n\nAssumption 4.3 summarizes our assumptions about the gradient descent algorithm on the adversarial loss. The learning rate conditions here are to ensure the convergence of adversarial training, and is inspired by Li et al. (2020).\n\n\nWe first present our theorem for standard risk of adversarial training method (Algorithm 1).\n\n\nTheorem 4.4 (Standard Risk of Adversarial Training). For any  $p \\in [1, +\\infty)$ , suppose that Assumptions 4.1, 4.2 and 4.3 hold with  $\\kappa \\in (0,1]$  and large enough constants  $R$  and  $G$ . Moreover, for any  $\\delta \\in (0,1)$ , suppose the number of training samples  $n \\geq C \\log(1/\\delta)$ , the dimension  $d \\geq C \\cdot \\max \\{n \\| \\pmb{\\mu} \\|_2^2, n^2 \\log(n/\\delta)\\}$ , the noise level  $\\eta < 1/C$ , and  $\\| \\pmb{\\mu} \\|_2^2 \\geq C \\max \\{\\log(n/\\delta), \\epsilon \\| \\pmb{\\mu} \\|_q\\}$  for a large enough constant  $C$ . Then with probability at least  $1 - \\delta$ , adversariably trained linear classifier  $f_{\\theta_t}$  for sufficiently large  $t$  under  $\\ell_p$ -norm  $\\epsilon$ -perturbation satisfies the following standard risk\n\n\n$$\n\n$$\n\\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} [ f _ {\\boldsymbol {\\theta} _ {t}} (\\mathbf {x}) \\neq y ] \\leq \\eta + \\exp \\left(- C ^ {\\prime} \\left(\\frac {\\left(\\| \\boldsymbol {\\mu} \\| _ {2} ^ {2} - 4 \\epsilon \\| \\boldsymbol {\\mu} \\| _ {q}\\right)}{\\left(C ^ {\\prime \\prime} + \\epsilon\\right) \\sqrt {d}} - \\frac {C ^ {\\prime \\prime \\prime} \\| \\boldsymbol {\\mu} \\| _ {2} \\log n}{\\log t}\\right) ^ {2}\\right),\n$$\n\n$$\n\n\nwhere  $C', C'', C''', 0$  are absolute constants,  $1/p + 1/q = 1$ .\n\n\nRemark 4.5. Theorem 4.4 presents the standard risk of adversarial training under  $\\ell_p$  norm perturbations. Note that adversarially trained linear classifier enjoys a bounded population risk which decreases as the number of training iterations  $t$  increases. Specifically, when  $t\\to \\infty$ , we have\n\n\n$$\n\n$$\n\\lim  _ {t \\rightarrow \\infty} \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} [ f _ {\\boldsymbol {\\theta} _ {t}} (\\mathbf {x}) \\neq y ] \\leq \\eta + \\exp \\left(- C ^ {\\prime} \\left(\\frac {\\left(\\| \\boldsymbol {\\mu} \\| _ {2} ^ {2} - 4 \\epsilon \\| \\boldsymbol {\\mu} \\| _ {q}\\right)}{\\left(C ^ {\\prime \\prime} + \\epsilon\\right) \\sqrt {d}}\\right) ^ {2}\\right). \\tag {4.1}\n$$\n\n$$\n\n\nRemark 4.6. For (4.1), consider the case when the sample size  $n$  is fixed but dimension  $d$  and  $\\| \\boldsymbol{\\mu} \\|_2$  are growing, we discuss the conditions to reach minimum standard risk of noise level  $\\eta$ . Note that when  $1 \\leq p \\leq 2$  we have  $q \\geq 2$  and  $\\| \\boldsymbol{\\mu} \\|_q \\leq \\| \\boldsymbol{\\mu} \\|_2$ . In this case, if  $\\| \\boldsymbol{\\mu} \\|_2 = \\Omega(d^{1/4})$ , the standard risk will come close to the noise level  $\\eta$  when  $d$  is sufficiently large. When  $p > 2$  and therefore  $q < 2$ , we have  $\\| \\boldsymbol{\\mu} \\|_q \\leq d^{1/q-1/2} \\| \\boldsymbol{\\mu} \\|_2$ . In this case, if  $\\| \\boldsymbol{\\mu} \\|_2 = \\Omega(d^{1/4})$  and  $\\epsilon = O(\\| \\boldsymbol{\\mu} \\|_2 / d^{1/q-1/2})$ , the standard risk will come close to the noise level  $\\eta$  with sufficiently large  $d$ . Note that our theorem condition also requires that  $\\| \\boldsymbol{\\mu} \\|_2 = O(\\sqrt{d})$ . Therefore, in order to reach the standard risk of  $\\eta$ , we need  $\\| \\boldsymbol{\\mu} \\|_2 = \\Theta(d^r)$  for some  $r \\in (1/4, 1/2]$ .\n\n\nRemark 4.7. Choosing  $\\epsilon = 0$  will reduce to the standard training case. Specifically, if we set  $\\epsilon = 0$  in (4.1), it reduces to the same conclusion as Theorem 3.1 in Chatterji & Long (2020). However, our result is more general, as it covers the setting of adversarial training and gives risk bounds for the linear model obtained with a finite number of gradient descent iterations.\n\n\nTheorem 4.8 (Adversarial Risk of Adversarial Training). For any  $\\delta \\in (0,1)$ , under the same conditions as in Theorem 4.4, with probability at least  $1 - \\delta$ , the adversarially trained linear classifier  $f_{\\theta_t}$  for sufficiently large  $t$  under  $\\ell_p$ -norm  $\\epsilon$ -perturbation satisfies the following adversarial risk if  $1 \\leq p \\leq 2$\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\exists \\mathbf {x} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x}) s. t., f _ {\\boldsymbol {\\theta}} \\left(\\mathbf {x} ^ {\\prime}\\right) \\neq y \\right] \\\\ \\leq \\eta + \\exp \\left(- C ^ {\\prime} \\bigg (\\frac {\\left(\\| \\boldsymbol {\\mu} \\| _ {2} ^ {2} - 4 \\epsilon \\| \\boldsymbol {\\mu} \\| _ {q}\\right)}{\\left(C ^ {\\prime \\prime} + \\epsilon\\right) \\sqrt {d}} - \\frac {C ^ {\\prime \\prime \\prime} \\| \\boldsymbol {\\mu} \\| _ {2} \\log n}{\\log t} - \\epsilon\\right) ^ {2}\\left. \\right), \\\\ \\end{array}\n$$\n\n$$\n\n\nand if  $p > 2$\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\exists \\mathbf {x} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x}) s. t., f _ {\\boldsymbol {\\theta}} (\\mathbf {x} ^ {\\prime}) \\neq y \\right] \\\\ \\leq \\eta + \\exp \\left(- C ^ {\\prime} \\left(\\frac {\\left(\\| \\boldsymbol {\\mu} \\| _ {2} ^ {2} - 4 \\epsilon \\| \\boldsymbol {\\mu} \\| _ {q}\\right)}{\\left(C ^ {\\prime \\prime} + \\epsilon\\right) \\sqrt {d}} - \\frac {C ^ {\\prime \\prime \\prime} \\| \\boldsymbol {\\mu} \\| _ {2} \\log n}{\\log t} - \\epsilon d ^ {\\frac {1}{q} - \\frac {1}{2}}\\right) ^ {2}\\right), \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere  $C', C'', C'' > 0$  are absolute constants,  $1/p + 1/q = 1$ .\n\n\nRemark 4.9. Theorem 4.8 shows the adversarial risk of adversarial training under  $\\ell_p$  norm perturbations. The major difference from the standard risk (Theorem 4.4) lies in the additional  $\\epsilon$  or\n\n\n$\\epsilon d^{1/q-1/2}$  term in the exponential function. This aligns with common sense that adversarial risk should always be larger than the standard risk. This also suggests that for larger  $p$ -norm  $(p > 2)$  perturbation, the same magnitude of perturbation would lead to a larger gap between the adversarial risk and the standard risk. In terms of the perturbation strength, we can also observe that with a larger  $\\epsilon$ , adversarially trained classifiers obtain worse adversarial risk. This has been verified by many empirical observations of adversarial training (Madry et al., 2018; Zhang et al., 2019).\n\n\nRemark 4.10. Note that when  $t \\to \\infty$ , if  $1 \\leq p \\leq 2$ , we have the following adversarial risk bound:\n\n\n$$\n\n$$\n\\lim _ {t \\to \\infty} \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\exists \\mathbf {x} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x}), f _ {\\boldsymbol {\\theta}} (\\mathbf {x} ^ {\\prime}) \\neq y \\right] \\leq \\eta + \\exp \\left(- C ^ {\\prime} \\left(\\frac {\\left(\\| \\boldsymbol {\\mu} \\| _ {2} ^ {2} - 4 \\epsilon \\| \\boldsymbol {\\mu} \\| _ {q}\\right)}{\\left(C ^ {\\prime \\prime} + \\epsilon\\right) \\sqrt {d}} - \\epsilon\\right) ^ {2}\\right),\n$$\n\n$$\n\n\nand if  $p > 2$ , we have\n\n\n$$\n\n$$\n\\lim  _ {t \\rightarrow \\infty} \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\exists \\mathbf {x} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x}), f _ {\\boldsymbol {\\theta}} (\\mathbf {x} ^ {\\prime}) \\neq y \\right] \\leq \\eta + \\exp \\left(- C ^ {\\prime} \\left(\\frac {\\left(\\| \\boldsymbol {\\mu} \\| _ {2} ^ {2} - 4 \\epsilon \\| \\boldsymbol {\\mu} \\| _ {q}\\right)}{\\left(C ^ {\\prime \\prime} + \\epsilon\\right) \\sqrt {d}} - \\epsilon d ^ {\\frac {1}{q} - \\frac {1}{2}}\\right) ^ {2}\\right).\n$$\n\n$$\n\n\nSimilar to the standard risk case (Remark 4.6), when  $1 \\leq p \\leq 2$ , if  $\\| \\boldsymbol{\\mu} \\|_2 = \\Theta(d^r)$  for some  $r \\in (1/4, 1/2]$ , the adversarial risk will also come close to the noise level  $\\eta$  with sufficiently large  $d$ . When  $p > 2$ , if we have  $\\| \\boldsymbol{\\mu} \\|_2 = \\Theta(d^r)$  for some  $r \\in (1/4, 1/2]$  and  $\\epsilon = O(\\| \\boldsymbol{\\mu} \\|_2 / d^{1/q})$ , the adversarial risk will be close to  $\\eta$  with sufficiently large  $d$ . Note that compared to the standard risk, this requirement on  $\\epsilon$  is slightly stronger.\n\n\nRemark 4.11. Note that our results imply a striking fact that unlike those observed in previous studies (e.g., Rice et al. (2020) showed that overfitting leads to worse empirical robustness on real image distributions), overfitting in adversarial training can be benign for certain distributions. Specifically, Remark 4.10 shows that for linear models with sub-Gaussian mixture data, the overfitting effect is indeed benign. This is later empirically verified in the experiments for both linear and neural network models.\n\n\n# 5 PROOF OUTLINE OF THE MAIN RESULTS\n\n\nIn this section, we present the proofs of our main theorems, which consists of three main steps.\n\n\nStatistical properties of the training data points. We first list some basic properties of the training data points based on our data model defined in Section 3.\n\n\nLemma 5.1 (Lemma 4.7 in Chatterji & Long (2020)). Let  $\\mathbf{z}_k = y_k\\mathbf{x}_k$ . There exist absolute constants  $R, \\kappa$  and  $G$  and  $C$ , such that if the assumptions in Theorem 4.4 hold, then with probability at least  $1 - \\delta$ ,\n\n\n$$\n\n$$\n\\frac {d}{c _ {0}} \\leq \\| \\mathbf {z} _ {k} \\| _ {2} ^ {2} \\leq c _ {0} d \\text {f o r a l l} k \\in [ n ], \\tag {5.1}\n$$\n\n$$\n\n\n$$\n\n$$\n\\left| \\mathbf {z} _ {i} ^ {\\top} \\mathbf {z} _ {j} \\right| \\leq c _ {0} \\left(\\| \\boldsymbol {\\mu} \\| _ {2} ^ {2} + \\sqrt {d \\log (n / \\delta)}\\right) \\text {f o r a l l} i \\neq j, \\tag {5.2}\n$$\n\n$$\n\n\n$$\n\n$$\n\\left| \\boldsymbol {\\mu} ^ {\\top} \\mathbf {z} _ {k} - \\| \\boldsymbol {\\mu} \\| _ {2} ^ {2} \\right| \\leq \\| \\boldsymbol {\\mu} \\| _ {2} ^ {2} / 2 \\text {f o r a l l} k \\in \\mathcal {C}, \\tag {5.3}\n$$\n\n$$\n\n\n$$\n\n$$\n\\left| \\boldsymbol {\\mu} ^ {\\top} \\mathbf {z} _ {k} - \\left(- \\| \\boldsymbol {\\mu} \\| _ {2} ^ {2}\\right) \\right| \\leq \\| \\boldsymbol {\\mu} \\| _ {2} ^ {2} / 2 \\text {f o r a l l} k \\in \\mathcal {N}, \\tag {5.4}\n$$\n\n$$\n\n\nthe number of noisy samples  $|\\mathcal{N}| \\leq (\\eta + c_1)n$ , and all training samples are linearly separable, where  $c_0 > 1$  is an absolute constant.\n\n\nLemma 5.1 directly follows Lemma 4.7 in Chatterji & Long (2020). It provides direct high probability bounds for  $\\| \\mathbf{z}_k\\| _2$  and  $\\pmb{\\mu}^{\\top}\\mathbf{z}_{k}$  and also suggests that  $\\mathbf{z}_k$  vectors are nearly pairwise orthogonal in over-parameterized settings. It also guarantees that training examples are linearly separable with high probability.\n\n\nLandscape properties of the training objective function. Given the properties of the training data points, we proceed to establish landscape properties of the objective function  $L(\\theta_1)$ . The following lemma bound the loss for the adversarially trained classifier in step 1.\n\n\nLemma 5.2. [Theorem 3.4 in Li et al. (2020)] Under the same conditions as in Theorem 4.4, with probability at least  $1 - \\delta$ , we have  $L(\\theta_1) \\leq 2n$ , and\n\n\n$$\n\n$$\nL \\left(\\boldsymbol {\\theta} _ {t + 1}\\right) \\leq L \\left(\\boldsymbol {\\theta} _ {t}\\right), \\tag {5.5}\n$$\n\n$$\n\n\n$$\n\n$$\n1 - \\frac {\\boldsymbol {\\theta} _ {t} ^ {\\top} \\mathbf {w}}{\\| \\boldsymbol {\\theta} _ {t} \\| _ {2}} \\leq \\frac {c _ {3} \\log n}{\\log t} \\tag {5.6}\n$$\n\n$$\n\n\nfor all  $t > 0$ , where  $c_{3}$  is an absolute constant.\n\n\nBy Lemma 5.2, one can easily observe that the adversarial training loss is bounded by  $2n$  along the entire training trajectory. Lemma 5.2 also suggests that when  $t \\to \\infty$ , the adversially trained classifier  $\\theta_t$  will converge in direction to the max adversarial margin classifier  $\\mathbf{w}$  defined in (3.3).\n\n\nLength and direction of the adversarial training iterates  $\\theta_{t}$ . We also establish properties of the adversarial training iterates  $\\theta_{t}$ . We have the following lemmas.\n\n\nLemma 5.3. Under the same conditions as in Theorem 4.4, for all adversarial training iteration  $t > 0$ , with probability at least  $1 - \\delta$ , we have  $\\| \\pmb{\\theta}_{t + 1}\\|_2 \\leq (\\sqrt{c_0} +\\epsilon)\\sqrt{d}\\sum_{m = 0}^{t}\\alpha_mL(\\pmb{\\theta}_m)$ , where  $c_{0}$  is the absolute constant in Lemma 5.1.\n\n\nLemma 5.3 upper bound the  $L_{2}$  norm of adversarially trained classifier  $\\theta_t$  by the summation of training losses along the training trajectory.\n\n\nLemma 5.4. Let  $\\mathbf{z}_k = y_k\\mathbf{x}_k$ , under the same conditions as in Theorem 4.4, for all adversarial training iteration  $t \\geq 0$ , with probability as least  $1 - \\delta$ , we have  $\\max_{k=1}^{n}\\exp(-\\boldsymbol{\\theta}_t^\\top\\mathbf{z}_k) \\leq c_3\\min_{k=1}^{n}\\exp(-\\boldsymbol{\\theta}_t^\\top\\mathbf{z}_k)$ , where  $c_3 > 0$  is an absolute constant.\n\n\nLemma 5.4 provides us a way to control the loss the noisy examples during the training procedure. Note that if  $\\max_{k=1}^{n} \\exp(-\\pmb{\\theta}_{t}^{\\top} \\mathbf{z}_{k}) \\leq c_{3} \\min_{k=1}^{n} \\exp(-\\pmb{\\theta}_{t}^{\\top} \\mathbf{z}_{k})$ , we also have  $\\max_{k=1}^{n} \\exp(-\\pmb{\\theta}_{t}^{\\top} \\mathbf{z}_{k} + \\epsilon \\| \\pmb{\\theta}_{t} \\|_{q}) \\leq c_{3} \\min_{k=1}^{n} \\exp(-\\pmb{\\theta}_{t}^{\\top} \\mathbf{z}_{k} + \\epsilon \\| \\pmb{\\theta}_{t} \\|_{q})$ . Therefore, the worst example training loss can be bounded via the best example training loss and further be bounded by the average training loss  $L(\\pmb{\\theta}_{t})$ . In this way, we can guarantee that those noisy examples will not have major influence on model training even in later training stages.\n\n\nBy using Lemmas 5.1-5.4, we establish the following key lemma for our main theorems.\n\n\nLemma 5.5. Under the same condition as in Theorem 4.4, with probability at least  $1 - \\delta$ , the adversarially trained linear model parameter  $\\theta_{t}$  satisfies\n\n\n$$\n\n$$\n\\frac {\\boldsymbol {\\mu} ^ {\\top} \\boldsymbol {\\theta} _ {t}}{\\| \\boldsymbol {\\theta} _ {t} \\| _ {2}} \\geq \\left(\\frac {\\| \\boldsymbol {\\mu} \\| _ {2} ^ {2}}{4} - \\epsilon \\| \\boldsymbol {\\mu} \\| _ {q}\\right) \\frac {1}{(\\sqrt {c _ {0}} + \\epsilon) \\sqrt {d}} - \\frac {c _ {3} \\| \\boldsymbol {\\mu} \\| _ {2} \\log n}{\\log t}.\n$$\n\n$$\n\n\nwhere  $c_{0}$  is the absolute constant in Lemma 5.1.\n\n\nLemma 5.5 provides the lower bound for the inner product of  $\\pmb{\\mu}$  and the direction of  $\\theta_t$ . This lemma extends Lemma 4.4 in Li et al. (2020) by considering the training iteration  $t$  rather than just the converged classifier  $\\mathbf{w}$ , and also extends to the adversarial training setting. Notice that this lower bound actually gets larger with the increase of iteration  $t$ .\n\n\nFinalizing the proof. We now present the proof for Theorems 4.4 and 4.8.\n\n\nProof of Theorem 4.4. First, following standard coupling lemma (Lindvall, 2002), there always exists a joint distribution on original data and noisy data  $((\\widetilde{\\mathbf{x}},\\widetilde{y}),(x,y))$  such that the marginal distribution for  $(\\widetilde{\\mathbf{x}},\\widetilde{y})$  is  $\\widetilde{\\mathcal{D}}$ , the marginal distribution for  $(\\mathbf{x},y)$  is  $\\mathcal{D}$ ,  $\\mathbb{P}[\\mathbf{x} = \\widetilde{\\mathbf{x}}] = 1$  and  $\\mathbb{P}[y\\neq \\widetilde{y}]\\leq \\eta$ . Notice that the standard population risk can be written as\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} [ f _ {\\boldsymbol {\\theta} _ {t}} (\\mathbf {x}) \\neq y ] = \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} [ y \\cdot \\boldsymbol {\\theta} _ {t} ^ {\\top} \\mathbf {x} <   0 ] \\\\ \\leq \\eta + \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} [ y \\cdot \\boldsymbol {\\theta} _ {t} ^ {\\top} \\mathbf {x} <   0, y = \\widetilde {y} ] \\\\ = \\eta + \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} [ \\widetilde {\\boldsymbol {y}} \\cdot \\boldsymbol {\\theta} _ {t} ^ {\\top} \\mathbf {x} <   0 ], \\tag {5.7} \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere the inequality holds since  $\\mathbb{P}[y\\neq \\widetilde{y} ]\\leq \\eta$ . Since  $\\widetilde{y}$  is the clean label for  $\\mathbf{x}$ ,  $\\widetilde{y}\\mathbf{x}$  follows the same distribution as  $\\pmb {\\xi} + \\pmb{\\mu}$  and  $\\mathbb{E}[\\widetilde{y}\\cdot \\widehat{\\pmb{\\theta}}^{\\top}\\mathbf{x}] = \\widehat{\\pmb{\\theta}}^{\\top}\\pmb{\\mu}$ . Therefore, (5.7) can be further written as\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ f _ {\\boldsymbol {\\theta} _ {t}} (\\mathbf {x}) \\neq y \\right] \\leq \\eta + \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\widetilde {y} \\cdot \\boldsymbol {\\theta} _ {t} ^ {\\top} \\mathbf {x} - \\mathbb {E} [ \\widetilde {y} \\cdot \\boldsymbol {\\theta} _ {t} ^ {\\top} \\mathbf {x} ] <   - \\boldsymbol {\\theta} _ {t} ^ {\\top} \\boldsymbol {\\mu} \\right] \\\\ = \\eta + \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\boldsymbol {\\theta} _ {t} ^ {\\top} (\\widetilde {\\boldsymbol {y}} \\mathbf {x} - \\mathbb {E} [ \\widetilde {\\boldsymbol {y}} \\mathbf {x} ]) <   - \\boldsymbol {\\theta} _ {t} ^ {\\top} \\boldsymbol {\\mu} \\right] \\\\ \\leq \\eta + \\exp \\left(- c \\frac {\\left(\\boldsymbol {\\theta} _ {t} ^ {\\top} \\boldsymbol {\\mu}\\right) ^ {2}}{\\left\\| \\boldsymbol {\\theta} _ {t} \\right\\| _ {2} ^ {2}}\\right), \\tag {5.8} \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere the last inequality holds by applying a Hoeffding-type concentration inequality (Theorem C.1) with  $t = (\\pmb{\\theta}_t^\\top \\pmb{\\mu})^2$ . This bound in (5.8) enables the application of Lemma 5.5 which characterizes\n\n\nhow the direction of  $\\theta_t$  aligns with  $\\mu$  during training. By direct calculation, we have\n\n\n$$\n\n$$\n\\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} [ f _ {\\pmb {\\theta} _ {t}} (\\mathbf {x}) \\neq y ] \\leq \\eta + \\exp \\left(- c \\bigg (\\frac {\\big (\\| \\pmb {\\mu} \\| _ {2} ^ {2} - \\epsilon \\| \\pmb {\\mu} \\| _ {q} \\big)}{(\\sqrt {c _ {0}} + \\epsilon) \\sqrt {d}} - \\frac {c _ {3} \\| \\pmb {\\mu} \\| _ {2} \\log n}{\\log t}\\right) ^ {2}\\left. \\right).\n$$\n\n$$\n\n\nThis concludes the proof.\n\n\n![](images/8168e98da3652017879b8f3b33c92ae6d4bfff26ed9214a78296889373151d98.jpg)\n\n\nProof of Theorem 4.8. Similar as in the proof of Theorem 4.4, we start with a calculating an upper bound of the population risk based on the formulation of the label noise. By the definition of the adversarial risk, we have\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\exists \\mathbf {x} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x}) s. t., f _ {\\boldsymbol {\\theta} _ {t}} (\\mathbf {x} ^ {\\prime}) \\neq y \\right] = \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} [ \\exists \\mathbf {x} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x}) s. t., y \\cdot \\boldsymbol {\\theta} _ {t} ^ {\\top} \\mathbf {x} ^ {\\prime} <   0 ] \\\\ \\leq \\eta + \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} [ \\exists \\mathbf {x} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x}) s. t., y \\cdot \\boldsymbol {\\theta} _ {t} ^ {\\top} \\mathbf {x} ^ {\\prime} <   0, y = \\widetilde {y} ] \\\\ = \\eta + \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\min  _ {\\mathbf {u} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {0})} \\widetilde {y} \\cdot \\boldsymbol {\\theta} _ {t} ^ {\\top} (\\mathbf {x} + \\mathbf {u}) <   0 \\right] \\\\ = \\eta + \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\widetilde {\\boldsymbol {y}} \\cdot \\boldsymbol {\\theta} _ {t} ^ {\\top} \\mathbf {x} - \\epsilon \\| \\boldsymbol {\\theta} _ {t} \\| _ {q} <   0 \\right], \\tag {5.9} \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere the inequality holds in the same way as in (5.7). Since  $\\widetilde{y}$  is the clean label for  $\\mathbf{x}$ ,  $\\widetilde{y}\\mathbf{x}$  follows the same distribution as  $\\pmb{\\xi} + \\pmb{\\mu}$  and  $\\mathbb{E}[\\widetilde{y}\\cdot \\pmb{\\theta}_t^\\top \\mathbf{x}] = \\pmb{\\theta}_t^\\top \\pmb{\\mu}$ . Therefore, (5.9) can be further written as\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\exists \\mathbf {x} ^ {\\prime} \\in \\mathcal {B} _ {\\epsilon} ^ {p} (\\mathbf {x}) s. t., f _ {\\boldsymbol {\\theta} _ {t}} (\\mathbf {x} ^ {\\prime}) \\neq y \\right] \\leq \\eta + \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\widetilde {y} \\cdot \\boldsymbol {\\theta} _ {t} ^ {\\top} \\mathbf {x} - \\mathbb {E} [ \\widetilde {y} \\cdot \\boldsymbol {\\theta} _ {t} ^ {\\top} \\mathbf {x} ] <   - \\boldsymbol {\\theta} _ {t} ^ {\\top} \\boldsymbol {\\mu} + \\epsilon \\| \\boldsymbol {\\theta} _ {t} \\| _ {q} \\right] \\\\ = \\eta + \\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ \\pmb {\\theta} _ {t} ^ {\\top} \\left(\\widetilde {y} \\mathbf {x} - \\mathbb {E} [ \\widetilde {y} \\mathbf {x} ]\\right) <   - \\pmb {\\theta} _ {t} ^ {\\top} \\pmb {\\mu} + \\epsilon \\| \\pmb {\\theta} _ {t} \\| _ {q} \\right] \\\\ \\leq \\eta + \\exp \\left(- c \\frac {\\left(\\boldsymbol {\\theta} _ {t} ^ {\\top} \\boldsymbol {\\mu} - \\epsilon \\| \\boldsymbol {\\theta} _ {t} \\| _ {q}\\right) ^ {2}}{\\| \\boldsymbol {\\theta} _ {t} \\| _ {2} ^ {2}}\\right), \\tag {5.10} \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere the second inequality holds by applying the Hoeffding-type concentration inequality (Theorem C.1) with  $t = (\\pmb{\\theta}_t^\\top \\pmb{\\mu} - \\epsilon \\| \\pmb{\\theta}_t\\|_q)^2$ . Based on (5.10) and Lemma 5.5, we can further give bounds of the adversarial risk. We consider the two settings  $1 \\leq p \\leq 2$  and  $2 < p < \\infty$  separately.\n\n\nWhen  $1 \\leq p \\leq 2$ , we have  $q \\geq 2$  and  $\\| \\pmb{\\theta} \\|_q \\leq \\| \\pmb{\\theta} \\|_2$ . In this case, by Lemma 5.5 we obtain\n\n\n$$\n\n$$\n\\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} [ f _ {\\boldsymbol {\\theta} _ {t}} (\\mathbf {x}) \\neq y ] \\leq \\eta + \\exp \\left(- c \\left(\\frac {\\left(\\frac {\\| \\boldsymbol {\\mu} \\| _ {2} ^ {2}}{4} - \\epsilon \\| \\boldsymbol {\\mu} \\| _ {q}\\right)}{\\left(\\sqrt {c _ {0}} + \\epsilon\\right) \\sqrt {d}} - \\frac {c _ {3} \\| \\boldsymbol {\\mu} \\| _ {2} \\log n}{\\log t} - \\epsilon\\right) ^ {2}\\right).\n$$\n\n$$\n\n\nWhen  $p > 2$  and therefore  $q < 2$ , we have  $\\| \\pmb{\\mu} \\|_q \\leq d^{1/q - 1/2} \\| \\pmb{\\mu} \\|_2$ . In this case, by Lemma 5.5 we obtain\n\n\n$$\n\n$$\n\\mathbb {P} _ {(\\mathbf {x}, y) \\sim \\mathcal {D}} \\left[ f _ {\\boldsymbol {\\theta} _ {t}} (\\mathbf {x}) \\neq y \\right] \\leq \\eta + \\exp \\left(- c \\left(\\frac {\\left(\\frac {\\| \\boldsymbol {\\mu} \\| _ {2} ^ {2}}{4} - \\epsilon \\| \\boldsymbol {\\mu} \\| _ {q}\\right)}{\\left(\\sqrt {c _ {0}} + \\epsilon\\right) \\sqrt {d}} - \\frac {c _ {3} \\| \\boldsymbol {\\mu} \\| _ {2} \\log n}{\\log t} - \\epsilon d ^ {\\frac {1}{q} - \\frac {1}{2}}\\right) ^ {2}\\right).\n$$\n\n$$\n\n\nThis concludes the proof.\n\n\n![](images/62c93d71db6635f2430ddff901dd5f5f6c138ae97c9bd451c15c8b9695f0be67.jpg)\n\n\n# 6 EXPERIMENTS\n\n\nIn this section, we experimentally study the behavior of the adversarially trained linear classifier in the over-parameterized regime on synthetic data. Specifically, we generate 50 training samples and 2000 test samples and set the label noise ratio  $\\eta = 0.1$  for all experiments. Each clean sample  $(\\widetilde{\\mathbf{x}},\\widetilde{y})$  is drawn from a Gaussian mixture model such that  $\\widetilde{y}\\sim \\mathrm{Unif}(\\{\\pm 1\\})$  and  $\\widetilde{\\mathbf{x}} = \\widetilde{y}\\pmb {\\mu} + \\pmb{\\xi}$  where  $\\pmb {\\xi}\\in \\mathbb{R}^d$  and  $\\xi_1,\\xi_2,\\ldots ,\\xi_d$  are i.i.d. standard Gaussian variables and  $\\pmb{\\mu}$  simply shares the same direction as an all-one vector but has various different magnitudes. This aligns with our model assumptions in Section 3. For the adversarial training algorithm, we directly follows Algorithm 1 except using a more practical Xavier normal initialization (Glorot & Bengio, 2010), i.e., sampling  $\\theta_0$  i.i.d. from from  $\\mathcal{N}(0,1 / \\sqrt{d})$ . We set the learning rate  $\\alpha_{t} = 0.001$  and the total number of iterations  $T = 1000$  for all experiments. All results are obtained by averaging over 10 independent runs (both data sampling and training).\n\n\nIn the first set of experiments, we verify our main conclusions in this paper, that benign overfitting can occur in adversarial training. Figure 1 (a-d) illustrates the risk and the adversarial risk of adversarially trained linear classifiers versus the dimension  $d$  under different scalings of  $\\mu$  for both\n\n\n![(a)  $\\ell_2$  perturbation](images/5d506e3cdc67d008a97677264f6976dca75cbcffbdfe1ea7133ddc8edb117585.jpg)\n\n\n![(b)  $\\ell_2$  perturbation](images/3811a687e56eda4e9ddabcb67a5f4465a27b8e48f7708bd8450a5b1f788c9f11.jpg)\n\n\n![(c)  $\\ell_{\\infty}$  perturbation](images/873eff81f1a583f5da1178d46d7766b2014e676d340ea28b12bd8523c2ea7212.jpg)\n\n\n![(d)  $\\ell_{\\infty}$  perturbation](images/b814d85107883544cc482ad27beb8b90330755178509c019b574d2080664186d.jpg)\n\n\n![(e)  $\\ell_2$  perturbation](images/85800b4a8788ae5fe043da0805853f883a0f300b446b8b619478a28b258c20a1.jpg)\n\n\n![(f)  $\\ell_{\\infty}$  perturbation Figure 1: (a-d) Risk and adversarial risk of adversariably trained linear classifiers versus the dimension  $d$  under different scalings of  $\\pmb{\\mu}$ . (a)(b) show the results for  $\\ell_2$  perturbation with  $\\epsilon = 0.1$  and (c)(d) show the results for  $\\ell_{\\infty}$  perturbation with  $\\epsilon = 0.01$ . (e-f) Adversarial risk of adversariably trained linear classifiers versus the training iterations  $t$  for different  $\\epsilon$  with  $d = 200$  and  $\\| \\pmb{\\mu}\\| _2 = d^{0.3}$ . The training error reaches 0 for all experiments.](images/12a6cf79e9e05bbaf348c4879d342eea3e08cc2514b248990af0d23f290b077f.jpg)\n\n\n$\\ell_2$ -norm and  $\\ell_{\\infty}$ -norm perturbations. We can observe that when  $\\| \\pmb{\\mu}\\| _2 = d^{0,2}$ , the (adversarial) risk starts to increase as the dimension  $d$  increases after an initial dive for both  $\\ell_2$ -norm and  $\\ell_{\\infty}$ -norm perturbations. While for cases where  $\\| \\pmb {\\mu}\\| _2 = d^{0,3}$  and  $\\| \\pmb {\\mu}\\| _2 = d^{0,4}$ , we can observe that the (adversarial) risk decreases steadily to the optimal risk  $\\eta$  as the dimension  $d$  increases. This results backup our theory in Section 4 that the optimal risk is achievable when  $\\| \\pmb {\\mu}\\| _2 = \\Theta (d^r)$  and  $r\\in (1 / 4,1 / 2]$ . Note that the training error reaches 0 for all settings in Figure 1.\n\n\nIn Figure 1 (e-f), we present the adversarial risk $^2$  of adversariably trained linear classifiers versus the training iterations  $t$  with different  $\\epsilon$  but fixed dimension  $d$  and  $\\|\\pmb{\\mu}\\|_2$  for both  $\\ell_2$ -norm and  $\\ell_{\\infty}$ -norm perturbations. We can also observe that in general, a larger  $\\epsilon$  will lead to the worse adversarial risk of the adversariably trained classifier. This also backs up our theory in Theorem 4.8.\n\n\nAs our ultimate goal is to study the benign overfitting phenomenon in real-world adversarial training settings, we also conducted experiments on 2-layer neural networks with ReLU activation functions. In fact, the performances on the 2-layer ReLU network suggest very similar trends as the linear model. Due to space limit, we display these results in the supplemental materials.\n\n\n# 7 CONCLUSIONS AND FUTURE WORK\n\n\nIn this paper, we show that the benign overfitting phenomenon also occurs in adversarial training, a principled approach to defend against adversarial examples. Specifically, we derive the risk bounds of the adversarially trained linear classifiers and show that under moderate  $\\ell_p$ -norm perturbations, they can achieve the near-optimal standard and adversarial risks, despite overfitting the noisy training data. The numerical experimental results also validate our theoretical findings.\n\n\nOur current analysis is limited to linear classifiers, while in practice, adversarial training is commonly used with neural networks. We believe our work is the first step towards analyzing benign overfitting in adversarially trained neural networks. Yet extending our current analysis to adversari-ally trained neural networks is highly non-trivial and we leave it as a future work.\n\n\n# REFERENCES\n\n\nZeyuan Allen-Zhu and Yanzhi Li. Feature purification: How adversarial training performs robust deep learning. arXiv preprint arXiv:2005.10190, 2020.\n\n- Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Peter L Bartlett, Philip M Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.\n\n- Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel learning. In International Conference on Machine Learning, pp. 541-549. PMLR, 2018.\n\n- Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849-15854, 2019a.\n\n- Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv preprint arXiv:1903.07571, 2019b.\n\n- Yuan Cao, Quanquan Gu, and Mikhail Belkin. Risk bounds for over-parameterized maximum margin classification on sub-gaussian mixtures. arXiv preprint arXiv:2104.13628, 2021.\n\n- Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled data improves adversarial robustness. In NeurIPS, pp. 11192-11203, 2019.\n\n- Zachary Charles, Shashank Rajput, Stephen Wright, and Dimitris Papailiopoulos. Convergence and margin of adversarial training on separable data. arXiv preprint arXiv:1905.09209, 2019.\n\n- Niladri S Chatterji and Philip M Long. Finite-sample analysis of interpolating linear classifiers in the overparameterized regime. arXiv preprint arXiv:2004.12019, 2020.\n\n- Lin Chen, Yifei Min, Mingrui Zhang, and Amin Karbasi. More data can expand the generalization gap between adversariably robust and standard models. In International Conference on Machine Learning, pp. 1670-1680. PMLR, 2020.\n\n- Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. Pac-learning in the presence of evasion adversaries. Proceedings of the 32nd International Conference on Neural Information Processing Systems, 2018.\n\n- Chen Dan, Yuting Wei, and Pradeep Ravikumar. Sharp statistical guarantees for adversarially robust gaussian classification. In International Conference on Machine Learning, pp. 2345-2355. PMLR, 2020.\n\n- Ilias Diakonikolas, Daniel M Kane, and Pasin Manurangsi. The complexity of adversarially robust proper learning of halfspaces with agnostic noise. Advances in Neural Information Processing Systems, 2020.\n\n- Edgar Dobriban, Hamed Hassani, David Hong, and Alexander Robey. Provable tradeoffs in adversarially robust classification. arXiv preprint arXiv:2006.05161, 2020.\n\n- Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 1186-1195, 2018.\n\n- Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason D Lee. Convergence of adversarial training in overparametrized neural networks. Advances in Neural Information Processing Systems, 32:13029-13040, 2019.\n\n- Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256. JMLR Workshop and Conference Proceedings, 2010.\n\n- Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. *ICLR*, 2015.\n\n- Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pp. 1832-1841. PMLR, 2018a.\n\n- Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro. Implicit regularization in matrix factorization. In 2018 Information Theory and Applications Workshop (ITA), pp. 1-10. IEEE, 2018b.\n\n- Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770-778, 2016.\n\n- Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Brian Kingsbury, et al. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal processing magazine, 29, 2012.\n\n- Adel Javanmard and Mahdi Soltanolkotabi. Precise statistical analysis of classification accuracies for adversarial training. arXiv preprint arXiv:2010.11213, 2020.\n\n- Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In International Conference on Learning Representations, 2019a.\n\n- Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In Conference on Learning Theory, pp. 1772-1798. PMLR, 2019b.\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Neurips, pp. 1097-1105, 2012.\n\n- Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.\n\n- Yan Li, Ethan X.Fang, Huan Xu, and Tuo Zhao. Implicit bias of gradient descent based adversarial training on separable data. In International Conference on Learning Representations, 2020.\n\n- Torgny Lindvall. Lectures on the coupling method. Courier Corporation, 2002.\n\n- Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In International Conference on Learning Representations, 2020.\n\n- Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. ICML, 2018.\n\n- Omar Montasser, Surbhi Goel, Ilias Diakonikolas, and Nathan Srebro. Efficiently learning adversarially robust halfspaces with noise. In International Conference on Machine Learning, pp. 7010-7021. PMLR, 2020.\n\n- Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 3051-3059. PMLR, 2019.\n\n- Behnam Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953, 2017.\n\n- Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding and mitigating the tradeoff between robustness and accuracy. arXiv preprint arXiv:2002.10716, 2020.\n\n- Leslie Rice, Eric Wong, and J Zico Kolter. Overfitting in adversarially robust deep learning. ICML, 2020.\n\n- Amartya Sanyal, Puneet K. Dokania, Varun Kanade, and Philip Torr. How benign is benign overfitting? In International Conference on Learning Representations, 2021.\n\n- Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarily robust generalization requires more data. Advances in Neural Information Processing Systems, 2018.\n\n- Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. *The Journal of Machine Learning Research*, 19 (1):2822-2878, 2018.\n\n- Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\n- Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis. Asymptotic behavior of adversarial training in binary classification. arXiv preprint arXiv:2010.13275, 2020.\n\n- Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. arXiv preprint arXiv:2009.14286, 2020.\n\n- Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In International Conference on Learning Representations, 2019.\n\n- Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.\n\n- Ke Wang and Christos Thrampoulidis. Benign overfitting in binary classification of gaussian mixtures. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4030-4034. IEEE, 2021.\n\n- Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the convergence and robustness of adversarial training. In ICML, pp. 6586-6595, 2019.\n\n- Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In International Conference on Learning Representations, 2020.\n\n- Boxi Wu, Jinghui Chen, Deng Cai, Xiaofei He, and Quanquan Gu. Do wider neural networks really help adversarial robustness? arXiv preprint arXiv:2010.01279, 2020.\n\n- Denny Wu and Ji Xu. On the optimal weighted  $\\ell_2$  regularization in overparameterized linear regression. Advances in Neural Information Processing Systems, 33, 2020.\n\n- Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaudhuri. A closer look at accuracy vs. robustness. Advances in Neural Information Processing Systems, 33, 2020.\n\n- Dong Yin, Ramchandran Kannan, and Peter Bartlett. Rademacher complexity for adversarially robust generalization. In International Conference on Machine Learning, pp. 7085-7094. PMLR, 2019.\n\n- Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.\n\n- Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In ICML, pp. 7472-7482, 2019.\n\n- Xiao Zhang, Jinghui Chen, Quanquan Gu, and David Evans. Understanding the intrinsic robustness of image distributions using conditional generative models. In International Conference on Artificial Intelligence and Statistics, pp. 3883-3893. PMLR, 2020a.\n\n- Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song, and Sanjeev Arora. Overparameterized adversarial training: An analysis overcoming the curse of dimensionality. Advances in Neural Information Processing Systems, 2020b.\n\n- Difan Zou, Spencer Frei, and Quanquan Gu. Provable robustness of adversarial training for learning halfspaces with noise. International Conference on Machine Learning, 2021.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638580.9232676", "msecs": "923.0", "relativeCreated": "177753.76510620117", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# AdaFisher: Adaptive Second Order Optimization via Fisher Information\n\n# Abstract\nFirst-order optimization methods are currently the mainstream in training deep neural networks (DNNs). Optimizers like Adam incorporate limited curvature information by employing the diagonal matrix preconditioning of the stochastic gradient during the training. Despite their widespread, second-order optimization algorithms exhibit superior convergence properties compared to their first-order counterparts e.g. Adam and SGD. However, their practicality in training DNNs is still limited due to increased per-iteration computations compared to the first-order methods. We present *AdaFisher*--an adaptive second-order optimizer that leverages a *diagonal block-Kronecker* approximation of the Fisher information matrix for adaptive gradient preconditioning. AdaFisher aims to bridge the gap between enhanced *convergence/generalization* capabilities and computational efficiency in second-order optimization framework for training DNNs. Despite the slow pace of second-order optimizers, we showcase that AdaFisher can be reliably adopted for image classification, language modeling and stands out for its stability and robustness in hyper-parameter tuning. We demonstrate that AdaFisher **outperforms the SOTA optimizers** in terms of both accuracy and convergence speed.\n\n\n# 1 INTRODUCTION\n\n\nDeep Neural network (DNN) optimization often struggles with the challenge of generalizing across varied architectures and complex data distributions. Current methods such as Adam optimizer (Kingma & Ba, 2017) and its variants (AdamP (Heo et al., 2020), AdaInject (Dubey et al., 2022), AdaBelief (Zhuang et al., 2020) and YOGI Zaheer et al. (2018)) require extensive hyperparameter tuning and often fail to generalize efficiently. In response, DNN training typically minimizes an empirical loss function  $\\mathcal{L}(\\theta)$ , updating parameters  $\\theta$  using the expression  $\\theta_{t + 1} = \\theta_t - \\alpha G_t^{-1}\\nabla \\mathcal{L}(\\theta_t)$  at time step  $t$ , where  $G_{t}$  represents curvature information, i.e. Hessian or Fisher Information Matrix (FIM) (Amari & Nagaoka, 2000). The Hessian matrix interfaces with the deterministic Newton method (Martens, 2020), whereas the FIM harmonizes with the Natural Gradient Descent (NGD) approach (Amari & Nagaoka, 2000) as a statistical method. These curvature information crucially optimize the gradient's preconditioning by accurately rescaling and orienting it. This adjustment significantly accelerates convergence by ensuring more direct progress towards minima, thus enhancing training efficiency and reducing the number of required iterations.\n\n\nWhile first-order methods such as SGD (Kiefer & Wolfowitz, 1952) simplify  $G_{t}$  by treating it as the identity matrix, second-order methods employ curvature matrices to enhance the optimization process. Although these matrices accelerate convergence by effectively navigating saddle points and swiftly moving towards minima (Foret et al., 2021), they require higher computational resources. In fact, when the number of learnable parameters increases, the curse of dimensionality associated with curvature matrix  $G_{t}$  makes the entire process completely intractable for routine deep learning training tasks. Noteworthy approaches, such as Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton et al.), and Adam family utilize a simple diagonal approximation of the empirical FIM, which often results in convergence to suboptimal local minima and poor generalization (Wilson et al., 2018; Luo et al., 2019). Advanced methods like AdaHessian (Yao et al., 2021) and Shampoo (Gupta et al., 2018) improve on this by integrating structured matrices such as the diagonal Hessian or tensor-based preconditioners to enhance optimization. However, these second-order approaches including K-FAC (Martens & Grosse, 2020; Eschenhagen et al., 2024) still face challenges of high\n\n\ncomputational demands, lack generalization and the need for extensive hyperparameter tuning when applied to large models (Ma et al., 2019).\n\n\nTo address these challenges, we present AdaFisher, an adaptive second-order optimizer, as an innovative solution to address the generalization challenges raised in training DNNs. Substituting the second moment of Adam by a novel diagonal block-Kronecker approximation of the FIM, it strikes a balance between simplicity and generalization by introducing one additional hyperparameter compared to Adam but fewer than K-FAC, AdaHessian or Shampoo. This feature, combined with comparable memory and time requirements than the first-order methods, enhances AdaFisher's practicality for achieving effective generalization in DNN optimization. As illustrated in Figure 1, AdaFisher not only converges more rapidly but also reaches a superior local minimum by effectively navigating through saddle points compared to its counterparts. Further details regarding the visualiza\n\n\ntion can be found in Appendix C. Our main contributions are as follows: [C1] We empirically showcase the diagonal dominance of Kronecker factors' energy and provide fresh insights of FIM in optimization; [C2] We introduce a diagonal block-Kronecker approximation of the FIM applicable to various layers, including normalization layers, enhancing model adaptability; [C3] We demonstrate AdaFisher's robustness and stability across diverse settings, proving its effectiveness; [C4] We showcase AdaFisher's superior performance against traditional optimizers empirically in image classification and language modeling; [C5] We develop a new technique that visualizes trajectories across different optimizers for better understanding of model behavior in the loss landscape. Additionally, we introduce an explainable FIM measure from AdaFisher, enabling comparative analysis of optimizer behavior.\n\n\n![Figure 1: Visualization of optimization paths for various optimizers on a loss surface, comparing their convergence efficiency.](images/84a0571bad6eba8210cd1589e33f43568869fea57f5d7f9ab612ca8680a748b4.jpg)\n\n\n# 2 BACKGROUND\n\n\nWe consider a supervised learning framework with a dataset  $\\mathbf{D}$  containing  $N$  i.i.d samples,  $\\mathbf{D} := \\{x_n, y_n\\}_{n=1}^N$  where  $x_n \\in \\mathbb{R}^d$  and  $y_n \\in \\mathbb{R}^C$ . Let  $f_\\theta : \\mathbb{R}^d \\to \\mathbb{R}^C$  be a L-layer neural network parametrized by  $\\theta$  where  $\\theta_i = \\text{concat}(W_i, b_i) \\in \\mathbb{R}^{P_i}$ , and  $P_i = P_i^{out} \\times (P_i^{in} + 1)$ . Let  $\\mathcal{L} : \\mathbb{R}^C \\times \\mathbb{R}^C \\to \\mathbb{R}$  be the loss function defined by negative log likelihood, i.e.  $\\mathcal{L}(y, f_\\theta(x)) := -\\log p_\\theta(y|x)$  where  $p_\\theta(y|x)$  is the likelihood of the neural network  $f_\\theta$ . The network computes its output  $h_L = f_\\theta(x)$  according to:  $a_i = \\theta_i \\bar{h}_{i-1}$ ,  $h_i = \\phi_i(a_i)$ ,  $\\forall i \\in \\{1, \\dots, L\\} \\mid h_0 = x_n$  where  $\\bar{h}_i = [1, h_i^T]^T \\in \\mathbb{R}^{P_i^{in} + 1}$  terminated by  $z := h_L \\in \\mathbb{R}^{P_L^{out}}$ . For a given input target pair  $(x,y)$ , the gradient of the loss  $\\mathcal{L}(y, f_\\theta(x))$  concerning the weights are computed by the backpropagation algorithm (Lecun, 2001). For convenience, we adopt the special symbol  $s_i = \\nabla_{a_i} \\mathcal{L}$  for the pre-activation derivative. Starting from  $\\nabla_{h_L} \\mathcal{L} = \\partial_z \\mathcal{L}(y, z = h_L)$ , we perform:  $s_i := \\nabla_{a_i} \\mathcal{L} = \\nabla_{h_i} \\mathcal{L} \\odot \\phi_i'(a_i)$ ,  $\\nabla_{\\theta_i} \\mathcal{L} = s_i \\bar{h}_{i-1}^T$ ,  $\\nabla_{\\bar{h}_{i-1}} \\mathcal{L} = \\theta_i^T s_i \\mid \\forall i \\in \\{L, \\dots, 1\\}$ , where  $\\odot$  denotes the element-wise product. Finally, the gradient  $\\nabla_\\theta \\mathcal{L}$  is retrieved by:  $\\nabla_\\theta \\mathcal{L} = [\\mathrm{vec}(\\nabla_{\\theta_1} \\mathcal{L})^T, \\mathrm{vec}(\\nabla_{\\theta_2} \\mathcal{L})^T, \\dots, \\mathrm{vec}(\\nabla_{\\theta_L} \\mathcal{L})^T]^T$ . Optimization of a DNN can be recast as a problem of finding the parameter set  $\\theta$  that maximizes the likelihood, or equivalently, minimizes the negative log-likelihood of the observed data. This Maximum Likelihood Estimation approach can be expressed as an unconstrained optimization problem:  $\\min_\\theta J(\\theta) = \\sum_{n=1}^N \\mathcal{L}(y_n, f_\\theta(x_n))$ , where  $J(\\theta)$  denotes the objective function, corresponding to the negative log-likelihood of the data. The FIM, utilized in lieu of the Hessian for Newton's method (Holmgren, 1996), approximates the curvature of the log-likelihood function (Amari, 1998). It is defined as\n\n\n$$\n\n$$\nF = \\sum_ {n = 1} ^ {N} \\mathbb {E} _ {y \\sim p (y | f _ {\\theta} (x _ {n}))} \\left[ \\nabla_ {\\theta} \\log p _ {\\theta} (y | x _ {n}) \\nabla_ {\\theta} \\log p _ {\\theta} (y | x _ {n}) ^ {T} \\right] = \\mathbb {E} \\left[ \\nabla_ {\\theta} \\mathcal {L} (\\nabla_ {\\theta} \\mathcal {L}) ^ {T} \\right], \\tag {1}\n$$\n\n$$\n\n\nwhere  $F$  measures the expected information that an observable  $y$  conveys about the parameter  $\\theta$ . For brevity, we write  $\\mathbb{E}$  instead of  $\\mathbb{E}_{y \\sim p(y | f_{\\theta}(x_n))}$ . The K-FAC approach further simplifies FIM calculation using a block-diagonal approximation in DNNs, known as Empirical FIM (EFIM), denoted  $\\hat{F}$ . In Eq. (1),  $F$  is construed as a block matrix with dimensions  $L \\times L$ , where each  $(i,j)$ th block\n\n\n$F_{i,j}$  is articulated by  $F_{i,j} = \\mathbb{E}[\\mathrm{vec}(\\nabla_{\\theta_i}\\mathcal{L})\\mathrm{vec}(\\nabla_{\\theta_j}\\mathcal{L})^T]$ . By harnessing the vectorization identity  $\\mathrm{vec}(uv^T) = v\\otimes u$ , we express  $\\mathrm{vec}(\\nabla_{\\theta_i}\\mathcal{L})$  as  $\\bar{h}_{i - 1}\\otimes s_i$  (Petersen & Pedersen, 2008), where  $\\nabla_{\\theta_i}\\mathcal{L}$  is defined as  $s_i\\bar{h}_{i - 1}^T$ . By segmenting the FIM into discrete layer-specific blocks, we can effectuate a systematic factorization of each block\n\n\n$$\n\n$$\n\\hat {F} _ {i, j} = \\mathbb {E} \\left[ \\operatorname {v e c} \\left(\\nabla_ {\\theta_ {i}} \\mathcal {L}\\right) \\operatorname {v e c} \\left(\\nabla_ {\\theta_ {j}} \\mathcal {L}\\right) ^ {T} \\right] = \\mathbb {E} \\left[ \\bar {h} _ {i - 1} \\bar {h} _ {j - 1} ^ {T} \\otimes s _ {i} s _ {j} ^ {T} \\right] \\approx \\mathbb {E} \\left[ \\bar {h} _ {i - 1} ^ {T} \\bar {h} _ {j - 1} \\right] \\otimes \\mathbb {E} \\left[ s _ {i} ^ {T} s _ {j} \\right],\n$$\n\n$$\n\n\n![Figure 2: Illustration of EFIM computation using K-FAC for a given layer  $i$ .](images/a31591d2f6d227c85ac1c5cb94de548c4ab72614991976683bc082f7d1551c4b.jpg)\n\n\nwhere  $i,j$  span the layer indices from 1 to  $L$ . Here,  $\\mathbb{E}[\\bar{h}_{i - 1}^T\\bar{h}_{j - 1}]$  and  $\\mathbb{E}[s_i^T s_j]$  are empirically approximated using batch statistics, simplifying the computation for large-scale DNNs (Tang et al., 2021). Notably,  $\\bar{h}_{i - 1}\\in \\mathbb{R}^{M\\times (P_i^{in} + 1)}$  and  $s_i\\in \\mathbb{R}^{M\\times P_i^{out}}$ , where  $M$  is the size of a batch, rendering  $\\hat{F}_{i,j}\\in \\mathbb{R}^{(P_i^{in} + 1)P_i^{out}\\times (P_i^{in} + 1)P_i^{out}}$ . Initially, K-FAC estimates the expectation of the Kronecker product under the presumption that activations and pre-activation derivatives are mutually independent, succinctly represented as the Kronecker product of the individual expectations:  $\\hat{F}_{i,j} = \\mathcal{H}_{i - 1,j - 1}\\otimes S_{i,j}$ , where  $\\mathcal{H}_{i - 1,j - 1} =$\n\n\n$\\mathbb{E}[\\bar{h}_{i - 1}^T\\bar{h}_{j - 1}]$  and  $S_{i,j} = \\mathbb{E}[s_i^T s_j]$ , denoting the Kronecker factors. The assumption for the block-diagonal structure posits that weight derivatives across distinct layers are uncorrelated, expressed as:  $F\\approx \\hat{F} = \\mathrm{diag}(\\hat{F}_{1,1},\\dots ,\\hat{F}_{L,L}) = \\mathrm{diag}(\\hat{F}_1,\\dots ,\\hat{F}_L)$ . Figure 2 shows EFIM computation via K-FAC.\n\n\n# 3 METHODOLOGY\n\n\nOur methodology consists of four primary components: (i) Analyzing the Kronecker factors' structure in Section 3.1 and showcase their diagonal dominance; (ii) Introducing a novel approximation of the FIM that retains only the diagonals of the Kronecker factors, detailed in Section 3.2; (iii) Enhancing the Adam optimizer by incorporating our diagonal FIM approximation as an alternative to the conventional second moment, described in Section 3.3; (iv) Providing a theoretical proof of AdaFisher's convergence under both convex and non-convex conditions in Section 3.4.\n\n\n# 3.1 DIAGONAL CONCENTRATION OF KRONECKER FACTORS\n\n\nInspired by Gersgorin's Circle Theorem (Horn & Johnson, 2012), we empirically conclude the diagonal concentration property of the Kronecker factors from the eigenvalue distribution and its perturbation under Gaussian noise. For demonstration, we focus on the eigenvalue spectrum of weight matrices from the 37th layer of ResNet-18 (He et al., 2015), after training for 50 epochs on CIFAR-10 (Krizhevsky et al., 2009). As Figure 3 illustrates, the eigenvalues (denoted as red crosses) predominantly cluster within the Gersgorin discs, which are\n\n\n![](images/16d7ddb45a8170f7dee23444d1f3c0ab36ea483ce25b61c0846ec447820c6a3a.jpg)\n\n\n![Figure 3: Gersgorin disks and eigenvalue perturbations in the 37th Convolutional Layer of a ResNet-18 at steps 5200 (middle of training) and 9800 (end of training). Left: Gersgorin circles; Right: Eigenvalue spectrum with/without noise.](images/fdcd92955efc24eeea68a82150d0660ab7cc2f8314ebac7209b41dfa49dc25b1.jpg)\n\n\n![](images/b0aacc8a52d08e9c872e7db9d212d7b3ccb53fbb867527c5889a73824573ba31.jpg)\n\n\n![](images/39f62d5499e1a2149db51ab4567f534362eb99d618a22c8d248507643a342144.jpg)\n\n\ncentered along the matrix's diagonal elements (denoted as black circles), signifying substantial diagonal dominance. This phenomenon is quantitatively supported by the Gersgorin's Circle Theorem, which posits that every eigenvalue  $\\lambda$  of a complex square matrix  $\\mathcal{A}$  lies within at least one of the Gersgorin discs  $D(a_{ii},R_i)$ , where  $R_{i} = \\sum_{j\\neq i}|a_{ij}|$  represents the radius computed as the sum of the absolute values of the off-diagonal entries of the  $i$ th row. Next, we introduce Gaussian noise  $\\mathcal{N}(0,\\sigma^2)$ ,  $\\sigma = 10^{-3}$  to the off-diagonal elements. The perturbed matrix  $\\hat{\\mathcal{M}}$  is then expressed as:  $\\hat{\\mathcal{M}} = \\mathcal{A} + \\mathcal{E}$ , where  $\\mathcal{E} = [e_{ij}]$  and  $e_{ij}\\sim \\mathcal{N}(0,\\sigma^2)$  for  $i\\neq j$ . This allows us to scrutinize the noise-induced perturbation on eigenvalues, which are pivotal for comprehending the dynamics and stability of the system. This demonstrates that the introduction of noise to the off-diagonal elements induces only minimal perturbations in the eigenvalues, particularly those surpassing the\n\n\nKaiser criterion, which remain virtually unchanged (Braeken & Van Assen, 2017). This negligible shift corroborates the robustness of the matrix's diagonal dominance. An extensive discussion of this analysis is available in Appendix A.1.\n\n\n# 3.2 EFFICIENT COMPUTATION OF THE FIM\n\n\nIn the realm of optimization, NGD offers a geometrically nuanced adaptation of the classical steepest descent approach, transitioning the focus from parameter space to the model's distribution space underpinned by the adoption of a Riemannian metric, Amari & Nagaoka (2000). The formulation of NGD is articulated as\n\n\n$$\n\n$$\n\\triangle \\theta_ {t} = F _ {t} ^ {- 1} \\nabla J (\\theta_ {t}), \\tag {2}\n$$\n\n$$\n\n\nwhere  $F_{t}$  denotes the FIM at time  $t$  distinguished from  $F_{i}$ , the FIM at layer  $i$ . One of the distinguishing features of NGD within this framework is its reparameterization invariance, a direct consequence of leveraging the model's distribution properties rather than its parameters. Nevertheless, the direct computation of the FIM poses significant challenges due to its computational demands.\n\n\nFor the efficient approximation of the FIM, we present the following: (1) a methodology for calculating the Kronecker factors for normalization layers, and (2) diagonal approximation of the Kronecker factors. Beyond conventional layers such as convolutional and linear layers that are well established (Grosse & Martens, 2016; Martens & Grosse, 2020), the Kronecker factors for BatchNorm (Ioffe & Szegedy, 2015) and LayerNorm (Ba et al., 2016) normalization can be derived with Proposition 3.1.\n\n\nProposition 3.1. Consider a neural network layer indexed by  $i$ , and a mini-batch  $\\mathbb{B} \\subset \\mathbf{D}$  of size  $M$  ( $|\\mathbb{B}| = M$ ). The empirical statistics of the Kronecker factors for the normalization layers can be characterized by\n\n\n$$\n\n$$\n\\mathcal {H} _ {i - 1} = \\frac {\\left(\\sum_ {\\mathbb {B}} \\sum_ {\\mathcal {T}} \\bar {h} _ {i - 1}\\right) ^ {T} \\left(\\sum_ {\\mathbb {B}} \\sum_ {\\mathcal {T}} \\bar {h} _ {i - 1}\\right)}{(M | \\mathcal {T} |) ^ {2}}, \\mathcal {S} _ {i} = \\frac {\\left(\\sum_ {\\mathbb {B}} \\sum_ {\\mathcal {T}} s _ {i}\\right) \\left(\\sum_ {\\mathbb {B}} \\sum_ {\\mathcal {T}} s _ {i}\\right) ^ {T}}{M}\n$$\n\n$$\n\n\nHere,  $\\mathcal{T}$  represents the spatial size dimension for Batch Norm layer or for LayerNorm layer; it signifies the product of the number of heads and the per-head dimension.\n\n\nFor the proof of this proposition and the extended computation of other type of layers are given in Appendix A.2 and Section A.3. Note that in the context of online and stochastic optimization, the Kronecker factors for a given layer  $i$  can be estimated using an Exponentially Moving Average (EMA) scheme across batches, defined by\n\n\n$$\n\n$$\n\\mathcal {H} _ {i - 1} = \\gamma \\mathcal {H} _ {i - 2} + (1 - \\gamma) \\hat {\\mathcal {H}} _ {i - 1}, \\mathcal {S} _ {i} = \\gamma \\mathcal {S} _ {i - 1} + (1 - \\gamma) \\hat {\\mathcal {S}} _ {i}, \\tag {3}\n$$\n\n$$\n\n\nwhere  $0 < \\gamma < 1$  is the exponential decay factor, and  $\\hat{\\mathcal{H}}_{i-1}, \\hat{S}_i$  represent the current estimates of the Kronecker factors derived from the latest mini-batch of data. This EMA scheme is commonly used in methods involving diagonal or block-diagonal approximations to the curvature matrix (e.g. LeCun et al. (2012); Park et al. (2000); Schaul et al. (2013). Such schemes have the desirable property that they allow the curvature estimation to depend on much more data than what can be reasonably processed in a single mini-batch.\n\n\nInvestigation from Section 3.1 suggests that the FIM's critical information predominantly resides along its diagonal. Building upon these insights, we propose a novel approximation for the FIM that conceptualizes the Kronecker factors as diagonal matrices, denoted as  $\\tilde{F}_{D_i}$  for layer  $i$ .\n\n\nProposition 3.2. Assume that  $\\mathcal{H}_{i-1}$  and  $\\mathcal{S}_i$  can be closely approximated by diagonal matrices, denoted by  $\\mathcal{H}_{D_{i-1}}$  and  $\\mathcal{S}_{D_i}$  respectively at layer  $i$ , such that  $\\mathcal{H}_{D_{i-1}} = \\operatorname{Diag}(\\mathcal{H}_{i-1})$ ,  $\\mathcal{S}_{D_i} = \\operatorname{Diag}(\\mathcal{S}_i)$  where  $\\operatorname{Diag}$  denote the diagonal of a matrix. Then the Empirical FIM can be defined by\n\n\n$$\n\n$$\n\\tilde {F} _ {D _ {i}} \\triangleq \\mathcal {H} _ {D _ {i - 1}} ^ {\\prime} \\otimes \\mathcal {S} _ {D _ {i}} ^ {\\prime} + \\lambda , \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\mathcal{H}_{D_{i-1}}'$  and  $\\mathcal{S}_{D_i}'$  denote the Min-Max normalization of  $\\mathcal{H}_{D_{i-1}}$  and  $\\mathcal{S}_{D_i}$  (Patro & Sahu, 2015) and  $\\lambda$  is a regularization parameter.\n\n\nThis approximation strikes a balance between computational time and space complexity, and the accuracy of performance, as discussed in Section 4. We set the regularization parameter  $\\lambda = 0.001$ ,\n\n\nwhich acts as a damping factor following Tikhonov regularization principles, enhancing computational stability and conditioning of the FIM. For foundational details on  $\\lambda$ , refer to Martens & Grosse (2015) and for methodology, see Appendix A.2. The closed-form solution for the augmented gradient  $\\triangle \\theta_{t}$  is derived from the diagonal approximation of the FIM, given by  $\\triangle \\theta_{t} = \\tilde{F}_{D_{t}}^{-1}\\nabla J(\\theta_{t})$ . Detailed derivation in Appendix A.2, this represents the AdaFisher optimizer's update rule, focusing on the diagonal elements to reduce computational overhead while maintaining a reasonable FIM approximation. This strategic simplification enhances the efficiency of the optimization process, crucial for training deep neural networks where computational resources are limited.\n\n\nTable 1: Summary of the first, second moments, regret bound and Applicability used in Adam Kingma & Ba (2017), AdaHessian Yao et al. (2021), K-FAC Martens & Grosse (2020), Shampoo Gupta et al. (2018), and AdaFisher for updating model parameters  $\\theta_{t + 1} = \\theta_t - \\eta m_t / \\sqrt{v_t}$ . Here  $\\beta_{1}$  and  $\\beta_{2}$  are first and second moment hyperparameters.  $L_{t}$  and  $R_{t}$  refer to the preconditioning method used by Shampoo Gupta et al. (2018),  $g_{t} = \\mathrm{vec}(G_{t})$ , and  $T$  denotes the total number of steps. Note that Trans. denotes Transformers\n\n\n<table><tr><td rowspan=\"2\">Optimizer</td><td rowspan=\"2\">mt</td><td rowspan=\"2\">vt</td><td rowspan=\"2\">Regret Bound</td><td colspan=\"2\">Applicability</td><td></td></tr><tr><td>CNNs</td><td>Trans.</td><td></td></tr><tr><td>Adam</td><td>(1-\u03b21)\u03a3t i=1 \u03b21t-i g i 1-\u03b21t</td><td>(1-\u03b22)\u03a3t i=1 \u03b22t-i g i g i 1-\u03b22t</td><td>1/2</td><td>O(log T\u221aT)</td><td>\u2713</td><td>\u2713</td></tr><tr><td>AdaHessian</td><td>(1-\u03b21)\u03a3t i=1 \u03b21t-i g i 1-\u03b21t</td><td>(1-\u03b22)\u03a3t i=1 \u03b22t-i D i (s) D i (s) 1-\u03b22t</td><td>1/2</td><td>O(log T\u221aT)</td><td>\u2713</td><td>\u2713</td></tr><tr><td>K-FAC</td><td>F-1gt</td><td>1</td><td>O(\u221aT)</td><td>\u2713</td><td>\u00d7</td><td></td></tr><tr><td>Shampoo</td><td>Lt-1/4G tR t-1/4</td><td>1</td><td>O(\u221aT)</td><td>\u2713</td><td>\u00d7</td><td></td></tr><tr><td>AdaFisher</td><td>(1-\u03b21)\u03a3t i=1 \u03b21t-i g i 1-\u03b21t</td><td>F Dt</td><td>O(log T\u221aT)</td><td>\u2713</td><td>\u2713</td><td></td></tr></table>\n\n\n# 3.3 AUGMENTING FIM INTO ADAM\n\n\nAdam, which combines the methodologies of RMSProp and momentum (Sutskever et al., 2013), updates parameters following  $\\theta_{t + 1} = \\theta_t - \\alpha_t\\frac{m_t}{v_t}$ . Here,  $\\alpha_{t}$  represents the learning rate, while  $m_{t}$  and  $v_{t}$  denote the first and second moment estimates, respectively. Although Adam is widely used, its approximation of the second moment using simple diagonal elements of second order statistics through squared gradients (Kunstner et al., 2019) can mirror stability challenges observed in simpler methods like SGD (Ruder, 2016). To overcome these limitations, we introduce AdaFisher, which utilizes a more refined diagonal block-Kronecker approximation of the FIM, derived using the K-FAC framework. This enhancement significantly improves curvature understanding and optimization dynamics. AdaFisher distinguishes itself from Adam by incorporating a higher fidelity approximation of the FIM, enhancing both optimization efficiency and model robustness in complex scenarios. As demonstrated in Figure 4, AdaFisher's FIM values exhibit narrow variations and lower mean values during training, suggesting a convergence towards flatter local minima. In contrast, Adam shows broader variations, indicating less efficient convergence. For more details regarding the convergence behavior refer to Appendix B.1. Moreover, AdaFisher omits the square root and the traditional EMA applied over the second moment, since the FIM naturally incorporates an EMA of its Kronecker factors (detailed in Eq. (3)). The exclusion of the square root aligns with the theoretical definition of second order methods, as using a square root deviates from the second-order Taylor expansion approximation that these methods aim to follow. A comparative summary of different moment estimates,  $m_{t}$  and  $v_{t}$ , along with their regret bounds and applicability across various optimizers, is presented in Table 1. Building on the principles of AdamW (Loshchilov & Hutter, 2019), which modifies Adam by integrating weight decay directly into the weight update step to counteract suboptimal decay behaviors and b we introduce AdaFisherW. This variant adapts the AdamW framework\n\n\n![](images/557a4e8d2a56f3e2e60d4dce302ec4041a5e11ef848ce11f6fc514964d829540.jpg)\n\n\n![Figure 4: Comparison of FIM Diagonal Histograms during ResNet18 Training on CIFAR10: The figure displays the FIM diagonal elements for the first convolutional layer with Adam and AdaFisher over 1,000 training iterations.](images/c93ab72ab56c51520670894c8e0e185cfd050c54a539793eecd272b08fd129b0.jpg)\n\n\nmizer by leveraging curvature information from AdaFisher. Finally, AdaFisher is compatible with multi-GPU environments, with a distributed version detailed in Appendix A.4. The implementation for both AdaFisher<sup>1</sup> variants are delineated in the pseudo-code presented in Algorithm 1.\n\n\n# 3.4 CONVERGENCE ANALYSIS\n\n\nIn this section, we provide a theoretical analysis of AdaFisher's convergence in both convex optimization and non-convex stochastic optimization. We first present a standard convergence behaviour of Eq. (2) for a simple strongly convex and strictly smooth function  $f(J)$ .\n\n\nProposition 3.3 (Convergence in convex optimization). For FIM defined in Eq. (4), the updating scheme  $\\triangle \\theta_{t} = \\tilde{F}_{t}^{-1}\\nabla J(\\theta_{t})$  converges. Further, if  $\\nabla J$  is Lipschitz, the convergence rate is bounded.\n\n\nFor non-convex case, we adopt the similar derivations of Chen et al. (2019), since AdaFisher belongs to the family of generalized Adam-type methods.\n\n\nProposition 3.4 (Convergence in non-convex stochastic optimization). Under the assumptions:  \n(i)  $J$  is lower bounded and differentiable;  $\\| \\nabla J(\\theta) - \\nabla J(\\theta')\\|_2 \\leq L\\|\\theta - \\theta'\\|_2$ ,  $\\| \\tilde{F}_{D_t}\\|_\\infty < L$ ,  $\\forall t, \\theta, \\theta'$ , (ii) Both the true and stochastic gradient are bounded, i.e.,  $\\| \\nabla J(\\theta_t)\\|_2 \\leq \\lambda$  and  $\\| g_t\\|_2 \\leq \\lambda$ ,  $\\forall t$  for some  $\\lambda > 0$ , (iii) Unbiased and independent noise in  $g_t$ , i.e.,  $g_t = \\nabla J(\\theta_t) + \\zeta_t$ ,  \n $\\mathbb{E}[\\zeta_t] = 0$ , and  $\\zeta_i \\perp \\zeta_j$ ,  $\\forall i \\neq j$ . Assume  $\\eta_t = \\frac{\\eta}{\\sqrt{t}}$ ,  $\\beta_t \\leq \\beta \\leq 1$  is non-increasing,  $\\frac{\\tilde{F}_{D_{t-1}}[j]}{\\eta_{t-1}} \\leq \\frac{\\tilde{F}_{D_t}[j]}{\\eta_t}$ ,  $\\forall t \\in [T]$ ,  $j \\in [d]$ , we then have\n\n\n$$\n\n$$\n\\min  _ {t \\in [ T ]} \\mathbb {E} [ | | \\nabla J (\\theta_ {t}) | | _ {2} ^ {2} ] \\leq \\frac {L}{\\sqrt {T}} \\left(C _ {1} \\eta^ {2} \\lambda^ {2} (1 + \\log T) + C _ {2} d \\eta + C _ {3} d \\eta^ {2} + C _ {4}\\right) \\tag {5}\n$$\n\n$$\n\n\nwhere  $C_1, C_2, C_3$  are constants independent of  $d$  and  $T$ ,  $C_4$  is a constant independent of  $T$ , the expectation is taken w.r.t all the randomness corresponding to  $\\{g_t\\}$ .\n\n\nProposition 3.4 implies the convergence rate for AdaFisher in the non-convex case is at  $O(\\log T / \\sqrt{T})$ , which is similar to Adam-type optimizer. While DNNs often include nonsmooth components like ReLU and max pooling, which create nondifferentiable points in the loss landscape, optimizers like AdaFisher handle these cases effectively as shown by our results in Section 4.\n\n\nAlgorithm 1 AdaFisher optimization algorithm. Good default settings for the tested machine learning problems are  $\\alpha = 0.001$  (learning rate),  $\\lambda = 0.001$  (Tikhonov damping parameter),  $\\gamma = 0.92$  (Exponentially decaying factor). [Default parameters are:  $\\beta = 0.9$  (Exponentially decaying factor of Adam),  $\\kappa$  (weight decay) (Kingma & Ba (2017), Loshchilov & Hutter (2019))].\n\n\nRequire: Step size  $\\alpha$ ; Exponential decay rate for Kronecker factors  $\\gamma \\in [0,1)$ ; Tikhonov damping parameter  $\\lambda$ ; Exponential decay rate for first moments  $\\beta$  in  $[0,1)$ ; Initial parameters  $\\theta$  Initialize 1st moment variable  $m = 0$ ; FIM  $\\tilde{F}_{D_i} = \\mathbf{I}$ ; time step  $t = 0$\n\n- 1: while stopping criterion not met do\n\n- 2: Sample a minibatch of  $M$  examples from the training set  $\\{(x^{(i)},y^{(i)})\\}_{i = 1}^{m}$\n\n- 3: Compute  $\\mathcal{H}_{D_{i - 1}},\\mathcal{S}_{D_i}$  for  $i\\in \\{1,\\dots ,L\\}$  using Section A.3 (notice that:  $\\mathcal{H}_{D_0} = x$\n\n- 4: Compute EMAs of  $\\mathcal{H}_{D_{i-1}}$  and  $\\mathcal{S}_{D_i}$  using Eq.(3)\n\n- 5: Compute  $\\tilde{F}_{D_i}$  for  $i\\in \\{1,\\dots ,L\\}$  using Eq. (4)\n\n- 6:  $h_t \\gets \\frac{1}{M} \\sum_i \\nabla_{\\theta_t} \\mathcal{L}(f(x^{(i)}; \\theta_t), y^{(i)})$  (Compute gradient)\n\n- 7:  $m_{t + 1}\\gets \\frac{\\beta m_t + (1 - \\beta)h_t}{1 - \\beta^t}$  (Update and correct biased first moment)\n\n- 8: Case AdaFisher:  $\\Delta \\theta_{t} = -\\alpha \\tilde{F}_{D_{t}}^{-1}m_{t}$ ; Case AdaFisherW:  $\\Delta \\theta_{t} = -\\alpha \\left(\\tilde{F}_{D_{t}}^{-1}m_{t} + \\kappa \\theta_{t}\\right)$\n\n- 9:  $\\theta_{t + 1}\\gets \\theta_t + \\Delta \\theta_t$  (Apply update)\n\n- 10:  $t\\gets t + 1$\n\n- 11: end while\n\n\n# 4 RESULTS\n\n\nTo evaluate AdaFisher, we conduct experiments on six benchmark datasets across Image Classification for CV and Language Modelling for NLP that are commonly used to evaluate optimization\n\n\nalgorithms: CIFAR-10, CIFAR100 (Krizhevsky et al., 2009), TinyImageNet (Le & Yang, 2015), and ImageNet (Deng et al., 2009) for image classification; Wikitext-2 (Merit et al., 2016) and Penn Treebank (PTB) (Marcus et al., 1993) for language modelling. The six baseline methods we compare with are SGD, Adam/AdamW, K-FAC, AdaHessian, and Shampoo. For CIFAR experiments, we report the average over 5 runs. We also perform a transfer learning task using the ImageNetV1 weights Paszke et al. (2019). Detailed descriptions of the experimental setups (including hyperparameters, datasets, and data augmentation), results, and analyses are provided in Appendix D.\n\n\nTable 2: Performance metrics (mean, std) of different networks and optimizers on CIFAR10 and CIFAR100 using batch size 256 with a 200-epoch AdaFisher training cutoff.\n\n\n<table><tr><td rowspan=\"2\">Network</td><td colspan=\"6\">CIFAR10</td><td colspan=\"6\">CIFAR100</td></tr><tr><td>SGD</td><td>Adam</td><td>AdaHessian</td><td>K-FAC</td><td>Shampoo</td><td>AdaFisher</td><td>SGD</td><td>Adam</td><td>AdaHessian</td><td>K-FAC</td><td>Shampoo</td><td>AdaFisher</td></tr><tr><td>ResNet18</td><td>95.640.1</td><td>94.850.1</td><td>95.440.1</td><td>95.170.2</td><td>94.080.2</td><td>96.250.2</td><td>76.560.2</td><td>75.740.1</td><td>71.790.2</td><td>76.030.3</td><td>76.780.2</td><td>77.280.2</td></tr><tr><td>ResNet50</td><td>95.710.1</td><td>94.450.2</td><td>95.540.1</td><td>95.660.1</td><td>94.590.1</td><td>96.340.2</td><td>78.010.1</td><td>74.650.5</td><td>75.810.3</td><td>77.400.4</td><td>78.070.4</td><td>79.770.4</td></tr><tr><td>ResNet101</td><td>95.980.2</td><td>94.570.1</td><td>95.290.6</td><td>96.010.1</td><td>94.630.1</td><td>96.390.1</td><td>78.890.2</td><td>75.560.3</td><td>73.380.2</td><td>77.010.4</td><td>78.830.2</td><td>80.650.4</td></tr><tr><td>DenseNet121</td><td>96.090.1</td><td>94.860.1</td><td>96.110.1</td><td>96.120.1</td><td>95.660.1</td><td>96.720.1</td><td>80.130.4</td><td>75.870.4</td><td>74.800.9</td><td>79.790.2</td><td>80.240.3</td><td>81.360.3</td></tr><tr><td>MobileNetV3</td><td>94.430.2</td><td>93.320.1</td><td>92.863.1</td><td>94.340.1</td><td>93.810.2</td><td>95.280.1</td><td>73.890.3</td><td>70.620.3</td><td>56.584.5</td><td>73.750.3</td><td>70.850.3</td><td>77.560.1</td></tr><tr><td>Tiny Swin</td><td>82.340.2</td><td>87.370.6</td><td>84.150.2</td><td>64.790.5</td><td>63.910.4</td><td>88.740.4</td><td>54.890.4</td><td>60.210.4</td><td>56.860.5</td><td>34.450.4</td><td>30.391.2</td><td>66.050.5</td></tr><tr><td>FocalNet</td><td>82.030.2</td><td>86.230.1</td><td>64.180.2</td><td>38.940.8</td><td>37.960.7</td><td>87.900.1</td><td>47.7603</td><td>52.710.5</td><td>32.330.3</td><td>9.980.6</td><td>9.180.1</td><td>53.690.3</td></tr><tr><td>CCT-2/3\u00d72</td><td>78.760.3</td><td>83.890.4</td><td>-</td><td>33.082.3</td><td>35.160.4</td><td>84.940.3</td><td>54.050.4</td><td>59.780.5</td><td>-</td><td>7.170.2</td><td>8.600.1</td><td>62.910.5</td></tr></table>\n\n\n# 4.1 IMAGE CLASSIFICATION\n\n\nWe commence our analysis by assessing the convergence and generalization capabilities of various models on image classification tasks. Specifically, we deploy ResNet architectures (ResNetX where  $X \\in \\{18, 50, 101\\}$ ), MobileNetV3 (Howard et al., 2019), Tiny Swin (Liu et al., 2021), FocalNet (Yang et al., 2022) and CCT-2/3\u00d72 (Hassani et al., 2021) on CIFAR10 and CIFAR100, while utilizing standard ResNet50 for TinyImageNet and ImageNet. The performance outcomes for CIFAR datasets are detailed in Table 2. Our empirical evaluation of AdaFisher optimizer across these models and datasets illustrates its efficiency in optimizing in image classification, surpassing contemporary SOTA optimizers, including SGD, Adam, AdamW, AdaHessian, Shampoo, and K-FAC. We employ the Wall-Clock-Time (WCT) method with a cutoff of 200 epochs for AdaFisher's training, except for ImageNet, where we use a 90-epoch WCT for Adam, which surprisingly matched AdaFisher's training duration. Results confirm AdaFisher's superior classification accuracy on both CNNs and VITs. Please note that the results for Tiny ImageNet are described in Appendix D.2.4.\n\n\nImageNet Training. Training on ImageNet typically requires multiple GPUs and large batch sizes. Our study showcases that AdaFisher achieves superior validation accuracy on a single GPU than its counterparts in scenarios marked by the light blue region. This performance outstrips traditional approaches like SGD, LAMB (You et al., 2019), and LARS (You et al., 2017), which typically utilize batch sizes of 16K. While AdaFisher attains SOTA results on a single GPU, it further excels when scaled up in a distributed setting with larger batch sizes. The results, benchmarked using 256 batch size and a WCT of 90 Adam training epochs, are detailed in Table 3 and illustrated in Figure 5. Distributed AdaFisher curves are illustrated in Figure 15. The light blue highlights in the table represent our experiments with a batch size of 256 on a single GPU. The light green indicates results from a distributed version of AdaFisher employing larger batch sizes, whereas the orange reflects results from SOTA methods using a higher batch size of 16K, SGD with a batch size of 256 and AdamW with batch size of 1024. Overall, AdaFisher demonstrates robust generalization capabilities, in contrast to K-FAC, which tends to overfit.\n\n\nTable 3: Validation of ImageNet-1K / ResNet50 by different optimizers reported on Top-1 and Top-5 accuracy.\n\n\n<table><tr><td>Optimizers</td><td>Batch size</td><td>Top-1</td><td>Top-5</td></tr><tr><td>Adam</td><td>256</td><td>67.78</td><td>88.37</td></tr><tr><td>K-FAC</td><td>256</td><td>70.96</td><td>89.44</td></tr><tr><td>Shampoo</td><td>256</td><td>72.82</td><td>91.42</td></tr><tr><td>AdaFisher</td><td>256</td><td>76.95</td><td>93.39</td></tr><tr><td>AdaFisher</td><td>512</td><td>77.01</td><td>93.45</td></tr><tr><td>AdaFisher</td><td>1024</td><td>77.09</td><td>93.56</td></tr><tr><td>SGD Goyal et al. (2017)</td><td>256</td><td>76.40</td><td>-</td></tr><tr><td>AdamW Chen et al. (2024)</td><td>1024</td><td>76.34</td><td>-</td></tr><tr><td>LAMB You et al. (2019)</td><td>16K</td><td>76.66</td><td>93.22</td></tr><tr><td>SGD You et al. (2019)</td><td>16K</td><td>75.20</td><td>-</td></tr><tr><td>LARS Huo et al. (2021)</td><td>16K</td><td>75.1</td><td>-</td></tr></table>\n\n\n![Figure 5: Training loss and validation error of ResNet-50 on ImageNet. AdaFisher consistently achieves lower test error as compared to its counterparts.](images/f9df22dfa9c59a5f967c0f5fbd581982a953a1d31fae2db35154dc29e60fbbee.jpg)\n\n\n![](images/f9fe1f44e7fb69988253a45fa3f61ac39c4a701c2b2cf838afb429e227abee65.jpg)\n\n\n# 4.2 TRANSFER LEARNING\n\n\nOwing to global warming concerns, more sustainable practices are essential in training DNNs. Employing pretrained models from ImageNet-1k by PyTorch for transfer learning on datasets like CIFAR10 and CIFAR100 reduces the carbon footprint and leverages prior computations, promoting eco-friendly AI methodologies. We applied these pretrained weights across various CNN architectures, to train on these datasets. The results, presented in Table 4, highlight the significant advantages of using AdaFisher, consistently achieving top accuracy across both datasets. More details can be found in Appendix D.2.3.\n\n\nTable 4: Performance comparison of different networks and optimizers on CIFAR10 and CIFAR100 using ImageNet-1K pretrained weights. Evaluation is based on wall clock time of 50 training epochs with AdaFisher.\n\n\n<table><tr><td rowspan=\"2\">Network</td><td colspan=\"6\">CIFAR10</td><td colspan=\"6\">CIFAR100</td></tr><tr><td>SGD</td><td>Adam</td><td>AdaHessian</td><td>K-FAC</td><td>Shampoo</td><td>AdaFisher</td><td>SGD</td><td>Adam</td><td>AdaHessian</td><td>K-FAC</td><td>Shampoo</td><td>AdaFisher</td></tr><tr><td>ResNet50</td><td>96.500.2</td><td>96.450.2</td><td>96.350.3</td><td>96.450.1</td><td>96.030.4</td><td>97.130.2</td><td>82.120.1</td><td>82.010.4</td><td>80.640.9</td><td>80.550.4</td><td>81.700.2</td><td>82.230.2</td></tr><tr><td>ResNet101</td><td>97.070.2</td><td>96.700.1</td><td>96.650.2</td><td>96.840.1</td><td>96.630.1</td><td>97.220.1</td><td>84.010.1</td><td>82.430.2</td><td>81.360.8</td><td>82.260.3</td><td>82.650.2</td><td>84.470.2</td></tr><tr><td>DenseNet121</td><td>94.800.1</td><td>94.770.1</td><td>93.080.1</td><td>94.410.2</td><td>94.760.1</td><td>95.030.1</td><td>75.980.2</td><td>75.650.3</td><td>71.060.9</td><td>76.100.3</td><td>76.080.2</td><td>76.920.3</td></tr><tr><td>MobileNetV3</td><td>91.760.3</td><td>90.920.3</td><td>86.452.5</td><td>91.720.2</td><td>91.390.3</td><td>92.780.2</td><td>71.860.4</td><td>66.110.8</td><td>59.692.3</td><td>69.850.4</td><td>68.870.3</td><td>72.380.4</td></tr></table>\n\n\n# 4.3 LANGUAGE MODEL\n\n\nWe employ the WikiText-2 dataset, which encompasses approximately 100 million tokens derived from over 2 million words extracted from a curated set of 'Good' and 'Featured' articles on Wikipedia. Additionally, we utilize the PTB dataset, renowned for its extensive collection of English words with part-of-speech tags, which has been widely used in natural language processing tasks for training and benchmarking language models. Our experiments utilize a scaled-down version of GPT-1 (Radford et al., 2019), featuring four self-attention layers with masking capabilities with a total of 28,351,488 learnable parameters. More details about hyperparameters and model can be found in Appendix D.3. The corresponding to the best-performing model during validation, is c to approaches in image classification, we apply the WCT method 'AdaFisher as the cutoff period. Notice that Shampoo did not achieve timal hyperparameters, and the K-FAC was unable to train with AS.\n\n\nTable 5: Language Modelling performance (PPL) on Wikitext2 and PTB test dataset (lower is better).\n\n\n<table><tr><td rowspan=\"2\">Optimizer</td><td colspan=\"2\">Test PPL</td></tr><tr><td>WikiText-2</td><td>PTB</td></tr><tr><td>AdamW</td><td>175.06</td><td>44.70</td></tr><tr><td>AdaHessian</td><td>407.69</td><td>59.43</td></tr><tr><td>Shampoo</td><td>1727.75</td><td>-</td></tr><tr><td>AdaFisherW</td><td>152.72</td><td>41.15</td></tr></table>\n\n\n![Adam](images/1c2c31e502bafb6865236bd59e393f4d09f96cbdff426f5250d0f56fe4ae83bf.jpg)\n\n\n![Figure 6: Performance comparison of AdaFisher and other optimizers using the ResNet50 network on the CIFAR100 dataset. (A) Test accuracy by batch size. (B) Accuracy vs. learning rates. (C) Accuracy related to epoch time across batch sizes. (D) Epoch time for different optimizers with batch size of 256.](images/f353505a38c9b0c22de624d7da1a35e8406a26041f73f8b4723b26c89f79ae8a.jpg)\n\n\n![](images/26559768e1e5fef9d2a63b94775a1bf1b516c05353b72513f528ca3d15d4294a.jpg)\n\n\n![](images/ee962f946eed698cc5bca0253456e6ad9a1621d5bf2fc33b190a760e460a9f07.jpg)\n\n\n# 4.4 STABILITY ANALYSIS\n\n\nIn this section, we assess AdaFisher's stability under varying learning rates and batch sizes using ResNet50 on CIFAR100 and compare its performance to other optimizers. Improved stability indicates a reduced need for hyperparameter tuning while maintaining high performance. To ensure a fair comparison, all methods were evaluated using a consistent experimental setup, with parameters tailored to each optimizer's strengths. However, we exclude AdaHessian results for a batch size of 1024 due to its significant computational cost.\n\n\nBatch Size Analysis. We examine the impact of batch size on AdaFisher's performance, as shown in Panels (A) and (C) of Figure 6. AdaFisher maintains high test accuracy across various batch\n\n\nsizes, excelling particularly at smaller sizes despite some sensitivity to larger ones. Panel (C) highlights AdaFisher's efficiency, achieving high accuracy with shorter epoch times compared to Adam, detailed further in Panel (D) where AdaFisher shows competitive epoch durations against other optimizers. These results, discussed in Appendix D.2.7, underscore AdaFisher's effective performance across batch size variations without adjusting other hyperparameters.\n\n\nLearning Rate Stability. This analysis evaluates the impact of learning rate variations on AdaFisher's performance, as depicted in Panel (B) of Figure 6. AdaFisher demonstrates superior stability, particularly at lower learning rates, maintaining consistent performance across a broad spectrum. This stability alleviates the need for meticulous learning rate adjustments, thereby streamlining model training in various computational environments. Additionally, AdaFisher's stability across various learning rates can be attributed to its effective approximation of the curvature matrix.\n\n\nAblation Studies. We further conduct extensive ablation studies on additional components of AdaFisher, including the convergence efficiency, our novel approximation of the FIM, the significance of EMA for Kronecker factors, the impact of the square root, the stability across learning rate schedulers and the updated computation of the FIM for normalization layers. These analyses are thoroughly detailed in Appendix B.\n\n\n# 5 RELATED WORK\n\n\nTractable Approximation of the FIM. Efficient approximations of the FIM for neural network optimization have evolved significantly, beginning with block-diagonal strategies exemplified by TONGA (Roux et al., 2007) and extending to the Kronecker-factored approaches like K-FAC. Further innovations have emerged, such as SK-FAC (Tang et al., 2021), EVA (Zhang et al., 2023), which accelerates the computation of the FIM, and Eschenhagen et al. (2023), who propose a generalized framework for FIM computation that enhances preconditioning methods like Shampoo. More recently, Liu et al. (2023), Huang et al. (2024) and Duvvuri et al. (2024) have introduced tractable solutions for computing the FIM. AdaFisher distinguishes itself by integrating enhanced FIM computations with novel diagonal Kronecker factors, enriching the Adam optimization framework. This integration, outlined in Proposition 3.2 and detailed in Appendix A.3, advances the fusion of second-order optimization principles with first-order methods. This builds upon innovations like AdaHessian, which incorporates Hessian diagonals into the Adam framework.\n\n\nAdaptive First-Order Methods. Building upon the diagonal approximation heritage of the FIM, AdaFisher extends traditional diagonally-scaled first-order methods such as AdaGrad (Duchi et al., 2011), AdamP, AdaInject, AdaBelief, and Adam. These methods have inspired advancements like AMSGrad (Reddi et al., 2019), AdaBound (Luo et al., 2019), RAdam (Liu et al., 2019), and enhanced AdaBelief, FOOF (Benzing, 2021) and INNAprop (Bolte et al., 2024), improving both theoretical rigor and practical effectiveness. A recent study by Jiang et al. (2024a) illustrates that first-order adaptive methods can bias training trajectories and effectively navigate through saddle points. In response, Mishchenko & Stich (2023) propose an empirical solution involving the addition of noise to mitigate these biases. Leplat et al. (2022) introduces a novel method accelerating convergence via Gauss-Seidel type discretization. AdaFisher differentiates itself by eliminating the conventional square root in the second moment calculation, with benefits underscored by Lin et al. (2024a) and Malladi et al. (2022) in CNN architectures, and Zhang et al. (2024) demonstrates the critical role of Adam family optimizers in Transformer models. Its unique preconditioning, based on the Fisher Information, is elaborated in Algorithm 1.\n\n\n# 6 CONCLUSION, LIMITATIONS AND FUTURE RESEARCH\n\n\nIn this work, we introduced AdaFisher, an adaptive optimizer that leverages a novel diagonal block-Kronecker approximation of the FIM to improve gradient rescaling and descent directions. Incorporated within the Adam framework, AdaFisher speeds up training, reduces hyperparameter sensitivity, and delivers higher accuracy and stability across image classification and language modeling tasks. Empirical and theoretical analyses demonstrate its superiority over current optimizers, with efficient space and time usage facilitating its application across diverse tasks. Notably, AdaFisher excels in SOTA comparisons on ImageNet under both single and multi-GPU setups. Although optimized for statistical tasks, AdaFisher is less suited for tasks involving non-exponential loss families due to its reliance on statistical data for FIM computation. Future work will expand testing to other\n\n\n# REFERENCES\n\n- iris, 2018. URL https://dx.doi.org/10.21227/rz7n-kj20.\n\n- Shun-ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 10(2):251-276, Feb 1998. ISSN 0899-7667. doi: 10.1162/089976698300017746.\n\n- Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry. 2000. URL https://api-semanticscholar.org/CorpusID:116976027.\n\n- Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\n\n- Frederik Benzing. Kronecker-factored second-order optimizers perform first-order descent on neurons. 2021.\n\n- J\u00e9r\u00f4me Bolte, Ryan Boustany, Edouard Pauwels, and Andrei Purica. A second-order-like optimizer with adaptive gradient scaling for deep learning. arXiv preprint arXiv:2410.05871, 2024.\n\n- Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\n\n- Johan Braeken and Marcel ALM Van Assen. An empirical kaiser criterion. *Psychological methods*, 22(3):450, 2017.\n\n- Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. Advances in Neural Information Processing Systems, 34:22405-22418, 2021.\n\n- Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. Advances in neural information processing systems, 36, 2024.\n\n- Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type algorithms for non-convex optimization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1x-x309tm.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255, 2009. doi: 10.1109/CVPR.2009.5206848.\n\n- Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE Signal Processing Magazine, 29(6):141-142, 2012. doi: 10.1109/MSP.2012.2211477.\n\n- Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.\n\n- Shiv Ram Dubey, SH Shabbeer Basha, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Adinject: Injection based adaptive gradient descent optimizers for convolutional neural networks. IEEE Transactions on Artificial Intelligence, 2022.\n\n- John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(61):2121-2159, 2011. URL http://jmlr.org/papers/v12/duchi11a.html.\n\n- Sai Surya Duvvuri, Fnu Devvrit, Rohan Anil, Cho-Jui Hsieh, and Inderjit S Dhillon. Combining axes preconditioners through kronecker approximation for deep learning. In The Twelfth International Conference on Learning Representations, 2024.\n\n- Muhammad ElNokrashy, Badr AlKhamissi, and Mona Diab. Depth-wise attention (dwatt): A layer fusion method for data-efficient classification. arXiv preprint arXiv:2209.15168, 2022.\n\n- Runa Eschenhagen, Alexander Immer, Richard E Turner, Frank Schneider, and Philipp Henning. Kronecker-factored approximate curvature for modern neural network architectures. arXiv preprint arXiv:2311.00636, 2023.\n\n- Runa Eschenhagen, Alexander Immer, Richard Turner, Frank Schneider, and Philipp Hennig. Kronecker-factored approximate curvature for modern neural network architectures. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization, 2021.\n\n- Karl Pearson F.R.S. Lii, on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559-572, 1901. doi: 10.1080/14786440109462720.\n\n- Thomas George. NNGeometry: Easy and Fast Fisher Information Matrices and Neural Tangent Kernels in PyTorch, February 2021. URL https://doi.org/10.5281/zenodo.4532597.\n\n- Priya Goyal, Piotr D\u00f3lar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n\n- Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution layers, 2016.\n\n- Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization, 2018.\n\n- Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu AbuDuweili, Jiachen Li, and Humphrey Shi. Escaping the big data paradigm with compact transformers. 2021. URL https://arxiv.org/abs/2104.05704.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, and Jung-Woo Ha. Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights. arXiv preprint arXiv:2006.08217, 2020.\n\n- Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent.\n\n- Richard A. Holmgren. Newton's Method, pp. 127-151. Springer New York, New York, NY, 1996. ISBN 978-1-4419-8732-7. doi: 10.1007/978-1-4419-8732-7_12. URL https://doi.org/10.1007/978-1-4419-8732-7_12.\n\n- Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 2 edition, 2012.\n\n- Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig Adam. Searching for mobilenetv3, 2019.\n\n- Keke Huang, Ruize Gao, Bogdan Cautis, and Xiaokui Xiao. Scalable continuous-time diffusion framework for network inference and influence estimation, 2024.\n\n- Zhouyuan Huo, Bin Gu, and Heng Huang. Large batch optimization for deep learning using new complete layer-wise adaptive rate scaling. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 7883-7890, 2021.\n\n- Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift, 2015.\n\n- Kaiqi Jiang, Dhruv Malik, and Yuanzhi Li. How does adaptive optimization impact local neural network geometry? Advances in Neural Information Processing Systems, 36, 2024a.\n\n- Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Pan. Pre-rmsnorm and pre-crmsnorm transformers: equivalent and efficient pre-ln transformers. Advances in Neural Information Processing Systems, 36, 2024b.\n\n- Jack Kiefer and Jacob Wolfowitz. Stochastic estimation of the maximum of a regression function. The Annals of Mathematical Statistics, pp. 462-466, 1952.\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\n- Frederik Kunstner, Philipp Hennig, and Lukas Balles. Limitations of the empirical fisher approximation for natural gradient descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/46a558d97954d0692411c861cf78ef79-Paper.pdf.\n\n- Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.\n\n- Yann Lecun. A theoretical framework for back-propagation. 08 2001.\n\n- Yann A. LeCun, L\u00e9on Bottou, Genevieve B. Orr, and Klaus Robert M\u00fcller. Efficient backprop, pp. 9-48. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Springer Verlag, 2012. ISBN 9783642352881. doi: 10.1007/978-3-642-35289-8_3. Copyright: Copyright 2021 Elsevier B.V., All rights reserved.\n\n- Valentin Leplat, Daniil Merkulov, Aleksandr Katrutsa, Daniel Bershatsky, Olga Tsymboi, and Ivan Oseledets. Nag-gs: Semi-implicit, accelerated and robust stochastic optimizer. arXiv preprint arXiv:2209.14937, 2022.\n\n- Wu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E Turner, and Alireza Makhzani. Can we remove the square-root in adaptive gradient methods? a second-order perspective. arXiv preprint arXiv:2402.03496, 2024a.\n\n- Wu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E. Turner, and Alireza Makhzani. Can we remove the square-root in adaptive gradient methods? a second-order perspective, 2024b.\n\n- Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training. arXiv preprint arXiv:2305.14342, 2023.\n\n- Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019.\n\n- Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 10012-10022, 2021.\n\n- Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\n\n- Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic bound of learning rate, 2019.\n\n- Linjian Ma, Gabe Montague, Jiayu Ye, Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W. Mahoney. Inefficiency of k-fac for large batch size training, 2019.\n\n- Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On the sdes and scaling rules for adaptive gradient algorithms. Advances in Neural Information Processing Systems, 35: 7697-7711, 2022.\n\n- Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330, 1993. URL https://aclanthology.org/J93-2004.\n\n- James Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21(146):1-76, 2020.\n\n- James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2408-2417, Lille, France, 07-09 Jul 2015. PMLR. URL https://proceedings.mlrpress/v37/martens15.html.\n\n- James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature, 2020.\n\n- Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.\n\n- Konstantin Mishchenko and Sebastian U Stich. Noise injection irons out local minima and saddle points. In OPT 2023: Optimization for Machine Learning, 2023.\n\n- Alan V. Oppenheim, Ronald W. Schafer, and John R. Buck. Discrete-Time Signal Processing. Prentice-hall Englewood Cliffs, second edition, 1999.\n\n- Kazuki Osawa, Satoki Ishikawa, Rio Yokota, Shigang Li, and Torsten Hoefler. Asdl: A unified interface for gradient preconditioning in pytorch, 2023.\n\n- H Park, S.-I Amari, and K Fukumizu. Adaptive natural gradient learning algorithms for various stochastic models. Neural Networks, 13(7):755-764, 2000. ISSN 0893-6080. doi: https://doi.org/10.1016/S0893-6080(00)00051-4. URL https://www.sciencedirect.com/science/article/pii/S0893608000000514.\n\n- Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n\n- SGOPAL Patro and Kishore Kumar Sahu. Normalization: A preprocessing stage. arXiv preprint arXiv:1503.06462, 2015.\n\n- K. B. Petersen and M. S. Pedersen. The matrix cookbook, October 2008. URL http://www2.imm.dtu.dk/pubdb/p.php?3274. Version 20081110.\n\n- Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\n\n- Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237, 2019.\n\n- Nicolas Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gradient algorithm. Advances in neural information processing systems, 20, 2007.\n\n- Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.\n\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.\n\n- Ernest Ryu and Stephen Boyd. A primer on monotone operator methods survey. Applied and computational mathematics, 15:3-43, 01 2016.\n\n- Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates, 2013.\n\n- Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost, 2018.\n\n- Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 1139-1147, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL https://proceedings.mlr.press/v28/sutskever13.html.\n\n- Ryo Takahashi, Takashi Matsubara, and Kuniaki Uehara. Data augmentation using random image cropping and patching for deep cnns. IEEE Transactions on Circuits and Systems for Video Technology, 30(9):2917-2931, September 2020. ISSN 1558-2205. doi: 10.1109/tcsvt.2019.2935128. URL http://dx.doi.org/10.1109/TCSVT.2019.2935128.\n\n- Zedong Tang, Fenlong Jiang, Maoguo Gong, Hao Li, Yue Wu, Fan Yu, Zidong Wang, and Min Wang. Skfac: Training neural networks with faster kronecker-factored approximate curvature. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13479-13487, 2021.\n\n- Ross Wightman, Hugo Touvron, and Herv\u00e9 J\u00e9gou. Resnet strikes back: An improved training procedure in timm. arXiv preprint arXiv:2110.00476, 2021.\n\n- Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning, 2018.\n\n- Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, and Jianfeng Gao. Focal modulation networks, 2022.\n\n- Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney. Adahessian: An adaptive second order optimizer for machine learning. In proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 10665-10673, 2021.\n\n- Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks, 2017.\n\n- Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.\n\n- Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for nonconvex optimization. Advances in neural information processing systems, 31, 2018.\n\n- Matthew D. Zeiler. Adadelta: An adaptive learning rate method, 2012.\n\n- Lin Zhang, Shaohuai Shi, and Bo Li. Eva: A general vectorized approximation framework for second-order optimization. arXiv preprint arXiv:2308.02123, 2023.\n\n- Yushun Zhang, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, and Zhi-Quan Luo. Why transformers need adam: A hessian perspective. arXiv preprint arXiv:2402.16788, 2024.\n\n- Zhilu Zhang and Mert R. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels, 2018.\n\n- Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18795-18806. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638582.167568", "msecs": "167.0", "relativeCreated": "178998.06547164917", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models\n\n# Abstract\nWhile large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing methods. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, demonstrates the superiority and versatility of LATS for both reasoning and acting. In particular, LATS achieves 94.4\\% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop, demonstrating the effectiveness and generality of our method.\n\n\n# 1 INTRODUCTION\n\n\nGeneral autonomous agents capable of reasoning and decision-making in a variety of environments (Wooldridge & Jennings, 1995) have been of longstanding interest in the field of artificial intelligence. While this has traditionally been studied in reinforcement learning, the recent rise of large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; OpenAI, 2023) with strong reasoning and general adaptability offers an alternative paradigm. Not only have LLMs excelled on standard NLP tasks such as text summarization (Nallapati et al., 2016) or natural language inference (Bowman et al., 2015), but they have been adapted to an increasingly diverse set of tasks that often require advanced common-sense reasoning or quantitative skills (Cobbe et al., 2021; Saparov & He, 2022). LLMs are also capable of performing in complex environments that involve knowledge and reasoning, such as web navigation (Yao et al., 2022; Deng et al., 2023), tool-use (Schick et al., 2023), or open-ended games (Fan et al., 2022).\n\n\nReasoning and acting abilities have also been improved by prompting techniques that augment LLMs with feedback or observations from an external environment (Yao et al., 2023b; Gao et al., 2022; Shinn et al., 2023). This eliminates the need to rely entirely on the base abilities of the Language Model (LM), enhancing it through external tools or semantic feedback. Despite this strength, these methods are reflexive and fall short of humans' deliberate and thoughtful decision-making characteristics to solve problems (Sloman, 1996; Evans, 2010). In particular, such methods fail to consider multiple reasoning paths or to plan ahead. Recent search-guided LLM works (Xie et al., 2023; Yao et al., 2023a; Hao et al., 2023) address this\n\n\n![Figure 1: An overview of LATS. LATS uses an external environment and self-reflection to improve reasoning and decision-making. Prior states, values, and reflections are stored in external memory.](images/cc628a678fe24b4a13746b69096509f9f561c1bd0ece32cfc17a423a2e48ed1c.jpg)\n\n\n<table><tr><td>Approach</td><td>Reasoning</td><td>Acting</td><td>Planning</td><td>Self Reflection</td><td>External Memory</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>\u2713</td><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td></tr><tr><td>ReAct (Yao et al., 2023b)</td><td>\u2713</td><td>\u2713</td><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td></tr><tr><td>ToT (Yao et al., 2023a)</td><td>\u2713</td><td>\u00d7</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><td>RAP (Hao et al., 2023)</td><td>\u2713</td><td>\u00d7</td><td>\u2713</td><td>\u00d7</td><td>\u2713</td></tr><tr><td>Self-Refine (Madaan et al., 2023)</td><td>\u2713</td><td>\u00d7</td><td>\u00d7</td><td>\u2713</td><td>\u00d7</td></tr><tr><td>Beam Search (Xie et al., 2023)</td><td>\u2713</td><td>\u00d7</td><td>\u00d7</td><td>\u2713</td><td>\u00d7</td></tr><tr><td>Reflexion (Shinn et al., 2023)</td><td>\u2713</td><td>\u2713</td><td>\u00d7</td><td>\u2713</td><td>\u2713</td></tr><tr><td>LATS (Ours)</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></table>\n\n\nTable 1: A summary of related work on reasoning, acting, and planning. LATS is the first work incorporating designs from all three domains, allowing use in all corresponding tasks. We refer to planning as the use of a search algorithm, self-reflection as the use of LM-generated feedback, and external memory as storaging past text context for future updates of solution.\n\n\nissue by searching over multiple reasoning chains. While these methods enable planning, these methods operate in isolation and do not incorporate external feedback that can improve reasoning.\n\n\nTo help address these issues, we propose LATS (Language Agent Tree Search), a general framework for decision-making and reasoning with language models. LATS unifies LM planning, acting, and reasoning strategies by expanding ReAct (Yao et al., 2023b) into a search over a combinatorial space of possible reasoning and acting steps. We adapt Monte Carlo tree search (MCTS) from model-based reinforcement learning (Silver et al., 2017; Anthony et al., 2017; Jiang et al., 2018) to language agents, repurposing a pretrained LLM as an agent, value function, and optimizer. Utilizing the strong natural language understanding and in-context learning ability of modern LMs, we use text as an interface between each component of the framework, allowing LATS to adapt planning to environmental conditions without additional training. To the best of our knowledge, LATS is the first framework that combines reasoning, acting, and planning to enhance LLMs. Notably, LATS doubles the performance of GPT-3.5 on HotPotQA (Yang et al., 2018) over ReAct (Yao et al., 2023b) and raises the average score by 22.1 on WebShop (Yao et al., 2022). When used with GPT-4, LATS achieves a 94.4 Pass@1 rate for programming on HumanEval (Chen et al., 2021), setting the state of the art. To summarize, our contributions are the following:\n\n- - We introduce an LM-based Monte Carlo tree search variant to deliberately construct the best trajectory from sampled actions, enabling more flexible and adaptive problem-solving compared to reflexive prompting methods. This is guided by heuristics from the LM.\n\n- - By integrating external feedback and self-reflection, LATS enhances model sensibility and enables agents to learn from experience, surpassing reasoning-based search methods.\n\n- - Through experiments across diverse domains like programming, interactive QA, and web navigation, we demonstrate the versatility of LATS in harnessing LLMs for autonomous reasoning and decision-making.\n\n\n# 2 RELATED WORK\n\n\nLLMs for reasoning. For LLMs, reasoning typically involves decomposing complex inputs into sequential intermediate steps towards a final answer (Cobbe et al., 2021), demonstrated with Chain-of-Thought (CoT) prompting (Wei et al., 2022) and its variants (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022). However, these methods, which create chains autoregressively in a single step, often suffer from error propagation as the number of steps increases (Guo et al., 2018; Chen et al., 2022b) due to compound errors. Various advancements aim to mitigate this issue; some approaches, such as Self-Consistency (Wang et al., 2022), employ majority voting over sampled chains, while others focus on multi-step decomposition, such as least-to-most prompting (Zhou et al., 2022), or use of external tools such as a scratchpad (Nye et al., 2021) or compiler (Gao et al., 2022). Recently, CoT has been improved with search algorithms (Yao et al., 2023a; Hao et al., 2023; Besta et al., 2023) that can sample trajectories more effectively. Tree-of-thought (ToT) prompting (Yao et al., 2023a) uses DFS or BFS-based search guided by an LM-generated heuristic while Reasoning via Planning (RAP) (Hao et al., 2023) uses MCTS with rollouts simulated by the LM. However, they rely solely on LM internal knowledge and cannot adapt to useful external feedback.\n\n\n![a) Tree-of-Thoughts](images/f2521c9e11cbdc323def5cfd3308e7423d9fbb419347330140e74875d9364496.jpg)\n\n\n![b) Reasoning via Planning](images/31a350608c30e11d47d2af14bb85498ceb2957cc4532131838a2b8f121b57942.jpg)\n\n\n![c) Language Agent Tree Search Figure 2: An overview of the differences between LATS and recently proposed LM search algorithms ToT (Yao et al., 2023a) and RAP (Hao et al., 2023). LATS leverages environmental feedback and self-reflection to further adapt search and improve performance.](images/9c6e73fb5e33533411f8bdd98d2e3a148cb36cb5e950020eac40df4696df4ed2.jpg)\n\n\nLLMs for acting. The strong reasoning and common-sense abilities of LLMs have also been adapted for decision-making or acting tasks as a policy model in interactive environments. In the realm of robotics LLMs have been employed as high-level controllers of control policies (Ahn et al., 2022; Huang et al., 2022; Driess et al., 2023). Similar work (Baker et al., 2022; Wang et al., 2023; Zhu et al., 2023) has also adapted LLM agents to complex multimodal games such as Minecraft (Guss et al., 2019; Fan et al., 2022). LLMs are particularly useful in text-based environments (Liu et al., 2018; Shridhar et al., 2020; Liu et al., 2023), where acting-based prompting techniques such as ReAct (Yao et al., 2023b) have seen success. Similar to CoT, ReAct is limited by its simplicity and cannot effectively adapt to environment conditions. Many extensions have been proposed to address this, including Self-refine (Madaan et al., 2023) and Reflexion (Shinn et al., 2023; Yao et al., 2023c), which uses self-reflection to enhance reasoning and decision-making, and AdaPlanner (Sun et al., 2023), which incorporates both positive and negative environmental feedback. However these methods focus on refining an individual plan or trajectory and do not consider alternative choices at each step. In addition, recent work (Huang et al., 2023) has suggested LLMs cannot self-correct their internal reasoning, making it critical to use external feedback. Alternatively to pure decision-making environments, the reasoning and practical abilities of LLMs have been enhanced by access to external tools, such as APIs, search engines, calculators, or other models (Schick et al., 2023; Shen et al., 2023; Sur\u00eds et al., 2023). Contrary to reasoning-based approaches, these methods have not been improved with planning, limiting their effectiveness. We summarize them in Tab. 1.\n\n\nTree-based search. Tree-based search, where multiple branches of outcomes are explored during search, is widely used in many planning algorithms (\u015awiechowski et al., 2023; LaValle et al., 2001) and Reinforcement Learning (RL) (Hafner et al., 2019; Du et al., 2023; Wu et al., 2023) algorithms for its good exploration-exploitation trade-off. Though tree-based search requires an environment model that can expand from arbitrary state (Vodopivec et al., 2017), which often requires extra training in RL (Hafner et al., 2023), such problem does not exist for LM tasks as we can conveniently backup to any state by setting the input to be the context and corresponding previous output by the LM. Thus, we work on the tree-based framework and use MCTS (\u015awiechowski et al., 2023) to fully release the potential of LMs, while avoiding the cost of training a value function over language descriptions by leveraging the in-context learning (Brown et al., 2020) abilities of LLMs.\n\n\n# 3 PRELIMINARIES\n\n\n# 3.1 PROBLEM SETTING AND PROMPTING\n\n\nBefore describing LATS, we first define our problem and outline a few established methods that leverage large language models for reasoning or decision-making. In LM reasoning or decision mak\n\n\ning, we are given an input  $x$  in natural language and a pretrained language model  $p_{\\theta}(x)$  parameterized by  $\\theta$ ; our goal is to generate a final output  $y \\sim p_{\\theta}(x)$  corresponding to the answer (reasoning) or completes the task (decision-making). Both  $x$  and  $y$  are language sequences, which are comprised of a list of tokens (the basic elements of natural language, often words), denoted as  $x = (x[1], \\ldots, x[n])$  and  $y = (y[1], \\ldots, y[n])$ . The LM decodes text autoregressively, i.e., without other inputs, the probability for an LM to generate a sequence  $x$  is given by  $p_{\\theta}(x) = \\prod_{i=1}^{n} p_{\\theta}(x[i] | x[1 \\ldots i-1])$ . Usually, to improve the LM, prompts are provided along with the input  $x$ , which are specific instructions or few-shot input-output examples. We denote the generic process where an input  $x$  is transformed into an output  $y$  by LM:  $y \\sim p_{\\theta}(y | \\text{prompt}_{IO}(x))$ , where  $\\text{prompt}_{IO}(x)$  denotes the input  $x$ .\n\n\nChain-of-thought (CoT) Prompting (Wei et al., 2022) was introduced to cater to scenarios where direct mapping from  $x$  to  $y$  is intricate, such as when  $x$  is from a mathematical query or challenging question. This method hinges on creating thoughts  $z_{1},\\ldots ,z_{n}$  that act as stepping stones between  $x$  and  $y$ ; each thought  $z_{i}$  is a language sequence. To employ CoT prompting, thoughts are extracted sequentially as  $z_{i}\\sim p_{\\theta}^{CoT}(z_{i}|x,z_{1\\dots i - 1})$ , with the final output being  $y\\sim p_{\\theta}^{CoT}(y|x,z_{1\\dots n})$ .\n\n\nTree-of-thought (ToT) Prompting (Yao et al., 2023a) extends CoT prompting by exploring multiple reasoning paths over thoughts. It frames problems as a search over a tree where each node  $s = [x, z_{1:i}]$  represents a partial solution state comprising the original input  $x$  and thought sequence  $z_{1\\dots i}$ . Thoughts  $z_{i}$  are generated by proposal or sampling with CoT  $z_{i} \\sim p_{\\theta}^{CoT}(z_{i}|x,z_{1\\dots i-1})$ . Deliberate search algorithms like breadth-first or depth-first search are used to systematically explore the tree, guided by heuristics based on language model evaluations  $V(s)$  of each state.\n\n\nReasoning via Planning (RAP) (Hao et al., 2023) is similar to ToT, except that MCTS is used over DFS or BFS. Heuristics are designed from an LM, such as the likelihood or confidence of an action, and the LM is used as a world model to predict subsequent states during the simulation step.\n\n\nReAct (Yao et al., 2023b) extends language models to tasks where the mapping from  $x$  to  $y$  is enhanced by or requires interactions with an external environment, such as a game or API. This technique constructs an action space  $\\hat{A} = A \\cup Z$  that adds permissible actions  $a$  to the reasoning traces  $z$  from CoT. Observations  $o$  from the environment are used to improve both reasoning and acting. To solve problems with ReAct, after each observation, actions are generated from  $p_{\\theta}$  sequentially as  $a_i \\sim p_{\\theta}^{ReAct}(a_i | x, o_{1\\dots i-1}, a_{1\\dots i-1})$ , with the final output being  $y \\sim p_{\\theta}^{ReAct}(y | x, o_{1\\dots n}, a_{1\\dots n})$ .\n\n\nWhile the previously described prompting techniques improve LM performance on reasoning tasks, they falter on difficult tasks that involve multifaceted decision-making due to several shortcomings: 1) Flexibility: Base prompting methods (CoT or ReAct) autoregressively sample from the LM, neglecting potential alternative continuations from specific states. 2) Sensibility: Reasoning-based methods (CoT, RAP, or ToT) rely solely on the internal representations of the LM and cannot consider external observations. This dependency risks fact hallucination and error propagation while setting a performance ceiling. 3) Adaptability: Current planning frameworks (RAP or ToT) use simple search algorithms such as BFS or cannot leverage environmental feedback to improve planning. Additionally, the agent is static and cannot reuse previous experience or learn from trial and error. While RAP also adopts MCTS, it is constrained to tasks where the LM can become a world model and accurately predict states. These shortcomings limit the ability of LMs to be deployed as general problem-solving agents and form the motivation for LATS.\n\n\n# 3.2 MONTE-CARLO TREE SEARCH (MCTS)\n\n\nMonte-Carlo Tree Search (MCTS) is a heuristic search algorithm that is proved successful on many decision-making environments such as Atari (Ye et al., 2021) and Go (Silver et al., 2016). MCTS builds a decision tree where every node in the tree is a state and edge is an action. MCTS runs for  $k$  episodes; for each episode, it starts from the root (i.e., initial state) and iteratively conducts two steps to expand the tree: 1) Expansion, where multiple children states  $s$  are explored from the current parent state  $p$  by sampling  $n$  actions, and 2) Selection, where the children with the highest UCT (Upper Confidence bounds applied to Trees) (Kocsis & Szepesv\u00e1ri, 2006) value is selected by the next iteration. The UCT of a child state  $s$  is calculated as follows:\n\n\n$$\n\n$$\nU C T (s) = V (s) + w \\sqrt {\\frac {\\ln N (p)}{N (s)}}, \\tag {1}\n$$\n\n$$\n\n\n![Figure 3: An overview of the six operations of LATS. A node is selected, expanded, evaluated, then simulated until a terminal node is reached, then the resulting value is backpropagated. If the trajectory fails, a reflection is generated and used as additional context for future trials. These operations are performed in succession until the budget is reached or task is successful.](images/6d42d9cca60751b3dec25def2ea12bb16ebdf2eb288e31a3c9d76dae558f3c03.jpg)\n\n\nwhere  $N(s)$  is the number of visits to a node  $s$ ,  $V(s)$  is the value function (expected return) from the subtree of  $s$ ,  $w$  is the exploration weight, and  $p$  is the parent node of  $s$ . The child node with the highest UCT value is selected for expansion in the next iteration. When the end of an episode is reached, a backpropagation is carried out: the return  $r$  is used for updating every  $V(s)$  along the path with the formula  $V(s) = \\frac{V_{\\mathrm{old}}(s)(N(s) - 1) + r}{N(s)}$ , where  $V_{\\mathrm{old}}(s)$  is the old value function. Normally, the major shortcoming of MCTS is that it requires an environment model to undo previous steps and form a searching tree, which is often a strong assumption. However, such a limitation does not exist for LMs, as we can conveniently reset to any step by simply copy-pasting historical text input. Such a special property is the key motivation of our work.\n\n\n# 4 UNIFYING PLANNING, REASONING, AND ACTING\n\n\n# 4.1 LM AGENT\n\n\nLATS supports sequential reasoning or decision-making tasks on the basis of ReAct. At time step  $t$ , an agent receives an observation  $o_t \\in O$  from the environment and takes an action  $a_t \\in A$  following some policy  $\\pi(a_t | x, o_1 \\dots i-1, a_1 \\dots i-1)$ , where  $x$  consists of the task instruction and a number of few-shot examples. We initialize the agent with  $p_\\theta$  to leverage the useful language representations of an LM as a base decision-maker. We follow the ReAct instantiation in which the action space  $\\hat{A} = A \\cup Z$  consists of both the space of permissible actions  $A$  and language space of reasoning traces  $Z$ . Actions directly affect the environment and result in observation, while thoughts are used to formalize decisions by organizing information, planning future actions, or injecting internal knowledge. The exact instantiation of the action space depends on the particular environment; for decision-making tasks actions might consist of commands on a website while for reasoning tasks the action space might be limited to a few external tools or APIs.\n\n\nInstead of greedily decoding one trajectory or solution, we sample  $n$  actions from  $p_{\\theta}$  using the current state. This is based on the intuition that for complex decision-making tasks, there is likely to be a range of potential trajectories or reasoning paths that are correct (Evans, 2010). Sampling a diverse set of candidates at each step mitigates the stochastic nature of LM text generation and enables greater exploration in both the decision-making and reasoning space. We wrap  $p_{\\theta}$  within our proposed search algorithm to deliberately construct the best trajectory from sampled actions.\n\n\n# 4.2 LATS\n\n\nThe main component of LATS is a search algorithm that controls the overall problem-solving process with deliberate planning. To find the most promising trajectory and systemically balance exploration with exploitation, we adopt a variant of Monte Carlo Tree Search (MCTS) that frames decision-making as a tree search, in which each node  $s = [x, a_{1\\dots i}, o_{1\\dots i}]$  represents a state comprising the original input  $x$ , action sequence  $a_{1\\cdot i}$ , and observation sequence  $o_{1\\cdot i}$ .\n\n\nTo adapt MCTS for language agents, LATS repurposes  $p_{\\theta}$  as an agent, state evaluator, and feedback generator, leveraging the useful language priors of modern LMs to facilitate planning. While standard MCTS and RAP Hao et al. (2023) rely on internal dynamics models to facilitate simulation, LATS is model-free and uses environment interaction. LATS consists of a series of operations, selection, expansion, evaluation, simulation, backpropagation, and reflection, performed in succession until the task is successfully completed or a computational limit is reached. The full psuedocode of LATS can be found in Sec. A in the Appendix.\n\n\nSelection. In the first operation, the algorithm identifies a segment of the current tree most suitable for subsequent expansion. Starting from the root node, denoted as the initial state  $s_0$ , a child node is selected at each tree level until a leaf node is reached. To balance exploration and exploitation, we use the UCT algorithm as shown in Eq. 1.\n\n\nExpansion. After selecting a node, the second operation expands the tree by sampling  $n$  actions from  $p_{\\theta}$ , as described in the prior section. The environment receives each action and returns corresponding feedback as an observation. This results in  $n$  new child nodes added to the tree. This tree is stored in an external long-term memory structure.\n\n\nEvaluation. The third operation assigns a scalar value to each new child node to be used for selection and backpropagation. This value effectively quantifies the agent's progress in task completion, serving as a heuristic to steer the search algorithm towards the most promising regions of the tree. Following Yao et al. (2023a) we repurpose  $p_{\\theta}$  into a value function by prompting it to reason about a given state. To obtain a scalar value, we instruct  $p_{\\theta}$  to end its reasoning trace with a score indicating the correctness of the trajectory. This method offers enhanced flexibility over programmed heuristics (Campbell et al., 2002) and greater efficiency than learned heuristics (Silver et al., 2017).\n\n\nSimulation. The fourth operation expands the currently selected node until a terminal state is reached. At each depth level we sample and evaluate nodes with the same operations, but prioritize nodes of highest value. Reaching a terminal state provides objective feedback on the correctness of a trajectory. If the task is completed successfully, then LATS terminates the search. If the solution is partially successful or unsuccessful, then we perform two additional operations as described below.\n\n\nBackpropagation. This operation updates the values of the tree based on the outcome of a trajectory. For each node  $s_0, s_1, \\ldots, s_n$  in the trajectory from root (initial state  $s_0$ ) of the searching tree to leaf (terminal state  $s_n$ ), its value is updated to reflect the outcome of the simulation by  $N(s_i) = N_{\\mathrm{old}}(s_i) + 1$  and  $V(s_i) = \\frac{r + N_{\\mathrm{old}}(s_i)V_{\\mathrm{old}}(s_i)}{N(s_i)}$ , where  $r$  is the return and  $N_{\\mathrm{old}}$ ,  $V_{\\mathrm{old}}$  are the old number of visits and value function. These updated values are used in the UCT formula (Eq. 1) to guide the selection of the next node for exploration.\n\n\nReflection. In addition to the environmental feedback, we also leverage self-reflection to further refine the decision-making process (Shinn et al., 2023; Madaan et al., 2023). Upon encountering an unsuccessful terminal node,  $p_{\\theta}$  is prompted with the trajectory and final reward to provide a verbal self-reflection that summarizes the errors in the reasoning or acting process and proposes superior alternatives. We store both failed trajectories and corresponding reflections in the memory. In subsequent iterations, these are integrated as additional context to the agent and value function, refining both through in-context learning. This imparts a semantic gradient signal more useful than a scalar value, enabling the agent to learn from trial and error without the cost of expensive optimization processes such as reinforcement learning.\n\n\nConceptually, LATS has the following advantages as a general framework for reasoning and decision-making with LM agents. (1) Generality: LATS supports both reasoning and decision-making tasks by defining a shared space of thoughts and actions. (2) Deliberate: The use of MCTS and LM value function ensures a principled search that selects options with high value while explor\n\n\n<table><tr><td>Prompt Method</td><td>HotpotQA (EM)</td></tr><tr><td>I/O</td><td>0.32</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>0.34</td></tr><tr><td>CoT - SC (Wang et al., 2022)</td><td>0.38</td></tr><tr><td>ToT (Yao et al., 2023a)</td><td>0.55</td></tr><tr><td>RAP (Hao et al., 2023)</td><td>0.60</td></tr><tr><td>RAP (n = 10)</td><td>0.60</td></tr><tr><td>LATS (CoT)</td><td>0.60</td></tr></table>\n\n\n<table><tr><td>Prompt Method</td><td>HotpotQA (EM)</td></tr><tr><td>ReAct (Yao et al., 2023b)</td><td>0.32</td></tr><tr><td>ReAct (best of k)</td><td>0.38</td></tr><tr><td>Reflexion (Shinn et al., 2023)</td><td>0.51</td></tr><tr><td>LATS</td><td>0.61</td></tr><tr><td>LATS (n = 1)</td><td>0.48</td></tr><tr><td>LATS (n = 3)</td><td>0.56</td></tr><tr><td>LATS (n = 10)</td><td>0.64</td></tr><tr><td>LATS (CoT + ReAct)</td><td>0.71</td></tr></table>\n\n\nTable 2: GPT-3.5 reasoning-based prompting (left) and acting-based prompting (right) results on HotpotQA. LATS achieves the highest exact match (EM) for acting and is competitive on reasoning. Unless otherwise specified, we sample  $n = 5$  nodes during expansion and  $k = 50$  trajectories.\n\n\ning promising alternatives. (3) Adaptability: LATS is designed around the use of external feedback through observations and self-reflection, enabling greater adaptation during problem-solving. (4) Flexibility: LATS can accommodate different scenarios, environments, and resource stipulations by modifying state design and tree dimensions. (5) Modularity: The base LM agent, reflection generator, and value function can be independently altered and adapted to individual LM properties.\n\n\n# 5 EXPERIMENTS\n\n\nTo demonstrate the general applicability of LATS, we evaluate our method on a variety of decision-making domains that requires both reasoning and acting ability: programming (Chen et al., 2021; Austin et al., 2021), HotPotQA (Yang et al., 2018), and WebShop (Yao et al., 2022).\n\n\n# 5.1 HOTPOTQA\n\n\nFor a task that can be approached with both reasoning-based and acting-based strategies, we consider HotPotQA (Yang et al., 2018), a multi-hop question-answering benchmark that requires retrieval over two or more Wikipedia passages. For the action space, in addition to LM thoughts we follow the setup from Yao et al. (2023b), which provides the agent with API calls to search and lookup information. The output of these API calls and self-generated reflections form the observation space. We use a subset of 100 questions and three few-shot examples for each method. For ToT, we use DFS as the base search algorithm and scoring with the LM as the heuristic. For all methods that involve sampling, including LATS, we sample  $k = 50$  trajectories. More details and prompts can be found in Sec. D and Sec. E in the Appendix.\n\n\nWe evaluate internal reasoning strategies by removing actions and observations from the context, corresponding to CoT (Wei et al., 2022) and its variants, CoT-SC (Wang et al., 2022), ToT (Yao et al., 2023a), and RAP (Hao et al., 2023). These methods rely solely on the agent's existing knowledge to answer the question. We also consider acting-based methods ReAct, Reflexion, and LATS, which augment the agent with the interactive API environment and primarily evaluate its information retrieval abilities. While LATS is designed for scenarios where external feedback can enhance reasoning, we also implement a reasoning-only version with CoT as the base prompt. We also combine internal and external reasoning in LATS by first prompting with a CoT-based prompt, then switching to a ReAct-based prompt upon failure. This is closer to how humans might approach this task, by using tools to lookup additional information only when the answer is not already known.\n\n\nResults. We observe in Tab. 2 that both internal reasoning and external retrieval strategies perform well on HotPotQA. Due to their large-scale training corpus, modern LLMs already encode factual knowledge and can often directly answer the question correctly. While CoT can slightly enhance performance on questions requiring reasoning, larger gains are observed with search methods ToT and RAP, which can sample and explore more outputs. We observe similar results for acting-based methods. LATS surpasses ReAct, even when sampling the same number of trajectories, by expanding more nodes with principled search (see Fig. 5 in Appendix D for a qualitative sample). This is demonstrated when modifying  $n$ , the number of nodes expanded during each iteration. Increasing  $n$\n\n\n<table><tr><td>Prompt Method</td><td>Model</td><td>Pass@1</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>GPT-3.5</td><td>46.9</td></tr><tr><td>ReAct (Yao et al., 2023b)</td><td>GPT-3.5</td><td>56.9</td></tr><tr><td>Reflexion (Shinn et al., 2023)</td><td>GPT-3.5</td><td>68.1</td></tr><tr><td>ToT (Yao et al., 2023a)</td><td>GPT-3.5</td><td>54.4</td></tr><tr><td>RAP (Hao et al., 2023)</td><td>GPT-3.5</td><td>63.1</td></tr><tr><td>LATS (Ours)</td><td>GPT-3.5</td><td>86.9</td></tr><tr><td>I/O</td><td>GPT-4</td><td>80.1</td></tr><tr><td>Reflexion</td><td>GPT-4</td><td>91.0</td></tr><tr><td>LATS</td><td>GPT-4</td><td>94.4</td></tr></table>\n\n\n<table><tr><td>Prompt Method</td><td>Pass@1</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>54.9</td></tr><tr><td>ReAct (Wei et al., 2022)</td><td>67.0</td></tr><tr><td>Reflexion (Shinn et al., 2023)</td><td>70.0</td></tr><tr><td>ToT (Yao et al., 2023a)</td><td>65.8</td></tr><tr><td>RAP (Hao et al., 2023)</td><td>71.4</td></tr><tr><td>LATS (Ours)</td><td>81.1</td></tr></table>\n\n\nTable 3: GPT-3.5 and GPT-4 Pass@1 accuracy on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). Prompting with LATS achieves the highest performance. We sample 5 solutions during expansion for 8 iterations.\n\n\ncan consistently improve performance, although at greater computational and inference costs. LATS is also competitive to RAP on internal reasoning but performs worse than acting. Combining internal and external reasoning in LATS results in the highest performance, indicating the importance of external feedback in augmenting reasoning even in tasks the base LM can already perform.\n\n\n# 5.2 PROGRAMMING\n\n\nTo demonstrate the importance of external observations for complex reasoning tasks, we evaluate the baselines and LATS on programming with Humaneval (Chen et al., 2021) and MBPP (Austin et al., 2021). Both datasets measure the correctness of synthesized programs in Python from natural language docstrings. We use individual solutions as the action space and test suite and compiler feedback as the external observation. We follow Chen et al. (2022a) and use an LLM to generate a synthetic test suite of syntactically valid \"assert\" statements for each question. For each step, the solution is evaluated on this test suite, and the results including successful and failed tests and compiler output, are added to the context as an observation. We use the same test suite for Reflexion.\n\n\nFor this task, the reasoning and acting baselines share an action space, but acting methods are able to incorporate observations as additional context. For LATS, since each action corresponds to a complete solution, we skip the simulation step of LATS and directly use the percentage of passed tests as the backpropagated reward. We use  $k = 8$  iterations, set the number of generated tests at 4, and sample  $n = 5$  solutions during expansion. After the search is completed, we select the solution with the highest value and evaluate it on the real test suite for the pass@1 accuracy evaluation. More details and prompts can be found in Sec. D and Sec. F in the Appendix.\n\n\nResults. We find in Tab 3 that both search and semantic feedback are crucial for better performance. Despite not using observations, ToT and RAP are competitive with Reflexion. LATS has the highest performance on both datasets. Since RAP uses a similar search algorithm as LATS, this reveals the importance of external feedback for difficult reasoning tasks such as programming. With GPT-4, using LATS sets the state of the art for HumanEval, showing LATS can be used with more advanced LLMs for higher performance.\n\n\n# 5.3 WEBSHOP\n\n\nFor a complex decision-making environment with practical applications, we consider WebShop (Yao et al., 2022), an online shopping environment composed of a website with 1.18M real-world products and 12k human instructions. Agents must navigate a website through a variety of commands to purchase an item matching a user specification. We use the preconstructed action space of search and click commands and browser feedback and reflections for the observation. The performance is gauged using two metrics: an average score, reflecting the percentage of user-specified attributes met by the selected product, and a success rate, indicating the frequency with which the chosen product fulfills all given conditions. We compare against acting-based prompting methods and RL-based approaches. We evaluate on 50 instructions, expand  $n = 5$  children for LATS, and set  $k = 30$  for LATS, ReAct best of  $k$ , and Reflexion. More details and prompts are in Appendix D and G.\n\n\n<table><tr><td>Method</td><td>Score</td><td>SR</td></tr><tr><td>ReAct (Yao et al., 2023b)</td><td>53.8</td><td>28.0</td></tr><tr><td>ReAct (best of k)</td><td>59.1</td><td>32.0</td></tr><tr><td>Reflexion (Shinn et al., 2023)</td><td>64.2</td><td>35.0</td></tr><tr><td>LATS</td><td>75.9</td><td>38.0</td></tr><tr><td>IL Yao et al. (2022)</td><td>59.9</td><td>29.1</td></tr><tr><td>IL+RL Yao et al. (2022)</td><td>62.4</td><td>28.7</td></tr><tr><td>Fine-tuning (Furuta et al., 2023)</td><td>67.5</td><td>45.0</td></tr><tr><td>Expert</td><td>82.1</td><td>59.6</td></tr></table>\n\n\nTable 4: Score and success rate (SR) on Webshop, separated into prompting, training, and human performance. For the same number of iterations, LATS improves both score and success rate and surpasses RL-based training.\n\n\n<table><tr><td>Prompt Method</td><td>HotPotQA</td></tr><tr><td>ReAct (best k = 250)</td><td>0.42</td></tr><tr><td>CoT-SC (k = 250)</td><td>0.40</td></tr><tr><td>ToT (ReAct)</td><td>0.49</td></tr><tr><td>RAP (ReAct)</td><td>0.54</td></tr><tr><td>LATS (DFS)</td><td>0.53</td></tr><tr><td>LATS (No Reflection)</td><td>0.56</td></tr><tr><td>LATS</td><td>0.61</td></tr></table>\n\n\nTable 5: Standardized comparisons between LATS, ToT, and RAP; we use ReAct as the base prompt for all methods and sample  $n = 5$  children for MCTS and DFS and  $k = {50}$  maximum trajectories.\n\n\nResults. We find in Tab. 4 that GPT-3.5 with ReAct is competitive to imitation learning, and can exceed reinforcement learning techniques with stronger prompting strategies. Sampling  $k = 30$  trajectories with ReAct and Reflexion results in a similar performance, suggesting the semantic feedback is not as helpful in complex environments like WebShop. Indeed like in Shinn et al. (2023), we find that generated reflections are often generic and do not provide useful feedback, resulting in a tendency for the agent to become stuck in local minima. However, using LATS indeed results in a noticeable improvement, indicating a more effective exploration for the same number of iterations.\n\n\n# 5.4 ADDITIONAL OBSERVATIONS\n\n\nWe also conduct additional experiments on HotPotQA to demonstrate the effect of each component of LATS. We also design a version of ToT and RAP with ReAct prompt and can handle external observations. We use HotPotQA as our setup incorporates both reasoning (through thoughts) and acting (through API calls); the results are shown in Tab. 5. More ablations for token consumption on HotPotQA are in Tab. 7 in Appendix C. Note that baselines generally perform worse than the reasoning-only setting of HotPotQA, which indicates that the acting-based setting is more challenging and adaption of search algorithms to decision-making scenarios is non-trivial.\n\n\nSelf-reflection. We use self-reflection to provide additional semantic signals for the agent. We observe a 0.05 performance drop when removed from LATS, suggesting this is useful. This is a smaller gain Reflexion (Shinn et al., 2023) observes over ReAct (Yao et al., 2023b) as shown in Tab. 2, suggesting overlap between the types of questions where there is an improvement with self-reflection and search. This variant outperforms RAP-ReAct, reflecting our improvements to MCTS.\n\n\nSearch Algorithm. MCTS is a more principled search algorithm than variants like A* or DFS search and the basis for observed performance gains. We observe the effects of using DFS, and incorporate the LM-based heuristic used in ToT (Yao et al., 2023a) in which branches with low values are pruned. This removes the selection and backpropagation operations, and we observe a 0.08 drop in performance when sampling the same number of nodes, but outperforms ToT-ReAct.\n\n\n# 6 CONCLUSION\n\n\nIn this work, we introduce Language Agent Tree Search (LATS), the first framework to unify planning, acting, and reasoning for enhanced LLM problem solving. By deliberately constructing trajectories with search algorithms, incorporating external feedback, and enabling agents to learn from experience, LATS addresses key limitations of prior prompting techniques. Our evaluations demonstrate the ability of LATS to harness LLM capabilities for a variety of decision-making tasks while keeping its reasoning ability without additional training. The proposed synergies between search, interaction, and reflection offer a versatile approach to autonomous decision-making, highlighting the potential of LLMs as generalist agents. A full discussion of the limitations and broader impacts is in Appendix B.\n\n\n# REFERENCES\n\n- Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances. arXiv:2204.01691, 2022.\n\n- T. Anthony, Z. Tian, and D. Barber. Thinking fast and slow with deep learning and tree search. In NIPS, 2017.\n\n- Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv:2108.07732, 2021.\n\n- Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv:2206.11795, 2022.\n\n- Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. arXiv:2308.09687, 2023.\n\n- Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In EMNLP, 2015.\n\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.\n\n- Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu. Deep blue. Artificial intelligence, 2002.\n\n- Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv:2207.10397, 2022a.\n\n- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv:2107.03374, 2021.\n\n- Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022b.\n\n- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv:2204.02311, 2022.\n\n- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv:2110.14168, 2021.\n\n- Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. arXiv:2306.06070, 2023.\n\n- Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. arXiv:2303.03378, 2023.\n\n- Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. arXiv:2302.00111, 2023.\n\n- Jonathan St BT Evans. Intuition and reasoning: A dual-process perspective. *Psychological Inquiry*, 2010.\n\n- Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In NeurIPS Datasets and Benchmarks Track, 2022.\n\n- Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023.\n\n- Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.\n\n- Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. Long text generation via adversarial training with leaked information. AAAI, 2018.\n\n- William H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Mineral: A large-scale dataset of apache demonstrations. In *IJCAI*, 2019.\n\n- Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In ICML, 2019.\n\n- Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv:2301.04104, 2023.\n\n- Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv:2305.14992, 2023.\n\n- Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv:2310.01798, 2023.\n\n- Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Thompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv:2207.05608, 2022.\n\n- D. Jiang, E. Ekwedike, and H. Liu. Feedback-based tree search for reinforcement learning. In ICML, 2018.\n\n- Levente Kocsis and Csaba Szepesv\u00e1ri. Bandit based monte-carlo planning. In ECML, 2006.\n\n- Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv:2205.11916, 2022.\n\n- Steven M LaValle, James J Kuffner, BR Donald, et al. Rapidly-exploring random trees: Progress and prospects. Algorithmic and computational robotics: new directions, 2001.\n\n- Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In ICLR, 2018.\n\n- Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. arXiv:2308.03688, 2023.\n\n- Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. arXiv:2303.17651, 2023.\n\n- Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence rnns and beyond. In SIGNLL, 2016.\n\n- Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henrik Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv:2112.00114, 2021.\n\n- OpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023.\n\n- Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv:2210.01240, 2022.\n\n- Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv:2302.04761, 2023.\n\n- Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv:2303.17580, 2023.\n\n- Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv:2303.11366, 2023.\n\n- Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv:2010.03768, 2020.\n\n- David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 2016.\n\n- David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, L. Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 2017.\n\n- Steven A. Sloman. The empirical case for two systems of reasoning. Psychological Bulletin, 1996.\n\n- Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. arXiv:2305.16653, 2023.\n\n- D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.\n\n- Maciej \u015awiechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek Ma\u0144dziuk. Monte carlo tree search: A review of recent modifications and applications. Artificial Intelligence Review, 2023.\n\n- Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wen Yin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\n\n- Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023.\n\n- Tom Vodopivec, Spyridon Samothrakis, and Branko Ster. On monte carlo tree search and reinforcement learning. Journal of Artificial Intelligence Research, 2017.\n\n- Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv:2305.16291, 2023.\n\n- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv:2203.11171, 2022.\n\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv:2201.11903, 2022.\n\n- Michael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. The knowledge engineering review, 1995.\n\n- Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World models for physical robot learning. In CoRL. PMLR, 2023.\n\n- Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. arXiv:2305.00633, 2023.\n\n- Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv:1809.09600, 2018.\n\n- Shunyu Yao, Howard Chen, John Yang, and Karthik R Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In NeurIPS, 2022.\n\n- Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv:2305.10601, 2023a.\n\n- Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In ICLR, 2023b.\n\n- Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. Retroformer: Retrospective large language agents with policy gradient optimization. arXiv preprint arXiv:2308.02151, 2023c.\n\n- Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. In NeurIPS, 2021.\n\n- Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv:2205.10625, 2022.\n\n- Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. arXiv:2305.17144, 2023.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638583.599817", "msecs": "599.0", "relativeCreated": "180430.31454086304", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Navigation-Guided Sparse Scene Representation for End-to-End Autonomous Driving\n\n# Abstract\nEnd-to-End Autonomous Driving (E2EAD) methods typically rely on supervised perception tasks to extract explicit scene information (e.g., objects, maps). This reliance necessitates expensive annotations and constrains deployment and data scalability in real-time applications. In this paper, we introduce SSR, a novel framework that utilizes only 16 navigation-guided tokens as Sparse Scene Representation, efficiently extracting crucial scene information for E2EAD. Our method eliminates the need for human-designed supervised sub-tasks, allowing computational resources to concentrate on essential elements directly related to navigation intent. We further introduce a temporal enhancement module, aligning predicted future scenes with actual future scenes through self-supervision. SSR achieves a 27.2\\% relative reduction in L2 error and a 51.6\\% decrease in collision rate to UniAD in nuScenes, with a 10.9\u00d7 faster inference speed and 13\u00d7 faster training time. Moreover, SSR outperforms VAD-Base with a 48.6-point improvement on driving score in CARLA's Town05 Long benchmark. This framework represents a significant leap in real-time autonomous driving systems and paves the way for future scalable deployment.\n\n\n# 1 INTRODUCTION\n\n\n![Figure 1: Performance Comparison of Various Methods in Speed and Accuracy on nuScenes.](images/88fb3d1b223dd0a4e105e455b53a4c5161aed7e805df78c2e1cd144c85a4f0dc.jpg)\n\n\n![](images/a988d76689122a4f51b49dafa4f3c65a616510bc8ee643b9e405a1718c5ff462.jpg)\n\n\nVision-based E2EAD (Hu et al., 2023; Jiang et al., 2023; Sima et al., 2023; Zheng et al., 2024b; Sun et al., 2024; Weng et al., 2024; Li et al., 2024a; Guo et al., 2024) has gained significant attention in recent research as a cost-effective alternative for autonomous driving systems. Traditional architectures typically consist of separate perception and planning modules. While most perception modules are handled by neural networks (NN), planning modules often rely on rule-based pipelines. This separation can lead to information loss during the transfer between modules, resulting in suboptimal performance. E2EAD addresses this by using entire neural networks to predict planning trajectories from images, thereby minimizing information loss and improving overall performance.\n\n\nHowever, most current E2EAD approaches build upon complex perception frameworks, often incorporating additional NN-based planning modules. These approaches typically inherit tasks such as object detection (Li et al., 2022b; Philion & Fidler, 2020), mapping (Li et al., 2022a; Liao et al., 2022), and occupancy prediction (Sima et al., 2023; Huang et al., 2023), resulting in large and\n\n\ncomputationally intensive neural networks. Despite their integration, these models still maintain modular framework design, requiring independent sub-tasks' supervision. As a result, they remain annotation-intensive, suffer from scalability issues, and are inefficient for real-time deployment.\n\n\nWhile many E2EAD methods continue to mimic the paradigm established by prior BEV perception works, they often overlook a critical question: Do E2EAD systems still require such extensive perception tasks? In traditional AD systems, the perception module has to extract all elements for the planning module, as there is no back-propagation from planning to perception. Existing E2EAD methods largely ignore the advanced planning-oriented insight of E2E paradigms, instead retaining a cascade structure rooted in traditional AD. In contrast, we seek for a more targeted approach that directly identifies driving-relevant elements without relying on auxiliary perception supervision. This raises a key question: How can we efficiently identify and focus computational resources on the crucial parts of the scene without auxiliary perception supervision?\n\n\nTo address this, we introduce SSR, a novel framework that leverages navigation-guided Sparse Scene Representation, learning from temporal context in self-supervision rather than explicit perception supervision. Inspired by how human drivers selectively focus on scene elements based on navigation cues, we find that only a minimal set of tokens from dense BEV features is necessary for effective scene representation in autonomous driving. Since E2EAD methods do not rely on high-definition maps as input, a high-level command (e.g., \"turn left\", \"turn right\", \"go straight\" following common practices in Hu et al. (2023); Jiang et al. (2023)) is required for navigation. Our method, therefore, extracts scene queries guided by the navigation commands, akin to human attention mechanisms.\n\n\nAs illustrated in Fig. 2(a), existing methods typically extract all perception elements following previous BEV perception paradigms. These methods rely on Transformer (Vaswani et al., 2017) to identify relevant ones in the additional planning stage. In contrast, as shown in Fig. 2(b), SSR directly extracts only the essential perception elements in the guidance of navigation commands, thereby minimizing redundancy. Our approach takes full advantage of the end-to-end framework, breaking the modular cascade architecture in a Navigation-Guided Perception manner. While prior works (Sun et al., 2024; Zhang et al., 2024) attempt to reduce computation by skipping BEV feature construction, they still depend on hundreds of task-specific queries. However, our method drastically reduces computational overhead by using just 16 tokens guided by navigation commands.\n\n\nAdditionally, SSR capitalizes on temporal context to circumvent the need for perception tasks supervision. We hypothesize that if a predicted action aligns with the actual action, the resulting scene should match the real future scene. Specifically, we predict future BEV features, which are then self-supervised by the actual future BEV features. This future feature predictor, which takes current BEV features and the planning trajectory as input to predict future BEV features, offers an alternative for supervising both scene representation and planning trajectories without auxiliary annotations.\n\n\nBy leveraging navigation-guided perception paradigm and temporal self-supervision, SSR provides an effective and efficient solution for real-time autonomous driving. As illustrated in Fig. 1, SSR delivers state-of-the-art performance on the nuScenes (Caesar et al., 2020) dataset, with minimal computational overhead. Specifically, our method decreases average L2 error by 0.28 meters (a  $27.2\\%$  relative improvement) and reduces the average collision rate by  $51.6\\%$  relatively compared to UniAD (Hu et al., 2023), even without any annotations. Meanwhile, SSR also achieves superior performance on CARLA's (Dosovitskiy et al., 2017) Town05 Long benchmark. Remarkably, our method reduces training time to 1/13th of that required by UniAD and is  $10.9\\times$  faster during inference. Therefore, SSR has the potential to manage large-scale data in real-time applications, further enhancing performance.\n\n\nOur contributions are summarized as follow:\n\n- - We introduce a human-inspired E2EAD framework that utilizes learned sparse query representations guided by navigation commands, significantly reduces computational costs by adaptively focusing on essential parts of scenes.\n\n- - We highlight the critical role of temporal context in autonomous driving by introducing a future feature predictor for self-supervision on dynamic scene changes, eliminating the need for costly perception tasks supervision.\n\n- - Our framework achieves state-of-the-art performance on both open-loop and closed-loop experiments, establishing a new benchmark for real-time E2EAD.\n\n\n![(a) Task-Specific Supervised Paradigm](images/35c8890cd88733cfaa1b523180ba475557cb470dc7d30dc52f313c16232e4a2c.jpg)\n\n\n![(b) Adaptive Unsupervised Paradigm Figure 2: Comparison of Various End-to-End Paradigms. Compared to previous task-specific supervised paradigms, our adaptive unsupervised approach takes full advantage of end-to-end framework by utilizing navigation-guided perception, without the need to differentiate between sub-tasks.](images/b2225d449a9b6f7389c6c80555e1754309b991d7b196c8e7756e591eefb96c29.jpg)\n\n\n# 2 RELATED WORK\n\n\n# 2.1 VISION-BASED END-TO-END AUTONOMOUS DRIVING\n\n\nResearch on End-to-End autonomous driving dates back to 1988 with ALVINN (Pomerleau, 1988), which used a simple neural network to generate steering outputs. NVIDIA developed a prototype E2E system (Bojarski et al., 2016) based on convolutional neural networks (CNN), bypassing manual decomposition. The recent resurgence in vision-based E2EAD has been driven by rapid advancements in BEV perception (Li et al., 2022b; Liao et al., 2022; Liu et al., 2022; Huang et al., 2023) and modern architectures like Transformer (Vaswani et al., 2017).\n\n\nST-P3 (Hu et al., 2022) introduced improvements in perception, prediction, and planning modules for enhanced spatial-temporal feature learning, integrating auxiliary tasks such as depth estimation and BEV segmentation. UniAD (Hu et al., 2023) built on previous BEV perception works to create a cascade framework with a variety of auxiliary tasks, including detection, tracking, mapping, occupancy, and motion estimation. VAD (Jiang et al., 2023) sought to streamline scene representation by vectorizing it, reducing the tracking and occupancy tasks seen in UniAD. GenAD (Zheng et al., 2024b) explored the use of generative models for trajectory generation, jointly optimizing motion and planning heads based on VAD. PARA-Drive (Weng et al., 2024) further examined the relationship between auxiliary tasks, reorganizing them to run in parallel while deactivating them during inference. In contrast, our approach eliminates all perception tasks, achieving remarkable performance in both accuracy and efficiency.\n\n\n# 2.2 SCENE REPRESENTATION IN AUTONOMOUS DRIVING\n\n\nMost prior works in autonomous driving (Hu et al., 2022; 2023; Jiang et al., 2023; Zheng et al., 2024b) have inherited approaches from perception tasks, such as Li et al. (2022b), which leverages dense BEV features as the primary scene representation. In these frameworks, task-specific queries (e.g., for detection and mapping) are used to extract information from the BEV features under manual labels' supervision. While these approaches benefit from rich scene information, they also introduce significant model complexity, hindering real-time application, particularly in occupancy-based representations (Sima et al., 2023; Zheng et al., 2024a).\n\n\nFollowing the trend of sparse paradigms in BEV detection (Lin et al., 2022; Liu et al., 2023), recent sparse E2EAD approaches (Sun et al., 2024; Zhang et al., 2024) directly utilize task-specific queries to interact with image features. These methods attempt to bypass BEV feature generation altogether by directly interacting with image features through task-specific queries. However, despite the reduction in BEV processing, these models still rely on hundreds of queries, which diminish the promised simplicity and efficiency of the end-to-end paradigm. LAW (Li et al., 2024a) proposed the use of view latent queries to represent each camera image with a single query. However, this approach compromises information fidelity, leading to suboptimal performance. UAD (Guo et al., 2024) attempted to divide the BEV feature into angular-wise sectors but still relied on open-set detector labels for supervision, maintaining the complexity of task-specific queries. In this work, we introduce SSR, a novel approach that represents the scene by a minimal set of adaptively learned queries, enhancing both efficiency and performance.\n\n\n![Figure 3: Overview of SSR: SSR consists of two parts: the purple part, which is used during both training and inference, and the gray part, which is only used during training. In the purple part, the dense BEV feature is first compressed by the Scenes TokenLearner into sparse queries, which are then used for planning via cross-attention. In the gray part, the predicted BEV feature is obtained from the Future Feature Predictor. The future BEV feature is then used to supervise the predicted BEV feature, enhancing both the scene representation and the planning decoder.](images/6446928dcf178315976033f5c69c41dc1bdc82ce7ad66e8c729be5b2c9bf38f2.jpg)\n\n\n# 3 METHOD\n\n\n# 3.1 OVERVIEW\n\n\nProblem Formulation: At timestamp  $t$ , given surrounding N-views camera images  $\\mathbf{I}_t = [\\mathbf{I}_t^i]_{i=1}^N$  and a high-level navigation command cmd, the vision-based E2EAD model aims to predict the planning trajectory  $\\mathbf{T}$ , which consists of a set of points in BEV space.\n\n\nBEV Feature Construction: As shown in Fig. 3, N-views camera images  $\\mathbf{I}_t$  are processed by a BEV encoder to generate the BEV feature. In the BEV encoder such as BEVFormer (Li et al., 2022b),  $\\mathbf{I}_t$  is first processed by an image backbone to obtain image features  $\\mathbf{F}_t = [\\mathbf{F}_t^i]_{i=1}^N$ . A BEV query  $\\mathbf{Q} \\in \\mathbb{R}^{H \\times W \\times C}$  is then used to query temporal information from the previous frame's BEV feature  $\\mathbf{B}_{t-1}$  and spatial information from  $\\mathbf{F}_t$  by cross-attention iteratively, resulting in the current BEV feature  $\\mathbf{B}_t \\in \\mathbb{R}^{H \\times W \\times C}$ . Here,  $H \\times W$  represents the spatial dimensions of the BEV feature, and  $C$  denotes the feature's channel dimension.\n\n\n$$\n\n$$\n\\mathbf {Q} = \\text {C r o s s A t t e n t i o n} (\\mathbf {Q}, \\mathbf {B} _ {t - 1}, \\mathbf {B} _ {t - 1}), \\tag {1}\n$$\n\n$$\n\n\n$$\n\n$$\n\\mathbf {B} _ {t} = \\text {C r o s s A t t e n t i o n} (\\mathbf {Q}, \\mathbf {F} _ {t}, \\mathbf {F} _ {t}). \\tag {2}\n$$\n\n$$\n\n\nThe key component of our framework is the novel Scenes TokenLearner module to extract crucial scene information, introduced in Sec. 3.2. Unlike traditional methods that rely on dense BEV features or hundreds of queries, our approach uses a small set of tokens to effectively represent the scene. Leveraging these sparse scene tokens, we generate the planning trajectory, a process detailed in Sec. 3.3. Additionally, in Sec. 3.4, we introduce an augmented future feature predictor designed to further enhance the scene representation through self-supervised learning of BEV features.\n\n\n# 3.2 NAVIGATION-GUIDED SCENESTOKEN LEARNER\n\n\nBEV features are a popular scene representation as they contain rich perception information. However, this dense representation increases the inference time when searching for relevant perception elements. To address this, we introduce a sparse scene representation using adaptive spatial attention, significantly reducing computational load while maintaining high-fidelity scene understanding.\n\n\nSpecifically, we propose the Scenes TokenLearner (STL) module to extract scene queries  $\\mathbf{S}_t = [\\mathbf{s}_i]_{i=1}^{N_s} \\in \\mathbb{R}^{N_s \\times C}$  from the BEV feature, where  $N_s$  is the number of scene queries, to efficiently\n\n\n![Figure 4: Structure of Modules: Scenes TokenLearner and Future Feature Predictor.](images/4fdc352bff9e3a12d7bcbd6118f503ef4d03b06371ad7d106af9499133195dd4.jpg)\n\n\nrepresent the scene. The structure of Scenes TokenLearner is illustrated in Fig. 4. To better focus on scene information related to our navigation intent, we adopt a Squeeze-and-Excitation (SE) layer (Hu et al., 2018) to encode the navigation command cmd into the dense BEV feature, producing the navigation-aware BEV feature  $\\mathbf{B}_t^{navi}$ :\n\n\n$$\n\n$$\n\\mathbf {B} _ {t} ^ {\\text {n a v i}} = S E \\left(\\mathbf {B} _ {t}, c m d\\right). \\tag {3}\n$$\n\n$$\n\n\nThe navigation-aware BEV feature is then passed into the BEV TokenLearner (Ryoo et al., 2021) module  $TL_{BEV}$  to adaptively focus on the most important information. Unlike previous applications of TokenLearner in image or video domains, we utilize it in BEV space to derive a sparse scene representation via spatial attention:\n\n\n$$\n\n$$\n\\mathbf {S} _ {t} = T L _ {B E V} \\left(\\mathbf {B} _ {t} ^ {\\text {n a v i}}\\right). \\tag {4}\n$$\n\n$$\n\n\nFor each scene query  $\\mathbf{s}_i$ , we adopt a tokenizer function  $M_{i}$  that maps  $\\mathbf{B}_t^{navi}$  into a token vector:  $\\mathbb{R}^{H\\times W\\times C}\\to \\mathbb{R}^C$ . The tokenizer predicts spatial attention maps of shape  $H\\times W\\times 1$ , and the learned scene tokens are obtained through global average pooling:\n\n\n$$\n\n$$\n\\mathbf {s} _ {i} = M _ {i} \\left(\\mathbf {B} _ {t} ^ {\\text {n a v i}}\\right) = \\rho \\left(\\mathbf {B} _ {t} ^ {\\text {n a v i}} \\odot \\varpi_ {i} \\left(\\mathbf {B} _ {t} ^ {\\text {n a v i}}\\right)\\right), \\tag {5}\n$$\n\n$$\n\n\nwhere  $\\varpi(\\cdot)$  is the spatial attention function and  $\\rho(\\cdot)$  is the global average pooling function. The multi-layer self-attention (Vaswani et al., 2017) is applied to further enhance the scene queries:\n\n\n$$\n\n$$\n\\mathbf {S} _ {t} = \\text {S e l f A t t e n t i o n} (\\mathbf {S} _ {t}). \\tag {6}\n$$\n\n$$\n\n\n# 3.3 PLANNING BASED ON SPARSE SCENE REPRESENTATION\n\n\nSince  $\\mathbf{S}_t$  contains all relevant perception information, we use a set of way point queries  $\\mathbf{W}_t \\in \\mathbb{R}^{N_m \\times N_t \\times C}$  to extract multi-modal planning trajectories, where  $N_t$  is the number of future timestamps and  $N_m$  denotes the number of driving commands.\n\n\n$$\n\n$$\n\\mathbf {W} _ {t} = \\text {C r o s s A t t e n t i o n} \\left(\\mathbf {W} _ {t}, \\mathbf {S} _ {t}, \\mathbf {S} _ {t}\\right). \\tag {7}\n$$\n\n$$\n\n\nWe then obtain the predicted trajectory from  $\\mathbf{W}_t$  using a multi-layer perceptron (MLP), and select output trajectory  $\\mathbf{T} \\in \\mathbb{R}^{N_t \\times 2}$  based on the navigation command cmd:\n\n\n$$\n\n$$\n\\mathbf {T} = \\operatorname {S e l e c t} \\left(M L P \\left(\\mathbf {W} _ {t}\\right), \\operatorname {c m d}\\right). \\tag {8}\n$$\n\n$$\n\n\nThe output trajectory is supervised by the ground truth (GT) trajectory  $\\mathbf{T}_{GT}$  using L1 loss, defined as the imitation loss  $\\mathcal{L}_{imi}$ :\n\n\n$$\n\n$$\n\\mathcal {L} _ {i m i} = \\left\\| \\mathbf {T} _ {G T} - \\mathbf {T} \\right\\| _ {1}. \\tag {9}\n$$\n\n$$\n\n\n# 3.4 TEMPORAL ENHANCEMENT BY FUTURE FEATURE PREDICTOR\n\n\nWe prioritize temporal context to enhance scene representation by self-supervision instead of perception sub-tasks. The motivation behind this module is straight: if our predicted actions correspond to real actions, the predicted future scenes should closely resemble the actual future scenes.\n\n\nAs illustrated in Fig. 4, we introduce the Future Feature Predictor (FFP) to predict future BEV features. First, we use the output trajectory  $\\mathbf{T}$  to translate the current scene queries into the future\n\n\nframe using a Motion aware Layer Normalization (MLN) (Wang et al., 2023) module. The MLN module helps current scene queries encode motion information, producing dreaming queries  $\\mathbf{D}_t$ :\n\n\n$$\n\n$$\n\\mathbf {D} _ {t} = M L N \\left(\\mathbf {S} _ {t}, \\mathbf {T}\\right). \\tag {10}\n$$\n\n$$\n\n\nWe then apply multi-layer self-attention on  $\\mathbf{D}_t$  to predict the future scene queries  $\\hat{\\mathbf{S}}_{t + 1}$ :\n\n\n$$\n\n$$\n\\hat {\\mathbf {S}} _ {t + 1} = \\operatorname {S e l f A t t e n t i o n} \\left(\\mathbf {D} _ {t}\\right). \\tag {11}\n$$\n\n$$\n\n\nHowever, since the autonomous driving system may focus on different regions even in consecutive frames, we do not directly supervise the predicted scene queries  $\\hat{\\mathbf{S}}_{t + 1}$  with the future scene queries  $\\mathbf{S}_{t + 1}$ . Instead, we reconstruct the dense BEV feature  $\\hat{\\mathbf{B}}_{t + 1}$  using TokenFuser (Ryoo et al., 2021):\n\n\n$$\n\n$$\n\\hat {\\mathbf {B}} _ {t + 1} = \\operatorname {T o k e n F u s e r} \\left(\\hat {\\mathbf {S}} _ {t + 1}, \\mathbf {B} _ {t}\\right) \\tag {12}\n$$\n\n$$\n\n\n$$\n\n$$\n= \\psi (\\mathbf {B} _ {t}) \\otimes \\hat {\\mathbf {S}} _ {t + 1}, \\tag {13}\n$$\n\n$$\n\n\nwhere  $\\psi (\\cdot)$  is a simple MLP with the sigmoid function to remap the BEV feature  $\\mathbf{B}_t$  to a weight tensor:  $\\mathbb{R}^{H\\times W\\times C}\\to \\mathbb{R}^{H\\times W\\times N_s}$ . After the multiplication  $\\otimes$  with  $\\hat{\\mathbf{S}}_{t + 1}\\in \\mathbb{R}^{N_s\\times C}$ , we obtain the predicted dense BEV feature  $\\hat{\\mathbf{B}}_{t + 1}\\in \\mathbb{R}^{H\\times W\\times C}$ . This process aims to recover the BEV feature from the predicted scene queries for further self-supervision.\n\n\nUnlike prior works (Zong et al., 2023; Zou et al., 2024) which utilize predicted BEV features for subsequent object- or pixel-level supervision, we supervise  $\\hat{\\mathbf{B}}_{t + 1}$  directly using an L2 loss with the real future BEV feature  $\\mathbf{B}_{t + 1}$ . This is defined as the BEV reconstruction loss  $\\mathcal{L}_{\\text{bev}}$ :\n\n\n$$\n\n$$\n\\mathcal {L} _ {b e v} = \\left\\| \\hat {\\mathbf {B}} _ {t + 1} - \\mathbf {B} _ {t + 1} \\right\\| _ {2}. \\tag {14}\n$$\n\n$$\n\n\nIn summary, we apply imitation loss  $\\mathcal{L}_{imi}$  for the predicted trajectory, and BEV reconstruction loss  $\\mathcal{L}_{bev}$  for the predicted BEV feature. The total loss of SSR is:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {i m i}} + \\mathcal {L} _ {\\text {b e v}}. \\tag {15}\n$$\n\n$$\n\n\n# 4 EXPERIMENTS\n\n\n# 4.1 DATASET AND METRIC\n\n\nOpen-Loop We evaluate the proposed SSR framework for autonomous driving using the widely adopted nuScenes dataset (Caesar et al., 2020), following prior works (Hu et al., 2023; Jiang et al., 2023). To assess planning performance, we use displacement error and collision rate (CR), as in previous studies. Displacement error is calculated by L2 error with respect to the GT trajectory, measuring the quality of predicted trajectory. Collision rate quantifies the percentage of collisions with other objects when following the predicted trajectory. All metrics are calculated in 3s future horizon with a 0.5s interval and evaluated at 1s, 2s and 3s.\n\n\nWe observe that VAD (Jiang et al., 2023) and UniAD (Hu et al., 2023) utilize different evaluation methods to calculate results across all predicted frames. VAD computes the average across all previous frames, while UniAD uses the latest result as well as the maximum value. Additionally, UniAD excludes pedestrians from the GT occupancy map, resulting in lower collision rates. We denote the VAD approach with the subscript AVG and the UniAD approach with MAX. For example, the L2 error at frame  $t$  (maximum  $3s / 0.5s = 6$ ) is calculated as  $L2_{AVG}^{t} = \\frac{1}{t}\\sum_{i = 1}^{t}L2^{i}$  for VAD and  $L2_{MAX}^{t} = L2^{t}$  for UniAD. We apply MAX metric by default but also calculate AVG metric for comparison with other methods in Tab. 1. In our MAX metric, pedestrians are included in the calculation of collision rate.\n\n\nClosed-Loop We conduct closed-loop experiments using the CARLA simulator (Dosovitskiy et al., 2017), leveraging the widely adopted Town05 Long benchmark to evaluate performance. The training dataset consists of 189K frames collected by Roach (Zhang et al., 2021) at  $2\\mathrm{Hz}$  across 4 CARLA towns (Town01, Town03, Town04, and Town06), following previous works (Jia et al., 2023a;b; Wu et al., 2022). The training data has no overlap with Town05 Long benchmark.\n\n\nWe utilize the official metric provided by CARLA. The Route Completion (RC) is the percentage of the route completed by the autonomous agent. The Infraction Score (IS) quantifies the number of infractions made along the route, with pedestrians, vehicles, road layouts, and traffic signals. The Driving Score (DS) serves as the main metric, calculated as the product of RC and IS.\n\n\nTable 1: Comparison of state-of-the-art methods on the nuScenes dataset. The ego status was not utilized in the planning module.  $\\diamond$ : Lidar-based methods.  $*$ : Backbone with ResNet-101 (He et al., 2016), while others use ResNet-50 or similar.  $\\dagger$ : FPS measured on an NVIDIA A100 GPU, while others were tested on an NVIDIA RTX 3090.  $\\ddagger$ : AVG metric protocol as same as VAD.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Auxiliary Task</td><td colspan=\"4\">L2 (m) \u2193</td><td colspan=\"4\">Collision Rate (%) \u2193</td><td rowspan=\"2\">FPS</td></tr><tr><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td></tr><tr><td>NMP\u25c7 (Zeng et al., 2019)</td><td>Det &amp; Motion</td><td>0.53</td><td>1.25</td><td>2.67</td><td>1.48</td><td>0.04</td><td>0.12</td><td>0.87</td><td>0.34</td><td>-</td></tr><tr><td>FF\u25c7 (Hu et al., 2021)</td><td>FreeSpace</td><td>0.55</td><td>1.20</td><td>2.54</td><td>1.43</td><td>0.06</td><td>0.17</td><td>1.07</td><td>0.43</td><td>-</td></tr><tr><td>EO\u25c7 (Khurana et al., 2022)</td><td>FreeSpace</td><td>0.67</td><td>1.36</td><td>2.78</td><td>1.60</td><td>0.04</td><td>0.09</td><td>0.88</td><td>0.33</td><td>-</td></tr><tr><td>ST-P3 (Hu et al., 2022)</td><td>Det &amp; Map &amp; Depth</td><td>1.72</td><td>3.26</td><td>4.86</td><td>3.28</td><td>0.44</td><td>1.08</td><td>3.01</td><td>1.51</td><td>1.6</td></tr><tr><td>UniAD* (Hu et al., 2023)</td><td>Det&amp;Track&amp;Map&amp;Motion&amp;Occ</td><td>0.48</td><td>0.96</td><td>1.65</td><td>1.03</td><td>0.05</td><td>0.17</td><td>0.71</td><td>0.31</td><td>1.8\u2020</td></tr><tr><td>OccNet* (Sima et al., 2023)</td><td>Det &amp; Map &amp; Occ</td><td>1.29</td><td>2.13</td><td>2.99</td><td>2.14</td><td>0.21</td><td>0.59</td><td>1.37</td><td>0.72</td><td>2.6</td></tr><tr><td>VAD-Base (Jiang et al., 2023)</td><td>Det &amp; Map &amp; Motion</td><td>0.54</td><td>1.15</td><td>1.98</td><td>1.22</td><td>0.04</td><td>0.39</td><td>1.17</td><td>0.53</td><td>4.5</td></tr><tr><td>PARA-Drive (Weng et al., 2024)</td><td>Det&amp;Track&amp;Map&amp;Motion&amp;Occ</td><td>0.40</td><td>0.77</td><td>1.31</td><td>0.83</td><td>0.07</td><td>0.25</td><td>0.60</td><td>0.30</td><td>5.0</td></tr><tr><td>GenAD (Zheng et al., 2024b)</td><td>Det &amp; Map &amp; Motion</td><td>0.36</td><td>0.83</td><td>1.55</td><td>0.91</td><td>0.06</td><td>0.23</td><td>1.00</td><td>0.43</td><td>6.7</td></tr><tr><td>UAD-Tiny (Guo et al., 2024)</td><td>Det</td><td>0.47</td><td>0.99</td><td>1.71</td><td>1.06</td><td>0.08</td><td>0.39</td><td>0.90</td><td>0.46</td><td>18.9\u2020</td></tr><tr><td>UAD* (Guo et al., 2024)</td><td>Det</td><td>0.39</td><td>0.81</td><td>1.50</td><td>0.90</td><td>0.01</td><td>0.12</td><td>0.43</td><td>0.19</td><td>7.2\u2020</td></tr><tr><td>SSR (Ours)</td><td>None</td><td>0.24</td><td>0.65</td><td>1.36</td><td>0.75</td><td>0.00</td><td>0.10</td><td>0.36</td><td>0.15</td><td>19.6</td></tr><tr><td>ST-P3\u2021 (Hu et al., 2022)</td><td>Det &amp; Map &amp; Depth</td><td>1.33</td><td>2.11</td><td>2.90</td><td>2.11</td><td>0.23</td><td>0.62</td><td>1.27</td><td>0.71</td><td>1.6</td></tr><tr><td>UniAD*\u2021 (Hu et al., 2023)</td><td>Det&amp;Track&amp;Map&amp;Motion&amp;Occ</td><td>0.44</td><td>0.67</td><td>0.96</td><td>0.69</td><td>0.04</td><td>0.08</td><td>0.23</td><td>0.12</td><td>1.8\u2020</td></tr><tr><td>VAD-Tiny\u2021 (Jiang et al., 2023)</td><td>Det &amp; Map &amp; Motion</td><td>0.46</td><td>0.76</td><td>1.12</td><td>0.78</td><td>0.21</td><td>0.35</td><td>0.58</td><td>0.38</td><td>16.8</td></tr><tr><td>VAD-Base\u2021 (Jiang et al., 2023)</td><td>Det &amp; Map &amp; Motion</td><td>0.41</td><td>0.70</td><td>1.05</td><td>0.72</td><td>0.07</td><td>0.17</td><td>0.41</td><td>0.22</td><td>4.5</td></tr><tr><td>BEV-Planner\u2021 (Li et al., 2024b)</td><td>None</td><td>0.28</td><td>0.42</td><td>0.68</td><td>0.46</td><td>0.04</td><td>0.37</td><td>1.07</td><td>0.49</td><td>-</td></tr><tr><td>PARA-Drive\u2021 (Weng et al., 2024)</td><td>Det&amp;Track&amp;Map&amp;Motion&amp;Occ</td><td>0.25</td><td>0.46</td><td>0.74</td><td>0.48</td><td>0.14</td><td>0.23</td><td>0.39</td><td>0.25</td><td>5.0</td></tr><tr><td>LAW\u2021 (Li et al., 2024a)</td><td>None</td><td>0.26</td><td>0.57</td><td>1.01</td><td>0.61</td><td>0.14</td><td>0.21</td><td>0.54</td><td>0.30</td><td>19.5</td></tr><tr><td>GenAD\u2021 (Zheng et al., 2024b)</td><td>Det &amp; Map &amp; Motion</td><td>0.28</td><td>0.49</td><td>0.78</td><td>0.52</td><td>0.08</td><td>0.14</td><td>0.34</td><td>0.19</td><td>6.7</td></tr><tr><td>SparseDrive\u2021 (Sun et al., 2024)</td><td>Det &amp; Track &amp; Map &amp; Motion</td><td>0.29</td><td>0.58</td><td>0.96</td><td>0.61</td><td>0.01</td><td>0.05</td><td>0.18</td><td>0.08</td><td>9.0</td></tr><tr><td>UAD*\u2021 (Guo et al., 2024)</td><td>Det</td><td>0.28</td><td>0.41</td><td>0.65</td><td>0.45</td><td>0.01</td><td>0.03</td><td>0.14</td><td>0.06</td><td>7.2\u2020</td></tr><tr><td>SSR\u2021 (Ours)</td><td>None</td><td>0.18</td><td>0.36</td><td>0.63</td><td>0.39</td><td>0.01</td><td>0.04</td><td>0.12</td><td>0.06</td><td>19.6</td></tr></table>\n\n\n# 4.2 IMPLEMENTATION DETAILS\n\n\nSettings We build up SSR on VAD (Jiang et al., 2023) and follow the setting of VAD-Tiny. We adopt ResNet-50 (He et al., 2016) as image backbone operating at an image resolution of  $640 \\times 360$ . The BEV representation is generated at a  $100 \\times 100$  resolution and then compressed into sparse scene tokens with shape  $16 \\times 256$ . The number of navigation commands remains 3 as prior works. Other settings follow VAD-Tiny unless otherwise specified. In closed-loop simulation, we utilize ResNet-34 (He et al., 2016) as the image backbone, resizing the input image size to  $900 \\times 256$ . The target point is concatenated with driving commands as the navigation information. The TCP head (Wu et al., 2022) is applied for planning module.\n\n\nTraining Parameters Our open-loop model is trained for 12 epochs on 8 NVIDIA RTX 3090 GPUs with a batch size of 1 per GPU. The training phase costs about 11 hours which is  $13 \\times$  faster than UniAD. We utilize the AdamW (Loshchilov & Hutter, 2019) optimizer with a learning rate set to  $5 \\times 10^{-5}$ . The weight of imitation loss and BEV loss is both 1.0. The closed-loop model is trained for 60 epochs on 4 NVIDIA RTX 3090 GPUs with a batch size of 32 per GPU. The learning rate is set to  $1 \\times 10^{-4}$  while being halved after 30 epochs.\n\n\n# 4.3 MAIN RESULT\n\n\nOpen-Loop Evaluation Our method outperforms existing E2EAD approaches in nuScenes, achieving superior results in both L2 error and collision rate, as shown in Tab. 1. For the well-known method UniAD, which employs most auxiliary tasks, our method reduces  $0.28\\mathrm{m}$  (27.2% relatively) average  $L2_{MAX}$  error and 0.16% (51.6% relatively) average  $CR_{MAX}$  without any auxiliary tasks. When compared to our baseline method VAD-Tiny, SSR not only reduces 0.39m (50.0% relatively) average  $L2_{AVG}$  error and 0.46% (79.3% relatively) average  $CR_{AVG}$ , but also outperforms the VAD-Base with an obvious margin (45.8% average  $L2_{AVG}$  and 70.7% average  $CR_{AVG}$  reduction relatively). Furthermore, our method demonstrates real-time efficiency, achieving 19.6 FPS (latency analysis in Appendix D), which is  $10.9\\times$  faster than UniAD and  $4.3\\times$  faster than VAD-Base. Remarkably, it is also  $2.2\\times$  faster than the prior sparse work SparseDrive (Sun et al., 2024), while reducing the average  $L2_{AVG}$  error by 0.22m.\n\n\nTable 2: Performance on Town05 Long benchmark.\n\n\n<table><tr><td>Method</td><td>Modality</td><td>DS\u2191</td><td>RC\u2191</td><td>IS\u2191</td></tr><tr><td>CILRS (Codevilla et al., 2019)</td><td>C</td><td>7.8</td><td>10.3</td><td>0.75</td></tr><tr><td>LBC (Chen et al., 2020)</td><td>C</td><td>12.3</td><td>31.9</td><td>0.66</td></tr><tr><td>Transfuser (Prakash et al., 2021)</td><td>C+L</td><td>31.0</td><td>47.5</td><td>0.77</td></tr><tr><td>Roach (Zhang et al., 2021)</td><td>C</td><td>41.6</td><td>96.4</td><td>0.43</td></tr><tr><td>ST-P3 (Hu et al., 2022)</td><td>C</td><td>11.5</td><td>83.2</td><td>-</td></tr><tr><td>TCP (Wu et al., 2022)</td><td>C</td><td>57.2</td><td>80.4</td><td>0.73</td></tr><tr><td>VAD-Base (Jiang et al., 2023)</td><td>C</td><td>30.3</td><td>75.2</td><td>-</td></tr><tr><td>ThinkTwice (Jia et al., 2023b)</td><td>C+L</td><td>65.0</td><td>95.5</td><td>0.69</td></tr><tr><td>DriveAdapter(Jia et al., 2023a)</td><td>C+L</td><td>65.9</td><td>94.4</td><td>0.72</td></tr><tr><td>SSR (Ours)</td><td>C</td><td>78.9</td><td>95.5</td><td>0.83</td></tr></table>\n\n\nTable 3: Component-wise Ablation.\n\n\n<table><tr><td colspan=\"2\">Modules</td><td colspan=\"4\">L2 (m) \u2193</td><td colspan=\"4\">CR (%) \u2193</td></tr><tr><td>STL</td><td>FFP</td><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td></tr><tr><td></td><td></td><td>0.23</td><td>0.65</td><td>1.41</td><td>0.76</td><td>0.04</td><td>0.58</td><td>0.66</td><td>0.43</td></tr><tr><td>\u2713</td><td></td><td>0.23</td><td>0.64</td><td>1.39</td><td>0.75</td><td>0.02</td><td>0.10</td><td>0.47</td><td>0.20</td></tr><tr><td>\u2713</td><td>\u2713</td><td>0.24</td><td>0.65</td><td>1.36</td><td>0.75</td><td>0.00</td><td>0.10</td><td>0.36</td><td>0.15</td></tr></table>\n\n\nTable 4: Number of Scene Queries.\n\n\n<table><tr><td rowspan=\"2\">Number</td><td colspan=\"4\">L2 (m) \u2193</td><td colspan=\"4\">CR (%) \u2193</td></tr><tr><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td></tr><tr><td>8</td><td>0.22</td><td>0.59</td><td>1.25</td><td>0.69</td><td>0.04</td><td>0.14</td><td>0.43</td><td>0.20</td></tr><tr><td>16</td><td>0.24</td><td>0.65</td><td>1.36</td><td>0.75</td><td>0.00</td><td>0.10</td><td>0.36</td><td>0.15</td></tr><tr><td>32</td><td>0.26</td><td>0.67</td><td>1.38</td><td>0.77</td><td>0.04</td><td>0.12</td><td>0.31</td><td>0.16</td></tr><tr><td>64</td><td>0.30</td><td>0.74</td><td>1.47</td><td>0.84</td><td>0.18</td><td>0.39</td><td>0.66</td><td>0.41</td></tr></table>\n\n\nTable 5: Ablation of navigation guidance. GS means go straight and LR denotes turn left / right.\n\n\n<table><tr><td rowspan=\"2\">Navigation Guidance</td><td colspan=\"4\">L2-GS (m) \u2193</td><td colspan=\"4\">L2-LR (m) \u2193</td><td colspan=\"4\">CR-GS (%) \u2193</td><td colspan=\"4\">CR-LR (%) \u2193</td></tr><tr><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td></tr><tr><td>\u00d7</td><td>0.24</td><td>0.61</td><td>1.31</td><td>0.72</td><td>0.34</td><td>0.96</td><td>1.98</td><td>1.09</td><td>0.09</td><td>0.38</td><td>0.40</td><td>0.29</td><td>0.00</td><td>0.44</td><td>1.90</td><td>0.78</td></tr><tr><td>\u2713</td><td>0.23</td><td>0.61</td><td>1.28</td><td>0.71</td><td>0.33</td><td>0.91</td><td>1.88</td><td>1.04</td><td>0.00</td><td>0.08</td><td>0.18</td><td>0.10</td><td>0.00</td><td>0.29</td><td>1.70</td><td>0.66</td></tr></table>\n\n\nWhen compared to previous approaches that eliminate auxiliary annotations, SSR demonstrates impressive performance across all metrics. LAW (Li et al., 2024a), for instance, achieves a similar inference speed to SSR but retains a substantial gap in both L2 error and collision rate. The method closest in performance to ours is UAD (Guo et al., 2024), using a larger ResNet-101 backbone and a  $1600 \\times 900$  image resolution input, and requires an additional open-set 2D detector to supervise objectness information. Despite these additional resources, UAD still shows a  $0.15\\mathrm{m}$  higher average  $L2_{MAX}$  error compared to SSR, along with a  $2.7\\times$  lower inference speed.\n\n\nClosed-Loop Evaluation As presented in Tab. 2, our method significantly outperforms existing works in terms of driving score, including those utilizing LiDAR input (Jia et al., 2023b;a). For camera-based methods, SSR achieves a 31.7-point improvement in driving score over TCP and a remarkable  $2.6 \\times$  increase over VAD-Base. SSR also achieves the highest infraction score, demonstrating its comprehensive capabilities and robust performance in challenging environments.\n\n\n# 4.4 ABLATION STUDY\n\n\n# 4.4.1 COMPONENT-WISE ABLATION\n\n\nIn Tab. 3, we present an ablation study on the proposed components. When the STL is enabled instead of directly interacting waypoint queries with the BEV feature, the collision rate is reduced by more than half. This significant decrease underscores the STL's ability to effectively distill critical information from the dense scene data, thereby minimizing the impact of irrelevant features and reducing computational redundancy. Furthermore, when incorporating the Future Feature Predictor, we observe a further reduction in the average collision rate to  $0.15\\%$ . This improvement highlights the Future Feature Predictor's role in enhancing SSR's comprehension of scene dynamics, contributing to more safe trajectory planning and overall performance gains.\n\n\n# 4.4.2 NUMBER OF SCENE QUERIES\n\n\nWe evaluate the impact of varying the number of scene queries in Tab. 4. For L2 error, using 8 queries yields the best performance, with performance declining as the number of queries increases. Interestingly, when considering the collision rate, the optimal performance is achieved with 16 queries. Therefore, we select 16 queries as the default setting in SSR to strike a balance between minimizing L2 error and reducing collision rate. The poor performance observed with 64 scene queries suggests that an excessive number of queries may overwhelm the model with too much perception information, leading to confusion similar to directly interacting with dense BEV features.\n\n\n![](images/8dcc454e510a62a418bcbe7d4291d72158e9ad8c911702cc8d09b01ffa70b729.jpg)\n\n\n![](images/07ea718008230051512a491466ad7cd39b371ddc09b2c51fa5e20ec0e79ca5ae.jpg)\n\n\n![](images/e4647699365be7a1f4f200632b0c95e85c48e16f20b87124d5dcfa9fe0a3488b.jpg)\n\n\n![](images/7a17aaa7a2ab239d75d8f3d1bc50977b6b364556f23f5220e4dacac09a9ec736.jpg)\n\n\n![](images/d6132452652466333793cdf08d8094c5b7b80e78fad67807abf7057a4035585b.jpg)\n\n\n![Scene Query 0 Scene Query 8 Figure 5: Visualization of BEV Square Attention Map of Scene Queries. Attention maps for 8 of the 16 tokens are displayed. Ego vehicle is located at the center while up direction indicates the front of ego. Brighter areas represent higher attention weights. The full set is provided in Appendix A.](images/ffcfb95bd17c8a494d6b52b17ea60724178a542477fc526db9f14b81e21f6b75.jpg)\n\n\n![Scene Query 1 Scene Query 10](images/b23008d1b6cc998778d0233aaa8397371e76bf0c3b869486ceb34f0df9ac2c0b.jpg)\n\n\n![Scene Query 4 Scene Query 12](images/04726a3512e80a6ca4deac3aff4024c28e154f69cba7d66eb82ac0455b667322.jpg)\n\n\n![Scene Query 5 Scene Query 14](images/f06db817c7a45bac8c2a641ea755ec5ae8a42a41a99bf05e58a52494510ae974.jpg)\n\n\n![FR#10 Sum-Attn FR#65 Sum-Attn Figure 6: Sum-Attention Map in Different Scenes. FR: frame number.](images/ca7dd2d4a3ebc7f2560964a598127985ff27534ba49631a54578457848c421a0.jpg)\n\n\n# 4.5 ANALYSIS AND DISCUSSION\n\n\nHow does scene queries represent the scene? To understand why only a handful of queries can effectively represent the entire scene and even outperform more complex designs, we visualize 8 out of the 16 BEV square attention maps  $\\varpi(\\mathbf{B}_t^{navi})$  from the STL module in Fig. 5. The results reveal that each query focuses uniformly on a distinct region of the BEV space, with different queries attending to different areas. When summing the attention maps of all scene queries, we observe that the sum-attention map surprisingly covers the entire scene in a balanced manner, with greater emphasis on the front region than the back. Furthermore, the attention maps remain relatively consistent across different frames, as shown in Fig. 6, indicating that  $\\varpi(\\mathbf{B}_t^{navi})$  offers stable spatial compression guidance for SSR. In essence, the scene queries act as a compressed representation of dense BEV features, where each query concentrates on a specific spatial region.\n\n\nWhat does scene queries learn? In Fig. 7, we visualize the navigation-aware BEV features  $\\mathbf{B}_t^{navi}$  as background color map and highlight the activation positions of the scene queries across different scenarios. When overtaking a vehicle on the left as shown in Fig. 7(a), the activation positions primarily focus on the overtaked vehicle and the left rear area, anticipating potential risks. In a straightforward driving scenario (Fig. 7(b)), the scene queries are more dispersed, with attention directed towards a front-right vehicle, potentially anticipating a cut-in. During a right turn at an intersection (Fig. 7(c)), the scene queries not only activate around the right rear vehicle but also pay attention to the left crosswalk, where pedestrians might appear. When we further compare the same command in different scenes between Fig. 7(b) and Fig. 8(b), the scene queries focus on different regions adaptively. These observations demonstrate how our sparse scene queries can understand various scenes and manage different driving scenarios.\n\n\nHow does navigation information work? In Tab. 5, We conduct experiments to assess the effect of navigation commands in different scenarios, demonstrating that navigation guidance improves planning results across all cases. We further test different commands within the STL module for the same frame at an intersection, and visualize the corresponding scene queries in Fig. 8. Compared to the original command go straight, when the command is changed to turn left, the module shifts its attention to pedestrians on the left. Similarly, with the turn right command, the STL module increases its focus on the front-right area, particularly highlighting a vehicle on the right that is not prominent in the other navigation scenarios. These findings demonstrate that navigation commands effectively guide the STL module to extract relevant information from the scene. Additionally, we conduct experiments to evaluate the impact of confusing navigation commands in Appendix E.\n\n\n![(a) Turn Left](images/40e70bc84b28c5a166ca6f7ae07b90a7331d35e9260ed77cdd3976bef1e01483.jpg)\n\n\n![(b) Go Straight](images/f015003f81c11c4e9fddedd2d05a1dd718047516140d3595a8137a302c6243de.jpg)\n\n\n![(c) Turn Right](images/795b016563cd8dccfbf5fcf49e1fd3a29e4bb33ee7cf1d51c0e0b8288cd2a6dc.jpg)\n\n\n![Figure 7: Visualization of Scenes Queries in Different Scenarios. The central green box represents ego vehicle. The red boxes indicate the ground truth object, while the dotted lines denote ground truth map. The red star marker is the most activated position of each scene query.](images/1289d6ad08c16159f4e15b220ec49dcb36661afc0587a72e40b88702e8276011.jpg)\n\n\n![(a) Turn Left (b) Go Straight](images/29848db335bf305a5f1329912f26b3435b6a0ac149f3bf167b898ddde3258e40.jpg)\n\n\n![(c) Turn Right Figure 8: Visualization of Scene Queries for Different Navigation Commands in Same Scene. Different navigation commands are input to the SSR within the same scene to investigate their impact on scene queries. The original command is go straight.](images/3a06acfbc4051e83e0958ce90e3fb2ffa2b14e94b11d39a1ab7de9cad58f65f4.jpg)\n\n\nTo summarize above discussions, we revisit the generation of scene query in Eq. 5, where  $\\mathbf{B}_t^{navi}$  encodes the navigation information, and  $\\varpi_i(\\mathbf{B}_t^{navi})$  is responsible for spatial compression:\n\n\n$$\n\n$$\n\\mathbf {s} _ {i} = \\rho \\left(\\underbrace {\\mathbf {B} _ {t} ^ {\\text {n a v i}}} _ {\\text {N a v i G u i d a n c e}} \\odot \\underbrace {\\varpi_ {i} \\left(\\mathbf {B} _ {t} ^ {\\text {n a v i}}\\right)} _ {\\text {S p a t i a l C o m p r e s s i o n}}\\right). \\tag {16}\n$$\n\n$$\n\n\n# 4.6 VISUALIZATION\n\n\n![Figure 9: Visualization of Planning Results. The perception results are rendered from annotations. In Fig. 9, we present a qualitative result of SSR on planning trajectories, demonstrating strong alignment with the ground truth compared to VAD-Base. Additional visualizations across various scenes, including failure cases, can be found in the Appendix B and C due to space limitations.](images/e742f18efb2de74c686ea334027f61af6127f9a93a4a37e21100420c4d36b291.jpg)\n\n\n# 5 CONCLUSION\n\n\nThe SSR framework presents a significant advancement in the field of E2EAD by challenging the conventional reliance on perception tasks. By utilizing navigation-guided sparse tokens and temporal self-supervision, SSR addresses the limitations of perception-heavy architectures, achieving state-of-the-art performance with minimal costs. Moreover, visualization of the sparse tokens enhances the interpretability and transparency of SSR's navigation-guided process. We hope SSR can provide a strong foundation for scalable, interpretable, and efficient autonomous driving systems.\n\n\nLimitations and Future Work. Despite these advances, current simple navigation commands may constrain SSR's effectiveness in more complex driving scenarios. Future work will explore integrating more sophisticated navigation prompts, such as routing and natural language.\n\n\n# REFERENCES\n\n- Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.\n\n- Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, 2020.\n\n- Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Krahenbuhl. Learning by cheating. In CoRL, pp. 66-75. PMLR, 2020.\n\n- Felipe Codevilla, Eder Santana, Antonio M L\u00f3pez, and Adrien Gaidon. Exploring the limitations of behavior cloning for autonomous driving. In CVPR, pp. 9329-9338, 2019.\n\n- Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: an open urban driving simulator. In CoRL, pp. 1-16, 2017.\n\n- Mingzhe Guo, Zhipeng Zhang, Yuan He, Ke Wang, and Liping Jing. End-to-end autonomous driving without costly modularization and 3d manual annotation. arXiv preprint arXiv:2406.17680, 2024.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\n- Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.\n\n- Peiyun Hu, Aaron Huang, John Dolan, David Held, and Deva Ramanan. Safe local motion planning with self-supervised freespace forecasting. In CVPR, 2021.\n\n- Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. St-p3: End-to-end vision-based autonomous driving via spatial-temporal feature learning. In ECCV, 2022.\n\n- Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. In CVPR, 2023.\n\n- Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Tri-perspective view for vision-based 3d semantic occupancy prediction. In CVPR, 2023.\n\n- Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, and Hongyang Li. Driveadapter: Breaking the coupling barrier of perception and planning in end-to-end autonomous driving. In ICCV, 2023a.\n\n- Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, and Hongyang Li. Think twice before driving: towards scalable decoders for end-to-end autonomous driving. In CVPR, 2023b.\n\n- Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In ICCV, 2023.\n\n- Tarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar, David Held, and Deva Ramanan. Differentiable raycasting for self-supervised occupancy forecasting. In ECCV, 2022.\n\n- Qi Li, Yue Wang, Yilun Wang, and Hang Zhao. Hdmapnet: An online hd map construction and evaluation framework. In ICRA, 2022a.\n\n- Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang, and Tieniu Tan. Enhancing end-to-end autonomous driving with latent world model. arXiv preprint arXiv:2406.08481, 2024a.\n\n- Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers. In ECCV, 2022b.\n\n- Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose M. Alvarez. Is ego status all you need for open-loop end-to-end autonomous driving? In CVPR, 2024b.\n\n- Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Wenyu Liu, and Chang Huang. Maptr: Structured modeling and learning for online vectorized hd map construction. arXiv preprint arXiv:2208.14437, 2022.\n\n- Xuewu Lin, Tianwei Lin, Zixiang Pei, Lichao Huang, and Zhizhong Su. Sparse4d: Multi-view 3d object detection with sparse spatial-temporal fusion. arXiv:2211.10581, 2022.\n\n- Haisong Liu, Yao Teng, Tao Lu, Haiguang Wang, and Limin Wang. Sparsebev: High-performance sparse 3d object detection from multi-camera videos. In ICCV, 2023.\n\n- Yicheng Liu, Yue Wang, Yilun Wang, and Hang Zhao. Vectormapnet: End-to-end vectorized hd map learning. arXiv preprint arXiv:2206.08920, 2022.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n\n- Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In ECCV, 2020.\n\n- Dean A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In NeurIPS, 1988.\n\n- Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-modal fusion transformer for end-to-end autonomous driving. In CVPR, 2021.\n\n- Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Token-learner: Adaptive space-time tokenization for videos. In NeurIPS, 2021.\n\n- Chonghao Sima, Wenwen Tong, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin, and Hongyang Li. Scene as occupancy. In ICCV, 2023.\n\n- Wenchao Sun, Xuewu Lin, Yining Shi, Chuang Zhang, Haoran Wu, and Sifa Zheng. Sparsedrive: End-to-end autonomous driving via sparse scene representation. arXiv preprint arXiv:2405.19620, 2024.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n\n- Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xiangyu Zhang. Exploring object-centric temporal modeling for efficient multi-view 3d object detection. arXiv:2303.11926, 2023.\n\n- Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone. Para-drive: Parallelized architecture for real-time autonomous driving. In CVPR, 2024.\n\n- Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, and Yu Qiao. Trajectory-guided control prediction for end-to-end autonomous driving: a simple yet strong baseline. NeurIPS, 2022.\n\n- Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. End-to-end interpretable neural motion planner. In CVPR, 2019.\n\n- Diankun Zhang, Guoan Wang, Runwen Zhu, Jianbo Zhao, Xiwu Chen, Siyu Zhang, Jiahao Gong, Qibin Zhou, Wenyuan Zhang, Ningzi Wang, Feiyang Tan, Hangning Zhou, Ziyao Xu, Haotian Yao, Chi Zhang, Xiaojun Liu, Xiaoguang Di, and Bin Li. Sparsead: Sparse query-centric paradigm for efficient end-to-end autonomous driving. arXiv preprint arXiv:2404.06892, 2024.\n\n- Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. End-to-end urban driving by imitating a reinforcement learning coach. In ICCV, 2021.\n\n- Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu. Occworld: Learning a 3d occupancy world model for autonomous driving. In ECCV, 2024a.\n\n- Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming Zhang, and Long Chen. Genad: Generative end-to-end autonomous driving. In ECCV, 2024b.\n\n- Zhuofan Zong, Dongzhi Jiang, Guanglu Song, Zeyue Xue, Jingyong Su, Hongsheng Li, and Yu Liu. Temporal enhanced training of multi-view 3d object detector via historical object prediction. In ICCV, 2023.\n\n- Jialv Zou, Bencheng Liao, Qian Zhang, Wenyu Liu, and Xinggang Wang. Mim4d: Masked modeling with multi-view video for autonomous driving representation learning. arXiv preprint arXiv:2403.08760, 2024.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "TextualGradientDescent prompt for update", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "165", "funcName": "_update_prompt", "created": "1768638583.6072583", "msecs": "607.0", "relativeCreated": "180437.75582313538", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "prompt": "Here is the role of the variable you will improve: <ROLE>System prompt that sets the AI reviewer's role and approach for paper evaluation</ROLE>.\n\nThe variable is the text within the following span: <VARIABLE> You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}. </VARIABLE>\n\nHere is the context and feedback we got for the variable:\n\n<CONTEXT></CONTEXT>\n\nImprove the variable (System prompt that sets the AI reviewer's role and approach for paper evaluation) using the feedback provided in <FEEDBACK> tags.\nYou must follow the following constraints:\n\n<CONSTRAINTS>Constraint 1: Keep the system prompt under 500 characters\nConstraint 2: Keep the human prefix under 800 characters\nConstraint 3: The human prefix MUST retain the output format instruction: \\boxed{Accept} or \\boxed{Reject}\nConstraint 4: The human prefix should clearly state the task is predicting paper acceptance</CONSTRAINTS>\n\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.", "message": "TextualGradientDescent prompt for update"}
{"name": "textgrad", "msg": "TextualGradientDescent optimizer response", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "179", "funcName": "step", "created": "1768638610.2991254", "msecs": "299.0", "relativeCreated": "207129.62293624878", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "optimizer.response": "<think>\nOkay, let's see. The user wants to improve the system prompt for an AI reviewer that evaluates research papers. The original prompt is: \"You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\" \n\nFirst, I need to check the constraints. The system prompt must be under 500 characters. Let me count the original. The original is 103 characters. That's well under, so maybe that's not an issue. But maybe it can be more precise.\n\nThe constraints also mention the human prefix (which I think refers to the input or the task description) must be under 800 characters. Wait, the original variable is the system prompt. Wait, the user says the variable is the system prompt. So maybe the human prefix is part of the input? Wait, the original variable is the system prompt. The user says the variable is the text within the span, which is the system prompt. So maybe the constraints are about the system prompt's length. Let me check again.\n\nConstraint 1: Keep the system prompt under 500 characters. The original is 103, so that's fine. But maybe the user wants to make it more specific. \n\nConstraint 2: Keep the human prefix under 800 characters. Wait, the human prefix might be the input given to the model. But in this case, the variable is the system prompt. Maybe the user is referring to the input (the user's query) being under 800 characters. But the original variable is the system prompt, not the input. Hmm, maybe the user is confused here. But since the variable is the system prompt, perhaps the constraints are about the system prompt's length. \n\nConstraint 3: The human prefix must retain the output format instruction: \\boxed{Accept} or \\boxed{Reject}. Wait, the original system prompt already includes that. So maybe the human prefix refers to the input that the user gives to the model. But the variable is the system prompt. So perhaps the user is saying that the output format instruction must be present. The original does have that, so maybe that's already satisfied.\n\nConstraint 4: The human prefix should clearly state the task is predicting paper acceptance. The original says \"Predict acceptance with...\", so that's covered. But maybe it can be more explicit.\n\nSo the feedback might be that the original system prompt is too vague. The user wants the AI reviewer to assess methodology, significance, and clarity, but maybe the feedback wants more specific criteria or a clearer structure. Also, the original is concise but perhaps could be more detailed to ensure the AI reviewer follows the correct approach.\n\nLet me think about improving it. Maybe adding more specific instructions on how to evaluate each aspect. For example, specifying that the reviewer should check for originality, experimental design, data analysis, and contribution to the field. Also, ensuring that the prediction is based on these factors. \n\nBut the original prompt is already concise. Maybe the feedback is that the system prompt is too short and needs to include more elements. However, the constraints say to keep it under 500 characters. Let me check the original length again. Let's count:\n\nOriginal: \"You are an expert academic reviewer evaluating research papers. Assess methodology, significance, and clarity. Predict acceptance with \\boxed{Accept} or \\boxed{Reject}.\"\n\nThat's 103 characters. So there's room to add more details without exceeding the limit. \n\nPerhaps adding more specific criteria for each assessment area. For example, \"Evaluate methodology for rigor and reproducibility, significance for impact and novelty, and clarity for exposition and structure.\" Then, the prediction should be based on these factors. \n\nAlso, maybe adding that the reviewer should consider the paper's contribution to the field and whether the conclusions are supported by the data. \n\nBut I need to keep it under 500 characters. Let me try to rephrase. \n\nAnother angle: the original prompt might not explicitly mention the criteria for acceptance. The feedback might want the system prompt to include the criteria for acceptance. For example, specifying that the paper should have a clear contribution, sound methodology, and significant results. \n\nSo, the improved system prompt could be: \"You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\" \n\nLet me count the characters. Let's see: \n\n\"You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\"\n\nThat's 228 characters. Well under 500. It adds more specific criteria for each assessment area, which might make the AI reviewer's evaluation more precise. Also, it explicitly states that the prediction is based on these factors, which aligns with constraint 4. \n\nAdditionally, the output format is retained with the boxed options. \n\nSo this should meet all the constraints and improve the original prompt by making the evaluation criteria more explicit. That should make the AI reviewer's task clearer and more structured, leading to better predictions.\n</think>\n\n<IMPROVED_VARIABLE>You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.</IMPROVED_VARIABLE>", "message": "TextualGradientDescent optimizer response"}
{"name": "textgrad", "msg": "TextualGradientDescent updated text", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "187", "funcName": "step", "created": "1768638610.2992508", "msecs": "299.0", "relativeCreated": "207129.7483444214", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "parameter.value": "You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.", "message": "TextualGradientDescent updated text"}
{"name": "textgrad", "msg": "TextualGradientDescent prompt for update", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "165", "funcName": "_update_prompt", "created": "1768638610.299328", "msecs": "299.0", "relativeCreated": "207129.82559204102", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "prompt": "Here is the role of the variable you will improve: <ROLE>Task instruction prefix that appears before each paper, specifying the task (accept/reject prediction) and output format (\\boxed{Accept} or \\boxed{Reject})</ROLE>.\n\nThe variable is the text within the following span: <VARIABLE> Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}. </VARIABLE>\n\nHere is the context and feedback we got for the variable:\n\n<CONTEXT></CONTEXT>\n\nImprove the variable (Task instruction prefix that appears before each paper, specifying the task (accept/reject prediction) and output format (\\boxed{Accept} or \\boxed{Reject})) using the feedback provided in <FEEDBACK> tags.\nYou must follow the following constraints:\n\n<CONSTRAINTS>Constraint 1: Keep the system prompt under 500 characters\nConstraint 2: Keep the human prefix under 800 characters\nConstraint 3: The human prefix MUST retain the output format instruction: \\boxed{Accept} or \\boxed{Reject}\nConstraint 4: The human prefix should clearly state the task is predicting paper acceptance</CONSTRAINTS>\n\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.", "message": "TextualGradientDescent prompt for update"}
{"name": "textgrad", "msg": "TextualGradientDescent optimizer response", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "179", "funcName": "step", "created": "1768638624.618661", "msecs": "618.0", "relativeCreated": "221449.1584300995", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "optimizer.response": "<think>\nOkay, let's see. The user wants me to improve the task instruction prefix for a paper acceptance prediction task. The original variable is \"Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\" \n\nFirst, I need to check the constraints. The system prompt must be under 500 characters, but since the variable is the human prefix, maybe that's about the input. Wait, the constraints mention keeping the system prompt under 500, but the variable here is the human prefix. The user's role is the task instruction prefix before each paper. \n\nThe constraints say the human prefix must be under 800 characters. The original is pretty short, but maybe it can be more clear. The key points are: clearly state the task is predicting acceptance, and retain the output format. \n\nThe original has \"Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\" Maybe making it more explicit. Like \"Determine if the paper should be accepted. Output: \\boxed{Accept} or \\boxed{Reject}.\" That way, the task is clearer. \n\nAlternatively, \"Classify the paper's acceptance status. Output: \\boxed{Accept} or \\boxed{Reject}.\" But the original uses \"predict,\" which is good. Maybe \"Predict whether the paper will be accepted. Output: \\boxed{Accept} or \\boxed{Reject}.\" That's more explicit. \n\nCheck character count. Let me count: \"Predict whether the paper will be accepted. Output: \\boxed{Accept} or \\boxed{Reject}.\" That's 67 characters. Well under 800. \n\nWait, the original was \"Predict paper acceptance. Output: \\boxed{Accept} or \\boxed{Reject}.\" Which is 46 characters. But maybe adding \"whether\" makes it clearer. Also, the task is to predict acceptance, so \"Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\" \n\nAlternatively, \"Determine the acceptance status of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\" \n\nBut the original uses \"predict,\" which is more aligned with the task. Let me ensure that the output format is retained. All options include the output format. \n\nMaybe the original is sufficient, but the feedback might have indicated that it's not clear enough. Since there's no specific feedback given, but the user wants optimization. Maybe making it more explicit about the task. \n\nSo, the improved version could be: \"Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\" That's clear, concise, and meets all constraints. Let me check the character count: 55 characters. Well under 800. \n\nYes, that should work. It clearly states the task and the output format.\n</think>\n\n<IMPROVED_VARIABLE>Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.</IMPROVED_VARIABLE>", "message": "TextualGradientDescent optimizer response"}
{"name": "textgrad", "msg": "TextualGradientDescent updated text", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "187", "funcName": "step", "created": "1768638624.6187801", "msecs": "618.0", "relativeCreated": "221449.27763938904", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "parameter.value": "Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.", "message": "TextualGradientDescent updated text"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638625.680711", "msecs": "680.0", "relativeCreated": "222511.20853424072", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Visual Timing For Sound Source Depth Estimation in the Wild\n\n# Abstract\nDepth estimation enables a wide variety of 3D applications, such as robotics and autonomous driving. Despite significant work on various depth sensors, it is challenging to develop an all-in-one method to meet multiple basic criteria. In this paper, we propose a novel audio-visual learning scheme by integrating semantic features with physical spatial cues to boost monocular depth with only one microphone. Inspired by the flash-to-bang theory, we develop FBDepth, the first passive audio-visual depth estimation framework. It is based on the difference between the time-of-flight (ToF) of the light and the sound. We formulate sound source depth estimation as an audio-visual event localization task for collision events. To approach decimeter-level depth accuracy, we design a coarse-to-fine pipeline to push the temporary localization accuracy from event-level to millisecond-level by aligning audio-visual correspondence and manipulating optical flow. FBDepth feeds the estimated visual timestamp together with the audio clip and object visual features to regress the source depth. We use a mobile phone to collect 3.6K+ video clips with 24 different objects at up to 60m. FBDepth shows superior performance especially at a long range compared to monocular and stereo methods.\n\n\n# 1 INTRODUCTION\n\n\nDepth estimation is the fundamental functionality to enable 3D perception and manipulation. Although there have been significant efforts on developing depth estimation methods with various sensors, current depth estimation schemes fail to achieve a good balance on multiple basic metrics including accuracy, range, angular resolution, cost, and power consumption.\n\n\nActive depth sensing methods actively emit signals, such as LiDAR (Caesar et al., 2020), structured-light (Zhang, 2012), mmWave (Barnes et al., 2020), ultrasound (Mao et al., 2016), WiFi (Vasisht et al., 2016). They compare the reflected signal with the reference signal to derive time-of-flight (ToF), phase change, or Doppler shift to estimate the depth. Active methods can achieve high accuracy because of the physical fundamental and well-designed modulated sensing signals. Lidar is the most attractive active senor due to its large sensing range and dense point cloud. However, the density is not sufficient enough to enable a small angular resolution. Therefore, the points are too sparse to be recognized at a long distance. Besides, the prohibitive cost and power consumption limit the availability of Lidar on general sensing devices.\n\n\nPassive depth sensing takes signals from the environment for sensing directly. It commonly uses RGB monocular camera (Bhoi, 2019; Laga et al., 2020), stereo camera (Cheng et al., 2020), thermal camera (Lu & Lu, 2021), or multi-view cameras (Long et al., 2021a). These sensors can achieve pixel-wise angular resolution and consume pretty less energy due to omitting the signal emitting. Among them, stereo matching can effectively estimate the disparity and infer a dense depth map since it transforms the spatial depth to the visual disparity based on the solid physical law. The baseline of the stereo camera determines the effective range and accuracy. Therefore, the dimension of the stereo camera is placed as the critical trade-off with sensing metrics. Thanks to the advance in deep learning, the cheap monocular depth estimation keeps on improving performance with new network structures and high-quality datasets. However, the accuracy is still not satisfactory especially at a long range because it can only regress depth based on the implicit visual cues. It is ill-posed without any physical formulation. Besides, it heavily relies on the dataset. It requires domain adaption and camera calibration for various camera intrinsics (Li et al., 2022).\n\n\nIn this paper, we propose to add only one microphone to enable explicit physical depth measurement and boost the performance of a single RGB camera. It does not rely on the intrinsic of cameras and implicit visual cues. We develop a novel passive depth estimation scheme with a solid physical formulation, called Flash-to-Bang Depth (FBDepth). Flash-to-Bang is used to estimate the distance to the lightning strike according to the difference between the arrival time of a lightning flash and a thunder crack. This works because light travels a million times faster than sound. When the sound source is several miles away, the delay is large enough to be perceptible. Applying it to our context, FBDepth can estimate the depth of a collision that triggers audio-visual events. The collision event has been explored for navigation and physical search in (Gan et al., 2022), but our work is the first that uses the collision for depth estimation. Collisions are common and can arise when a ball bounces on the ground, a person takes a step, or a musician hits a drum. We identify and exploit several unique properties related to various collisions in the wild. First, the duration of a collision is short and collision events are sparse. Thus, there are few overlapped collisions. Second, though the motion of objects changes dramatically after the collision, they are almost static at the collision moment. Third, the impact sound is loud enough to propagate to a long range.\n\n\nFlash-to-Bang is applied to the range of miles for human perception. Using it for general depth estimation poses several significant challenges: (i) It is inaccessible to ground truth collision time from video and audio. Video only offers up to 240 frames per second(fps), and may not capture the exact instance when the collision occurs. Audio has a high sampling rate but it is hard to detect the start of a collision solely based on the collision sound due to different sound patterns arising from collisions as well as ambient noise. (ii) We need highly accurate collision time. 1 ms error can result in a depth error of  $34~\\mathrm{cm}$ . (iii) Noise present in both audio and video further exacerbate the problem.\n\n\nTo realize our idea, we formulate the sound source depth estimation as the audio-visual localization task. Whereas existing work (Wu et al., 2019; Xia & Zhao, 2022) still focuses on 1-second-segment level localization. FBdepth performs event-level localization by aligning correspondence between the audio and the video. Apart from audio-visual semantic features as input in existing work (Tian et al., 2018; Chen et al., 2021a), we incorporate optical flow to exclude static objects with similar visual appearances. Furthermore, FBDepth applies the impulse change of optical flow to locate collision moments at the frame level. Finally, we formulate the ms-level estimation as an optimization problem of video interpolations. FBDepth succeeds to interpolate the best collision moment by maximizing the intersection between extrapolations of before-collision and after-collision flows.\n\n\nWith the estimated timestamp of visual collision, we regress the sound source depth with the audio clip and visual features. FBdepth avoids the requirement to know the timestamp of audio collision. Besides, different objects have subtle differences in audio-visual temporal alignment. For example, a rigid body generates the sound peak once it touches another body. But an elastic body produces little sound during the initial collision and takes several ms to produce the peak with the maximum deformation. We feed semantic features to enable the network aware of the material, size, etc.\n\n\nOur main contributions are as follows:\n\n- 1. To the best of our knowledge, FBDepth is the first passive audio-visual depth estimation. It brings the physical propagation property to audio-visual learning.\n\n- 2. We introduce the ms-level audio-visual localization task. We propose a novel coarse-to-fine method to improve temporal resolution by leveraging the unique properties of collisions.\n\n- 3. We collect  $3.6\\mathrm{K}+$  audio-visual samples across 24 different objects in the wild. Our extensive evaluation shows that FBDepth achieves  $0.64\\mathrm{m}$  absolute error(AbsErr) and  $2.98\\%$  AbsRel across a wide range from  $2\\mathrm{m}$  to  $60\\mathrm{m}$ . Especially, FBDepth shows more improvement in the longer range.\n\n\n# 2 RELATED WORK\n\n\nMulti-modality Depth estimation. Recent work on depth estimation has shown the benefits of fusing cameras and other active sensors. (Qiu et al., 2019; Imran et al., 2021) recover dense depth maps from sparse Lidar point clouds and a single image. (Long et al., 2021b) associates pixels with pretty sparse radar points to achieve superior accuracy. The effective range can be increased as well by Lidar-camera (Zhang et al., 2020) or Radar-camera (Zhang et al., 2021). However, these methods are still expensive in cost and power consumption.\n\n\n(Gao et al., 2020; Parida et al., 2021) emit audio chirps and learn the depth map implicitly with audio reflections and a single image. However, these methods require many nearby acoustic reflectors to produce effective echos so the setup is limited in rooms. Besides, they are evaluated in an audio-visual simulator. FBDepth only uses one extra microphone to perceive natural sounds directly. It keeps the passive design of the audio but applies the physical measurement explicitly. The one-path sound propagation has a longer effective range than echoes.\n\n\nSound source localization. Previous systems localize sound sources with microphone arrays (Valin et al., 2003; Rascon & Meza, 2017) or one microphone with a camera (Hershey & Movellan, 1999). They intend to estimate the direction of arrival(DOA) or the distance. The DOA is inferred by the subtle difference in arrival time from the sound source to each microphone(Mao et al., 2019; Sun et al., 2022) or by semantic matching with the visual appearance if given images(Tian et al., 2018; Arandjelovic & Zisserman, 2018). The distance can be estimated by triangulation methods with multiple DOAs and room structures(Wang et al., 2021; Shen et al., 2020). Many work study the room acoustic and the distance cues from the reverberation(Singh et al., 2021; Chen et al., 2021b) but (Zahorik, 2002) shows that the reverberation has a coarse coding with the distance. Compared to these methods, FBDepth directly estimates the distance by the ToF and achieves superior accuracy to indirect triangulation methods and implicitly depth learning networks on reverberation.\n\n\nAudio-visual event localization aims to detect and localize events in videos. (Tian et al., 2018) first propose the task and build up the audio-visual event(AVE) dataset. They apply an audio-guided visual attention mechanism to learn visual regions with the related sounding object or motions. Recent works develop dual-modality sequence-sequence framework (Lin et al., 2019) and dual attention matching mechanism (Wu et al., 2019) to leverage global features. However, the temporal event boundary is 1s-level in AVE dataset so it is split as 1s-long segments. We study the instant collision event and solve the coarse boundary problem as well.\n\n\n(Gan et al., 2022) has a similar setup to ours. They use an embodied robot agent to navigate to a dropped object in 3D virtual rooms. They integrate asynchronous vision and audition and navigate to the object. The asynchronism comes from the invisibility of the object. Even though their simulator has been pretty vivid enough for semantic tasks, it has a gap in the real-world collision for the m-level formulation. Falling objects dataset(Kotera et al., 2020), TbD dataset(Kotera et al., 2019) and TbD-3D dataset(Rozumnyi et al., 2020) explore falling motions and fast movements but they do not have audio and depth information.\n\n\nVideo frame interpolation aims to synthesize intermediate frames between existing ones of a video. Most state-of-the-art approaches explicitly or implicitly assume a simplistic linear motion. Warping-based methods (Baker et al., 2011; Park et al., 2020) apply optical flow and forward warping to shift pixels to intermediate frames linearly. Phase-based methods (Meyer et al., 2015; 2018) combine the phase information across different scales but the phase is modeled as a linear function of time. Recent methods are developed to approximate non-linear motion, such as kernel-based methods (Niklaus et al., 2017a;b), quadratic interpolation (Xu et al., 2019a), cubic motion modeling (Chi et al., 2020), etc. However, they still fail to complex non-linear motions because precise motion dynamics cannot be captured in the blind time between keyframes. Unfortunately, collisions are super non-linear and instant. Given two keyframes before and after the collision, it is ambiguous to decide whether there is a collision. Hence, these methods are not applicable. We analyze the motions before and after the collision and extrapolate optical flows to find the most potential collision position.\n\n\n# 3 PROBLEM FORMULATION\n\n\nWe formulate the depth estimation by the physical law of wave propagation. We have:\n\n\n$$\n\n$$\n\\frac {d}{v} - \\frac {d}{c} = T \\tag {1}\n$$\n\n$$\n\n\nwhere the depth of the sound source is  $d$  and the difference between the ToF of sound and light is  $T$ .  $c$  and  $v$  denote the propagation speeds of light and sound, respectively. We can estimate  $d$  based on  $d = \\frac{cvT}{c - v} \\approx vT$  since  $c \\gg v$ . We observe  $T = T_{audio} - T_{video} + T_{hardware}$ , where  $T_{audio}$  and  $T_{video}$  denote the event time in the audio and video recordings, respectively, and  $T_{hardware}$\n\n\ndenotes the start time difference in the audio and video recordings. It can be small as well as have a small variance with a well-designed media system such as the Apple AVFoundation framework. We regard it as a constant unknown bias to learn.\n\n\nIt is impossible to label the precise  $T_{video}$  and  $T_{audio}$  manually.  $T_{video}$  can be tagged at most frame-level. Even though many commercial cameras can support up to 240 FPS, it results in a 4-ms segment and 1.43m depth variation. Moreover, it is tough to determine the exact frame that is nearest to the collision in high FPS mode by a human being due to the constrained view of the camera.  $T_{audio}$  is challenging to recognize in the wild as well. Although the audio sampling rate is high enough, we can recognize the significant early peaks instead of the first sample triggered by the collision. The best effort of segmentation is 10-ms level based on real data.\n\n\nWe cannot learn the timestamp with supervision. We propose a 2-stage estimation framework. The goal of the first stage is to estimate the numerical  $T_{video}$ . As figure 1 shows, we localize the audio-visual event in the stream and then take advantage of the unique optical flow of the collision to estimate  $T_{video}$  at ms-level. In the second stage, we place the  $T_{video}$  as an anchor into the audio clip and direct regress the depth with depth supervision. We make the network optimize  $T_{audio}$  automatically with knowledge of the  $T_{video}$ , the audio waveform and visual features.\n\n\n# 4 APPROACH\n\n\n![Figure 1: Model architecture. Our audio-visual depth estimation uses the video, audio and optical flow to perform the event-level localization to retrieve the collision event. It analyzes the collision flow and estimate the collision timestamp in video. It uses multiple modalities including RGB, flow, audio and the timestamp to estimate the depth.](images/f9516a01b247146bba42363b6346fbc836500a6ee1429746ee21e85ad481c069.jpg)\n\n\nWe demonstrate a novel coarse-to-fine pipeline to localize the collision with a super temporal resolution in the video. This method does not require annotations on ms-level, which is at least two orders of magnitude finer than previous approaches. They rely on the supervision of segment annotations, such as AVE dataset with 1-second segments (Tian et al., 2018), Lip Reading Sentences 2 dataset with word-level segments (Chung & Zisserman, 2016), BOBSL with sentence-level alignments (Bull et al., 2021).\n\n\n# 4.1 EVENT-LEVEL LOCALIZATION\n\n\nAudio-visual modeling for collisions. In this step, our goal is to localize the audio-visual event for the region and the period of interest. It is similar to (Tian et al., 2018), but the unique properties of collisions bring new opportunities to learning strategy. Collisions have a significant motion than other sound sources. We can use the optical flow to inform the network of moving pixels. Besides, the impact sound is highly correlated to the rich information of objects (Gan et al., 2022), such as shape, materials, size, mass, etc. It makes audio-visual cross-matching easier than general audio-visual events so that we do not need to apply a complex scheme to learn. Another fact is that\n\n\ncollisions are pretty sparse temporally in the wild because the duration of collisions is extremely short. It is rare to come across overlapped collisions based on our empirical study on the basketball court. Only two frames have double collisions among all 1208 frames and a total of 203 collisions when 7 basketballs are played during a 40-s duration.\n\n\nWe propose a motion-guided audio-visual correspondence network (MAVNet). Similar to (Tian et al., 2018; Wu et al., 2019), MAVNet performs the cross-matching for the audio features and the RGB-F channels. Besides, it predicts audio-visual segmentation to capture whole pixels of the target object. It can achieve fine-grained audio-visual scene understanding (Zhou et al., 2022). We use the segmentation mask to filter flows of interest and perform high-resolution estimation in the next steps.\n\n\nMAVNet has two backbones to deal with RGB-F channels and audio clips respectively. A U-Net (Ronneberger et al., 2015) style encoder is applied to extract the frame features conditioned by optical flows. It uses a series of convolution layers to extract visual features. Another branch is the audio encoder which takes in the time-domain signal. It has a 1D convolution layer to learn an STFT-like representation and a stack of 2D convolution layers with batch normalization to learn the semantic audio features. We replicate the audio feature, tile them to match the visual feature dimension, and concatenate the audio and visual feature maps. MAVNet has two output heads as well. The U-Net decoder applies a series of up-convolutions and skip-connections from the RGB-F encoder to fused feature maps to learn the binary segmentation mask  $M$ . Meanwhile, the fused feature map is fed into a binary classification head consisting of convolution layers and linear layers to predict the audio-visual event relevance  $y \\in \\{0,1\\}$ .\n\n\nTraining We use the weighted sum Binary Cross Entropy (BCE) loss as the training objective for both segmentation and the cross matching. We train all components to jointly optimize the location predictions and energy reconstruction. We minimize the total loss  $\\mathcal{L}_{total} = BCE(M, \\hat{M}) + \\lambda * BCE(y, \\hat{y})$  where  $\\lambda$  is the hypermeter to set.\n\n\nInference We only use low FPS to perform MAVNet to avoid dense inference at this stage. Moreover, we do not need to activate the segmentation head until the audio clip and the frame are highly matched. Finally, MAVNet uses this audio clip to retrieve a sequence of frames including the full collision procedure.\n\n\n# 4.2 FRAME-LEVEL LOCALIZATION\n\n\nGiven a sequence of video frames, our goal is to split them into two sets: the frames before the collision  $\\mathbf{V_0}$  and the frames after the collision  $\\mathbf{V_1}$ . This essentially requires us to determine the last frame  $I_{e}$  in  $\\mathbf{V_0}$  before the collision and the first frame  $I_{s}$  in  $\\mathbf{V_1}$  after the collision. Thus, we locate the collision between the frame  $I_{e}$  and  $I_{s}$ .\n\n\nBased on the analysis of the physical motion, we make an important observation that can help determine  $I_{e}$  and  $I_{s}$ . The collision results in a significant acceleration change due to the strong impulse force. Let  $a_{t} = v_{t} - v_{t-1}$  and  $\\delta a_{t} = a_{t} - a_{t-1}$  denote the acceleration and acceleration change of frame  $I_{t}$ , respectively.  $\\delta a$  between  $I_{e}$  and  $I_{s}$  is large, while  $\\delta a$  between adjacent frames before or after the collision is small. If the object stops moving immediately after the collision, we take the static frame  $I_{e+1}$  as  $I_{s}$ . Finally, we select the frames before  $I_{e}$  to generate  $\\mathbf{V}_{0}$ , and select the frames after  $I_{s}$  to generate  $\\mathbf{V}_{1}$ .\n\n\nWe use the retrieved mask in the last stage to determine the object positions in the frames and calculate the velocity, acceleration, and acceleration change. We find the  $I_{e}$  and  $I_{s}$  at the low FPS and then replicate the procedure for frames between  $I_{e}$  and  $I_{s}$  at high FPS. Finally, we locate  $I_{e}$  and  $I_{s}$  in the high FPS mode efficiently.\n\n\n# 4.3 MS-LEVEL LOCALIZATION\n\n\nTo further locate the exact moment of the collision, we try to interpolate frames between  $I_{e}$  and  $I_{s}$  to recover the skipped frame. Unfortunately, the common assumption of frame-based interpolation is fully broken down.\n\n\nMotion consistency is fundamental for spatio-temporal video processing. If the motion of the object is temporally stable across several frames (e.g., due to a constant force), the position and pose can be predicted in the future frames as well as be interpolated between two frames. We denote it\n\n\nas motion first consistency. However, the impact sound is caused by an impulse force, which results in a rapid change of the motion status. It breaks the motion continuity and consistency. When we observe  $I_{e}$  and  $I_{s}$ , we cannot determine whether a collision happens or the object just flies in the air.\n\n\nLuckily, the collision moment retains a new form of motion consistency. We denote it as motion second consistency. It reveals that the motions before and after the collision share the same intersection position. Besides, they keep the motion first consistency separately. Therefore, we can extrapolate the motions based on the motion first consistency and search for the most similar motion extrapolations by leveraging motion second consistency. Note that our final goal is to find the timestamp of the collision instead of the motion status at the shared position. (Kotera et al., 2019; Rozumnyi et al., 2020) try to recover the sub-frame motions and trajectories as well but they require the high FPS ground truth to guide the training. In our context, we care more about when the collision happens than what it looks like.\n\n\nOptical flow extrapolation Optical flow is widely used for frame prediction () and interpolation (Baker et al., 2011) by warping the frame with the estimated optical flow. Because it can capture all motions of pixels and get a finer understanding of the object dynamics. The optical flow sequence is usually generated by adjacent video frames. However, it is not efficient for extrapolation. The drift of pixels in the flow requires extra iterative wrappings to align the corresponding pixels, which results in accumulation errors.\n\n\nTherefore, we compute the optical flows from an anchor frame  $I_{a}$  to the frame sequence  $\\mathcal{V}$  as  $\\{I_0,I_1,\\ldots I_n\\}$ . We can estimate the flow sequence  $\\mathcal{F}_{a\\to \\nu}$  as  $\\{f_{a\\to 0},f_{a\\to 1},\\dots f_{a\\to n}\\}$ . As  $f_{a\\to n}(x,y)$  represents the movement of the pixel  $I_{a}(x,y)$  to  $I_{n}$ ,  $\\mathcal{F}_{a\\to \\mathcal{V}}(x,y)$  describes how the pixel in  $I_{a}(x,y)$  moves across the frame sequence  $\\mathcal{V}$ . Hence,  $\\mathcal{F}_{a\\to \\mathcal{V}}$  tracks the global motion of each pixel without iterative warpings. With the historical positions of  $I_{a}(x,y)$  from frame  $I_0$  to  $I_{n}$ , we can regress the motion of this pixel and extrapolate the flow to  $f_{a\\to n + \\delta t}$ , which is the relative pixel position to  $I_{n + \\delta t}$  with an arbitrary  $\\delta t$ .\n\n\nIn our context, We pick  $k$  consecutive frames before the collision  $\\mathcal{V}_{pre}$  as  $\\{I_{e - k + 1}, I_{e - k + 2}, \\dots, I_e\\}$  and after the collision  $\\mathcal{V}_{post}$  as  $\\{I_{s + k - 1}, I_{s + k - 2}, \\dots, I_s\\}$ . We select the frame  $I_e$  as the anchor frame. It is near the collision moment, so its motion to other frames is not dramatic and easy to be estimated. Hence, we can estimate the optical flow sequences  $\\mathcal{F}_{e \\to \\mathcal{V}_{pre}}$  and  $\\mathcal{F}_{e \\to \\mathcal{V}_{post}}$ . Meanwhile, we apply the predicted segmentation mask of  $I_e$  to filter the pixels of the target object. In the last step, we build up regressors  $\\mathcal{R}$  for each pixel's motion individually and predict future locations in any sub-frame.\n\n\nOptical flow interpolation We have constructed pixel level regressors for  $\\mathcal{F}_{e\\rightarrow \\mathcal{V}_{pre}}$  and corresponding  $\\mathcal{F}_{e\\rightarrow \\mathcal{V}_{post}}$ . They can extrapolate the flow  $f_{e\\rightarrow e + \\delta t_0}$  and  $f_{a\\rightarrow s + \\delta t_1}$ , respectively.  $\\delta t_0, \\delta t_1$  are extrapolation steps. The optimization goal is to\n\n\n$$\n\n$$\n\\min  _ {e - s \\leq \\delta t _ {1} \\leq 0 \\leq \\delta t _ {0} \\leq s - e} | | f _ {e \\rightarrow e + \\delta t _ {0}}, f _ {a \\rightarrow s + \\delta t _ {1}} | | _ {2}, \\text {s . t .} e + \\delta t _ {0} <   s + \\delta t _ {1}\n$$\n\n$$\n\n\nThe collision duration is  $s + \\delta t_1 - (e + \\delta t_0)$ , which is always more than 0.  $e + \\delta t_0$  is the target m-level localization  $\\hat{T}_{video}$ . We can apply this interpolation methodology to search the intersection of the object's center trajectory or maximize the Intersection over Union (IoU) of the object's bounding box. However, both only use several key points so they cannot achieve a fine granularity since the optical flow takes advantage of thousands of pixels.\n\n\n# 4.4 DEPTH REGRESSION\n\n\nBased on the estimation  $\\hat{T}_{video}$ , we directly regress the depth to fit the  $T_{audio}$  and the bias  $T_{Hardware}$  with the supervision of ground truth depth. We observe that the sound generation procedure varies a lot across different objects, materials, shapes, and motions. On one hand, the diverse waveforms make it impractical to measure the exact  $T_{audio}$  manually. On the other hand, each specific waveform has significant implications on what is the best  $T_{audio}$  corresponding to  $\\hat{T}_{video}$ . To combat the background noise from other sources, we also feed the RGB-F crop of the target object from frame  $I_e$  to the depth predictor. It includes the semantic features of the object as well as the motion status just before the collision. These cues can guide the predictor to find the waveform pattern easily.\n\n\nWe select a sequence of audio samples starting from  $I_{e}$  and label some anchor samples as 1 at  $\\hat{T}_{video}$ . It informed the audio sequence about the timestamp of the visual collision directly. We feed the enriched sequence into the 1D convolution layer to extract a 2D representation. It is followed\n\n\nby two residual blocks to learn high-dimension features. Meanwhile, we use ResNet-18 (He et al., 2015) to extract the RGB-F features of the target object. We tile and concatenate the RGB-F features to the audio features along the channel dimension and append another two residual blocks to fuse the features. Finally, it is followed by a pooling layer and a fully connected layer to predict the depth. The output maps to depth by the 2D projection. We use Mean Square Error (MSE)  $\\mathcal{L}_{depth} = ||d,d||_2$  as the learning objective where  $d$  and  $\\hat{d}$  are the target depth and the predicted depth.\n\n\n# 5 EXPERIMENTS\n\n\n# 5.1 SETUP\n\n\nDataset platform and collection We use an iPhone XR with a 240-fps slow-motion mode to collect the video with audio. The audio sampling rate is 48Khz. We set a stereo camera and a Lidar together to collect ground truth. We include details of data collection in the Appendix B.\n\n\nAVD Dataset We collect  $3.6\\mathrm{K}+$  raw audio-visual sequences with a single collision event as the audio-visual depth(AVD) dataset. We randomly sample raw sequences to generate train/val/eval splits, which have 2600/500/522 sequences. We augment the raw sequences by cropping one moving object from a raw video sequence and inserting it into another raw sequence with a random temporal location. Besides, we augment the raw depth with a maximum  $3\\%$  random change to diversify the depth and shift audio samples accordingly to the video timestamp. More details are described in Appendix B.\n\n\nBaselines We include three types of baseline for comparison. We compare to a monocular depth estimation method NeWCRFs (Yuan et al., 2022), a state-of-the-art(SOTA) on multiple benchmarks. We also compare to stereo matching methods including the ZED built-in ultra depth estimation SDK and a SOTA method LEAStereo (Cheng et al., 2020). We use dense depth maps collected by the Lidar to finetune the NeWCRFs and LEAStereo on images collected by the stereo camera. Despite optical flow based interpolation, we compare to interpolation using key points such as the trajectories of center or bounding boxes.\n\n\nMetrics We use the mean absolute depth errors as  $AbsErr = \\frac{1}{n}\\sum_{i=1}^{n}|d - \\hat{d}|$ , root mean square absolute relative errors  $RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(d - \\hat{d})^2}$ ,  $AbsRel = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{|d - \\hat{d}|}{d}$  as the end-to-end performance metrics. FBDepth is a sparse depth estimation. We evaluate the depth of each target object. However, monocular and stereo baselines have dense depth estimations for all pixels of the object. We evaluate the median estimation depth with the median depth of the ground truth dense map. We provide the results over different distance ranges as close(\u2264 10m), mid(10m-30m), and far(\u2265 30m). Intuitively, there is an upper bound for the temporal resolution so  $AbsRel$  at close depths performs worse than at further distances.\n\n\n5.2 RESULTS\n\n\n<table><tr><td>Method</td><td>Input</td><td>FPS</td><td>AbsErr(m) close/mid/far/all</td><td>AbsRel(%) close/mid/far/all</td><td>RMSE(m) close/mid/far/all</td></tr><tr><td>NeWCRFs</td><td>V</td><td>-</td><td>0.553/1.09/3.27/1.68</td><td>11.1/6.74/8.64/9.48</td><td>0.895/1.51/5.82/3.49</td></tr><tr><td>ZED SDK</td><td>S</td><td>-</td><td>0.083/0.96/5.10/2.03</td><td>1.78/6.05/12.7/7.28</td><td>0.108/1.07/6.30/3.69</td></tr><tr><td>LEAStereo</td><td>S</td><td>-</td><td>0.067/0.66/2.47/0.88</td><td>1.48/4.24/5.98/4.09</td><td>0.083/0.76/5.08/2.95</td></tr><tr><td>FBDepth</td><td>A+V</td><td>30</td><td>0.485/0.83/1.33/0.95</td><td>10.9/5.20/3.32/4.26</td><td>0.731/1.01/2.29/1.51</td></tr><tr><td>FBDepth</td><td>A+V</td><td>60</td><td>0.418/0.70/1.11/0.72</td><td>8.94/4.33/2.79/3.34</td><td>0.597/0.83/1.86/1.27</td></tr><tr><td>FBDepth</td><td>A+V</td><td>120</td><td>0.392/0.61/0.98/0.67</td><td>8.42/3.79/2.49/3.11</td><td>0.534/0.75/1.68/1.09</td></tr><tr><td>FBDepth</td><td>A+V</td><td>240</td><td>0.337/0.58/0.95/0.64</td><td>7.25/3.55/2.41/2.98</td><td>0.476/0.69/1.61/1.03</td></tr></table>\n\n\nTable 1: A comprehensive comparison for different depth estimation approaches. V, S, A represent visual, stereo, audio respectively. We input video with different frame rates as well.\n\n\nTable 1 shows the results on the depth estimation. In all, FBDepth can achieve better performance on all metrics than baselines across different FPS. Several important trends can be observed. Stereo matching methods perform extraordinarily on close objects, where more clear view difference can be captured. The AbsErr and RMSE increase dramatically as the targets become further because\n\n\nthe limited baseline cannot resolve the view difference easily. In the other side, the AbsErr and RMSE of FBDepth grows slowly with the increasing distance while its AbsRel decreases gradually. Intuitively, there is a upper bound for the temporal resolution due to the limited FPS, the lack of the accurate timestamp and the small disturbance of audio-video software. Thus FBDepth may not achieve the centimeter level easily. A Further depth can break the assumption of stereo matching methods as well as monocular methods which has a fixed depth range of training data, but FBDepth still holds the physical propagation law in this condition.\n\n\nFBdepth also shows advantages on NeWCRFs. The monocular methods rely on the training set, which includes various scenarios and depths. Although we apply camera remapping with intrinsic matrix and finetuning, NeWCRFs still cannot achieve the best performance as the one in the pretrained dataset. The implicit depth regression has difficulty in domain adaption. In the contrast, stereo methods can be directly applied to the new scenario and achieve awesome estimation because its fundamental is the explicit spatial view difference on stereo images. FBDepth applies the explicit spatial measurement and does not reply on the camera and scenarios heavily. It requires several learning models but these model can be applied to common cameras and microphones. FBDepth can be more general with a more diverse dataset. We show some visual qualitative results in the Appendix B.3. Compared to other methods, the delay between audio and video can be visually recognized, which is similar to object detection. In another word, FBDepth transforms the tough depth estimation problem to a simple interpretable problem.\n\n\n# 5.3 ABLATION\n\n\nIn the ablation study, we show how each stage contributes to the final results.\n\n\n<table><tr><td>Method</td><td>AbsErr(m)</td><td>AbsRel(%)</td><td>RMSE(m)</td><td>Recall</td><td>Precision</td></tr><tr><td>event loc w/o flow</td><td>-</td><td>-</td><td>-</td><td>87.3</td><td>93.7</td></tr><tr><td>FBDepth w/o interp</td><td>1.95</td><td>9.16</td><td>4.07</td><td>-</td><td>-</td></tr><tr><td>FBDepth w/ center</td><td>1.39</td><td>6.57</td><td>2.31</td><td>-</td><td>-</td></tr><tr><td>FBDepth w/_bbox</td><td>1.23</td><td>5.65</td><td>2.06</td><td>-</td><td>-</td></tr><tr><td>FBDepth w/o RGB-F</td><td>0.92</td><td>4.25</td><td>1.42</td><td>-</td><td>-</td></tr><tr><td>FBDepth</td><td>0.64</td><td>2.98</td><td>1.03</td><td>94.5</td><td>98.7</td></tr></table>\n\n\nTable 2: Ablation study for FBDepth using different setups at each stage. The input is 240 FPS.\n\n\nEvent-level localization We invest how the optical flow can help detect the collision event as well as contour the object mask. We define recall and precision as the percentage of correct recognized audio-visual events in all audio-visual events and all recognized events with an IoU more than 0.5, respectively. With the flow, both recall and precision improve as the flow can work as a pre-mask to guide the network. The main failures in recall come from weak collision sounds or simultaneous collisions. The incorrect recognition is mainly due to similar objects in the frame.\n\n\n![(a) Temporal error of the estima- (b) Improvement ratio of tempo  \ntion of low FPS compared to 240 ral resolution FPS](images/b6344b31ea4e5e006eea0189956a758bb60c7e91ea6b33e750c46593450129e3.jpg)\n\n\n![Figure 2: Effectiveness the video event detection in the second stage; change figure ratio](images/64ac59a66a20880f4c1ef4240ed7444fad4900cd841c86af619a94e426b1ffb5.jpg)\n\n\nFrame-level localization Frame rate is most related to the frame-level stage. We observe that increasing the frame rate reduces the numerical error of FBDepth in Table 1. Especially, increasing 30 FPS to 60 FPS yields the largest improvement, and the benefit gradually tapers off with a further increase in the frame rate. We observe that 30 FPS is too slow to capture sudden movements and\n\n\nfast dynamics while 60 FPS is around the borderline. It is consistent with the trend to set 60 FPS as the default video recording and playing. The motion in 120FPS and 240 FPS is even slower so it is more difficult to distinguish the frame  $I_{e}$ . The frame error is no more than the one in the low FPS mode. Thus, 120 FPS and 240 FPS bring less improvement.\n\n\nMs-level localization We investigate our special interpolation in two perspectives. First, we need to verify whether this method works. However, there is no ground truth timestamp so we cannot directly quantify the accuracy. We set the estimation of 240 FPS as a baseline and compare it with the estimation of lower FPS. If it can get similar numerical results from independent input, which means the algorithm is reliable. In Figure refmicro2, the median temporal error for 30, 60, and 120 FPS is 2.3ms, 0.65ms, and 0.5ms respectively. Considering the frame resolution, we can compute the improvement ratio as frame_duration temporal_error. The 60 FPS has the largest 25x improvement over the frame duration. This is strong evidence that our ms-level localization is reasonable and robust.\n\n\nSecond, we compare the performance of depth estimation with different interpolation strategies in Table 2. We use the result from frame localization to predict the depth when there is no interpolation. The error is large since this timestamp is ambiguous for the depth prediction. Interpolation with the traces of centers or bounding boxes does not work well. A few key points cannot capture the dynamics in fine granularity.\n\n\nDepth regression Without the RGB-F channel of the target object in depth regression, the estimation will be less robust due to the ambient sound and the background noise as shown in the Table 2\n\n\n# 6 LIMITATION AND FUTURE WORKS\n\n\nWe classify audio-visual events into 3 categories by the quality and the quantity of visual cues during the sound production.\n\n\nObvious visual cues during sound production (e.g. collision) This is the main scenario we try to address in this paper. It requires both visible procedure and audible sound to estimate the depth. We can apply it to sports analytics, human stepping, etc. Moreover, it can collect the sparse depth point and accumulate depth points over time. According to existing work on depth completion (Long et al., 2021b; Xu et al., 2019b), adding some accurate depth points can boost the performance of monocular depth.\n\n\nIndirect visual cues during sound production (e.g.speech, playing the piano) This scenario is challenging but common every day. They do not show the vibration visually. Fortunately, there are still lots of visual cues. Existing work on speech synthesis with lip motion(Ephrat & Peleg, 2017), and music generation with pose(Gan et al., 2020) indicates the strong semantic relationship between video and audio. The spatial correlation still holds here. We propose to apply a high-resolution multi-frame alignment between the video and audio to find the accurate propagation delay.\n\n\nNo visual cues during sound production (e.g. car engines, mobile phone speaker) We admit that we have no idea to estimate the depth when these sound sources are static because we cannot see them at all. Luckily, we still have a chance when these sound sources move. We propose a Doppler-like formulation to associate visual cues and audio cues.\n\n\nAnother urgent problem is that the microphone is pretty challenging to synchronize with other sensors. Pushing the latency to the sub-ms level can boost many applications including FBDepth.\n\n\n# 7 CONCLUSION\n\n\nIn this paper, we develop a novel depth estimation method based on the \"Flash-to-Bang\". By aligning the video with the audio and detecting the events from both, we can estimate the depth in the wild without calibration or prior knowledge about the environment or target. Our extensive evaluation shows that our approach yields similar errors across varying distances. In comparison, the errors of several existing methods increase rapidly with distance. Therefore, our method is particularly attractive for large distances. As part of our future work, we are interested in further enhancing the accuracy of our method, generalizing to more contexts, and using the estimated depth to the collision to estimate the depth to other objects in the scene.\n\n\n# REFERENCES\n\n- Apple. Apple developer documentation., 2022. URL https://developer.apple.com/ documentation/avfoundation/additional_data Capture/capturing_ photos_with_depth.\n\n- Relja Arandjelovic and Andrew Zisserman. Objects that sound. In Proceedings of the European conference on computer vision (ECCV), pp. 435-451, 2018.\n\n- Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth, Michael J Black, and Richard Szeliski. A database and evaluation methodology for optical flow. International journal of computer vision, 92(1):1-31, 2011.\n\n- Dan Barnes, Matthew Gadd, Paul Murcutt, Paul Newman, and Ingmar Posner. The oxford radar robotcar dataset: A radar extension to the oxford robotcar dataset. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 6433-6438. IEEE, 2020.\n\n- Amlaan Bhoi. Monocular depth estimation: A survey. pre-print, 2019. https://arxiv.org/pdf/1901.09402.pdf.\n\n- Hannah Bull, Triantafyllos Afouras, G\u00fcl Varol, Samuel Albanie, Liliane Momeni, and Andrew Zisserman. Aligning subtitles in sign language videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11552-11561, 2021.\n\n- Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11621-11631, 2020.\n\n- Changan Chen, Ziad Al-Halah, and Kristen Grauman. Semantic audio-visual navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15516-15525, 2021a.\n\n- Changan Chen, Wei Sun, David Harwath, and Kristen Grauman. Learning audio-visual dereverberation. arXiv preprint arXiv:2106.07732, 2021b.\n\n- Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao Dai, Xiaojun Chang, Hongdong Li, Tom Drummond, and Zongyuan Ge. Hierarchical neural architecture search for deep stereo matching. Advances in Neural Information Processing Systems, 33, 2020.\n\n- Zhixiang Chi, Rasoul Mohammadi Nasiri, Zheng Liu, Juwei Lu, Jin Tang, and Konstantinos N Plataniotis. All at once: Temporally adaptive multi-frame interpolation with advanced motion modeling. In European Conference on Computer Vision, pp. 107-123. Springer, 2020.\n\n- Joon Son Chung and Andrew Zisserman. Lip reading in the wild. In *Asian conference on computer vision*, pp. 87-103. Springer, 2016.\n\n- Ariel Ephrat and Shmuel Peleg. Vid2speech: speech reconstruction from silent video. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5095-5099. IEEE, 2017.\n\n- Chuang Gan, Deng Huang, Peihao Chen, Joshua B Tenenbaum, and Antonio Torralba. Foley music: Learning to generate music from videos. In European Conference on Computer Vision, pp. 758-775. Springer, 2020.\n\n- Chuang Gan, Yi Gu, Siyuan Zhou, Jeremy Schwartz, Seth Alter, James Traer, Dan Gutfreund, Joshua B Tenenbaum, Josh H McDermott, and Antonio Torralba. Finding fallen objects via asynchronous audio-visual integration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10523\u201310533, 2022.\n\n- Ruohan Gao, Changan Chen, Ziad Al-Halah, Carl Schissler, and Kristen Grauman. Visualechoes: Spatial image representation learning through echolocation. In European Conference on Computer Vision, pp. 658-676. Springer, 2020.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.\n\n- John Hershey and Javier Movellan. Audio vision: Using audio-visual synchrony to locate sounds. Advances in neural information processing systems, 12, 1999.\n\n- Saif Imran, Xiaoming Liu, and Daniel Morris. Depth completion with twin surface extrapolation at occlusion boundaries. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2583-2592, 2021.\n\n- Jan Kotera, Denys Rozumnyi, Filip Sroubek, and Jiri Matas. Intra-frame object tracking by deblating. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pp. 0-0, 2019.\n\n- Jan Kotera, Ji\u0159\u00ed Matas, and Filip \u0160roubek. Restoration of fast moving objects. IEEE Transactions on Image Processing, 29:8577-8589, 2020.\n\n- Hamid Laga, Laurent Valentin Jospin, Farid Boussaid, and Mohammed Bennamoun. A survey on deep learning techniques for stereo-based depth estimation. pre-print, 2020. https://arxiv.org/pdf/2006.02535.pdf.\n\n- Zhenyu Li, Zehui Chen, Ang Li, Liangji Fang, Qinhong Jiang, Xianming Liu, and Junjun Jiang. Unsupervised domain adaptation for monocular 3d object detection via self-training. arXiv preprint arXiv:2204.11590, 2022.\n\n- Yan-Bo Lin, Yu-Jhe Li, and Yu-Chiang Frank Wang. Dual-modality seq2seq network for audiovisual event localization. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2002-2006. IEEE, 2019.\n\n- LIVOX. Livox mid-70 spec, 2021. URL https://www.livoxtech.com/mid-70/specs.\n\n- Xiaoxiao Long, Lingjie Liu, Wei Li, Christian Theobalt, and Wenping Wang. Multi-view depth estimation using epipolar spatio-temporal networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8258-8267, 2021a.\n\n- Yunfei Long, Daniel Morris, Xiaoming Liu, Marcos Castro, Punarjay Chakravarty, and Praveen Narayanan. Radar-camera pixel depth association for depth completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12507-12516, 2021b.\n\n- Yawen Lu and Guoyu Lu. An alternative of lidar in nighttime: Unsupervised depth estimation based on single thermal image. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3833-3843, 2021.\n\n- Wenguang Mao, Jian He, and Lili Qiu. CAT: high-precision acoustic motion tracking. In Proc. of ACM MobiCom, 2016.\n\n- Wenguang Mao, Mei Wang, Wei Sun, Lili Qiu, Swadhin Pradhan, and Yi-Chao Chen. Rnn-based room scale hand motion tracking. In The 25th Annual International Conference on Mobile Computing and Networking, pp. 1-16, 2019.\n\n- Simone Meyer, Oliver Wang, Henning Zimmer, Max Grosse, and Alexander Sorkine-Hornung. Phase-based frame interpolation for video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1410-1418, 2015.\n\n- Simone Meyer, Abdelaziz Djelouah, Brian McWilliams, Alexander Sorkine-Hornung, Markus Gross, and Christopher Schroers. Phasenet for video frame interpolation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 498-507, 2018.\n\n- Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable convolution. In Proceedings of the IEEE International Conference on Computer Vision, pp. 261-270, 2017a.\n\n- Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive convolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 670-679, 2017b.\n\n- Kranti Kumar Parida, Siddharth Srivastava, and Gaurav Sharma. Beyond image to depth: Improving depth prediction using echoes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8268-8277, 2021.\n\n- Junheum Park, Keunsoo Ko, Chul Lee, and Chang-Su Kim. Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation. In European Conference on Computer Vision, pp. 109-125. Springer, 2020.\n\n- Jiaxiong Qiu, Zhaopeng Cui, Yinda Zhang, Xingdi Zhang, Shuaicheng Liu, Bing Zeng, and Marc Pollefeys. Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3313-3322, 2019.\n\n- Caleb Rascon and Ivan Meza. Localization of sound sources in robotics: A review. Robotics and Autonomous Systems, 96:184-210, 2017.\n\n- Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. Springer, 2015.\n\n- Denys Rozumnyi, Jan Kotera, Filip Sroubek, and Jiri Matas. Sub-frame appearance and 6d pose estimation of fast moving objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6778-6786, 2020.\n\n- Sheng Shen, Daguan Chen, Yu-Lin Wei, Zhijian Yang, and Romit Roy Choudhury. Voice localization using nearby wall reflections. In Proc. of ACM MobiCom, 2020.\n\n- Nikhil Singh, Jeff Mentch, Jerry Ng, Matthew Beveridge, and Iddo Drori. Image2reverb: Cross-modal reverb impulse response synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 286-295, 2021.\n\n- StereoLab. Zed 2 camera dataset. https://www.stereolabs.com/assets/datasheets/zed2-camera-datasheet.pdf, 2021.\n\n- Wei Sun, Mei Wang, and Lili Qiu. Spatial aware multi-task learning based speech separation. arXiv preprint arXiv:2207.10229, 2022.\n\n- Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 247-263, 2018.\n\n- J-M Valin, Fran\u00e7ois Michaud, Jean Rouat, and Dominic L\u00e9tourneau. Robust sound source localization using a microphone array on a mobile robot. In Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003)(Cat. No. 03CH37453), volume 2, pp. 1228-1233. IEEE, 2003.\n\n- Deepak Vasisht, Swarun Kumar, and Dina Katabi. Decimeter-level localization with a single wifi access point. In 13th {USENIX} Symposium on Networked Systems Design and Implementation (\\{NSDI\\} 16), pp. 165-178, 2016.\n\n- Mei Wang, Wei Sun, and Lili Qiu. {MAVL}: Multiresolution analysis of voice localization. In 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21), pp. 845-858, 2021.\n\n- Gordon Wichern, Joe Antognini, Michael Flynn, Licheng Richard Zhu, Emmett McQuinn, Dwight Crow, Ethan Manilow, and Jonathan Le Roux. Wham!: Extending speech separation to noisy environments. arXiv preprint arXiv:1907.01160, 2019.\n\n- Yu Wu, Linchao Zhu, Yan Yan, and Yi Yang. Dual attention matching for audio-visual event localization. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6292\u20136300, 2019.\n\n- Yan Xia and Zhou Zhao. Cross-modal background suppression for audio-visual event localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19989-19998, 2022.\n\n- Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and Ming-Hsuan Yang. Quadratic video interpolation. Advances in Neural Information Processing Systems, 32, 2019a.\n\n- Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun Bao, and Hongsheng Li. Depth completion from sparse lidar data with depth-normal constraints. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2811-2820, 2019b.\n\n- Chongjian Yuan, Xiyuan Liu, Xiaoping Hong, and Fu Zhang. Pixel-level extrinsic self calibration of high resolution lidar and camera in targetless environments. IEEE Robotics and Automation Letters, 6(4):7517-7524, 2021.\n\n- Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Newcrfs: Neural window fully-connected crfs for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Pavel Zahorik. Direct-to-reverberant energy ratio sensitivity. The Journal of the Acoustical Society of America, 112(5):2110-2117, 2002.\n\n- Diana Zhang, Akarsh Prabhakara, Sirajum Munir, Aswin Sankaranarayanan, and Swarun Kumar. A hybrid mmwave and camera system for long-range depth imaging. arXiv preprint arXiv:2106.07856, 2021.\n\n- Kai Zhang, Jiaxin Xie, Noah Snavely, and Qifeng Chen. Depth sensing beyond lidar range. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1692-1700, 2020.\n\n- Zhengyou Zhang. Microsoft Kinect sensor and its effect. IEEE multimedia, 19(2):4-10, 2012.\n\n- Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. Audio-visual segmentation. arXiv preprint arXiv:2207.05042, 2022.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638627.124582", "msecs": "124.0", "relativeCreated": "223955.07955551147", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Pyramidal Flow Matching for Efficient Video Generative Modeling\n\n# Abstract\nVideo generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution latent. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours.\n\n\n# 1 INTRODUCTION\n\n\nVideo is a media form that records the evolvement of the physical world. Teaching the AI system to generate various video content plays a vital role in simulating the real-world dynamics (Hu et al., 2023; Brooks et al., 2024) and interacting with humans (Bruce et al., 2024; Valevski et al., 2024). Nowadays, the cutting-edge diffusion models (Ho et al., 2022c; Blattmann et al., 2023a; OpenAI, 2024) and autoregressive models (Yan et al., 2021; Hong et al., 2023; Kondratyuk et al., 2024) have made remarkable breakthroughs in generating realistic and long-duration video through scaling of data and computation. However, the necessity of modeling a significantly large spatiotemporal space makes the training of such video generative models computationally and data intensive.\n\n\nTo ease the computational burden of generating high-dimensional video data, a crucial component is to compress the original video pixels into a lower-dimensional latent space using a VAE (Kingma & Welling, 2014; Esser et al., 2021; Rombach et al., 2022). However, the regular compression rate (typically  $8 \\times$ ) still results in excessive tokens, especially for high-resolution samples. In light of this, prevalent approaches utilize a cascaded architecture (Ho et al., 2022b; Pernias et al., 2024; Teng et al., 2024) to break down the high-resolution generation process into multiple stages, where samples are first created in a highly compressed latent space and then successively upsampled using additional super-resolution models. Although the cascaded pipeline avoids directly learning at high resolution and reduces the computational demands, the requirement for employing distinct models at different resolutions separately sacrifices flexibility and scalability. Besides, the separate optimization of multiple sub-models also hinders the sharing of their acquired knowledge.\n\n\nThis work presents an efficient video generative modeling framework that transcends the limitations of the previous cascaded approaches. Our motivation stems from the observation in Fig. 1a that the initial timesteps in diffusion models are quite noisy and uninformative. This suggests that operating at full resolution throughout the entire generation trajectory may not be necessary. To this end, we reinterpret the original generation trajectory as a series of pyramid stages that operate on compressed representations of different scales. Notably, the efficacy of image pyramids (Adelson et al., 1984)\n\n\n![(a) Video diffusion model like Sora (OpenAI, 2024)](images/bc23bb4c2a344a6f6a1f32948f2d7111275607b90246392f9d81a41da6b8053c.jpg)\n\n\n![(b) Our proposed pyramidal flow matching Figure 1: A motivating example for pyramidal flow matching: (a) Existing diffusion models operate at full resolution, spending a lot of computation on very noisy latents. (b) Our method harnesses the flexibility of flow matching to interpolate between latents of different resolutions. This allows for simultaneous generation and decompression of visual content with better computational efficiency. Note that the black arrows are denoising trajectories, and the blue ones are their temporal conditions.](images/0a731558103aa801baeeb6b8f09db2e5187518002e259b3efd211e75112cf1b4.jpg)\n\n\nhas been widely validated for discriminative neural networks (Lin et al., 2017; Wang et al., 2020) and more recently for diffusion models (Ho et al., 2022b; Pernias et al., 2024; Teng et al., 2024) and multimodal LLMs (Yu et al., 2023; Tian et al., 2024). Here, we investigate two types of pyramids: the spatial pyramid within a frame and the temporal one between consecutive frames (as illustrated in Fig. 1b). In such a pyramidal generation trajectory, only the final stage operates at full resolution, drastically reducing redundant computations in earlier timesteps. The main advantages are twofold: (1) The generation trajectories of different pyramid stages are interlinked, with the subsequent stage continuing to generate from the previous ones. This eliminates the need for each stage to regenerate from pure noise in some cascade models. (2) Instead of relying on separate models for each image pyramid, we integrate them into a single unified model for end-to-end optimization, which admits drastically-expedited training with more elegant implementation as validated by experiments.\n\n\nBased on the aforementioned pyramidal representations, we introduce a novel pyramidal flow matching algorithm that builds upon recent prevalent flow matching framework (Lipman et al., 2023; Liu et al., 2023; Albergo & Vanden-Eijnden, 2023). Specifically, we devise a piecewise flow for each pyramid resolution, which together form a generative process from noise to data. The flow within each pyramid stage takes a similar formulation, interpolating between a pixelated (compressed) and noisier latent and a pixelate-free (decompressed) and cleaner latent. Through our design, they can be jointly optimized by the unified flow matching objective in a single Diffusion Transformer (DiT) (Peebles & Xie, 2023), allowing simultaneous generation and decompression of visual content without multiple separate models. During inference, the output of each stage is renoised by a corrective Gaussian noise, which contributes to maintaining the continuity of the probability path between successive pyramid stages. Furthermore, we formulate the video generation in an autoregressive manner that iteratively predicts the next video latent conditioned on previously generated history. Given the high redundancy in the full-resolution history, we curate a temporal pyramid sequence using progressively compressed, lower-resolution history as conditions, thereby further reducing the token count and improving training efficiency.\n\n\nThe collaboration of the spatial and temporal pyramids results in remarkable training efficiency for video generation. Compared to the commonly used full-sequence diffusion, our method significantly reduces the number of video tokens during training (e.g.,  $\\leq 15,360$  tokens versus 119,040 tokens for a 10-second, 241-frame video), thereby reducing both computational resources required and training time. By training only on open-source datasets, our model generate high-quality 10-second videos at 768p resolution and 24 fps. The core contributions of this paper are summarized as follows:\n\n- - We present pyramidal flow matching, a novel video generative modeling algorithm that incorporates both spatial and temporal pyramid representations. Utilizing this framework can significantly improve training efficiency while maintaining good video generation quality.\n\n- - The proposed unified flow matching objective facilitates joint training of pyramid stages in a single Diffusion Transformer (DiT), avoiding the separate optimization of multiple models. The support for end-to-end training further enhances its simplicity and scalability.\n\n- - We evaluate its effectiveness on VBenchmark (Huang et al., 2024) and EvalCrafter (Liu et al., 2024), with highly competitive performance among video generative models trained on public datasets.\n\n\n# 2 RELATED WORK\n\n\nVideo Generative Models have seen rapid progress with autoregressive models (Yan et al., 2021; Hong et al., 2023; Kondratyuk et al., 2024; Jin et al., 2024) and diffusion models (Ho et al., 2022c; Blattmann et al., 2023b;a). A notable breakthrough is the high-fidelity video diffusion models (OpenAI, 2024; Kuaishou, 2024; Luma, 2024; Runway, 2024) by scaling up DiT pre-training (Peebles & Xie, 2023), but they induce significant training costs for long videos. An alternative line of research integrates diffusion models with autoregressive modeling (Chen et al., 2024a; Valevski et al., 2024) to natively support long video generation, but is still limited in context length and training efficiency. Our work advances both approaches in terms of efficiency from a compression perspective, featuring a spatially compressed pyramidal flow and a temporally compressed pyramidal history.\n\n\nImage Pyramids (Adelson et al., 1984) have been studied extensively in visual representation learning (Lowe, 2004; Dalal & Triggs, 2005; Lin et al., 2017; Wang et al., 2020). For generative models, the idea is explored by cascaded diffusion models that first generate at low resolution and then perform super-resolution (Ho et al., 2022b; Sahara et al., 2022; Pernias et al., 2024; Teng et al., 2024). It is also extended to video by spatiotemporal super-resolution (Ho et al., 2022a; Singer et al., 2023). However, they require training several separate models, which prevents knowledge sharing. Possible unified modeling solutions for pyramids include hierarchical architectures (Rombach et al., 2022; Crowson et al., 2024; Hatamizadeh et al., 2024) or via next-token prediction (Yu et al., 2023; Tian et al., 2024), but involve architectural changes. Instead, we propose a simple flow matching objective that allows joint training of pyramids, thus facilitating efficient video generative modeling.\n\n\n# 3 METHOD\n\n\nThis work proposes an efficient video generative modeling scheme named pyramidal flow matching. In the following text, we first extend the flow matching algorithm (Section 3.1) to an efficient spatial pyramid representation (Section 3.2). Then, a temporal pyramid design is proposed in Section 3.3 to further improve training efficiency. Lastly, practical implementations are discussed in Section 3.4.\n\n\n# 3.1 PRELIMINARIES ON FLOW MATCHING\n\n\nSimilar to diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020), flow generative models (Papamakarios et al., 2021; Song et al., 2021; Xu et al., 2022; Lipman et al., 2023; Liu et al., 2023; Albergo & Vanden-Eijnden, 2023) aim to learn a velocity field  $\\boldsymbol{v}_t$  that maps random noise  $\\boldsymbol{x}_0 \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I})$  to data samples  $\\boldsymbol{x}_1 \\sim q$  via an ordinary differential equation (ODE):\n\n\n$$\n\n$$\n\\frac {d \\boldsymbol {x} _ {t}}{d t} = \\boldsymbol {v} _ {t} (\\boldsymbol {x} _ {t}). \\tag {1}\n$$\n\n$$\n\n\nRecently, Lipman et al. (2023); Liu et al. (2023); Albergo & Vanden-Eijnden (2023) proposed the flow matching framework, which provides a simple simulation-free training objective for flow generative models by directly regressing the velocity  $\\boldsymbol{v}_t$  on a conditional vector filed  $\\boldsymbol{u}_t(\\cdot|\\boldsymbol{x}_1)$ :\n\n\n$$\n\n$$\n\\left. \\mathbb {E} _ {t, q \\left(\\boldsymbol {x} _ {1}\\right), p _ {t} \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {1}\\right)} \\left\\| \\boldsymbol {v} _ {t} \\left(\\boldsymbol {x} _ {t}\\right) - \\boldsymbol {u} _ {t} \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {1}\\right) \\right\\| ^ {2}, \\right. \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\pmb{u}_t(\\cdot|\\pmb{x}_1)$  uniquely determines a conditional probability path  $p_t(\\cdot|\\pmb{x}_1)$  toward data sample  $\\pmb{x}_1$ . An effective choice of the conditional probability path is linear interpolation of data and noise:\n\n\n$$\n\n$$\n\\boldsymbol {x} _ {t} = t \\boldsymbol {x} _ {1} + (1 - t) \\boldsymbol {x} _ {0}, \\tag {3}\n$$\n\n$$\n\n\n$$\n\n$$\n\\boldsymbol {x} _ {t} \\sim \\mathcal {N} (t \\boldsymbol {x} _ {1}, (1 - t) ^ {2} \\boldsymbol {I}), \\tag {4}\n$$\n\n$$\n\n\nand  $\\pmb{u}(\\pmb{x}_t|\\pmb{x}_1) = \\pmb{x}_1 - \\pmb{x}_0$ . Notably, flow matching can be flexibly extended to interpolate between distributions other than standard Gaussians. This enables us to devise a new flow matching algorithm that specializes in reducing the computational cost of video generative modeling.\n\n\n# 3.2 PYRAMIDAL FLOW MATCHING\n\n\nThe main challenge in video generative modeling is the spatio-temporal complexity, and we address its spatial complexity first. According to previous key observation in Fig. 1, the initial generation\n\n\n![Figure 2: Illustration of spatial pyramid. (a) The pyramidal flow is divided into multiple stages, each from a pixelated and noisy starting point to a pixelate-free and cleaner result. (b) During inference, we add a corrective noise at jump points across stages to ensure continuity of the probability path.](images/b4925ad3e7ea0c415aa902616afa1a47556fe0d8be878d9fbe4a0c05da01516a.jpg)\n\n\nsteps are usually very noisy and less informative, and thus may not need to operate at full resolution.   \nlatent. This motivates us to study a spatially compressed pyramidal flow, illustrated in Fig. 2.\n\n\nTo alleviate redundant computation in early steps, we interpolate flow between data and compressed low-resolution noise. Let  $\\oplus$  denote the interpolation between latents of different resolutions, and let there be  $K$  resolutions, each halving the previous one, then our flow may be expressed as:\n\n\n$$\n\n$$\n\\hat {\\boldsymbol {x}} _ {t} = t \\boldsymbol {x} _ {1} \\oplus (1 - t) \\operatorname {D o w n} \\left(\\boldsymbol {x} _ {0}, 2 ^ {K}\\right), \\tag {5}\n$$\n\n$$\n\n\nwhere  $\\text{Down}(\\cdot, \\cdot)$  is a downsampling function. Since the interpolation concerns varying-dimensional  $\\pmb{x}_t$ , we decompose it as a piecewise flow (Yan et al., 2024) that divides [0,1] into  $K$  time windows, where each window interpolates between successive resolutions. For the  $k$ -th time window  $[s_k, e_k]$ , let  $t' = (t - s_k) / (e_k - s_k)$  denote the rescaled timestep, then the flow within it follows:\n\n\n$$\n\n$$\n\\hat {\\boldsymbol {x}} _ {t} = t ^ {\\prime} \\operatorname {D o w n} \\left(\\boldsymbol {x} _ {e _ {k}}, 2 ^ {k}\\right) + \\left(1 - t ^ {\\prime}\\right) U p \\left(\\operatorname {D o w n} \\left(\\boldsymbol {x} _ {s _ {k}}, 2 ^ {k + 1}\\right)\\right), \\tag {6}\n$$\n\n$$\n\n\nwhere  $U_{p}(\\cdot)$  is an upsampling function. This way, only the last stage is performed at full resolution, while most stages are performed at lower resolutions using less computation. Under a uniform stage partitioning, the idea of spatial pyramid reduces the computational cost to a factor of nearly  $1 / K$ . Below, we describe the instantiation of pyramidal flow from training and inference, respectively.\n\n\n# 3.2.1 UNIFIED TRAINING\n\n\nIn the construction of pyramidal flow, our main concern is unified modeling of different stages, as previous works (Ho et al., 2022b; Pernias et al., 2024; Teng et al., 2024) all require training multiple models for separate generation and super-resolution, which hinders knowledge sharing.\n\n\nTo unify the objectives of generation and decompression/super-resolution, we curate the probability path by interpolating between different noise levels and resolutions. It starts with a more noisy and pixelated latent upsampled from a lower resolution, and yields cleaner and fine-grained results at a higher resolution, as illustrated in Fig. 2a. Formally, the conditional probability path is defined by:\n\n\n$$\n\n$$\n\\text {E n d :} \\quad \\hat {\\boldsymbol {x}} _ {e _ {k}} | x _ {1} \\sim \\mathcal {N} \\left(e _ {k} \\operatorname {D o w n} \\left(\\boldsymbol {x} _ {1}, 2 ^ {k}\\right), \\left(1 - e _ {k}\\right) ^ {2} \\boldsymbol {I}\\right), \\tag {7}\n$$\n\n$$\n\n\n$$\n\n$$\n\\text {S t a r t :} \\quad \\hat {\\boldsymbol {x}} _ {s _ {k}} | \\boldsymbol {x} _ {1} \\sim \\mathcal {N} \\left(s _ {k} \\operatorname {U p} \\left(\\operatorname {D o w n} \\left(\\boldsymbol {x} _ {1}, 2 ^ {k + 1}\\right)\\right), (1 - s _ {k}) ^ {2} \\boldsymbol {I}\\right), \\tag {8}\n$$\n\n$$\n\n\nwhere  $s_k < e_k$ , and the upsampling and downsampling functions for the clean  $\\mathbf{x}_1$  are well defined, e.g., by nearest or bilinear resampling. In addition, to enhance the straightness of the flow trajectory, we couple the sampling of its endpoints by enforcing the noise to be in the same direction. Namely, we first sample a noise  $\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$  and then jointly compute the endpoints  $(\\hat{\\mathbf{x}}_{e_k}, \\hat{\\mathbf{x}}_{s_k})$  as:\n\n\n$$\n\n$$\n\\text {E n d :} \\quad \\hat {\\boldsymbol {x}} _ {e _ {k}} = e _ {k} \\operatorname {D o w n} \\left(\\boldsymbol {x} _ {1}, 2 ^ {k}\\right) + \\left(1 - e _ {k}\\right) \\boldsymbol {n}, \\tag {9}\n$$\n\n$$\n\n\n$$\n\n$$\n\\text {S t a r t :} \\quad \\hat {\\boldsymbol {x}} _ {s _ {k}} = s _ {k} U p (D o w n (\\boldsymbol {x} _ {1}, 2 ^ {k + 1})) + (1 - s _ {k}) \\boldsymbol {n}. \\tag {10}\n$$\n\n$$\n\n\nThereafter, we can regress the flow model  $\\pmb{v}_t$  on the conditional vector field  $\\pmb{u}_t(\\hat{\\pmb{x}}_t|\\pmb{x}_1) = \\hat{\\pmb{x}}_{e_k} - \\hat{\\pmb{x}}_{s_k}$  with the following flow matching objective to unify generation and decompression:\n\n\n$$\n\n$$\n\\mathbb {E} _ {k, t, \\left(\\hat {\\boldsymbol {x}} _ {e _ {k}}, \\hat {\\boldsymbol {x}} _ {s _ {k}}\\right)} \\left\\| \\boldsymbol {v} _ {t} \\left(\\hat {\\boldsymbol {x}} _ {t}\\right) - \\left(\\hat {\\boldsymbol {x}} _ {e _ {k}} - \\hat {\\boldsymbol {x}} _ {s _ {k}}\\right) \\right\\| ^ {2}. \\tag {11}\n$$\n\n$$\n\n\n```\nRequire: flow model  $\\pmb{v}$  , number of stages  $K$  , time windows  $[s_k,e_k]$  Initialize a starting point  $\\hat{\\pmb{x}}_0\\sim \\mathcal{N}(\\mathbf{0},\\pmb {I})$  for  $k\\gets K - 1$  to 0 do Compute endpoint  $\\hat{\\pmb{x}}_{e_k}$  from starting point  $\\hat{\\pmb{x}}_{s_k}$  based on the flow model  $\\pmb{v}$  Compute next starting point by upsampling  $\\hat{\\pmb{x}}_{e_k}$  with renoising. \u25b3Eq. (15)   \nEnsure: generated sample  $\\hat{x}_1$\n```\n\n\n# 3.2.2 INFERENCE WITH RENOISING\n\n\nDuring inference, standard sampling algorithms can be applied within each pyramid stage. However, we must carefully handle the jump points (Campbell et al., 2023) between successive pyramid stages of different resolutions to ensure continuity of the probability path.\n\n\nTo ensure continuity, we first upsample the previous low-resolution endpoint with nearest or bilinear resampling. The result, as a linear combination of the input, follows a Gaussian distribution:\n\n\n$$\n\n$$\n\\left. U p \\left(\\hat {\\boldsymbol {x}} _ {e _ {k + 1}}\\right) \\mid x _ {1} \\sim \\mathcal {N} \\left(e _ {k + 1} U p \\left(D o w n \\left(\\boldsymbol {x} _ {1}, 2 ^ {k + 1}\\right)\\right), \\left(1 - e _ {k + 1}\\right) ^ {2} \\boldsymbol {\\Sigma}\\right), \\right. \\tag {12}\n$$\n\n$$\n\n\nwhere  $\\Sigma$  is a covariance matrix depending on the upsampling function. Comparing Eqs. (8) and (12), we find it possible to match the Gaussian distributions at each jump point by a linear transformation of the upsampled result. Specifically, the following rescaling and renoising scheme would suffice:\n\n\n$$\n\n$$\n\\hat {\\boldsymbol {x}} _ {s _ {k}} = \\frac {s _ {k}}{e _ {k + 1}} U p \\left(\\hat {\\boldsymbol {x}} _ {e _ {k + 1}}\\right) + \\alpha \\boldsymbol {n} ^ {\\prime}, \\quad \\text {s . t .} \\boldsymbol {n} ^ {\\prime} \\sim \\mathcal {N} (\\boldsymbol {0}, \\boldsymbol {\\Sigma} ^ {\\prime}), \\tag {13}\n$$\n\n$$\n\n\nwhere the rescaling coefficient  $s_k / e_{k + 1}$  allows matching the means of these distributions, and the corrective noise  $n'$  with a weight of  $\\alpha$  allows matching their covariance matrices.\n\n\nTo derive the corrective noise and its covariance, we consider a simplest scenario with nearest neighbor upsampling. In this case,  $\\Sigma$  has a blockwise structure with non-zero elements only in the  $4\\times 4$  blocks along the diagonal (corresponding to those upsampled from the same pixel). Then, it can be inferred that the corrective noise's covariance matrix  $\\Sigma^{\\prime}$  also has a blockwise structure:\n\n\n$$\n\n$$\n\\boldsymbol {\\Sigma} _ {b l o c k} = \\left( \\begin{array}{l l l l} 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\end{array} \\right) \\Rightarrow \\boldsymbol {\\Sigma} _ {b l o c k} ^ {\\prime} = \\left( \\begin{array}{l l l l} 1 & \\gamma & \\gamma & \\gamma \\\\ \\gamma & 1 & \\gamma & \\gamma \\\\ \\gamma & \\gamma & 1 & \\gamma \\\\ \\gamma & \\gamma & \\gamma & 1 \\end{array} \\right), \\tag {14}\n$$\n\n$$\n\n\nwhere  $\\Sigma_{block}^{\\prime}$  contains negative elements  $\\gamma \\in [-1/3, 0]^1$  to reduce the correlation within each block, as illustrated in Fig. 2b. Since it is desirable to maximally preserve the signals at each jump point, we opt to add a small amount of noise with  $\\gamma = -1/3$  such that it is most specialized for decorrelation. Substituting this into the above gives the update rule at jump points (see Appendix A for derivations):\n\n\n$$\n\n$$\n\\hat {\\boldsymbol {x}} _ {s _ {k}} = \\frac {1 + s _ {k}}{2} U p \\left(\\hat {\\boldsymbol {x}} _ {e _ {k + 1}}\\right) + \\frac {\\sqrt {3} \\left(1 - s _ {k}\\right)}{2} \\boldsymbol {n} ^ {\\prime}, \\tag {15}\n$$\n\n$$\n\n\nwith  $e_{k + 1} = 2s_k / (1 + s_k)$ . The resulting inference process with renoising is shown in Algorithm 1.\n\n\n# 3.3 PYRAMIDAL TEMPORAL CONDITION\n\n\nBeyond the spatial complexity addressed in above sections, video presents another significant challenge due to its temporal length. The prevailing full-sequence diffusion methods generate all video frames simultaneously, restricting them to fixed-length generation (consistent with training). In contrast, the autoregressive video generation paradigm supports flexible-length generation during inference. Recent advancements (Chen et al., 2024a; Valevski et al., 2024) have also demonstrated its effectiveness in creating long-duration video content. However, their training is still severely limited by the computational complexity arising from the full-resolution long-history condition.\n\n\nWe observe that there is a high redundancy in full-resolution history conditions. For example, earlier frames in a video tend to provide high-level semantic conditions and are less related to appearance\n\n\n![(a) Temporal pyramids rearranged in rows](images/13bc80b2d9a8e77afcb89ebfe61522b02e67e3bc693e9705bdc34d38381a27b1.jpg)\n\n\n![(b) Position encoding in spatial and temporal pyramid Figure 3: Illustration of temporal pyramid. (a) At each pyramid stage, the generation is conditioned on a compressed, lower-resolution history to improve training efficiency of the autoregressive model, as indicated by the rows. (b) A compatible position encoding scheme is devised that extrapolates in the spatial pyramid but interpolates in the temporal pyramid to allow spatial alignment of conditions.](images/176ffb8e2c5a5849716b5046550e61f6f445ff5b59d536dea49426c8e3196358.jpg)\n\n\ndetails. This motivates us to use compressed, lower-resolution history for autoregressive video generation. As shown in Fig. 3a, we adopt a history condition of gradually increasing resolutions:\n\n\n$$\n\n$$\n\\underbrace {\\dots \\rightarrow D o w n \\left(\\boldsymbol {x} _ {t ^ {\\prime}} ^ {i - 2} , 2 ^ {k + 1}\\right)\\rightarrow D o w n \\left(\\boldsymbol {x} _ {t ^ {\\prime}} ^ {i - 1} , 2 ^ {k}\\right)} _ {\\text {H i s t o r y c o n d i t i o n}} \\rightarrow \\hat {\\boldsymbol {x}} _ {t} ^ {i}, \\tag {16}\n$$\n\n$$\n\n\nwhere the superscripts are the history latent index, and the subscript  $t'$  indicates small noise added to history latents in training to mitigate error accumulation with autoregressive generation, as in (Chen et al., 2024a; Valevski et al., 2024). After training, we use clean generated frames for inference:\n\n\n$$\n\n$$\n\\underbrace {\\dots \\rightarrow D o w n \\left(\\boldsymbol {x} _ {1} ^ {i - 2} , 2 ^ {k + 1}\\right)\\rightarrow D o w n \\left(\\boldsymbol {x} _ {1} ^ {i - 1} , 2 ^ {k}\\right)} _ {\\text {H i s t o r y c o n d i t i o n}} \\rightarrow \\hat {\\boldsymbol {x}} _ {t} ^ {i}. \\tag {17}\n$$\n\n$$\n\n\nThe above design significantly reduces the computational and memory overhead of video generative pre-training. Let there be  $T$  history latents over  $K$  lower resolutions, then most frames are computed at the lowest resolution of  $1 / 2^K$ , which reduces the number of training tokens by up to  $1 / 4^K$  times. As a result, training efficiency is improved by up to  $16^K / T$  times.\n\n\n# 3.4 PRACTICAL IMPLEMENTATION\n\n\nIn this section, we show that the above pyramid designs can be easily implemented using standard Transformer architecture (Vaswani et al., 2017) and pipelines. This is crucial for efficient and scalable video generative pre-training based on existing acceleration frameworks.\n\n\nUnlike previous methods (Ma et al., 2024) that utilize factorized spatial and temporal attention to reduce computational complexity, we directly employ full sequence attention, thanks to much fewer tokens required by our pyramidal representation. Furthermore, blockwise causal attention is adopted in each transformer layer, ensuring that each token cannot attend to its subsequent frames. The ablation results in Appendix C.2 illustrate that such casual attention design is crucial for autoregressive video generation. Another important design choice is the position encoding, as the pyramid designs introduce multiple spatial resolutions. As shown in Fig. 3b, we extrapolate position encoding in the spatial pyramid for better fine-grained detail (Yang et al., 2024), while interpolating it in the temporal pyramid input to spatially align the history conditions.\n\n\nDuring training, different pyramidal stages are uniformly sampled in each update iteration. The autoregressive nature of our method inherently supports joint training of images and videos, since the first frame in a video acts as an image. We pack training samples with varying token counts together to form the length-balanced training batch following Patch n' Pack (Dehghani et al., 2023). After training, our method natively possesses the capability of text-to-video and text-conditioned image-to-video generation. During inference sampling, the classifier-free guidance strategy can be employed to enhance temporal consistency and motion smoothness of the generated video.\n\n\n# 4 EXPERIMENTS\n\n\n# 4.1 EXPERIMENTAL SETTINGS\n\n\nTraining Dataset. Our model is trained on a mixed corpus of open-source image and video datasets. For images, we utilize a high-aesthetic subset of LAION-5B (Schuhmann et al., 2022), 11M from CC-12M (Changpinyo et al., 2021), 6.9M non-blurred subset of SA-1B (Kirillov et al., 2023), 4.4M from JourneyDB (Sun et al., 2023), and 14M publicly available synthetic data. For video data, we incorporate the WebVid-10M (Bain et al., 2021), OpenVid-1M (Nan et al., 2024), and another 1M high-resolution non-watermark video primarily from the Open-Sora Plan (PKU-Yuan Lab et al., 2024). After postprocessing, around 10M single-shot videos are available for training.\n\n\nEvaluation Metrics. We utilize the VBench (Huang et al., 2024) and EvalCrafter (Liu et al., 2024) for quantitative performance evaluation. VBench is a comprehensive benchmark that includes 16 fine-grained dimensions to systematically measure both motion quality and semantic alignment of video generative models. EvalCrafter is another large-scale evaluation benchmark including around 17 objective metrics for assessing video generation capabilities. In addition to automated evaluation metrics, we also conducted a study with human participants to measure the human preference for our generated videos. The compared baselines are summarized in Appendix B.\n\n\nImplementation Details. We utilize the prevailing MM-DiT architecture from SD3 Medium (Esser et al., 2024) as the base model, with 2B parameters in total. It employs sinusoidal position encoding (Vaswani et al., 2017) in the spatial dimensions. As for the temporal dimension, the 1D Rotary Position Embedding (RoPE) (Su et al., 2024) is added to support flexible training with different video durations. In addition, we use a 3D Variational Autoencoder (VAE) to compress videos both spatially and temporally with a downsampling ratio of  $8 \\times 8 \\times 8$ . It shares a similar structure with MAGVIT-v2 (Yu et al., 2024) and is trained from scratch on the WebVid-10M dataset (Bain et al., 2021). The number of pyramid stages is set to 3 in all the experiments. Following Valevski et al. (2024), we add some corruptive noise of strength uniformly sampled from  $[0, 1/3]$  to the history pyramid conditions, which is critical for mitigating the autoregressive generation degradation.\n\n\n# 4.2 EFFICIENCY\n\n\nThe proposed pyramidal flow matching framework significantly reduces the computational and memory overhead in video generation training. Consider a video with  $T$  frame latents, where each frame contains  $N$  tokens at the original resolution. The full-sequence diffusion has  $TN$  input tokens in DiT and requires  $T^2 N^2$  computations. In contrast, our method uses only approximately  $TN / 4^K$  tokens and  $T^2 N^2 / 16^K$  computations even for the final pyramid stage, which significantly improves the training efficiency. Specifically, it takes only 20.7k A100 GPU hours to train a 10s video generation model with 241 frames. Compared to existing models that require significant training resources, our method achieves superior video generation performance with much fewer computations. For example, the Open-Sora 1.2 (Zheng et al., 2024) requires 4.8k Ascend and 37.8k H100 hours to train the generation of only 97 video frames, consuming more than two times the computation of our approach, yet producing videos of worse quality. At inference, our model takes just 56 seconds to create a 5-second, 384p video clip, which is comparable to full-sequence diffusion counterparts.\n\n\n# 4.3 MAIN RESULTS\n\n\nText-to-Video Generation. We first evaluate the text-to-video generation capability of the proposed method. For each text prompt, a 5-second 121 frames video is generated for evaluation. The detailed quantitative results on VBench (Huang et al., 2024) and EvalCrafter (Liu et al., 2024) are summarized in Tables 1 and 2, respectively. Overall, our method surpasses all the compared open-sourced video generation baselines in these two benchmarks. Even with only publicly accessible video data in training, it achieves comparable performance to commercial competitors trained on much larger proprietary data like Kling (Kuaishou, 2024) and Gen-3 Alpha (Runway, 2024). In particular, we demonstrated exceptional performance in quality score (84.74 vs. 84.11 of Gen-3), and motion smoothness in VBench, which are crucial criteria in reflecting the visual quality of generated videos. When evaluated in EvalCrafter, our method achieves better visual and motion quality scores than most compared methods. The semantic score is relatively lower than others, mainly because we use coarse-grained synthetic captions, which can be improved with more accurate video captioning.\n\n\nTable 1: Experimental results on VBench (Huang et al., 2024). In terms of total score and quality score, our model even outperforms CogVideoX-5B (Yang et al., 2024) with twice the model size. In the following tables, we use blue to denote the highest scores among models trained on public data.\n\n\n<table><tr><td>Model</td><td>Public Data</td><td>Total Score</td><td>Quality Score</td><td>Semantic Score</td><td>Motion Smoothness</td><td>Dynamic Degree</td></tr><tr><td>Gen-2</td><td>\u00d7</td><td>80.58</td><td>82.47</td><td>73.03</td><td>99.58</td><td>18.89</td></tr><tr><td>Pika 1.0</td><td>\u00d7</td><td>80.69</td><td>82.92</td><td>71.77</td><td>99.50</td><td>47.50</td></tr><tr><td>CogVideoX-2B</td><td>\u00d7</td><td>80.91</td><td>82.18</td><td>75.83</td><td>97.73</td><td>59.86</td></tr><tr><td>CogVideoX-5B</td><td>\u00d7</td><td>81.61</td><td>82.75</td><td>77.04</td><td>96.92</td><td>70.97</td></tr><tr><td>Kling</td><td>\u00d7</td><td>81.85</td><td>83.38</td><td>75.68</td><td>99.40</td><td>46.94</td></tr><tr><td>Gen-3 Alpha</td><td>\u00d7</td><td>82.32</td><td>84.11</td><td>75.17</td><td>99.23</td><td>60.14</td></tr><tr><td>Open-Sora Plan v1.3</td><td>\u2713</td><td>77.23</td><td>80.14</td><td>65.62</td><td>99.05</td><td>30.28</td></tr><tr><td>Open-Sora 1.2</td><td>\u2713</td><td>79.76</td><td>81.35</td><td>73.39</td><td>98.50</td><td>42.39</td></tr><tr><td>VideoCrafter2</td><td>\u2713</td><td>80.44</td><td>82.20</td><td>73.42</td><td>97.73</td><td>42.50</td></tr><tr><td>T2V-Turbo</td><td>\u2713</td><td>81.01</td><td>82.57</td><td>74.76</td><td>97.34</td><td>49.17</td></tr><tr><td>Ours</td><td>\u2713</td><td>81.72</td><td>84.74</td><td>69.62</td><td>99.12</td><td>64.63</td></tr></table>\n\n\nTable 2: Experimental results on EvalCrafter (Liu et al., 2024). See Appendix C.1 for raw metrics.\n\n\n<table><tr><td>Model</td><td>Public Data</td><td>Final Sum Score</td><td>Visual Quality</td><td>Text-Video Alignment</td><td>Motion Quality</td><td>Temporal Consistency</td></tr><tr><td>Pika 1.0</td><td>\u00d7</td><td>250</td><td>63.05</td><td>66.97</td><td>56.43</td><td>63.81</td></tr><tr><td>Gen-2</td><td>\u00d7</td><td>254</td><td>69.09</td><td>63.92</td><td>55.59</td><td>65.40</td></tr><tr><td>ModelScope</td><td>\u2713</td><td>218</td><td>53.09</td><td>54.46</td><td>52.47</td><td>57.80</td></tr><tr><td>Show-1</td><td>\u2713</td><td>229</td><td>52.19</td><td>62.07</td><td>53.74</td><td>60.83</td></tr><tr><td>LaVie</td><td>\u2713</td><td>234</td><td>57.99</td><td>68.49</td><td>52.83</td><td>54.23</td></tr><tr><td>VideoCrafter2</td><td>\u2713</td><td>243</td><td>63.98</td><td>63.16</td><td>54.82</td><td>61.46</td></tr><tr><td>Ours</td><td>\u2713</td><td>244</td><td>67.94</td><td>57.01</td><td>55.31</td><td>63.41</td></tr></table>\n\n\n![Figure 4: User preference on sampled VBench prompts. Our videos are generated at 5s, 768p, 24fps.](images/570121d11d56924ddf4ef1555c778633c38d7027dc07047a94c62d59906f85b6.jpg)\n\n\nWe also present some generated 5-10 second videos in Fig. 5, showing cinematic visual quality and validate the efficacy of pyramidal flow matching. More visualizations are provided in Appendix C.3.\n\n\nUser study. While quantitative evaluation scores reflect the video generation capability to some extent, they may not align with human preferences for visual quality. Hence, an additional user study is conducted to compare our performance with six baseline models, including CogVideoX (Yang et al., 2024) and Kling (Kuaishou, 2024). We utilized 50 prompts sampled from VBench and asked  $20+$  participants to rank each model according to the aesthetic quality, motion smoothness, and semantic alignment of the generated videos. As seen in Fig. 4, our method is preferred over open-source models such as Open-Sora and CogVideoX-2B especially in terms of motion smoothness. This is due to the substantial token savings achieved by pyramidal flow matching, enabling generation of 5-second (up to 10-second) 768p videos at 24 fps, while the baselines usually support video synthesis of similar length only at 8 fps. The detailed user study settings are presented in Appendix B.\n\n\n![](images/3cd2dfa29f9801c6684e0a6c21738869b19e30de668622f8b0cd58544f1033cf.jpg)\n\n\n![](images/efecb77a4c85c2955d1adb54ff2def0e0ca2f4063b5fbe8f1273b354077a480d.jpg)\n\n\n![](images/04dd0ea14b817e34e1e18bdca19b8a04af1ffd7f20e7fdff96bf92cbf6062a3c.jpg)\n\n\n![](images/3ab1f54f7f46b329a7cfd41219840822ada7ee2d4dca73c6b73025d6c8eec774.jpg)\n\n\n![](images/27a2cbe814b06b8163096caaabf4e64e70e9fa481792e80b2ef9ed6b5c82a9fa.jpg)\n\n\n![](images/79bde22a77122332dff4d553a01c9e8c00f2672153dbcad4ff12730edd474ef3.jpg)\n\n\n![](images/25a1e3d3969744212693b98a5817206096cd57e0b53fa4999fa680a83045435a.jpg)\n\n\n![](images/e6474b03384a379a0e45ce56a264d9cb93fe1cee20f0a770b4ffc670b664edf2.jpg)\n\n\n![](images/1ce7203174cd78fc29e5af5bd6f2f9ca76aef4883fd05fbf911fa3b3f7c612ea.jpg)\n\n\n![](images/0c29d11daa89444ea16e921b88416dfd0f864222096417f4c8c43d82f071d33e.jpg)\n\n\n![](images/e5f15d414e7e1385fd8f5c88ddd4d9b628956d72d84e46495f4f9ecc720ef68b.jpg)\n\n\n![](images/110dc2b4ee5decfd2d9599a9062492a47b5499b185f5bc5278ed1a8b6608778c.jpg)\n\n\n![](images/c0d9c6b543f702242bc619f5445afcd5fe73b9effab766ebd0b340c6906abebe.jpg)\n\n\n![](images/6d45cdd4ebefdd4efb0f9bea4a7daa54c4e5913a6b6dd6575572f1110f2e99f2.jpg)\n\n\n![](images/b2ba484477453fca253fe0e3997011573bd42651625d97056813808ba761d676.jpg)\n\n\n![](images/e9a5b3487dbc2d0df8028069d0c3ba409050b55536eaf09b3342bb7f65eb46e0.jpg)\n\n\n![](images/e198d9294dea4d3398e04eb3194bf7b9abbe0dbfb5d33b028031f46ef4c328bb.jpg)\n\n\n![](images/2d42d50e8f774d938f1ed50dbda74db722cb6b31f5d09cd97ac0f1c971b3c42c.jpg)\n\n\n![](images/c307ef4812e836eda6e57b294d7b0b76de60fcda7b4a9337bcbcef70f91ea377.jpg)\n\n\n![](images/b54a1ec619acd02e06f1910aaa10e417d5a0b8e3149db88d3599a4a6f3d5e0d5.jpg)\n\n\nImage-to-Video Generation. Thanks to the autoregressive property of our model and the causal attention design, the first frame of each video acts similarly to an image condition during the training. Consequently, although our model is optimized solely for text-to-video generation, it naturally accommodates text-conditioned image-to-video generation during inference. Given an image and a textual prompt, it is able to animate the static input image by autoregressively predicting the future frames without further fine-tuning. In Fig. 6, we illustrate qualitative examples of its image-to-video generation performance, where each example consists of 120 newly synthesized frames spanning a duration of 5 seconds. As can be seen, our model successfully predicts reasonable subsequent motion, endowing the images with rich temporal dynamic information. More generated video examples are best viewed on our project page at https://anonymous-pyramid-flow.github.io.\n\n\n# 4.4 ABLATION STUDY\n\n\nIn this section, we conduct ablation studies to validate the crucial component of our methods, including the spatial pyramid in denoising trajectory and the temporal pyramid in history condition. Due to limited space, the ablations for other design choices are provided in Appendix C.2.\n\n\n![Figure 7: Ablation study of spatial pyramid at  $50\\mathrm{k}$  image training step. On the right is a quantitative comparison of the FID results, where our method achieves almost three times the convergence speed.](images/fe475018be426fc533d603510c7f92a375b40163a97f37bec4e573fdd54677fe.jpg)\n\n\n![](images/4087c7bce9a50969cc1815223a2283468ac6ab8e4bd32f647a7dc90c618c1d6b.jpg)\n\n\n![Figure 8: Ablation study of temporal pyramid at 100k low-resolution video training step.](images/ae3a5a45d5e1ffb299dac3f1ec3f4a94e57618b4fffc1213379942fb63304b24.jpg)\n\n\nEffectiveness of spatial pyramid. In the generation trajectory of the proposed spatial pyramid, only the final stage operates at full resolution, which significantly reduces the number of tokens for most denoising timesteps. With the same computational resources, it can handle more samples per training batch, greatly enhancing the convergence rate. To validate its efficiency, we designed a baseline that employs the standard flow matching objective for training text-to-image generation in our early experiments. This baseline is optimized using the same training data, number of tokens per batch, hyperparameter configurations, and model architecture to ensure fairness. The performance comparison is illustrated in Fig. 7. It can be observed that the variant using pyramidal flow demonstrates superior visual quality and prompt-following capability. We further quantitatively evaluate the FID metric of these methods on the MS-COCO benchmark (Lin et al., 2014) by randomly sampling 3K prompts. The FID performance curve over training steps is presented on the right of Fig. 7. Compared to standard flow matching, the convergence rate of our method is significantly improved.\n\n\nEffectiveness of temporal pyramid. As mentioned in Section 4.2, the temporal pyramid design can drastically reduce the computation demands compared to traditional full-sequence diffusion. Similar to the spatial pyramid, we also established a full-sequence diffusion baseline under the same experimental setting to investigate its training efficiency improvement. The qualitative comparison with the baseline is presented in Fig. 8, where the generated videos of our pyramidal variant demonstrate much better visual quality and temporal consistency under the same training steps. In contrast, the full-sequence diffusion baseline is far from convergence. It fails to produce coherent motion, leading to fragmented visual details and severe artifacts in the generated videos. This performance gap clearly highlights the training acceleration achieved by our method in video generative modeling.\n\n\n# 5 CONCLUSION\n\n\nThis work presents an efficient video generative modeling framework based on pyramidal visual representations. In contrast to cascaded diffusion models that use separate models for different image pyramids to improve efficiency, we propose a unified pyramidal flow matching objective that simultaneously generates and decompresses visual content across pyramid stages with a single model, effectively facilitating knowledge sharing. Furthermore, a temporal pyramid design is introduced to reduce computational redundancy in the full-resolution history of a video. The proposed method is extensively evaluated on VBench and EvalCrafter, demonstrating advantageous performance.\n\n\nReproducibility Statement. We will open-source our code and models to enhance reproducibility. Detailed descriptions of the experimental settings are provided in Section 4.1 and Appendix B.\n\n\n# REFERENCES\n\n- Edward H Adelson, Charles H Anderson, James R Bergen, Peter J Burt, and Joan M Ogden. Pyramid methods in image processing. RCA Engineer, 29(6):33-41, 1984.\n\n- Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In International Conference on Learning Representations, 2023.\n\n- Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1728-1738, 2021.\n\n- Black Forest Labs. FLUX. https://github.com/black-forest-labs/flux, 2024.\n\n- Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023a.\n\n- Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22563-22575, 2023b.\n\n- Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. https://openai.com/research/video-generation-models-as-world-simulators, 2024.\n\n- Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In International Conference on Machine Learning, pp. 4603-4623, 2024.\n\n- Andrew Campbell, William Harvey, Christian Weilbach, Valentin De Bortoli, Tom Rainforth, and Arnaud Doucet. Trans-dimensional generative modeling via jump diffusion models. In Advances in Neural Information Processing Systems, pp. 42217-42257, 2023.\n\n- Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3558-3568, 2021.\n\n- Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In Advances in Neural Information Processing Systems, 2024a.\n\n- Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. VideoCrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7310-7320, 2024b.\n\n- Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. VideoLLaMA 2: Advancing spatial-temporal modeling and audio understanding in video-LLMs. arXiv preprint arXiv:2406.07476, 2024.\n\n- Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Z Kaplan, and Enrico Shippole. Scalable high-resolution pixel-space image synthesis with hourglass diffusion transformers. In International Conference on Machine Learning, pp. 9550-9575, 2024.\n\n- Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 886-893, 2005.\n\n- Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch n' pack: NaViT, a vision transformer for any aspect ratio and resolution. In Advances in Neural Information Processing Systems, pp. 2252-2274, 2023.\n\n- Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12873-12883, 2021.\n\n- Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, pp. 12606-12633, 2024.\n\n- Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. DiffiT: Diffusion vision transformers for image generation. In European Conference on Computer Vision, 2024.\n\n- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, pp. 6840-6851, 2020.\n\n- Jonathan Ho, William Chan, Chitwan Sahara, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a.\n\n- Jonathan Ho, Chitwan Sahara, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):1-33, 2022b.\n\n- Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In Advances in Neural Information Processing Systems, pp. 8633-8646, 2022c.\n\n- Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-scale pretraining for text-to-video generation via transformers. In International Conference on Learning Representations, 2023.\n\n- Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. GAIA-1: A generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023.\n\n- Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21807-21818, 2024.\n\n- Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, et al. Video-LaVIT: Unified video-language pre-training with decoupled visual-motional tokenization. In International Conference on Machine Learning, pp. 22185-22209, 2024.\n\n- Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014.\n\n- Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4015-4026, 2023.\n\n- Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos\u00e9 Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. VideoPoet: A large language model for zero-shot video generation. In International Conference on Machine Learning, pp. 25105-25124, 2024.\n\n- Kuaishou. Kling. https://kling.kuaishou.com, 2024.\n\n- Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2V-Turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. In Advances in Neural Information Processing Systems, 2024.\n\n- Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. VideoDirectorGPT: Consistent multi-scene video generation via LLM-guided planning. In Conference on Language Modeling, 2024.\n\n- Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr D\u00f3l\u00e1r, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European Conference on Computer Vision, pp. 740-755, 2014.\n\n- Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2117-2125, 2017.\n\n- Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In International Conference on Learning Representations, 2023.\n\n- Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, 2023.\n\n- Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. EvalCrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22139-22149, 2024.\n\n- David G Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60:91-110, 2004.\n\n- Luma. Dream machine. https://lumalabs.ai/dream-machine, 2024.\n\n- Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024.\n\n- Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. OpenVid-1M: A large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024.\n\n- OpenAI. Sora. https://openai.com/index/sora, 2024.\n\n- George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):1-64, 2021.\n\n- William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4195-4205, 2023.\n\n- Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. W\u00fcrstchen: An efficient architecture for large-scale text-to-image diffusion models. In International Conference on Learning Representations, 2024.\n\n- Pika. Pika 1.0. https://pika.art, 2023.\n\n- PKU-Yuan Lab, Tuzhan AI, et al. Open-Sora plan. https://github.com/PKU-YuanGroup/Open-Sora-Plan, 2024.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763, 2021.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(1):5485-5551, 2020.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684-10695, 2022.\n\n- Runway. Gen-2. https://research.runwayml.com/gen2, 2023.\n\n- Runway. Gen-3 alpha. https://runwayml.com/research/introducing-gen-3-alpha, 2024.\n\n- Chitwan Sahara, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, pp. 36479-36494, 2022.\n\n- Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. In Advances in Neural Information Processing Systems, pp. 25278-25294, 2022.\n\n- Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-A-Video: Text-to-video generation without text-video data. In International Conference on Learning Representations, 2023.\n\n- Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256-2265, 2015.\n\n- Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pp. 11918-11930, 2019.\n\n- Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.\n\n- Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. RoFormer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\n\n- Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. JourneyDB: A benchmark for generative image understanding. In Advances in Neural Information Processing Systems, pp. 49659-49678, 2023.\n\n- Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. In International Conference on Learning Representations, 2024.\n\n- Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In Advances in Neural Information Processing Systems, 2024.\n\n- Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000-6010, 2017.\n\n- Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43 (10):3349-3364, 2020.\n\n- Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. ModelScope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023a.\n\n- Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. LaVie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023b.\n\n- Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A large video description dataset for bridging video and language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5288-5296, 2016.\n\n- Yilun Xu, Ziming Liu, Max Tegmark, and Tommi Jaakkola. Poisson flow generative models. In Advances in Neural Information Processing Systems, pp. 16782-16795, 2022.\n\n- Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. PeRFlow: Piecewise rectified flow as universal plug-and-play accelerator. In Advances in Neural Information Processing Systems, 2024.\n\n- Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. VideoGPT: Video generation using VQ-VAE and transformers. arXiv preprint arXiv:2104.10157, 2021.\n\n- Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, JiaZheng Xu, Yuanming Yang, Xiaohan Zhang, Xiaotao Gu, Guanyu Feng, et al. CogVideoX: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024.\n\n- Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al. SPAE: Semantic pyramid autoencoder for multimodal generation with frozen LLMs. In Advances in Neural Information Processing Systems, pp. 52692-52704, 2023.\n\n- Lijun Yu, Jos\u00e9 Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion\u2013tokenizer is key to visual generation. In International Conference on Learning Representations, 2024.\n\n- David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023.\n\n- Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-Sora: Democratizing efficient video production for all. https://github.com/hpcaitech/Open-Sora, 2024.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638628.1530275", "msecs": "153.0", "relativeCreated": "224983.5250377655", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Data-centric Prediction Explanation via Kernelized Stein Discrepancy\n\n# Abstract\nExisting example-based prediction explanation methods often bridge test and training data points through the model\u2019s parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant computational overhead or producing coarse-grained explanations. This paper presents a Highly-precise and Data-centric Explanation (HD-Explain) prediction explanation method that exploits properties of Kernelized Stein Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized kernel function for a trained model that encodes model-dependent data correlation. By leveraging the kernel function, one can identify training samples that provide the best predictive support to a test point efficiently. We conducted thorough analyses and experiments across multiple classification domains, where we show that HD-Explain outperforms existing methods from various aspects, including 1) preciseness (fine-grained explanation), 2) consistency, and 3) computation efficiency, leading to a surprisingly simple, effective, and robust prediction explanation solution.\n\n\n# 1 INTRODUCTION\n\n\nAs one of the decisive factors affecting the performance of a Machine Learning (ML) model, training data points are of great value in promoting the model's transparency and trustworthiness, including explaining prediction results, tracing sources of errors, or summarizing the characteristics of the model (Cai et al., 2019; Anik & Bunt, 2021; Nam et al., 2022). The challenges of example-based prediction explanation mainly come from retrieving relevant data points from a vast pool of training samples or justifying the rationale of such explanations (Lim et al., 2019; Zhou et al., 2021).\n\n\nModern example-based prediction explanation methods commonly approach the above challenges by constructing an influence chain between training and test data points (Li et al., 2020; Nam et al., 2022; Tsai et al., 2023). The influence chain could be either data points' co-influence on model parameters or their similarity in terms of latent representations. In particular, Influence Function (Koh & Liang, 2017), one of the representative, model-aware explanation methods, looks for the shift of the model parameters (due to up-weighting each training sample) as the sample's influence score. Since computing the inverse Hessian matrix is challenging, the approach adapts Conjugate Gradients Stochastic Estimation and the Perlmutter trick to reduce its computation cost. Representer Point Selection (RPS) (Yeh et al., 2018), as another example, reproduces the representer theorem by refining the trained neural network model with  $L2$  regularization, such that the influence score of each training sample can be represented as the gradient of the predictive layer. While computationally efficient, RPS is criticized for producing coarse-grained explanations that are more class-level rather than instance-level explanations (Sui et al., 2021) (In this paper we use the instance level explanation and example-based explanation interchangeably.). Multiple later variants (Pruthi et al., 2020; Sui et al., 2021) attempted to mitigate the drawbacks above, but their improvements were often limited by the cause of their shared theoretical scalability bounds.\n\n\nThis paper presents Highly-precise and Data-centric Explanation (HD-Explain), a post-hoc, model-aware, example-based explanation solution for neural classifiers. Instead of relying on data co-influence on model parameters or feature representation similarity, HD-Explain retains the influence chain between training and test data points by exploiting the underrated properties of Kernelized Stein Discrepancy (KSD) (Liu et al., 2016) between the trained predictive model and its training\n\n\nTable 1: Summary of existing Post-hoc Example-based Prediction Explanation Methods that work with deep neural networks. Practicality of the whole model explanation is measured by the feasibility of explaining the prediction of ResNet-18 trained on CIFAR-10 with a single A100 GPU machine. CIFAR-10 is a small benchmark data with 50000 training samples.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Explanation of</td><td rowspan=\"2\">Need optimization as sub-routine</td><td colspan=\"2\">Whole model explanation</td><td rowspan=\"2\">Inference computation complexity bounded by</td><td rowspan=\"2\">Memory/cache (of each training sample) bounded by</td></tr><tr><td>Theoretical</td><td>Practical</td></tr><tr><td>Influence Function</td><td>Original Model</td><td>Yes (Iterative HVP approximation)</td><td>Yes</td><td>No</td><td>1.H\u03b8-1\u2207\u03b8L(xt,\u03b8) approximation 2. &lt; \u2207\u03b8L(x,\u03b8), H\u03b8-1\u2207\u03b8L(xt,\u03b8) &gt;</td><td>Size of model parameters</td></tr><tr><td>RPS</td><td>Fine-tuned Model</td><td>Yes (L2 regularized last layer retrain)</td><td>No</td><td>No</td><td>1.last layer representation ft 2.&lt; \u03b1tf, ft &gt;</td><td>Size of model parameters of the last layer</td></tr><tr><td>TracIn*</td><td>Original Model</td><td>No</td><td>Yes</td><td>No</td><td>1.\u2207\u03b8L(xt,\u03b8) approximation 2. &lt; \u2207\u03b8L(x,\u03b8), \u2207\u03b8L(xt,\u03b8) &gt;</td><td>Size of model parameters</td></tr><tr><td>HD-Explain</td><td>Original Model</td><td>No</td><td>Yes</td><td>Yes</td><td>1.\u2207xf(xt,\u03b8)yt 2. Closed-form k\u03b8(x,xt) defined by KSD</td><td>Size of data dimension</td></tr></table>\n\n\ndataset. Specifically, we note that the Stein operator augmented kernel uniquely defines a pairwise data correlation (in the context of a trained model) whose expectation on the training dataset results in the minimum KSD (as a discrete approximation) compared to that of the dataset sampled from different distributions. By exploiting this property, we can 1) reveal a subset of training data points that provides the best predictive support to the test point and 2) identify the potential distribution mismatch among training data points. Jointly leveraging these advantages, HD-Explain can produce explanations faithful to the original trained model.\n\n\nThe contributions of our work are summarized as follows: 1) We propose a novel example-based explanation method. 2) We propose several quantitative evaluation metrics to measure the correctness and effectiveness of generated explanations. 3) We perform a thorough evaluation comparing several existing explanation methods across a wide set of classification tasks.\n\n\nOur findings conclude that HD-Explain offers fine-grained, instance-level explanations with remarkable computational efficiency, compared to well-known example-based prediction explanation methods. Its algorithmic simplicity, effective performance and scalability enables its deployment to real world scenarios where transparency and trustworthiness is essential.\n\n\n# 2 PRELIMINARY AND RELATED WORK\n\n\n# 2.1 POST-HOC CLASSIFIER EXPLANATION BY EXAMPLES\n\n\nPost-hoc Classifier Explanation by Examples (a.k.a prototypes, representatives) refers to a category of classifier explanation approaches that pick a subset of training data points as prediction explanations without accessing the model training process. Its research history spans from the model-intrinsic approach (see survey Molnar (2020)) to the recent impact-based approach (Li et al., 2020).\n\n\nModel-inherent approaches (Molnar, 2020) refer to machine learning models that are considered interpretable such as  $k$ -nearest neighbor (Peterson, 2009) or decision tree; For a given test data point, similar data points on the raw feature space can be efficiently selected as explanations through the inherent decision making mechanism of the self-explanatory machine learning models. In fact, attracted by their inherent explanatory power, multiple well-known works attempted to compile complex black-box models into self-explanatory models for enabling prediction explanation (Frosst & Hinton, 2017), while computationally inefficient.\n\n\nTo unlock the general explanatory power applicable to black-box models, multiple later studies suggested to fall back to statistics-based solutions, looking for prototype samples that represent data points that either are common in the dataset or play critical roles in data distribution. MMD-critic (Kim et al., 2016) and Normative and Comparative explanations (Cai et al., 2019) are the well-known examples in this category. Unfortunately, those approaches are often with strong assumption of good prototypes (which often overlook the characteristics of trained models) (Li et al., 2020), making their prediction explanations general to the training dataset rather than a trained model instance.\n\n\nRecently, influence-based methods have emerged as the prevailing technique in model explanation (Li et al., 2020; Nam et al., 2022; Bae et al., 2022; Park et al., 2023a). Influence function (Koh & Liang, 2017), as one of the earliest influence-based solutions, bridges the outcome of a prediction task to training data points by, first, evaluating training data's influence on the model parameters and, then, estimating how model parameter changes affect prediction. Similarly, Representative Point Selection (RPS) (Yeh et al., 2018) builds such an influence chain by fitting the representation theorem, where the weighted product between the representations of test and training samples comes into play. Concerning the computational overhead of previous work, the later solution TracIn (Pruthi et al., 2020) proposed a simple approximation of the influence function via a first-order Taylor expansion (essentially Neural Tangent Kernel (Jacot et al., 2018)), successfully discarding the inverse Hessian matrix from the influence chain formulation. BoostIn (Brophy et al., 2023) further extends TracIn and is dedicated to interpreting the predictions of gradient-boosted decision trees. RPS-LJE (Sui et al., 2021), on the other hand, alleviated the inconsistent explanation problem of RPS through Local Jacobian Expansion. In the latest publication (Tsai et al., 2023), all of the above methods described in this paragraph are identified as special cases of Generalized Representatives but with different chosen kernels.\n\n\nOne limitation of the current influence-based methods is that they attribute the influence of each training data point to the parameters of the trained model as an essential intermediate step. Indeed, as the nature of stochastic gradient descent (the dominating training strategy of neural networks), isolating such contribution is barely possible without 1) relying on approximations or 2) accessing the training process. Unfortunately, either solution would result in performance degradation or heavy computational overhead (Schioppa et al., 2022). Hence, this work delves into the exploration of an alternative influence connection between training and test data points without exploiting the perturbation of model parameters.\n\n\n# 2.2 KERNELIZED STEIN DISCREPANCY\n\n\nThe idea of Kernelized Stein Discrepancy (KSD) (Liu et al., 2016) can be traced back to a theorem called Stein Identity (Kattumannil, 2009) that states, if a smooth distribution  $p(x)$  and a function  $\\phi(x)$  satisfy  $\\lim_{||x|| \\to \\infty} p(x)\\phi(x) = 0$ ,\n\n\n$$\n\n$$\n\\mathbb {E} _ {x \\sim p} [ \\phi (x) \\nabla_ {x} \\log p (x) + \\nabla_ {x} \\phi (x) ] = 0, \\quad \\forall \\phi .\n$$\n\n$$\n\n\nThe identity can characterize distribution  $p(x)$  such that it is often served to assess the goodness-of-fit (Kubokawa, 2024) of the model. The above expression could be further abstracted to use function operator  $\\mathcal{A}_p$  (a.k.a Stein operator) such that\n\n\n$$\n\n$$\n\\mathcal {A} _ {p} \\phi (x) = \\phi (x) \\nabla_ {x} \\log p (x) + \\nabla_ {x} \\phi (x),\n$$\n\n$$\n\n\nwhere the operator encodes distribution  $p(x)$  in the form of derivative to input (a.k.a score function).\n\n\nStein's identity offers a mechanism to measure the gap between two distributions by assuming the variable  $x$  is sampled from a different distribution  $q \\neq p$  such that\n\n\n$$\n\n$$\n\\sqrt {\\mathbb {S} (q , p)} = \\max _ {\\phi \\in \\mathcal {F}} \\mathbb {E} _ {x \\sim q} [ \\mathcal {A} _ {p} \\phi (x) ],\n$$\n\n$$\n\n\nwhere the expression takes the most discriminant  $\\phi$  that maximizes the violation of Stein's identity to quantify the distribution discrepancy. This discrepancy is, accordingly, referred as Stein Discrepancy.\n\n\nThe challenge of computing Stein Discrepancy comes from the selection of function set  $\\mathcal{F}$ , which motivates the later innovation of KSD that takes  $\\mathcal{F}$  to be the unit ball of a reproducing kernel Hilbert space (RKHS). By leveraging the reproducing property of RKHS, the KSD could be eventually transformed into\n\n\n$$\n\n$$\n\\mathbb {S} (q, p) = \\mathbb {E} _ {x, x ^ {\\prime} \\sim q} [ \\kappa_ {p} (x, x ^ {\\prime}) ]\n$$\n\n$$\n\n\nwhere  $\\kappa_{p}(x,x^{\\prime}) = \\mathcal{A}_{p}^{x}\\mathcal{A}_{p}^{x^{\\prime}}k(x,x^{\\prime})$  that can work with arbitrary kernel function  $k(x,x^{\\prime})$ . See Appendix C for expanded derivations.\n\n\nIn the literature, KSD has been adopted for tackling three types of application tasks - 1) parameter inference (Barp et al., 2019), 2) Goodness-of-fit tests (Chwialkowski et al., 2016; Liu et al., 2016; Yang et al., 2018), and 3) particle filtering (sampling) (Gorham et al., 2020; Korba et al., 2021). However, to the best of our knowledge, its innate property that uniquely defines model-dependent data correlation has never been exploited, which, we note, is valuable to interpret model behaviour from various aspects, including instance-level prediction explanation and global prototypical explanations.\n\n\n![Figure 1: Varying of Kernelized Stein Discrepancy given the shift of training data distribution on Two Moon dataset.](images/29fd56d5ca01c162d34e47e66ed4c4d2a26156520176e453f3284b16211e0f5c.jpg)\n\n\n# 3 HIGHLY-PRECISE AND DATA-CENTRIC EXPLANATION\n\n\nHD-Explain is an example-based prediction explanation method based on Kernelized Stein Discrepancy. Consider a trained classifier  $f_{\\theta}$  as the outcome of a training process with Maximum Likelihood Estimation (MLE)\n\n\n$$\n\n$$\n\\underset {\\theta} {\\operatorname {a r g m a x}} \\mathbb {E} _ {(\\mathbf {x}, y) \\sim P _ {D}} \\left[ \\log P _ {\\theta} (y | \\mathbf {x}) \\right].\n$$\n\n$$\n\n\nTheoretically, maximizing observation likelihood is equivalent to minimizing a KL divergence between data distribution  $P_{D}$  and the parameterized distribution  $P_{\\theta}$  such that\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathbb {D} _ {\\mathrm {K L}} \\left(P _ {D}, P _ {\\theta}\\right) = \\mathbb {E} _ {(\\mathbf {x}, y) \\sim P _ {D}} \\left[ \\log \\frac {P _ {D} (\\mathbf {x} , y)}{P _ {\\theta} (\\mathbf {x} , y)} \\right] \\\\ = - \\underbrace {\\mathbb {E} _ {(\\mathbf {x} , y) \\sim P _ {D}} \\left[ \\log P _ {\\theta} (y | \\mathbf {x}) \\right]} _ {\\text {L i k e l i h o o d}} + \\underbrace {\\mathbb {E} _ {(\\mathbf {x} , y) \\sim P _ {D}} \\left[ \\log P _ {D} (y | \\mathbf {x}) \\right]} _ {\\text {c o n s t a n t}} + \\underbrace {\\mathbb {E} _ {(\\mathbf {x} , y) \\sim P _ {D}} \\left[ \\log P _ {D} (\\mathbf {x} / P _ {\\theta} (\\mathbf {x}) \\right]} _ {\\text {c o n s t a n t a s \\theta d o e s n o t m o d e l i n p u t s}}, \\\\ \\end{array}\n$$\n\n$$\n\n\nwhich, in turn, is proven to align with minimizing KSD in the form of gradient descent (Liu & Wang, 2016)\n\n\n$$\n\n$$\n\\nabla_ {\\epsilon} \\mathbb {D} _ {\\mathrm {K L}} \\left(P _ {D}, P _ {\\theta}\\right) | _ {\\epsilon} = - \\mathbb {S} \\left(P _ {D}, P _ {\\theta}\\right),\n$$\n\n$$\n\n\nwhere  $\\epsilon$  is the step size of gradient decent.\n\n\nThe chain of reasoning above shows that a well-trained classifier  $f_{\\theta}$  through gradient-descent should lead to minimum discrepancy between the training dataset distribution and the model encoded distribution  $\\mathbb{S}(P_D, P_\\theta)^1$ . We empirically verify the connection through simple examples as shown in Figure 1, where the changes in training data distribution would result in larger KSD compared to that of the original training data distribution. Intuitively, the connection shows that there is a tie between a model and its training data points, encoded in the form of a Stein kernel function  $k_{\\theta}(\\cdot, \\cdot)$  defined on each pair of data points. As the kernel function is conditioned on model  $f_{\\theta}$ , we note it is an encoding of data correlation under the context of a trained model, which paves the foundation of the example-based prediction explanation.\n\n\n# 3.1 KSD BETWEEN MODEL AND TRAINING DATA\n\n\nRecall that KSD,  $\\mathbb{S}(P_D,P_\\theta)$ , defines the correlation between pairs of training samples through model  $\\theta$  dependent kernel function with closed-form decomposition\n\n\n$$\n\n$$\n\\begin{array}{l} \\kappa_ {\\theta} \\left(\\left(\\mathbf {x} _ {a}, y _ {a}\\right), \\left(\\mathbf {x} _ {b}, y _ {b}\\right)\\right) = \\mathcal {A} _ {\\theta} ^ {a} \\mathcal {A} _ {\\theta} ^ {b} k (a, b) \\\\ = \\nabla_ {a} \\nabla_ {b} k (a, b) + k (a, b) \\nabla_ {a} \\log P _ {\\theta} (a) \\nabla_ {b} \\log P _ {\\theta} (b) \\tag {1} \\\\ + \\nabla_ {a} k (a, b) \\nabla_ {b} \\log P _ {\\theta} (b) + \\nabla_ {b} k (a, b) \\nabla_ {a} \\log P _ {\\theta} (a), \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere we denote data point  $(\\mathbf{x}_a, y_a)$  with  $a$  for clean notation. The only model-dependent factor in the above decomposition is a derivative  $\\nabla_{\\mathbf{x},y} \\log P_\\theta(\\mathbf{x},y)$  (for both data  $a$  or  $b$ ).\n\n\nHowever, as the KSD only models the discrepancy between joint distributions rather than conditional distributions, it is challenging to estimate discrepancy between predictive models  $P_{\\theta}(y|\\mathbf{x})$  and its training set  $\\mathbf{z} = (\\mathbf{x}, y) \\sim P_D$  without including  $P_{\\theta}(\\mathbf{x})$  into consideration, even though the marginal distribution  $P_{\\theta}(\\mathbf{x})$  is not estimated by the predictive model at all.\n\n\n![Figure 2: Demonstration of HD-Explain on 2D Rectangular synthetic dataset. Left shows the training dataset with three classes. Middle figure shows the explanation support of training data points to a given test point (as black cross), where green shows a higher KSD kernel value. Right shows the distribution of KSD kernel values (over the training set) to the test point, where only a small number of training data points provide strong support to this prediction.](images/1774b418c6af2cab669415f058b4718e6857a15c519ccec6f693df2a0d666015.jpg)\n\n\nInspired by the previous study on Goodness of Fit (Jitkrittum et al., 2020), to unlock KSD support on predictive models, we propose to set  $P_{\\theta}(\\mathbf{x}) \\equiv P_D(\\mathbf{x})$  such that the identical marginal distribution would not contribute to the discrepancy between the joint distributions  $P_{D}(\\mathbf{x},y)$  and  $P_{\\theta}(\\mathbf{x},y)$ . In addition, while original distribution  $P(\\mathbf{x})$  that generates data could be arbitrary complex distribution (that is out of modelling scope of the predictive model), we may simply set data point distribution  $P_{D}$  (not original distribution  $P$ ) as an Uniform distribution over data points in the dataset under the particular context, given a data point is sampled from a generated dataset uniformly. Although the relaxation appears hasty, we believe that the relaxation is valid in the prediction explanation context (and probably only valid in this particular context) in terms of measuring correlation between a pair of data points, where all train/test data points' input are valid observation (e.g. images) rather than random continuous valued sample in the same space form an unknown distribution.\n\n\nWith the above relaxations, the score function  $\\nabla_{\\mathbf{x},y}\\log P_{\\theta}(\\mathbf{x},y)$  in the Stein operator  $\\mathcal{A}_{\\theta}$  could be derived as a concatenation of the gradient of model  $f_{\\theta}(\\mathbf{x})_y$  to its input  $\\mathbf{x}$  and its probabilistic prediction  $f_{\\theta}(\\mathbf{x})$  in logarithm form, since\n\n\n$$\n\n$$\n\\begin{array}{l} \\nabla_ {\\mathbf {x}, \\mathbf {y}} \\log P _ {\\theta} (\\mathbf {x}, \\mathbf {y}) = \\nabla_ {\\mathbf {x}, \\mathbf {y}} [ \\log P _ {\\theta} (\\mathbf {y} | \\mathbf {x}) + \\log P _ {D} (\\mathbf {x}) ] \\\\ = \\nabla_ {\\mathbf {x}, \\mathbf {y}} \\log P _ {\\theta} (\\mathbf {y} | \\mathbf {x}) + [ \\nabla_ {\\mathbf {x}} \\log P _ {D} (\\mathbf {x}) ] [ \\nabla_ {\\mathbf {y}} P _ {D} (\\mathbf {x}) ] \\\\ = \\left[ \\nabla_ {\\mathbf {x}} \\mathbf {y} ^ {\\top} \\log f _ {\\theta} (\\mathbf {x}) \\right\\rvert   \\left\\lvert   \\right. \\nabla_ {\\mathbf {y}} \\mathbf {y} ^ {\\top} \\log f _ {\\theta} (\\mathbf {x}) ] + [ \\mathbf {0} | | \\mathbf {0} ] \\tag {2} \\\\ = \\left[ \\nabla_ {\\mathbf {x}} \\log f _ {\\theta} (\\mathbf {x}) _ {y} \\right| | \\log f _ {\\theta} (\\mathbf {x}) ], \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere  $[\\cdot ||\\cdot ]$  denotes concatenation operation. We use one-hot vector representation  $\\mathbf{y}$  to represent data label  $y$  here. Since  $P_{D}(\\mathbf{x})$  follows uniform distribution, its gradient to the inputs is a 0 vector.\n\n\nIn the above derivation, we treat discrete label  $y$  as-is without specialized discrete distribution treatments (see (Yang et al., 2018)) to avoid significant computation overhead. In fact, data space in practice is unlikely dense even if a group of features are continuous (e.g. images). Treating the label as a sparse continuous feature can be viewed as an approximation.\n\n\nCombining Equation 1 and 2, we can estimate the correlation of any pairs of training data points conditioned on the trained machine learning model. Computationally, since a score function  $\\nabla_{\\mathbf{x},y}\\log P_{\\theta}(\\mathbf{x},y)$  depends on a single data point, its outputs of the training set could be pre-computed and cached to accelerate the kernel computation. In particular, the output dimension of the score function is simply  $m + k$  for data with  $m$  dimensional features and  $k$  class labels. Compared to the existing solutions, whose training data cache (or influence) are bounded by the dimension of model parameters (such as Influence function, TracIn, RPS, RPS-JLE), the explanation method built on KSD would come with a significant advantage in terms of scalability (see comparison in Table 1). This statement is generally true for neural network based classifiers, whenever the size of model parameters is far larger than the data dimension.\n\n\n# 3.2 PREDICTION EXPLANATION\n\n\nThe computation of kernel function in Equation 1 requires access to features and labels of a data point. While the ground-truth label information is available for the training set, it is inaccessible for a test data point. We consider the predicted class  $\\hat{y}_t$  of the test data point  $\\mathbf{x}_t$  as a label to construct a complete data point  $(\\mathbf{x}_t, \\hat{y}_t)$  and apply the KSD kernel function. For a test data point  $\\mathbf{x}_t$ , we search for top-k training data points that maximize the KSD defined kernel.\n\n\nFigure 2 demonstrates HD-Explain on a 2d synthetic dataset. The distribution of  $\\kappa_{\\theta}(\\mathbf{d},\\cdot)$  in the right most plot shows that only a small number of training data points have a strong influence on a particular prediction.\n\n\n# 4 EVALUATION AND ANALYSIS\n\n\nIn this section, we conduct several qualitative and quantitative experiments to demonstrate various properties of HD-Explain and compare it with the existing example-based solutions.\n\n\nDatasets: We consider multiple disease classification tasks where diagnosis explanation is highly desired. We also introduced synthetic and benchmark classification datasets to deliver the main idea without the need for medical background knowledge. Concretely, we use CIFAR-10  $(32 \\times 32 \\times 3)$ , Brain Tumor (Magnetic Resonance Imaging,  $128 \\times 128 \\times 3$ ), Ovarian Cancer (Histopathology Images,  $128 \\times 128 \\times 3$ ) datasets, and SVHN  $(32 \\times 32 \\times 3)$ . More details are listed in the Appendix F.\n\n\nBaselines: The baseline explainers used in our experiments include Influence Function, Representer Point Selection, and TracIn. While other variants of these baseline explainers exist (Barshan et al., 2020; Sui et al., 2021; Chen et al., 2021), we note they don't offer fundamental performance improvements over the classic ones. In addition, as Influence Function and TracIn face scalability issues, we limit the influence of parameters to the last layer of the model so that they can work with models that contain a large number of parameters. Our experiments use ResNet-18 as the backbone model architecture (with around 11 million trainable parameters) for all image datasets (see Appendix H for detail on our hardware setup).\n\n\nFinally, we also introduce an HD-Explain variant (HD-Explain*) to match the last layer setting of other baseline models, even though HD-Explain can scale up to the whole model without computation pressure. The HD-Explain* is a simple change of HD-Explain in terms of using data representations (the output of the last non-predictive layer of the neural classifier) rather than the raw features. Specifically, we assume a neural network model  $f_{\\theta}$  could be decomposed into two components  $f_{\\theta_2} \\cdot f_{\\theta_1}$ , where  $f_{\\theta_1}$  is a representation encoder and  $f_{\\theta_2}$  is a linear model for prediction. With this decomposition, we define the KSD kernel function for HD-Explain* as\n\n\n$$\n\n$$\n\\begin{array}{l} \\kappa_ {\\theta} \\big ((f _ {\\theta_ {1}} (\\mathbf {x} _ {a}), y _ {a}), (f _ {\\theta_ {1}} (\\mathbf {x} _ {b}), y _ {b}) \\big) = \\mathcal {A} _ {\\theta_ {2}} ^ {a} \\mathcal {A} _ {\\theta_ {2}} ^ {b} k (a, b) \\\\ = \\nabla_ {a} \\nabla_ {b} k (a, b) + k (a, b) \\nabla_ {a} \\log P _ {\\theta_ {2}} (a) \\nabla_ {b} \\log P _ {\\theta_ {2}} (b) \\\\ + \\nabla_ {a} k (a, b) \\nabla_ {b} \\log P _ {\\theta_ {2}} (b) + \\nabla_ {b} k (a, b) \\nabla_ {a} \\log P _ {\\theta_ {2}} (a), \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere we define  $a = (f_{\\theta_1}(\\mathbf{x}_a),y_a)$  and  $b = (f_{\\theta_1}(\\mathbf{x}_b),y_b)$  for short. This setting reduces the prediction explanation to the last layer of the neural network in a similar fashion to RPS.\n\n\nMetrics: In existing example-based explanation works, the experimental results are often demonstrated qualitatively, as visualized explanation instances, without quantitative evaluation. This results in subjective evaluation. In this paper, we propose several quantitative evaluation metrics to measure the effectiveness of each method.\n\n- - Hit Rate: Hit rate measures how likely an explanation sample hits the desired example cases where the desired examples are guaranteed to be undisputed. Specifically, we modify a training data point with minor augmentations (adding noise or flipping horizontally) and use it as a test data point, such that the best explanation for the generated test data point should be the original data point in the training set.\n\n- - Coverage: Given n test data points, the metric measures the number of unique explanation samples an explanation method produces when configuring to return top-k training samples.\n\n\n![(a) High-Conf Correct Predict (b) Low-Conf Correct Predict (c) Low-Conf Incorrect Predict](images/7be398a60d0134ac83305630465254173348b2e52345f0daff375de7faf03248.jpg)\n\n\n![Figure 3: Qualitative evaluation of various example-based explanation methods using CIFAR10. We show three scenarios where the target model makes a) a highly-confident prediction that matches ground truth label, b) a low-confident prediction that matches ground truth label, c) low-confident prediction that does not match ground truth label (which is a bird). For each sub plot, we show top-3 influential training data points picked by the explanation methods for the test example. (a) High-Conf Correct Predict (b) High-Conf Correct Predict (c) Low-Conf Correct Predict Figure 4: Qualitative evaluation of various example-based explanation methods using SVHN. We show two scenarios where the target model makes a-b) a highly-confident prediction that matches ground truth label, c) a low-confident prediction that matches ground truth label. For each sub plot, we show top-3 influential training data points picked by the explanation methods for the test example. We include two samples of high-confidence correct predictions to show the overlap of explanations.](images/34183ff80692e226c3dfb205020e4773748da6ef97f062bf380b94c07eef9bbb.jpg)\n\n\nFormally,\n\n\n$$\n\n$$\n\\text {C o v e r a g e} = \\frac {\\left| \\cup_ {i = 1} ^ {n} e _ {i} \\right|}{n \\times k}\n$$\n\n$$\n\n\nwhere  $e_i$  is the set of top-k explanations for test data point  $i$ . Coverage is motivated to measure the diversity of explanations across a test set where a high value reflects higher granularity (per test point) of the explanation.\n\n\n- Run Time: It measures the run time of an explanation method in wall clock time.\n\n\n# 4.1 QUALITATIVE EVALUATION\n\n\nFigure 3 shows three test cases of the CIFAR10 classification task that cover different classification outcomes, including high-confidence correct prediction, low-confident correct prediction, and low-confident incorrect prediction. For both correct prediction cases, we are confident that HD-Explain provides a better explanation than others in terms of visually matching test data points e.g. brown\n\n\n![(a) Hit Rate](images/8cadb84a504c741375e1fdc483c1c68a10a3d63d2aa51908257fb18f3e3ed192.jpg)\n\n\n![(b) Coverage](images/3e4e1512a4885aa02f5ba409bc07f1f1558ef73aa5406a0ae5eb7a27038f855d.jpg)\n\n\n![(c) Execution Time](images/e775f7f9232d5e7d1c90b51dda90381e6bfd2f5b46a1c92026eac2d8654cd017.jpg)\n\n\n![Figure 5: Quantitative explanation comparison among candidate example-based explanation methods. Data augmentation strategy used is Noise Injection. Error bar shows  $95\\%$  confidence interval. (a) Hit Rate Figure 6: Quantitative explanation comparison among candidate example-based explanation methods. Data augmentation strategy used is Horizontal Flip. Error bar shows  $95\\%$  confidence interval. We reuse the legend of Figure 12.](images/2ecb5f2368d113728dc8312d3c711dbd95cc0a222d617925ce601ddac7e304e2.jpg)\n\n\n![(b) Coverage](images/ca9130d1ad190a2f5b4d36da28d3e958179cff9d938f146ea62adce40b4cc357.jpg)\n\n\n![(c) Execution Time](images/5a7d342ef28ff441af2698140a3172ac3a5c2e7fad5dcbed59f758b7425a22b6.jpg)\n\n\nfrogs in Figure 3 (a) and deer on the grass in Figure 3 (b). In contrast, for the misclassified prediction case (as shown in Figure 3 (c)), we note the HD-Explain produces an example that does not even belong to the same class as the predicted one. Specifically, the predicted class is cat (while the ground truth label is bird), and HD-Explain generates an explanation sample from the deer class. This reflects a low confidence in model's prediction for the particular test example and highlight a potential error in the prediction. RPS also shows such inconsistency in explanation, which aligns with its claim (Yeh et al., 2018). The other two baseline methods do not offer such properties and still produce explanations that match the predicted label well. It is hard to justify how those training samples support such prediction visually (since no clear shared pattern is obvious to us). In addition, it is interesting to see that Influence Function and TracIn produce near identical explanations, reflecting their similarity in leveraging the perturbation of model parameters.\n\n\nFigure 4 provides additional insights on SVHN dataset. HD-Explain again shows a better explanation for producing training samples that appear similar to the test samples. In addition, we notice that RPS produces the same set of explanations for different sample cases, as shown in Fig. 4 (a-b), which reveals its limitation in providing instance-level explanations. To verify this observation further, we conducted a quantitative evaluation as described in the next section.\n\n\nThe qualitative evaluation for OCH and MRI datasets are given in the appendix due to the page limit of the main paper. The overall observations remain consistent with CIFAR10 and SVHN.\n\n\n# 4.2 QUANTITATIVE EVALUATION\n\n\nIn order to perform quantitative evaluation, we limit our experiments to datasets where ground-truth explanation samples are available. Specifically, given a training data sample  $(\\mathbf{x}_i,y_i)$ , we generate a test point  $\\mathbf{x}_t$  by adopting two image data augmentation methods:\n\n- - Noise Injection:  $\\mathbf{x}_t = \\mathbf{x}_i + \\epsilon$  s.t.  $\\epsilon \\sim \\mathcal{N}(0,0.01\\sigma)$ , where  $\\sigma$  is the element-wise standard deviation of features in the entire training dataset.\n\n- - Horizontal Flip:  $\\mathbf{x}_t = \\mathrm{flip}(\\mathbf{x}_i)$ , where we flip images horizontally that do not compromise the semantic meaning of images.\n\n\n![(a) Hit Rate (Noise Injection) Figure 7: Quantitative explanation comparison among HD-Explainers with different kernel functions on all image classification datasets. Error bar shows  $95\\%$  confidence interval.](images/5168ad98de467946936e0b9ac5a8ce73274677e4f580df2c9629c6885ed6d858.jpg)\n\n\n![(b) Coverage both)](images/f21ba73d78b8cfdfd456203c1f5244ebf5c9c3ba3f75af154f8eb72b31190484.jpg)\n\n\n![(c) Hit Rate (Horizontal Flip)](images/18ea99add895f129d01ba446b5cbfe367659861f7e5a336568a39e4bef79b525.jpg)\n\n\n![(d) Execution Time both)](images/e332dde6889af94e070c313fd81915462c39d473764e075147d6fd320422f1c1.jpg)\n\n\nWe created 30 augmented test points for each training data point ( $>10,000$  data points) in each dataset, resulting in more than 300,000 independent runs. Since the data augmentation is guaranteed to maintain prediction consistency, the ideally best explanation for the generated test point is the original data point  $\\mathbf{x}_i$  itself. Hence, the quantitative evaluation could be a sample retrieval evaluation where Hit Rate measures the probability of successful retrieval.\n\n\nFigure 12(a) shows the hit rate comparison among candidate methods on the four image classification datasets under Noise Injection data augmentation. The existing methods face significant difficulty in retrieving the ideal explanatory sample ( $\\leq 10\\%$ ), even with such a simple problem setup; only HD-Explain (and its variant) produces a reasonable successful rate ( $>80\\%$ ). We further investigate the diversity of explanations across a testset using the Coverage metric. Here, diversity indirectly reflects the granularity of an explanation when accumulated over the test dataset. Figure 12(b) shows the Coverage score, the ratio of explanation samples that are unique over many test points. It turns out that existing solutions produce only  $10\\% - 50\\%$  coverage \u2013 many test points receive the same set of explanations, disregarding their unique characteristic. We further observed that the explanations of baselines are often dominated by the class labels; data points predicted as the same class would receive a similar set of explanations. In contrast, HD-Explain shows substantially higher coverage, generating explanations that considers the unique characteristics of each test point.\n\n\nRegarding computation efficiency, while we have summarized the scalability limitation of the candidate methods in Table 1, there was no computational efficiency evaluation conducted in previous works. We recorded the wall clock execution time of each experiments as shown in Figure 12(c). As expected, the Influence Function takes longer to return its explanation than other methods. HD-Explain*, TracIn* and RPS, all use the last layer to generate explanation. RPS showed the lowest compute time since it does not require auto-differentiation for computing the training data influence. HD-Explain* showed the second best compute time and is efficient than TracIn $^{*2}$  and IF. HD-Explain considers the whole model for explanation and its compute time is not directly comparable to others. However, it shows better efficiency than IF across all datasets and is better than TracIn* on CIFAR10.\n\n\nWe observe a similar trend in the other data augmentation scenario, Horizontal Flip, where computation time and coverage are roughly the same, as shown in Figure 6. However, we do notice that, as the outcome of image flipping, the raw feature (pixel) level similarity between  $\\mathbf{x}_t$  and  $\\mathbf{x}_i$  is destroyed. As an outcome, the HD-Explain that works on raw features suffers from performance deduction while other methods, including HD-Explain*, are less impacted. This observation suggests that choosing the layer of explanation might be considered in the practical usage of this approach.\n\n\n# 4.3 KERNEL OPTIONS\n\n\nWe use the Radial Basis Function (RBF) as our default choice of kernel. However, another kernel may better fit a particular application domain. In this experiment, we compare three well-known kernels i.e. Linear, RBF, and Inverse Multi-Quadric (IMQ) on the three image classification datasets.\n\n\nFigure 7 presents the results under both data augmentation scenarios. Overall, the IMQ kernel performs better than the RBF kernel regarding explanation quality (Hit Rate). The advantage is significant when the data augmentation scenario is Horizontal Flip (Figure 7c, which appears more challenging than Noise Injection. IMQ also showed better performance on Coverage (Figure 7b). The linear kernel performs worse compared to other kernels. However, it is substantially efficient than the others, as shown in Figure 7d, highlighting its utility on large datasets. Compared to the baselines presented in Figure 12, we note that the Linear kernel is sufficient for HD-Explain to stand out from other methods in both performance and efficiency.\n\n\n# 4.4 DISCUSSION: INTUITION ON WHY HD-EXPLAIN WORKS\n\n\nAfter showing HD-Explain's empirical performance, we now present our understanding of how HD-Explain finds the explanations for the prediction of a test data point. In particular, we want to understand why the approach is faithful to the pre-trained model.\n\n\nIn HD-Explain, the key metric on measuring the predictive supports of a test point  $\\mathbf{x}_t$  given a training data  $(\\mathbf{x}_i,\\mathbf{y}_i)$  is the KSD defined kernel  $\\kappa_{\\theta}([\\mathbf{x}_t||\\hat{\\mathbf{y}}_t],[\\mathbf{x}_i||\\mathbf{y}_i])$ , where  $\\hat{\\mathbf{y}}_t$  denotes the predicted class label by model  $f_{\\theta}$  in one-hot encoding. By definition, the kernel  $\\kappa_{\\theta}((\\mathbf{x}_a,y_a),(\\mathbf{x}_b,y_b)) = k_{\\theta}(a,b)$  between two data points can be decomposed into four terms\n\n\n$$\n\n$$\n\\begin{array}{l} \\underbrace {\\operatorname {t r a c e} (\\nabla_ {a} \\nabla_ {b} k (a , b))} _ {\u2460} + \\underbrace {k (a , b) \\nabla_ {a} \\log P _ {\\theta} (a) ^ {\\top} \\nabla_ {b} \\log P _ {\\theta} (b)} _ {\u2461} \\\\ \\qquad + \\underbrace {\\nabla_ {a} k (a , b) ^ {\\top} \\nabla_ {b} \\log P _ {\\theta} (b)} _ {\u2462} + \\underbrace {\\nabla_ {b} k (a , b) ^ {\\top} \\nabla_ {a} \\log P _ {\\theta} (a)} _ {\u2463}. \\end{array}\n$$\n\n$$\n\n\nWe examine the effect of each term as follows:\n\n- - \u2460: We note that the first term is often a similarity bias of raw data points given a specified kernel function. In particular, for the RBF kernel  $k(a,b) = \\exp (-\\gamma ||a - b||^2)$ , the first term is simply  $\\sum_{i}^{d + l}2\\gamma k(a,b)$ , where  $d + l$  refers to the sum of input and output (in one-hot) dimensions of a data point. Intuitively, the term shows how similar the two data points are given the RBF kernel. For linear kernel  $k(a,b) = a^\\top b$ , on another hand, the first term is simply  $d + l$  as a constant bias term, which does not deliver any similarity information between the two data points.\n\n- - \u2461: The second term reflects the similarity between two data points in the context of the trained model. In particular, considering the sub-term  $\\nabla_{a}\\log P_{\\theta}(a)^{\\top}\\nabla_{b}\\log P_{\\theta}(b)$ , based on our derivation in Equation 2 (in the main paper), we note it is equivalent to\n\n\n$$\n\n$$\n\\begin{array}{l} \\left[ \\nabla_ {\\mathbf {x} _ {a}} \\log f _ {\\theta} (\\mathbf {x} _ {a}) _ {y _ {a}} | | \\log f _ {\\theta} (\\mathbf {x} _ {a}) \\right] ^ {\\top} [ \\nabla_ {\\mathbf {x} _ {b}} \\log f _ {\\theta} (\\mathbf {x} _ {b}) _ {y _ {b}} | | \\log f _ {\\theta} (\\mathbf {x} _ {b}) ] \\\\ = \\underbrace {\\nabla_ {\\mathbf {x} _ {a}} \\log f _ {\\theta} (\\mathbf {x} _ {a}) _ {y _ {a}} ^ {\\top} \\nabla_ {\\mathbf {x} _ {b}} \\log f _ {\\theta} (\\mathbf {x} _ {b}) _ {y _ {b}}} _ {\\text {s i m i l a r i t y o f s c o r e s (i n p u t g r a d i e n t s)}} + \\underbrace {\\log f _ {\\theta} (\\mathbf {x} _ {a}) ^ {\\top} \\log f _ {\\theta} (\\mathbf {x} _ {b})} _ {\\text {s i m i l a r i t y o f p r e d i c t i o n s}}, \\end{array}\n$$\n\n$$\n\n\nwhere both terms could be viewed as similarity between data points in the context of trained model.\n\n\n- \u2462-\u2463: Both of the last two terms examine the alignment between the score of one data point and the kernel derivative of another data point. We conjecture that this alignment reflects how a test prediction would change if there is a training data point present closer to it than before.\n\n\n# 5 CONCLUSION\n\n\nThis paper presents HD-Explain, a Kernel Stein Discrepancy-driven example-based prediction explanation method. We performed comprehensive qualitative and quantitative evaluation comparing three baseline explanation methods using three datasets. The results demonstrated the efficacy of HD-Explain in generating explanations that are accurate and effective in terms of their granularity level. In addition, compared to other methods, HD-Explain is flexible to apply on any layer of interest and can be used to analyze the evolution of a prediction across layers. HD-Explain serves as an important contribution towards improving the transparency of machine learning models.\n\n\n# REFERENCES\n\n- Ariful Islam Anik and Andrea Bunt. Data-centric explanations: explaining training data of machine learning systems to promote transparency. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-13, 2021.\n\n- Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger B Grosse. If influence functions are the answer, then what is the question? Advances in Neural Information Processing Systems, 35:17953-17967, 2022.\n\n- Alessandro Barp, Francois-Xavier Briol, Andrew Duncan, Mark Girolami, and Lester Mackey. Minimum stein discrepancy estimators. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Elnaz Barshan, Marc-Etienne Brunet, and Gintare Karolina Dziugaite. Relatif: Identifying explanatory training samples via relative influence. In International Conference on Artificial Intelligence and Statistics, pp. 1899-1909. PMLR, 2020.\n\n- Jonathan Brophy, Zayd Hammoudeh, and Daniel Lowd. Adapting and evaluating influence-estimation methods for gradient-boosted decision trees. Journal of Machine Learning Research, 24(154): 1-48, 2023.\n\n- Carrie J Cai, Jonas Jongejan, and Jess Holbrook. The effects of example-based explanations in a machine learning interface. In Proceedings of the 24th international conference on intelligent user interfaces, pp. 258-262, 2019.\n\n- Yuanyuan Chen, Boyang Li, Han Yu, Pengcheng Wu, and Chunyan Miao. Hydra: Hypergradient data relevance analysis for interpreting deep neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 7081-7089, 2021.\n\n- Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. In International conference on machine learning, pp. 2606-2615. PMLR, 2016.\n\n- Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. arXiv preprint arXiv:1711.09784, 2017.\n\n- Jackson Gorham, Anant Raj, and Lester Mackey. Stochasticstein discrepancies. Advances in Neural Information Processing Systems, 33:17931-17942, 2020.\n\n- Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Understanding predictions with data and data with predictions. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 9525-9587. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/ilyas22a.html.\n\n- Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\n- Wittawat Jitkrittum, Heishiro Kanagawa, and Bernhard Scholkopf. Testing goodness of fit of conditional density models with kernels. In Conference on Uncertainty in Artificial Intelligence, pp. 221-230. PMLR, 2020.\n\n- Sudheesh Kumar Kattumannil. On stein's identity and its applications. Statistics & probability letters, 79(12):1444-1449, 2009.\n\n- Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. Examples are not enough, learn to criticize! criticism for interpretability. Advances in neural information processing systems, 29, 2016.\n\n- Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International conference on machine learning, pp. 1885-1894. PMLR, 2017.\n\n- Anna Korba, Pierre-Cyril Aubin-Frankowski, Szymon Majewski, and Pierre Ablin. Kernel stein discrepancy descent. In International Conference on Machine Learning, pp. 5719-5730. PMLR, 2021.\n\n- Tatsuya Kubokawa. Stein's identities and the related topics: an instructive explanation on shrinkage, characterization, normal approximation and goodness-of-fit. *Japanese Journal of Statistics and Data Science*, pp. 1-45, 2024.\n\n- Xiao-Hui Li, Caleb Chen Cao, Yuhan Shi, Wei Bai, Han Gao, Luyu Qiu, Cong Wang, Yuanyuan Gao, Shenjia Zhang, Xun Xue, et al. A survey of data-driven and knowledge-aware explainable ai. IEEE Transactions on Knowledge and Data Engineering, 34(1):29-49, 2020.\n\n- Brian Y Lim, Qian Yang, Ashraf M Abdul, and Danding Wang. Why these explanations? selecting intelligibility types for explanation goals. In IUI Workshops, 2019.\n\n- Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. Advances in neural information processing systems, 29, 2016.\n\n- Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-fit tests. In International conference on machine learning, pp. 276-284. PMLR, 2016.\n\n- Christoph Molnar. Interpretable machine learning. Lulu. com, 2020.\n\n- Chang S Nam, Jae-Yoon Jung, and Sangwon Lee. Human-Centered Artificial Intelligence: Research and Applications. Academic Press, 2022.\n\n- Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale. In International Conference on Machine Learning (ICML), 2023a.\n\n- Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. TRAK: Attributing model behavior at scale. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 27074-27113. PMLR, 23-29 Jul 2023b. URL https://proceedings.mlr.press/v202/park23c.html.\n\n- Leif E Peterson. K-nearest neighbor. *Scholarpedia*, 4(2):1883, 2009.\n\n- Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence by tracing gradient descent. Advances in Neural Information Processing Systems, 33: 19920-19930, 2020.\n\n- Andrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8179-8186, 2022.\n\n- Yi Sui, Ga Wu, and Scott Sanner. Representer point selection via local jacobian expansion for post-hoc classifier explanation of deep neural networks and ensemble models. Advances in neural information processing systems, 34:23347-23358, 2021.\n\n- Che-Ping Tsai, Chih-Kuan Yeh, and Pradeep Ravikumar. Sample based explanations via generalized representatives. Advances in Neural Information Processing Systems., 36, 2023.\n\n- Jiasen Yang, Qiang Liu, Vinayak Rao, and Jennifer Neville. Goodness-of-fit testing for discrete distributions via stein discrepancy. In International Conference on Machine Learning, pp. 5561-5570. PMLR, 2018.\n\n- Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection for explaining deep neural networks. Advances in neural information processing systems, 31, 2018.\n\n- Jianlong Zhou, Amir H Gandomi, Fang Chen, and Andreas Holzinger. Evaluating the quality of machine learning explanations: A survey on methods and metrics. *Electronics*, 10(5):593, 2021.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638629.1844435", "msecs": "184.0", "relativeCreated": "226014.94097709656", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Pixab-CAM: Attend Pixel, not Channel\n\n# Abstract\nTo understand the internal behaviors of convolution neural networks (CNNs), many class activation mapping (CAM) based methods, which generate an explanation map by a linear combination of channels and corresponding weights, have been proposed. Previous CAM-based methods have tried to define a channel-wise weight that represents the importance of a channel for the target class. However, these methods have two common limitations. First, all pixels in the channel share a single scalar value. If the pixels are tied to a specific value, some of them are overestimated. Second, since the explanation map is the result of a linear combination of channels in the activation tensor, it is inevitably dependent on the activation tensor. To address these issues, we propose gradient-free Pixel-wise Ablation-CAM (Pixab-CAM), which utilizes pixel-wise weights rather than channel-wise weights to break the link between pixels in a channel. In addition, in order not to generate an explanation map dependent on the activation tensor, the explanation map is generated only with pixel-wise weights without linear combination with the activation tensor. In this paper, we also propose novel evaluation metrics to measure the quality of explanation maps using an adversarial attack. We demonstrate through experiments the qualitative and quantitative superiority of Pixab-CAM.\n\n\n# 1 INTRODUCTION\n\n\nAs the performance of convolution neural networks (CNNs) converges to some extent(Szegedy et al., 2015; Simonyan & Zisserman, 2014; Huang et al., 2017; Howard et al., 2017; Chollet, 2017), researchers have become interested in interpreting their predictions(Springenberg et al., 2014; Smilkov et al., 2017; Fong & Vedaldi, 2017; Bach et al., 2015; Lundberg & Lee, 2017; Ribeiro et al., 2016; Petsiuk et al., 2018; Shrikumar et al., 2017). One of them is class activation mapping (CAM) based approach(Zhou et al., 2016), which generates an explanation map by a linear combination of channels in the activation tensor and corresponding weights. The explanation map  $L_{ij}^{c}$  for the target class  $c$  is calculated as Eqn.1.  $A_{ij}^{k}$  represents activation of pixel at spatial location  $(i,j)$  for  $k$ -th channel of the activation tensor  $A$ , and  $w_{k}^{c}$  denotes the coefficient of the  $k$ -th channel for class  $c$ .\n\n\n$$\n\n$$\nL _ {i j} ^ {c} = \\sum_ {k} w _ {k} ^ {c} \\cdot A _ {i j} ^ {k} \\tag {1}\n$$\n\n$$\n\n\nThe activation tensor  $A$  is fixed since it is determined by the given input image and pretrained model(LeCun et al., 1998). Therefore,  $L^c$  depends on how  $w_k^c$  is defined. Thus, different  $w_k^c$  have been proposed in many CAM-based methods to generate high-quality explanation maps. The previous CAM-based methods can be divided into two categories, one is gradient-based CAMs(Selvaraju et al., 2017; Chattopadhyay et al., 2018; Fu et al., 2020), and the other is gradient-free CAMs(Wang et al., 2020; Ramaswamy et al., 2020). Gradient-based CAMs and gradient-free CAMs can be distinguished by whether or not gradients are used in the process of deriving weights.\n\n\n# 1.1 GRADIENT-BASED CAMS\n\n\nGrad-CAM(Selvaraju et al., 2017) obtains the weight of  $k$ -th channel by averaging their gradients. However, it has a problem in that the weight is crushed due to the offset of positive and negative gradients. ( $Z$  is the number of pixels in a channel and  $Y^{c}$  is the classification score for class  $c$ )\n\n\n$$\n\n$$\nw _ {k} ^ {c} = \\frac {1}{Z} \\sum_ {i} \\sum_ {j} \\frac {\\partial Y ^ {c}}{\\partial A _ {i j} ^ {k}} \\tag {2}\n$$\n\n$$\n\n\nGrad-CAM++(Chattopadhyay et al., 2018) introduced higher-order derivatives  $\\alpha_{ij}^{kc}$  in Grad-CAM equation to solve the problem that channels with fewer footprints fade away in the explanation map.\n\n\n$$\n\n$$\nw _ {k} ^ {c} = \\sum_ {i} \\sum_ {j} \\alpha_ {i j} ^ {k c} \\cdot R e L U \\left(\\frac {\\partial Y ^ {c}}{\\partial A _ {i j} ^ {k}}\\right) \\tag {3}\n$$\n\n$$\n\n\nXGrad-CAM(Fu et al., 2020) points out that previous CAM-based methods lack sufficient theoretical support and propose weight that satisfies two basic axioms: Sensitivity and Conservation.\n\n\n$$\n\n$$\nw _ {k} ^ {c} = \\sum_ {i} \\sum_ {j} \\left(\\frac {A _ {i j} ^ {k}}{\\sum_ {i} \\sum_ {j} A _ {i j} ^ {k}} \\frac {\\partial Y ^ {c}}{\\partial A _ {i j} ^ {k}}\\right) \\tag {4}\n$$\n\n$$\n\n\nHowever, Gradient-based CAMs suffer from gradient saturation problems, leading to localization failures for relevant regions.(Wang et al., 2020; Ramaswamy et al., 2020; Jung & Oh, 2021)\n\n\n# 1.2 GRADIENT-FREE CAMS\n\n\nScore-CAM(Wang et al., 2020) defines \"increase of confidence\" as the coefficient of the channel. For input  $X$  and baseline input  $X_{b}$ , the weight of the  $k$ -th channel for class  $c$  is defined as\n\n\n$$\n\n$$\nw _ {k} ^ {c} = f \\left(X \\circ s \\left(u \\left(A ^ {k}\\right)\\right)\\right) - f \\left(X _ {b}\\right) \\tag {5}\n$$\n\n$$\n\n\nwhere  $u(\\cdot)$  indicates the upsampling operation into the input size,  $s(\\cdot)$  is a function that normalizes all elements to [0, 1] and  $\\circ$  is the \"Hadamard product\".\n\n\nAblation-CAM(Ramaswamy et al., 2020), which inspired our research, defines the weight of a specific channel as the fraction of drop in activation score of target class when that channel is removed. In the equation below,  $Y_{k}^{c}$  is the score of class  $c$  when the  $k$ -th channel has been replaced by zero.\n\n\n$$\n\n$$\nw _ {k} ^ {c} = \\frac {Y ^ {c} - Y _ {k} ^ {c}}{Y ^ {c}} \\tag {6}\n$$\n\n$$\n\n\nGradient-free CAMs remove the dependence of gradients, but they are time-consuming because the prediction process must be repeated as many times as the number of channels to acquire the weights.\n\n\nRegardless of whether gradient is used or not, all CAM-based methods so far have two things in common. First, they all use channel-wise weights  $w_{k}^{c}$  (see Eqn.1). However, since the channel-wise weights allow all pixels in a specific channel to share a single weight, it is an unfair weighting method from the pixel perspective. (Although  $\\alpha_{ij}^{kc}$  in Grad-CAM++ (Eqn.3) modifies the gradients at the pixel level, but these gradients are eventually crushed into a single scalar value, so it doesn't solve the problem of pixels being tied together.) Furthermore, channel-wise weight-based methods tend to assign high weight if there are pixels that have a high contribution to the target class among the active pixels in the channel. Therefore, within a specific channel, other pixels that are active with the pixels related to the target class are also given a high weight. If channels containing these overestimated pixels are accumulated, overestimated pixels also occur in the explanation map.(see Sec.3.1) Second, when generating the explanation map  $L^{c}$ , all CAM-based methods follow Eqn.1, which contains the activation tensor  $A$ . Therefore, the explanation map has no choice but to depend on the activation tensor, and eventually may capture too much meaningless information since the channels in the activation tensor are not necessarily related to the target category(Zhang et al., 2021).(see Sec.3.2)\n\n\n# 2 PROPOSED EVALUATION METRICS OF THE EXPLANATION MAP\n\n\nSince it is hard to know where CNN looks and makes a specific prediction, the existing evaluation metrics of the explanation map have limitations. AD & IIC(Chattopadhy et al., 2018) and Deletion & Insertion(Petsiuk et al., 2018) are coarse proxy evaluations because removing pixels by zero-ing/graying is unnatural in real photos(Dabkowski & Gal, 2017). Also, in Localization Evaluation(Wang et al., 2020), there may not be a correlation between localization ability and the quality of the explanation map since the human-annotated bounding box may not match where CNN is actually focusing. However, if we could know where CNN is looking, we could make a more accurate assessment. We use an adversarial attack for this.(we use FGSM attack(Goodfellow et al., 2014).) If a targeted adversarial attack is made on a specific patch in the zero image, it is self-evident that CNN sees that patch and determines that it is a target, so the performance can be evaluated by whether the explanation map correctly captures that part. We propose 2 evaluation tasks (DAP1 and DAP2) from this idea. Before introducing them, we will explain 2 adversarial examples (AE1 and AE2) first.\n\n\n# 2.1 ADVERSARIAL EXAMPLE 1 (AE1)\n\n\n![(a) Zero Image](images/0547c4b6d71475e684ba360c9057bf44787776b0dee47b625c401f3d4c5367df.jpg)\n\n\n![(b) Adversarial Patch](images/34a56fbfedb3c4b9991f04d0c61ea56233537ae75579b678bb0ac765a04763ab.jpg)\n\n\n![(c) Input Image Figure 1: Adversarial patch in (b) was attacked to become a \"cucumber\".](images/9a7331d6091839304faeec8abe46dee286a15a2cf92c4c49952a596f20537928.jpg)\n\n\n![(d) Zero-ing Patch](images/14649134b4ef1119e9045e1c73419ffbbb385563b5a0972cfb9b3c25b6281eba.jpg)\n\n\n![(e) Adversarial Example](images/c63587c9b49760a6b673c20dadaa6363fa425de921dfb481f4401d08b2605e82.jpg)\n\n\nIt is simple to attach an adversarial patch classified as a given target class to an input image. First, we apply a targeted adversarial attack so that the patch at a specific location in the zero image is classified into the class we want. Since the attack is applied only to the patch, the rest of the area is still zero as in (b). Then, the input image with only the patch area of zero((d)) is added to the image (b). The finally obtained adversarial example AE1((e)) is used for our evaluation task DAP1.\n\n\n# 2.2 ADVERSARIAL EXAMPLE 2 (AE2)\n\n\n![(a) Adversarial Patch 1 Figure 2: (a): \"chihuahua\", (b): \"tusker\", (c): \"soccer ball\", (d): \"broccoli\"](images/f2a28c53596ea16de2afb64c3b9480e5d5bb35aa719fa5547c36c8906e00928c.jpg)\n\n\n![(b) Adversarial Patch 2](images/642343fe98276e9b444fd5628e09b117ff9420b4aa47f623a3fd62cec9b96e52.jpg)\n\n\n![(c) Adversarial Patch 3](images/ff17ede68541d2ad9a5475f96e0b226d05c938cd6abb5b7026687ea13feb162c.jpg)\n\n\n![(d) Adversarial Patch 4](images/1da15e2405c417e05a22ab7458c416c9ac0cec6dac44c513e04f0e39912eccb5.jpg)\n\n\n![(e) Adversarial Example](images/52e5849cb2139511002d1151d5f31b342fd142409fead83d77c5bd864af8737e.jpg)\n\n\nFor generating a second adversarial example AE2, only adversarial patches are used without input images. This can also be easily created. After generating four adversarial patches in the same way as in Sec.2.1 (b), combine them to make the final adversarial example AE2. (Each adversarial patch is attacked to become a different class.) AE2 created in this way is used for evaluation task DAP2.\n\n\n# 2.3 DETECTING ADVERSARIAL PATCH\n\n\n![Explanation Map  $^{AE1}$ Figure 3: Columns 1-3 and 4-6 correspond to the tasks \"Detecting Adversarial Patch 1 (DAP1)\" and \"Detecting Adversarial Patch 2 (DAP2)\". Also, each is an explanation of \"cucumber\" and \"tusker\".](images/5446c927929562c3b97e8ca2ed1add362c6afb3cbc92763f72bf113b2c4a0f8e.jpg)\n\n\n![Pointing Game  $^{AE1}$](images/824c7eeca3c53012f6feb03325c87ed1b2bf36627d0bde584addcafb5af08729.jpg)\n\n\n![Object Localization  $^{AE1}$](images/5daaf8ca105155ec514decb4d3640db30e92af7fe9a0f18f7faa3859c1e4f72a.jpg)\n\n\n![Explanation Map  $^{AE2}$](images/efa5d92f88fd86f70e1e715268823d8a5c9c5594269c7a5634414cf18db1c20e.jpg)\n\n\n![Pointing Game  $^{AE2}$  Object Localization  $^{AE2}$](images/789888ff9023bbd900f8400b1834ba0e460274d521ccb62c43af0863c09370fe.jpg)\n\n\n![](images/2f317a590398c539a2a3911200fc38a6a56d64d04f531dfe8e0b8b0e45890d62.jpg)\n\n\nIn this paper, \"Pointing Game\" and \"Object Localization\" are used as metrics in \"Detecting Adversarial Patch 1 (DAP1)\" and \"Detecting Adversarial Patch 2 (DAP2)\" tasks.\n\n\nPointing Game (PG) (Petsiuk et al., 2018): The Pointing Game (PG) is a metric that measures whether the maximum point of the explanation map is inside the human-annotated object bounding box. As in columns 2 and 5 of Figure 3, if the point lies inside the target bounding box, it is counted as a hit. Pointing Game accuracy is calculated as  $\\frac{\\#Hits}{\\#Hits + \\#Misses}$ .\n\n\nObject Localization (OL) (Wang et al., 2020): Object Localization (OL) is a metric that measures how much the energy of the explanation map falls into the target bounding box instead of using only the maximum point. This metric can be formulated as  $\\frac{\\sum L_{ij\\in bbox}^c}{\\sum L_{ij\\in bbox}^c + \\sum L_{ij\\notin bbox}^c}$ .\n\n\nHowever, in both DAP1 and DAP2, the ground truth bounding box of the target object is the patch, not the human-annotated bounding box. So, the ground truth bounding box in Sec.5.2.2 is the human-annotated bounding box, and the ground truth bounding box in Sec.5.2.3 is the patch.\n\n\n# 3 MOTIVATION\n\n\n# 3.1 CHANNELS WITH OVERESTIMATED PIXELS (COP)\n\n\n![(a) Input Image](images/5a4b9ce0b0acbcc8393e6ab13e5caf5a5759097b91e1c0dd1413871e5f908649.jpg)\n\n\n![(b) Target Only Figure 4: Target: \"chihuahua\", non-Target: \"tusker\", \"soccer ball\", \"broccoli\"](images/9affd75dfb6b0a9a21ce176d96cee64a5c71bf55cae4cf11b5a504642f8e9397.jpg)\n\n\n![(c) Target & non-Target](images/d31211c0f6aa2bad99e892e87e4f26bef8e284d39a7efdcec5065f0b032f2564.jpg)\n\n\n![(d) non-Target Only](images/409962fbaa30a2e416af83cf261d326801ec3703ad28211f959c87c727ff9306.jpg)\n\n\n<table><tr><td>Name</td><td>#COP</td></tr><tr><td>Grad-CAM</td><td>56</td></tr><tr><td>Grad-CAM++</td><td>39</td></tr><tr><td>Score-CAM</td><td>26</td></tr><tr><td>Ablation-CAM</td><td>53</td></tr><tr><td>XGrad-CAM</td><td>37</td></tr></table>\n\n\nTable 1: # COP\n\n\nIn Figure 4, (b)-(d) indicate which part of the input image the specific channels of the activation tensor focus on (Masking input image with upsampled channels.). Channel (b) is focusing only on the target part, channel (c) is focusing on both the target and non-target part and channel (d) is focusing only on the non-target part. We investigated the weights for channels (b)-(d), and it was confirmed that Grad-CAM, Grad-CAM++, Score-CAM, Ablation-CAM, and XGrad-CAM all gave high weights to channels (b) and (c) and low weights to channel (d). In other words, regardless of whether a specific channel contains a non-target part or not, as long as it contains a target part, that channel tends to be given a high weight. However, since channel-wise weight-based methods assign one weight per channel, all pixels in the channel share a single weight. As a result, for a channel like (c), pixels in the non-target part are also given a high weight. As shown in Appendix A, these channels are commonly found in the activation tensor. Since the explanation map of previous methods is generated by a linear combination of channels, if the number of channels with overestimated pixels(COP) increases, the overestimated pixels also appear in the explanation map. These overestimated pixels suppress other pixels, so the explanation map may highlight the wrong places or only part of the target. Table 1 shows the average number of channels focusing on both the target and non-target part among the channels with the top 100 weights(Inception V3 has 2048 channels). (If the channel contains more than  $80\\%$  of the patch, the channel is considered to be focusing on that patch.) The values in Table 1 are averaged for 500 randomly generated AE2s. Among the five methods, Score-CAM best distinguishes the channel focusing only on the target part and the channel focusing on both the target and non-target part. However, this is also not a satisfactory value.\n\n\n# 3.2 DEPENDENCY ON ACTIVATION TENSOR (DOA)\n\n\nReason for removing the dependency on activation tensor: To check the dependence between the explanation map and the activation tensor, we compare Pixab-CAM, which will be covered in Sec.4, with the previous methods for the \"Detecting Adversarial Patch 1 (DAP1)\" task presented in Sec.2. In addition, in order to check where the activation values are intensively distributed in the activation tensor, we defined Activation-CAM, which is expressed as  $\\sum_{k} 1 \\cdot A_{ij}^{k}$ . Since the channel-wise weights are fixed at 1, Activation-CAM can indicate where the activation tensor focuses.\n\n\nIf there is only an adversarial patch in the image, like the first row in Figure 5, all methods detect this patch well. However, if features are evenly distributed in the image, such as when a natural image and an adversarial patch are together (the result of Activation-CAM in the second row of Figure 5), the channel-wise weight-based methods cannot capture the patch properly even if the same patch is in the same location. Since the explanation map is generated from a linear weighted combination of channels in the activation tensor, it is difficult to properly focus on target if the target is located where the activation tensor is sparsely distributed (= the number of channels capturing the target is small), such as the result of Activation-CAM in the second row of the Figure 5 (In this case, the target is a patch.). This situation is common in the real world since channels in the activation tensor are not necessarily related to the target class(Zhang et al., 2021). On the other hand,\n\n\n![Input Image Activation-CAM Grad-CAM Grad-CAM++ Score-CAM Ablation-CAM XGrad-CAM Pixab-CAM Figure 5: Both the first row and the second row are explanations of \"keypad\".](images/118b443b8eb0c1ef7f4228984d9a891aac6e53bbadda14f1eccb6c3aa41fce7a.jpg)\n\n\nPixab-CAM can handle any situation flexibly regardless of how the activation tensor is distributed on the image because the activation tensor does not participate in the process of generating the explanation map (see Sec.4). Another finding is that the explanation maps generated by channel-wise weight-based methods except Grad-CAM and Ablation-CAM do not deviate much from the result of Activation-CAM, as shown in Figures 5, 6, and 8. In particular, the explanation map of Score-CAM is very similar to the results of Activation-CAM. From this, it can be inferred that the reason that all the channel-wise weight-based methods in the first row of Figure 5 produced the correct explanation map is that the features are distributed only in the adversarial patch, as shown in the result of Activation-CAM. In other words, since there were no features that prevented the detection of the target in the first place, all of them were able to provide a proper explanation.\n\n\nHow to measure dependence with activation tensor: To quantitatively check how dependent the explanation maps of CAM-based methods are on the activation tensor, we measured the cosine distance between Activation-CAM and CAM-based methods. However, for natural images like ImageNet, this cosine distance may not give reliable results as the activation tensor may be properly distributed on the target. As in the first row of Figure 5, if the activation tensor is distributed only on the target, it is impossible to distinguish a correct explanation map from an explanation map that depends on the activation tensor. To solve this, we use \"Adversarial Example 2 (AE2)\" introduced in Sec.2.2. We calculate the cosine distance after making the activation tensor evenly distributed over the four patches, such as the result of Activation-CAM in Figure 6. This approach ensures that the activation tensor is not distributed only to the target, so the dependency can be computed more accurately.(Generating an explanation map similar to Activation-CAM (= Low cosine distance) means it is not capturing the target properly.) We measured the cosine distances for 500 AE2s.\n\n\n![Input Image Activation-CAM Grad-CAM Grad-CAM++ Score-CAM Ablation-CAM XGrad-CAM Pixab-CAM Figure 6: 1st row: \"chihuahua\", 2nd row: \"tusker\", 3rd row: \"soccer ball\", 4th row: \"broccoli\".](images/a8e711d1b31271faae5fb60ec0d0a6ca4b3a3a1d164278e8044d4f14a90ad89b.jpg)\n\n\nThe result for \"Dependency on Activation Tensor (DOA)\" is in Table 2. Grad-CAM++, Score-CAM, and XGrad-CAM have very low DOA values. In particular, Score-CAM has a value close to 0. In\n\n\nother words, the explanation map they generate is highly dependent on the activation tensor. We also observed that the less the activation tensor is distributed in the target, the more they depend on the activation tensor. This result implies that in the case of a very weakly activated feature on the image, the explanation maps of these methods may not be able to capture that part even though it is a target. The explanation maps of Grad-CAM and Ablation-CAM do not depend on the activation tensor, but they focus on the wrong place or only part of the target, as shown in Figures 5, 6, and 8.\n\n\n# 4 PROPOSED METHODOLOGY\n\n\n![Figure 7: An overview of Pixab-CAM.](images/00de87240f72dd92d5b04179d9e5b3174b292419adad56889e0293b6b8156f9a.jpg)\n\n\nPixab-CAM has three main features. The first is that it doesn't use gradients to avoid problems with gradient saturation that negatively affect visualization quality (Wang et al., 2020; Ramaswamy et al., 2020). Second, we define a pixel-wise weight rather than a channel-wise weight to solve the problem that pixels within a specific channel are tied to a single scalar value. (First motivation in Sec.3.1.) Finally, in order to completely remove the dependency on the activation tensor, we do not include the activation tensor in generating the explanation map. (Second motivation in Sec.3.2.)\n\n\nAs in Eqn.7, we define the weight by using the idea of Ablation-CAM in terms of pixels. Although Ablation-CAM uses only the channel contribution from the deletion(=ablation) perspective when deriving the channel coefficient(Ramaswamy et al., 2020), we define pixel coefficient using both the pixel contribution from the deletion(first term) and preservation(second term) perspectives. If a specific pixel is associated with a target class, the class score will decrease when it is removed(first term) and increase when it appears(second term). These two terms are complementary to each other because they reflect the contribution of pixels from different perspectives.(It is a strict weight assignment method because a high coefficient can be obtained only by showing a high contribution from both perspectives.) As shown in Figure 7, Eqn.7 can be expressed as a tensor composed of  $0s$  (first term) and a tensor composed of  $1s$  (second term), each of size  $1 \\times 1 \\times$  channel_size, moving on the activation tensor in a sliding window manner. We'll call this tensor an ablation box, and use four ablation boxes-  $1 \\times 1$ ,  $1 \\times 2$ ,  $2 \\times 1$ ,  $2 \\times 2$ . (An evaluation of each design choice and why these four ablation boxes were used are covered in Appendix B.) The reason for using multiple scale boxes is to detect objects of various sizes and shapes as in Faster R-CNN(Ren et al., 2015). For reference, even if all boxes are used, the prediction process is only 1/4 of Ablation-CAM. Also, regardless of the size of the box, stride size is 1, and padding is used in all except the  $1 \\times 1$  box to reflect the pixel contribution fairly. Then, we average over all boxes of each perspective. Finally, the explanation map is obtained after applying ReLU and the normalization function as in Eqn.8. What should be noted here is that the activation tensor is not used in the process of generating the explanation map.\n\n\n$$\n\n$$\np _ {i j} ^ {c} = \\left(\\frac {Y ^ {c} - Y _ {i j} ^ {c}}{Y ^ {c}}\\right) \\cdot \\left(\\frac {\\widetilde {Y} _ {i j} ^ {c} - \\widetilde {Y} ^ {c}}{\\widetilde {Y} ^ {c}}\\right) \\tag {7}\n$$\n\n$$\n\n\n$$\n\n$$\nL _ {i j} ^ {c} = s \\left(R e L U \\left(p _ {i j} ^ {c}\\right)\\right) \\tag {8}\n$$\n\n$$\n\n\nwhere  $Y_{ij}^{c}$  is the score of class  $c$  when the pixel at spatial location  $(i,j)$  has been replaced by zero, conversely,  $\\widetilde{Y}_{ij}^{c}$  is the score of class  $c$  when the pixel at spatial location  $(i,j)$  is the only remaining pixel and  $\\widetilde{Y}^{c}$  is the score of class  $c$  when a tensor with all values of 0 is input to the model.\n\n\n# 5 EXPERIMENTS\n\n\nIn this section, we mainly compare Pixab-CAM with state-of-the-art CAM-based methods. Qualitative (see Sec.5.1) and quantitative evaluations (see Sec.5.2) are performed to evaluate the quality of the explanation map. In addition, we use Inception V3 network(Szegedy et al., 2016) pretrained on the ImageNet provided by TensorFlow as a black box model.\n\n\n# 5.1 QUALITATIVE EVALUATION (MORE EXAMPLES ARE PROVIDED IN APPENDIX C)\n\n\n![Input Image Activation-CAM Grad-CAM Grad-CAM++ Score-CAM Ablation-CAM XGrad-CAM Pixab-CAM Figure 8: Explanation Map Comparison. (Rows 1-2: Natural Image, Rows 3-4: AE1)](images/b395fe373047b3877dca1ad3073fdaf803426c85862f317959214f993e1a5255.jpg)\n\n\n![Input Image Guided-BP Grad-CAM Grad-CAM++ Score-CAM Ablation-CAM XGrad-CAM Pixab-CAM Figure 9: Guided CAM-based Explanation Comparison (Springenberg et al., 2014)](images/54ed4d761f3bc3d62ca5ef7065f84cfb2f61803dc57e653a11cc0d585292abfb.jpg)\n\n\nAs shown in Figures 8, 9, and 10, we qualitatively compare the explanation maps, guided CAM-based explanations, and weekly supervised localization results generated by Activation-CAM, five state-of-the-art CAM-based methods, and Pixab-CAM. Compared to previous CAM-based methods, Pixab-CAM produces higher quality explanation maps in all examples, whether or not the activation tensor is focusing on the target. On the other hand, the explanation maps of previous CAM-based methods tend not to focus properly on the target. First, Grad-CAM and Ablation-CAM tend to generate insufficient explanation maps, and even Ablation-CAM captures places that are not related to the target class at all. Second, if a part not related to the target class is emphasized in the result of Activation-CAM, that part is also highlighted in the explanation maps of Grad-CAM++, Score-CAM, and XGrad-CAM, which means that they are greatly affected by the activation tensor.\n\n\n# 5.2 QUANTITATIVE EVALUATION\n\n\nWe scraped 2,000 images with two or more objects with different labels (MOD) to evaluate the performance of the explanation map in situations where fine-grained weight adjustment is required. MOD images consist of objects with different labels but similar textures, such as cats and dogs,\n\n\n![Figure 10: Weakly Supervised Localization Comparison (Oquab et al., 2015)](images/d131efcba37813c40177e8174cbd8493af5e83db32d7f44c49e560f913cf795e.jpg)\n\n\nlions and tigers, trucks and cars, so the explanation map cannot properly capture the target unless the activation tensor is precisely adjusted by weight. In addition, ILSVRC 2012 validation set (IMGN)(Russakovsky et al., 2015), PASCAL VOC 2007 test set (VOC)(Everingham et al., 2010), and COCO 2014 validation set (COCO)(Lin et al., 2014) were used for evaluation.\n\n\n# 5.2.1 FAITHFULNESS EVALUATION\n\n\nTable 2: DOA\n\n\n<table><tr><td>Name</td><td>DOA</td></tr><tr><td>Grad-CAM</td><td>0.391</td></tr><tr><td>Grad-CAM++</td><td>0.028</td></tr><tr><td>Score-CAM</td><td>0.014</td></tr><tr><td>Ablation-CAM</td><td>0.223</td></tr><tr><td>XGrad-CAM</td><td>0.031</td></tr><tr><td>Pixab-CAM</td><td>0.402</td></tr></table>\n\n\nTable 3: AD, IIC, and Compactness\n\n\n<table><tr><td rowspan=\"2\">Name</td><td colspan=\"4\">AD (%)</td><td colspan=\"4\">IIC (%)</td><td colspan=\"4\">Compactness</td></tr><tr><td>IMGN</td><td>VOC</td><td>COCO</td><td>MOD</td><td>IMGN</td><td>VOC</td><td>COCO</td><td>MOD</td><td>IMGN</td><td>VOC</td><td>COCO</td><td>MOD</td></tr><tr><td>Grad-CAM</td><td>34.8</td><td>24.1</td><td>24.5</td><td>20.0</td><td>22.0</td><td>31.3</td><td>33.1</td><td>37.6</td><td>14.7</td><td>13.7</td><td>14.2</td><td>11.3</td></tr><tr><td>Grad-CAM++</td><td>33.7</td><td>22.4</td><td>22.8</td><td>21.3</td><td>22.1</td><td>33.3</td><td>31.0</td><td>35.2</td><td>18.4</td><td>19.8</td><td>20.4</td><td>16.9</td></tr><tr><td>Score-CAM</td><td>33.2</td><td>22.8</td><td>23.6</td><td>22.8</td><td>22.2</td><td>32.5</td><td>30.8</td><td>35.0</td><td>19.4</td><td>21.1</td><td>21.7</td><td>19.4</td></tr><tr><td>Ablation-CAM</td><td>32.7</td><td>21.2</td><td>22.4</td><td>20.1</td><td>24.5</td><td>33.3</td><td>32.9</td><td>37.6</td><td>16.3</td><td>17.9</td><td>18.3</td><td>14.5</td></tr><tr><td>XGrad-CAM</td><td>33.7</td><td>22.1</td><td>23.2</td><td>20.3</td><td>23.8</td><td>32.7</td><td>33.0</td><td>36.8</td><td>17.8</td><td>18.7</td><td>19.4</td><td>16.2</td></tr><tr><td>Pixab-CAM</td><td>30.2</td><td>19.4</td><td>19.1</td><td>18.9</td><td>24.2</td><td>36.2</td><td>35.9</td><td>39.7</td><td>15.4</td><td>16.1</td><td>16.6</td><td>12.6</td></tr></table>\n\n\nAD, IIC, and Compactness: Average Drop (AD)(Chattopadhy et al., 2018) is a metric that measures how much the explanation image, which is generated by point-wise multiplication of the upsampled explanation maps with the original image, maintains the part that plays essential roles in correct decision-making. Increase in Confidence (IIC)(Chattopadhy et al., 2018) measures whether the part that confuses correct decisions in the image has been sufficiently removed. Compactness is the metric proposed for the first time in this paper to measure how large the area the explanation map occupies in an image. An ideal explanation map should be of a suitable size to cover the target compactly. Compactness is expressed as  $\\frac{1}{N}\\sum_{i=1}^{N}(\\sum_{a}\\sum_{b}L_{i,(a,b)}^{c})$ , where  $L_{i,(a,b)}^{c}$  is the explanation map value at spatial location  $(a,b)$  for the  $i$ -th input  $X_{i}$  and  $N$  is the number of evaluation images. Results for 1,000 images randomly sampled for each dataset are in Table 3. From the results of IIC and AD, except for IIC in IMGN, Pixab-CAM outperforms other CAM-based methods by large scale on all metrics. This result reveals that Pixab-CAM can successfully leave only the essential parts for the correct decision while sufficiently removing the parts that interfere with decision making. In addition, Pixab-CAM has the second smallest Compactness value after Grad-CAM. It means that Grad-CAM generates an explanation map with a size less than necessary, and other previous CAM-based methods generate an explanation map with excessive size. Meanwhile, Pixab-CAM generates an explanation map of an appropriate size enough to cover the target compactly.\n\n\nDeletion and Insertion: The deletion metric(Petsiuk et al., 2018) measures the decrease in the probability of the predicted class as more and more important pixels are removed. The insertion metric(Petsiuk et al., 2018), on the other hand, measures the increase in probability as more and more pixels are introduced. Table 4 reports the average result of 1,000 images randomly selected from ImageNet. Pixab-CAM exhibited the highest insertion AUC and lowest deletion AUC. This experiment shows that Pixab-CAM successfully assigned coefficients in order of importance.\n\n\n# 5.2.2 OBJECT LOCALIZATION EVALUATION\n\n\nIn this section, we compare the localization ability of the generated explanation maps. The \"Object Localization (OL)\" metric was discussed in Sec.2.3 and is therefore omitted from this section. The values in Table 5 are averaged for 1,000 images randomly selected from ImageNet. Pixab-CAM has the highest proportion compared to other CAM-based methods. This means that the explanation maps generated by Pixab-CAM compactly focus the essential parts of the image.\n\n\n# 5.2.3 \u201cDETECTING ADVERSARIAL PATCH 1\u201d & \u201cDETECTING ADVERSARIAL PATCH 2\u201d\n\n\nWe also evaluate the performance of explanation maps for tasks \"Detecting Adversarial Patch 1 (DAP1)\" and \"Detecting Adversarial Patch 2 (DAP2)\", which are proposed for the first time in this paper (see Section 2). The number of patches in the image is randomly selected from 1-9, and the width and height of the patches are also randomly selected from  $15\\% - 20\\%$  of the input image size. DAP1 and DAP2 were tested on 500 AE1s and 500 AE2s, respectively, and the results are in Table 6. We observe that Pixab-CAM outperforms previous CAM-based methods in both DAP1 and DAP2.\n\n\nTable 4: Deletion & Insertion\n\n\n<table><tr><td>Name</td><td>Deletion</td><td>Insertion</td></tr><tr><td>Grad-CAM</td><td>0.090</td><td>0.433</td></tr><tr><td>Grad-CAM++</td><td>0.094</td><td>0.428</td></tr><tr><td>Score-CAM</td><td>0.101</td><td>0.425</td></tr><tr><td>Ablation-CAM</td><td>0.088</td><td>0.457</td></tr><tr><td>XGrad-CAM</td><td>0.093</td><td>0.434</td></tr><tr><td>Pixab-CAM</td><td>0.082</td><td>0.469</td></tr></table>\n\n\nTable 5: Object Localization\n\n\n<table><tr><td>Name</td><td>Proportion (%)</td></tr><tr><td>Grad-CAM</td><td>46.39</td></tr><tr><td>Grad-CAM++</td><td>48.17</td></tr><tr><td>Score-CAM</td><td>50.77</td></tr><tr><td>Ablation-CAM</td><td>51.32</td></tr><tr><td>XGrad-CAM</td><td>47.19</td></tr><tr><td>Pixab-CAM</td><td>53.22</td></tr></table>\n\n\nTable 6: DAP1 & DAP2\n\n\n<table><tr><td rowspan=\"2\">Name</td><td colspan=\"2\">DAP1</td><td colspan=\"2\">DAP2</td></tr><tr><td>PG</td><td>OL</td><td>PG</td><td>OL</td></tr><tr><td>Grad-CAM</td><td>46.5</td><td>42.7</td><td>45.3</td><td>43.1</td></tr><tr><td>Grad-CAM++</td><td>44.3</td><td>40.7</td><td>43.8</td><td>40.2</td></tr><tr><td>Score-CAM</td><td>43.2</td><td>40.2</td><td>42.9</td><td>39.5</td></tr><tr><td>Ablation-CAM</td><td>46.8</td><td>43.1</td><td>46.7</td><td>44.9</td></tr><tr><td>XGrad-CAM</td><td>44.0</td><td>41.9</td><td>43.1</td><td>40.6</td></tr><tr><td>Pixab-CAM</td><td>55.3</td><td>50.1</td><td>54.9</td><td>51.3</td></tr></table>\n\n\n# 5.3 SANITY CHECK (ADEBAYO ET AL., 2018)\n\n\nModel parameter randomization test was proposed to compare the explanation map on a trained model to the explanation map on a randomly initialized, untrained network of the same architecture. As in Figure 11, Pixab-CAM is sensitive to model parameters and can judge the quality of the model.\n\n\n![Figure 11: Cascading randomization from top to bottom layers on Inception v3 trained on ImageNet](images/469d837d051ab5454a5918fdc2814a56c822722304e16310173bc873d7ab9610.jpg)\n\n\n# 5.4 APPLICATIONS\n\n\nTo prove the applicability of Pixab-CAM in downstream tasks, we applied it to the Brain Tumor Detection task. Since the Brain Tumor dataset is small (N: 253), we used \"Transfer Learning\"(Pan & Yang, 2009). We downloaded VGG-16(Simonyan & Zisserman, 2014) pretrained on the ImageNet dataset, then trained only the final fully connected layer while keeping the weights of all ConvNets fixed. Figure 12 shows the explanation map on a trained VGG-16 for brain tumor images. Pixab-CAM detects tumor better than other CAM-based methods. This experiment demonstrates that Pixab-CAM can also be applied to fine-tuned models using small datasets.\n\n\n![Figure 12: Explanation Map of Brain Tumor images](images/4837cb96391c9039923cdce85b73937830b6acdc92cc4f2c065c9425dc1d28aa.jpg)\n\n\n# 6 CONCLUSION\n\n\nWe proposed Pixab-CAM, a novel CAM-based approach with three characteristics. First, we use pixel-wise weights rather than channel-wise weights so that the importance of the pixels is not distorted. Second, we exclude the activation tensor to create an explanation map independent of the activation tensor. Third, to avoid problems with gradient-based CAMs, gradients are not used when obtaining pixel-wise weights. In addition, we propose a new evaluation metric using adversarial attack and demonstrate the superiority of Pixab-CAM through qualitative and quantitative evaluation.\n\n\n# REFERENCES\n\n- Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. arXiv preprint arXiv:1810.03292, 2018.\n\n- Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. *PloS one*, 10(7):e0130140, 2015.\n\n- Aditya Chattopadhy, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Gradcam++: Generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 839-847. IEEE, 2018.\n\n- Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1251-1258, 2017.\n\n- Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. arXiv preprint arXiv:1705.07857, 2017.\n\n- Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The Pascal visual object classes (voc) challenge. International journal of computer vision, 88(2): 303-338, 2010.\n\n- Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3429-3437, 2017.\n\n- Ruigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, and Biao Li. Axiom-based grad-cam: Towards accurate visualization and explanation of cnns. arXiv preprint arXiv:2008.02312, 2020.\n\n- Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\n\n- Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n\n- Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708, 2017.\n\n- Hyungsik Jung and Youngrock Oh. Lift-cam: Towards better explanations for class activation mapping. arXiv preprint arXiv:2102.05228, 2021.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\n- Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr D\u00f3lar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740-755. Springer, 2014.\n\n- Scott Lundberg and Su-In Lee. A unified approach to interpreting model predictions. arXiv preprint arXiv:1705.07874, 2017.\n\n- Maxime Aquab, L\u00e9on Bottou, Ivan Laptev, and Josef Sivic. Is object localization for free?-weakly-supervised learning with convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 685-694, 2015.\n\n- Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345-1359, 2009.\n\n- Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of black-box models. arXiv preprint arXiv:1806.07421, 2018.\n\n- Harish Guruprasad Ramaswamy et al. Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 983-991, 2020.\n\n- Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015.\n\n- Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135-1144, 2016.\n\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211-252, 2015.\n\n- Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626, 2017.\n\n- Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In International Conference on Machine Learning, pp. 3145-3153. PMLR, 2017.\n\n- Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\n- Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda V\u00e9gas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.\n\n- Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.\n\n- Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.\n\n- Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826, 2016.\n\n- Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu. Score-cam: Score-weighted visual explanations for convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 24-25, 2020.\n\n- Qinglong Zhang, Lu Rao, and Yubin Yang. A novel visual interpretability for deep neural networks by optimizing activation maps with perturbation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3377-3384, 2021.\n\n- Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2921-2929, 2016.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638630.6924758", "msecs": "692.0", "relativeCreated": "227522.9732990265", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# A Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables\n\n# Abstract\nMost existing causal discovery methods rely on the assumption of no latent confounders, limiting their applicability in solving real-life problems. In this paper, we introduce a novel, versatile framework for causal discovery that accommodates the presence of causally-related hidden variables almost everywhere in the causal network (for instance, they can be effects of measured variables), based on rank information of covariance matrix over measured variables. We start by investigating the efficacy of rank in comparison to conditional independence and, theoretically, establish necessary and sufficient conditions for the identifiability of certain latent structural patterns. Furthermore, we develop a Rank-based Latent Causal Discovery algorithm, RLCD, that can efficiently locate hidden variables, determine their cardinalities, and discover the entire causal structure over both measured and hidden ones. We also show that, under certain graphical conditions, RLCD correctly identifies the Markov Equivalence Class of the whole latent causal graph asymptotically. Experimental results on both synthetic and real-world personality data sets demonstrate the efficacy of the proposed approach in finite-sample cases. Our code will be publicly available.\n\n\n# 1 INTRODUCTION AND RELATED WORK\n\n\nCausal discovery aims at finding causal relationships from observational data and has received successful applications in many fields (Spirtes et al., 2000; 2010; Pearl, 2019). However, traditional methods, such as PC (Spirtes et al., 2000), GES (Chickering, 2002b), and LiNGAM (Shimizu et al., 2006b), generally assume that there are no latent confounders in the graph, which hardly holds in many real-world scenarios. Therefore, extensive efforts have been dedicated to addressing this issue for causal structure learning.\n\n\nOne line of research focuses on inferring the causal structure among the observed variables, despite the possible existence of latent confounders. Notable approaches include FCI and its variants (Spirtes et al., 2000; Pearl, 2000; Colombo et al., 2012; Akbari et al., 2021) that leverage conditional independence tests, and over-complete ICA-based techniques (Hoyer et al., 2008; Salehkaleybar et al., 2020) that further leverage non-Gaussianity.\n\n\nAnother line of thought focuses more on uncovering the causal structure among latent variables, by assuming observed variables are not directly adjacent. This includes Tetrad condition-based (Silva et al., 2006; Kummerfeld & Ramsey, 2016), high-order moments-based (Shimizu et al., 2009; Cai et al., 2019; Xie et al., 2020; Adams et al., 2021; Chen et al., 2022), matrix decomposition-based (Anandkumar et al., 2013), and mixture oracles-based (Kivva et al., 2021) approaches. Recently, Huang et al. (2022) propose an approach that makes use of rank constraints to identify general latent hierarchical structures, and yet observed variables can only be leaf nodes. Although Chandrasekaran et al. (2012) allow direct causal influences within observed variables and the existence of latent variables, it cannot recover the causal relationships among latent variables and has strong graphical constraints. For a more detailed discussion of related work, please refer to our Appx. C.1.\n\n\n![Figure 1: An example graph that we aim to handle, where latent variables are denoted by L and observed variables are denoted by X. The latent variables can act as a cause, effect, or mediator for both observed variables and other latent variables. See the appendix for more examples.](images/396ed343e02c446d0a65dfdbb78f3810b02809e7c6e8b8e827a89fd42dac2589.jpg)\n\n\nIn this paper, we aim to handle a more general scenario for causal discovery with latent variables, where observed variables are allowed to be directly adjacent, and latent variables to be flexibly related to all the other variables. That is, hidden variables can serve as confounders, mediators, or effects of latent or observed variables, and even form a hierarchical structure (illustrated in Fig. 1). This setting is rather general and practically meaningful to deal with many real-world problems.\n\n\nTo address such a challenging problem, we are confronted with three fundamental questions: (i) What information and constraints can be discovered from the observed variables to reveal the underlying causal structure? (ii) How can we effectively and efficiently search for these constraints? (iii) What graphical conditions are needed to uniquely locate latent variables and ascertain the complete causal structure? Remarkably, these questions can be addressed by harnessing the power of rank deficiency constraints on the covariance of observed variables. By carefully identifying and utilizing rank properties in specific ways, we are able to determine the Markov equivalence class of the entire graph. Our contributions are mainly three-fold:\n\n- - We investigate the efficacy of rank in comparison to conditional independence in latent causal graph discovery, and theoretically introduce necessary and sufficient conditions for the identifiability of certain latent structural properties. For instance, the condition we proposed for nonadjacency generalizes the counterpart in Spirtes et al. (2000) to graphs with latent variables.\n\n- - We develop RLCD, an efficient three-phase causal discovery algorithm that is able to locate latent variables, determine their cardinalities, and identify the whole causal structure involving measured and latent variables, by properly leveraging rank properties. In the special case with no latent variables, it asymptotically returns the same graph as the PC algorithm (Spirtes et al., 2000) does.\n\n- - We provide a set of graphical conditions that are sufficient for RLCD to asymptotically identify the correct Markov Equivalence Class of the latent causal graph; notably, these graphical conditions are significantly weaker than those in previous works. Our empirical study on both synthetic and real-world datasets validates RLCD on finite samples.\n\n\n# 2 PROBLEM SETTING\n\n\nIn this paper, we aim to identify the causal structure of a latent linear causal model defined as follows.\n\n\nDefinition 1. (Latent Linear Causal Models) Suppose a directed acyclic graph  $\\mathcal{G} \\coloneqq (\\mathbf{V}_{\\mathcal{G}}, \\mathbf{E}_{\\mathcal{G}})$ , where each variable  $V_{i} \\in \\mathbf{V}_{\\mathcal{G}}$  is generated following a linear causal structural model:\n\n\n$$\n\n$$\nV _ {i} = \\sum_ {V _ {j} \\in P a _ {\\mathcal {G}} (V _ {i})} a _ {i j} V _ {j} + \\varepsilon_ {V _ {i}}, \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\mathbf{V}_{\\mathcal{G}} \\coloneqq \\mathbf{L}_{\\mathcal{G}} \\cup \\mathbf{X}_{\\mathcal{G}}$  contains a set of  $n + m$  random variables, with  $m$  latent variables  $\\mathbf{L}_{\\mathcal{G}} \\coloneqq \\{\\mathsf{L}_i\\}_{i=1}^m$ , and  $n$  observed variables  $\\mathbf{X}_{\\mathcal{G}} \\coloneqq \\{\\mathsf{X}_i\\}_{i=1}^n$ .  $P_{\\mathcal{G}}(V_i)$  denotes the parent set of  $V_i$ ,  $a_{ij}$  the causal coefficient from  $V_j$  to  $V_i$ , and  $\\varepsilon_{\\mathsf{V}_i}$  represents the noise term.\n\n\nOur objective is to identify the underlying causal structure  $\\mathcal{G}$  over all the variables  $\\mathbf{L}_{\\mathcal{G}} \\cup \\mathbf{X}_{\\mathcal{G}}$  (detailed in Sec. 5) that are generated according to a latent linear causal model, given i.i.d. samples of observed variables  $\\mathbf{X}_{\\mathcal{G}}$  only. To address this challenging problem, traditional wisdom often relies on strong graphical constraints (Pearl, 1988; Zhang, 2004; Huang et al., 2022; Maeda & Shimizu, 2020) (detailed in Appx. B.1 with illustrative graphs). In contrast, Definition 1 allows all the variables including observed and latent variables to be very flexibly related. We basically allow the presence of edges between any two variables such that a node  $\\mathsf{V}$ , no matter whether it is observed or not, can act as a cause, effect, or mediator for both observed and latent variables.\n\n\nA summary of notations is in Tab. 1. The rest of the paper is organized as follows. In Sec. 3, we motivate the use of rank and propose conditions for nonadjacency and the existence of latent variables. In Sec. 4, we establish the minimal identifiable substructure of a linear latent graph, based on which we propose RLCD for latent variable causal discovery. In Sec. 5, we introduce the identifiability of RLCD. In Sec. 6, we validate our method using both synthetic and real-life data.\n\n\nTable 1: Graphical notations used throughout this paper.\n\n\n<table><tr><td>Pa: Parents</td><td>V: Variables</td><td>V: Variable</td><td>G: The underlying graph</td></tr><tr><td>Ch: Children</td><td>L: Latent variables</td><td>L: Latent variable</td><td>G&#x27;: Output Graph</td></tr><tr><td>PCh: Pure children</td><td>X: Observed variables</td><td>X: Observed variable</td><td>S: Set of covers</td></tr><tr><td>Sib: Siblings</td><td>MDe: Measured descendants</td><td>PDe: Pure descendants</td><td>S: Set of sets of covers</td></tr></table>\n\n\n# 3 WHY USE RANK INFORMATION?\n\n\nIn this section, we first motivate the use of rank constraints for causal discovery in the presence of latent variables, and then establish some fundamental theories about what rank implies graphically.\n\n\n# 3.1 PRELIMINARIES ABOUT TREKS AND RANK\n\n\nWhen there is no latent variable, a common approach for causal discovery is to use conditional independence (CI) relationships to identify d-separations in a graph; see, e.g., the PC algorithm (Spirtes et al., 2000). The following theorem illustrates this idea.\n\n\nTheorem 1 (Conditional Independence and D-separation (Pearl, 1988)). Under the Markov and faithfulness assumption, for disjoint sets of variables A, B and C, C d-separates A and B in graph  $\\mathcal{G}$ , iff A  $\\perp$  B|C holds for every distribution in the graphical model associated to  $\\mathcal{G}$ .\n\n\nAs for latent linear causal models, trek-separations (t-separations) provide more information than d-separations (for readers who are not very familiar with treks and t-separations, kindly refer to Appx. A.2 for examples). The definitions of treks and t-separation are given as follows, together with Theorem 2 showing the relations between t-separations and d-separations.\n\n\nDefinition 2 (Treks (Sullivant et al., 2010)). In  $\\mathcal{G}$ , a trek from  $X$  to  $Y$  is an ordered pair of directed paths  $(P_1, P_2)$  where  $P_1$  has a sink  $X$ ,  $P_2$  has a sink  $Y$ , and both  $P_1$  and  $P_2$  have the same source  $Z$ .\n\n\nDefinition 3 (T-separation (Sullivant et al., 2010)). Let  $\\mathbf{A}$ ,  $\\mathbf{B}$ ,  $\\mathbf{C_A}$ , and  $\\mathbf{C_B}$  be four subsets of  $\\mathbf{V}_{\\mathcal{G}}$  in graph  $\\mathcal{G}$  (not necessarily disjoint).  $(\\mathbf{C_A},\\mathbf{C_B})$ $t$ -separates  $\\mathbf{A}$  from  $\\mathbf{B}$  if for every trek  $(P_1,P_2)$  from a vertex in  $\\mathbf{A}$  to a vertex in  $\\mathbf{B}$ , either  $P_{1}$  contains a vertex in  $\\mathbf{C_A}$  or  $P_{2}$  contains a vertex in  $\\mathbf{C_B}$ .\n\n\nTheorem 2 (T- and D-sep (Di, 2009)). For disjoint sets  $\\mathbf{A}$ ,  $\\mathbf{B}$  and  $\\mathbf{C}$ ,  $\\mathbf{C}$ $d$ -separates  $\\mathbf{A}$  and  $\\mathbf{B}$  in graph  $\\mathcal{G}$ , iff there is a partition  $\\mathbf{C} = \\mathbf{C}_{\\mathbf{A}} \\cup \\mathbf{C}_{\\mathbf{B}}$  such that  $(\\mathbf{C}_{\\mathbf{A}}, \\mathbf{C}_{\\mathbf{B}})$ $t$ -separates  $\\mathbf{A} \\cup \\mathbf{C}$  from  $\\mathbf{B} \\cup \\mathbf{C}$ .\n\n\nThe theorem above reveals that all d-sep can be reformulated by t-sep, and thus, t-sep encompasses d-sep information. Just as we use CI tests to find d-sep, t-sep can be identified by the rank of cross-covariance matrix over specific combinations of variables, which is formally stated as follows.\n\n\nTheorem 3 (Rank and T-separation (Sullivant et al., 2010)). Given two sets of variables  $\\mathbf{A}$  and  $\\mathbf{B}$  from a linear model with graph  $\\mathcal{G}$ , we have:\n\n\n$$\n\n$$\n\\operatorname {r a n k} \\left(\\Sigma_ {\\mathbf {A}, \\mathbf {B}}\\right) \\leq \\min  \\left\\{\\left| \\mathbf {C} _ {\\mathbf {A}} \\right| + \\left| \\mathbf {C} _ {\\mathbf {B}} \\right|: \\left(\\mathbf {C} _ {\\mathbf {A}}, \\mathbf {C} _ {\\mathbf {B}}\\right) t - s e p a r a t e s \\mathbf {A} f r o m \\mathbf {B} i n \\mathcal {G} \\right\\}, \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\Sigma_{\\mathbf{A},\\mathbf{B}}$  is the cross-covariance over  $\\mathbf{A}$  and  $\\mathbf{B}$ , and equality generically holds.\n\n\nThe left-hand side of Equation 2 is about properties of the observational distribution, while the right-hand side describes properties of the graph. Equality in Equation 2 generally holds, if there are no specific ways for the parameters of the underlying causal model to be coupled (e.g., accidental path cancellations). Recall that the classical faithfulness (Spirtes et al., 2000) is assumed so that we can infer d-separations from CI relations, here we formulate rank faithfulness in a similar spirit such that we can infer t-separations from rank deficiency constraints, as follows.\n\n\nAssumption 1 (Rank Faithfulness). A probability distribution  $p$  is rank faithful to  $\\mathcal{G}$  if the inequality in Equation 2 can be replaced by equality.\n\n\nThis formulation of rank faithfulness differs from that of Spirtes (2013), though the spirit is shared. The faithfulness assumption in causal discovery (Spirtes et al., 2000) is critical and prevalent. If faithfulness is violated (illustrated in Appx. B.2), even classical methods like PC cannot guarantee asymptotic correctness. On the other hand, with infinite data, the set of values of the SCM's free parameters s.t. rank faithfulness does not hold is of Lebesgue measure 0 (Spirtes, 2013).\n\n\nWithout latent variables, rank and CI are equally informative about the underlying DAG. Yet, in the presence of latent variables, t-separations inferred by rank offer more graphical information. Thus, we next show how rank constraint plays a pivotal role in identifying latent causal structures.\n\n\n# 3.2 RANK: AN INFORMATIVE GRAPHICAL INDICATOR FOR LATENT VARIABLES\n\n\nIn the presence of latent variables, CI is not enough: FCI and its variants make full use of CI but only recover a representation which is not informative enough about the latent confounders. Fortunately, leveraging rank information can naturally make causal discovery results more informative.\n\n\n![(a)  $\\mathcal{G}_1$](images/54a860d41eb4360824f169edeeff35e2017302c4aa26b9b32c65bae12445077c.jpg)\n\n\n![(b) CI skeleton of  $\\mathcal{G}_1$](images/19b2b8b490f3a50b2ae011c8a9afee7405ee29b72a70063a97bc9059c8459ba4.jpg)\n\n\n![(c)  $\\mathcal{G}_2$ Figure 2: Examples that illustrate the basic intuition for a latent to be identifiable.](images/10eecdb06704839ece197b627ee44641e40007dd25f8909d24e350d43fa3b45e.jpg)\n\n\n![(d) CI skeleton of  $\\mathcal{G}_2$](images/4ed40387af28d7ccac0f79ba2bdf62b643603c88b0094aaf940cbcf7c358b7a4.jpg)\n\n\n![(e)  $\\mathcal{G}_3$](images/dbee37ace3ffe14a38047b9df0536e3068d1fdd55a07c74884fc8c80e244f73b.jpg)\n\n\n![(f) CI skeleton of  $\\mathcal{G}_3$](images/c1ad173bc6f215eeec5bb18b362311e333714b1f6f5d564dcee0cfd9ceb14fa7.jpg)\n\n\nAn example highlighting the greater informativeness of rank compared to CI is as follows. Consider the graph  $\\mathcal{G}_1$  in Fig. 7, where  $\\{\\mathrm{X}_1,\\mathrm{X}_2\\}$  and  $\\{\\mathrm{X}_3,\\mathrm{X}_4\\}$  are d-separated by  $\\mathsf{L}_1$ , but we cannot infer that from a CI test (i.e., whether  $\\{\\mathrm{X}_1,\\mathrm{X}_2\\} \\perp \\{\\mathrm{X}_3,\\mathrm{X}_4\\} | \\mathsf{L}_1$ ), as  $\\mathsf{L}_1$  is not observed. In contrast, with rank information, we can infer that  $\\mathrm{rank}(\\Sigma_{\\{\\mathrm{X}_1,\\mathrm{X}_2\\},\\{\\mathrm{X}_3,\\mathrm{X}_4\\}}) = 1$ , which implies  $\\{\\mathrm{X}_1,\\mathrm{X}_2\\}$  and  $\\{\\mathrm{X}_3,\\mathrm{X}_4\\}$  are t-separated by one latent variable. The rationale behind is that the t-sep of  $\\mathbf{A}$ ,  $\\mathbf{B}$  by  $(\\mathbf{C}_{\\mathbf{A}},\\mathbf{C}_{\\mathbf{B}})$  can be deduced through rank (as in Theorem 3) without observing any element in  $(\\mathbf{C}_{\\mathbf{A}},\\mathbf{C}_{\\mathbf{B}})$ .\n\n\nWith this intuition in mind, below we present three theorems that characterize the graphical implications of rank constraints, in scenarios where latent variables might exist: Theorem 4 gives conditions for observed variables to be nonadjacent, illustrated by Example 6; Theorem 5 gives conditions for the existence of latent variables, illustrated by Example 1; Theorem 6 implies how to utilize pure children as surrogates for calculating rank, illustrated with Example 7. All proofs are in Appendix.\n\n\nTheorem 4 (Condition for Nonadjacency). Consider a latent linear causal model. Two observed variables  $\\mathsf{X}_1, \\mathsf{X}_2 \\in \\mathbf{X}_{\\mathcal{G}}$  are not adjacent, if there exist two sets  $\\mathbf{A}, \\mathbf{B} \\subseteq \\mathbf{X}_{\\mathcal{G}} \\setminus \\{\\mathsf{X}_1, \\mathsf{X}_2\\}$  that are not necessarily disjoint, such that  $\\text{rank}(\\Sigma_{\\mathbf{A} \\cup \\{\\mathsf{X}_1\\}, \\mathbf{B} \\cup \\{\\mathsf{X}_2\\}}) = \\text{rank}(\\Sigma_{\\mathbf{A}, \\mathbf{B}})$  and  $\\text{rank}(\\Sigma_{\\mathbf{A} \\cup \\{\\mathsf{X}_1, \\mathsf{X}_2\\}, \\mathbf{B} \\cup \\{\\mathsf{X}_1, \\mathsf{X}_2\\}}) = \\text{rank}(\\Sigma_{\\mathbf{A}, \\mathbf{B}}) + 2$ .\n\n\nRemark 1. Theorem 4 presents a sufficient condition for determining nonadjacency between two observed variables. In the absence of latent variables, this condition becomes a both necessary and sufficient one. Note that  $\\mathbf{A}$  and  $\\mathbf{B}$  may overlap. SGS and PC (Spirtes et al., 2000) also introduced a necessary and sufficient condition for determining nonadjacency between two observed variables in the absence of latent variables: there exist a set of observed variables  $\\mathbf{C} \\subseteq \\mathbf{X}_{\\mathcal{G}}, \\mathsf{X}_1, \\mathsf{X}_2 \\notin \\mathbf{C}$ , such that  $\\mathsf{X}_1 \\perp \\mathsf{X}_2|\\mathbf{C}$ . Interestingly, this condition can be expressed in the form of Theorem 4, with  $\\mathbf{C} = \\mathbf{A} = \\mathbf{B}$ ; thus Theorem 4 generalizes PC's condition to where latent variables may exist (detailed in Appx. A.8). Even in the presence of latent variables, with further graphical constraints the condition can also become a both necessary and sufficient one, as in Proposition 1.\n\n\nTheorem 5 (Condition for Existence of Latent Variable). Suppose a latent linear causal model with graph  $\\mathcal{G}$  and observed variables  $\\mathbf{X}_{\\mathcal{G}}$ . If there exist three disjoint sets of variables  $\\mathbf{A}, \\mathbf{B}, \\mathbf{C} \\subseteq \\mathbf{X}_{\\mathcal{G}}$  such that (i)  $|\\mathbf{B}| \\geq |\\mathbf{A}| \\geq 2$ , (ii)  $\\forall$  distinct  $\\mathsf{A}_1, \\mathsf{A}_2 \\in \\mathbf{A}$ ,  $\\mathsf{A}_1, \\mathsf{A}_2$  are adjacent in the CI skeleton over  $\\mathbf{X}_{\\mathcal{G}}$  with CI skeleton defined in Appx. A.3), (iii)  $\\forall \\mathsf{A} \\in \\mathbf{A}, \\mathsf{B} \\in \\mathbf{B}, \\mathsf{A}, \\mathsf{B}$  are adjacent in the CI skeleton over  $\\mathbf{X}_{\\mathcal{G}}, (\\mathrm{i}) \\mathbf{C} \\subseteq \\{\\mathsf{X} | \\exists \\mathsf{Y} \\in \\mathbf{A} \\cup \\mathbf{B} s.t. \\mathsf{X}, \\mathsf{Y}$  are adjacent in the CI skeleton\\} (i.e., all elements in  $\\mathbf{C}$  are neighbours of an element in  $\\mathbf{A} \\cup \\mathbf{B}$  in the CI skeleton), (v) rank  $(\\Sigma_{\\mathbf{A} \\cup \\mathbf{C}, \\mathbf{B} \\cup \\mathbf{C}}) < |\\mathbf{A}| + |\\mathbf{C}|$ , then there must exist at least one latent variable in the treks between  $\\mathbf{A}, \\mathbf{B}$ .\n\n\nRemark 2. Theorem 5 provides a sufficient condition for determining the existence of latent variables. The intuition is that, in the absence of latent variables, rank information should align with CI skeleton; if not, there must exist latent variable. Furthermore, we will show in Theorem 9 that, with further graphical constraints the condition in Theorem 5 becomes both necessary and sufficient.\n\n\nMoreover, it can be shown that observed children, or even descendants of latent variables can be used as surrogates to calculate rank as stated in Theorem 6 with the definition of pure children below.\n\n\nDefinition 4 (Pure Children). In  $\\mathcal{G}$ , we denote the pure children set of a set of variables  $\\mathbf{V}$ , as  $PCh_{\\mathcal{G}}(\\mathbf{V}) = \\{\\mathsf{C} | Pa_{\\mathcal{G}}(\\mathsf{C}) \\neq \\emptyset$  and  $Pa_{\\mathcal{G}}(\\mathsf{C}) \\subseteq \\mathbf{V}\\}$ .\n\n\nTheorem 6 (Pure Children as Surrogate for Rank Estimation). Let  $\\mathbf{C} \\subseteq PCh_{\\mathcal{G}}(\\mathbf{A})$  be a subset of pure children of  $\\mathbf{A}$ , and  $\\mathbf{B}$  be a set of variables such that  $\\mathbf{B} \\cap De_{\\mathcal{G}}(\\mathbf{C}) = \\emptyset$ . We have  $\\text{rank}(\\Sigma_{\\mathbf{A},\\mathbf{B}}) \\geq \\text{rank}(\\Sigma_{\\mathbf{C},\\mathbf{B}})$ . Moreover, if  $\\text{rank}(\\Sigma_{\\mathbf{A},\\mathbf{C}}) = |\\mathbf{A}|$ , then  $\\text{rank}(\\Sigma_{\\mathbf{A},\\mathbf{B}}) = \\text{rank}(\\Sigma_{\\mathbf{C},\\mathbf{B}})$ .\n\n\nRemark 3. Theorem 6 informs us that under certain conditions, we can estimate the rank of covariance involving latent variables by using their pure children as surrogates. Even when the pure children are not observed one can recursively examine the children's children until reaching observed descendants (defined in Appx. A.1). This enables us to deduce graphical information associated with latent variables through the use of observed ones as surrogates.\n\n\n# 4 DISCOVERING LATENT STRUCTURE THROUGH RANK CONSTRAINTS\n\n\nIn this section, we begin with the concept of atomic cover and explore its rank deficiency, and then develop an efficient algorithm based on rank-deficiency for latent causal discovery, as in Alg. 1.\n\n\n# 4.1 ATOMIC COVER AND RANK DEFICIENCY\n\n\nBelow, we introduce atomic covers and their associated rank deficiency properties, which allows us to define the minimal identifiable substructure of a graph. We start with an example in Figure 2 that motivates the conditions for a latent variable to be identifiable.\n\n\nExample 1. Consider  $\\mathcal{G}_1$  in Fig. 2 (a). It can be shown that the latent variable  $\\mathsf{L}_1$  in  $\\mathcal{G}_1$  is not identifiable. We can easily find a graph  $\\mathcal{G}_1'$  with no latent variable, e.g., as in Fig. 10 (b), such that  $\\mathcal{G}_1'$  shares the same skeleton as Fig. 2 (a), but all the observational rank information entailed by  $\\mathcal{G}_1$  is the same as by  $\\mathcal{G}_1'$  - they are indistinguishable. However, if  $\\mathsf{X}_4$  becomes the children of  $\\mathsf{L}_1$ , as in  $\\mathcal{G}_2$  given in Fig. 2 (c), the whole structure becomes identifiable. Specifically, the conditions in Theorem 5 hold when we take  $\\mathbf{A} = \\{\\mathsf{X}_1, \\mathsf{X}_2\\}$ ,  $\\mathbf{B} = \\{\\mathsf{X}_3, \\mathsf{X}_4\\}$ , and  $\\mathbf{C} = \\emptyset$ , which informs the existence of latent variables. The same conditions also hold for Fig. 2 (e) and thus  $\\mathsf{L}_1$  in  $\\mathcal{G}_3$  is also identifiable, though  $\\mathsf{L}_1$  has only two children together with another two neighbors.\n\n\nThe intuition is that, for a latent variable to be identifiable, it should have enough children and enough neighbors. We next formalize this intuition into the concept of atomic cover as the minimal identifiable unit in a graph. The formal definition of an atomic cover is given in Definition 5, where effective cardinality of a set of covers  $\\mathcal{V}$  is defined as  $||\\mathcal{V}|| = |(\\cup_{\\mathbf{V}\\in \\mathcal{V}}\\mathbf{V})|$ .\n\n\nDefinition 5 (Atomic Cover and Atomic  $+p$  Cover). Let  $\\mathbf{V} \\subseteq \\mathbf{V}_{\\mathcal{G}}$ , where  $l$  of the  $|\\mathbf{V}|$  variables are latent, and the remaining  $|\\mathbf{V}| - l$  are observed.  $\\mathbf{V}$  is an atomic  $+p$  cover ( $p \\geq 0$ ), if  $\\mathbf{V}$  contains a single observed variable, or the following conditions hold:\n\n- (i) There exists a set of atomic  $+0$  covers  $\\mathcal{C}$ , s.t.  $\\cup_{\\mathbf{C} \\in \\mathcal{C}} \\mathbf{C} \\subseteq Pch_{\\mathcal{G}}(\\mathbf{V})$  and  $||\\mathcal{C}|| \\geq l + 1 + p$ .\n\n- (ii) There exists a set of covers  $\\mathcal{N}$ , s.t.  $(\\cup_{\\mathbf{N} \\in \\mathcal{N}} \\mathbf{N}) \\cap (\\cup_{\\mathbf{C} \\in \\mathcal{C}} \\mathbf{C}) = \\emptyset$ , every element in  $\\cup_{\\mathbf{N} \\in \\mathcal{N}} \\mathbf{N}$  is a neighbour of  $\\mathbf{V}$ ,  $\\mathcal{N}$  and  $\\mathcal{C}$  are  $d$ -separated by  $\\mathbf{V}$ , and  $||\\mathcal{N}|| \\geq l + 1$ .\n\n- (iii) There does not exist a partition of  $\\mathbf{V} = \\mathbf{V}_1 \\cup \\mathbf{V}_2$ , s.t., both  $\\mathbf{V}_1$  and  $\\mathbf{V}_2$  are atomic  $+0$  covers. For brevity atomic  $+0$  cover is often referred to as atomic cover.\n\n\nWe allow the definition to take different values of  $p$  as we can benefit from more pure children for structure identification (as shown in Theorem 1). Yet, in this paper, we mainly focus on atomic covers (i.e., atomic+0 covers). We note that if  $\\mathbf{V}$  satisfies the definition of atomic+1 cover, it also satisfies the definition of atomic cover. In other words, a theorem that holds under atomic covers will also hold under atomic+1 covers. In the definition above, each observed variable is treated as an atomic cover, e.g.,  $\\{\\mathsf{X}_1\\}$  in Figure 1. We define atomic cover as the minimal identifiable unit with the rationale that, when two or more latent variables share exactly the same set of neighbors (e.g.,  $\\mathsf{L}_1$  and  $\\mathsf{L}_2$  in Fig. 6), they can never be distinguished from observational information. Hence, it is more convenient and unified to consider them together in an atomic cover. Examples of atomic covers can be found in Appx. B.3. Based on the definition of atomic covers, we define a cluster as the set of pure children of an atomic cover, and refer  $k$ -cluster to a cluster whose parents' cardinality is  $k$ . We further define an operator  $\\operatorname{Rep}(\\mathbf{X}) = \\cup_{\\mathsf{X} \\in \\mathbf{X}} \\{\\{\\mathsf{X}\\}\\}$ . We next show that every atomic cover possesses a useful rank deficiency property, which is formally stated in Theorem 7 (proof in Appx. A.11).\n\n\nTheorem 7 (Rank Deficiency of an Atomic Cover). Let  $\\mathbf{V} = \\mathbf{X} \\cup \\mathbf{L}$  be an atomic cover where  $\\mathbf{X}$  are observed,  $\\mathbf{L}$  are latent, and  $|\\mathbf{V}| = k$ . Let  $\\mathcal{X} = \\operatorname{Rep}(\\mathbf{X})$ ,  $\\mathcal{X}_{\\mathcal{G}} = \\operatorname{Rep}(\\mathbf{X}_{\\mathcal{G}})$ , and a set of atomic covers  $\\mathcal{C}$ , satisfying (i) in Definition 5. Then  $\\operatorname{rank}(\\Sigma_{\\mathcal{C} \\cup \\mathcal{X}, \\mathcal{X} \\cup \\mathcal{X}_{\\mathcal{G}} \\setminus \\mathcal{C} \\setminus MDe_{\\mathcal{G}}(\\mathcal{C})) = k$  and  $k < \\min(||\\mathcal{C} \\cup \\mathcal{X}||, ||\\mathcal{X} \\cup \\mathcal{X}_{\\mathcal{G}} \\setminus \\mathcal{C} \\setminus MDe_{\\mathcal{G}}(\\mathcal{C})||)$  ( $MDe_{\\mathcal{G}}$  denotes measured descendants).\n\n\nExample 2 (Example for atomic cover and rank deficiency). Consider an atomic cover in Fig. 4(d):  $\\mathbf{V} = \\{\\mathsf{X}_2,\\mathsf{L}_2\\}$ .  $\\mathbf{V}$  is an atomic cover, because  $\\mathbf{V}$  has at least 2 pure children and has additional 3 neighbors, satisfying the conditions in Definition 5. If we take  $\\mathcal{C} = \\{\\{\\mathsf{X}_4\\},\\{\\mathsf{X}_5\\}\\}$ , and  $\\mathcal{X} = \\{\\{\\mathsf{X}_2\\}\\}$ , we have  $\\mathrm{rank}(\\Sigma_{\\mathcal{C}\\cup \\mathcal{X},\\mathcal{X}\\cup \\mathcal{X}_{\\mathcal{G}}\\backslash \\mathcal{C}\\backslash MDe_{\\mathcal{G}}(\\mathcal{C})) = \\mathrm{rank}(\\Sigma_{\\{\\mathsf{X}_4,\\mathsf{X}_5,\\mathsf{X}_2\\}},\\{\\mathsf{X}_1,\\mathsf{X}_2,\\mathsf{X}_3,\\mathsf{X}_6,\\mathsf{X}_7,\\mathsf{X}_8\\}) = 2 = |\\mathbf{V}|.$  Noted that both  $||\\mathcal{C}\\cup \\mathcal{X}||,||\\mathcal{X}\\cup \\mathcal{X}_{\\mathcal{G}}\\backslash \\mathcal{C}\\backslash Mde_{\\mathcal{G}}(\\mathcal{C})|| > 2$  so here the rank is deficient.\n\n\nTheorem 7 establishes the rank-deficiency property of an atomic cover. Furthermore, if we can build a unique connection between rank deficiency and atomic covers under certain conditions, then we can exploit rank deficiency to identify atomic covers in a graph. In the following, we present the graphical conditions to achieve identifiability and Theorem 8 delineates under what conditions the uniqueness of the rank-deficiency property can be ensured.\n\n\nCondition 1 (Basic Graphical Conditions for Identifiability). Every latent variable belongs to at least one atomic cover in  $\\mathcal{G}$  and for each atomic cover that contains at least one latent variable, any of its children is not adjacent to any of its neighbours.\n\n\n![](images/e59e62e58ee02197514fd306af09a6b96a891aea80d92c8d5581aaba3d90fb52.jpg)\n\n\n![(a) The underlying graph  $\\mathcal{G}$ , all the observed variables of which are taken as input to Phase 1.](images/d53761362a46a4aae4620a4132e4ea6ad5c4595456d78bed4615d0f1f8165725.jpg)\n\n\n![](images/32180787ac4716dd58a83bff478a4718a5c064aa76a99df12df3cfd1b924b757.jpg)\n\n\n![(b) Given the CI skeleton from Phase 1, variables are partitioned into two groups, shown in the two dashed areas. (d) Do the same thing as in (c) for the second dashed area and update all the remaining directions.](images/05c917fad61d76b6dc5240e6d415a2232659d6ebe93466fe3d8300b168395cd9.jpg)\n\n\n![(c) Take variables from the first dashed area to Phase 2 and 3, use the result to update the CI skeleton. Figure 3: An illustrative example of the overall search procedure in Alg. 1. Figure 4: An illustrative example of the process of Phase 2 in Alg. 3.](images/df7a3f0a553eef20359b17ef32c04f76b03881204bdc81a633903fc3472eaba0.jpg)\n\n\nTheorem 8 (Uniqueness of Rank Deficiency). Suppose  $\\mathcal{G}$  satisfies Condition 1. We further assume (i) all the atomic covers with cardinality  $k' < k$  have been discovered and recorded, and (ii) there is no collider in  $\\mathcal{G}$ . If there exists a set of observed variables  $\\mathbf{X}$  and a set of atomic covers  $\\mathcal{C}$  satisfying  $\\mathcal{X} = \\operatorname{Seq}(\\mathbf{X})$ ,  $\\mathcal{C} \\cap \\mathcal{X} = \\emptyset$ , and  $||\\mathcal{C}|| + ||\\mathcal{X}|| = k + 1$ , such that (i) For all recorded  $k'$  cluster  $\\mathcal{C}'$ ,  $||\\mathcal{C} \\cap \\mathcal{C}'|| \\leq |P a_{\\mathcal{G}}(\\mathcal{C}')|$ , (ii)  $\\text{rank}(\\Sigma_{\\mathcal{C} \\cup \\mathcal{X}, \\mathcal{X} \\cup \\mathcal{X}_{\\mathcal{G}} \\backslash \\mathcal{C} \\backslash M d e_{\\mathcal{G}}(\\mathcal{C})) = k$ , then there exists an atomic cover  $\\mathbf{V} = \\mathbf{L} \\cup \\mathbf{X}$  in  $\\mathcal{G}$ , with  $|\\mathbf{L}| = k - |\\mathbf{X}|$ , and  $\\cup_{\\mathbf{C} \\in \\mathcal{C}} \\mathbf{C} \\subseteq P c h_{\\mathcal{G}}(\\mathbf{V})$ .\n\n\nFor a better understanding, we provide an illustrative example in Appx. A.12. Basically, Theorem 8 says that under certain conditions, we can build a unique connection between rank deficiency and atomic covers, and thus we can identify atomic covers in a graph by searching for combinations of  $\\mathcal{C}$  and  $\\mathcal{X}$  that induce rank deficient property. We further introduce Theorem 9, which is useful in that it provides necessary and sufficient conditions for the existence of latent variables, under Condition 1 and Theorem 1, which provides necessary and sufficient conditions for nonadjacency.\n\n\nTheorem 9 (Necessary and Sufficient Condition for Existence of Latent Variable). If a graph satisfies Condition 1, then the \"if\" in Theorem 5 becomes \"if and only if\".\n\n\nProposition 1 (Necessary and Sufficient Condition for Nonadjacency). In  $\\mathcal{G}$ , if every latent variable belongs to at least one atomic+1 cover, then the \"if\" in Theorem 4 becomes \"if and only if\".\n\n\nProposition 1 shows the further identifiability that can be established by assuming one additional pure child of atomic covers. As we mainly focus on atomic covers, the next question is, how to design a search procedure that strives to fulfill these conditions, in order to cash out Theorem 8 and Theorem 9, for identifying atomic covers, and consequently the whole latent causal structure.\n\n\n# 4.2 METHOD-RANK-BASED LATENT CAUSAL DISCOVERY\n\n\nIn this section, we propose a computationally efficient and scalable search algorithm to identify the latent causal structure, referred to as Rank-based Latent Causal Discovery (RLCD), that leverages the connection between graph structures and rank deficiency, as we discussed in previous sections. The search process mainly comprises three phases: (1) Phase 1: FindCISkeleton, (2) Phase 2: FindCausalClusters, and (3) Phase 3: RefineCausalClusters, as outlined in Alg. 1.\n\n\nSpecifically, Phase 1 is to find the CI skeleton deduced by conditional independence tests over  $\\mathbf{X}_{\\mathcal{G}}$  and Phase 2 and Phase 3 are designed such that the conditions in Theorem 8 are satisfied to the largest extent, in order to make use of the unique rank deficiency property to identify the latent structure. We initiate the process by finding the CI skeleton first, for the reason as follows. According to Theorem 9, latent variables exist iff conditions (i)-(v) in Theorem 5 are satisfied, while conditions\n\n\n```\nAlgorithm 1: The overall procedure for Rank-based Latent Causal Discovery (RLCD).  \nInput : Samples from all  $n$  observed variables  $\\mathbf{X}_{\\mathcal{G}}$   \nOutput: Markov equivalence class  $\\mathcal{G}'$   \ndef LatentVariableCausalDiscovery(X_G):  \nPhase 1:  $\\mathcal{G}' = \\text{FindCISkeleton}(\\mathbf{X}_{\\mathcal{G}})$  (Algorithm 2);  \nfor Each  $\\mathcal{Q}$ , a group of overlapping maximal cliques, in  $\\mathcal{G}'$  do  \nSet an empty graph  $\\mathcal{G}''$ ,  $\\mathbf{X}_{\\mathcal{Q}} = \\cup_{\\mathbf{Q} \\in \\mathcal{Q}} \\mathbf{Q}$ ,  $\\mathbf{N}_{\\mathcal{Q}} = \\{\\mathsf{N} : \\exists \\mathsf{X} \\in \\mathbf{X}_{\\mathcal{Q}} \\text{ s.t. } \\mathsf{N}, \\mathsf{X} \\text{ are adjacent in } \\mathcal{G}'\\}$ ;  \nPhase 2:  $\\mathcal{G}'' = \\text{FindCausalClusters}(\\mathcal{G}', \\mathbf{X}_{\\mathcal{Q}} \\cup \\mathbf{N}_{\\mathcal{Q}})$  (Algorithm 3);  \nPhase 3:  $\\mathcal{G}'' = \\text{RefineCausalClusters}(\\mathcal{G}', \\mathbf{X}_{\\mathcal{Q}} \\cup \\mathbf{N}_{\\mathcal{Q}})$  (Algorithm 4);  \nTransfer the estimated DAG  $\\mathcal{G}''$  to the Markov equivalence class and update  $\\mathcal{G}'$  by  $\\mathcal{G}''$ ;  \nOrient remaining causal directions that can be inferred from v structures;  \nreturn  $\\mathcal{G}'$\n```\n\n\n(i)-(iv) can be directed inferred from the CI skeleton. Therefore, there is no need to consider all variables in  $\\mathbf{X}_{\\mathcal{G}}$  as inputs for Phases 2 and 3. Instead, we make use of the CI skeleton over  $\\mathbf{X}_{\\mathcal{G}}$  and select some groups of observed variables as inputs into Phases 2 and 3, where variables in each group together have the potential to satisfy conditions (i)-(iv). This also benefits the computational efficiency as the Phase 2 and 3 for different groups can be done in parallel.\n\n\nAn example of the entire search process is illustrated in Fig. 3. After Phase 1, variables are partitioned into groups as shown in Fig. 3 (b). For each group (dashed area in Fig. 3 (b)), we conduct Phases 2 and 3, and have the final result shown in Fig. 3 (d). Details of each phase are given below.\n\n\n# 4.3 PHASE 1: FINDING CI SKELETON\n\n\nThe objective of Phase 1 is to find the CI skeleton over  $\\mathbf{X}_{\\mathcal{G}}$  by utilizing CI relations. To this end, we employ the first stage of the PC algorithm (Spirtes et al., 2000), with the difference that we replace all CI tests with rank tests, according to the following Lemma 10 (proof in Appx. A.16).\n\n\nLemma 10 (D-separation by Rank Test). Suppose a linear latent causal model with graph  $\\mathcal{G}$ . For disjoint  $\\mathbf{A},\\mathbf{B},\\mathbf{C} \\in \\mathbf{X}_{\\mathcal{G}}$ ,  $\\mathbf{C}$ $d$ -separates  $\\mathbf{A}$  and  $\\mathbf{B}$  in graph  $\\mathcal{G}$ , if and only if  $\\text{rank}(\\Sigma_{\\mathbf{A} \\cup \\mathbf{C}}, \\mathbf{B} \\cup \\mathbf{C}) = |\\mathbf{C}|$ . We summarize the procedure of Phase 1 in Alg. 2 in the appendix. Although, asymptotically, using CI and rank information will provide the same d-separation result over observed variables, we use rank instead of CI in Phase 1 just for the purpose of having a unified causal discovery framework with rank constraints (as Phases 2 and 3 are also based on rank).\n\n\nGiven the CI skeleton  $\\mathcal{G}'$  (result from Phase 1), the next step is to find the substructures in  $\\mathcal{G}'$  that might contain latent variables. Specifically, Theorem 9 informs us that latent variable exists iff (i)-(v) in Theorem 5 holds, where (i)-(iv) can be directly inferred from the CI skeleton. Specifically, we consider all the maximal cliques  $\\mathbf{Q}$  in  $\\mathcal{G}'$  (a clique is a set of variables that are fully connected and a maximal clique is a clique that cannot be extended), s.t.,  $|\\mathbf{Q}| \\geq 3$ . We then partition these cliques into groups such that two cliques  $\\mathbf{Q}_1, \\mathbf{Q}_2$  are in the same group if  $|\\mathbf{Q}_1 \\cap \\mathbf{Q}_2| \\geq 2$ . Finally, for each group of cliques  $\\mathcal{Q}$  (as in line 3 in Alg. 1), we combine them to form a set of variables  $\\mathbf{X}_{\\mathcal{Q}} = \\cup_{\\mathbf{Q} \\in \\mathcal{Q}} \\mathbf{Q}$ . We further determine the neighbour set for each  $\\mathbf{X}_{\\mathcal{Q}}$ , as  $\\mathbf{N}_{\\mathcal{Q}} = \\{\\mathsf{N} : \\exists \\mathsf{X} \\in \\mathbf{X}_{\\mathcal{Q}} \\text{ s.t. } \\mathsf{N}, \\mathsf{X} \\text{ are adjacent in CI skeleton } \\mathcal{G}'\\}$ . It can be shown that variables that satisfy (i)-(iv) will be in the same set with  $\\mathbf{X}_{\\mathcal{Q}} \\cup \\mathbf{N}_{\\mathcal{Q}}$  (examples and proof in Appx. B.4). Therefore our next step is to take each  $\\mathbf{X}_{\\mathcal{Q}} \\cup \\mathbf{N}_{\\mathcal{Q}}$  separately as input to our Phases 2 and 3, detailed as follows.\n\n\n# 4.4 PHASE 2: FINDING CAUSAL CLUSTERS\n\n\nIn this section, we introduce the second phase of our algorithm, FindCausalClusters, summarized in Alg. 3 in the appendix and illustrated in Fig. 4. The objective here is to design an effective search procedure to find combinations of sets of covers  $\\mathcal{C}$  and  $\\mathcal{X}$  (as defined in Theorem 8), such that rank deficiency holds and all the conditions required in Theorem 8 are satisfied. To be specific, given Condition 1, Theorem 8 further requires that (i) when we are searching for  $k$ -clusters, all the  $k'$ -clusters,  $k' < k$  have been found and recorded, and that (ii) there is no collider in  $\\mathcal{G}$ . We next introduce the key designs for that end, accompanied by examples.\n\n\nAs for the requirement (i), we design our search procedure such that it starts with  $k = 1$ . If a k-cluster is found, we update the graph and reset  $k$  to 1; otherwise, we increase  $k$  by 1 (as in line 5 Alg 3). This ensures to a large extent that when searching for a  $k$ -cluster, all  $k'$  clusters such that  $k' < k$ , can be found, in order to fulfill the requirement (i).\n\n\nRegarding the requirement (ii), we need to ensure that all the rank deficiencies are from atomic covers, rather than from colliders. One could directly assume the absence of colliders in the underlying  $\\mathcal{G}$ , but this would impose rather strong structural constraints and thus limit the applicability of a discovery method. Therefore, we add a collider check function NoCollider defined in Alg. 5, together with the designed search procedure to allow incorporating colliders timely such that they will not induce unexpected rank deficiency anymore. With these two designs, we only rely on a much weaker condition about colliders, as in Condition 2, under which it can be guaranteed that our search procedure will not be affected by the existence of colliders (proof in Appx. A.18).\n\n\nCondition 2 (Graphical condition on colliders for identifiability). In a latent graph  $\\mathcal{G}$ , if (i) there exists a set of variables  $\\mathbf{C}$  such that every variable in  $\\mathbf{C}$  is a collider of two atomic covers  $\\mathbf{V}_1, \\mathbf{V}_2$ , and denote by  $\\mathbf{A}$  the minimal set of variables that  $d$ -separates  $\\mathbf{V}_1$  from  $\\mathbf{V}_2$ , (ii) there is a latent variable in  $\\mathbf{V}_1, \\mathbf{V}_2, \\mathbf{C}$  or  $\\mathbf{A}$ , then we must have  $|\\mathbf{C}| + |\\mathbf{A}| \\geq |\\mathbf{V}_1| + |\\mathbf{V}_2|$ .\n\n\nWe summarize the whole process of Phase 2 in Alg. 3 and provide an illustration in Fig. 4, where the input variables are from the left dash area in Fig. 3 (b). For a better understanding, please refer to Appx. B.5 for a detailed description of key steps and illustrative examples.\n\n\n# 4.5 PHASE 3: REFINING CAUSAL CLUSTERS\n\n\nIn Phase 2, we strive to fulfill all required conditions such that we can correctly identify causal clusters and related structures. However, there still exist some rare cases where our search cannot ensure the requirement (i) in Theorem 8. In this situation, Phase 2 might produce a big cluster in the resulting  $\\mathcal{G}'$  that should be split into smaller ones (see examples in Appx. B.6). Fortunately, the incorrect cluster will not do harm to the identification of other substructures in the graph, and thus we can employ Phase 3 to characterize and refine the incorrect ones, by making use of the following Theorem 11 (proof in Appx. A.17).\n\n\nTheorem 11 (Refining Clusters). Denote by  $\\mathcal{G}'$  the output from FindCausalClusters and by  $\\mathcal{G}$  the true graph. For an atomic cover  $\\mathbf{V}$  in  $\\mathcal{G}'$ , if  $\\mathbf{V}$  is not a correct cluster in  $\\mathcal{G}$  but consists of some smaller clusters, then  $\\mathbf{V}$  can be refined into correct ones by FindCausalClusters( $\\hat{\\mathcal{G}}$ ,  $\\mathbf{X}$ ), where  $\\hat{\\mathcal{G}}$  is got by deleting  $\\mathbf{V}$ , all neighbors of  $\\mathbf{V}$  that are latent, and all relating edges of them, from  $\\mathcal{G}'$ .\n\n\nTo be specific, we search through all the atomic covers  $\\mathbf{V}$  in  $\\mathcal{G}'$ , the output of Phase 2, and then perform FindCausalClusters( $\\hat{\\mathcal{G}}$ ,  $\\mathbf{X}$ ), where  $\\hat{\\mathcal{G}}$  is defined as in Theorem 11. With this procedure, we can make sure that all the found clusters in  $\\mathcal{G}'$  are correct as in  $\\mathcal{G}$ . We summarized Phase 3 in Alg. 4 and provide illustrative examples in Appx. B.6.\n\n\n# 5 IDENTIFIABILITY THEORY OF CAUSAL STRUCTURE\n\n\nHere we show the identifiability of the proposed RLCD algorithm. Specifically, RLCD asymptotically produces the correct Markov equivalence class of the causal graph over both observed and latent variables under certain graphical conditions, up to the minimal-graph operator  $\\mathcal{O}_{\\mathrm{min}}(\\cdot)$  and skeleton operator  $\\mathcal{O}_{\\mathrm{s}}(\\cdot)$ . They are defined in Appx. A.4 following but differently with Huang et al. (2022).  $\\mathcal{O}_{\\mathrm{min}}(\\cdot)$  is to absorb redundant latent variables under certain conditions and  $\\mathcal{O}_{\\mathrm{s}}(\\cdot)$  is to introduce edges involving latent variables if certain conditions hold, and we note that the observational rank information is invariant to these two graph operators (examples in Appx. B.12). We summarize the identifiability result in Theorem 12, along with Corollary 1 (proof in Appx. A.18).\n\n\nTheorem 12 (Identifiability of the Proposed Alg. 1). Suppose  $\\mathcal{G}$  is a DAG associated with a Linear Latent Causal Model that satisfies Condition 1 and Condition 2. Algorithm 1 can asymptotically identify the Markov equivalence class of  $\\mathcal{O}_{\\min}(\\mathcal{O}_s(\\mathcal{G}))$\n\n\nCorollary 1. Assume linear causal models. Asymptotically, when no latent variable exists, Alg. 1's output is the same as that of PC; as another special case, when there is no edge between observed variables, the output of Alg. 1 is the same as that of Hier. rank (Huang et al., 2022).\n\n\n# 6 EXPERIMENTS\n\n\nWe validate our method using both synthetic and real-life data. In finite sample cases, we employ canonical correlations (Anderson, 1984) to estimate the rank (detailed in Appx. A.19).\n\n\nSynthetic Data Specifically, we considered different types of latent graphs: (i) latent tree models (Appx. B.8), (ii) latent measurement models (Appx. B.9), and (iii) general latent models (Fig. 1 and Appx. B.10). The causal strength is uniformly from  $[-10, 10]$ , and the noise is either Gaussian or uniform (which is for RCD and GIN). We propose to use the following two metrics: (1) F1 score of\n\n\nTable 2: F1 scores (mean (standard deviation)) of compared methods on different types of latent graphs.\n\n\n<table><tr><td colspan=\"2\"></td><td colspan=\"6\">F1 score for skeleton among all variables Vg (both Xg and Lg)</td></tr><tr><td colspan=\"2\">Algorithm</td><td>Ours</td><td>Hier. rank</td><td>PC</td><td>FCI</td><td>GIN</td><td>RCD</td></tr><tr><td rowspan=\"3\">Latent+tree</td><td>2k</td><td>0.84 (0.11)</td><td>0.58 (0.01)</td><td>0.36 (0.01)</td><td>0.37 (0.01)</td><td>0.37 (0.03)</td><td>0.24 (0.04)</td></tr><tr><td>5k</td><td>0.92 (0.05)</td><td>0.60 (0.01)</td><td>0.37 (0.00)</td><td>0.37 (0.01)</td><td>0.41 (0.03)</td><td>0.33 (0.00)</td></tr><tr><td>10k</td><td>0.98 (0.02)</td><td>0.60 (0.01)</td><td>0.37 (0.00)</td><td>0.38 (0.02)</td><td>0.41 (0.03)</td><td>0.33 (0.01)</td></tr><tr><td rowspan=\"3\">Latent+measm</td><td>2k</td><td>0.81 (0.12)</td><td>0.52 (0.05)</td><td>0.44 (0.01)</td><td>0.38 (0.02)</td><td>0.40 (0.02)</td><td>0.26 (0.03)</td></tr><tr><td>5k</td><td>0.88 (0.11)</td><td>0.52 (0.05)</td><td>0.49 (0.01)</td><td>0.40 (0.01)</td><td>0.46 (0.03)</td><td>0.29 (0.01)</td></tr><tr><td>10k</td><td>0.91 (0.09)</td><td>0.53 (0.05)</td><td>0.49 (0.01)</td><td>0.40 (0.01)</td><td>0.47 (0.05)</td><td>0.34 (0.04)</td></tr><tr><td rowspan=\"3\">Latent general</td><td>2k</td><td>0.66 (0.01)</td><td>0.44 (0.02)</td><td>0.31 (0.01)</td><td>0.25 (0.02)</td><td>0.30 (0.04)</td><td>0.32 (0.03)</td></tr><tr><td>5k</td><td>0.72 (0.03)</td><td>0.45 (0.03)</td><td>0.32 (0.01)</td><td>0.28 (0.02)</td><td>0.38 (0.04)</td><td>0.34 (0.02)</td></tr><tr><td>10k</td><td>0.80 (0.05)</td><td>0.45 (0.04)</td><td>0.32 (0.01)</td><td>0.28 (0.02)</td><td>0.35 (0.01)</td><td>0.36 (0.01)</td></tr></table>\n\n\n![Figure 5: Identified latent graph on Big Five personality dataset.](images/71cf1aea54c50a11bcb1562e0a9d4b2b6a3fb095981d5726c7b1d7dfc607b2a5.jpg)\n\n\nskeleton among observed variables  $\\mathbf{X}_{\\mathcal{G}}$ , (2) F1 score of skeleton among all variables  $\\mathbf{V}_{\\mathcal{G}}$ . We consider combinations and permutations of latent variables during evaluation (detailed in Appx. B.15 together with definition of F1 score).\n\n\nWe compared with many competitive baselines, including (i) Hier. rank (Huang et al., 2022), (ii) PC (Spirtes et al., 2000), (iii) FCI (Spirtes et al., 2013), (iv) RCD (Maeda & Shimizu, 2020), and (v) GIN (Xie et al., 2020). The results are reported in Tab. 2 and Tab. 3, where we run experiments with different random seeds and sample sizes  $2k$ ,  $5k$ , and  $10k$ . Our proposed RLCD gives the best results on all types of graphs, in terms of both metrics, with a clear margin. This result serves as strong empirical support for the identifiability of latent linear causal graphs by our proposed method.\n\n\nReal-World Data To further verify our proposed method, we employed a real-world Big Five Personality dataset https://openpsychometrics.org/. It consists of 50 personality indicators and close to 20,000 data points. Each Big Five personality dimension, namely, Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism (O-C-E-A-N), are measured with their own 10 indicators. Data is processed to have zero mean and unit variance. We employ the proposed method to determine the Markov equivalence class and employ GIN (Xie et al., 2020) to further decide other directions between latent variables (more details in Appendix B.17).\n\n\nWe analyzed the data using RLCD, producing a causal graph in Fig 5 that exhibits interesting psychological properties. First, most of the variables related to the same Big Five dimension are in the same cluster. Strikingly, our result reconciles two currently deemed distinct theories of personality: latent personality dimensions and network theory (Cramer et al., 2012; Wright, 2017). We see groups of closely connected items that are predictable from latent dimensions (L1, L2, L3), interactions among latents  $(\\mathrm{L}1\\rightarrow \\mathrm{L}6\\rightarrow \\mathrm{L}3$ $\\mathrm{L}1\\rightarrow \\mathrm{L}2\\rightarrow \\mathrm{L}3)$  , and latents influencing the same indicators (L1, L3). We also observe plausible causal links between indicators (e.g.,  $\\mathrm{O}2\\to \\mathrm{O}4$  and  $\\mathrm{O}1\\to \\mathrm{O}8$  ). We argue that our findings are consistent with pertinent personality literature, but more importantly, offer new, plausible explanations as to the nature of human personality (detailed analysis in Appx. B.18).\n\n\n# 7 DISCUSSION AND CONCLUSION\n\n\nWe developed a versatile causal discovery approach that allows latent variables to be causally-related in a flexible way, by making use of rank information. We showed the proposed method can asymptotically identify the Markov equivalence class of the underlying graph under mild conditions. One limitation of our method is that it cannot directly handle cyclic graphs, and as future work, we will extend this line of thought to cyclic graphs and to nonlinear models.\n\n\n# REFERENCES\n\n- Jeffrey Adams, Niels Hansen, and Kun Zhang. Identification of partially observed linear causal models: Graphical conditions for the non-gaussian and heterogeneous cases. Advances in Neural Information Processing Systems, 34, 2021.\n\n- Raj Agrawal, Chandler Squires, Neha Prasad, and Caroline Uhler. The decamfounder: Non-linear causal discovery in the presence of hidden variables. arXiv preprint arXiv:2102.07921, 2021.\n\n- Sina Akbari, Ehsan Mokhtarian, AmirEmad Ghassami, and Negar Kiyavash. Recursive causal structure learning in the presence of latent variables and selection bias. In Advances in Neural Information Processing Systems, volume 34, pp. 10119-10130, 2021.\n\n- Animashree Anandkumar, Daniel Hsu, Adel Javanmard, and Sham Kakade. Learning linear bayesian networks with latent variables. In International Conference on Machine Learning, pp. 249-257, 2013.\n\n- T. W. Anderson. An Introduction to Multivariate Statistical Analysis. 2nd ed. John Wiley & Sons, 1984.\n\n- Ruichu Cai, Feng Xie, Clark Glymour, Zhifeng Hao, and Kun Zhang. Triad constraints for learning causal structure of latent variables. In Advances in Neural Information Processing Systems, pp. 12863-12872, 2019.\n\n- Chandler Squires. _causaldag: creation, manipulation, and learning of causal models_, 2018. URL https://github.com/uhlerlab/causaldag.\n\n- V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky. Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572-596, 2011.\n\n- V. Chandrasekaran, P. A. Parrilo, and A. S. Willsky. Latent variable graphical model selection via convex optimization. Annals of Statistics, 40(4):1935-1967, 2012.\n\n- Zhengming Chen, Feng Xie, Jie Qiao, Zhifeng Hao, Kun Zhang, and Ruichu Cai. Identification of linear latent variable model with arbitrary distribution. In Proceedings 36th AAAI Conference on Artificial Intelligence (AAAI), 2022.\n\n- David Maxwell Chickering. Learning equivalence classes of bayesian-network structures. The Journal of Machine Learning Research, 2:445-498, 2002a.\n\n- David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine learning research, 3(Nov):507-554, 2002b.\n\n- David Maxwell Chickering. A transformational characterization of equivalent bayesian network structures. arXiv preprint arXiv:1302.4938, 2013.\n\n- Diego Colombo, Marloes H Maathuis, Markus Kalisch, and Thomas S Richardson. Learning high-dimensional directed acyclic graphs with latent and selection variables. The Annals of Statistics, pp. 294-321, 2012.\n\n- Ang\u00e9lique OJ Cramer, Sophie Van der Sluis, Arjen Noordhof, Marieke Wichers, Nicole Geschwind, Steven H Aggen, Kenneth S Kendler, and Denny Borsboom. Dimensions of normal personality as networks in search of equilibrium: You can't like parties if you don't like people. European Journal of Personality, 26(4):414-431, 2012.\n\n- Petru Lucian Curseu, Remus Ilies, Delia V\u00edrg\u0103, Laurentiu Maricu\u0163oiu, and Florin A Sava. Personality characteristics that are valued in teams: Not always \u201cmore is better\u201d? International Journal of Psychology, 54(5):638-649, 2019.\n\n- Haoyue Dai, Peter Spirtes, and Kun Zhang. Independence testing-based approach to causal discovery under measurement error and linear non-gaussian models. Advances in Neural Information Processing Systems, 35:27524-27536, 2022.\n\n- Yanming Di. t-separation and d-separation for directed acyclic graphs. preprint, 2009.\n\n- Doris Entner and Patrik O Hoyer. Discovering unconfounded causal relationships using linear non-gaussian models. In JSAI International Symposium on Artificial Intelligence, pp. 181-195. Springer, 2010.\n\n- Thomas Gatzka. Aspects of openness as predictors of academic achievement. *Personality and Individual Differences*, 170:110422, 2021.\n\n- William G Graziano and Renee M Tobin. Agreeableness: Dimension of personality or social desirability artifact? Journal of personality, 70(5):695-728, 2002.\n\n- Pierce J Howard and Jane Mitchell Howard. The owner's manual for personality at work: How the Big Five personality traits affect performance, communication, teamwork, leadership, and sales. Center for Applied Cognitive Studies (CentACS), 2010.\n\n- Patrik O Hoyer, Shohei Shimizu, Antti J Kerminen, and Markus Palviainen. Estimation of causal effects using linear non-gaussian causal models with hidden variables. International Journal of Approximate Reasoning, 49(2):362-378, 2008.\n\n- Patrik O Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlinear causal discovery with additive noise models. In Advances in neural information processing systems, pp. 689-696, 2009.\n\n- B. Huang*, K. Zhang*, J. Zhang, R. Sanchez-Romero, C. Glymour, and B. Sch\u00f6lkopf. Causal discovery from heterogeneous/nonstationary data. In JMLR, volume 21(89), 2020.\n\n- Biwei Huang, Charles Jia Han Low, Feng Xie, Clark Glymour, and Kun Zhang. Latent hierarchical causal structure discovery with rank constraints. arXiv preprint arXiv:2210.01798, 2022.\n\n- Bohdan Kivva, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Learning latent causal graphs via mixture oracles. Advances in Neural Information Processing Systems, 34, 2021.\n\n- Erich Kummerfeld and Joseph Ramsey. Causal clustering for 1-factor measurement models. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1655-1664. ACM, 2016.\n\n- Huy Le, In-Sue Oh, Steven B Robbins, Remus Ilies, Ed Holland, and Paul Westrick. Too much of a good thing: curvilinear relationships between personality traits and job performance. Journal of Applied Psychology, 96(1):113, 2011.\n\n- Wendy Lord. NEO PI-R: A guide to interpretation and feedback in a work context. Hogrefe, 2007.\n\n- Takashi Nicholas Maeda and Shohei Shimizu. Rcd: Repetitive causal discovery of linear nongaussian acyclic models with latent confounders. In International Conference on Artificial Intelligence and Statistics, pp. 735-745. PMLR, 2020.\n\n- Christopher Meek. Causal inference and causal explanation with background knowledge. arXiv preprint arXiv:1302.4972, 2013.\n\n- J. Pearl. Probabilistic reasoning in intelligent systems: Networks of plausible inference. Morgan Kaufmann, 1988.\n\n- Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York, NY, USA, 2000. ISBN 0-521-77362-8.\n\n- Judea Pearl. The seven tools of causal inference, with reflections on machine learning. Communications of the ACM, 62(3):54-60, 2019.\n\n- Saber Salehkaleybar, AmirEmad Ghassami, Negar Kiyavash, and Kun Zhang. Learning linear non-gaussian causal models in the presence of latent variables. Journal of Machine Learning Research, 21(39):1-24, 2020.\n\n- Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvarinen, and Antti Kerminen. A linear non-gaussian acyclic model for causal discovery. J. Mach. Learn. Res., 7:2003-2030, December 2006a. ISSN 1532-4435.\n\n- Shohei Shimizu, Patrik O Hoyer, Aapo Hyvarinen, Antti Kerminen, and Michael Jordan. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006b.\n\n- Shohei Shimizu, Patrik O Hoyer, and Aapo Hyvarinen. Estimation of linear non-gaussian acyclic models for latent factors. Neurocomputing, 72(7-9):2024-2027, 2009.\n\n- Ricardo Silva, Richard Scheine, Clark Glymour, and Peter Spirtes. Learning the structure of linear latent variable models. Journal of Machine Learning Research, 7(Feb):191-246, 2006.\n\n- Peter Spirtes. An anytime algorithm for causal inference. In International Workshop on Artificial Intelligence and Statistics, pp. 278-285. PMLR, 2001.\n\n- Peter Spirtes. Calculation of entailed rank constraints in partially non-linear and cyclic models. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, pp. 606-615. AUAI Press, 2013.\n\n- Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction, and search. MIT press, 2000.\n\n- Peter Spirtes, Clark Glymour, Richard Scheines, and Robert Tillman. Automated search for causal relations: Theory and practice. 2010.\n\n- Peter L Spirtes, Christopher Meek, and Thomas S Richardson. Causal inference in the presence of latent variables and selection bias. arXiv preprint arXiv:1302.4983, 2013.\n\n- Seth Sullivant, Kelli Talaska, and Jan Draisma. Trek separation for gaussian graphical models. arXiv:0812.1938., 2010.\n\n- Tatsuya Tashiro, Shohei Shimizu, Aapo Hyv\u00e4rinen, and Takashi Washio. ParceLiNGAM: a causal ordering method robust against latent confounders. *Neural Computation*, 26(1):57-83, 2014.\n\n- Sofia Triantafillou and Ioannis Tsamardinos. Constraint-based causal discovery from multiple interventions over overlapping variable sets. The Journal of Machine Learning Research, 16(1): 2147-2205, 2015.\n\n- S. Wang. Causal clustering for 1-factor measurement models on data with various types. arXiv preprint arXiv:2009.08606, 2020.\n\n- Aidan GC Wright. Factor analytic support for the five-factor model. The Oxford handbook of the five factor model, pp. 217-242, 2017.\n\n- Feng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng Hao, and Kun Zhang. Generalized independent noise condition for estimating latent variable causal graphs. In Advances in Neural Information Processing Systems, pp. 14891-14902, 2020.\n\n- Kun Zhang and Aapo Hyv\u00e4rinen. On the identifiability of the post-nonlinear causal model. In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence, pp. 647-655. AUAI Press, 2009.\n\n- Nevin L Zhang. Hierarchical latent class models for cluster analysis. The Journal of Machine Learning Research, 5:697-723, 2004.\n\n- Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming Gong, Ruichu Cai, Shohei Shimizu, Peter Spirtes, and Kun Zhang. Causal-learn: Causal discovery in python. arXiv preprint arXiv:2307.16405, 2023.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638631.746299", "msecs": "746.0", "relativeCreated": "228576.79653167725", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Cross-Attention Head Position Patterns Can Align with Human Visual Concepts in Text-to-Image Generative Models\n\n# Abstract\nRecent text-to-image diffusion models leverage cross-attention layers, which have been effectively utilized to enhance a range of visual generative tasks. However, our understanding of cross-attention layers remains somewhat limited. In this study, we introduce a mechanistic interpretability approach for diffusion models by constructing Head Relevance Vectors (HRVs) that align with human-specified visual concepts. An HRV for a given visual concept has a length equal to the total number of cross-attention heads, with each element representing the importance of the corresponding head for the given visual concept. To validate HRVs as interpretable features, we develop an ordered weakening analysis that demonstrates their effectiveness. Furthermore, we propose concept strengthening and concept adjusting methods and apply them to enhance three visual generative tasks. Our results show that HRVs can reduce misinterpretations of polysemous words in image generation, successfully modify five challenging attributes in image editing, and mitigate catastrophic neglect in multi-concept generation. Overall, our work provides an advancement in understanding cross-attention layers and introduces new approaches for fine-controlling these layers at the head level.\n\n\n# 1 INTRODUCTION\n\n\nRecent advancements in Text-to-Image (T2I) models have demonstrated an unprecedented ability to generate high-quality images with strong image-text alignment. These models often leverage powerful pre-trained text encoders; for instance, Stable Diffusion (Rombach et al., 2022) uses CLIP (Radford et al., 2021), while Imagen (Saharia et al., 2022) uses T5 (Raffel et al., 2020). Given that language is more expressive than previously used supervision signals (Gandelsman et al., 2023), text representations have empowered visual generative tasks, allowing for a high degree of control over the generation process. However, our understanding of the inner workings of T2I models remains limited, and even the latest models continue to struggle with certain failure cases.\n\n\nSignificant progress has been made in understanding the inner workings of deep neural networks. Olah et al. (2018) demonstrated numerous examples showing interpretable features at various levels in neural networks; Olah et al. (2020) expanded this analysis by exploring connections between units in the networks. Notably, Templeton et al. (2024) identified interpretable features in the cutting-edge large language model (LLM)-Claude 3 Sonnet-and used these features to guide the model's generation towards safer outcomes. Building on this line of work, we analyze T2I generative models with a focus on their cross-attention (CA) layers. We introduce a novel method to construct head relevance vectors (HRVs) that align with user-specified concepts. An HRV for a given visual concept is a vector whose length equals to the total number of CA heads, with each element representing the importance of the corresponding head for that concept. We demonstrate that these vectors reflect human-interpretable features by using ordered weakening analysis, where we sequentially weaken the activation of CA heads and examine the resulting generated images (Figure 1a).\n\n\nTo demonstrate the utility of HRVs, we propose concept strengthening and concept adjusting methods to enhance three visual generative tasks (Figure 1b). In image generation, our approach significantly reduces the misinterpretation of polysemous words. When generating images from text prompts containing such words, our method decreased the misinterpretation rate from  $63.0\\%$  to\n\n\n![(a) Ordered weakening analysis of CA heads from high to low relevance for two visual concepts.](images/f23b58618cc0bd939a1bce9f07fed77aba92383f2a3111d6c393a35df8a6802b.jpg)\n\n\n![(b) Enhancing three visual generative tasks with our head relevance vectors. Figure 1: We develop a method for constructing head relevance vectors (HRVs) that align with useful visual concepts. For a specified visual concept, an HRV assigns a relevance score to individual cross-attention heads, revealing their importance for the visual concept. Our analysis shows that the constructed HRVs can serve as interpretable features. We also demonstrate that HRV can be effectively integrated for improving three visual generative tasks.](images/af29c9a034a0efbec07214cf7b26b03cc4bd06a11e6496014363af8ea624360e.jpg)\n\n\n15.9%. In image editing, our method enhances the widely used Prompt-to-Prompt (P2P) (Hertz et al., 2022) algorithm in modifying object attributes (color, material, geometric patterns) and image attributes (image style, weather conditions), achieving 2.32% to 11.79% higher image-text alignment scores compared to state-of-the-art methods. Additionally, in human evaluations, it received more than twice the preference scores compared to existing methods. In multi-concept generation, our method improves upon the state-of-the-art Attend-and-Excite (Chefer et al., 2023) algorithm. By reducing catastrophic neglect \u2013 the omission of objects or attributes in generated images \u2013 our approach enhances performance by 2.3% to 6.3% across two benchmark types.\n\n\nWe use Stable Diffusion v1 (Rombach et al., 2022) as the primary model for our analysis and experiments. To demonstrate that our findings are not limited to a single model, we also show that they are consistent in Stable Diffusion XL (Podell et al., 2024). Furthermore, we show that our approach allows for flexible adjustment of human-specified concepts. These results indicate that our method for constructing HRVs might be also useful for different model architectures and target concept.\n\n\n# 2 RELATED WORK\n\n\nEarly works on interpretable neurons and interpretable features: Early works on visual generative models have trained variational autoencoders (VAEs) using specifically designed loss functions on datasets with distinct attributes (Kulkarni et al., 2015; Higgins et al., 2017; Chen et al., 2018; Klys et al., 2018). While these methods successfully controlled a few attributes present in the training dataset, they were limited by a lack of fine-grained control, strong dependence on training data, and possible need for manual supervision of attributes. Another approach is to identify meaningful features in intermediate layers of neural networks (e.g., generative adversarial networks (GANs)), and modify those features to control the generation outputs (Plumerault et al., 2020; Shen & Zhou, 2021). Also, a seminal work by Olah et al. (2018) demonstrated that interpretable features in neural networks can be identified at various levels: single neurons, spatial positions, channels, or groups of neurons across different positions and channels. They presented numerous examples showing how these interpretable features emerge at various levels within a neural network's architecture. While these efforts successfully revealed meaningful features, they were limited by an inability to specify user-specified concepts. Moreover, the identified features were not always directly usable for controlling attributes in generative tasks.\n\n\nRecent works with multi-modal and generative models: Since the development of powerful multi-modal models that can map images and text to a joint embedding space, few studies have used text to interpret intermediate representations in vision models (Goh et al., 2021; Hernandez et al., 2022; Yuksekgonul et al., 2023b). Most recently, Gandelsman et al. (2023) examined self-attention heads from the last layers of CLIP-ViT (Dosovitskiy et al., 2021), identifying meaningful correlations with several visual concepts. While these studies primarily focused on non-generative models, our study shifts focus to the interpretable features within visual generative models. Regarding the recently proposed large language model, Claude 3 Sonnet, Templeton et al. (2024) demonstrated that identifying meaningful concepts from model activations provides valuable insights into understanding model behavior. Among the various features identified, their use of safety-related features to guide text generation towards safer outcomes is particularly relevant for large-scale text-generative models. Our study also explores large-scale generative models, showing that user-specified visual concepts can be captured and applied in three distinct visual generative tasks.\n\n\nText-to-image diffusion models with cross-attention layers: Building on the large-scale T2I diffusion models, researchers have developed methods to tackle various visual generative tasks, such as image editing (Hertz et al., 2022) and multi-concept generation (Chefer et al., 2023). Many of these studies utilize the publicly available Stable Diffusion (Rombach et al., 2022), which efficiently generates images through a diffusion denoising process in latent space. This model incorporates an autoencoder and a U-Net (Ronneberger et al., 2015) with multi-head cross-attention (CA) layers that integrate CLIP (Radford et al., 2021) text embeddings. Researchers have explored these CA layers to enhance control over text-conditioned image generation (Feng et al., 2022; Parmar et al., 2023; Chefer et al., 2023; Tumanyan et al., 2023; Wu et al., 2023). For example, P2P (Hertz et al., 2022) manipulates image layout and structure by swapping CA maps between source and target prompts. However, these methods typically update entire CA layers without offering fine-grained control over individual attention heads. We construct head relevance vectors, which align with a human visual concept, and uses these vectors to improve three visual generative tasks.\n\n\n# 3 METHOD FOR CONSTRUCTING HEAD RELEVANCE VECTORS\n\n\nThe core idea involves selecting a set of visual concepts of interest, using a large language model (LLM) to pre-select 10 associated words for each concept, and utilizing random image generations for updating head relevance vectors (HRVs). The key to a successful update lies in identifying the visual concept that best matches a particular head and increasing the corresponding element of the HRVs. Figure 2 illustrates the process of a single update. We begin by providing background on the cross-attention (CA) layer, followed by detailed descriptions of our methodology for constructing HRVs that correspond to a set of human-specified visual concepts.\n\n\nCross-attention in T2I diffusion models: Let  $\\mathbf{P}$  be a generation prompt,  $\\mathbf{Z}_t$  be a noisy image at generation timestep  $t$ , and  $\\psi(\\cdot)$  be a CLIP text-encoder. In each CA head, the spatial features of the noisy image  $\\phi(\\mathbf{Z}_t)$  are projected to a query matrix  $\\mathbf{Q} = l_Q(\\phi(\\mathbf{Z}_t))$ , while the CLIP text embedding  $\\psi(\\mathbf{P})$  is projected to a key matrix  $\\mathbf{K} = l_K(\\psi(\\mathbf{P}))$  and a value matrix  $\\mathbf{V} = l_V(\\psi(\\mathbf{P}))$ , using learned linear layers  $l_Q, l_K$ , and  $l_V$ . Then, the CA map is calculated by measuring the correlations between  $\\mathbf{Q}$  and  $\\mathbf{K}$  as\n\n\n$$\n\n$$\n\\mathbf {M} = \\operatorname {s o f t m a x} \\left(\\frac {\\mathbf {Q} \\mathbf {K} ^ {T}}{\\sqrt {d}}\\right), \\tag {1}\n$$\n\n$$\n\n\nwhere  $d$  is the projection dimension of the keys and queries. The CA output  $\\mathbf{MV}$  is a weighted average of the value  $\\mathbf{V}$ , with the weights determined by the CA map  $\\mathbf{M}$ . This operation is performed in parallel across multi-heads in the CA layer, and their outputs are concatenated and linearly projected using a learned linear layer to produce the final CA output.\n\n\nImage generation prompts, visual concepts, and concept-words: To generate 2,100 random images, we used 2,100 generation prompts. Of these, 1,000 prompts were constructed using 1,000 ImageNet classes (Deng et al., 2009), formatted as 'A photo of a {class name}' The remaining 1,100 prompts were adopted from PromptHero (PromptHero) to enhance diversity. The visual concepts can be specified as an arbitrary set. In our study, we have interacted with GPT-4o (OpenAI, 2024) to list 34 commonly used visual concepts including object categories and image properties. While\n\n\n![Figure 2: Overview of a single HRV update for a cross-attention (CA) head position  $h$ . While generating a random image, the most relevant visual concept is identified. Then the concept's head relevance vector (HRV) is updated to have an increased value in position  $h$ . For illustration purpose, we are showing only 5 visual concepts ( $N = 5$ ) and 6 CA heads ( $H = 6$ ). In our main experiments, we adopt  $N = 34$  and  $H = 128$ . This update is repeated over all the head positions  $h = 1, \\ldots, H$  and all timesteps  $t = 1, \\ldots, T$  for a sufficiently large number of random image generations.](images/b591ed66eb0c8031e01994134c9de07fccd6982242121a385e9b1cd3271a7a67.jpg)\n\n\nour study mainly focuses on the 34 visual concepts, we demonstrate in Appendix J that a new visual concept can be flexibly and easily added. For each of the selected visual concepts, we have generated 10 representative concept-words using GPT-4o. The full list of 34 visual concepts, along with 10 corresponding concept-words for each, can be found in Table 3 of Appendix A.\n\n\nT2I models and head relevance vectors: We focus on Stable Diffusion v1, which contains  $H = 128$  CA heads. Let  $C_1, \\ldots, C_N$  represent the specified visual concepts, where  $N = 34$  in our main experiments. For each visual concept  $C_n$ , we define a head relevance vector (HRV) as an  $H$ -dimensional vector that expresses how each CA head is activated by the corresponding visual concept. Initially, all  $N$  head relevance vectors are set to zero and are iteratively updated following the process illustrated in Figure 2. The full iteration involved 2,100 generated images, with each image used to iterate over all  $H = 128$  heads and  $T = 50$  timesteps. Extension to a larger diffusion model, Stable Diffusion XL, is discussed in Section 6.1.\n\n\nConcatenation of the  $N$  token embeddings: In each HRV update shown in Figure 2, we generate a concatenation of token embeddings for the  $N$  visual concepts and use it to identify the best matching visual concept for the  $h$ -th head. To enhance diversity in the process, we randomly sample one concept-word for each visual concept from the list provided in Table 3 of Appendix A. The sampled  $N$  concept-words are individually embedded using the CLIP text encoder, followed by the learned linear key projection layer  $l_{K}$  at the  $h$ -th CA head position. This produces  $N$  key matrices  $\\mathbf{K}_1, \\mathbf{K}_2, \\ldots, \\mathbf{K}_N \\in \\mathbb{R}^{77 \\times F}$ , each corresponding to a concept, with each key matrix containing 77 token embeddings, where  $F$  is a feature dimension. For instance, if the sampled word for the third concept  $C_3$  is 'white,' the corresponding key matrix  $\\mathbf{K}_3$  would be  $[\\langle \\mathrm{SOT} \\rangle, < \\mathrm{white}>$ ,  $\\langle \\mathrm{EOT} \\rangle, \\dots, \\langle \\mathrm{EOT} \\rangle]$ , where  $\\langle \\mathrm{SOT} \\rangle$  and  $\\langle \\mathrm{EOT} \\rangle$  are key-projected embeddings of special tokens. We only extract the embedding of the semantic token from  $\\mathbf{K}_3$ , denoted as  $\\widehat{\\mathbf{K}}_3 \\in \\mathbb{R}^{n_3 \\times F}$ , where  $n_3 = 1$  since there is only one semantic token,  $\\langle \\mathrm{white} \\rangle$ . Similarly, we extract the embeddings for the other concepts, obtaining  $\\widehat{\\mathbf{K}}_1, \\widehat{\\mathbf{K}}_2, \\ldots, \\widehat{\\mathbf{K}}_N$ . These  $N$  embeddings are then concatenated to form  $\\widehat{\\mathbf{K}} = [\\widehat{\\mathbf{K}}_1, \\widehat{\\mathbf{K}}_2, \\ldots, \\widehat{\\mathbf{K}}_N] \\in \\mathbb{R}^{N' \\times F}$ , where  $N'$  represents the total number of semantic tokens across all  $N$  concepts ( $N' = N$  if each concept-word consists of a single token).\n\n\nUpdating of the head relevance vectors: We calculate the cross-attention (CA) maps  $\\widehat{\\mathbf{M}} \\in \\mathbb{R}^{R^2 \\times N'}$ , where  $R$  is the width or height of the image latent, by applying Eq. 1 using the con\n\n\ncatenated token embeddings  $\\widehat{\\mathbf{K}}\\in \\mathbb{R}^{N'\\times F}$ , in place of  $\\mathbf{K}$ , and the image query  $\\mathbf{Q}\\in \\mathbb{R}^{R^2\\times F}$  from  $h$ -th head position. This measures the correlation between the visual information at  $h$ -th head position and the textual information of the  $N$  visual concepts, resulting in  $N$  groups of CA maps. If a word consists of multiple tokens, we average the CA maps across the token dimension so that each word corresponds to a single CA map. This process produces a matrix of shape  $\\mathbb{R}^{R^2\\times N}$ . This matrix is then averaged across spatial dimension (i.e.,  $R^2$ ) to yield a single strength value for each visual concept  $C_n$ ,  $n = 1,\\dots ,N$ . We use the argmax operation on these  $N$  strength values to identify the visual concept with the largest value, which eliminates the problem of different representation scales across  $H$  CA heads (Appendix B.2). Then, we update that visual concept's HRV by increasing its  $h$ -th component by one. This update process is repeated over all head positions  $h = 1,\\ldots ,128$  and all timesteps  $t = 1,\\ldots ,50$  for 2100 random image generations. At the end, each HRV is normalized to have its  $L1$  norm equivalent to  $H = 128$ . Pseudo-code is provided in Appendix B.1.\n\n\n# 4 ORDERED WEAKENING ANALYSIS OF HEAD RELEVANCE VECTORS\n\n\nIn this section, we investigate whether the constructed HRVs can effectively and reliably serve as interpretable features. As the primary tool of analysis, we introduce an ordered weakening across the  $H$  cross-attention heads. Weakening of a target head is performed by multiplying  $-2$  to the corresponding CA maps of the head. This weakening is applied exclusively to the CA maps of semantic tokens, leaving the CA maps of special tokens unaffected. Additionally, the weakening is applied consistently across all timesteps. Using this simple weakening approach, we compare images generated while following two different orderings of head weakening. In the most relevant head positions first (MoRHF), we weaken  $H$  heads starting from the strongest to the weakest, where the strength of head  $h$  is defined as the value of the  $h$ -th element in the HRV. In the least relevant head positions first (LeRHF), the order is reversed, from the weakest to the strongest. If the HRV of a visual concept is indeed effective and reliable, we expect MoRHF to impact the corresponding visual concept in the generated images more quickly than LeRHF. The head weakening was inspired by a rescaling technique in P2P (Hertz et al., 2022) and the ordering was inspired by the metrics defined in Samek et al. (2016) and Tomsett et al. (2020).\n\n\nAnalysis results for three visual concepts are shown in Figure 3. Comparison of generated images are shown in Figure 3a. In the top case, where the visual concept of Material is weakened, the characteristics of copper in the generated image already disappeared when the most relevant 11 heads are weakened. In contrast, the copper remains visible until the least relevant 71 heads are weakened. A similar observation can be made for the visual concepts of Animals and Geometric Patterns. An additional 33 examples can be found in Figures 11 and 13-15 of Appendix C. It is noted that a caution is required when analyzing the results, especially for LeRHF. For the HRV of a given visual concept, the least relevant heads have little effect on the concept of interest but could be significantly relevant to other visual concepts. Therefore, interpreting the changes observed in the LeRHF-generated images requires careful consideration of other visual concepts.\n\n\nWe have also plotted the trends of CLIP image-text similarity in Figure 3b. CLIP similarity was measured between the generated images and the concept-words used in the prompts. For each data point in the CLIP score plots, we used between 30 and 150 images, depending on the specific visual concept. The prompt templates and concept-words used for each visual concept are detailed in Appendix C.1. These plots clearly show that relevant concepts are removed significantly faster during MoRHF weakening, while they are preserved for a longer duration in LeRHF weakening. Additional similarity plots for six more visual concepts can be found in Figure 12 of Appendix C.2.\n\n\n# 5 STEERING VISUAL CONCEPTS IN THREE VISUAL GENERATIVE TASKS\n\n\nHead relevance vectors are not only valuable as interpretable features but can also be used to steer visual concepts in generative tasks. In this section, we demonstrate that polysemous word challenges can be addressed and that state-of-the-art methods, such as P2P (Hertz et al., 2022) for image editing and Attend-and-Excite (A&E) (Chefer et al., 2023) for multi-concept generation, can be enhanced. All of our experiments are conducted using Stable Diffusion v1.4, 50 timesteps with PNDM sampling (Liu et al., 2022), and classifier-free guidance at a scale of 7.5. All CLIP-based metrics are calculated using the OpenCLIP ViT-H/14 model (Ilharco et al., 2021).\n\n\n![(a) Generated images as weakening progresses in either MoRHF or LeRHF order.](images/932cac461686d77276117e9edd1e14fd4f434cbd61816f95bfbbf3eb6c75e8d9.jpg)\n\n\n![(b) Change in CLIP image-text similarity score as weakening progresses in either MoRHF or LeRHF order.](images/8ca3f83df64a72bc44168ce19f4c098b90fe0b783d0d5e597190beacdd56cced.jpg)\n\n\n![](images/8d468947c47829766e3756c9d517ba0e7c0cefdff727428366f409271ea479ce.jpg)\n\n\n![](images/b81590ec2ecb441233d17b4920d39466daa16a07296560afca7dff20f5106a71.jpg)\n\n\n![Figure 3: Ordered weakening analysis for three visual concepts: The visual concept of interest disappears significantly faster with MoRHF, where the most relevant heads in the corresponding HRV are weakened first. Note that 128 corresponds to the weakening of all heads.](images/5005866fb061473883e6c26b0f5f31c9a2d12fbb00b30ee174d72a5c5b048d3b.jpg)\n\n\n![Figure 4: Two rescaling vectors for visual concept steering. Left: Concept strengthening uses HRV of a desired visual concept as the rescaling vector. Concept adjusting combines HRVs of a desired and an undesired visual concepts to define the rescaling vector:  $2 \\cdot$  (HRV of desired concept)  $-1 \\cdot$  (HRV of undesired concept). Here,  $H = 128$  denotes the number of CA heads. Right: For both concept steering methods, the  $h$ -th CA map of a target token is rescaled using  $r_h$ , the  $h$ -th element of the rescaling vector, where  $h = 1, \\ldots, H$ . Here,  $L = 77$  denotes the token length.](images/dabe26e6ba2d5e0cf39d840d1f708e8f9dd74e4c8a216c6d09ad1cc76bf4b2ee.jpg)\n\n\nTwo rescaling vectors for visual concept steering - concept strengthening and concept adjusting: We define two rescaling vectors as illustrated in Figure 4, both utilizing pre-constructed HRVs. In Section 3, we have constructed 34 HRVs for the 34 visual concepts listed in Table 3 of Appendix A. We utilize the pre-constructed HRVs to steer the corresponding concepts. For some visual generative tasks, only a desired concept can be identified, and we apply concept strengthening. In other tasks, both a desired and an undesired concept can be identified, typically in cases where the generative model fails to meet the user's intention. In such cases, we apply concept adjusting. The rescaling vector of concept adjusting is designed to be closely related to that of concept strengthening; the two vectors become equivalent when desired and undesired concepts are identical.\n\n\n# 5.1 IMAGE GENERATION-CORRECTING MISINTERPRETATION OF POLYSEMOUS WORDS\n\n\nThe same word can have different meanings depending on the context. Stable Diffusion (SD) models are known for misinterpreting such polysemous words, often generating images that do not comply with the user's intended meaning. Two examples are shown in Figure 5. SD fails to recognize that 'lavender' clearly refers to a color and 'Apple' to an electronic device. This misinterpretation by SD may arise from limitations in the CLIP text encoder, which has been reported to exhibit bag-of-words issues (Yuksekgonul et al., 2023a). The problem can be resolved by adopting concept adjusting to SD as shown in Figure 5, where we name our method as SD-HRV. For the lavender case, SD-HRV resolves this issue by applying con\n\n\n![Figure 5: Examples of image generations from Stable Diffusion (SD) and SD-HRV (ours) using prompts frequently misinterpreted by T2I models. SD-HRV effectively reduces misinterpretation compared to SD.](images/90cefe251519c14c1db2d126fadacf33b77e1c02c3c49f75a4cc7906fdd72a59.jpg)\n\n\ncept adjusting to the token 'lavender,' using Color as the desired visual concept and Plants as the undesired visual concept. For the Apple case, concept adjusting is applied to the token 'Apple,' using Brand Logos as the desired and Fruits as the undesired. Examples of 10 cases, including the 2 cases shown in Figure 5, can be found in Figures 18-19 of Appendix D.1. There, we provide 10 images generated with 10 random seeds for each case. Additionally, we investigated the performance of concept strengthening compared to concept adjusting and found, as expected, that concept strengthening performs worse. The details can be found in Appendix D.3.\n\n\nWe have also performed a human evaluation over the 10 cases, each with 10 random seeds, and the details can be found in Appendix D.2. Based on the human evaluation, the human perceived misinterpretation rate dropped significantly from  $63.0\\%$  to  $15.9\\%$  when SD-HRV was adopted.\n\n\n# 5.2 IMAGE EDITING - SUCCESSFUL EDITING FOR FIVE CHALLENGING VISUAL CONCEPTS\n\n\nImage editing involves generating an image that aligns with the target prompt while minimizing structural changes from the source image. Although recently developed methods excel at image editing, certain visual concepts remain challenging to edit. For example, concepts related to materials, geometric patterns, image styles, and weather conditions are known to be particularly difficult to edit. To address this problem, we propose applying concept strengthening to P2P (Hertz et al., 2022). We refer to our method as P2P-HRV. The key idea is to apply concept strengthening to the edited token, the token that describes how the attribute of the source image should be changed, thereby strengthening the concept related to the editing target. The detailed explanations of P2P-HRV method are provided in Appendix E.1.\n\n\nExperimental settings: We focus on five challenging visual concepts as editing targets, including three object attributes (Color, Material, and Geometric Patterns) and two image attributes (Image Styles and Weather Conditions). We compare P2P-HRV with SDEdit (Meng et al., 2021), P2P (Hertz et al., 2022), PnP (Tumanyan et al., 2023), MasaCtrl (Cao et al., 2023), and FPE (Liu et al., 2024). For each method, we generate 500 edited images for each editing target (250 for Weather Conditions) using the prompts described in Appendix E.2. For object attributes (Color, Material, and Geometric Patterns), we evaluated performance using both CLIP (Radford et al., 2021) and BG-DINO scores. The CLIP score evaluates CLIP image-text similarity between the edited image and the target prompt. The BG-DINO score assesses structure preservation using GroundedSAM-2 (Ravi et al., 2024; Ren et al., 2024) for extracting non-object parts from the source and edited images and then comparing them with DINOv2 (Oquab et al., 2023) embeddings. For image attributes (Image Styles and Weather Conditions), we conducted a human evaluation to assess human preference (HP) scores. This was necessary because evaluating structure preservation using BG-DINO is not meaningful for Image Styles and Weather Conditions, which require editing across the entire image. In the human evaluation, we compared P2P-HRV with the other methods by asking 'Which edited image better matches the target description, while maintaining essential details of the source image?' The HP-score was normalized so that P2P-HRV's preference score was set to 100.\n\n\n![Figure 6: Examples of image editing for the five challenging visual concepts. In these examples, all comparison methods frequently fail to make the desired edits, whereas our P2P-HRV successfully achieves the intended modifications.](images/4d719b19ea17b2264d1ee738919ff8f899d115414d4f2401c516b81015a55488.jpg)\n\n\n![Figure 7: Quantitative comparison of image editing methods for three object attributes using CLIP and BG-DINO scores.](images/c1f00b8f579a0eecd10fea8072360977d71431dd7c0bacafe45cad27bc7d6b63.jpg)\n\n\nTable 1: CLIP scores and human evaluation scores for two image attributes. The best and second-best results are highlighted in bold and underlined, respectively.\n\n\n<table><tr><td rowspan=\"3\">Method</td><td colspan=\"4\">Image Attribute</td></tr><tr><td colspan=\"2\">Image Style</td><td colspan=\"2\">Weather Conditions</td></tr><tr><td>CLIP</td><td>HP-score</td><td>CLIP</td><td>HP-score</td></tr><tr><td>SDEdit (0.5)</td><td>0.2938</td><td>-</td><td>0.2817</td><td>-</td></tr><tr><td>SDEdit (0.7)</td><td>0.3217</td><td>15.8</td><td>0.2908</td><td>39.5</td></tr><tr><td>P2P</td><td>0.3120</td><td>30.6</td><td>0.2788</td><td>33.9</td></tr><tr><td>PnP</td><td>0.3286</td><td>41.9</td><td>0.3046</td><td>35.0</td></tr><tr><td>MassaCtrl</td><td>0.2722</td><td>-</td><td>0.2524</td><td>-</td></tr><tr><td>FPE</td><td>0.3236</td><td>25.7</td><td>0.2962</td><td>35.5</td></tr><tr><td>P2P-HRV (Ours)</td><td>0.3424</td><td>100</td><td>0.3348</td><td>100</td></tr></table>\n\n\nExperimental results: Figure 6 presents five exemplary cases of image editing across the five challenging visual concepts. The figure demonstrates that our approach significantly improves image-text alignment compared to the previously known methods. While only five cases are shown in this figure, an extensive list of additional examples can be found in Figures 24-34 of Appendix E.5. For the editing of three object attributes, CLIP similarity and BG-DINO scores are presented in Figure 7. P2P-HRV achieves Pareto-optimal performance compared to previous SOTA methods across all three object attributes. For the editing of two image attributes, CLIP similarity and human preference performance are presented in Table 1. Our P2P-HRV improves CLIP perfor\n\n\nmance by  $4.20\\%$  on Image Style and  $9.91\\%$  on Weather Conditions compared to the previous SOTA, PnP. In the human evaluation, we have found that P2P-HRV received 2.39 and 2.53 times more votes in HP-scores for Image Style and Weather Conditions, respectively, compared to the second-best methods.\n\n\n# 5.3 MULTI-CONCEPT GENERATION - REDUCING CATASTROPHIC NEGLECT\n\n\nT2I generative models often struggle with multi-concept generation, failing to capture all the specified visual concepts in a prompt. Attend-and-Excite (A&E) (Chefer et al., 2023) tackles this issue, known as catastrophic neglect, by gradient updating the CA maps of selected tokens during image generation. To improve A&E further, we propose applying our concept strengthening to A&E. We refer to this enhancement method as A&E-HRV.\n\n\nExperimental settings: We investigated A&E-HRV using two types of prompts: (i) 'a {Animal A} and a {Animal B}' (Type 1) and (ii) 'a {Color A} {Animal A} and a {Color B} {Animal B}' (Type 2), originally examined in the A&E work (Chefer et al., 2023). Type 1 evaluates multi-object generation, while Type 2 adds the challenge of binding color attributes to each animal. In these experiments, we used 12 animals and 10 colors, as detailed in Appendix F. We assessed 66 prompts for Type 1 and 150 for Type 2, with 30 random seeds applied across all methods. Following A&E, we adopted three evaluation metrics. Full prompt similarity measures the CLIP similarity between the generated image and the full prompt. Minimum object similarity is calculated by measuring the CLIP image-text similarity between the image and two sub-prompts, which are created by splitting the original prompt at 'and.' The lower similarity score between the two sub-prompts is then reported. BLIP-score measures the CLIP text-text similarity between the prompt and the image caption generated with BLIP-2 (Li et al., 2023).\n\n\nExperimental results: Table 2 presents the quantitative comparisons, while Figure 8 shows qualitative results. As shown in Table 2, A&E-HRV outperforms other methods across all metrics and prompt types. In Figure 8, the top row shows results for Type 1 prompt, while the bottom row shows results for Type 2 prompt. The existing methods either neglect key visual concepts or fail to generate realistic images. In contrast, our approach captures all concepts and generates realistic images for both prompt types. Additional comparisons are provided in Figure 36 of Appendix F.2.\n\n\nTable 2: Quantitative results of multi-concept generation. The best and second-best results are highlighted in bold and underlined, respectively. The percentage in parentheses indicates the improvement over the second best result, A&E.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">Type1: a {Animal A} and a {Animal B}</td><td colspan=\"3\">Type2: a {Color A} {Animal A} and a {Color B} {Animal B}</td></tr><tr><td>Full Prompt</td><td>Min. Object</td><td>BLIP-score</td><td>Full Prompt</td><td>Min. Object</td><td>BLIP-score</td></tr><tr><td>Stable Diffusion</td><td>0.3000</td><td>0.1611</td><td>0.5934</td><td>0.3420</td><td>0.1458</td><td>0.5633</td></tr><tr><td>Structured Diffusion</td><td>0.2831</td><td>0.1545</td><td>0.5626</td><td>0.3307</td><td>0.1424</td><td>0.5508</td></tr><tr><td>Attend-and-Excite (A&amp;E)</td><td>0.3544</td><td>0.2017</td><td>0.7049</td><td>0.3883</td><td>0.1972</td><td>0.6373</td></tr><tr><td>A&amp;E-HRV (Ours)</td><td>0.3702 (+4.5%)</td><td>0.2078 (+3.0%)</td><td>0.7491 (+6.3%)</td><td>0.3971 (+2.3%)</td><td>0.2073 (+5.1%)</td><td>0.6580 (+3.2%)</td></tr></table>\n\n\n![Figure 8: Examples of multi-concept generation for Type 1 and Type 2 prompts. We compare Stable Diffusion (Rombach et al., 2022), Structured Diffusion (Feng et al., 2022), and Attend-and-Excite (Chefer et al., 2023) with ours.](images/e2e23822b7cef0f0c2f182fe226be54a444935e82f37b69dd840a5e96ff26f40.jpg)\n\n\n# 6 DISCUSSION\n\n\n# 6.1 EXTENSIONS TO A LARGER ARCHITECTURE\n\n\nThe recently introduced Stable Diffusion XL (SDXL) (Podell et al., 2023) adopts a U-Net backbone that is three times larger than its predecessors, scaling up to 1300 cross-attention (CA) heads. To investigate the generalization capability of HRV, we have performed an extended study with SDXL. Using SDXL, we conducted the ordered weakening analysis to evaluate whether HRVs can serve as interpretable features. An exemplary result for the visual concept Furniture is shown in Figure 9. The sofa disappeared when the most relevant 211 heads were weakened. In contrast, the sofa remained visible until the least relevant 711 heads were weakened. Additional 45 examples and the similarity plots for nine visual concepts can be found in Figures 37-42 of Appendix G.1. For SDXL experiments, we utilize the SD-XL 1.0-base model.\n\n\n![Figure 9: Ordered weakening analysis for SDXL: generated images are shown as weakening progresses in either MoRHF or LeRHF order.](images/5095e72452c6363c16e64966bfe66eeb3f88bf8ba3d93049ef23a7737d3bbf1f.jpg)\n\n\n# 6.2 DO GENERATION TIMESTEPS AFFECT HOW HEADS RELATE TO VISUAL CONCEPT?\n\n\nDiffusion models generate images by iteratively processing an image latent through the same U-Net network. A natural question is whether the patterns of head relevance vectors change across different timesteps during generation. To explore this, we calculated head relevance vectors for each visual concept at every timestep, resulting in 1700 vectors (34 visual concepts  $\\times$  50 timesteps). Figure 10 presents a t-SNE (Van der Maaten & Hinton, 2008) plot of these 1700 vectors. In the t-SNE plot, visual concepts are clearly separated, while timesteps are not. This indicates that generation timesteps do not significantly alter the patterns of head relevance vector of each visual concept. Further analysis, including cosine similarity plots, is provided in Appendix I.\n\n\n![Figure 10: t-SNE plot of 1700 head relevance vectors across visual concepts and generation timesteps.](images/42a30e6455495f57fe2225a55bd178586daf778dc6986bf0f5f7d78e932a9f85.jpg)\n\n\n# 6.3 LIMITATION\n\n\nWe examined the 34 concepts listed in Table 3 of Appendix A and identified two types of failure cases. The first type arises from limitations in the underlying T2I model, while the second is related to our proposed algorithm or the concept-words used to represent the concept. Appendix H provides a detailed discussion of these failure cases, along with examples for each, shown in Figures 45-46.\n\n\n# 7 CONCLUSION\n\n\nIn this work, we present findings from our exploration of cross-attention layers in T2I models. We demonstrate that head relevance vectors (HRVs) can be effectively and reliably constructed for human-specified visual concepts without requiring any modifications or fine-tuning of the T2I model. Furthermore, we show that HRVs can be successfully applied to improve performance of three visual generative tasks. Our work provides an advancement in understanding cross-attention layers and introduces novel approaches for exploiting these layers.\n\n\n# 9 ETHIC STATEMENT\n\n\nOur work presents new techniques for controlling and refining text-to-image diffusion models, enhancing both image generation and editing capabilities. While these advancements hold significant potential for creative and practical applications, they also raise ethical concerns about the possible misuse of generative models, such as creating manipulated media for disinformation. It is important to recognize that any image editing or generation tool can be used for both positive and negative purposes, making responsible use essential. Fortunately, various research in detecting harmful content and preventing malicious editing are making significant progress. We believe our detailed analysis of cross-attention layers will contribute to these efforts by providing a deeper understanding of the mechanisms behind image generation and editing.\n\n\n# REFERENCES\n\n- Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Mas-actr: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22560-22570, 2023.\n\n- Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):1-10, 2023.\n\n- Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement in variational autoencoders. Advances in neural information processing systems, 31, 2018.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255. IEEE, 2009.\n\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\n\n- Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022.\n\n- Yossi Gandelsman, Alexei A Efros, and Jacob Steinhardt. Interpreting clip's image representation via text-based decomposition. arXiv preprint arXiv:2310.05916, 2023.\n\n- Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. Multimodal neurons in artificial neural networks. Distill, 6(3):e30, 2021.\n\n- Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas. Natural language descriptions of deep features. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=NudBMY-tzDr.\n\n- Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.\n\n- Irina Higgins, Loic Matthew, Arka Pal, Christopher P Burgess, Xavier Glorot, Matthew M Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. *ICLR (Poster)*, 3, 2017.\n\n- Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/ zenodo.5143773. If you use this software, please cite it as below.\n\n- Jack Klys, Jake Snell, and Richard Zemel. Learning latent subspaces in variational autoencoders. Advances in neural information processing systems, 31, 2018.\n\n- Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse graphics network. Advances in neural information processing systems, 28, 2015.\n\n- Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 19730\u201319742. PMLR, 2023.\n\n- Bingyan Liu, Chengyu Wang, Tingfeng Cao, Kui Jia, and Jun Huang. Towards understanding cross and self-attention in stable diffusion for text-guided image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7817-7826, 2024.\n\n- Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022.\n\n- Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.\n\n- Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. The building blocks of interpretability. Distill, 3(3):e10, 2018.\n\n- Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001. https://distill.pub/2020/circuits/zoom-in.\n\n- OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Accessed: 2024-06-11.\n\n- Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.\n\n- Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pp. 1-11, 2023.\n\n- Antoine Plumerault, Herv\u00e9 Le Borgne, and C\u00e9line Hudelot. Controlling generative models with continuous factors of variations. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1laeJrKDB.\n\n- Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.\n\n- Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=di52zR8xgf.\n\n- PromptHero. https://prompthero.com/. Accessed: 2024-06-03.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.\n\n- Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, and Gal Chechik. Linguistic binding in diffusion models: Enhancing attribute correspondence through attention map alignment. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024.\n\n- Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684-10695, 2022.\n\n- Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp. 234-241. Springer, 2015.\n\n- Chitwan Sahara, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479-36494, 2022.\n\n- Wojciech Samek, Alexander Binder, Gregoire Montavon, Sebastian Lapuschkin, and Klaus-Robert M\u00fcller. Evaluating the visualization of what a deep neural network has learned. IEEE transactions on neural networks and learning systems, 28(11):2660-2673, 2016.\n\n- Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in gans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1532-1540, 2021.\n\n- Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monoseismicity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/scaling-monoseismicity/index.html.\n\n- Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun Preece. Sanity checks for saliency metrics. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 6021-6029, 2020.\n\n- Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1921-1930, 2023.\n\n- Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\n\n- Qiucheng Wu, Yujuan Liu, Handong Zhao, Trung Bui, Zhe Lin, Yang Zhang, and Shiyu Chang. Harnessing the spatial-temporal attention of diffusion models for high-fidelity text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7766-7776, 2023.\n\n- Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International Conference on Learning Representations, 2023a.\n\n- Mert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=nA5AZ8CEyow.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638632.7706046", "msecs": "770.0", "relativeCreated": "229601.10211372375", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# EEG-ImageNet: An Electroencephalogram Dataset and Benchmarks with Image Visual Stimuli of Multi-Granularity Labels\n\n# Abstract\nExploring how brain activity translates into visual perception offers valuable insights into the biological visual system's representation of the world. Recent advancements have enabled effective image classification and high-quality reconstruction using brain signals obtained through Functional Magnetic Resonance Imaging (fMRI) or magnetoencephalography (MEG). However, the cost and bulkiness of these technologies hinder their practical application. In contrast, Electroencephalography (EEG) presents advantages such as ease of use, affordability, high temporal resolution, and non-invasive operation, yet it remains underutilized in related research due to a shortage of comprehensive datasets. To fill this gap, we introduce EEG-ImageNet, a novel EEG dataset featuring recordings from 16 participants exposed to 4000 images sourced from the ImageNet dataset. This dataset offers five times the number of EEG-image pairs compared to existing benchmarks. EEG-ImageNet includes image stimuli labeled with varying levels of granularity, comprising 40 images with coarse labels and 40 with fine labels. We establish benchmarks for both object classification and image reconstruction based on this dataset. Experiments with several commonly used models show that the best-performing models can achieve object classification with an accuracy around 60% and image reconstruction with two-way identification around 64%. These findings highlight the dataset's potential to enhance EEG-based visual brain-computer interfaces, deepen our understanding of visual perception in biological systems, and suggest promising applications for improving machine vision models.\n\n\n# 1 INTRODUCTION\n\n\nRecent advancements in reconstructing visual experiences from the human brain have seen significant progress, largely driven by the extensive use of functional magnetic resonance imaging (fMRI) (Heeger and Ress (2002); Logothetis (2008); Logothetis et al. (2001)) and magnetoencephalogram (MEG) (Benchetrit et al. (2023)) datasets. fMRI and MEG are widely used to investigate various cognitive functions, neurological disorders, and brain connectivity patterns (Antonello et al. (2024); Ye et al. (2024); Toneva et al. (2022); Tang et al. (2024)). Driven by the use of deep neural networks, particularly diffusion-based and transformer-based models, it is even possible to reconstruct human's visual perceptions from fMRI or MEG recordings (Takagi and Nishimoto (2023); Scotti et al. (2024); Ozcelik and VanRullen (2023); Cheng et al. (2023)). These models, with their powerful generalization and learning capabilities, have accelerated advancements in the field. This success in reconstructing visual objects from fMRI signals is largely attributed to the availability of large-scale datasets, which offer comprehensive data essential for conducting extensive studies and in-depth analyses (Richards et al. (2019); Lin et al. (2014)). For example, the Natural Scenes Dataset (Allen et al. (2022)), contains up to hundreds of thousands of high-quality natural image-fMRI pairs of 8 subjects, providing a solid data foundation for recent work in visual neuroscience. These models and large-scale datasets, in turn, have opened new avenues for understanding the brain's intricate functions and for developing advanced applications in brain-computer interfaces, neuroimaging, and beyond (St-Yves et al. (2023)).\n\n\n![Figure 1: The overall procedure of our dataset construction and benchmark design. The experimental paradigm involves four stages: S1: Category Presentation (displaying the category label), S2: Fixation (500 ms), S3: Image Presentation (each image displayed for 500 ms), and S4: an optional random test to verify participant engagement. Each image presentation sequence includes 50 images from the given category, during which EEG signals are recorded. Data flow is indicated by blue arrows, while collected data is highlighted in gray. The stimuli images are sourced from ImageNet, with EEG signals aligned to image indices, granularity levels, and labels. The benchmarks (image reconstruction and object classification) are designed to evaluate coarse and fine granularity classification tasks.](images/7321d1a249a9a01d3cd7ff85dd00832cbf392b612a96773ecbefd7e77c9f3fc9.jpg)\n\n\nIn addition to fMRI and MEG, electroencephalography (EEG) is another vital tool in neuroscience research. EEG is easy to use, cost-efficient, and has high temporal resolution, making it a valuable tool for capturing rapid and real-time brain dynamics on the order of milliseconds (Teplan et al. (2002)). EEG signals can be obtained non-invasively by placing electrodes on the scalp, making it a less intrusive method for monitoring brain activity. These attributes position EEG as another promising modality for visual neuroscience research.\n\n\nAlthough visual reconstruction has been achieved using fMRI and MEG, the high cost and inconvenience of these devices limit their widespread application in practical settings. In contrast, EEG presents advantages over both fMRI and MEG with its cost-efficient and portable features. However, studies on visual perception with EEG signals are limited because of two challenges: (1) the lack of large-scale, high-quality EEG datasets and (2) existing EEG datasets typically featured coarse-grained image categories, lacking fine-grained categories.\n\n\nTo the best of our knowledge, the most frequently used dataset is the data set provided by Spampinato et al. (2017), which involves 6 participants each watching 2000 image stimuli. However, this dataset's scale is smaller than existing fMRI datasets, limiting the possibilities for investigating neural aspects related to visual perception and developing deep learning models for relevant visual classification and reconstruction tasks. The limited data volume of current EEG datasets limits research findings' statistical power and generalizability.\n\n\nOn the other hand, the labels in existing EEG datasets are frequently coarse and lack the granularity needed for detailed analysis. Multi-granularity labels are essential because they allow for a more nuanced analysis at different levels of detail. For instance, labels can range from broad categories like \"panda\" or \"golf ball\" to more specific attributes like \"Rottweiler\" or \"Samoyed\". These challenges underscore the necessity for new, large-scale EEG datasets with high-quality, multi-granularity labels. Such datasets would enable researchers to explore the intricacies of visual processing with\n\n\ngreater accuracy and depth, facilitating advancements in both basic neuroscience and applied fields like brain-computer interfaces and clinical diagnostics.\n\n\nTo address these challenges, we present EEG-ImageNet, a novel EEG dataset specifically designed to promote research related to visual neuroscience, biomedical engineering, etc. As shown in Figure 1, EEG-ImageNet is a comprehensive dataset that includes EEG recordings from 16 subjects, each exposed to 4,000 images sourced from the ImageNet-21k (Ridnik et al. (2021)). These images span 80 different categories, with 50 images per category. The dataset is structured to support multi-granularity analysis, with 40 categories dedicated to coarse-grained tasks and 40 to fine-grained tasks. We further establish benchmarks for two primary tasks: object classification and image reconstruction. For the object classification task, we evaluated the dataset using several commonly used models, achieving a best accuracy of  $60.88\\%$  on the 80-class classification. In the image reconstruction task, our experiments with advanced generative models, with the best model achieving a two-way identification of  $64.67\\%$ . These benchmarks demonstrate the dataset's potential for advancing EEG-based research. By addressing the lack of large-scale, high-quality EEG datasets, EEG-ImageNet aims to drive EEG-based visual research forward, improve machine learning models, and provide deeper insights into visual perception and processing.\n\n\n# 2 RELATED WORK\n\n\nIn this section, we review some datasets related to visual recognition and neuroscience and compare them with EEG-ImageNet, as shown in Table 1.\n\n\n# 2.1 VISUAL RECOGNITION DATASET\n\n\nVisual recognition is a cornerstone of computer vision, driven by datasets like ImageNet (Ridnik et al. (2021)), CIFAR (Krizhevsky et al. (2009)), and MS COCO (Lin et al. (2014)). ImageNet contains over 14 million annotated images, CIFAR-10/100 consist of thousands of  $32 \\times 32$  images in 10 and 100 classes, respectively, and MS COCO is known for its rich annotations supporting tasks such as object detection and segmentation.\n\n\nEfforts to combine visual recognition with neuroscience have led to datasets like the SALICON dataset (Jiang et al. (2015)), which extends MS COCO with eye-tracking data, enabling the study of visual attention and saliency through large-scale annotations. Neuroscience datasets utilizing fMRI have further enriched this field. The BOLD5000 dataset (Chang et al. (2019)) includes fMRI data from subjects viewing 5000 images, aiding the exploration of visual perception. The Generic Object Decoding dataset (Horikawa and Kamitani (2017)) captures brain activity while subjects view and imagine objects, facilitating the decoding of mental images. The VIM-1 dataset (Kay et al. (2008)) from the study \"Identifying Natural Images from Human Brain Activity\" demonstrates the feasibility of decoding viewed images from brain activity. Additionally, the NSD (Allen et al. (2022)) is a large-scale fMRI dataset in visual neuroscience, recording high-resolution (1.8-mm) whole-brain 7T fMRI data from eight subjects exposed to 9,000\u201310,000 color natural scenes from the MS COCO dataset over the course of a year. Integrating neuroimaging and eye-tracking datasets with visual recognition tasks has opened new avenues for understanding how the brain interprets visual information, leading to insights and applications in brain-computer interfaces and neural decoding.\n\n\n# 2.2 EEG DATASET\n\n\nfMRI is renowned for its high spatial resolution, allowing researchers to obtain detailed images of brain activity by measuring changes in blood flow (Logothetis et al. (2001)). This capability makes fMRI particularly effective for identifying the specific brain regions involved in various cognitive and sensory tasks. In contrast, EEG offers several distinct advantages over fMRI. EEG is relatively easy to use and cost-efficient, with a straightforward setup that involves placing electrodes on the scalp to measure electrical activity. One of the most significant benefits of EEG is its exceptional temporal resolution, which captures neural dynamics on the order of milliseconds (Teplan et al. (2002)). This high temporal resolution makes EEG ideal for studying fast-occurring brain processes and real-time neural responses, providing insights into the timing and sequence of neural events (Liu et al. (2021)). EEG's non-invasive nature also makes it suitable for a wider range of participants.\n\n\nTable 1: Detailed metadata for various neurological datasets based on visual stimuli.\n\n\n<table><tr><td>Dataset</td><td>#Subjects</td><td>Modalities</td><td>Visual Stimuli</td><td>#Stimuli</td><td>#Stimuli per Subject</td></tr><tr><td>SALICON (Jiang et al. (2015))</td><td>-</td><td>Eye-tracking</td><td>MS COCO</td><td>20,000</td><td>-</td></tr><tr><td>BOLD5000 (Chang et al. (2019))</td><td>4</td><td>fMRI</td><td>ImageNet, MS COCO, SceneUN (Xiao et al. (2010))</td><td>5,254</td><td>1,157-1,798</td></tr><tr><td>GOD (Horikawa and Kamitani (2017))</td><td>5</td><td>fMRI</td><td>ImageNet</td><td>1,200</td><td>1,200</td></tr><tr><td>VIM-1 (Kay et al. (2008))</td><td>2</td><td>fMRI</td><td>Corel Stock Photo Library (Joshi and Guerzhoy (2017))</td><td>1,870</td><td>1,870</td></tr><tr><td>NSD (Allen et al. (2022))</td><td>8</td><td>fMRI, Eye-tracking</td><td>MS COCO</td><td>9,000-10,000</td><td>22,000-30,000</td></tr><tr><td>SEED (Zheng and Lu (2015))</td><td>15</td><td>EEG</td><td>movie clips (4 min)</td><td>15</td><td>15</td></tr><tr><td>DEAP (Koelstra et al. (2011))</td><td>32</td><td>EEG, ECG, ...</td><td>music video (1 min)</td><td>40</td><td>40</td></tr><tr><td>AMIGOS (Miranda-Correa et al. (2018))</td><td>40</td><td>EEG, ECG, ...</td><td>long/short videos</td><td>4+16</td><td>4+16</td></tr><tr><td>Spampinato et al. (2017)</td><td>6</td><td>EEG</td><td>ImageNet</td><td>2000</td><td>2000</td></tr><tr><td>Things EEG1 (Grootswagers et al. (2022))</td><td>50</td><td>EEG</td><td>Things (Hebart et al. (2019))</td><td>22248</td><td>22248</td></tr><tr><td>Things EEG2 (Gifford et al. (2022))</td><td>10</td><td>EEG</td><td>Things</td><td>16740</td><td>16740</td></tr><tr><td>EEG-SVRRec (Zhang et al. (2024))</td><td>30</td><td>EEG, ECG</td><td>short videos</td><td>2636</td><td>121.9</td></tr><tr><td>EIT-1M (Zheng et al. (2024))</td><td>5</td><td>EEG</td><td>CIFAR-10</td><td>60,000</td><td>60,000</td></tr><tr><td>Alljoined (Xu et al. (2024))</td><td>8</td><td>EEG</td><td>MS COCO</td><td>10,000</td><td>10,000</td></tr><tr><td>EEG-ImageNet</td><td>16</td><td>EEG</td><td>ImageNet</td><td>4,000</td><td>4,000</td></tr></table>\n\n\nHowever, EEG signals collected using portable devices often have a low signal-to-noise ratio, which can complicate data analysis and reduce the accuracy of the results (Kannathal et al. (2005)).\n\n\nExisting EEG datasets span a variety of research areas. The SEED (Zheng and Lu (2015)) focuses on emotion recognition with detailed EEG recordings from subjects exposed to various emotional stimuli. The BCI Competition IV datasets (Zhang et al. (2012)) provide EEG data for motor imagery tasks, while the TUH EEG Corpus (Shah et al. (2018)) is a large clinical EEG collection often used for benchmarking EEG data quality across different conditions.\n\n\nIn the realm of visual recognition, datasets like the EEG-Classification dataset (Spampinato et al. (2017)) involve 6 subjects viewing 2,000 images across 40 object classes from the ImageNet10k. The DEAP (Koelstra et al. (2011)) collects EEG and peripheral physiological signals from 32 participants as they watch 40 one-minute music videos, providing comprehensive emotional responses. Similarly, the AMIGOS (Miranda-Correa et al. (2018)) captures EEG and physiological responses from participants watching short video clips designed to evoke specific emotional states. The Things EEG (Grootswagers et al. (2022); Gifford et al. (2022)) utilizes in the RSVP paradigm features extremely short image presentation durations (50/100 ms) and collects EEG-image pairs from large-\n\n\nscale datasets, making it a valuable resource for research in visual neuroscience. Another significant dataset is the EEG-SVRec (Zhang et al. (2024)), which includes EEG recordings from 30 participants interacting with short videos, aiming to capture detailed affective experiences.\n\n\nIn comparison, the EEG-ImageNet dataset offers several advantages. Firstly, its larger scale is conducive to training deep learning models. Secondly, the greater number of subjects facilitates intersubject experiments and analyses. Additionally, the dataset features multi-granularity image labels, and the images are of high quality, selected from ImageNet21k and filtered to exclude small, blurry, or watermarked pictures.\n\n\n# 3 DATASET CONSTRUCTION\n\n\nDuring the data collection process of our user study, participants are presented with a visual stimuli dataset containing 4000 natural images from ImageNet. Throughout this process, we continuously record their EEG signals. The whole experimental process is carried out in the laboratory environment. This section describes the entire process of EEG-ImageNet dataset construction. The detailed code can be accessed openly through the url https://anonymous.4open.science/r/EEG-ImageNet-Dataset-anonymous-237A.\n\n\n# 3.1 ETHICS AND PRIVACY\n\n\nTo protect participants' privacy and physical health, our user study adheres to strict ethical guidelines for human research, with approval from the ethics committee<sup>1</sup>. The study has undergone a comprehensive ethical review to safeguard participants' rights. In accordance with ethical standards, we have taken several steps to protect participants' privacy, including data anonymization and obtaining informed consent from all participants. Additionally, participants are thoroughly informed about the study's objectives, procedures, and potential outcomes. The EEG data collection method employed in this research is non-invasive and poses no risk to participants. This approach ensures compliance with ethical standards while maintaining the integrity of the research findings.\n\n\n# 3.2 PARTICIPANTS\n\n\nWe enlist a total of 16 participants via social media, including 10 males and 6 females. These participants are all college students aged between 21 and 27, with an average age of 24.06 and a standard deviation of 1.69. Their majors encompass computer science, mechanical engineering, chemistry, and environmental engineering, and they range from undergraduate to postgraduate levels. All participants are right-handed and assert their proficiency in utilizing image search engines in their daily routines. Each participant dedicates approximately 2 hours to complete the experiment, which includes 30 minutes for equipment setup and task instructions. Before the experiment, participants are informed of a compensation of US$11.8 per hour upon completion, to ensure the quality of the data collected for the study.\n\n\n# 3.3 STIMULIDATASET\n\n\nThe dataset used for visual stimuli was a subset of ImageNet21k, containing 80 categories of objects. Each category comprises 50 manually curated images, ensuring that each image has a width and height greater than 300 pixels and prominently features an object corresponding to its class label in ImageNet. Additionally, every image is free of watermarks. In this manner, we have selected a total of 4000 high-quality natural images as our visual stimulus dataset.\n\n\nAmong all categories, the first half is consistent with the EEG-Classification dataset (Spampinato et al. (2017)), comprising 40 significantly distinct categories from ImageNet1k. We treat these as coarse-grained tasks. The latter 40 categories are designed as a fine-grained task, divided into 5 groups with 8 categories each. The categories within the same group share the same parent node in WordNet, and each category label is either a leaf node or a sub-leaf node in WordNet. This selection ensures that the chosen categories represent similar granularity while avoiding overly obscure categories, thereby minimizing potential biases in the experimental results. For instance, coarse-grained\n\n\ncategories include items such as African elephants, pandas, mobile phones, golf balls, bananas, and pizzas. Under the parent node \"musical instruments,\" the fine-grained categories include accordions, cellos, flutes, oboes, snare drums, and trombones. Detailed information about all the visual stimuli categories and their respective WordNet IDs can be found in our GitHub repository.\n\n\n# 3.4 PROCEDURE\n\n\nA pilot study can ensure the correctness of the overall experimental process and the reliability of the acquisition equipment. We conduct a pilot study on two participants whose data are not included in the final analysis to determine hyperparameters of the experimental design, such as font size, fixation time, number of categories, etc.\n\n\nBefore engaging in the user study, participants are required to fill out an entry questionnaire and sign a consent about the protection of privacy security. They will receive an orientation regarding the primary tasks and operational procedures. Additionally, they will be notified of their right to withdraw from the study at any point. Before the main trials, participants will undergo a series of training trials designed to acquaint participants with the procedures of the formal experiments.\n\n\nEvery participant is required to select a random seed before the experiment to randomize the order of the categories. This randomization guarantees a fair distribution of categories and images among participants. The experimental platform follows a sequential and repetitive process as illustrated in Figure 1. (S1) The experimental platform presents the current category label. Participants can proceed to the next stage by pressing the space key. (S2) A fixation cross is shown at the center of the screen, ensuring attention is drawn when images are displayed. This fixation period lasts for  $500~\\mathrm{ms}$ . (S3) The 50 images of this category are sequentially presented using the Rapid Serial Visual Presentation (RSVP) paradigm, which is commonly employed in psychological experiments. Each image is presented for a duration of  $500~\\mathrm{ms}$  (Kaneshiro et al. (2015)). (S4) Random tests are conducted to verify the participant's engagement in the experiment after the presentation. Data from categories for which participants fail the test will not be included in final analyses. The EEG signals of the participant will be captured and recorded continuously during the entire process. The program will cycle back to step S1 and display the next category, repeating this process until all the images have been presented.\n\n\nUltimately, the dataset we construct includes the EEG signals of participants exposed to each image visual stimulus in each valid session, along with the corresponding category's wnid and the image's index in ImageNet21k.\n\n\n# 3.5 DATASET DESCRIPTION\n\n\nThe EEG-ImageNet dataset contains a total of 63,850 EEG-image pairs from 16 participants. Each EEG data sample has a size of (n_channels,  $f_{s} \\cdot T$ ), where n_channels is the number of EEG electrodes, which is 62 in our dataset;  $f_{s}$  is the sampling frequency of the device, which is  $1000\\mathrm{Hz}$  in our dataset; and T is the time window size, which in our dataset is the duration of the image stimulus presentation, i.e., 0.5 seconds. Due to ImageNet's copyright restrictions, our dataset only provides the file index of each image in ImageNet and the wnid of its category corresponding to each EEG segment. Additional information about the dataset is shown in Appendix A.1.\n\n\n# 4 BENCHMARKS\n\n\nIn this section, we detail the benchmarks of our study by outlining the preprocessing steps, feature extraction methods, task definitions, and models used.\n\n\n# 4.2 FEATURE EXTRACTION\n\n\nIn our benchmarks, for models requiring time-domain signals as direct input, we extract the 40ms-440ms segment of each EEG signal as the feature input. This approach helps to minimize the influence of preceding and subsequent image stimuli on the current stimulus. For models requiring frequency-domain features as input, we extract the differential entropy (DE) of the extracted time-domain signals as features, as this characteristic effectively captures the complexity and variability of brain activity in the frequency domain (Duan et al. (2013)). According to the general division in neuroscience, the frequency bands are categorized as delta  $(0.5 - 4\\mathrm{Hz})$ , theta  $(4 - 8\\mathrm{Hz})$ , alpha  $(8 - 13\\mathrm{Hz})$ , beta  $(13 - 30\\mathrm{Hz})$ , and gamma  $(30 - 80\\mathrm{Hz})$ . We use the Welch method with a sliding window to estimate the power spectral density  $P(f)$  in each frequency band. Then, we normalize the data and calculate the differential entropy (DE) using the formula,  $DE = -\\int P(f)\\log (P(f))df$ . Consequently, for each segment of EEG signals, we obtain the differential entropy (DE) for each electrode and each frequency band.\n\n\n# 4.3 TASK DEFINITION\n\n\nIn our benchmarks, we test our dataset on two tasks: object classification and image reconstruction. The object classification task aims to classify the category of the corresponding image stimulus the participant is exposed to with their EEG signals. We evaluate the models using classification accuracy. In the image reconstruction task, given a specific EEG segment, the goal is to reconstruct the image stimulus the participant is exposed to. We evaluate the generated results using two-way identification (Scotti et al. (2024)) under different visual neural networks (refer to Appendix A.3 for details.).\n\n\nOn our multi-granularity labeled image dataset, we test various tasks with different levels of granularity. Additionally, due to the inherent significant inter-individual differences in EEG signals, all models in our benchmarks are trained exclusively in an intra-subject experimental setup. Furthermore, to mitigate the significant temporal effects (Li et al. (2020)) observed in the EEG-Classification dataset, all our experimental setups strictly adhere to a dataset split where the first 30 images of each category are used as the training set, and the last 20 images are used as the test set. This approach ensures that the model learns features that are less correlated with irrelevant information. We strongly recommend that all researchers using the EEG-ImageNet dataset adopt a similar dataset split methodology. We also explain the impact of this split in Appendix A.5.\n\n\n# 4.4 MODELS\n\n\nIn the object classification task, we employ simple machine-learning classification models such as ridge regression, KNN, random forest, and SVM. Additionally, we implement deep learning models including MLP, EEGNet (Lawhern et al. (2018)), and RGNN (Zhong et al. (2020)). These models are the most commonly used and have demonstrated excellent performance in EEG-related research, which is why we have selected them for our study. MLP consists of two hidden layers, while EEGNet and RGNN are implemented using their original architectures. We use cross-entropy as the loss function. We train the models for each participant for 2000-3000 epochs on an NVIDIA 4090 GPU.\n\n\nIn the image reconstruction task, we use a Stable Diffusion 1.4 (Rombach et al. (2022)) as the backbone and train a two-layer MLP as an encoder to generate the prompt embeddings for input. For our reconstructions, we use 50 denoising timesteps with PNDM noise scheduling to generate  $512 \\times 512$  images. We use MSE as the loss function to align the encoder's output with the CLIP (ViT-L/14) (Radford et al. (2021)) embeddings of the captions of real images obtained through BLIP (Li et al. (2022)). The model for each participant is trained for 2000 epochs with on an NVIDIA 4090 GPU.\n\n\nFor model and training details, please refer to the open-source code and Appendix A.3.\n\n\nTable 2: The average results of all participants in the object classification task. * indicates the use of time-domain features, otherwise it indicates the use of frequency-domain features. \u2020 indicates that the difference compared to the best-performing model is significant with p-value; 0.05.\n\n\n<table><tr><td colspan=\"2\">Model</td><td>Acc (all)</td><td>Acc (coarse)</td><td>Acc (fine)</td></tr><tr><td rowspan=\"4\">Classic model</td><td>Ridge</td><td>0.286\u00b10.074\u2020</td><td>0.394\u00b10.081\u2020</td><td>0.583\u00b10.074\u2020</td></tr><tr><td>KNN</td><td>0.304\u00b10.086\u2020</td><td>0.401\u00b10.097\u2020</td><td>0.696\u00b10.068\u2020</td></tr><tr><td>RandomForest</td><td>0.349\u00b10.087\u2020</td><td>0.454\u00b10.105\u2020</td><td>0.729\u00b10.072\u2020</td></tr><tr><td>SVM</td><td>0.392\u00b10.086\u2020</td><td>0.506\u00b10.099\u2020</td><td>0.778\u00b10.054\u2020</td></tr><tr><td rowspan=\"6\">Deep model</td><td>MLP</td><td>0.404\u00b10.103\u2020</td><td>0.534\u00b10.115</td><td>0.816\u00b10.054</td></tr><tr><td>EEGNet*</td><td>0.260\u00b10.098\u2020</td><td>0.303\u00b10.108\u2020</td><td>0.365\u00b10.095\u2020</td></tr><tr><td>RGNN</td><td>0.405\u00b10.095\u2020</td><td>0.470\u00b10.092\u2020</td><td>0.706\u00b10.073\u2020</td></tr><tr><td>LSTM</td><td>0.356\u00b10.082\u2020</td><td>0.459\u00b10.101\u2020</td><td>0.745\u00b10.068\u2020</td></tr><tr><td>Transformer</td><td>0.367\u00b10.085\u2020</td><td>0.460\u00b10.108\u2020</td><td>0.750\u00b10.070\u2020</td></tr><tr><td>EEGConformer</td><td>0.416\u00b10.109</td><td>0.519\u00b10.108</td><td>0.801\u00b10.067</td></tr><tr><td colspan=\"2\">Model</td><td>F1 (all)</td><td>F1 (coarse)</td><td>F1 (fine)</td></tr><tr><td rowspan=\"4\">Classic model</td><td>Ridge</td><td>0.261\u00b10.070\u2020</td><td>0.373\u00b10.082\u2020</td><td>0.610\u00b10.121\u2020</td></tr><tr><td>KNN</td><td>0.286\u00b10.081\u2020</td><td>0.380\u00b10.096\u2020</td><td>0.717\u00b10.132\u2020</td></tr><tr><td>RandomForest</td><td>0.323\u00b10.083\u2020</td><td>0.425\u00b10.099\u2020</td><td>0.723\u00b10.092\u2020</td></tr><tr><td>SVM</td><td>0.378\u00b10.083\u2020</td><td>0.486\u00b10.105\u2020</td><td>0.770\u00b10.054\u2020</td></tr><tr><td rowspan=\"6\">Deep model</td><td>MLP</td><td>0.397\u00b10.100\u2020</td><td>0.523\u00b10.108</td><td>0.819\u00b10.053</td></tr><tr><td>EEGNet*</td><td>0.251\u00b10.095\u2020</td><td>0.291\u00b10.098\u2020</td><td>0.374\u00b10.102\u2020</td></tr><tr><td>RGNN</td><td>0.401\u00b10.098\u2020</td><td>0.455\u00b10.087\u2020</td><td>0.723\u00b10.079\u2020</td></tr><tr><td>LSTM</td><td>0.347\u00b10.076\u2020</td><td>0.437\u00b10.092\u2020</td><td>0.729\u00b10.058\u2020</td></tr><tr><td>Transformer</td><td>0.353\u00b10.079\u2020</td><td>0.451\u00b10.096\u2020</td><td>0.761\u00b10.075\u2020</td></tr><tr><td>EEGConformer</td><td>0.413\u00b10.102</td><td>0.513\u00b10.101</td><td>0.812\u00b10.070</td></tr></table>\n\n\n# 5 EXPERIMENTS, RESULTS\n\n\nIn this chapter, we present the experimental results of our study, focusing on two benchmark tasks: object classification and image reconstruction.\n\n\n# 5.1 OBJECT CLASSIFICATION\n\n\nIn the object classification task, we conduct experiments at three levels of granularity: all, coarse, and fine. The \"all\" category represents the 80-class classification accuracy, the \"coarse\" category represents the 40-class classification accuracy, and the \"fine\" category represents the average accuracy of five 8-class classification tasks. The performance of each model is detailed in Table 2, which shows the average results of all participants.\n\n\nThe table shows that RGNN achieves the highest accuracy in the \"all\" task among all models, with an 80-class classification accuracy reaching  $40.50\\%$ . MLP slightly lags behind RGNN in the \"all\" task accuracy, but it significantly outperforms in the \"coarse\" and \"fine\" tasks, with a 40-class accuracy of  $53.39\\%$  and an average 8-class accuracy of  $81.63\\%$ . Among the classical machine learning models, SVM exhibits the highest classification performance, only slightly trailing RGNN and MLP across the three different granularity scenarios. The lower accuracy of RGNN in the \"fine\" task might be due to the complex model structure, which may require more fine-tuning to better adapt to easier tasks. Meanwhile, the lower performance of time-domain features compared to frequency-domain features suggests that frequency-domain features might be more informative for the classification tasks in this study. Specific performance details for each participant in each task are provided in Appendix A.4. We find that the ranking of participants' accuracy is relatively consistent across different models.\n\n\nWe compile the accuracy of each participant for the \"all\" task under the SVM, MLP, and RGNN models, as shown in Figure 2a. We observe significant differences between participants, indicating high variability in individual responses. The best-performing participant achieves an accuracy of\n\n\n![(a) Classification accuracy for each participant in the object classification task (all) across models.](images/1254a196b0711769ec6bda016c94d53a3158b865ae81efa74222326e7a3fbb62.jpg)\n\n\n![(b) Classification accuracy of the \"coarse\" (vertical axis) and \"fine\" task (horizontal axis).](images/f7e39069c9b475afaa1c1c616aadb7a25f505462ecbe3f9e0d1da8de47072e1a.jpg)\n\n\n![Figure 2: Classification performance of the object classification, each dot represents the classification performance of a single participant. Figure 3: The image reconstruction results of a single participant (S8).](images/4cab87c2bce3e7a5e71d0d4e826ce197589a5584d8ebf3703b9cc7bcd9511f5c.jpg)\n\n\n$60.88\\%$ . To better compare the differences between \"fine\" and \"coarse\" tasks in EEG-ImageNet, we modify the coarse-grained task. We randomly select 8 coarse-grained categories and use the RGNN model for training and testing. This process is repeated 5 times, and the average accuracy for each participant is calculated and plotted alongside their average \"fine\" task accuracy in Figure 2b. We then perform linear regression on the data points, and the resulting function has a slope greater than 1, indicating that participants generally achieve better results on the \"coarse\" classification tasks. This finding is also consistent with intuition.\n\n\n# 5.2 IMAGE RECONSTRUCTION\n\n\nFigure 3 shows some of the results from our image reconstruction pipeline of a single participant (S8). From the selected images, it can be seen that the reconstruction pipeline can effectively restore the category information of the image stimuli. However, restoring low-level details such as color, position, and shape is inaccurate. This may be because we align the EEG signals to the image captions obtained from BLIP. Due to the limited descriptive precision of BLIP, we cannot fully leverage the diffusion model's generative capabilities.\n\n\nTable 3 shows the average two-way identification (chance  $= 50\\%$ ) of the images generated by our reconstruction pipeline using different visual neural networks. Two-way identification refers to comparing the generated image with the original and one distractor to evaluate accuracy. The highest accuracy achieved by CLIP suggests that integrating vision transformers with extensive pre-training on diverse image-text data can significantly enhance model performance for complex tasks like image reconstruction from EEG signals. Additional results can be found in Appendix A.4.\n\n\nTable 3: The average results of all participants in the image reconstruction task.\n\n\n<table><tr><td>Method</td><td>Alex(2)</td><td>Alex(5)</td><td>Incep</td><td>CLIP(ViT-L/14)</td><td>Eff</td><td>SwAV</td></tr><tr><td>Metrics</td><td>56.05%</td><td>62.99%</td><td>56.75%</td><td>64.67%</td><td>0.867</td><td>0.597</td></tr></table>\n\n\n# 6 DISCUSSION AND CONCLUSION\n\n\nIn this paper, we introduced EEG-ImageNet, a novel EEG dataset designed to advance research in visual neuroscience. EEG-ImageNet is comprehensive and richly annotated, comprising EEG recordings from 16 subjects exposed to 4000 images from 80 different categories. We established benchmarks for two primary tasks using EEG-ImageNet: object classification and image reconstruction. The introduction of EEG-ImageNet and the established benchmarks provide a robust foundation for subsequent research. We outline the limitations of our dataset and explore how it could guide future research efforts to advance machine learning and brain-computer interface design.\n\n\nLimitation. Firstly, while our dataset is more comprehensive than similar works, each participant's data is still relatively limited. This necessitates the development of inter-subject models to overcome this limitation and enhance generalizability. Additionally, it is limited in representation, as participants were drawn from a convenience sample at our university. This results in an age distribution skewed towards college-aged individuals and a racial composition predominantly White and Asian, which limits the dataset's generalizability. Future work should aim to include a more diverse and extensive participant pool. Secondly, while we employed methods such as reducing the segment length of each EEG recording and sequentially splitting the training and test sets to mitigate the temporal effect, we were unable to eliminate it completely. Future work should explore more sophisticated techniques to address this issue. Lastly, our benchmarks did not incorporate many of the latest deep-learning methods. We believe that recent advancements in deep learning could greatly benefit from our comprehensive dataset, potentially leading to significant breakthroughs in visual neuroscience.\n\n\nInsight for ML. The EEG-ImageNet dataset provides a comprehensive resource for developing models in visual recognition tasks, enabling the development of sophisticated deep-learning models capable of capturing intricate patterns within EEG data. Future research could leverage the dataset to enhance domain adaptation and transfer learning techniques, facilitating effective inter-subject task completion. Researchers might develop state-of-the-art models for the benchmarks we defined, or even create new tasks. By offering a diverse set of visual stimuli and supporting multi-level classification tasks, EEG-ImageNet could foster the creation of hierarchical models that mirror human cognitive processes and improve the generalization capabilities of machine learning algorithms.\n\n\nInsight for BCI. As hardware technology progresses, portable EEG devices are becoming increasingly feasible, offering new opportunities for real-time BCI applications. Researchers could use the dataset to develop robust BCI systems that accurately interpret user intent from EEG signals. The comprehensive size and diverse visual stimuli in EEG-ImageNet allow for the creation of adaptive BCI systems that learn and respond to individual user patterns. This paves the way for personalized neurotechnology solutions, particularly enhancing human-computer interaction for individuals with disabilities. Furthermore, addressing privacy protection and ethical concerns will be crucial as BCI technology advances, ensuring user data is securely handled and individual rights are respected.\n\n\nEthics Statement. Our dataset was collected from human participants, making privacy protection and ethical use of the data a priority. The user study received approval from the institutional ethics committee, and all participants provided informed consent, agreeing to the public release of the anonymized data for research purposes. It is important to note that our dataset exhibits certain biases, including racial bias, which was influenced by the demographics of available participants. Researchers using this dataset should be aware of this limitation and consider it in their analysis. Additionally, all participants were right-handed. This decision was made to control for variability, as previous research has shown significant differences in EEG signals between right- and left-handed individuals Provins and Cunliffe (1972). By selecting only right-handed participants, we aimed to ensure the consistency and reliability of the EEG signals across subjects. Researchers should consider this factor when conducting studies where handedness is not a variable of interest to minimize variability in the results.\n\n\n# REFERENCES\n\n\nEmily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. A massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, 25(1):116-126, 2022.\n\n- Richard Antonello, Aditya Vaidya, and Alexander Huth. Scaling laws for language encoding models in fmri. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Yohann Benchetrit, Hubert Banville, and Jean-R\u00e9mi King. Brain decoding: toward real-time reconstruction of visual perception. arXiv preprint arXiv:2310.19812, 2023.\n\n- Nadine Chang, John A Pyles, Austin Marcus, Abhinav Gupta, Michael J Tarr, and Elissa M Aminoff. Bold5000, a public fmri dataset while viewing 5000 visual images. Scientific data, 6(1):49, 2019.\n\n- Fan L Cheng, Tomoyasu Horikawa, Kei Majima, Misato Tanaka, Mohamed Abdelhack, Shuntaro C Aoki, Jin Hirano, and Yukiyasu Kamitani. Reconstructing visual illusory experiences from human brain activity. Science Advances, 9(46):eadj3906, 2023.\n\n- Ruo-Nan Duan, Jia-Yi Zhu, and Bao-Liang Lu. Differential entropy feature for eeg-based emotion classification. In 2013 6th international IEEE/EMBS conference on neural engineering (NER), pages 81-84. IEEE, 2013.\n\n- Alessandro T Gifford, Kshitij Dwivedi, Gemma Roig, and Radoslaw M Cichy. A large and rich eeg dataset for modeling human visual object recognition. NeuroImage, 264:119754, 2022.\n\n- Tijl Grootswagers, Ivy Zhou, Amanda K Robinson, Martin N Hebart, and Thomas A Carlson. Human eeg recordings for 1,854 concepts presented in rapid serial visual presentation streams. Scientific Data, 9(1):3, 2022.\n\n- Martin N Hebart, Adam H Dickter, Alexis Kidder, Wan Y Kwok, Anna Corriveau, Caitlin Van Wicklin, and Chris I Baker. Things: A database of 1,854 object concepts and more than 26,000 naturalistic object images. *PloS one*, 14(10):e0223792, 2019.\n\n- David J Heeger and David Ress. What does fmri tell us about neuronal activity? Nature reviews neuroscience, 3(2):142-151, 2002.\n\n- Tomoyasu Horikawa and Yukiyasu Kamitani. Generic decoding of seen and imagined objects using hierarchical visual features. Nature communications, 8(1):15037, 2017.\n\n- Ming Jiang, Shengsheng Huang, Juanyong Duan, and Qi Zhao. Silicon: Saliency in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1072-1080, 2015.\n\n- Ujash Joshi and Michael Guerzhoy. Automatic photo orientation detection with convolutional neural networks. In 2017 14th Conference on Computer and Robot Vision (CRV), pages 103-108. IEEE, 2017.\n\n- Blair Kaneshiro, Marcos Perreau Guimaraes, Hyung-Suk Kim, Anthony M Norcia, and Patrick Suppes. A representational similarity analysis of the dynamics of object processing using single-trial eeg classification. Plos one, 10(8):e0135697, 2015.\n\n- N Kannathal, U Rajendra Acharya, Choo Min Lim, and PK Sadasivan. Characterization of eeg\u2014a comparative study. Computer methods and Programs in Biomedicine, 80(1):17-23, 2005.\n\n- Kendrick N Kay, Thomas Naselaris, Ryan J Prenger, and Jack L Gallant. Identifying natural images from human brain activity. Nature, 452(7185):352-355, 2008.\n\n- Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emotion analysis; using physiological signals. IEEE transactions on affective computing, 3(1):18-31, 2011.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\n- Vernon J Lawhern, Amelia J Solon, Nicholas R Waytowich, Stephen M Gordon, Chou P Hung, and Brent J Lance. Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces. Journal of neural engineering, 15(5):056013, 2018.\n\n- Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pages 12888-12900. PMLR, 2022.\n\n- Ren Li, Jared S Johansen, Hamad Ahmed, Thomas V Ilyevsky, Ronnie B Wilbur, Hari M Bharadwaj, and Jeffrey Mark Siskind. The perils and pitfalls of block design for eeg classification experiments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(1):316-333, 2020.\n\n- Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr D\u00f3ll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.\n\n- Yiqun Liu, Jiaxin Mao, Xiaohui Xie, Min Zhang, and Shaoping Ma. Challenges in designing a brain-machine search interface. In ACM SIGIR Forum, volume 54, pages 1-13. ACM New York, NY, USA, 2021.\n\n- Nikos K Logothetis. What we can do and what we cannot do with fmri. Nature, 453(7197):869-878, 2008.\n\n- Nikos K Logothetis, Jon Pauls, Mark Augath, Torsten Trinath, and Axel Oeltermann. Neurophysiological investigation of the basis of the fmri signal. nature, 412(6843):150-157, 2001.\n\n- Juan Abdon Miranda-Correa, Mojtaba Khomami Abadi, Nicu Sebe, and Ioannis Patras. Amigos: A dataset for affect, personality and mood research on individuals and groups. IEEE transactions on affective computing, 12(2):479-493, 2018.\n\n- Furkan Ozcelik and Rufin VanRullen. Natural scene reconstruction from fmri signals using generative latent diffusion. Scientific Reports, 13(1):15666, 2023.\n\n- KA Provins and Penny Cunliffe. The relationship between eeg activity and handedness. Cortex, 8 (2):136-146, 1972.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.\n\n- Blake A Richards, Timothy P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal Bogacz, Amelia Christensen, Claudia Clopath, Rui Ponte Costa, Archy de Berker, Surya Ganguli, et al. A deep learning framework for neuroscience. Nature neuroscience, 22(11):1761-1770, 2019.\n\n- Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.\n\n- Paul Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Aidan Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth Norman, et al. Reconstructing the mind's eye: fmri-to-image with contrastive learning and diffusion priors. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Vinit Shah, Eva Von Weltin, Silvia Lopez, James Riley McHugh, Lillian Veloso, Meysam Golmohammadi, Iyad Obeid, and Joseph Picone. The temple university hospital seizure detection corpus. Frontiers in neuroinformatics, 12:83, 2018.\n\n- Concetto Spampinato, Simone Palazzo, Isaak Kavasidis, Daniela Giordano, Nasim Souly, and Mubarak Shah. Deep learning human mind for automated visual classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6809-6817, 2017.\n\n- Ghislain St-Yves, Emily J Allen, Yihan Wu, Kendrick Kay, and Thomas Naselaris. Brain-optimized deep neural network models of human visual areas learn non-hierarchical representations. Nature communications, 14(1):3329, 2023.\n\n- Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from human brain activity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14453-14463, 2023.\n\n- Jerry Tang, Meng Du, Vv Vo, Vasudev Lal, and Alexander Huth. Brain encoding models based on multimodal transformers can transfer across language and vision. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Michal Teplan et al. Fundamentals of eeg measurement. Measurement science review, 2(2):1-11, 2002.\n\n- Mariya Toneva, Jennifer Williams, Anand Bollu, Christoph Dann, and Leila Wehbe. Same cause; different effects in the brain. arXiv preprint arXiv:2202.10376, 2022.\n\n- Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485-3492. IEEE, 2010.\n\n- Jonathan Xu, Bruno Aristimunha, Max Emanuel Feucht, Emma Qian, Charles Liu, Tazik Shahjahan, Martyna Spyra, Steven Zifan Zhang, Nicholas Short, Jioh Kim, et al. Alljoined-a dataset for eegt-to-image decoding. arXiv preprint arXiv:2404.05553, 2024.\n\n- Dezhong Yao, Yun Qin, Shiang Hu, Li Dong, Maria L Bringas Vega, and Pedro A Vald\u00e9s Sosa. Which reference should we use for eeg and erp practice? Brain topography, 32:530-549, 2019.\n\n- Ziyi Ye, Xiaohui Xie, Qingyao Ai, Yiqun Liu, Zhihong Wang, Weihang Su, and Min Zhang. Relevance feedback with brain signals. ACM Transactions on Information Systems, 42(4):1-37, 2024.\n\n- Haihong Zhang, Cuntai Guan, Kai Keng Ang, Chuanchu Wang, and Zheng Yang Chin. Bci competition iv-data set i: learning discriminative patterns for self-paced eeg-based motor imagery detection. Frontiers in neuroscience, 6:7, 2012.\n\n- Shaorun Zhang, Zhiyu He, Ziyi Ye, Peijie Sun, Qingyao Ai, Min Zhang, and Yiqun Liu. Eegsvrec: An eeg dataset with user multidimensional affective engagement labels in short video recommendation. arXiv preprint arXiv:2404.01008, 2024.\n\n- Wei-Long Zheng and Bao-Liang Lu. Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks. IEEE Transactions on autonomous mental development, 7(3):162-175, 2015.\n\n- Xu Zheng, Ling Wang, Kanghao Chen, Yuanhuiyi Lyu, Jiazhou Zhou, and Lin Wang. Eit-1m: One million eeg-image-text pairs for human visual-textual recognition and more. arXiv preprint arXiv:2407.01884, 2024.\n\n- Peixiang Zhong, Di Wang, and Chunyan Miao. Eeg-based emotion recognition using regularized graph neural networks. IEEE Transactions on Affective Computing, 13(3):1290-1301, 2020.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638633.849408", "msecs": "849.0", "relativeCreated": "230679.9054145813", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image Editing\n\n# Abstract\nRecent advances in diffusion models have introduced a new era of text-guided image manipulation, enabling users to create realistic edited images with simple textual prompts. However, there is significant concern about the potential misuse of these methods, especially in creating misleading or harmful content. Although recent defense strategies, which introduce imperceptible adversarial noise to induce model failure, have shown promise, they remain ineffective against more sophisticated manipulations, such as editing with a mask. In this work, we propose DiffusionGuard, a robust and effective defense method against unauthorized edits by diffusion-based image editing models, even in challenging setups. Through a detailed analysis of these models, we introduce a novel objective that generates adversarial noise targeting the early stage of the diffusion process. This approach significantly improves the efficiency and effectiveness of adversarial noises. We also introduce a mask-augmentation technique to enhance robustness against various masks during test time. Finally, we introduce a comprehensive benchmark designed to evaluate the effectiveness and robustness of methods in protecting against privacy threats in realistic scenarios. Through extensive experiments, we show that our method achieves stronger protection and improved mask robustness with lower computational costs compared to the strongest baseline. Additionally, our method exhibits superior transferability and better resilience to noise removal techniques compared to all baseline methods.\n\n\n# 1 INTRODUCTION\n\n\nText-to-image diffusion models trained on large-scale datasets have demonstrated impressive results in generating high-quality images from text prompts (Betker et al., 2023; Sauer et al., 2024; Sahara et al., 2022b). These models have expanded beyond simple image generation to support text-guided image editing (Wang et al., 2023; Brooks et al., 2023; Yenphraphai et al., 2024), enabling users to modify existing images with both ease and precision. For instance,Imagen Editor (Wang et al., 2023) allows users to manipulate images using masks and textual descriptions. This facilitates detailed and intuitive adjustments to specific areas of an image. Similarly, Image Sculpting (Yenphraphai et al., 2024) identifies 3D objects in photos. With this tool, users can then manipulate these objects directly, unlocking new possibilities for altering images. These approaches improve the user-friendliness of image editing tools, which significantly enhance the creative process by allowing for precise modifications based on textual input.\n\n\nHowever, alongside these advances, there exists a significant concern regarding the potential misuse of text-guided image editing models. With their ability to create highly realistic and convincing content, image editing models can be exploited for malicious purposes such as generating fake news, spreading disinformation, and creating deceptive visual content. For example, with open-sourced text-to-image models (Rombach et al., 2023), one could easily manipulate a photo to falsely depict a celebrity being arrested, as shown in the bottom row of Fig. 1. As text-to-image models become more powerful, it is paramount to address these risks and implement safeguards to prevent misuse.\n\n\nTo mitigate the potential risks associated with the misuse of text-to-image diffusion models, protection methods based on adversarial noises have shown promise recently (Liang et al., 2023; Liang & Wu, 2023; Salman et al., 2024; Xue et al., 2024). These techniques involve adding imperceptible adversarial noise to the original image, which is designed to cause the model to fail in generating\n\n\n![Figure 1: Protecting against the misuse of text-to-image models using DiffusionGuard. (Bottom row) Images without protection are vulnerable to malicious editing, such as altering the background while preserving the face to create fake images (e.g., a celebrity being arrested). (Top row) DiffusionGuard protects the image by focusing on the face, a defining feature of personal identity. It disrupts diffusion models and results in failed edits when attackers attempt malicious changes.](images/9300886af9b7ebf06a882c1b6a8b4f4475654bf90bb2204f4e8a7fcc85fadbfe.jpg)\n\n\nhigh-quality images (see Fig. 1). By publishing images with this adversarial noise, the cost and difficulty of malicious editing are significantly increased. However, current methods do not provide robust protection against real-life scenarios, such as editing with freely chosen masks by adversaries, which can bypass the protection. This issue is especially problematic as adversaries may select the smallest possible region containing sensitive identities (e.g., a person's face), thereby minimizing the effectiveness of these protection methods.\n\n\nContributions In this work, we introduce DiffusionGuard, a robust and effective defense method against text-guided image editing models in challenging setups, such as editing with user-selected masks. Specifically, we propose a novel objective to generate adversarial noises targeting the early stage of the diffusion process. Through our analysis, we observe that editing models tend to generate key regions within the mask during these initial diffusion steps. Therefore, by directing adversarial perturbations at the early stages, we prevent the models from maintaining the key regions, which are crucial for high-quality editing. Additionally, we propose a mask-augmentation method to find robust adversarial perturbations that are effective against mask inputs of various shapes.\n\n\nFor concrete evaluation, we introduce InpaintGuardBench, a challenging evaluation benchmark designed to assess defense methods against image editing models. InpaintGuardBench comprises images paired with handcrafted masks of diverse shapes and text prompts for editing, enabling a comprehensive evaluation of robustness against various misuse scenarios. We conduct human surveys and measure qualitative metrics to evaluate DiffusionGuard. Through extensive experiments, we demonstrate both qualitatively and quantitatively that DiffusionGuard is effective, and most importantly, robust against changes in mask inputs. This makes it exceptionally useful in real-life scenarios. Moreover, our method proves to be more compute-efficient and performs well even in low noise budget setups compared to existing baselines (Liang et al., 2023; Salman et al., 2024).\n\n\n# 2 PRELIMINARIES\n\n\nThis section provides an overview of text-to-image diffusion models, emphasizing inpainting models and adversarial examples against them.\n\n\n# 2.1 DIFFUSION MODELS\n\n\nWe consider denoising diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Dhariwal & Nichol, 2021) in discrete time. Suppose  $\\mathbf{x} \\sim p_{\\mathrm{data}}(\\mathbf{x})$  represents the data distribution. A diffusion model defines a sequence of latent variables with noise scheduling functions  $\\alpha_{t}, \\sigma_{t}$  such that the log signal-to-noise ratio  $\\lambda_{t} = \\log (\\alpha_{t}^{2} / \\sigma_{t}^{2})$  decreases with  $t$ . The forward process of a diffusion model is described by gradually adding noise to the data  $\\mathbf{x}$ , where the marginal distribution is given as  $q(\\mathbf{x}_t|\\mathbf{x}) = \\mathcal{N}(\\mathbf{x}_t; \\alpha_t\\mathbf{x}, \\sigma_t^2\\mathbf{I})$ . For sufficiently large  $\\lambda_T$ ,  $\\mathbf{x}_T$  becomes indistinguishable from pure\n\n\nGaussian noise, and for sufficiently small  $\\lambda_0$ ,  $\\mathbf{x}_0$  is nearly identical to the data distribution. The reverse process starts from random noise  $\\mathbf{x}_T$ , and sequentially denoises it to generate  $\\mathbf{x}_0$ , which matches the training distribution.\n\n\nText-to-Image diffusion models Text-to-image (T2I) diffusion models (Rombach et al., 2023; Sahara et al., 2022b; Betker et al., 2023) are a class of diffusion models specifically designed to generate images conditioned on text prompts. These models incorporate text embeddings extracted from pre-trained text encoders like T5 (Raffel et al., 2020) or CLIP (Radford et al., 2021) to guide the image generation process. Given a pair of image  $\\mathbf{x}$  and text  $y_{\\mathrm{text}}$ , these models commonly employ a noise prediction model  $\\epsilon_{\\theta}(\\mathbf{x}_t; t)$  and are trained using a noise prediction loss as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {d i f f}} (\\theta ; \\mathbf {x}) = \\mathbb {E} _ {t \\sim \\mathcal {U} (1, T), \\epsilon \\sim \\mathcal {N} (\\mathbf {0}, \\mathbf {I})} \\left[ \\omega \\left(\\lambda_ {t}\\right) \\| \\epsilon_ {\\theta} (\\mathbf {x} _ {t}; y _ {\\text {t e x t}}, t) - \\epsilon \\| _ {2} ^ {2} \\right], \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\omega (\\lambda_t)$  is a weighting function of a timestep  $t$\n\n\nText-guided inpainting models In addition to T2I generation, it is of a great interest to edit a desired region of a given image with text prompts. To this end, T2I image inpainting models (Nichol et al., 2022; Sahara et al., 2022c;a) propose to fine-tune pretrained T2I diffusion models to leverage its rich generative prior. In specific, inpainting models are fine-tuned by adding conditions of source image  $\\mathbf{x}_{\\mathrm{src}}$  and binary mask  $M$  that designates the region to infill to the noise prediction loss in Eq. 1. During fine-tuning, random regions of an image are masked, and the source image and a mask are concatenated to the noisy latent  $\\mathbf{x}_t$  as an input of the diffusion model. The training objective of diffusion-based inpainting models is given as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {I n p a i n t}} (\\theta ; \\mathbf {x} _ {\\text {s r c}}, M) = \\mathbb {E} _ {t \\sim \\mathcal {U} (1, T), \\epsilon \\sim \\mathcal {N} (\\mathbf {0}, \\mathbf {I})} \\left[ \\omega (\\lambda_ {t}) \\| \\epsilon_ {\\theta} (\\mathbf {x} _ {t}; y _ {\\text {t e x t}}, t, M, \\mathbf {x} _ {\\text {s r c}}) - \\epsilon \\| _ {2} ^ {2} \\right]. \\tag {2}\n$$\n\n$$\n\n\n# 2.2 ADVERSARIAL EXAMPLES AGAINST DIFFUSION MODELS\n\n\nAdversarial examples are deliberately fabricated data to manipulate model behaviors (Szegedy et al., 2014; Biggio et al., 2013), often with malicious intent. Given a clean image  $\\mathbf{x}$  and a model, an adversarial example adds a perturbation  $\\delta$  to  $\\mathbf{x}$  so that  $\\mathbf{x} + \\delta$  deceives the model. These perturbations are typically crafted to be imperceptible to human eyes, e.g., via constrained optimization using  $\\ell_{\\infty}$  bound  $\\| \\delta \\|_{\\infty} \\leq \\eta$  for some  $\\eta > 0$ . In this paper, we consider crafting an adversarial example for text-guided image editing models, where we aim to find a perturbation  $\\delta$  of source image that enforces the editing models to generate low-quality images. A line of research (Liang et al., 2023; Liang & Wu, 2023; Xue et al., 2024; Salman et al., 2024) has investigated adversarial examples of this purpose, using them as a protective measure against unauthorized image editing. These works either perturb each individual step of the denoising process to maximize the diffusion model training loss (i.e., Eq. 1), or force diffusion models to generate a specifically undesirable image as follows:\n\n\n$$\n\n$$\n\\delta = \\underset {\\left| \\left| \\delta \\right| \\right| _ {\\infty} \\leq \\eta} {\\arg \\min } \\mathbb {E} _ {\\epsilon \\sim \\mathcal {N} (\\mathbf {0}, \\mathbf {I})} \\left[ \\| \\widehat {\\mathbf {x}} \\left(\\mathbf {x} _ {\\text {s r c}} + \\delta ; \\epsilon , y _ {\\text {t e x t}}, M\\right) - \\mathbf {x} _ {\\text {t a r g e t}} \\| _ {2} ^ {2} \\right], \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\widehat{\\mathbf{x}}$  is a generated image given source image  $\\mathbf{x}_{\\mathrm{src}} + \\delta$ , prompt  $y_{\\mathrm{text}}$ , and mask  $M$ .\n\n\n# 3 MAIN METHOD\n\n\nIn this section, we outline DiffusionGuard, a method designed to protect images against inpainting methods in challenging scenarios (Sec. 3.1). First, based on the unique behaviors of inpainting models, we develop a novel objective to target the early stages of the reverse diffusion process (Sec. 3.2). Next, we propose a mask-augmentation method to find a robust adversarial perturbation that remains effective against mask inputs of various shapes (Sec. 3.3).\n\n\n# 3.1 PROBLEM SETUP\n\n\nPrevious protection methods (Liang et al., 2023; Liang & Wu, 2023; Xue et al., 2024) typically consider a global perturbation  $\\delta$  applied across the entire image, i.e.,  $\\mathbf{x} + \\delta$ , as described in Sec. 2.2. However, such methods become ineffective against diffusion inpainting models, where a binary mask  $M$  is additionally applied to the source image, i.e., it processes masked source images  $(\\mathbf{x} + \\delta) \\odot M$ . This realistic setup poses a unique challenge for adversarial defense, given that now only the part of adversarial noise that intersects with the mask  $M$  can affect the model's behavior.\n\n\n![(a) Denoising process of standard diffusion models](images/f696fe874ce99a4d21f2b8ff405a128f153b2b22ebd2fc85fd4c26ee69c21244.jpg)\n\n\n![(b) Denoising process of inpainting diffusion models Figure 2: Denoising diffusion process of standard and inpainting diffusion models. (a) Standard text-to-image models typically generate only coarse features in the early stages of the denoising process. (b) In contrast, inpainting models, which are fine-tuned versions of these standard models, produce fine details (e.g., face) from the very first denoising step  $(T - 1)$ .](images/412cba0a4a7558a0f62c83f77aef617197c53540a78ec4f2d50a54704f4ffa05.jpg)\n\n\nThreat model We assume that a malicious user attempts to successfully edit an image protected by adversarial perturbations applied by a defender. This malicious user can freely choose the mask input  $M$ , and text prompt  $y_{\\text{text}}$  for editing. Because it is challenging to develop a defense method against any arbitrary mask, we consider a feasible yet practical scenario where both defender and malicious user share a common understanding of the sensitive region in the source image; typically, in a portrait, this could be the face or the body of a person, while in other contexts, it might be a specific object. We assume that the defender uses this sensitive region as a training mask  $M_{\\text{tr}}$  in generating adversarial noises. Meanwhile, a malicious user can utilize a different mask  $M_{\\text{te}}$  but based on the same conceptual sensitive region.\n\n\n# 3.2 PERTURBING THE EARLY STAGES OF THE DIFFUSION PROCESS\n\n\nIn this section, we introduce a novel objective that specifically exploits a unique behavior we have observed in inpainting models. As shown in Fig. 2a, it is well-known that during the denoising process of diffusion models, coarse features (such as the image outline) emerge first, while fine details are generated in the later stages (Ho et al., 2020; Hertz et al., 2023). However, we have found that this pattern does not hold for inpainting models. As illustrated in Fig. 2b, these models first produce fine details (e.g., facial features) even at the first denoising step.\n\n\nThis unique behavior likely originates from the additional inputs given during the fine-tuning process of inpainting models. Unlike standard diffusion models that only receive random noises as input, inpainting models are fine-tuned to utilize two additional inputs by modifying the input channel of the noise prediction model  $\\epsilon_{\\theta}$ . Specifically, these models take a binary mask  $M_{\\mathrm{tr}}$ , and a masked source image  $\\mathbf{x}_{\\mathrm{src}} \\odot M_{\\mathrm{tr}}$  as inputs. Inpainting models are fine-tuned using a reconstruction loss (Eq. 2), which encourages them to copy and paste the unmasked region of the image, leading to the emergent behavior observed in Fig. 2b.\n\n\nInspired by the unique behavior of inpainting models, we develop a novel objective that targets the initial step of the denoising process. Suppose we have a source image  $\\mathbf{x}_{\\mathrm{src}}$  to protect, an inpainting model  $\\epsilon_{\\theta}$ , and a binary mask  $M_{\\mathrm{tr}}$  which designates the part of the image to keep while rest of the image is recreated. We aim to find an adversarial perturbation  $\\delta$  that maximizes the  $\\ell_2$  norm of the initial predicted noise only (e.g., see Fig. 3):\n\n\n$$\n\n$$\n\\delta = \\underset {| | \\delta | | _ {\\infty} \\leq \\eta}{\\arg \\max } \\| \\epsilon_ {\\theta} (\\mathbf {x} _ {T}; y _ {\\text {t e x t}}, T, M _ {\\mathrm {t r}}, \\mathbf {x} _ {\\mathrm {s r c}} + \\delta) \\| _ {2} ^ {2}, \\tag {4}\n$$\n\n$$\n\n\nwhere  $T$  corresponds to the initial denoising step and  $\\mathbf{x}_T$  is random noise. Our proposed objective focuses on targeting the early stage of the diffusion process, in contrast to prior methods that target the entire diffusion process Liang et al. (2023) or the output images Salman et al. (2024). This approach makes generating adversarial noise both efficient and effective because only one forward pass through the noise prediction model is necessary. Additionally, unlike previous methods that aim to maximize reconstruction loss (Eq. 1) or minimize the distance to an arbitrary target image (Eq. 3), we propose to increase the norm of the noise. We have found that this maximum norm objective is more effective than previous approaches (see Fig. 6a for supporting results).\n\n\n![Figure 3: Overview of DiffusionGuard. We propose (a) mask augmentation for improving robustness, and (b) early state perturbation loss for generating effective noises.](images/8a9aca27977dda4dc3a2e1a8e7b0d5b4accd925b3c039d9d9291428db88d03d7.jpg)\n\n\n# 3.3 MASK-ROBUST ADVERSARIAL PERTURBATION\n\n\nIn practice, malicious users may utilize a mask that differs from the mask  $M_{\\mathrm{tr}}$  that is seen during the generation of adversarial noise. Therefore, it is crucial to find robust perturbations that are effective across various mask shapes. To achieve this, we propose a mask augmentation  $\\mathcal{A}(\\cdot)$  that generates a new binary mask with a similar shape to  $M_{\\mathrm{tr}}$ . Specifically, we first obtain the points along the contours of  $M_{\\mathrm{tr}}$  using contour detection. We then adjust these points inward by a random offset to define a new contour. The area inside this new contour is filled to form the augmented mask (see Fig. 3).\n\n\nTo make these modifications more realistic, we apply smoothing to the random offsets. The full procedure of mask augmentation is summarized in Algorithm 1.\n\n\nUsing the proposed mask augmentation function  $\\mathcal{A}(\\cdot)$ , we generate a robust  $\\eta$ -bounded adversarial perturbation  $\\delta$  by maximizing the following loss over the set  $\\mathcal{M}$  of augmented masks  $\\mathcal{A}(M_{\\tau r})$ :\n\n\n$$\n\n$$\n\\delta = \\underset {| | \\delta | | _ {\\infty} \\leq \\eta}{\\arg \\max } \\mathcal {L} _ {\\mathrm {a d v}} (\\theta ; \\mathbf {x} + \\delta , M _ {\\mathrm {t r}}) = \\mathbb {E} _ {M \\sim \\mathcal {A} (M _ {\\mathrm {t r}})} \\left[ \\left\\| \\boldsymbol {\\epsilon} _ {\\theta} \\left(\\mathbf {x} _ {T}; y _ {\\text {t e x t}}, T, M, \\mathbf {x} + \\delta\\right) \\right\\| _ {2} ^ {2} \\right], \\tag {5}\n$$\n\n$$\n\n\nwhere  $\\mathcal{L}_{\\mathrm{adv}}$  is from our adversarial loss in Eq. 4. In practice, we optimize  $\\delta$  by stochastically sampling masks from  $\\mathcal{M}$  during the adversarial noise generation. Each iteration, we sample a mask  $M \\sim \\mathcal{A}(M_{\\mathrm{tr}})$  and perform a projected gradient descent (PGD) step (Madry et al., 2018) to update  $\\delta$ :\n\n\n$$\n\n$$\n\\delta \\leftarrow \\operatorname {P r o j} _ {\\left| \\left| \\delta \\right| \\right| _ {\\infty} \\leq \\eta} (\\delta - \\gamma \\cdot \\operatorname {s i g n} \\left(\\nabla_ {\\delta} \\mathcal {L} _ {\\mathrm {a d v}}\\right)), \\tag {6}\n$$\n\n$$\n\n\nwhere  $\\gamma$  is the step size and  $\\mathrm{Proj}_{||\\delta ||_{\\infty}\\leq \\eta}(\\cdot)$  projects  $\\delta$  onto the  $\\ell_{\\infty}$  ball of radius  $\\eta$ . By iteratively updating  $\\delta$  using different augmented masks, we effectively minimize the expected adversarial loss over the set of masks  $\\mathcal{M}$ . This stochastic optimization approach allows us to find a perturbation  $\\delta$  that is robust to various mask shapes similar to  $M_{\\mathrm{tr}}$ .\n\n\n# 4 EXPERIMENTS\n\n\n# 4.1 INPAINTGUARDBENCH: INPAINTING-SPECIALIZED PROTECTION BENCHMARK\n\n\nBenchmark dataset To thoroughly validate protection effectiveness and mask robustness, we construct a benchmark specialized for masked inpainting models. Our benchmark, named Inpaint-GuardBench, consists of 42 images, each associated with five unique masks. Out of these, one mask per image is generated using SAM (Kirillov et al., 2023), a state-of-the-art segmentation method, and the remaining four masks are handcrafted using the most common tools employed by end-users. The most commonly used tool for drawing a mask is the circle brush, where users select the region to keep by painting over the image. This method is employed by popular inpainting tools such as OpenAI DALL-E inpainting (Ramesh et al., 2022) and Stable Diffusion web UI, $^{2}$  the most widely used open-source GUI for diffusion models. We also incorporate simple handcrafted mask shapes\n\n\n![Figure 4: Qualitative comparison between DiffusionGuard and baseline methods. DiffusionGuard demonstrates greater protective effectiveness (i.e., the ability to prevent editing diffusion models from generating images well-aligned with the edit prompt) compared to all baseline methods. Additionally, DiffusionGuard exhibits better robustness, maintaining its protective effectiveness despite changes in the mask shape.](images/49e28b31f5f77e601565bf3806905779ab3dfd233fc4a0d4bd85c416abe1be2c.jpg)\n\n\nsuch as rectangles and circles. Our benchmark contains 42 images, divided into three categories: 32 celebrity portraits, 5 inanimate objects, and 5 animals. We consider 10 edit prompts for each image, resulting in a total of 2,100 edit tasks (42 images, 5 masks, and 10 prompts).\n\n\nBaselines We compare our method to various baseline methods. Our primary baseline is PhotoGuard (Salman et al., 2024), a protection method specialized for inpainting models. It adds perturbation within the mask region, optimized to cause diffusion models to generate incorrect images, as detailed in Eq. 3. We also consider AdvDM (Liang et al., 2023), a protection method that targets standard text-to-image diffusion models. This approach involves perturbing the entire image to disrupt the denoising process, aiming to maximize reconstruction loss (Eq. 1). Furthermore, we consider Mist (Liang & Wu, 2023) and SDS(-) (Xue et al., 2024), both of which build on top of AdvDM to improve the method. These three methods originally propose to add perturbation to the entire image, without considering specific mask regions. In our main experiments, we report the results obtained from the original approaches of these methods. However, we also consider variants\n\n\nTable 1: Results on InpaintGuardBench. Our method achieves strong protection in both Seen and Unseen set, across all metrics. The lower number represents better protective effectiveness, indicating failed edits. All methods were optimized using constraint of  $\\| \\delta \\|_{\\infty} = 16 / 255$ .\n\n\n<table><tr><td>Method</td><td>PSNR \u2193</td><td>CLIP Dir. Sim. \u2193</td><td>ImageReward \u2193</td><td>CLIP Sim. \u2193</td></tr><tr><td colspan=\"5\">Seen (1 Mask, Train set)</td></tr><tr><td>Unprotected</td><td>N/A</td><td>24.40</td><td>-1.365</td><td>30.15</td></tr><tr><td>PhotoGuard (Salman et al., 2024)</td><td>12.87</td><td>21.17 (\u0394-3.23)</td><td>-1.537 (\u0394-0.172)</td><td>27.89 (\u0394-2.26)</td></tr><tr><td>DiffusionGuard</td><td>12.60</td><td>18.95 (\u0394-5.45)</td><td>-1.807 (\u0394-0.442)</td><td>26.55 (\u0394-3.60)</td></tr><tr><td colspan=\"5\">Unseen (4 Masks, Test set)</td></tr><tr><td>Unprotected</td><td>N/A</td><td>24.29</td><td>-1.315</td><td>30.72</td></tr><tr><td>PhotoGuard (Salman et al., 2024)</td><td>14.53</td><td>23.30 (\u0394-0.99)</td><td>-1.357 (\u0394-0.042)</td><td>30.30 (\u0394-0.42)</td></tr><tr><td>DiffusionGuard</td><td>13.19</td><td>21.84 (\u0394-2.45)</td><td>-1.557 (\u0394-0.242)</td><td>29.05 (\u0394-1.67)</td></tr><tr><td>AdvDM (Liang et al., 2023)</td><td>13.37</td><td>24.27 (\u0394-0.02)</td><td>-1.361 (\u0394-0.046)</td><td>30.97 (\u0394+0.25)</td></tr><tr><td>Mist (Liang &amp; Wu, 2023)</td><td>14.51</td><td>23.93 (\u0394-0.36)</td><td>-1.307 (\u0394+0.008)</td><td>30.79 (\u0394+0.07)</td></tr><tr><td>SDS(-) (Xue et al., 2024)</td><td>14.32</td><td>23.85 (\u0394-0.44)</td><td>-1.237 (\u0394+0.078)</td><td>30.78 (\u0394+0.06)</td></tr></table>\n\n\nof these methods that introduce perturbations only within the mask region  $M_{\\mathrm{tr}}$ , and the results of this modified approach are reported in Appendix E.3.1.\n\n\nSetup and evaluation metrics As the target model, we use Stable Diffusion Inpainting (Rombach et al., 2023), an open-sourced inpainting diffusion model. For generating adversarial noises, we use the SAM-generated mask as the training (\"seen\") mask. We then evaluate the effectiveness of the generated adversarial perturbations on all 5 masks, including the handcrafted 4 \"unseen\" masks.\n\n\nFor evaluation, we employ quantitative metrics to measure the fidelity of the prompt and the quality of the image. We use the following metrics:\n\n- - CLIP directional similarity (Gal et al., 2022): This measures the alignment between the deviation in the image (from the source to the edited result) and the deviation in the text (from the source caption to the edit instruction). The source caption, which describes the source image, is generated using the BLIP-Large model (Li et al., 2022).\n\n- - CLIP similarity (Radford et al., 2021): This measures the text fidelity of the edited image using the cosine similarity between their CLIP embeddings.\n\n- - ImageReward (Xu et al., 2023): A human-aligned vision-language model (Li et al., 2022) fine-tuned on a human preference dataset, assesses both the resulting image quality and text fidelity.\n\n\nAdditionally, we measure the PSNR between the edited results of unprotected and protected images, as done by (Salman et al., 2024), to quantify the differences in the edited result compared to the unprotected version.\n\n\n# 4.2 MAIN RESULTS\n\n\nWe compare DiffusionGuard with the baseline methods (PhotoGuard, AdvDM, Mist, SDS(-)) on InpaintGuardBench. For all experiments, we ensure a fair comparison by running the protection methods for an equal amount of GPU time.\n\n\nQualitative comparison As shown in Fig. 4, DiffusionGuard demonstrates superior protective effectiveness, causing the edit result to become nearly devoid of any recognizable content in most examples. In contrast, the baseline methods generate images that are more realistic and well-aligned with the prompts. Notably, the protected results of DiffusionGuard effectively prevent the diffusion inpainting model from 'recognizing' the object, as illustrated in the final example where a different dog is drawn in place of the original. Additionally, DiffusionGuard demonstrates its robustness against mask changes, in contrast to PhotoGuard (Salman et al., 2024), a protection method utilizing mask information, which loses the protective effectiveness even with small deviations in mask shape.\n\n\n![(a) Human survey](images/3c6d506ec4f52a1f1523c139f1ca9b041eb59aa05480e5199d07dce068cde059.jpg)\n\n\n![(b) Compute budget comparison](images/6dd5afdd9f87327fa89d5b5ab3f26a90b20003343a1a7f15de9905b05564cfae.jpg)\n\n\n![(c) Noise budget comparison Figure 5: (a) Human survey results. We visualize the win rates of DiffusionGuard and PhotoGuard (Salman et al., 2024) strength grouped into Seen and Unseen groups. (b) Comparison under limited compute budget. PSNR values are presented per running time. (c) Comparison under limited noise budget. With varying noise threshold values, we measure PSNR values of each method. For (b) and (c), the protection is stronger if the PSNR value is lower.](images/1cb320e709c2bfe5a7fa8a481988f6d467d3fc45ad64841e6ea91b4269c4878a.jpg)\n\n\nQuantitative results Table 1 reports the quantitative metrics for DiffusionGuard and the baseline methods.3 DiffusionGuard exhibits strongest protection among all methods for both Seen and Unseen masks. Note that DiffusionGuard outperforms the baseline methods in both mask categories by a significant margin, in line with the results presented in Fig. 4.\n\n\nHuman evaluation We also conduct a human evaluation as follows: for each edit instance, defined by a triplet of source image, mask shape, and edit instruction, we display two edit results using DiffusionGuard and PhotoGuard in random order. Human raters then select which result is better or tie (i.e., the two editing results are similar) for all 2100 pairs. We ask human evaluators to make decisions based on both image quality and edit prompt fidelity simultaneously. We calculate the win rates of a protection method by counting how often its result was not chosen (i.e., deemed worse). As shown in Fig. 5a, DiffusionGuard results in a superior win rate against PhotoGuard in both Seen and Unseen sets of masks, with approximately a  $17\\%$  higher win rate gap over the baseline.\n\n\n# 4.3 COMPARISON UNDER RESOURCE-RESTRICTED SCENARIOS\n\n\nIn this section, we compare our method against baselines in two resource-restricted scenarios. First, we evaluate each method with varying running times to compare computational efficiency. Second, we test each method under limited noise budget by setting the noise threshold  $\\| \\delta \\|_{\\infty}$  to 4/255, 6/255, 8/255, 12/255, and 16/255 in order to compare them under tighter noise constraints.\n\n\nComparison under limited compute budget Fig. 5b shows that DiffusionGuard is more effective than PhotoGuard when both are optimized for an equal number of steps. Specifically, our method with compute budget of 11 seconds achieves a similar PSNR of PhotoGuard at 90 seconds. Additionally, when comparing the performance gaps between the Unseen and Seen mask categories for each method, the gap is notably smaller for DiffusionGuard (blue). These results demonstrate that our method is faster, cheaper, and more effective than PhotoGuard.\n\n\nComparison under limited noise budget Fig. 5c shows that DiffusionGuard consistently achieves stronger performance (i.e., lower PSNR) under a tighter noise budget. In particular, our method with a noise budget of  $6/255$  is similar to PhotoGuard even when PhotoGuard uses a higher budget of  $16/255$ . These results show that DiffusionGuard maintains high levels of protection even with reduced perturbations (i.e., less visible). This makes it suitable for real-life applications where generating less detectable noise and preserving the original image quality are crucial.\n\n\n# 4.4 ABLATION STUDY\n\n\nWe conduct a comprehensive analysis on the effects of loss functions in adversarial noise generation, mask augmentation, and the efficacy of using an inpainting-specialized method.\n\n\n![(a) Optimization loss comparison](images/7f816b73b7b183250e3fcdbf852cb22c3e327f38680dd1d7b4f83b365bad014a.jpg)\n\n\n![Figure 6: Ablation study reporting PSNR, CLIP directional similarity (CDS), ImageReward (IR), and CLIP similarity (CS). Lower metric indicates better performance, indicating failed edits. (a) Loss functions in adversarial noise generation. We visualize Seen set results of the three loss functions with the same training mask. (b) Effect of mask augmentation. Using Unseen set, we present the effect of mask augmentation on the effectiveness of DiffusionGuard. (c) Comparison to mask-free protection. We visualize the protection effectiveness of using mask-free protection (AdvDM (Liang et al., 2023)) and mask-dependent protection (DiffusionGuard) on the Unseen mask set.](images/e3279fadb1ff86c8db85f7deb2d68e063a40e12886331d92258d169bbd8ec534.jpg)\n\n\n![(b) Mask augmentation comparison (c) Against mask-free protection](images/e8160fe9cf7bbedf735f7ca7289cc639db641de6fd9bec0d5d262ec2cb617297.jpg)\n\n\nLoss functions in adversarial noise generation To verify the effectiveness of the early stage perturbation loss (Eq. 4) in generating adversarial noises, we compare it with image-space loss (used in PhotoGuard (Salman et al., 2024)) and reconstruction loss (used in AdvDM (Liang et al., 2023)). To isolate the effects of mask augmentation, we use a single fixed mask  $M_{\\mathrm{tr}}$  for generation and evaluate using the Seen set of InpaintGuardBench. As shown in Fig. 6a, our early stage perturbation loss consistently outperforms both the other losses across most metrics.\n\n\nEffect of mask augmentation Additionally, we evaluate how much mask augmentation improves the protection strength on the Unseen mask set by measuring the performance of DiffusionGuard with and without mask augmentation. Fig. 6b shows that mask augmentation consistently improves all metrics for the Unseen set of InpaintGuardBench, clearly demonstrating its effectiveness in enhancing mask robustness. We provide qualitative examples in Appendix E.1.\n\n\nComparison with mask-free protection We compare DiffusionGuard with a mask-free protection method that applies a global perturbation over the entire image. As a baseline, we use AdvDM, a mask-free protection method based on reconstruction loss (Eq. 1). Fig. 6c presents the results on the Unseen set of InpaintGuardBench, showing that DiffusionGuard, by focusing on the mask region, provides significantly stronger protection than AdvDM across all metrics. We also remark that the noise perceptibility is much lower with DiffusionGuard. This is because the per-pixel noise threshold  $\\| \\delta \\|_{\\infty}$  is identical between the two methods, but DiffusionGuard adds  $\\delta$  over a smaller, focused region, whereas in AdvDM  $\\delta$  occupies the entire image, making it more visible.\n\n\n# 4.5 RESILIENCE AGAINST PURIFICATION METHODS\n\n\nWe also evaluate the robustness of DiffusionGuard and the baseline methods against noise purification tools. Specifically, we applied commonly used purification tools such as JPEG (Wallace, 1992) compression, crop-and-resize, and AdverseCleaner (Zhang, 2023), an algorithmic adversarial perturbation remover, to the protected images. As visualized in Fig. 7a, DiffusionGuard (blue) shows superior protection strength, maintaining its effectiveness even after noise removal. This demonstrates its reliable protection in real-world scenarios, where malicious users may attempt to circumvent defenses by eliminating noise. We include the full results for the purification experiments in Appendix F.\n\n\n# 4.6 BLACK-BOX TRANSFER\n\n\nFinally, we verify whether our method and the baselines can be black-box transferred to a different model. Our adversarial perturbations are optimized against Stable Diffusion Inpainting (SD Inpainting), which is based on the pre-trained model weights of SD 1.2. In this section, we test these perturbations on SD 2 Inpainting, which is based on the weights of SD 2.0 (Rombach et al., 2023). Since SD 2.0 was trained from scratch and does not build on SD 1.2, it represents a different model family, making this evaluation a test of black-box transfer. Fig. 7b shows the edited results for DiffusionGuard and PhotoGuard (Salman et al., 2024) using 4 different editing prompts (\"A man in a hospital\", \"A man in a gym\", \"A man getting on a bus\", \"Photo of\n\n\n![(a) Purification comparison](images/1c51aada8e7f371116fa4706eb3e2f2f192ba5c5402fe03c2eafdc8aaf0725ef.jpg)\n\n\n![(b) Transfer to SD Inpainting 2.0 Figure 7: Comparison of DiffusionGuard and the baseline methods after noise removal and black-box transferring. (a) Purification. We visualize the Seen set results for each method after removing protection using three different purification methods. Lower metric value represents stronger protection. (b) Black-box transfer. We visualize the protection effectiveness of DiffusionGuard and PhotoGuard (Salman et al., 2024) by demonstrating editing results after black-box transfer to SD Inpainting 2.0 on Unseen mask set, using 4 different editing prompts.](images/dd8e9377192a5a644da1db1da905c12e50a3285498147ab63c37856a80001cf3.jpg)\n\n\na construction worker\" from left to right). As presented, DiffusionGuard maintains its effectiveness even when transferred to another model, resulting in failed edits, while PhotoGuard loses its protection and results in successful edits that are aligned with the editing prompt. We include the full evaluation results and comparisons with other baselines in Appendix G.\n\n\n# 5 RELATED WORK\n\n\nSafety concerns of generative diffusion models Generative diffusion models have raised public safety concerns, particularly regarding the potential misuse for generating realistic but harmful content. There have been various studies that seek to mitigate these concerns by developing safety measures against them. Some works focus on watermarking the images to identify manipulated images (Cui et al., 2023; Fernandez et al., 2023; Peng et al., 2023; Zhao et al., 2023). A line of studies explore ways to remove potentially harmful concepts (Gandikota et al., 2023; 2024; Heng & Soh, 2023). SPM (Lyu et al., 2024) introduced one-dimensional adapters for precise and transferable concept erasure, while MACE (Lu et al., 2024) massively scaled concept erasure to  $100+$  concepts. Recent methods, such as SepME (Zhao et al., 2024), have further explored efficient multi-concept erasure. Another line of research explores and identifies biases embedded in diffusion models and ways to mitigate them (Friedrich et al., 2023; Choi et al., 2024; Shen et al., 2024). These safety issues need continued focus and attention to ensure the responsible use of generative diffusion models.\n\n\nAdversarial examples against diffusion-based image editing Several works have explored adversarial examples against diffusion-based image editing to protect images from being manipulated by these models. AdvDM (Liang et al., 2023) proposed to maximize diffusion model training loss to create adversarial examples. Xue et al. (2024) further improved AdvDM by applying score distillation sampling (Poole et al., 2023). However, most prior works did not specialize for inpainting models specifically. PhotoGuard (Salman et al., 2024) considered a defense against inpainting models but did not evaluate the protection under a challenging setup such as mask robustness.\n\n\n# 6 CONCLUSION\n\n\nThis work presents DiffusionGuard, a robust and effective defense method against malicious diffusion-based image editing. We introduce a novel adversarial objective in order to target the vulnerability of inpainting models and disrupt the early stages of the denoising process, where key regions are generated. Furthermore, we identify key limitations in existing protection methods, such as their inability to handle various shapes of masks, and address this with mask augmentation to develop a robust protection method. By leveraging these strategies, our method achieves stronger protection and improved mask robustness with lower computational costs, when compared to several baselines.\n\n\nAdditionally, DiffusionGuard demonstrates robustness against various purification methods, and proves effective in black-box transfer settings.\n\n\n# ETHICS STATEMENT\n\n\nText-to-image diffusion models have demonstrated superior capabilities in generating and editing images based on text prompts and additional user inputs such as masks, offering significant potential across creative industries, including art, graphics, and entertainment. However, it comes with the risk of misuse, such as creating fake or deceptive visual contents. For example, malicious users can manipulate images to spread misinformation, deliberately fabricate events, or misrepresent individuals using the generated fake content.\n\n\nOur work presents a defense mechanism against the malicious usage of text-to-image models, in specific, we focus on protecting sensitive areas of images from unauthorized modifications. While our method provides robust defenses, we recognize a remaining risk that advanced adversaries may find ways to circumvent protections. As such, we highlight that the necessity of ethical guidelines and legal frameworks to prevent any malicious use.\n\n\nTherefore, we have made a concerted effort to evaluate and document the limitations of our approach through rigorous testing. Specifically, we conducted purification experiments (e.g., Sec. 4.5 and Appendix F) to assess the resilience of our method against attempts to remove adversarial noise, as well as transfer experiments (e.g., Sec. 4.6 and Appendix G) to evaluate its generalizability under a black-box transferring scenario. These experiments demonstrate the robustness of our method even when attackers employ noise-removal techniques or apply it to different models.\n\n\nNevertheless, we acknowledge the possibility of future threats and emphasize the future research on strengthening these defenses. We encourage open discussion and regulatory measures to mitigate the broader ethical implications of advancements in text-to-image models.\n\n\n# REFERENCES\n\n- James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. In Annual Conference of the Association for Computational Linguistics, 2023.\n\n- Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim \u0160rndi\u0107, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Hendrik Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip \u017delezn\u00fd (eds.), Machine Learning and Knowledge Discovery in Databases, 2013.\n\n- Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In International Conference on Learning Representations, 2024.\n\n- Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In IEEE Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Yujin Choi, Jinseong Park, Hoki Kim, Jaewook Lee, and Saeroom Park. Fair sampling in diffusion models through switching mechanism. In AAAI Conference on Artificial Intelligence, 2024.\n\n- Kevin Crowston. Amazon mechanical turk: A research tool for organizations and information systems scholars. In Shaping the Future of ICT Research. Methods and Approaches, 2012.\n\n- Yingqian Cui, Jie Ren, Han Xu, Pengfei He, Hui Liu, Lichao Sun, Yue Xing, and Jiliang Tang. Diffusionshield: A watermark for copyright protection against generative diffusion models. arXiv preprint arXiv:2306.03436, 2023.\n\n- Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, 2021.\n\n- Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. In Advances in Neural Information Processing Systems, 2023.\n\n- Pierre Fernandez, Guillaume Couairon, Herv\u00e9 J\u00e9gou, Matthijs Douze, and Teddy Furon. The stable signature: Rooting watermarks in latent diffusion models. In IEEE International Conference on Computer Vision, 2023.\n\n- Felix Friedrich, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Patrick Schramowski, Sasha Luccioni, and Kristian Kersting. Fair diffusion: Instructing text-to-image generation models on fairness. arXiv preprint arXiv:2306.03436, 2023.\n\n- Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. In Advances in Neural Information Processing Systems, 2023.\n\n- Rinon Gal, Or Patashnik, Haggai Maron, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-guided domain adaptation of image generators. ACM Trans. Graph., 2022.\n\n- Rohit Gandikota, Joanna Materzy\u0144ska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. In IEEE International Conference on Computer Vision, 2023.\n\n- Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzy\u0144ska, and David Bau. Unified concept editing in diffusion models. In IEEE/CVF Winter Conference on Applications of Computer Vision, 2024.\n\n- Alvin Heng and Harold Soh. Selective amnesia: A continual learning approach to forgetting in deep generative models. In Advances in Neural Information Processing Systems, 2023.\n\n- Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. In International Conference on Learning Representations, 2023.\n\n- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020.\n\n- Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In IEEE International Conference on Computer Vision, 2023.\n\n- Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International Conference on Machine Learning, 2022.\n\n- Chumeng Liang and Xiaoyu Wu. Mist: Towards improved adversarial examples for diffusion models. arXiv preprint arXiv:2305.12683, 2023.\n\n- Chumeng Liang, Xiaoyu Wu, Yang Hua, Jiaru Zhang, Yiming Xue, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing Guan. Adversarial example does good: Preventing painting imitation from diffusion models via adversarial examples. In International Conference on Machine Learning, 2023.\n\n- Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural Information Processing Systems, 2022.\n\n- Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept erasure in diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2024.\n\n- Mengyao Lyu, Yuhong Yang, Haiwen Hong, Hui Chen, Xuan Jin, Yuan He, Hui Xue, Jungong Han, and Guiguang Ding. One-dimensional adapter to rule them all: Concepts, diffusion models and erasing applications. In IEEE Conference on Computer Vision and Pattern Recognition, 2024.\n\n- Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\n\n- Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, 2022.\n\n- Sen Peng, Yufei Chen, Cong Wang, and Xiaohua Jia. Intellectual property protection of diffusion models via the watermark diffusion process. arXiv preprint arXiv:2306.03436, 2023.\n\n- Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In International Conference on Learning Representations, 2023.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.\n\n- Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Nataniel Ruiz, Yanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In IEEE Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Chitwan Sahara, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, 2022a.\n\n- Chitwan Sahara, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022b.\n\n- Chitwan Sahara, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE transactions on pattern analysis and machine intelligence, 45(4):4713-4726, 2022c.\n\n- Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, and Aleksander Madry. Raising the cost of malicious ai-powered image editing. In International Conference on Machine Learning, 2024.\n\n- Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024.\n\n- Xudong Shen, Chao Du, Tianyu Pang, Min Lin, Yongkang Wong, and Mohan Kankanhalli. Finetuning text-to-image diffusion models for fairness. In International Conference on Learning Representations, 2024.\n\n- Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, 2015.\n\n- Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021.\n\n- Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014.\n\n- G.K. Wallace. The JPEG still picture compression standard. IEEE Transactions on Consumer Electronics, 1992.\n\n- Su Wang, Chitwan Sahara, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In IEEE Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In Advances in Neural Information Processing Systems, 2023.\n\n- Haotian Xue, Chumeng Liang, Xiaoyu Wu, and Yongxin Chen. Toward effective protection against diffusion based mimicry through score distillation. In International Conference on Learning Representations, 2024.\n\n- Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, and Jun Zhu. Dreamreward: Text-to-3d generation with human preference. arXiv preprint arXiv:2403.14613, 2024.\n\n- Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, and Saining Xie. Image sculpting: Precise object editing with 3d geometry control. arXiv preprint arXiv:2401.01702, 2024.\n\n- Lvmin Zhang. AdverseCleaner. https://github.com/111yasviel/AdverseCleaner, 2023.\n\n- Mengnan Zhao, Lihe Zhang, Tianhang Zheng, Yuqiu Kong, and Baocai Yin. Separable multi-concept erasure from diffusion models. arXiv preprint arXiv:2402.05947, 2024.\n\n- Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, and Min Lin. A recipe for watermarking diffusion models. arXiv preprint arXiv:2303.10137, 2023.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638634.7029412", "msecs": "702.0", "relativeCreated": "231533.43868255615", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines\n\n# Abstract\nFine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on commonly used datasets from the GLUE benchmark, and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than the previously proposed approaches.\n\n\n# 1 INTRODUCTION\n\n\nPre-trained transformer-based masked language models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ALBERT (Lan et al., 2020) have had a dramatic impact on the NLP landscape in the recent year. The standard recipe for using such models typically involves training a pretrained model for a few epochs on a supervised downstream dataset, which is known as fine-tuning. While fine-tuning has led to impressive empirical results, dominating a large variety of English NLP benchmarks such as GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a), it is still poorly understood. Not only have fine-tuned models been shown to pick up spurious patterns and biases present in the training data (Niven and Kao, 2019; McCoy et al., 2019), but also to exhibit a large training instability: fine-tuning a model multiple times on the same dataset, varying only the random seed, leads to a large standard deviation of the fine-tuning accuracy (Devlin et al., 2019; Dodge et al., 2020).\n\n\nFew methods have been proposed to solve the observed instability (Phang et al., 2018; Lee et al., 2020), however without providing a sufficient understanding of why fine-tuning is prone to such failure. The goal of this work is to address this shortcoming. More specifically, we investigate the following question:\n\n\nWhy is fine-tuning prone to failures and how can we improve its stability?\n\n\nWe start by investigating two common hypotheses for fine-tuning instability: catastrophic forgetting and small size of the fine-tuning datasets and demonstrate that both hypotheses fail to explain fine-tuning instability. We then investigate fine-tuning failures on datasets from the popular GLUE benchmark and show that the observed fine-tuning instability can be decomposed into two separate aspects: (1) optimization difficulties early in training, characterized by vanishing gradients, and (2) differences in generalization late in training, characterized by a large variance of development set accuracy for runs with almost equivalent training loss.\n\n\n![(a) RTE](images/6fa70f1c2024747997c22a96b7a511612167a9a504d824cc8f5338951aebf2e4.jpg)\n\n\n![(b) MRPC Figure 1: Our proposed fine-tuning strategy leads to very stable results with very concentrated development set performance over 25 different random seeds across all three datasets on BERT. In particular, we significantly outperform the recently proposed approach of Lee et al. (2020) in terms of fine-tuning stability.](images/bec6059046828002bc163d6c693b7719017e535d6def17236be21147cb18e795.jpg)\n\n\n![(c) CoLA](images/a08abfe83c053344c7ecd74362aadfcaa0988e1c6854d80f857495ba0c02e08e.jpg)\n\n\nBased on our analysis, we present a simple but strong baseline for fine-tuning pre-trained language models that significantly improves the fine-tuning stability compared to previous works (Fig. 1). Moreover, we show that our findings apply not only to the widely used BERT model but also to more recent pre-trained models such as RoBERTa and ALBERT.\n\n\n# 2 RELATED WORK\n\n\nThe fine-tuning instability of BERT has been pointed out in various studies. Devlin et al. (2019) report instabilities when fine-tuning  $\\mathrm{BERT}_{\\mathrm{LARGE}}$  on small datasets and resort to performing multiple restarts of fine-tuning and selecting the model that performs best on the development set. Recently, Dodge et al. (2020) performed a large-scale empirical investigation of the fine-tuning instability of BERT. They found dramatic variations in fine-tuning accuracy across multiple restarts and argue how it might be related to the choice of random seed and the dataset size.\n\n\nFew approaches have been proposed to directly address the observed fine-tuning instability. Phang et al. (2018) study intermediate task training (STILTS) before fine-tuning with the goal of improving performance on the GLUE benchmark. They also find that their proposed method leads to improved fine-tuning stability. However, due to the intermediate task training, their work is not directly comparable to ours. Lee et al. (2020) propose a new regularization technique termed Mixout. The authors show that Mixout improves stability during fine-tuning which they attribute to the prevention of catastrophic forgetting.\n\n\nAnother line of work investigates optimization difficulties of pre-training transformer-based language models (Xiong et al., 2020; Liu et al., 2020). Similar to our work, they highlight the importance of the learning rate warmup for optimization. Both works focus on pre-training and we hence view them as orthogonal to our work.\n\n\n# 3 BACKGROUND\n\n\n# 3.1 DATASETS\n\n\nWe study four datasets from the GLUE benchmark (Wang et al., 2019b): CoLA, MRPC, RTE, and QNLI. Detailed statistics for each of the three datasets can be found in Section 7.1 in the Appendix.\n\n\nCoLA. The Corpus of Linguistic Acceptability (Warstadt et al., 2018) is a sentence-level classification task containing sentences labeled as either grammatical or ungrammatical. Fine-tuning on CoLA was observed to be particularly stable in previous work (Phang et al., 2018; Dodge et al., 2020; Lee et al., 2020). Performance on CoLA is reported in Matthew's correlation coefficient (MCC).\n\n\nMRPC. The Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) is a sentence-pair classification task. Given two sentences, a model has to judge whether the sentences paraphrases of each other. Performance on MRPC is measured using the  $F_{1}$  score.\n\n\nRTE. The Recognizing Textual Entailment dataset is a collection of sentence-pairs collected from a series of textual entailment challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo\n\n\net al., 2007; Bentivogli et al., 2009). RTE is the second smallest dataset in the GLUE benchmark and fine-tuning on RTE was observed to be particularly unstable (Phang et al., 2018; Dodge et al., 2020; Lee et al., 2020). Accuracy is used to measure performance on RTE.\n\n\nQNLI. The Question-answering Natural Language Inference dataset contains sentence pairs obtained from SQuAD (Rajpurkar et al., 2016). Wang et al. (2019b) converted SQuAD into a sentence pair classification task by forming a pair between each question and each sentence in the corresponding paragraph. The task is to determine whether the context sentence contains the answer to the question, i.e. entails the answer. Accuracy is used to measure performance on QNLI.\n\n\n# 3.2 FINE-TUNING\n\n\nUnless mentioned otherwise, we follow the default fine-tuning strategy recommended by Devlin et al. (2019): we fine-tune uncased  $\\mathrm{BERT}_{\\mathrm{LARGE}}$  (henceforth BERT) using a batch size of 16 and a learning rate of  $2\\mathrm{e} - 5$ . The learning rate is linearly increased from 0 to  $2\\mathrm{e} - 5$  for the first  $10\\%$  of iterations\u2014which is known as a warmup\u2014and linearly decreased to 0 afterward. We apply dropout with probability  $p = 0.1$  and weight decay with  $\\lambda = 0.01$ . We train for 3 epochs on all datasets and use global gradient clipping. Following Devlin et al. (2019), we use the AdamW optimizer (Loshchilov and Hutter, 2019) without bias correction.\n\n\nWe decided to not show results for  $\\mathrm{BERT}_{\\mathrm{BASE}}$  since previous works observed no instability when fine-tuning  $\\mathrm{BERT}_{\\mathrm{BASE}}$  which we also confirmed in our experiments. Instead, we show additional results on  $\\mathrm{RoBERTa}_{\\mathrm{LARGE}}$  (Liu et al., 2019) and  $\\mathrm{ALBERT}_{\\mathrm{LARGE-V2}}$  (Lan et al., 2020) using the same fine-tuning strategy. We note that compared to BERT, both RoBERTa and ALBERT have slightly different hyperparameters. In particular, RoBERTa uses weight decay with  $\\lambda = 0.1$  and no gradient clipping, and ALBERT does not use dropout. A detailed list of all default hyperparameters for all models can be found in Section 7.2 of the Appendix. Our implementation is based on HuggingFace's transformers library (Wolf et al., 2019) and we will make it public after the reviewing process.\n\n\nFine-tuning stability. By fine-tuning stability we mean the standard deviation of the fine-tuning performance (measured, e.g., in terms of accuracy, MCC or  $F_{1}$  score) over the randomness of an algorithm. We follow previous works (Phang et al., 2018; Dodge et al., 2020; Lee et al., 2020) and measure fine-tuning stability using the development sets from the GLUE benchmark.\n\n\nFailed runs. Following Dodge et al. (2020), we refer to a fine-tuning run as a failed run if its accuracy at the end of training is less or equal to that of a majority classifier on the respective dataset. The majority baselines for all tasks are found in Section 7.1 in the Appendix.\n\n\n# 4 INVESTIGATING PREVIOUS HYPOTHESES FOR FINE-TUNING INSTABILITY\n\n\nPrevious works on fine-tuning predominantly state two hypotheses for what can be related to fine-tuning instability: catastrophic forgetting and small training data size of the downstream tasks. Despite the ubiquity of these hypotheses (Devlin et al., 2019; Phang et al., 2018; Dodge et al., 2020; Lee et al., 2020), we argue that none of them has a causal relationship with fine-tuning instability.\n\n\n# 4.1 DOES CATASTROPHIC FORGETTING CAUSE FINE-TUNING INSTABILITY?\n\n\nCatastrophic forgetting (McCloskey and Cohen, 1989; Kirkpatrick et al., 2017) refers to the phenomenon when a neural network is sequentially trained to perform two different tasks, and it loses its ability to perform the first task after being trained on the second. More specifically, in our setup, it means that after fine-tuning a pre-trained model, it can no longer perform the original masked language modeling task used for pre-training. This can be measured in terms of the perplexity on the original training data. Although the language modeling performance of a pre-trained model correlates with its fine-tuning accuracy (Liu et al., 2019; Lan et al., 2020), there is no clear motivation for why preserving the original masked language modeling performance after fine-tuning is important.\n\n\nIn the context of fine-tuning BERT, Lee et al. (2020) suggest that their regularization method has an effect of alleviating catastrophic forgetting. Thus, it is important to understand how exactly catastrophic forgetting occurs in fine-tuning and how it relates to the observed fine-tuning instability. To better understand this, we perform the following experiment. We fine-tune BERT on RTE,\n\n\n![(a) Perplexity of failed models](images/9f4312b0afa84146a0a80ec7f29e5b0cfc004bb8a94b63de5b81344da28c26b7.jpg)\n\n\n![(b) Perplexity of successful models](images/56781c60fd6134e1a9f60d6e85d9ebfbad1e29ae020ce003fec2bcfd99787c29.jpg)\n\n\n![(c) Training of failed models](images/3c87f4bb86246736da9d2804c15ea4f25fdfaf9738cbb65fb08364408492af5e.jpg)\n\n\n![Figure 2: Language modeling perplexity for three failed (a) and successful (b) fine-tuning runs of BERT on RTE where we replace the weights of the top- $k$  layers with their pre-trained values. We can observe that it is often sufficient to reset around 10 layers out of 24 to recover back the language modeling abilities of the pre-trained model. (c) shows the average training loss and development accuracy ( $\\pm 1$ std) for 10 failed fine-tuning runs on RTE. Failed fine-tuning runs lead to a trivial training loss suggesting an optimization problem. (a) MRPC Figure 3: Development set results on down-sampled MRPC, CoLA, and QNLI using the default fine-tuning scheme of BERT (Devlin et al., 2019). The leftmost boxplot in each sub-figure shows the development accuracy when training on the full training set.](images/521aa8a732342ed70d4698b342e8c80c4157f32806ff93c09b56cd1355b9e8d5.jpg)\n\n\n![(b) CoLA](images/046b509c7ad0b4da4a436980a3226cadee2089e1701363e479d2b9ef10a307c5.jpg)\n\n\n![(c)QNLI](images/b27fbc9011d6f08e5664a500135b10de6e3dbcd75d6db9c1da678fa0007b5ab5.jpg)\n\n\nfollowing the default strategy by Devlin et al. (2019). We select three successful and three failed fine-tuning runs and evaluate their masked language modeling perplexity on the test set of the WikiText-2 language modeling benchmark (Merit et al., 2016).<sup>1</sup> We sequentially substitute the top- $k$  layers of the network varying  $k$  from 0 (i.e. all layers are from the fine-tuned model) to 24 (i.e. all layers are from the pre-trained model). We show the results in Fig. 2 (a) and (b).\n\n\nWe can observe that although catastrophic forgetting occurs for the failed models (Fig. 2a) \u2014 perplexity on WikiText-2 is indeed degraded for  $k = 0$  \u2014 the phenomenon is much more nuanced. Namely, catastrophic forgetting affects only the top layers of the network \u2014 in our experiments often around 10 out of 24 layers, and the same is however also true for the successfully fine-tuned models, except for a much smaller increase in perplexity.\n\n\nAnother important aspect of our experiment is that catastrophic forgetting typically requires that the model at least successfully learns how to perform the new task. However, this is not the case for the failed fine-tuning runs. Not only is the development accuracy equal to that of the majority classifier, but also the training loss on the fine-tuning task (here RTE) is trivial, i.e. close to  $-\\ln(1/2)$  (see Fig. 2 (c)). This suggests that the observed fine-tuning failure is rather an optimization problem causing catastrophic forgetting in the top layers of the pre-trained model. We will show later that the optimization aspect is actually sufficient to explain most of the fine-tuning variance.\n\n\n# 4.2 DO SMALL TRAINING DATASETS CAUSE FINE-TUNING INSTABILITY?\n\n\nHaving a small training dataset is by far the most commonly stated hypothesis for fine-tuning instability. Multiple recent works (Devlin et al., 2019; Phang et al., 2018; Lee et al., 2020; Zhu et al., 2020; Dodge et al., 2020; Pruksachatkun et al., 2020) that have observed BERT fine-tuning to be unstable relate this finding to the small number of training examples.\n\n\n![(a) Failed run](images/be04410f356e2bb28c95721f319b409d5ec49367e22a4b92d49b2868a58b0700.jpg)\n\n\n![(b) Successful run Figure 4: Gradient norms (plotted on a logarithmic scale) of different layers on RTE for a failed and successful run of BERT fine-tuning. We observe that the failed run is characterized by vanishing gradients in the bottom layers of the network. Additional plots for other weight matrices can be found in the Appendix.](images/1d7919a03bf6b40047801251e33100af4ea27d1ea02d6ea2796376cbe804ac15.jpg)\n\n\nTo test if having a small training dataset inherently leads to fine-tuning instability we perform the following experiment:2 we randomly sample 1,000 training samples from the CoLA, MRPC, and QNLI training datasets and fine-tune BERT using 25 different random seeds on each dataset. We compare two different settings: first, training for 3 epochs on the reduced training dataset, and second, training for the same number of iterations as on the full training dataset. We show the results in Fig. 3 and observe that training on less data does indeed affect the fine-tuning variance, in particular, there are many more failed runs. However, when we simply train for as many iterations as on the full training dataset, we almost completely recover the original variance of the fine-tuning performance. We also observe no failed runs on MRPC and QNLI and only a single failed run on CoLA which is similar to the results obtained by training on the full training set. Further, as expected, we observe that training on fewer samples affects the generalization of the model, leading to a worse development set performance on all three tasks.\n\n\nWe conclude from this experiment, that the role of training dataset size per se is orthogonal to fine-tuning stability. What is crucial is rather the number of training iterations. As our experiment shows, the observed increase in instability when training with smaller datasets can rather be attributed to the reduction of the number of iterations (that changes the effective learning rate schedule) which, as we will show in the next section, has a crucial influence on the fine-tuning stability.\n\n\n# 5 DISENTANGLING OPTIMIZATION AND GENERALIZATION IN FINE-TUNING INSTABILITY\n\n\nOur findings in Section 4 detail that while both catastrophic forgetting and small size of the datasets indeed correlate with fine-tuning instability, none of them are causing it. In this section, we argue that the fine-tuning instability is an optimization problem, and it admits a simple solution. Additionally, we show that even though a large fraction of the fine-tuning instability can be explained by optimization, the remaining instability can be attributed to generalization issues where fine-tuning runs with the same training loss exhibit noticeable differences in the development set performance.\n\n\n# 5.1 THE ROLE OF OPTIMIZATION\n\n\nFailed fine-tuning runs suffer from vanishing gradients. We observed in Fig. 2c that the failed runs have practically constant training loss throughout the training (see Fig. 14 in the Appendix for a comparison with successful fine-tuning). In order to better understand this phenomenon, in Fig. 4 we plot the  $\\ell_2$  gradient norms of the loss function with respect to different layers of BERT, for one failed and successful fine-tuning run. For the failed run we see large enough gradients only for the top layers and vanishing gradients for the bottom layers. This is in large contrast to the successful run. While we also observe small gradients at the beginning of training (until iteration 70), gradients\n\n\n![(a) BERT](images/d94521c2c3e3e42b6f9b867c776eed07620324f1994f9659d97f88b474ab32e3.jpg)\n\n\n![(b) RoBERTa Figure 5: Box plots showing the fine-tuning performance of (a) BERT, (b) RoBERTa, (c) ALBERT for different learning rates  $\\alpha$  with and without bias correction (BC) on RTE. For BERT and ALBERT, having bias correction leads to more stable results and allows to train using larger learning rates. For RoBERTa, the effect is less pronounced but still visible.](images/97de9fa040342bca948419fb9aaca297a7be1ec03634ba9771ef35cd046417a6.jpg)\n\n\n![(c) ALBERT](images/a4219a942f4de6d1306ad00b72c0ff7f15cf62b188d9f77ef548294a74f08be0.jpg)\n\n\nstart to grow as training continues. Moreover, at the end of fine-tuning, we observe gradient norms nearly  $2 \\times$  orders of magnitude larger than that of the failed run. Similar visualizations for additional layers and weights can be found in Fig. 10 in the Appendix. Moreover, we observe the same behavior also for RoBERTa and ALBERT models, and the corresponding figures can be found in the Appendix as well (Fig. 11 and 12).\n\n\nImportantly, we note that the vanishing gradients we observe during fine-tuning are harder to resolve than the standard vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994). In particular, common weight initialization schemes (Glorot and Bengio, 2010; He et al., 2015) ensure that the pre-activations of each layer of the network have zero mean and unit variance in expectation. However, we cannot simply modify the weights of a pre-trained model on each layer to ensure this property since this would conflict with the idea of using the pre-trained weights.\n\n\nImportance of bias correction in ADAM. Following Devlin et al. (2019), subsequent works on fine-tuning BERT-based models use the ADAM optimizer (Kingma and Ba, 2014). A subtle detail of the fine-tuning scheme of Devlin et al. (2019) is that it does not include the bias correction in ADAM. Kingma and Ba (2014) already describe the effect of the bias correction as to reduce the learning rate at the beginning of training. By rewriting the update equations of ADAM as follows, we can clearly see this effect of bias correction:\n\n\n$$\n\n$$\n\\alpha_ {t} \\leftarrow \\alpha \\cdot \\sqrt {1 - \\beta_ {2} ^ {t}} / \\left(1 - \\beta_ {1} ^ {t}\\right), \\tag {1}\n$$\n\n$$\n\n\n$$\n\n$$\n\\theta_ {t} \\leftarrow \\theta_ {t - 1} - \\alpha_ {t} \\cdot m _ {t} / (\\sqrt {v _ {t}} + \\epsilon), \\tag {2}\n$$\n\n$$\n\n\nHere  $m_t$  and  $v_t$  are biased first and second moment estimates respectively. Equation (1) shows that bias correction simply boils down to reducing the original step size  $\\alpha$  by a multiplicative factor  $\\sqrt{1 - \\beta_2^t} / (1 - \\beta_1^t)$  which is significantly below 1 for the first iterations of training and approaches 1 as the number of training iterations  $t$  increases (see Fig. 6). Along the same lines, You et al. (2020) explicitly remark that bias correction in ADAM has a similar effect to the warmup which is widely used in deep learning to prevent divergence early in training (He et al., 2016; Goyal et al., 2017; Devlin et al., 2019; Wong et al., 2020).\n\n\n![Figure 6: The bias correction term of ADAM  $(\\beta_{1} = 0.9$  and  $\\beta_{2} = 0.999)$](images/fdfecc9325b2814b4be0164aaf1ad508f11213bc2a43c87d43b8e4b1413c202c.jpg)\n\n\nThe implicit warmup of ADAM is likely to be an important factor that contributed to its success. We argue that fine-tuning BERT-based language models is not an exception. In Fig. 5 we show the results of fine-tuning on RTE with and without bias correction for BERT, RoBERTa, and ALBERT models.3 We observe that there is a significant benefit in combining warmup with bias correction, particularly for BERT and ALBERT. Even though for RoBERTa fine-tuning is already more stable even without bias correction, adding bias correction gives an additional improvement.\n\n\n![(a) RTE](images/1b01202aa000e9f9bf607be283e7a807c2015f795aa59ce32fe4a55f8e06df15.jpg)\n\n\n![(b) MRPC Figure 7: 2D loss surfaces in the subspace spanned by  $\\delta_1 = \\theta_f - \\theta_p$  and  $\\delta_2 = \\theta_s - \\theta_p$  on RTE, MRPC, and CoLA.  $\\theta_p$ ,  $\\theta_f$ ,  $\\theta_s$  denote the parameters of the pre-trained, failed, and successfully trained model, respectively.](images/764746f432fde5ecb435d07db6aacdc59764017d4ee2bb70e6c4bcba9cd92ec1.jpg)\n\n\n![(c) CoLA](images/bdb2deca105eea5def15080178eeffc98975d64ce4c84b128db40b0931c988e5.jpg)\n\n\nOur results show that bias correction is useful if we want to get the best performance within 3 epochs, the default recommendation by Devlin et al. (2019). An alternative solution is to simply train longer with a smaller learning rate, which also leads to much more stable fine-tuning. We provide a more detailed ablation study in Appendix (Fig. 9) with analogous box plots for BERT using various learning rates, numbers of training epochs, with and without bias correction. Finally, concurrently to our work, Zhang et al. (2020) also make a similar observation about the importance of bias correction and longer training which gives further evidence to our findings.\n\n\nLoss surfaces. To get further intuition about the fine-tuning failure, we provide loss surface visualizations (Li et al., 2018; Hao et al., 2019) of failed and successful runs when fine-tuning BERT. Denote by  $\\theta_p, \\theta_f, \\theta_s$  the parameters of the pre-trained model, failed model, and successfully trained model, respectively. We plot a two-dimensional loss surface  $f(\\alpha, \\beta) = \\mathcal{L}(\\theta_p + \\alpha \\delta_1 + \\beta \\delta_2)$  in the subspace spanned by  $\\delta_1 = \\theta_f - \\theta_p$  and  $\\delta_2 = \\theta_s - \\theta_p$  centered at the weights of the pre-trained model  $\\theta_p$ . Additional details are specified in the Appendix.\n\n\nContour plots of the loss surfaces for RTE, MRPC, and CoLA are shown in Fig. 7. They provide additional evidence to our findings on vanishing gradients: for failed fine-tuning runs gradient descent converges to a \"bad\" valley with a sub-optimal training loss. Moreover, this bad valley is separated from the local minimum (to which the successfully trained run converged) by a barrier (see Fig. 13 in the Appendix). Interestingly, we observe a highly similar geometry for all three datasets providing further support for our interpretation of fine-tuning instability as a primarily optimization issue.\n\n\n# 5.2 THE ROLE OF GENERALIZATION\n\n\nWe now turn to the generalization aspects of fine-tuning instability. In order to show that the remaining fine-tuning variance on the development set can be attributed to generalization, we perform the following experiment. We fine-tune BERT on RTE for 20 epochs and show the development set accuracy for 10 successful runs in Fig. 8a. Further, we show in Fig. 8b the development set accuracy vs. training loss of all BERT models fine-tuned on RTE for the full ablation study (shown in Fig. 9 in the Appendix), in total 450 models.\n\n\nWe find that despite achieving close to zero training loss overfitting is not an issue during fine-tuning. This is consistent with previous work (Hao et al., 2019), which arrived at a similar conclusion. Based on our results, we argue that it is even desirable to train for a larger number of iterations since the development accuracy varies considerably during fine-tuning and it does not degrade even when the training loss is as low as  $10^{-5}$ .\n\n\nCombining these findings with results from the previous section, we conclude that the fine-tuning instability can be decomposed into two aspects: optimization and generalization. In the next section, we propose a simple solution addressing both issues.\n\n\n![](images/6fdc67533fc748ed9a35af06f8473619009c6982797b171d564b5036d30dd4de.jpg)\n\n\n![Figure 8: (a) Development accuracy on RTE during training. (b) Development accuracy vs. training loss at the end of the training.](images/ac7e120b2a1836ad466e9bb84ef853e4575c29c7cc170a5d4db480495efd7819.jpg)\n\n\nTable 1: Standard deviation, mean, and maximum performance on the development set of RTE, MRPC, and CoLA when fine-tuning BERT over 25 random seeds. Standard deviation: lower is better, i.e. fine-tuning is more stable. \\* denotes significant difference  $(p <   0.001)$  when compared to the second smallest standard deviation.\n\n\n<table><tr><td rowspan=\"2\">Approach</td><td colspan=\"3\">RTE</td><td colspan=\"3\">MRPC</td><td colspan=\"3\">CoLA</td></tr><tr><td>std</td><td>mean</td><td>max</td><td>std</td><td>mean</td><td>max</td><td>std</td><td>mean</td><td>max</td></tr><tr><td>Devlin et al. (2019)</td><td>4.5</td><td>50.9</td><td>67.5</td><td>3.9</td><td>84.0</td><td>91.2</td><td>25.6</td><td>45.6</td><td>64.6</td></tr><tr><td>Lee et al. (2020)</td><td>7.9</td><td>65.3</td><td>74.4</td><td>3.8</td><td>87.8</td><td>91.8</td><td>20.9</td><td>51.9</td><td>64.0</td></tr><tr><td>Ours</td><td>2.7*</td><td>67.3</td><td>71.1</td><td>0.8*</td><td>90.3</td><td>91.7</td><td>1.8*</td><td>62.1</td><td>65.3</td></tr></table>\n\n\n# 6 A SIMPLE BUT HARD-TO-BEAT BASELINE FOR FINE-TUNING BERT\n\n\nAs our findings in Section 5 show, the empirically observed instability of fine-tuning BERT can be attributed to vanishing gradients early in training as well as differences in generalization late in training. Given the new understanding of fine-tuning instability, we propose the following guidelines for fine-tuning transformer-based masked language models on small datasets:\n\n- - Use small learning rates with bias correction to avoid vanishing gradients early in training.\n\n- - Increase the number of iterations considerably and train to (almost) zero training loss.\n\n\nThis leads to the following simple baseline scheme: we fine-tune BERT using ADAM with bias correction and a learning rate of  $2\\mathrm{e} - 5$ . The training is performed for 20 epochs, and the learning rate is linearly increased for the first  $10\\%$  of steps and linearly decayed to zero afterward. All other hyperparameters are kept unchanged. A full ablation study on RTE testing various combinations of the changed hyperparameters is presented in Section 7.3 in the Appendix.\n\n\nResults. Despite the simplicity of our proposed fine-tuning strategy, we obtain strong empirical performance. Table 1 and Fig. 1 show the results of fine-tuning BERT on RTE, MRPC, and CoLA. We compare to the default strategy of Devlin et al. (2019) and the recent Mixout method from Lee et al. (2020). First, we observe that our method leads to a much more stable fine-tuning performance on all three datasets as evidenced by the significantly smaller standard deviation of the final performance. To further validate our claim about the fine-tuning stability, we run Levene's test (Levene, 1960) to check the equality of variances for the distributions of the final performances on each dataset. For all three datasets, the test results in a p-value less than 0.001 when we compare the variances between our method and the method achieving the second smallest variance. Second, we also observe that our method improves the overall fine-tuning performance: in Table 1 we achieve a higher mean value on all datasets and also comparable or better maximum performance on MRPC and CoLA respectively.\n\n\nFinally, we note that we suggest to increase the number of fine-tuning iterations only for small datasets, and thus the increased computational cost of our proposed scheme is not a problem in practice. Moreover, we think that overall our findings lead to more efficient fine-tuning because of the significantly improved stability which effectively reduces the number of necessary fine-tuning runs.\n\n\n# 7 CONCLUSIONS\n\n\nIn this work, we have discussed the existing hypotheses regarding the reasons behind fine-tuning instability and proposed a new baseline strategy for fine-tuning that leads to significantly improved fine-tuning stability and overall improved results on three datasets from the GLUE benchmark.\n\n\nBy analyzing failed fine-tuning runs, we find that neither catastrophic forgetting nor small dataset sizes sufficiently explain fine-tuning instability. Instead, our analysis reveals that fine-tuning instability can be characterized by two distinct problems: (1) optimization difficulties early in training, characterized by vanishing gradients, and (2) differences in generalization, characterized by a large variance of development set accuracy for runs with almost equivalent training performance.\n\n\nBased on our analysis, we propose a simple but strong baseline strategy for fine-tuning BERT which outperforms the previous works in terms of fine-tuning stability and overall performance.\n\n\n# REFERENCES\n\n- Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. 2006. The second pascal recognising textual entailment challenge. Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.\n\n- Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166.\n\n- Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth pascal recognizing textual entailment challenge. In Proceedings of the Second Text Analysis Conference (TAC 2009).\n\n- Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW'05, page 177-190, Berlin, Heidelberg. Springer-Verlag.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n- Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305.\n\n- William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).\n\n- Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third Pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, RTE '07, page 1-9, USA. Association for Computational Linguistics.\n\n- Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256.\n\n- Priya Goyal, Piotr D\u00e1l\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate, large minibatch sgd: TrainingImagenet in 1 hour.\n\n- Yaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visualizing and understanding the effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4143-4152, Hong Kong, China. Association for Computational Linguistics.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778.\n\n- Sepp Hochreiter. 1991. Untersuchungen zu dynamischen neuronalen netzen. *Diploma, Technische Universit\u00e4t M\u00fcnchen*, 91(1).\n\n- Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n\n- James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, and et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526.\n\n- Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations.\n\n- Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. 2020. Mixout: Effective regularization to finetune large-scale pretrained language models. In International Conference on Learning Representations.\n\n- Howard Levene. 1960. Robust tests for equality of variances. Contributions to probability and statistics: Essays in honor of Harold Hotelling, pages 278-292.\n\n- Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018. Visualizing the loss landscape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 6389-6399. Curran Associates, Inc.\n\n- Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. 2020. On the variance of the adaptive learning rate and beyond. In International Conference on Learning Representations.\n\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\n\n- Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.\n\n- Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in connectionist networks: The sequential learning problem. In *Psychology of learning and motivation*, volume 24, pages 109-165. Elsevier.\n\n- Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448, Florence, Italy. Association for Computational Linguistics.\n\n- Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.\n\n- Timothy Niven and Hung-Yu Kao. 2019. Probing neural network comprehension of natural language arguments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658-4664, Florence, Italy. Association for Computational Linguistics.\n\n- Jason Phang, Thibault F\u00e9vry, and Samuel R Bowman. 2018. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088.\n\n- Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R Bowman. 2020. Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work? arXiv preprint arXiv:2005.00628.\n\n- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.\n\n- Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019a. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9 Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3266-3280. Curran Associates, Inc.\n\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations.\n\n- Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.\n\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumont, Clement Delangue, Anthony Moi, Pierrick Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. 2019. Transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.\n\n- Eric Wong, Leslie Rice, and J. Zico Kolter. 2020. Fast is better than free: Revisiting adversarial training. In International Conference on Learning Representations.\n\n- Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. On layer normalization in the transformer architecture. arXiv preprint arXiv:2002.04745.\n\n- Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. 2020. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations.\n\n- Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. 2020. Revisiting Few-sample BERT Fine-tuning. arXiv preprint arXiv:2006.05987.\n\n- Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2020. Freelb: Enhanced adversarial training for natural language understanding. In International Conference on Learning Representations.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638636.1089296", "msecs": "108.0", "relativeCreated": "232939.42713737488", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code\n\n# Abstract\nCode has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline.\n\n\n# 1 INTRODUCTION\n\n\nVarious studies (Azerbayev et al., 2024; Shao et al., 2024) have shown that training on code enhances the mathematical reasoning abilities of large language models (LLMs). Previous research in continued mathematical pretraining often includes code that utilizes math-related packages (Azerbayev et al., 2024). This code is typically sourced from GitHub and is primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than focusing directly on mathematics. Recent models (Zhou et al., 2024; Yang et al., 2024b; Ying et al., 2024; Shao et al., 2024; Wang et al., 2023a) have adopted Tool-Integrated Reasoning (TIR) in fine-tuning. They utilize integrated natural language reasoning steps and Python code to improve performance on mathematical reasoning tasks. Reasoning with the help of code is particularly effective for more challenging problems, likely due to its precision and accuracy.\n\n\nAlthough utilizing existing open-source code in the pretraining phase can enhance the mathematical reasoning abilities of LLMs, such code often lacks accompanying natural language explanations or context. This might hinder the model's ability to fully understand them. In this paper, we propose a novel method for generating large amounts of mathematical code accompanied by corresponding natural language reasoning steps, which are extracted from math-related pretraining texts. Different from the existing math-related code, our generated code is paired with natural language reasoning steps, making the code more comprehensible. Also, as our code is generated based on math-related texts, they are all highly related to mathematical reasoning. When used in pretraining, the mathe\n\n\nmatical code paired with reasoning steps facilitates LLMs' understanding of math-related pretraining texts, as it effectively captures the underlying reasoning process. Furthermore, this data enhances the model's potential to be finetuned for TIR reasoning.\n\n\nOur data processing pipeline consists of two key steps: (1) carefully curating a robust basic dataset for pretraining, and (2) generating paired reasoning steps and mathematical code by extracting Latex expressions and their context, translating the extracted information into Python code snippets, executing the generated code snippets, and verifying their correctness.\n\n\nFirst, we gather and carefully filter a wide variety of math-related data sources, including web pages, model-generated data, math-related code, and textbooks. Through an advanced filtering process, we ensure the dataset is both large and highly relevant, minimizing irrelevant content while preserving the mathematical texts necessary for training. This results in a 16.5B-token dataset that forms the foundation of our pretraining efforts. By conducting experiments with smaller models, we show that this careful curation leads to more efficient training without sacrificing model performance.\n\n\nSecond, we propose a novel method for generating large amounts of paired mathematical reasoning steps and their corresponding Python code. Given a piece of text from the pretraining corpus collected above, we wrap it in a carefully designed prompt that instructs a Llama-3.1-70B-Instruct model to extract LaTeX expressions along with their relevant context, including the conditions for each expression and the result of its computation. This results in a list of comprehensive mathematical reasoning steps, complete with the necessary conditions, the computations taken, and the results. Then, we prompt the model to translate each reasoning step into a Python code snippet that captures the underlying reasoning process. The generated Python snippets are executed, and only those that run successfully and produce outputs matching the expected results are retained. By pairing the code with the corresponding reasoning step, we create the final data. The process is demonstrated in the lower half of Fig. 1. This process yields a 2.7B-token corpus of mathematical code snippets accompanied with their corresponding reasoning steps, which we combine with the data generated in the first step to create a 19.2B-token pretraining dataset, named MathCode-Pile.\n\n\nWe validate the effectiveness of MathCode-Pile on four popular base models: Llama-3-8B, DeepSeekMath-7B, Mistral-7B, and Code-Llama-7B, significantly improving their performance on five representative mathematical benchmarks. We name the resulting family of pretrained models MathCoder2. In particular, MathCoder2-Llama-3-8B achieves 4-shot accuracies of  $38.4\\%$  on MATH and  $69.9\\%$  on GSM8K, outperforming the baseline of training only on the basic data generated in the first step by  $3.1\\%$  and  $4.1\\%$ , respectively. This demonstrates that the data of mathematical code accompanied with reasoning steps effectively enhances LLMs' reasoning abilities.\n\n\nDifferent from recent works, such as DeepSeekMath (Shao et al., 2024), InternLM-Math (Ying et al., 2024), and Qwen2.5-Math (Yang et al., 2024b), which only release their model weights, we offer a detailed, open-source framework for data processing and training that achieves performance competitive with these models, fostering further progress in mathematical reasoning for LLMs.\n\n\nOur contributions include:\n\n- - A novel and effective method for generating large amounts of mathematical code with corresponding natural language reasoning steps, significantly enhancing pretraining outcomes.\n\n- - The creation of MathCode-Pile, a meticulously curated 19.2B-token dataset for continued mathematical pretraining. This dataset includes math-related web data, synthetic data, code, textbooks, and model-translated mathematical code.\n\n- - Full open-sourcing of all data processing and training code, ensuring transparency and reproducibility to support future research.\n\n\n# 2 CURATION OF MATHCODE-PILE\n\n\nWe curate our mathematical pretraining dataset, MathCode-Pile, in two steps: first, we collect the basic data in Sec. 2.1, and then we use it to generate mathematical code snippets with their corresponding natural language reasoning steps in Sec. 2.2.\n\n\n![Figure 1: The data processing pipeline. (a) shows the pipeline of prior works. (b) demonstrates our method. We first use a fastText classifier to filter the Common Crawl corpus, resulting in the initial filtered math texts. Then, we annotate part of the filtered texts to train a new fastText classifier, and conduct a second filtering, resulting in the finer filtered math texts. Then we use an instruction-tuned LLM to extract reasoning steps from these math-related texts, and translate the reasoning steps into corresponding code snippets. We execute the code snippets and compare the output with the expected result. If the code executes successfully and the result is as expected, the code is retained.](images/d9702e8efdfee905cc47e77b78e3b4c586dd9d0fe136c552c6913e5f0cee72b9.jpg)\n\n\n# 2.1 BASIC DATA\n\n\nWe collect and carefully filter a diverse range of mathematical data to ensure relevance and quality for continued pretraining of LLMs. The data includes math-related web content, synthetic data, code utilizing mathematical packages, and mathematical textbooks.\n\n\nMath-related Web Data. Web data offers a broad range of real-world mathematical examples. We start with the OpenWebMath (Paster et al., 2023) dataset, which contains mathematical web pages sourced from Common CWEl. Observing that a significant portion of these documents are unrelated to mathematics, we instruct the Mixtral-8x7B-Instruct model with a carefully designed prompt (detailed in Appendix A) to filter out irrelevant texts. Examples of irrelevant texts are shown in Appendix D. This reduces the dataset from 13.7B tokens to 4.8B tokens (measured using the Llama-3 tokenizer). We call this filtered version filtered-OpenWebMath.\n\n\nTo further expand the dataset, we train a fastText classifier (Joulin et al., 2016) using filtered-OpenWebMath as positive samples and random Common Crawl data as negative samples (training details are explained in Appendix. B). This model helps identify additional math-related documents within the Common Crawl data from Matrix (Zhang et al., 2024), a general pretraining dataset. A second round of filtering is performed, where Mixtral-8x7B-Instruct annotates a portion of these documents, and a new fastText classifier trained based on these annotations further refines the data. This produces 6.4B tokens, which we label as filtered-CC-En-math. Finally, we combine filtered-OpenWebMath and filtered-CC-En-math, resulting in a comprehensive 11.2B-token math-related web dataset.\n\n\nSynthetic Data. Synthetic data offers structured mathematical texts that complement the web data. We collect synthetic data from various open-source repositories on Hugging Face, including datasets like Education-College-Students<sup>1</sup>, Maths-College<sup>2</sup>, and synthetic math books from Matrix (Zhang\n\n\nPrompt: You will be presented with a text related to math. I need you to identify all the complex computations in it. For each complex computation, find out the conditions needed for the computation, the LaTeX expression that conducts the computation, and the result of the computation. Then generate a Python code snippet for each computation that demonstrates how the result is reached. Output each computation in the following format:\n\n\nConditions Needed:\n\n- 1. [Condition 1]\n\n- 2. [Condition 2]\n\n\n\u00b7\u00b7\u00b7\n\n\nComputation Expression:\n\n\n[ \\text{[LaTeX Expression]} ]\n\n\nComputation Result:\n\n\n[Computation Result]\n\n\nPython Code Snippets:\n\n\n``python\n\n\n[Python Code]\n\n\n1\n\n\nThere can be more than one complex computation in the text. Output only the computations that require calculation. Do not include mathematical statements or definitions as a computation. Make sure each snippet can be executed individually. The text is as follows: {TEXT}\n\n\nThe computations are:\n\n\nFigure 2: The prompt for extracting reasoning steps from texts in the pretraining corpus and generating the corresponding Python snippets. {TEXT} is replaced with the text from the dataset collected in Sec. 2.1.\n\n\net al., 2024). To ensure relevance, we apply a fastText classifier to filter out non-mathematical documents, refining the dataset to 2.2B tokens of high-quality synthetic math content.\n\n\nCode Utilizing Mathematical Packages. Code data offers practical examples of how mathematical libraries are used in programming. We collect code from Python and Jupyter files within the StarCoderData dataset (Li et al., 2023), retaining only programs that import math-related packages such as sympy, fractions, cmath, scipy, or statistics. The widely used numpy package is not used to filter the documents, as it appears frequently in non-mathematical contexts. After filtering, this collection process results in 1.7B tokens of code related to mathematical computations.\n\n\nMathematical Textbooks. Textbooks provide formal, structured presentations of mathematical concepts, making them a valuable source of math knowledge. We gather 8K PDFs of textbooks from online resources by identifying those with titles containing math-related keywords such as algebra, geometry, probability, etc. These PDF files are then converted into markdown format using the Nougat tool for easier integration into our training pipeline.\n\n\n# 2.2 MODEL-TRANSLATED MATHEMATICAL CODE\n\n\nIn this section, we propose a novel approach for extracting reasoning steps from the basic pretraining data and translating them into corresponding Python code snippets that capture the underlying reasoning processes. This extraction and translation process is performed using a strong instruction-tuned model, which is Llama-3.1-70B-Instruct in this paper.\n\n\nOur method begins with taking a piece of text from the basic pretraining data and wrapping it in a carefully designed prompt, as shown in Fig. 2. This prompt instructs the model to identify LaTeX expressions denoting complex computations, along with the necessary context, including the conditions required for the computation and the expected result. By explicitly extracting the conditions of the LaTeX expression, we enhance the model's ability to comprehend the underlying mathematical reasoning behind the usage of the expression. The expected result of the computation can later serve as a basis for verifying the correctness of the generated code. A mathematical reasoning step is con\n\n\nTable 1: The components and data statistics of MathCode-Pile.\n\n\n<table><tr><td>Components</td><td>Size (MB)</td><td>Documents</td><td>Tokens</td><td>Average (Tokens)</td></tr><tr><td>Filtered-OpenWebMath</td><td>16,999</td><td>2,824,705</td><td>4,826,902,621</td><td>1,709</td></tr><tr><td>Filtered-CC-En-math</td><td>23,465</td><td>7,597,718</td><td>6,341,745,645</td><td>835</td></tr><tr><td>Synthetic data</td><td>8,855</td><td>2,195,974</td><td>2,193,189,314</td><td>999</td></tr><tr><td>Code using math packages</td><td>6,120</td><td>513,059</td><td>1,703,226,005</td><td>3,320</td></tr><tr><td>Mathematical textbooks</td><td>4,431</td><td>8,373</td><td>1,390,268,773</td><td>166,042</td></tr><tr><td>Translated mathematical code</td><td>8,235</td><td>6,347,823</td><td>2,728,740,985</td><td>430</td></tr><tr><td>Total</td><td>68,105</td><td>19,487,652</td><td>19,184,073,343</td><td>984</td></tr></table>\n\n\nstructured by combining the conditions, expression and result. The prompt then directs the model to produce a Python code snippet that accurately reflects the underlying reasoning process behind the extracted reasoning step. The model is asked to present the conditions, LaTeX expression, result, and Python code snippet in a structured format, ensuring that each part can be easily extracted from the generated text. Examples of generated texts are shown in Appendix C.\n\n\nAfter the Python code snippets are generated, they are executed, and outputs of the execution are compared with the expected results extracted from the generated text. Only the Python code snippets that execute without errors and produce correct outputs are retained. This filtering process ensures a higher quality of generated code, making the resulting dataset more reliable for mathematical pretraining compared to approaches that rely on unverified and general-purpose code.\n\n\nLeveraging the Llama-3.1-70B-Instruct model, we initially generated 3.1B tokens of the data. After applying the filtering process, we obtain a total of 2.7B tokens of high-quality data of mathematical code snippets accompanied with their corresponding reasoning steps. This newly generated data significantly enriches our original pretraining corpus. By combining this data with the basic pretraining data, we create a comprehensive pretraining dataset totaling 19.2B tokens, which we refer to as MathCode-Pile. Detailed statistics of MathCode-Pile are presented in Tab. 1. This dataset is tailored specifically for enhancing the mathematical and coding abilities of LLMs. To avoid benchmark contamination, we follow Yang et al. (2024b) to filter out samples that have significant overlaps with any of the questions from benchmark datasets used in evaluation. We use exact match to remove the identical samples and further apply 13-gram dedduplication (with a condition that the Jaccard similarity should be larger than 0.6) to remove more samples that might cause contamination.\n\n\nIn comparison to traditional methods of curating math-related code, which often draw on general-purpose repositories, our method ensures that the code is not only syntactically correct but also mathematically sound, reflecting a deeper understanding of mathematical reasoning. Our mathematical code is accompanied with corresponding natural language reasoning steps, which makes understanding the reasoning process easier. This makes MathCode-Pile a superior resource for models aimed at performing advanced mathematical reasoning tasks.\n\n\n# 3 EXPERIMENTS\n\n\nTo demonstrate the effectiveness of our method, we first train several base models ranging from 7B to 8B parameters using MathCode-Pile and compare them to other best-performing models of the same size. The group of models resulting from the continued mathematical pretraining is named MathCoder2. Next, we train and compare various other open-source math pretraining datasets against MathCode-Pile using a smaller model, DeepSeekCoder-1.3B. To showcase the potential of the MathCoder2 models, we further perform supervised fine-tuning on them. Finally, we conduct ablation studies to analyze the impact of each component of the dataset.\n\n\n# 3.1 MAIN RESULTS\n\n\nBenchmark datasets. We evaluate the MathCoder2 models on five representative datasets: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), SAT-Math (Azerbayev et al., 2024), OCW (Lewkowycz et al., 2022), and MMLU-Math (Hendrycks et al., 2021a). GSM8K and MATH\n\n\nTable 2: Performance of various pretrained models on five representative mathematical datasets. All results reported are based on greedy decoding. \"Code-open\" shows whether the code for data-processing and model-training is open-sourced. The red numbers show the improvements compared to the base model from which each MathCoder2 model is trained.\n\n\n<table><tr><td>Model</td><td>Size</td><td>Code-open</td><td>MATH</td><td>GSM8K</td><td>SAT</td><td>OCW</td><td>MMLU-MATH</td></tr><tr><td>Qwen2-Math</td><td>7B</td><td>X</td><td>50.4</td><td>80.4</td><td>87.5</td><td>14.0</td><td>57.9</td></tr><tr><td>Qwen2.5-Math</td><td>7B</td><td>X</td><td>55.4</td><td>91.6</td><td>-</td><td>-</td><td>-</td></tr><tr><td>InternLM2.5</td><td>7B</td><td>X</td><td>34.0</td><td>74.8</td><td>65.6</td><td>8.1</td><td>49.6</td></tr><tr><td>InternLM2-Math-Base</td><td>7B</td><td>X</td><td>21.5</td><td>49.2</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Llemma</td><td>7B</td><td>\u2713</td><td>18.0</td><td>36.4</td><td>53.1</td><td>7.7</td><td>-</td></tr><tr><td>Llama-2</td><td>7B</td><td>X</td><td>3.2</td><td>11.8</td><td>25.0</td><td>3.7</td><td>-</td></tr><tr><td>Llama-3</td><td>8B</td><td>X</td><td>21.4</td><td>54.8</td><td>56.3</td><td>10.3</td><td>42.8</td></tr><tr><td>MathCoder2-Llama-3</td><td>8B</td><td>\u2713</td><td>38.4(+17.0)</td><td>69.9(+15.1)</td><td>84.4(+28.1)</td><td>18.0(+7.7)</td><td>46.5(+3.7)</td></tr><tr><td>DeepSeekMath</td><td>7B</td><td>X</td><td>36.2</td><td>64.2</td><td>84.4</td><td>15.4</td><td>47.4</td></tr><tr><td>MathCoder2-DeepSeekMath</td><td>7B</td><td>\u2713</td><td>38.6(+2.4)</td><td>68.8(+4.6)</td><td>90.6(+6.2)</td><td>16.9(+1.5)</td><td>48.3(+0.9)</td></tr><tr><td>Mistral</td><td>7B</td><td>X</td><td>13.1</td><td>52.2</td><td>75.0</td><td>8.5</td><td>38.3</td></tr><tr><td>MathCoder2-Mistral</td><td>7B</td><td>\u2713</td><td>36.7(+23.6)</td><td>68.2(+16.0)</td><td>81.3(+6.3)</td><td>13.2(+4.7)</td><td>42.2(+3.9)</td></tr><tr><td>Code-Llama</td><td>7B</td><td>X</td><td>6.7</td><td>14.6</td><td>25.0</td><td>3.7</td><td>26.4</td></tr><tr><td>MathCoder2-Code-Llama</td><td>7B</td><td>\u2713</td><td>28.8(+22.1)</td><td>52.3(+37.7)</td><td>71.9(+46.9)</td><td>8.5(+4.8)</td><td>33.7(+7.3)</td></tr></table>\n\n\nare tested using a 4-shot prompt with MAmmoTH's evaluation framework (Yue et al., 2023). SATMath and OCW are tested using a 4-shot prompt with DeepSeekMath's evaluation framework (Shao et al., 2024). MMLU-Math is tested using the lm-evaluation-harness's (Gao et al., 2024) default zero-shot setting for MMLU. These datasets cover a wide range of mathematical problems across various types and difficulty levels, from primary school math word problems to college-level challenges, providing a comprehensive evaluation of the models.\n\n\nBase models and training settings. To demonstrate that our pretraining corpus is effective across different base models, we continue pretraining four base models with MathCode-Pile: Llama-3-8B (Dubey et al., 2024), DeepSeekMath-7B (Shao et al., 2024), Mistral-7B (Jiang et al., 2023), and Code-Llama-7B (Rozi\u00e8re et al., 2024). MathCoder2-Llama-3-8B is trained for 3 epochs with a global batch size of 4 million tokens and an 8192 token context length. MathCoder2-DeepSeekMath, MathCoder2-Mistral, and MathCoder2-CodeLlama are each trained for 3 epochs with a global batch size of 4 million tokens and a 4096 token context length.\n\n\nBaselines. We compare our method with various other base models that possess strong mathematical abilities and are of similar sizes, including Qwen2-Math 7B (Yang et al., 2024a), Qwen2.5-Math 7B (Yang et al., 2024b), InternLM2-Math-Base 7B (Ying et al., 2024), InternLM2.5 7B (Cai et al., 2024), DeepSeekMath 7B (Shao et al., 2024), Llemma 7B (Azerbayev et al., 2024), Mistral 7B (Jiang et al., 2023), Llama2 7B (Touvron et al., 2023), Llama3 8B (Dubey et al., 2024) and Code-Llama 7B (Rozi\u00e8re et al., 2024).\n\n\nResults: As demonstrated in Tab. 2, continued pretraining on MathCode-Pile consistently improves performance across all five benchmark datasets. MathCoder2 models rival the performance of top models like InternLM2-Math-Base, InternLM2.5, and DeepSeekMath. In particular, MathCoder2-DeepSeekMath demonstrates that our method continues to enhance the performance of DeepSeekMath, a model that has already been extensively trained on large amounts of math-related data. However, there remains a performance gap between MathCoder2 and the Qwen2-Math and Qwen2.5-Math models. This gap might be attributed to their superior computational, manual, and financial resources, which enable the scaling of data size and the further improvement of data quality, reporting a mathemtcaical dataset of 700B tokens (Yang et al., 2024b).\n\n\nIn contrast to models like Qwen2-Math, which only open-source their model weights, with much of their data processing and training details undisclosed, MathCoder2 is fully open-sourced, including all data processing pipelines and training code. This openness facilitates transparency, reproducibil-\n\n\nTable 3: Performance of various finetuned models on five representative mathematical datasets. All results reported are based on greedy decoding.\n\n\n<table><tr><td>Model</td><td>Size</td><td>MATH</td><td>GSM8K</td><td>OCW</td><td>Olympiad Bench</td><td>SVAMP</td></tr><tr><td>Qwen2-Math-Instruct</td><td>7B</td><td>75.1</td><td>89.9</td><td>34.6</td><td>38.2</td><td>-</td></tr><tr><td>Qwen2.5-Math-Instruct</td><td>7B</td><td>83.6</td><td>95.2</td><td>37.1</td><td>41.6</td><td>-</td></tr><tr><td>DeepSeekMath-Instruct-CoT</td><td>7B</td><td>46.8</td><td>82.9</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DeepSeekMath-Instruct-TIR</td><td>7B</td><td>57.4</td><td>83.7</td><td>-</td><td>-</td><td>-</td></tr><tr><td>InternLM2-math-plus</td><td>7B</td><td>54.4</td><td>84.0</td><td>17.3</td><td>18.8</td><td>-</td></tr><tr><td>NuminaMath-7B-CoT</td><td>7B</td><td>55.2</td><td>75.4</td><td>19.1</td><td>19.9</td><td>-</td></tr><tr><td>NuminaMath-7B-TIR</td><td>7B</td><td>68.1</td><td>84.6</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ToRA-Code</td><td>7B</td><td>44.6</td><td>72.6</td><td>-</td><td>-</td><td>70.4</td></tr><tr><td>MathCoder</td><td>7B</td><td>30.2</td><td>67.8</td><td>-</td><td>-</td><td>70.7</td></tr><tr><td>MAmmoTH2-Plus</td><td>8B</td><td>42.8</td><td>84.1</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Llama-3.1-Instruct</td><td>8B</td><td>47.2</td><td>76.6</td><td>21.7</td><td>15.4</td><td>-</td></tr><tr><td>MathCoder2-Llama-3-Instruct-CoT</td><td>8B</td><td>58.5</td><td>83.9</td><td>29.4</td><td>25.8</td><td>92.7</td></tr><tr><td>MathCoder2-Llama-3-Instruct-TIR</td><td>8B</td><td>69.7</td><td>85.8</td><td>37.6</td><td>37.6</td><td>94.9</td></tr><tr><td>MathCoder2-DeepSeekMath-Instruct-CoT</td><td>7B</td><td>55.2</td><td>80.3</td><td>30.9</td><td>23.0</td><td>92.1</td></tr><tr><td>MathCoder2-DeepSeekMath-Instruct-TIR</td><td>7B</td><td>69.6</td><td>86.5</td><td>41.9</td><td>37.9</td><td>92.8</td></tr></table>\n\n\nity, and further research, which is crucial for advancing the field. Compared to Llemma, which also open-sources its code, our method achieves better performance on the five datasets. Particularly, when trained on the same base model, Code-Llama, our method performs significantly better, which demonstrates the effectiveness of the MathCode-Pile pretraining corpus.\n\n\n# 3.2 POST-TRAINING\n\n\nTo further demonstrate the potential of the MathCoder2 models in aligning to mathematical problem-solving tasks, we select the MathCoder2-Llama-3-8B model and MathCoder2-DeepSeekMath-7B for finetuning on mathematical problem-solution pairs. We first train the base model on general mathematical instructions following Yue et al. (2024) for two epochs. Subsequently, we finetune the model on NuminaMath-CoT $^3$ , and NuminaMath-TIR $^4$  datasets for three epochs.\n\n\nThe results are shown in Tab. 3. MathCoder2-Instruct-TIR achieves high performance on all five datasets, reaching  $69.7\\%$  on MATH and  $86.5\\%$  on GSM8K, outperforming many of the best open-source models of similar size and demonstrating our method's potential to improve performance on downstream mathematical reasoning tasks. As this paper focuses on continued mathematical pretraining, the post-training serves only as a validation of the potential of our models. We conducted only simple supervised fine-tuning, without performing reinforcement learning or direct preference optimization, which could further improve performance on downstream tasks.\n\n\n# 3.3 ABLATION STUDIES\n\n\nIn this session, we first analyze the impact of various components of the training data. Next, we compare MathCode-Pile to other open-source mathematical pretraining corpora.\n\n\nAnalysis of the impact of the mathematical code. We analyze the impact of the mathematical code on continued pretraining by comparing the results of adding and not adding the mathematical code. As shown in Tab. 4, the addition of the mathematical code in the pretraining corpus significantly improves performance across all five datasets. Note that the mathematical code only constitutes  $14.1\\%$  of the 19.2B tokens in the MathCode-Pile dataset, yet the improvement in accuracy it brings about compared to the total improvement in accuracy  $(\\frac{acc_{MathCode - Pile} - acc_{basic}}{acc_{MathCodePile} - acc_{orig}})$  is  $21.8\\%$ ,  $27.1\\%$ ,  $44.5\\%$ ,  $66.2\\%$ , and  $35.1\\%$  on the five benchmark datasets, respectively, demonstrating the effectiveness of the mathematical code. Comparison across different training steps is shown in Appendix F.\n\n\nTable 4: Analysis of the impact of the mathematical code. The upper half presents the results of using and not using the mathematical code data. The lower half analyzes design of concatenating the reasoning steps and code snippets. \"Basic + Reasoning-step-only\" represents only adding the conditions, expressions, and results, while \"Basic + Trans-code-only\" represents only adding the translated code. \"Basic + Separated Text&Code\" represents seperating corresponding code and text. \"Reasoning-Step&Code\" represents the concatenated data combining both. \"Basic + No-code-prompt\" represents using a prompt that simply instruct Llama-3.1-70B-Instruct to rewrite texts to improve their quality.\n\n\n<table><tr><td>Data Composition</td><td>Base Model</td><td>MATH</td><td>GSM8K</td><td>SAT</td><td>OCW</td><td>MMLU-MATH</td></tr><tr><td>Basic</td><td>Llama-3-8B</td><td>34.7</td><td>65.8</td><td>71.9</td><td>12.9</td><td>45.2</td></tr><tr><td>Basic + Reasoning-Step&amp;Code</td><td>Llama-3-8B</td><td>38.4(+3.7)</td><td>69.9(+4.1)</td><td>84.4(+12.5)</td><td>18.0(+5.1)</td><td>46.5(+1.3)</td></tr><tr><td>Basic + Reasoning-step-only</td><td>DeepSeekCoder-1.3B</td><td>16.7</td><td>22.7</td><td>40.6</td><td>4.8</td><td>25.9</td></tr><tr><td>Basic + Trans-code-only</td><td>DeepSeekCoder-1.3B</td><td>14.6</td><td>22.1</td><td>43.8</td><td>5.5</td><td>25.5</td></tr><tr><td>Basic + No-code-prompt</td><td>DeepSeekCoder-1.3B</td><td>15.7</td><td>21.3</td><td>37.5</td><td>4.8</td><td>24.4</td></tr><tr><td>Basic + Separated Text&amp;Code</td><td>DeepSeekCoder-1.3B</td><td>17.0</td><td>22.0</td><td>46.9</td><td>4.8</td><td>25.3</td></tr><tr><td>Basic + Reasoning-Step&amp;Code</td><td>DeepSeekCoder-1.3B</td><td>17.8</td><td>25.5</td><td>59.4</td><td>5.9</td><td>26.1</td></tr></table>\n\n\nTable 5: Analysis of the effect of different components in MathCoder2-Pile. The base model is DeepSeekCoder-1.3B.\n\n\n<table><tr><td>Data Composition</td><td>MATH</td><td>GSM8K</td><td>SAT</td><td>OCW</td><td>MMLU-MATH</td></tr><tr><td>No Math Training</td><td>4.8</td><td>4.3</td><td>18.8</td><td>2.6</td><td>24.8</td></tr><tr><td>filtered-OpenWebMath (4.8B)</td><td>9.0</td><td>11.4</td><td>34.4</td><td>3.7</td><td>25.4</td></tr><tr><td>OpenWebMath (12.9B)</td><td>9.4</td><td>11.2</td><td>31.3</td><td>2.6</td><td>24.4</td></tr><tr><td>filtered-CC-En-math (6.4B)</td><td>9.1</td><td>12.1</td><td>31.3</td><td>3.7</td><td>25.2</td></tr><tr><td>CC-En-math (22.1B)</td><td>8.4</td><td>13.0</td><td>25.0</td><td>2.9</td><td>25.0</td></tr><tr><td>filtered-OpenWebMath + textbooks</td><td>9.4</td><td>12.7</td><td>50.0</td><td>4.0</td><td>25.4</td></tr><tr><td>filtered-OpenWebMath + synthetic data</td><td>10.8</td><td>12.6</td><td>50.0</td><td>4.0</td><td>25.6</td></tr><tr><td>filtered-OpenWebMath + code</td><td>9.4</td><td>12.1</td><td>46.9</td><td>4.0</td><td>25.4</td></tr><tr><td>MathCoder2-Pile</td><td>17.8</td><td>25.5</td><td>59.4</td><td>5.9</td><td>26.1</td></tr></table>\n\n\nWe also analyze the design choice of concatenating the natural language reasoning step with the mathematical code for pretraining. This analysis is conducted by studying the results of adding only the natural language reasoning steps, and separately adding only the translated code. As shown in Tab. 4, Basic + Reasoning-step-only represents adding only the natural language reasoning steps; Basic + Trans-code-only represents adding only the translated code; Basic + Separated Text&Code represents seperating code and text; and Basic + Reasoning-Step&Code represents adding the concatenated data that combines both. The Basic + Reasoning-Step&Code configuration results in the best performance, demonstrating the importance of including both the natural language reasoning step and the translated mathematical code.\n\n\nTo rule out the possibility that the improvement comes from the higher quality of texts generated by Llama-3.1-70B-Instruct, we use a prompt that asks Llama-3.1-70B-Instruct to rewrite the given text. The details of this prompt are provided in Appendix E. We present the results of replacing the mathematical code with texts generated using this prompt in Tab. 4, labeled as \"Basic + No-code-prompt\". Our method of generating mathematical code accompanied with corresponding reasoning steps outperforms this baseline, demonstrating the effectiveness of our approach.\n\n\nAnalysis of the impact of various parts of the basic data. We perform experiments on a smaller model, DeepSeekCoder-1.3B, using different parts of the basic data. As demonstrated in Tab. 5, filtered-OpenWebMath and filtered-CC-En-math significantly improve the performance of the model. In comparison, textbooks, synthetic data, and code are smaller in data size and play a less important role. As each of these parts of data is too small for individual pretraining, we combine\n\n\nTable 6: Comparison between MathCode-Pile and other Mathematical Pretrain datasets.\n\n\n<table><tr><td>Pretrain Dataset</td><td>Base Model</td><td>MATH</td><td>GSM8K</td><td>SAT</td><td>OCW</td><td>MMLU-MATH</td></tr><tr><td>No Math Training</td><td>DeepSeekCoder-1.3B</td><td>4.8</td><td>4.3</td><td>18.8</td><td>2.6</td><td>24.8</td></tr><tr><td>OpenWebMath</td><td>DeepSeekCoder-1.3B</td><td>9.4</td><td>11.2</td><td>31.3</td><td>2.6</td><td>24.4</td></tr><tr><td>Proof-Pile-2</td><td>DeepSeekCoder-1.3B</td><td>9.2</td><td>11.2</td><td>50.0</td><td>4.4</td><td>25.8</td></tr><tr><td>MathPile</td><td>DeepSeekCoder-1.3B</td><td>5.3</td><td>3.4</td><td>21.9</td><td>2.2</td><td>24.9</td></tr><tr><td>DeepSeekMath Corpus</td><td>DeepSeekLLM-1.3B</td><td>13.6</td><td>23.8</td><td>56.3</td><td>4.8</td><td>-</td></tr><tr><td>MathCoder2-Pile</td><td>DeepSeekCoder-1.3B</td><td>17.8</td><td>25.5</td><td>59.4</td><td>5.9</td><td>26.1</td></tr></table>\n\n\nTable 7: Comparison between finetuning the original Llama-3-8B, MathCoder2-Basic-Llama-3-8B, and MathCoder2-Llama-3-8B on NuminaMath-TIR. MathCoder2-Basic-Llama-3-8B is the model resulting from continued pretraining on the basic data without adding the model-translated mathematical code.\n\n\n<table><tr><td>Base Model</td><td>MATH</td><td>GSM8K</td><td>OCW</td><td>Olympiad Bench</td><td>SVAMP</td></tr><tr><td>Llama-3-8B</td><td>56.1</td><td>80.1</td><td>24.6</td><td>28.4</td><td>83.8</td></tr><tr><td>MathCoder2-Basic-Llama-3-8B</td><td>62.9</td><td>81.3</td><td>26.8</td><td>32.9</td><td>86.7</td></tr><tr><td>MathCoder2-Llama-3-8B</td><td>65.1</td><td>84.5</td><td>34.6</td><td>34.4</td><td>87.9</td></tr></table>\n\n\nthem with OpenWebMath-filtered to show that they each bring a small yet noticeable improvement compared to using only OpenWebMath-filtered. Since we performed filtering on OpenWebMath and the initially filtered CC-En to remove irrelevant data, we also compare the performance before and after filtering. We observe that there is no obvious degradation in performance after removing irrelevant content, showing the effectiveness of the filtering.\n\n\nComparison with other open-source mathematical pretraining corpora. We compare MathCode-Pile with various other open-source mathematical pretraining corpora. We train each corpus for 3 epochs with a global batch size of 2 million tokens and a 4096 token context length, since we observe that the model's performance usually saturates around 3 epochs. As shown in Tab. 6, MathCode-Pile significantly outperforms OpenWebMath, Proof-Pile-2, and MathPile when trained on DeepSeekCoder-1.3B. The DeepSeekMath Corpus is not open-source, and its performance on DeepSeekLLM-1.3B is taken from Shao et al. (2024), which is trained for 150B tokens, more than our MathCode-Pile's training of approximately 60B tokens. The 1.3B model trained with MathCode-Pile outperforms the 1.3B model trained with DeepSeekMath Corpus.\n\n\nAnalysis of the improvement on the potential of being finetuned for TIR reasoning. To analyze the effect of the model-translated mathematical code on LLMs' potential to be finetuned for TIR reasoning, we finetune the original Llama-3-8B, MathCoder2-Basic-Llama-3-8B, and MathCoder2-Llama-3-8B on NuminaMath-TIR for three epochs, respectively. As shown in Tab. 7, the results of finetuning on MathCoder2-Basic-Llama-3-8B are higher than the results of finetuning on Llama-3-8B. Finetuning on MathCoder2-Llama-3-8B results in even higher performance than finetuning on MathCoder2-Basic-Llama-3-8B, showing that the addition of mathematical code effectively enhances the models' potential of being finetuned for TIR reasoning.\n\n\n# 4 RELATED WORK\n\n\nContinued mathematical pretraining. Several works (Shao et al., 2024; Azerbayev et al., 2024; Ying et al., 2024; Yang et al., 2024b) have explored the continued pretraining of LLMs on mathematical data, such as mathematical web content, synthetic data, and code. InternLM-Math (Ying et al., 2024) and Query of CC Fei et al. (2024) use BM25 for data retrieval, while other works such as DeepSeekMath (Shao et al., 2024) and Qwen2-Math (Yang et al., 2024b) employ fastText (Joulin et al., 2016) and other meta-information to retrieve texts from Common Crawl. Our approach follows these methods by using fastText for data filtering, and we introduce a second iteration of finer filtering to retain more relevant data. MathPile (Wang et al., 2023b) and phi (Gunasekar et al.,\n\n\n2023) utilize real or synthesized textbooks, while Llemma (Azerbayev et al., 2024) and Qwen2-Math (Yang et al., 2024b) incorporate math-related code in their datasets. However, unlike our method of generating mathematical code with accompanied natural language reasoning, their code mostly has no natural language explanations or context. Our work builds on these prior efforts by collecting and expanding upon these sources of math-related text. Unlike works that only open-source their model weights, we take a more transparent approach by open-sourcing both our data processing and model training code, thereby ensuring reproducibility and facilitating future research in this field. Compared to Llemma (Azerbayev et al., 2024), which also open-source their data and training code, our method results in better performance on mathematical reasoning tasks.\n\n\nSynthetic data. Numerous finetuning (Yu et al., 2024; Wang et al., 2023a; Lu et al., 2024a) and pretraining Gunasekar et al. (2023); Wang et al. (2023b); Yang et al. (2024b) studies have explored training on synthetic data generated using language models or predefined templates. MathGLM (Yang et al., 2023) and InternLM-Math (Ying et al., 2024) use templates to generate synthetic numerical operation data, while phi (Gunasekar et al., 2023) produces textbook-quality data with models. EntiGraph (Yang et al., 2024c) generates diverse text by drawing connections between sampled entities. Our work proposes a novel method for extracting mathematical reasoning steps and generating synthetic code snippets that captures the underlying reasoning processes.\n\n\nPost-training. There are many methods for further improving the mathematical problem-solving abilities of LLMs. Supervised finetuning adjusts pretrained models using math problems and solutions in various formats, such as Chain-of-Thought (Yu et al., 2024; Yuan et al., 2023), Program-of-Thought (Yue et al., 2023), and Tool-Integrated Reasoning (Gou et al., 2024; Wang et al., 2023a; Liao et al., 2024). Reinforcement learning Lightman et al. (2023); Wang et al. (2024) and Direct Preference Optimization Rafailov et al. (2024); Xu et al. (2024); Lu et al. (2024b) utilize mathematical preference data to adjust the models' outputs. These methods are diverse and reveal the potential of pretrained models. Their performance is often influenced by the quality of the training data used in the pretraining stage. To explore the potential of finetuning our pretrained models for downstream tasks, we conduct supervised finetuning with existing open-source data.\n\n\n# 5 LIMITATIONS AND FUTURE WORK\n\n\nOne limitation of our work is that our continued pretraining corpus focuses primarily on mathematics and does not intentionally include other STEM subjects. Additionally, our pretraining data consists entirely of English texts, without incorporating math-related content in other languages, like Chinese. Due to limitations in computational resources, we only trained models ranging from 1.3B to 8B parameters. Future work could address these limitations by expanding the dataset to include other subjects and languages and by training on larger language models. Also, this paper primarily focuses on continued mathematical pretraining, so we did not apply reinforcement learning methods like PPO and GRPO, or Direct Preference Optimization in our post-training phase, which can further improve performance on mathematical reasoning tasks. In the future, we could explore these methods on our finetuned models. Also, this work did not discuss theorem proving with formal languages such as Lean and Coq, which is worth investigating in future works.\n\n\n# 6 CONCLUSION\n\n\nIn this paper, we present an effective open-source continued mathematical pretraining pipeline for enhancing mathematical reasoning of LLMs. Through the meticulous collection and filtering of diverse math-related texts, such as mathematical web content, synthetic data, code that uses mathematical packages, and math textbooks, we curate a basic dataset for continued mathematical pretraining. We then propose a novel method for extracting mathematical reasoning steps from the previously collected dataset and translating them to code snippets reflecting the underlying reasoning processes. By combining the basic data with the model-generated mathematical code accompanied with their corresponding reasoning steps, we produce a 19.2B-token mathematical pretraining corpus named MathCode-Pile, which significantly improves the performance of four different base models across five representative mathematical benchmarks. By open-sourcing the entire data processing pipeline and model training code, we actively promote transparency, reproducibility, and collaboration within the research community, facilitating future research in this area.\n\n\n# REFERENCES\n\n- Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics, 2024. URL https://arxiv.org/abs/2310.10631.\n\n- Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piaq: Reasoning about physical commonsense in natural language, 2019. URL https://arxiv.org/abs/1911.11641.\n\n- Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zhang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internl m2 technical report, 2024.\n\n- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168.\n\n- Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\n- Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Conghui He, Hang Yan, Dahua Lin, and Xipeng Qiu. Query of cc: Unearthing large scale domain-specific knowledge from public corpora, 2024. URL https://arxiv.org/abs/2401.14624.\n\n- Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602.\n\n- Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving, 2024. URL https://arxiv.org/abs/2309.17452.\n\n- Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2023. URL https://arxiv.org/abs/2306.11644.\n\n- Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021a. URL https://arxiv.org/abs/2009.03300.\n\n- Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021b. URL https://arxiv.org/abs/2103.03874.\n\n- Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825.\n\n- Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification, 2016. URL https://arxiv.org/abs/1607.01759.\n\n- Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858.\n\n- Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023. URL https://arxiv.org/abs/2305.06161.\n\n- Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. Mario: Math reasoning with code interpreter output - a reproducible pipeline, 2024. URL https://arxiv.org/abs/2401.08190.\n\n- Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step, 2023. URL https://arxiv.org/abs/2305.20050.\n\n- Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation, 2023. URL https://arxiv.org/abs/2305.01210.\n\n- Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms, 2024a. URL https://arxiv.org/abs/2402.16352.\n\n- Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Step-controlled dpo: Leveraging stepwise error for enhanced mathematical reasoning, 2024b. URL https://arxiv.org/abs/2407.00782.\n\n- Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text, 2023. URL https://arxiv.org/abs/2310.06786.\n\n- Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model, 2024. URL https://arxiv.org/abs/2305.18290.\n\n- Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950.\n\n- Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. URL https://arxiv.org/abs/1907.10641.\n\n- Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300.\n\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288.\n\n- Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning, 2023a. URL https://arxiv.org/abs/2310.03731.\n\n- Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024. URL https://arxiv.org/abs/2312.08935.\n\n- Zengzhi Wang, Rui Xia, and Pengfei Liu. Generative ai for math: Part i - mathpile: A billion-token-scale pretraining corpus for math, 2023b. URL https://arxiv.org/abs/2312.17120.\n\n- Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, Jie Tang, and Yuxiao Dong. Chatglm-math: Improving math problem-solving in large language models with a self-critique pipeline, 2024. URL https://arxiv.org/abs/2404.02893.\n\n- An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a.\n\n- An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024b. URL https://arxiv.org/abs/2409.12122.\n\n- Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang. Gpt can solve mathematical problems without a calculator, 2023. URL https://arxiv.org/abs/2309.03241.\n\n- Zitong Yang, Neil Band, Shuangping Li, Emmanuel Cand\u00e8s, and Tatsunori Hashimoto. Synthetic continued pretraining, 2024c. URL https://arxiv.org/abs/2409.07431.\n\n- Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, and Dahua Lin. Internl m- math: Open math large language models toward verifiable reasoning, 2024.\n\n\nPrompt: You will be provided with a block of text. I need you to classify the text into one of the following types:\n\n- 1. The text describes a mathematical problem and its solution.\n\n- 2. The text explains a mathematical concept or mathematical theory.\n\n- 3. The text explains a scientific or engineering concept that requires mathematical knowledge.\n\n- 4. The text describes a programming problem and its solution.\n\n- 5. The text explains a concept or theory related to programming.\n\n- 6. The text explains the usage of a programming language or software tool.\n\n- 7. The text does not belong to any of the types above.\n\n- Here's the text I've provided. Kindly analyze and classify it into type 1, 2, 3, 4, 5, 6 or 7. Put your choice behind \"The type is:\" Please do not generate any unrelated additional comments! The type number must match the type description. Here's one of the texts that needs to be classified: {TEXT} The type is:\n\n\nFigure 3: The prompt for annotation of OpenWebMath and the initially filtered CC-En documents. {TEXT} is replaced with the content of the document.\n\n- Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2024. URL https://arxiv.org/abs/2309.12284.\n\n- Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models, 2023. URL https://arxiv.org/abs/2308.01825.\n\n- Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning, 2023. URL https://arxiv.org/abs/2309.05653.\n\n- Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web, 2024. URL https://arxiv.org/abs/2405.03548.\n\n- Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830.\n\n- Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. Map-neo: Highly capable and transparent bilingual large language model series, 2024. URL https://arxiv.org/abs/2405.19327.\n\n- Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=c8McWs4Av0.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638637.008842", "msecs": "8.0", "relativeCreated": "233839.3394947052", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Deep Innovation Protection\n\n# Abstract\nEvolutionary-based optimization approaches have recently shown promising results in domains such as Atari and robot locomotion but less so in solving 3D tasks directly from pixels. This paper presents a method called Deep Innovation Protection (DIP) that allows training complex world models end-to-end for such 3D environments. The main idea behind the approach is to employ multiobjective optimization to temporally reduce the selection pressure on specific components in a world model, allowing other components to adapt. We investigate the emergent representations of these evolved networks, which learn a model of the world without the need for a specific forward-prediction loss.\n\n\n# 1 INTRODUCTION\n\n\nThe ability of the brain to model the world arose from the process of evolution. It evolved because it helped organisms to survive and strive in their particular environments and not because such forward prediction was explicitly optimized for. In contrast to the emergent neural representations in nature, current world model approaches are often directly rewarded for their ability to predict future states of the environment (Schmidhuber, 1990; Ha & Schmidhuber, 2018; Hafner et al., 2018; Wayne et al., 2018). While it is undoubtedly useful to be able to explicitly encourage a model to predict what will happen next, in this paper we are interested in what type of representations can emerge from the less directed process of artificial evolution and what ingredients might be necessary to encourage the emergence of such predictive abilities.\n\n\nIn particular, we are building on the recently introduced world model architecture introduced by Ha & Schmidhuber (2018). This agent model contains three different components: (1) a visual module, mapping high-dimensional inputs to a lower-dimensional representative code, (2) an LSTM-based memory component, and (3) a controller component that takes input from the visual and memory module to determine the agent's next action. In the original approach, each component of the world model was trained separately and to perform a different and specialised function, such as predicting the future. While Risi & Stanley (2019) demonstrated that these models can also be trained end-to-end through a population-based genetic algorithm (GA) that exclusively optimizes for final performance, the approach was only applied to the simpler 2D car racing domain and it is an open question how such an approach will scale to the more complex 3D VizDoom task that first validated the effectiveness of the world model approach.\n\n\nHere we show that a simple genetic algorithm fails to find a solution to solving the VizDoom task and ask the question what are the missing ingredients necessary to encourage the evolution of more powerful world models? The main insight in this paper is that we can view the optimization of a heterogeneous neural network (such as world models) as a co-evolving system of multiple different sub-systems. The other important insight is that representational innovations discovered in one subsystem (e.g. the visual system learns to track moving objects) require the other sub-systems to adapt. In fact, if the other systems are not given time to adapt, such innovation will likely initially have an adversarial effect on overall performance!\n\n\nIn order to optimize such co-evolving heterogeneous neural systems, we propose to reduce the selection pressure on individuals whose visual or memory system was recently changed, given the controller component time to readapt. This Deep Innovation Protection (DIP) approach is inspired by the recently introduced morphological innovation protection method of Cheney et al. (2018), which allows for the scalable co-optimization of controllers and robot body plans. Our approach is able to find a solution to the VizDoom: Take Cover task, which was first solved by the original\n\n\nworld model approach (Ha & Schmidhuber, 2018). More interestingly, the emergent world models learned to predict events important for the survival of the agent, even though they were not explicitly trained to predict the future. Additionally, our investigates into the training process show that DIP allows evolution to carefully orchestrate the training of the components in these heterogeneous architectures. We hope this work inspires more research that focuses on investigating representations emerging from approaches that do not necessarily only rely on gradient-based optimization.\n\n\n# 2 DEEP INNOVATION PROTECTION\n\n\nThe hypothesis in this paper is that to optimize complex world models end-to-end for more complex tasks requires each of its components to be carefully tuned to work well together. For example, an innovation in the visual or memory component could adversely impact the controller component, leading to reduced performance. However, in the long run such innovation could allow an individual to outperform its predecessors.\n\n\nThe agent model is based on the world model approach introduced by Ha & Schmidhuber (2018). The network includes a sensory component, implemented as a variational autoencoder (VAE) that compresses the high-dimensional sensory information into a smaller 32-dimensional representative code (Fig. 1). This code is fed into a memory component based on a recurrent LSTM (Hochreiter & Schmidhuber, 1997), which should predict future representative codes based on previous information. Both the output from the sensory component and the memory component are then fed into a controller that decides on the action the agent should take at each time step. Following Risi & Stanley (2019), we train these world models end-to-end with a genetic algorithm, in which mutations add Gaussian noise to the parameter vectors of the networks:  $\\theta' = \\theta + \\sigma \\epsilon$ , where  $\\epsilon \\sim N(0, I)$ .\n\n\nThe approach introduced in this paper aims to train heterogeneous neural systems end-to-end by temporally reducing the selection pressure on individuals with recently changed modules, allowing other components to adapt. For example, in case of the world model, in which a mutation can either affect the VAE, MDN-RNN or controller, selection pressure should be reduced if a mutation affects the VAE or MDN-RNN, giving the controller time to readapt to the changes in the learned representation. Inspired by the multi-objective morphological innovation protection introduced by Cheney et al. (2018), we employ the well-known multiobjective optimization approach NSGA-II (Deb et al., 2002), in which a second \"age\" objective keeps track of when a mutation changes either the VAE or the MDN-RNN. Every generation an individual's age is increased by 1, however, if a mutation changes the VAE or MDN-RNN, this age objective is set to zero (lower is better). Therefore, if two neural networks reach the same performance (i.e. the same final reward), the one that had less time to adapt (i.e. whose age is lower) would have a higher chance of being selected for the next generation. The second objective is the accumulated reward received during an episode. Pseudocode of the approach applied to world models is shown in Algorithm 1.\n\n\nIt is important to note that this approach is different to the traditional usage of \"age\" in multi-objective optimization, in which age is used to increase diversity and keeps track of how long individuals have been in the population (Hornby, 2006; Schmidt & Lipson, 2011). In the approach in this paper, age counts how many generations the controller component of an individual had time to adapt to an unchanged visual and memory system.\n\n\nIn the original world model approach the visual and memory component were trained separately and through unsupervised learning based on data from random rollouts. In this paper they are optimized through a genetic algorithm without evaluating each component individually. In other words, the VAE is not directly optimized to reconstruct the original input data and neither is the memory component optimized to predict the next time step; the whole network is trained in an end-to-end fashion. Here we are interested in what type of neural representations emerge by themselves that allow the agent to solve the given task.\n\n\n# 3 EXPERIMENTS\n\n\nFollowing the original world model approach (Ha & Schmidhuber, 2018), in the experiments presented here an agent is trained to solve the car racing tasks, and the more challenging VizDoom task (Kempka et al., 2016) from  $64 \\times 64$  RGB pixel inputs (Fig. 2). In the continuous control task\n\n\n```\n1: Generate random population of size N with age objectives set to 0  \n2: for generation = 1 to i do  \n3: for Individual in Population do  \n4: Objective[1] = age  \n5: Objective[2] = accumulated task reward  \n6: Increase individual's age by 1  \n7: end for  \n8: Assign ranks based on Pareto fronts  \n9: Generate set of non-dominated solutions  \n10: Add solutions, starting from first front, until number solution = N  \n11: Generate child population through binary tournament selection and mutations  \n12: Reset age to 0 for all individuals whose VAE or MDN-RNN was mutated  \n13: end for\n```\n\n\n![Figure 1: Agent Model. The agent model consists of three modules. A visual component (the encoder of a variational autoencoder) produces a latent code  $z_{t}$  at each time step  $t$ , which is concatenated with the hidden state  $h_{t}$  of the LSTM-based memory component that takes  $z_{t}$  and previously performed action  $a_{t-1}$  as input. The combined vector  $(z_{t}, h_{t})$  is input into the controller component to determine the next action of the agent. In this paper, the agent model is trained end-to-end with a multiobjective genetic algorithm.](images/c9dc77848b13bbd3ebc3add65f2af5e776748acfdb661fde47d1a487ae6ebd47.jpg)\n\n\nCarRacing-v0 (Klimov, 2016) the agent is presented with a new procedurally generated track every episode, receiving a reward of -0.1 every frame and a reward of  $+100 / N$  for each visited track tile, where  $N$  is the total number of tiles in the track. The network controlling the agent (Fig. 1) has three outputs to control left/right steering, acceleration and braking. Further details on the network model, which is the same for both domains, can be found in the Appendix. In the VizDoom: Take Cover task the agent has to try to stay alive for 2,100 timesteps, while avoiding fireballs shot at it by strafing to the left or the right. The agent receives a +1 reward for every frame it is alive. The network controlling the agent has one output  $a$  to control left ( $a < -0.3$ ) and right strafing ( $a > 0.3$ ), or otherwise standing still. In this domain, a solution is defined as surviving for over 750 timesteps, averaged across 100 random rollouts (Kempka et al., 2016).\n\n\nFollowing the NSGA-II approach, individuals for the next generation are determined stochastically through 2-way tournament selection from the  $50\\%$  highest ranked individuals in the population (Algorithm 1). No crossover operation was employed. The population size was 200. Because of the randomness in this domain, we evaluate the top three individuals of each generation one additional time to get a better estimate of the true elite. We compare a total of four different approaches:\n\n- 1. Deep innovation protection (DIP): The age objective is reset to zero when either the VAE or MDN-RNN is changed. The idea behind this approach is that the controller should get time to readapt if one of the components that precede it in the network change.\n\n- 2. Controller innovation protection: Here the age objective is set to zero if the controller changes. This setting tests if protecting components upstream can be effective in optimizing heterogeneous neural models.\n\n\n![(a) Original](images/6699ad6ebe748f87457c2c1ac6c60ddf7d7ad70577fd41876e67bcbb77d6bc09.jpg)\n\n\n![](images/f8b18f3972f5301bba913be41ea3f90b4b3daba4bcd33ba6f46e9f84d826124f.jpg)\n\n\n![(b) Scaled  $(64\\times 64\\times 3)$ (c) Original (d) Scaled  $(64\\times 64\\times 3)$ Figure 2: In the CarRacing-v0 task the agent has to learn to drive across many procedurally generated tracks as fast as possible from  $64 \\times 64$  RGB color images. In the VizDoom: Take Cover domain the agent has to learn to avoid fireballs and to stay alive as long as possible.](images/02fbcb01458b844648de84d4a2417efcca256b8cb9451dac8131248a84c0ceeb.jpg)\n\n\n![](images/f76c0dbbcefb682a5217e9565c072112ac6e554900b1c3f9b8575056d8aa35fe.jpg)\n\n\n![(a) Figure 3: VizDoom Evolutionary Training. Shown is (a) mean performance over generations together with one standard error. For a representative run of DIP (b), we plot the euclidean distances of the weights of the intermediate solutions (i.e. individuals with the highest task reward discovered so far) compared to the final solution in addition to their age and the average population age.](images/e6a49a3f4e31d4def6d729a998e0e269494c13d4c3da1b82de45b43977dc5011.jpg)\n\n\n![(b)](images/5b46b0227f0dc796ff0ecee7a2e36317f85619fdbd6a2361690ee3c8a7c97051.jpg)\n\n- 3. MDN-RNN & Controller innovation protection: This setup is the same as the controller protection approach but we additionally reset age if the MDN-RNN changes. On average, this treatment will reset the age objective as often as DIP.\n\n- 4. Random age objective: In this setup the age objective is assigned a random number between [0, 20] at each evaluation. This treatment tests if better performance can be reached just through introducing more diversity in the population.\n\n- 5. Standard GA - no innovation protection: In this non-multi-objective setup, which is the same one as introduced in Risi & Stanley (2019)<sup>1</sup>, only the accumulated reward is taken into account when evaluating individuals.\n\n\nFor all treatments, a mutation has an equal probability to either mutate the visual, memory, or controller component of the network. Interestingly, while Risi & Stanley (2019) reported that this approach performs similarly well to an approach that always mutates all components, we noticed that it performs significantly worse in the more complicated VizDoom domain. This result suggests that the more complex the tasks, the more important it is to be able to fine-tune individual components in the overall world model architecture.\n\n\n# 4 EXPERIMENTAL RESULTS\n\n\nAll results are averaged over ten independent evolutionary runs. In the car racing domain we find that there is no noticeable difference between an approach with and without innovation protection\n\n\n![Figure 4: Still frames of a learned policy. The evolved champion learned to primarily pay attention to the walls and fireballs, while ignoring the floor and ceiling. Interestingly the agent also seems to pay attention to the health and ammo indicator.](images/cb8ab7e84a6f1ddc9880a2a770987f1b60ddb62781b1f53bea9092cd02162f60.jpg)\n\n\n![](images/e24ced1a87fec7c312d3352101c2a0fb6bd351545d850c02380ef65dc3686c24.jpg)\n\n\n![](images/d1be658da592ce65dfe4fbe2b3b01ca5cf5f9faef6547c8c0ca2e1b9e1758ddb.jpg)\n\n\n![Time](images/1a64cae852bc1fd7969798a25f42f40fe06ce9d7fc193fc51d088e791682ec50.jpg)\n\n\nand both can solve the domain. However, in the more complex VizDoom task, the DIP approach that protects innovations in both VAE and MDN-RNN, outperforms all other approaches (Fig. 3a). The approach is able to find a solution to the task, effectively avoiding fireballs and reaching an average score of 824.33 (sd=491.59). To better understand the decision-making process of the agent, we calculate perturbation-based saliency maps (see Appendix for details) to determine the parts of the environment the agent is paying most attention to (Fig. 4). The idea behind perturbation-based saliency maps is to measure to what extent the output of the model changes if parts of the input image are altered (Greydanus et al., 2017). Not surprisingly, the agent learned to pay particular attention to the walls, fireballs, and the position of the monsters.\n\n\nThe better performance of the random age objective compared to no innovation protection suggests that increasing diversity in the population improves performance but less effectively than selectivity resetting the age objective as in DIP. Interestingly, the controller and the MDN-RNN&Controller protection approach perform less well, confirming our hypothesis that it is important to protect innovations upstream in the network for downstream components.\n\n\n# LEARNED REPRESENTATIONS\n\n\nWe further investigate what type of world model can emerge from an evolutionary process that does not directly optimize for forward prediction or reconstruction loss. To gain insights into the learned representations we employ the t-SNE dimensionality reduction technique (Maaten & Hinton, 2008), which has proven valuable for visualizing the inner workings of deep neural networks (Such et al., 2018; Mnih et al., 2015). We are particularly interested in the information contained in the compressed 32-dimensional vector of the VAE and the information stored in the hidden states of the MDN-RNN (which are both fed into the controller that decides on the agent's action). Different combinations of sequences of these latent vectors collected during one rollout are visualized in two dimensions in Fig. 5. Interestingly, while the 32-dimensional  $z$  vector from the VAE does not contain enough information to infer the correct action, either the hidden state alone or in combination with  $z$  results in grouping the states into two distinct classes (one for moving left and one for moving right). The temporal dimension captured by the recurrent network proves invaluable in deciding what action is best. For example, not getting stuck in a position that makes avoiding incoming fireballs impossible, seems to require a level of forward prediction by the agent. To gain a deeper understanding of this issue we look more closely into the learned temporal representation next.\n\n\nLearned Forward Model Dynamics In order to analyze the learned temporal dynamics of the forward model, we are taking a closer look at the average activation  $x_{t}$  of all 256 hidden nodes at time step  $t$  and how much they differ from the overall average across all time steps  $\\bar{X} = \\frac{1}{N}\\sum_{1}^{N}\\bar{x}_{t}$ . The variance of  $\\bar{x}_t$  is thus calculated as  $\\sigma_t = (\\bar{X} -\\bar{x}_t)^2$ , and normalized to the range [0, 1] before plotting. The hypothesis is that activation levels far from the mean might indicate a higher importance and should have a greater impact on the agent's controller component. In other words, they likely indicate critical situations in which the agent needs to pay particular attention to the predictions of the MDN-RNN. Fig. 6 depicts frames from the learned policies in two different situations, which shows that the magnitude of LSTM activations are closely tied to specific situations. The forward model does not seem to react to fireballs by themselves but instead depends on the agent being in the line of impact of an approaching fireball, which is critical information for the agent to stay alive.\n\n\n![(a) t-SNE with  $\\mathbf{z} +$  hidden](images/964ceeac1e9faceceaca2db117d66c2a8f014e8c9e6074bfea256f60caf75a71.jpg)\n\n\n![(b) t-SNE with z alone](images/f3374f26306ec597e00e916d55ac8521334191c81aa32df7104b20182fce53a4.jpg)\n\n\n![(c) t-SNE with hidden alone](images/09cfe9cf4ac05434fe72a48e6f336f3d79dc3d88fa0d6ba6ace6c5b29474ced0.jpg)\n\n\n![Figure 5: t-SNE mapping of the latent+hidden vector (a), latent vector alone (b), and hidden vector alone (c). While the compressed latent vector is not enough to infer the correct action (b), the hidden LSTM vector alone contains enough information for the agent to decide on the correct action (c). Color legend: red = strafe left, blue = strafe right, black = no movement. (a)](images/737deac831c2b8c6be4ace49abdaacc36f255689e64c4d179c96be1bee1fb872.jpg)\n\n\n![(b) Figure 6: Average activation levels of LSTM in two different situations. For visualization purposes only, game images are colored more or less blue depending on the LSTM activation values. The evolved forward model seems to have learned to predict if a fireball would hit the agent at the current position. In (a) the agent can take advantage of that information to avoid the fireball while the agent does not have enough time to get out of the way in situation (b) and gets hit. Shown on top are the actions the agent takes in each frame.](images/9783c37b8bf50d32060d560b066c7859bf1101510279c67768e71b4c2d45d5a6.jpg)\n\n\nEvolutionary Innovations In addition to analyzing the learned representations of the final networks, it is interesting to study the different stepping stones evolution discovered to solve the Viz-Doom task. We show one particular evolutionary run in Fig. 7, with other ones following similar progressions. In the first 30 generations the agent starts to learn to pay attention to fireballs but only tries avoiding them by either standing still or moving to the right. A jump in performance happens around generation 34 when the agent starts to discover moving to either the left or right; however, the learned representation between moving left or right is not well defined yet. This changes around generation 56, leading to another jump in fitness and some generations of quick fine-tuning later the agent is able to differentiate well between situations requiring different actions, managing to survive for the whole length of the episode. Motivated by the approach of Raghu et al. (2017) to analyse the gradient descent-based training of neural networks, we investigate the weight distances of the world model components of the best-performing networks found during training to the final solution representation (Fig. 3b). The VAE is the component with the steepest decrease in distance with a noticeable jump around generation 60 due to another lineage taking over. The MDN-RNN is optimized slowest, which is likely due to the fact that the correct forward model dynamics are more complicated to discover than the visual component. These results suggest that DIP is able to orchestrate the training of these heterogeneous world model architectures in an automated way.\n\n\n![Figure 7: Development of the evolved representation over generations. Shown are t-SNE mappings of the 288-dimensional vectors (32-dimensional latent vectors + 256-dimensional hidden state vector) together with saliency maps of specific game situations. Early on in evolution the agent starts paying attention to the fireballs (generation 24) but only moves to the right (blue) or stands still (black). Starting around generation 34 the agent starts to move to the left and right, with the saliency maps becoming more pronounced. From generation 56 on the compressed learned representation (latent vector+hidden state vector) allows the agent to infer the correct action almost all the time. The champion discovered in generation 145 discovered a visual encoder and LSTM mapping that shows a clear division for left and right strafing actions.](images/836f356ec2efcc7982c56a1eb62f2d5846107e54367d92c7a3d98a2347763469.jpg)\n\n\n# 5 RELATED WORK\n\n\nA variety of different RL algorithms have recently been shown to work well on a diverse set of problems when combined with the representative power of deep neural networks (Mnih et al., 2015; Schulman et al., 2015; 2017). While most approaches are based on variations of Q-learning Mnih et al. (2015) or policy gradient methods (Schulman et al., 2015; 2017), recently evolutionary-based methods have emerged as a promising alternative for some domains (Such et al., 2017; Salimans et al., 2017). Salimans et al. Salimans et al. (2017) showed that a type of evolution strategy (ES) can reach competitive performance in the Atari benchmark and at controlling robots in MuJoCo. Additionally, Such et al. Such et al. (2017) demonstrated that a simple genetic algorithm is in fact able to reach similar performance to deep RL methods such as DQN or A3C. Earlier approaches that evolved neural networks for RL tasks worked well in complex RL tasks with lower-dimensional input spaces (Stanley & Miikkulainen, 2002; Floreano et al., 2008; Risi & Togelius, 2017).\n\n\nHowever, when trained end-to-end these networks are often still orders of magnitude simpler than networks employed for supervised learning problems (Justesen et al., 2019) or depend on additional losses that are responsible for training certain parts of the network (Wayne et al., 2018).\n\n\nFor complex agent models, different network components can be trained separately (Wahlstr\u00f6m et al., 2015; Ha & Schmidhuber, 2018). For example, in the world model approach (Ha & Schmidhuber, 2018), the authors first train a variational autoencoder (VAE) on 10,000 rollouts from a random policy to compress the high-dimensional sensory data and then train a recurrent network to predict the next latent code. Only after this process is a smaller controller network trained to perform the actual task, taking information from both the VAE and recurrent network as input to determine the action the agent should perform.\n\n\nEvolutionary approaches solving 3D tasks directly from pixels has so far proven difficult although a few notable approaches exist. Koutnik et al. (2013) evolved an indirectly encoded and recurrent controller for car driving in TORCS, which learned to drive based on a raw  $64 \\times 64$  pixel image. The approach was based on an indirect encoding of the network's weights analogous to the JPEG compression in images. To scale to 3D FPS tasks, Alvernaz & Togelius (2017) first trained an autoencoder in an unsupervised way and then evolved the controller giving the compressed representation as input. In another approach, Poulsen et al. (2017) trained an object recognizer in a supervised way and then in a separate step evolved a controller module. More recently, Lehman et al. (2018) introduced an approach called safe mutations, in which the magnitude of mutations to weight connections is scaled based on the sensitivity of the network's output to that weight. The safe mutations approach allowed the evolution of large-scale deep networks for a simple 3D maze task and is a complementary approach that could be combined with DIP in the future.\n\n\nThe approach introduced in this paper can be viewed as a form of diversity maintenance, in which selection pressure on certain mutated neural networks is reduced. Many other methods for encouraging diversity (Mouret & Doncieux, 2012) were invented by the evolutionary computation community, such as novelty search (Lehman & Stanley, 2008), quality diversity (Pugh et al., 2016), or speciation (Stanley & Miikkulainen, 2002). Using the concept of age to maintain diversity has a long history in evolutionary algorithms. Kubota & Fukuda (1997) first introduced the idea of using age in a genetic algorithm in which individuals are removed from the population if they reach a specific age limit. In the Age-Layered Population Structure approach introduced by Hornby (2006), the population is segregated into different age layers and newly generated individuals are introduced into the youngest age layer to increase diversity. Schmidt & Lipson (2011) combine the idea of age with a multi-objective approach, in which individuals are rewarded for performance and low age. Inspired by the morphological innovation approach (Cheney et al., 2018) and in contrast to previous approaches (Hornby, 2006; Schmidt & Lipson, 2011), DIP does not introduce new random individuals into the generation but instead resets the age of individuals whose sensory or memory system have been mutated. That is, it is not a measure of how long the individual has been in the population, as in the traditional usage of age in multi-objective optimization.\n\n\nApproaches to learning dynamical models have mainly focused on gradient descent-based methods, with early work on RNNs in the 1990s (Schmidhuber, 1990). More recent work includes PILCO (Deisenroth & Rasmussen, 2011), which is a probabilistic model-based policy search method and Black-DROPS (Chatzilygeroudis et al., 2017), which employs CMA-ES for data-efficient optimization of complex control problems. Additionally, interest has increased in learning dynamical models directly from high-dimensional pixel images for robotic tasks (Watter et al., 2015; Hafner et al., 2018) and also video games (Guzdial et al., 2017). Work on evolving forward models has mainly focused on neural networks that contain orders of magnitude fewer connections and lower-dimensional feature vectors (Norouzzadeh & Clune, 2016) than the models in this paper.\n\n\n# 6 DISCUSSION AND FUTURE WORK\n\n\nThe paper demonstrated that a world model representation for a 3D task can emerge under the right circumstances without being explicitly rewarded for it. To encourage this emergence, we introduced deep innovation protection, an approach that can dynamically reduce the selection pressure for different components in a heterogeneous neural architecture. The main insight is that when components upstream in the neural network change, such as the visual or memory system in a world model, components downstream need time to adapt to changes in those learned representations.\n\n\nThe neural model learned to represent situations that require similar actions with similar latent and hidden codes (Fig. 5 and 7). Additionally, without a specific forward-prediction loss, the agent learned to predict \"useful\" events that are necessary for its survival (e.g. predicting when the agent is in the line-of-fire of a fireball). In the future it will be interesting to compare the differences and similarities of emergent representations and learning dynamics resulting from evolutionary and gradient descent-based optimization approaches (Raghu et al., 2017).\n\n\nInterestingly, without the need for a variety of specialized learning methods employed in the original world model paper, a simple genetic algorithm augmented with DIP can not only solve the simpler 2D car racing domain (Risi & Stanley, 2019), but also more complex 3D domains such as VizDoom. That the average score across 100 random rollouts is lower when compared to the one reported in the original world model paper (824 compared to 1092) is maybe not surprising; if random rollouts are available, training each component separately can results in a higher performance. However, in more complicated domains, in which random rollouts might not be able to provide all relevant experiences (e.g. a random policy might never reach a certain level), the proposed DIP approach could become increasingly relevant. An exciting future direction is to combine the end-to-end training regimen of DIP with the ability of training inside the world model itself (Ha & Schmidhuber, 2018). However, because the evolved representation is not directly optimized to predict the next time step and only learns to predict future events that are useful for the agent's survival, it is an interesting open question how such a different version of a hallucinate environment could be used for training.\n\n\nA natural extension to this work is to evolve the neural architectures in addition to the weights of the network. Searching for neural architectures in RL has previously only been applied to smaller networks (Risi & Stanley, 2012; Stanley & Miikkulainen, 2002; Stanley et al., 2019; Gaier & Ha, 2019; Risi & Togelius, 2017; Floreano et al., 2008) but could potentially now be scaled to more complex tasks. While our innovation protection approach is based on evolution, ideas presented here could also be incorporated in gradient descent-based approaches that optimize neural systems with multiple interacting components end-to-end.\n\n\n# A APPENDIX\n\n\n# A.1 OPTIMIZATION AND MODEL DETAILS\n\n\nThe size of each population was 200 and evolutionary runs had a termination criterion of 200 generations. The genetic algorithm  $\\sigma$  was determined empirically and set to 0.03 for the experiments in this paper. The code for the DIP approach can be found at: [Removed for anonymous review].\n\n\nTable 1: Number of parameters and training procedures. The visual component of the agent (see Fig. 1) is effectively only utilizing and evolving the encoder part of the VAE, which has 755,744 parameters. The decoder network is composed of four deconvolutional layers and has 3,592,803 parameters.\n\n\n<table><tr><td>Model</td><td>#Params</td><td>WM Training Ha &amp; Schmidhuber (2018)</td><td>GA Training</td></tr><tr><td>VAE</td><td>4,348,547</td><td>SGD - 1 epoch</td><td rowspan=\"3\">Pop size 200 \nRollouts 1</td></tr><tr><td>MDN-RNN</td><td>384,071</td><td>SGD - 20 epochs</td></tr><tr><td>Controller</td><td>867</td><td>CMA-ES - Pop 64 \nRollouts 16</td></tr></table>\n\n\nAn overview of the agent model is shown in Fig. 1, which employs the same architecture as the original world model approach Ha & Schmidhuber (2018). The sensory model is implemented as a variational autoencoder that compresses the high-dimensional input to a latent vector  $z$ . The VAE takes as input an RGB image of size  $64 \\times 64 \\times 3$ , which is passed through four convolutional layers, all with stride 2. Details on the encoder are depicted in the visual component shown in Fig. 1, where layer details are shown as: activation type (e.g. ReLU), number of output channels \u00d7 filter size. The decoder, which is in effect only used to analyze the evolved visual representation, takes as input a tensor of size  $1 \\times 1 \\times 104$  and processes it through four deconvolutional layers each with stride 2 and sizes of  $128 \\times 5$ ,  $64 \\times 5$ ,  $32 \\times 6$ , and  $32 \\times 6$ . The network's weights are set using the default\n\n\n![Figure 8: Average reward across ages and number of individuals per age.](images/a8c9492e4c9f53172d380a004676d1074de99e8b403d600eeee3301ea2d102fe.jpg)\n\n\nPyTorch initialisation (He initialisation He et al. (2015)), with the resulting tensor being sampled from  $\\mathcal{U}(-\\mathrm{bound},\\mathrm{bound})$ , where bound =  $\\sqrt{\\frac{1}{\\mathrm{fan\\_in}}}$ .\n\n\nThe memory model (Ha & Schmidhuber, 2018) combines a recurrent LSTM network with a mixture density Gaussian model as network outputs, known as a MDN-RNN (Ha & Eck, 2017; Graves, 2013b). The network has 256 hidden nodes and models  $P(z_{t + 1}|a_t,z_t,h_t)$ , where  $a_{t}$  is the action taken by the agent at time  $t$  and  $h_t$  is the hidden state of the recurrent network. Similar models have previously been used for generating sequences of sketches (Ha & Eck, 2017) and handwriting (Graves, 2013a). The controller component is a simple linear model that directly maps  $z_{t}$  and  $h_t$  to actions:  $a_{t} = W_{c}[z_{t}h_{t}] + b_{c}$ , where  $W_{c}$  and  $b_{c}$  are weight matrix and bias vector. Table 1 summarizes the parameter counts of the different world model components and how they are trained here and in the world model paper. The code for all the experiments is available at: [removed for anonymous review].\n\n\n# A.2 INTERACTION BETWEEN REWARD AND AGE OBJECTIVE\n\n\nWe performed an analysis of the (1) cumulative reward per age and (2) the number of individuals with a certain age averaged across all ten runs and all generations (Fig. 8). While the average reward increases with age, there are fewer and fewer individuals at higher age levels. This result suggests that the two objectives (minimising age and increasing cumulative reward) are in competition with each other, motivating the choice for a multi-objective optimization approach. These results indicate that the multi-objective optimization is working as intended; staying alive for longer becomes increasingly difficult and a high age needs to be compensated for by a high task reward.\n\n\n# A.3 SALIENCY MAP CALCULATION\n\n\nSimilarly to the approach by Greydanus et al. (2017), we calculate perturbation-based saliency maps by applying a Gaussian blur of  $5 \\times 5$  pixels to the coordinates  $(i,j)$  of an image  $I$  from the game. The Gaussian blur can be interpreted as adding uncertainty to a particular location of the screen. For example, if a fireball is at location  $(i,j)$  then adding noise to that location makes the agent less certain about the fireball's location. The saliency map intensity  $S(i,j)$  is calculated as the difference between the policy output  $\\pi$  given the original image  $I$  and modified image  $I'$  with added Gaussian blur at location  $(i,j)$ :  $S(i,j) = |\\pi(I) - \\pi(I')|$ .\n\n\n# REFERENCES\n\n- Samuel Alvernaz and Julian Togelius. Autoencoder-augmented neuroevolution for visual doom playing. In Computational Intelligence and Games (CIG), 2017 IEEE Conference on, pp. 1-8. IEEE, 2017.\n\n- Konstantinos Chatzilygeroudis, Roberto Rama, Rituraj Kaushik, Dorian Goepp, Vassilis Vassiliades, and Jean-Baptiste Mouret. Black-box data-efficient policy search for robotics. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 51-58. IEEE, 2017.\n\n- Nick Cheney, Josh Bongard, Vytas SunSpiral, and Hod Lipson. Scalable co-optimization of morphology and control in embodied machines. Journal of The Royal Society Interface, 15(143), 2018. ISSN 1742-5689. doi: 10.1098/rsif.2017.0937. URL http://rsif.royalsocietypublishing.org/content/15/143/20170937.\n\n- Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multi-objective genetic algorithm: Nsga-ii. IEEE transactions on evolutionary computation, 6(2): 182-197, 2002.\n\n- Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp. 465-472, 2011.\n\n- Dario Floreano, Peter Durr, and Claudio Mattiussi. Neuroevolution: from architectures to learning. Evolutionary Intelligence, 1(1):47-62, 2008.\n\n- Adam Gaier and David Ha. Weight agnostic neural networks. arXiv preprint arXiv:1906.04358, 2019.\n\n- Alex Graves. Hallucination with recurrent neural networks. 2013a. URL https://www.youtube.com/watch?v=-yX1SYeDHbg&t=49m33s.\n\n- Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013b.\n\n- Sam Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atari agents. arXiv preprint arXiv:1711.00138, 2017.\n\n- Matthew Guzdial, Boyang Li, and Mark O Riedl. Game engine learning from video. In *IJCAI*, pp. 3707-3713, 2017.\n\n- David Ha and Douglas Eck. A neural representation of sketch drawings. arXiv preprint arXiv:1704.03477, 2017.\n\n- David Ha and J\u00fcrgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, pp. 2455-2467, 2018.\n\n- Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034, 2015.\n\n- Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.\n\n- Gregory S Hornby. Alps: the age-layered population structure for reducing the problem of premature convergence. In Proceedings of the 8th annual conference on Genetic and evolutionary computation, pp. 815-822. ACM, 2006.\n\n- Niels Justesen, Philip Bontrager, Julian Togelius, and Sebastian Risi. Deep learning for video game playing. To appear in: IEEE Transactions on Games, 2019.\n\n- Michal Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jaskowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In 2016 IEEE Conference on Computational Intelligence and Games (CIG), pp. 1-8. IEEE, 2016.\n\n- Oleg Klimov. Carracing-v0. 2016. URL https://gym.openai.com/envs/CarRacing-v0/.\n\n- Jan Koutnik, Giuseppe Cuccu, J\u00fcrgen Schmidhuber, and Faustino Gomez. Evolving large-scale neural networks for vision-based reinforcement learning. In Proceedings of the 15th annual conference on Genetic and evolutionary computation, pp. 1061-1068. ACM, 2013.\n\n- Naoyuki Kubota and Toshio Fukuda. Genetic algorithms with age structure. Soft Computing, 1(4): 155-161, 1997.\n\n- Joel Lehman and Kenneth O Stanley. Exploiting open-endedness to solve problems through the search for novelty. In ALIFE, pp. 329-336, 2008.\n\n- Joel Lehman, Jay Chen, Jeff Clune, and Kenneth O Stanley. Safe mutations for deep and recurrent neural networks through output gradients. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 117-124. ACM, 2018.\n\n- Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579-2605, 2008.\n\n- Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\n\n- J-B Mouret and St\u00e9phane Doncieux. Encouraging behavioral diversity in evolutionary robotics: An empirical study. Evolutionary computation, 20(1):91-133, 2012.\n\n- Mohammad Sadegh Norouzzadeh and Jeff Clune. Neuromodulation improves the evolution of forward models. In Proceedings of the Genetic and Evolutionary Computation Conference 2016, pp. 157-164. ACM, 2016.\n\n- Andreas Precht Poulsen, Mark Thorhauge, Mikkel Hvilshj Funch, and Sebastian Risi. Dlne: A hybridization of deep learning and neuroevolution for visual control. In Computational Intelligence and Games (CIG), 2017 IEEE Conference on, pp. 256-263. IEEE, 2017.\n\n- Justin K Pugh, Lisa B Soros, and Kenneth O Stanley. Quality diversity: A new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016.\n\n- Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. In Advances in Neural Information Processing Systems, pp. 6076-6085, 2017.\n\n- Sebastian Risi and Kenneth O Stanley. An enhanced hypercube-based encoding for evolving the placement, density, and connectivity of neurons. Artificial life, 18(4):331-363, 2012.\n\n- Sebastian Risi and Kenneth O. Stanley. Deep neuroevolution of recurrent and discrete world models. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO '19, pp. 456-462, New York, NY, USA, 2019. ACM. ISBN 978-1-4503-6111-8. doi: 10.1145/3321707.3321817. URL http://doi.acm.org/10.1145/3321707.3321817.\n\n- Sebastian Risi and Julian Togelius. Neuroevolution in games: State of the art and open challenges. IEEE Transactions on Computational Intelligence and AI in Games, 9(1):25-41, 2017.\n\n- Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.\n\n- J\u00fcrgen Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. In 1990 IJCNN international joint conference on neural networks, pp. 253-258. IEEE, 1990.\n\n- Michael Schmidt and Hod Lipson. Age-fitness pareto optimization. In Genetic programming theory and practice VIII, pp. 129-146. Springer, 2011.\n\n- John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.\n\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\n- Kenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary computation, 10(2):99-127, 2002.\n\n- Kenneth O Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. Designing neural networks through neuroevolution. Nature Machine Intelligence, 1(1):24-35, 2019.\n\n- Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Deep neuroevolution: genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.\n\n- Felipe Petroski Such, Vashisht Madhavan, Rosanne Liu, Rui Wang, Pablo Samuel Castro, Yulun Li, Ludwig Schubert, Marc Bellemare, Jeff Clune, and Joel Lehman. An atari model zoo for analyzing, visualizing, and comparing deep reinforcement learning agents. arXiv preprint arXiv:1812.07069, 2018.\n\n- Niklas Wahlstr\u00f6m, Thomas B Sch\u00f6n, and Marc Peter Deisenroth. From pixels to torques: Policy learning with deep dynamical models. arXiv preprint arXiv:1502.02251, 2015.\n\n- Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in neural information processing systems, pp. 2746-2754, 2015.\n\n- Greg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka Grabska-Barwinska, Jack Rae, Piotr Mirowski, Joel Z Leibo, Adam Santoro, et al. Unsupervised predictive memory in a goal-directed agent. arXiv preprint arXiv:1803.10760, 2018.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638637.9164007", "msecs": "916.0", "relativeCreated": "234746.8981742859", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step\n\n# Abstract\nWhen leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables training a GPT-2 Small model to solve 20-by-20 multiplication with 99.5% accuracy while being 26 times faster than explicit CoT, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.\n\n\n# 1 INTRODUCTION\n\n\nA prevalent approach to improving the performance of language models (LMs) to perform complex reasoning tasks is chain-of-thought (CoT) reasoning, in which the LM generates explicit intermediate reasoning steps before arriving at a final answer (Nye et al., 2021; Wei et al., 2022). This method allows models to break down complex problems into simpler, manageable parts, thereby improving the accuracy of their final predictions. However, this explicit reasoning process can be computationally expensive, especially when the reasoning chain is long (Deng et al., 2023). Additionally, using explicit intermediate steps might not align with the intrinsic computational strengths of LMs (Lehnert et al., 2024): for instance, multi-digit multiplication is very easy for calculators but remains challenging for GPT-4 (Yang et al., 2023).\n\n\nIn this work, we examine the possibility of internalizing the reasoning process in the model's hidden states. We propose an approach, Stepwise Internalization, which begins with a model trained for explicit CoT reasoning. We then gradually remove the intermediate steps and finetune the model, forcing it to internalize the reasoning process. Once all intermediate steps are internalized, we achieve a model capable of full implicit CoT reasoning. Moreover, even in cases where the model does not have the capacity for full implicit CoT reasoning, this method still allows for shortening the reasoning chain while maintaining accuracy.\n\n\nOur approach is an alternative to the approach proposed by Deng et al. (2023), which shares the goal of implicitly reasoning using the hidden states of transformers instead of relying on explicit CoT tokens. To teach the model to use hidden states for reasoning, that method employs a teacher model that performs explicit CoT reasoning, and then distills the teacher's hidden states into the student model's hidden states. In comparison, our approach is much simpler yet more effective.\n\n\nOur approach demonstrates significant improvements over standard training methods. For instance, a GPT-2 Small model trained with Stepwise Internalization on multiplication can solve 20-by-20 multiplication problems nearly perfectly, while standard training without CoT struggles even with 4-by-4 multiplication. Furthermore, our method scales effectively to larger models, such as the Mistral 7B model (Jiang et al., 2023), achieving over  $50\\%$  accuracy on the GSM8K dataset of grade-school math word problems (Cobbe et al., 2021), without producing any explicit intermediate steps, outperforming the much larger GPT-4 model without chain-of-thought reasoning, which only scores  $44\\%$  when prompted to directly generate the answer.\n\n\nIt is important to note that our empirical evaluation focuses on specific reasoning tasks like mult-digit multiplication and grade-school math problems. While our results show the potential of Stepwise Internalization in these contexts, and the simplicity of the method makes it applicable to chain-of-thought approaches in a wide range of tasks, further research is needed to explore its efficacy across a broader range of tasks and more diverse CoT traces. Due to limitations in available computational resources, experiments on other tasks are out of scope for this work. This paper aims to lay the groundwork for this new approach and highlight its promise, while acknowledging that its full generalization is still under investigation.\n\n\nThe contributions of our work are as follows: First, we introduce Stepwise Internalization, a simple method for implicit CoT reasoning. Second, we demonstrate the effectiveness of internalizing intermediate hidden states via Stepwise Internalization. Third, we provide empirical results showing the superior performance of models trained with Stepwise Internalization on different reasoning tasks and model scales. Our code, data, and pretrained models are available at https://anonymous.4open.science/r/Internalize_CoT_Sep_by_Sep.\n\n\n# 2 BACKGROUND: IMPLICIT CHAIN-OF-THOUGHT REASONING\n\n\nImplicit chain-of-thought reasoning (implicit CoT, or ICoT) is a concept introduced by Deng et al. (2023), where during generation, the language model does not produce explicit intermediate reasoning steps in words. It is distinct from not using chain-of-thought reasoning (No CoT), in that explicit reasoning steps are allowed during training, enabling the ICoT model to learn the underlying reasoning approach from the supervision provided on the reasoning process. The key insight of Deng et al. (2023) is that intermediate reasoning steps serve two purposes in explicit CoT: they provide supervision during training to facilitate learning the task (Nye et al., 2021), and they act as a scratchpad during inference to assist in solving the task (Wei et al., 2022). However, the latter purpose can be fulfilled by utilizing the internal states of the model instead of explicit tokens.\n\n\nAs an illustrative example, consider using a language model to solve a multi-digit multiplication problem, such as  $12 \\times 34$ . (The actual input reverses the digit order as  $2 \\times 1 \\times 4 \\times 3$  for consistency with Deng et al. (2023).) In the long multiplication algorithm,  $12 \\times 34$  is broken into:\n\n\n$$\n\n$$\n1 2 \\times 4 + 1 2 \\times 3 0 = \\underbrace {4 8} _ {\\text {r e v e r s e d :} 8 4} + \\underbrace {3 6 0} _ {\\text {r e v e r s e d :} 0 6 3}.\n$$\n\n$$\n\n\nIn explicit CoT, the model is trained to predict these intermediate steps  $84 + 063$  before predicting the final answer  $804$  (408 reversed). Predicting these intermediate steps facilitates the model's ability to solve the task. (The intermediate steps are also reversed to make it easier for the model to predict (Shen et al., 2023).)\n\n\nIn both No CoT and implicit CoT settings, the model needs to directly predict the answer 408 from the input, bypassing the intermediate steps. This approach can make inference much faster for long reasoning chains, albeit at the cost of accuracy.\n\n\nThe primary difference between implicit CoT and No CoT lies in the use of intermediate reasoning steps as supervision during training. In Deng et al. (2023), a knowledge distillation approach was employed to distill explicit reasoning into implicit reasoning within the hidden states. This method involves training a teacher model to perform explicit CoT reasoning and then transferring this knowledge to a student model, which internalizes the reasoning process within its hidden states.\n\n\nIn the present work, we propose a far simpler yet more effective approach based on a kind of curriculum learning that we call Stepwise Internalization, which we detail in the next section.\n\n\n# 3 STEPWISE INTERNALIZATION\n\n\nStepwise Internalization is a method designed to achieve implicit chain-of-thought reasoning by gradually removing intermediate reasoning steps during training. We define the input as  $x$ , the intermediate steps as  $z = z_{1},z_{2},\\dots ,z_{m}$ , and the final output as  $y$ . A language model with parameters  $\\theta$  is first trained using the following loss function:\n\n\n$$\n\n$$\n\\min _ {\\theta} - \\log P _ {\\theta} (y, z _ {1: m} \\mid x),\n$$\n\n$$\n\n\n![Figure 1: Stepwise Internalization for Implicit CoT Reasoning. This figure uses  $12 \\times 34$  as an example. The training process consists of multiple stages. At Stage 0, the model is trained to predict both the full chain-of-thought (CoT) and the final output, which is the same as explicit CoT training. At Stage 1, the first CoT token is removed, and the model is finetuned to predict the remaining CoT tokens and the output. This process continues with each subsequent stage removing an additional CoT token. By Stage 6, all CoT tokens have been removed, and the model is trained to directly predict the output from the input, achieving implicit CoT reasoning. This gradual removal and finetuning process allows the model to gradually internalize the reasoning steps.](images/4cdc5f27eddd20633fe2db904d6defb462146811c0c9a95ec4497a4483605937.jpg)\n\n\nwhere  $z_{1:m}$  denotes the sequence of intermediate steps  $z_1, z_2, \\dots, z_m$ .\n\n\nAt each step  $t$  of the training process, we remove (up to)  $s(t)$  tokens from the intermediate steps  $z$ . The updated loss function then becomes:\n\n\n$$\n\n$$\n\\min  _ {\\theta} - \\log P _ {\\theta} (y, z _ {1 + \\min  (s (t), m): m} \\mid x).\n$$\n\n$$\n\n\nThere are multiple ways to parameterize  $s(t)$ . For instance, it might be based on a threshold of the loss value or a predefined schedule similar to learning rate schedulers used in optimizers. In this work, for simplicity, we use a linear schedule for removing tokens:\n\n\n$$\n\n$$\ns (t) = \\left\\lfloor \\Delta \\frac {t}{T} \\right\\rfloor ,\n$$\n\n$$\n\n\nwhere  $T$  is the total number of steps per epoch, and  $\\Delta$  is a hyperparameter controlling how many CoT tokens are removed per epoch. (Once  $s(t)$  exceeds the number of actual chain-of-thought tokens, all tokens are removed.)\n\n\nDuring initial experiments, we observed instability in the training process due to changes in the loss function over time. This instability arises for two primary reasons:\n\n\nFirst, the optimizer commonly used in training language models, such as AdamW (Kingma & Ba, 2017; Loshchilov & Hutter, 2019), maintains estimates of second-order gradients. A sudden change in the loss function, caused by the removal of one more CoT token, results in abrupt changes in the second-order gradients. To address this issue, we reset the optimizer's state whenever an additional CoT token is removed.\n\n\nSecond, even if a model fits perfectly to the current loss when  $s$  tokens are removed, transitioning to the next stage, where  $s + 1$  tokens are removed, leads to a significant increase in the loss, as the model is not yet trained for this new setting. To mitigate this issue, we introduce a technique which we term \"Removal Smoothing\", where we add a small random offset to the original number of tokens to remove  $s(t)$ , such that:\n\n\n$$\n\n$$\ns (t) ^ {*} = s (t) + o,\n$$\n\n$$\n\n\nTable 1: Dataset statistics. The number of tokens is the median based on the GPT-2 tokenizer.\n\n\n<table><tr><td rowspan=\"2\">Dataset</td><td colspan=\"3\">Size</td><td colspan=\"3\"># Input Tokens</td><td colspan=\"3\"># CoT Tokens</td><td colspan=\"3\"># Output tokens</td></tr><tr><td colspan=\"3\">Train Dev Test</td><td colspan=\"3\">Train Dev Test</td><td colspan=\"3\">Train Dev Test</td><td colspan=\"3\">Train Dev</td></tr><tr><td>Parity</td><td>808k</td><td>1k</td><td>1k</td><td>159</td><td>159</td><td>159</td><td>128</td><td>128</td><td>128</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Coinflip</td><td>32k</td><td>1k</td><td>1k</td><td>930</td><td>930</td><td>930</td><td>533</td><td>533</td><td>533</td><td>1</td><td>1</td><td>1</td></tr><tr><td>4 \u00d7 4 Mult</td><td>808k</td><td>1k</td><td>1k</td><td>9</td><td>9</td><td>9</td><td>46</td><td>46</td><td>46</td><td>9</td><td>9</td><td>9</td></tr><tr><td>5 \u00d7 5 Mult</td><td>808k</td><td>1k</td><td>1k</td><td>11</td><td>11</td><td>11</td><td>74</td><td>74</td><td>74</td><td>11</td><td>11</td><td>11</td></tr><tr><td>7 \u00d7 7 Mult</td><td>808k</td><td>1k</td><td>1k</td><td>15</td><td>15</td><td>15</td><td>148</td><td>148</td><td>148</td><td>15</td><td>15</td><td>15</td></tr><tr><td>9 \u00d7 9 Mult</td><td>808k</td><td>1k</td><td>1k</td><td>19</td><td>19</td><td>19</td><td>246</td><td>246</td><td>246</td><td>19</td><td>19</td><td>19</td></tr><tr><td>GSM8K</td><td>378k</td><td>0.5k</td><td>1.3k</td><td>40</td><td>51</td><td>53</td><td>19</td><td>21</td><td>24</td><td>2</td><td>2</td><td>2</td></tr></table>\n\n\nwhere  $o$  is a random variable with support of non-negative integers  $\\mathbb{Z}_{\\geq 0}$ , and its distribution is parameterized by another hyperparameter  $\\lambda$ :\n\n\n$$\n\n$$\nP (o) \\propto \\exp (- \\lambda o).\n$$\n\n$$\n\n\nWhen  $\\lambda = \\infty$ ,  $o = 0$  and we recover the version without Removal Smoothing. However, when  $\\lambda < \\infty$ , the model is trained to remove more than  $s(t)$  tokens at step  $t$  with a small probability, which helps smooth the transition into the next stage of removing  $s(t) + 1$  tokens, reducing the abrupt jumps in the loss function.\n\n\nFigure 1 illustrates the high-level idea of the Stepwise Internalization approach. The training process consists of multiple stages, where the model progressively learns to internalize reasoning steps by removing tokens from the CoT at each stage, eventually achieving implicit CoT reasoning.\n\n\n# 4 EXPERIMENTAL SETUP\n\n\n# 4.1 DATA\n\n\nWe evaluate our proposed Stepwise Internalization method on four reasoning tasks: the parity task from Graves (2017), the coinflip task from Wei et al. (2022), and multi-digit multiplication and grade-school math reasoning following the setup from Deng et al. (2023).\n\n\nParity The parity task was introduced by Graves (2017), where the goal is to determine the parity of a sequence of numbers. In this task, the input is a sequence of 128 numbers. A random number of positions (between 1 and 128) were randomly set to 1 or  $-1$ , and the rest were set to 0. The target is 1 if there is an odd number of 1's in the input and 0 otherwise. The chain of thought in this task is also a sequence of 128 numbers, with the  $n$ -th number being the parity of the subsequence from 1 to  $n$ . An example from this dataset is:\n\n- - Input: 1 -100 -10 -1111 -1 [... 128 numbers in total]\n\n- CoT:111111101[...]\n\n- - Target: 0 [the number of 1's in input modulo 2]\n\n\nCoinflip We also evaluated the coinflip task introduced by Wei et al. (2022). This task asks the model to predict whether a coin is still heads-up after people either flip or do not flip the coin. The original task was constructed for evaluating few-shot prompting, but we adapted it by generating a training set of 32K examples with 64 names per example. The chain of thought in this case records whether the coin remains heads-up after each person's action. For example:\n\n- - Input: A coin is heads up. Yamy does not flip the coin. Nana flips the coin. [...64 names in total] Is the coin still heads up?\n\n- CoT: Yamy: yes Nana: no [...]\n\n- - Target: no [whether the coin is still heads up in the end]\n\n\nTable 2: Acc on parity & coinflip\n\n\n<table><tr><td>Task</td><td>Parity</td><td>Coinflip</td></tr><tr><td>Explicit CoT</td><td>1.00</td><td>1.00</td></tr><tr><td>Random</td><td>0.50</td><td>0.50</td></tr><tr><td>No CoT</td><td>0.53</td><td>0.51</td></tr><tr><td>ICoT-SI</td><td>1.00</td><td>0.99</td></tr></table>\n\n\nTable 3: Acc on GSM8K.\u2020: 5-shot prompted\n\n\n<table><tr><td>Model</td><td>GPT-2 S</td><td>GPT-2 M</td><td>Mistral 7B</td><td>GPT-3.5\u2020</td><td>GPT-4\u2020</td></tr><tr><td>Explicit CoT</td><td>0.41</td><td>0.44</td><td>0.68</td><td>0.62</td><td>0.91</td></tr><tr><td>No CoT</td><td>0.13</td><td>0.17</td><td>0.38</td><td>0.03</td><td>0.44</td></tr><tr><td>ICoT-KD</td><td>0.20</td><td>0.22</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ICoT-SI</td><td>0.30</td><td>0.35</td><td>0.52</td><td>-</td><td>-</td></tr></table>\n\n\nMulti-digit multiplication. We use two of the most challenging arithmetic tasks from BIG-bench (bench authors, 2023): 4-by-4 multiplication and 5-by-5 multiplication, as described by Deng et al. (2023). Given the effectiveness of Stepwise Internalization on these tasks, we extend our evaluation to 7-by-7 and 9-by-9 multiplication. The complexity of multiplication tasks grows significantly with the number of digits, as the program length grows quadratically with the number of digits (Dziri et al., 2024). We use the scripts and setup from Deng et al. (2023) to generate synthetic training data for our main experiments.\n\n\nGrade school math. We use the GSM8K dataset (Cobbe et al., 2021), with the augmented training data provided by Deng et al. (2023). Detailed dataset statistics are provided in Table 1.\n\n\n# 4.2 BASELINES AND MODELS\n\n\nWe compare our method to the following baselines:\n\n- - No CoT: Models directly trained or finetuned without chain-of-thought supervision, except for GPT-3.5 and GPT-4 which are prompted with five-shot demonstrations.\n\n- - Explicit CoT: Models finetuned or prompted with explicit chain-of-thought reasoning. We use 5-shot prompting for GPT 3.5 and GPT-4 but full finetuning for other models.\n\n- - ICoT-KD: The implicit chain-of-thought via knowledge distillation method proposed by Deng et al. (2023).\n\n\nOur proposed method, implicit chain-of-thought via Stepwise Internalization, is termed ICoT-SI. To verify the effectiveness of our approach across different model scales, we use pretrained models GPT-2 (Radford et al., 2019) and Mistral-7B (Jiang et al., 2023).\n\n\n# 4.3 EVALUATION\n\n\nBecause the premise for implicit chain-of-thought methods is to approach the speed of no chain-of-thought and the accuracy of explicit chain-of-thought, we use two main evaluation metrics: First, we evaluate the accuracy of each method on the respective tasks of generating the final output. Second, we compare the inference speed of each method to the No CoT baseline. We measure speed, in examples per second, on an Nvidia H100 GPU with a batch size of 1. For ICoT-KD, we directly take numbers from Deng et al. (2023). However, due to hardware differences, we recompute speed relative to No CoT when speed numbers from ICoT-KD are not available.\n\n\n# 5 RESULTS\n\n\nTable 2, Table 3, Table 4, Table 5, and Figure 2 present the main results, where we compare Stepwise Internalization to baselines.\n\n\nStepwise Internalization enables solving tasks not solvable by No CoT. Stepwise Internalization (ICoT-SI) enables solving problems previously not solvable without using CoT. For example, as shown in Table 2, No CoT cannot solve either the parity or coinflip tasks, achieving accuracy around random guessing. In contrast, ICoT-SI can solve both tasks nearly perfectly.\n\n\nTable 4: Results on multiplication. ICoT-KD: Implicit CoT via knowledge distillation (Deng et al., 2023). ICoT-SI: Implicit CoT via Stepwise Internalization (this work). Acc measures the exact match accuracy. Speed measures the number of examples per second during inference using a batch size of 1, normalized by the speed of the corresponding No CoT model. \u2020: 5-shot prompted.\n\n\n<table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">4 \u00d7 4</td><td colspan=\"2\">5 \u00d7 5</td><td colspan=\"2\">7 \u00d7 7</td><td colspan=\"2\">9 \u00d7 9</td></tr><tr><td>Acc</td><td>Speed</td><td>Acc</td><td>Speed</td><td>Acc</td><td>Speed</td><td>Acc</td><td>Speed</td></tr><tr><td colspan=\"9\">GPT-2 Small (117M)</td></tr><tr><td>Explicit CoT</td><td>1.00</td><td>0.17</td><td>1.00</td><td>0.14</td><td>1.00</td><td>0.12</td><td>1.00</td><td>0.09</td></tr><tr><td>No CoT</td><td>0.29</td><td>1.00</td><td>0.01</td><td>1.00</td><td>0.00</td><td>1.00</td><td>0.00</td><td>1.00</td></tr><tr><td>ICoT-KD</td><td>0.97</td><td>0.67</td><td>0.10</td><td>0.71</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ICoT-SI</td><td>1.00</td><td>1.02</td><td>1.00</td><td>1.00</td><td>0.95</td><td>1.00</td><td>0.99</td><td>1.00</td></tr><tr><td colspan=\"9\">GPT-3.5\u2020</td></tr><tr><td>Explicit CoT</td><td>0.43</td><td>0.10</td><td>0.05</td><td>0.07</td><td>0.00</td><td>0.15</td><td>0.00</td><td>0.11</td></tr><tr><td>No CoT</td><td>0.02</td><td>1.00</td><td>0.00</td><td>1.00</td><td>0.00</td><td>1.00</td><td>0.00</td><td>1.00</td></tr><tr><td colspan=\"9\">GPT-4\u2020</td></tr><tr><td>Explicit CoT</td><td>0.77</td><td>0.14</td><td>0.44</td><td>0.14</td><td>0.03</td><td>0.09</td><td>0.00</td><td>0.07</td></tr><tr><td>No CoT</td><td>0.04</td><td>1.00</td><td>0.00</td><td>1.00</td><td>0.00</td><td>1.00</td><td>0.00</td><td>1.00</td></tr></table>\n\n\nTable 5: Comparison between ICoT-SI and MathGLM (Yang et al., 2023), where input and target digits are not reversed, consistent with the setup in the original MathGLM paper.\n\n\n<table><tr><td>Approach</td><td>Model Architecture</td><td>Layers</td><td># Parameters</td><td>4 \u00d7 4 Acc</td><td>5 \u00d7 5 Acc</td></tr><tr><td rowspan=\"3\">MathGLM</td><td>MathGLM-100M</td><td>35</td><td>142M</td><td>0.80</td><td>0.56</td></tr><tr><td>MathGLM-500M</td><td>40</td><td>568M</td><td>0.90</td><td>0.60</td></tr><tr><td>MathGLM-2B</td><td>40</td><td>2.1B</td><td>0.95</td><td>0.90</td></tr><tr><td rowspan=\"2\">ICoT-SI</td><td>GPT-2 Small</td><td>12</td><td>117M</td><td>0.97</td><td>0.88</td></tr><tr><td>MathGLM-100M</td><td>35</td><td>142M</td><td>0.99</td><td>0.93</td></tr></table>\n\n\nOn multiplication tasks (Table 4), No CoT fails even on  $4 \\times 4$  multiplication. In comparison, ICoT-SI enables a GPT-2 Small model to solve even  $20 \\times 20$  multiplication problems with an accuracy of over  $99.5\\%$ , as shown in Figure 2a.\n\n\nWhen compared to existing literature, ICoT-SI is also competitive. For instance, as shown in Table 4, while ICoT-KD fails to solve  $5 \\times 5$  multiplication using a GPT-2 Small model, ICoT-SI can solve  $9 \\times 9$  (and even  $20 \\times 20$ ) multiplication. Additionally, while ICoT-KD is slightly slower than No CoT due to the additional emulator model, ICoT-SI has the same speed as No CoT. $^{3}$  When compared to MathGLM (Yang et al., 2023) following the same setup of not reversing input and target digits, ICoT-SI outperforms a 2.1B parameter model using only 142M parameters, as shown in Table 5. Another related work, Shen et al. (2023), is able to train a GPT-2 Small model to solve up to  $14 \\times 14$  multiplication, whereas the approach proposed in this work can solve up to  $20 \\times 20$  (Figure 2a).\n\n\nICoT-SI enables internalizing CoT in a general way, making it applicable to tasks beyond arithmetic, such as grade-school math problems. On the GSM8K dataset (Table 3), ICoT-SI finetunes Mistral-7B to achieve an accuracy of 0.52, whereas GPT-4 only gets 0.44 without using intermediate steps.\n\n\nStepwise Internalization lags behind explicit CoT in accuracy but is faster. In terms of accuracy, implicit CoT methods still lag behind explicit CoT. For instance, a finetuned Mistral-7B model achieves an accuracy of 0.68 on GSM8K with explicit CoT, but ICoT-SI achieves only 0.51. However, implicit CoT methods offer significant speed advantages. As shown in Figure 2b, ICoT-SI is over 26 times faster than explicit CoT on  $20 \\times 20$  multiplication.\n\n\n![(a)](images/9f390336abd9fe99d2352c06e30d87cecc861914846973255aad66332df2990d.jpg)\n\n\n![(b)](images/75542742ffb8168fe4d6d05667e82e47522adfb23861e64c6ee9659a8c9c252f.jpg)\n\n\n![Figure 2: (a) Accuracy of ICoT-SI on  $m$ -digit by  $n$ -digit multiplication tasks. (b) Speedup of ICoT-SI on  $m$ -digit by  $n$ -digit multiplication tasks compared to explicit CoT. Figure 3: Accuracy during training for various ablations. This figure plots the validation accuracy versus the target number of removed CoT tokens during training (7 \u00d7 7 Mult, GPT-2 Small). The black dashed vertical line indicates the point at which the schedule has removed all CoT tokens. The curves compare the following variants: \"Full Approach\"; \"Without Removal Smoothing\"; \"Without Optimizer Reset\"; \"Right-Side Removal\" (CoT tokens are removed from the end instead of the beginning); and \"Aggressive Removal\" (16 instead of 8 CoT tokens are removed per epoch).](images/8f50945a6dd4e7610ae060b9001fab3e055f614da9998a84f9a63193d067ee1e.jpg)\n\n\nOverall, our results demonstrate that Stepwise Internalization is an effective method for compressing chain-of-thought reasoning, offering a compelling trade-off between accuracy and speed. This makes it a valuable approach for reducing inference latency for tasks requiring long CoTs.\n\n\n# 6 ANALYSIS\n\n\n# 6.1 MECHANISTIC INTERPRETABILITY\n\n\nTo understand the internal algorithm used by the trained model, we conducted a probing analysis. We trained a probe to detect every digit in the explicit CoT for a  $3 \\times 3$  multiplication problem from\n\n\n![Input:  $\\boxed{a_1}$ $\\boxed{a_2}$ $\\boxed{a_3}$ $\\boxed{\\times}$ $\\boxed{b_1}$ $\\boxed{b_2}$ $\\boxed{b_3}$ Figure 4: Mechanistic interpretability analysis of  $3 \\times 3$  multiplication. The top row displays the input digits; the middle row shows the CoT tokens in explicit CoT reasoning (partial products  $p_1$ ,  $p_2$ , and  $p_3$ , and a partial sum  $s = p_1 + 10p_2$ ); the bottom row represents the hidden states of a trained implicit CoT model. Connections indicate the earliest positions where each CoT token can be probed from the hidden states with over  $95\\%$  accuracy.](images/b6ea17f2670fcf57fb96eb1c04e25c804e4a5df93460add0ff501b3e5928236d.jpg)\n\n\nthe hidden states at each input position in an implicit CoT model.4 Figure 4 shows the results. We observe that all partial products of the  $3\\times 1$  multiplications have been computed at the earliest possible positions. For example, immediately after processing  $b_{1}$ , the hidden states already encode  $p_1 = b_1\\times a_3a_2a_1$ ; similarly, after  $b_{2}$ , they encode  $p_2 = b_2\\times a_3a_2a_1$ . This suggests that the implicit CoT model internally replicates these partial product steps in explicit CoT.\n\n\nHowever, the implicit CoT reasoning process doesn't fully replicate explicit CoT reasoning: the partial sum  $s = p_1 + 10p_2$  cannot be reliably detected in any hidden state. This might be due to limitations of the probe, but it's also likely that the model doesn't compute the partial sum internally. We suspect that the model first computes the partial products and then sums them in parallel, leveraging the language model's ability to sum sequences of numbers together (Chen et al., 2024). This might explain why a 12-layer GPT-2 Small can be trained to perform  $20 \\times 20$  multiplication.\n\n\n# 6.2 ABLATION STUDIES\n\n\nFigure 3 plots the validation accuracy versus the schedule for the number of CoT tokens removed during training for the  $7 \\times 7$  multiplication task. This figure compares the full approach to several ablated variants. Even for the full approach, there are fluctuations in the curve, and the validation accuracy briefly drops to zero at one point during training but eventually recovers. However, the ablated variants do not fully recover when accuracy drops.\n\n\nRemoval smoothing. As mentioned in Section 3, adding a small random offset  $o$  to the number of removed tokens is crucial when the loss function changes due to the removal of more CoT tokens. The distribution of  $o$  is parameterized by a hyperparameter  $\\lambda$ , as introduced in Section 3. We use  $\\lambda = 4$  throughout this work, resulting in the distribution shown in Figure 5. In this distribution,  $98\\%$  of the time,  $o = 0$ , but about  $2\\%$  of the time, one or more additional tokens are removed. As shown in Figure 3, the \"Without Removal Smoothing\" curve fails to recover after the accuracy drops to zero at around  $s(t) = 50$ , whereas the full approach does much better.\n\n\nResetting the optimizer. Another important technique for stabilizing training is resetting the optimizer when more tokens are removed. This avoids large estimates of second-order derivatives and stabilizes training. In Figure 3, the \"Without Optimizer Reset\" curve drops to zero around 100 steps and does not recover, showing the importance of resetting the optimizer during training.\n\n\nRemoval side. In our main experiments, CoT tokens are removed from the beginning (left side). Removing CoT tokens from the right side performs significantly worse, as shown by the \"Right-Side Removal\" curve in Figure 3. We suspect this is because internalizing tokens at the beginning is easier than internalizing tokens at the end. CoT tokens at the end depend on the earlier tokens, so internalizing them between the end of CoT and the beginning of the final answer, which only has a few positions, is more challenging. In contrast, internalizing tokens at the beginning allows distributing them across the entire input.\n\n\nNumber of tokens removed per epoch. The number of tokens removed per epoch  $(\\Delta)$  significantly affects the training stability and speed. In the main experiments, we used  $\\Delta = 8$ , which removes 8 tokens per epoch. A higher  $\\Delta$  value leads to faster training but risks not converging, as the model may not be able to keep up with the rapid changes in the loss function. For instance, when using  $\\Delta = 16$ , the training fails to converge, as shown by the \"Aggressive Removal\" curve in Figure 3. Conversely, a lower  $\\Delta$  value is more likely to result in successful training but at a slower pace. Future work could explore adaptive  $\\Delta$  schedules based on loss values to balance speed and stability more effectively.\n\n\n![Figure 5: Distribution over Random Removal Offset  $o$  in Removal Smoothing with  $\\lambda = 4$ . The distribution is mostly concentrated at  $o = 0$  with a probability of 0.98, and  $o \\geq 1$  has a probability of only 0.02. Despite this, the removal smoothing proves to be effective, as demonstrated in the ablation studies.](images/94e692e40c4b4435bdb49769b1d64805809aae7900caced5973fb5c95c986bae.jpg)\n\n\n# 6.3 MODEL ARCHITECTURE\n\n\nWhen comparing different model architectures for ICoT-SI in Table 5, we find that MathGLM with 35 layers outperforms GPT-2 Small with 12 layers, despite having a similar number of parameters. This result suggests that deeper models may be better for implicit reasoning, echoing the findings of Ye et al. (2024).\n\n\n# 7 RELATED WORK\n\n\nNo CoT approaches. Several works in the literature focus on training language models to solve arithmetic tasks without outputting intermediate steps. MathGLM (Yang et al., 2023) demonstrated that with sufficient training data, including both lower-digit and higher-digit arithmetic task demonstrations, a 2 billion parameter LM can solve multi-digit arithmetic tasks without any intermediate steps. Compared to this\n\n\nwork, Stepwise Internalization achieves higher accuracy in solving multi-digit multiplication with much smaller models, likely due to leveraging chain-of-thought supervision during training. Another notable work by Shen et al. (2023) showed that by mixing lower-digit and higher-digit multiplication demonstrations, even a GPT-2 Small can learn up to 14-digit multiplication. However, Stepwise Internalization does not require specially prepared training data with mixed task difficulties. Additionally, Stepwise Internalization is theoretically applicable to any reasoning task with CoT reasoning steps, as demonstrated by its effectiveness on grade-school math problems.\n\n\nAlso relevant is the work of Pfau et al. (2024), which shows that transformer language models can reason using filler tokens as an alternative to CoT tokens. They showed reasoning using these filler tokens improves a language model's expressivity. Our approach has the potential to be combined with their approach to solve even more challenging tasks.\n\n\nInternalizing CoT. Our work is closely related to that of Deng et al. (2023) (ICoT-KD), which introduced the task of implicit CoT reasoning. ICoT-KD allows using CoT during training but not\n\n\nduring generation, and it implements this via knowledge distillation to internalize the reasoning steps within hidden states. Compared to ICoT-KD, Stepwise Internalization has three advantages: First, it is simpler to implement as it does not require a teacher model. Second, while ICoT-KD internalizes reasoning into a single \"column\" of states (corresponding to the final input position), Stepwise Internalization allows the model to internalize reasoning across all input positions. Lastly, Stepwise Internalization achieves better accuracy compared to ICoT-KD.\n\n\nOur work is also related to Context Distillation (Snell et al., 2022), which trains a model to produce the same output when conditioned on a scratchpad versus without it. Each stage of Stepwise Internalization can be viewed as a form of context distillation, where one CoT token is distilled into the model's internal states.\n\n\nAnother relevant work is Searchformer (Lehnert et al., 2024), which first trains a transformer to imitate A* search and then finetunes it on sampled shorter search traces. While Searchformer relies on sampling to find shorter traces, Stepwise Internalization forces the model to internalize steps.\n\n\n# 8 LIMITATIONS\n\n\nTraining costs. One limitation of the proposed approach is its high training cost due to the fine-tuning required when removing each set of CoT tokens. As discussed in Section 6.2, removing CoT tokens too fast leads to non-convergence. Therefore, the longer the CoT chain, the longer the training duration. For tasks like  $N$ -digit multiplication, where the reasoning chain length grows quadratically with  $N$ , training becomes expensive as  $N$  increases.\n\n\nInstability. Another practical issue we observed is the instability of training with aggressive  $\\Delta$  values. For example, Figure 6 in Appendix B shows a case where the model could not recover from a drop in accuracy. Using lower  $\\Delta$  values generally leads to more stable training, but at the cost of longer training time. Identifying and addressing unstable dynamics early on, potentially by restarting training as suggested by Hu et al. (2024), could be a valuable improvement.\n\n\nInterpretability. Models trained using our approach lose interpretable intermediate steps. However, as shown in Section 6.1, it is still possible to interpret the internal hidden states of these models using probing techniques (Belinkov, 2018; Hewitt & Liang, 2019). Additionally, combining implicit and explicit CoT training could allow users to choose between interpretability and latency, providing flexibility based on the requirements of future tasks.\n\n\nAccuracy. Undoubtedly, explicit CoT still achieves higher accuracies compared to our approach to implicit CoT. However, our method enables a trade-off between latency and accuracy. Moreover, our results demonstrate the potential of leveraging hidden states for reasoning: even a GPT-2 Small model can be trained to solve  $20 \\times 20$  multiplication, despite having only 12 layers, far fewer than the number of reasoning steps in the CoT for  $20 \\times 20$  multiplication. When scaled to larger models with hundreds of billions of parameters and up to a hundred layers, such as GPT-3 (Brown et al., 2020), they could potentially solve even more challenging reasoning tasks without explicit CoT steps.\n\n\n# 9 CONCLUSION AND FUTURE WORK\n\n\nIn this work, we introduced Stepwise Internalization, a novel approach for achieving implicit chain-of-thought reasoning in language models. By gradually removing intermediate CoT tokens and finetuning the model, we enable the internalization of reasoning steps incrementally. Our approach demonstrates significant improvements over existing methods, achieving high accuracy on up to  $20 \\times 20$  multiplication using GPT-2 Small and outperforming GPT-4 on GSM8K while not outputting any intermediate reasoning steps. Compared to explicit CoT methods, our approach can be up to 26 times faster while maintaining similar accuracies.\n\n\nFor future work, developing a mixed-mode approach that combines implicit and explicit CoT reasoning could potentially offer the best of both worlds, balancing accuracy, latency, and interpretability based on user preferences. Another promising direction is scaling Stepwise Internalization to larger models and more extensive training/pretraining setups, which could further enhance its effectiveness on a broader range of reasoning tasks.\n\n\n# REFERENCES\n\n- Yonatan Belinkov. On internal language representations in deep learning: An analysis of machine translation and speech recognition. PhD thesis, Massachusetts Institute of Technology, 2018.\n\n- BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj.\n\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n\n- Junhao Chen, Shengding Hu, Zhiyuan Liu, and Maosong Sun. States hidden in hidden states: Llms emerge discrete state representations implicitly, 2024. URL https://arxiv.org/abs/2407.11421.\n\n- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.\n\n- Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation, 2023.\n\n- Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Alex Graves. Adaptive computation time for recurrent neural networks, 2017. URL https:// arxiv.org/abs/1603.08983.\n\n- John Hewitt and Percy Liang. Designing and interpreting probes with control tasks, 2019.\n\n- Michael Y. Hu, Angelica Chen, Naomi Saphra, and Kyunghyun Cho. Latent state models of training dynamics, 2024.\n\n- Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023.\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n\n- Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping, 2024.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\n\n- Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henrik Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021.\n\n- Jacob Pfau, William Merrill, and Samuel R. Bowman. Let's think dot by dot: Hidden computation in transformer language models, 2024.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\n- Ruoqi Shen, S\u00e9bastien Bubeck, Ronen Eldan, Yin Tat Lee, Yanzhi Li, and Yi Zhang. Positional description matters for transformers arithmetic, 2023.\n\n- Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context, 2022.\n\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian richter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_VjQlMeSB_J.\n\n- Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang. Gpt can solve mathematical problems without a calculator, 2023.\n\n- Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1, grade-school math and the hidden reasoning process, 2024. URL https://arxiv.org/abs/2407.20311.\n\n\n![Figure 6: Validation Accuracy during Training for two different random seeds. This figure plots the validation accuracy as a function of the potential number of removed CoT tokens during training for the  $9 \\times 9$  multiplication task using GPT-2 Small and  $\\Delta = 8$ . The two curves only differ in random seeds. The black dashed vertical line indicates the point beyond which all CoT tokens are removed.](images/b190faba4e978c874c182823cee7017d9b5d4e45654d63680899359efc3a1229.jpg)\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638639.3797803", "msecs": "379.0", "relativeCreated": "236210.27779579163", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Breaking the Curse of Dimensionality for Parametric Elliptic PDEs\n\n# Abstract\nMotivated by recent empirical success, we examine how neural network-based ansatz classes can break the curse of dimensionality for high-dimensional, non-linear elliptic partial differential equations (PDEs) with variational structure. The high-dimensionality of the PDEs can either be induced through a high-dimensional physical domain or a high-dimensional parameter space. The latter include parametric right-hand sides, parametric domains, and material constants. Our main result shows that any scheme, that computes neural network based $W^{1,p}$-approximations, leverages the extraordinary approximation capabilities of neural networks and, thus, is able to beat the curse of dimensionality if the ground truth solution is smooth or possesses Barron regularity. Popular examples of $W^{1,p}$-convergent schemes include, e.g., the Deep Ritz Method and physics-informed neural networks. We present numerical experiments supporting our theoretical findings.\n\n\n# 1 INTRODUCTION\n\n\nHigh-dimensional partial differential equations (PDEs) arise naturally in applications with either a high-dimensional domain, a high-dimensional parameter space, or possibly with both. The former includes the Schr\u00f6dinger equation in quantum physics, the Black-Scholes equation in finance, and the Hamilton-Jacobi-Bellman equation in control theory, we refer to Weinan et al. (2021); Bellman (1954). On the other hand, examples of problems with high-dimensional parameter space are ubiquitous in engineering applications, for instance, in varying material properties, right-hand sides or even in the form of varying computational domains, as discussed in Hennigh et al. (2021); Ohlberger & Rave (2016).\n\n\nFor problems with a high-dimensional physical domain, classical mesh-based approximation schemes face the curse of dimensionality, meaning that the computational cost increases exponentially with the dimension of the problem. In the case of parametric problems, one is typically interested in querying the PDE solution for many different parameter instances, possibly with low inference time. To this end, classical methods need to repeatedly solve the equations for every required parameter instance, a potentially prohibitively expensive or slow computational task, see Biegler et al. (2007). Even assuming additional, favorable structure of the solution of a high-dimensional PDE \u2013 may it be a latent low-dimensionality of the solution or a high degree of smoothness \u2013 it remains a challenge for classical methods to approximate the solution with an acceptable accuracy, especially in situations of non-linear solution manifolds as discussed in Ohlberger & Rave (2016); Lee & Carlberg (2020).\n\n\nArtificial neural networks have shown great potential in the approximation of high-dimensional functions, among those computer vision, classification and natural language processing tasks and are known to possess extraordinary approximation capabilities with the possibility to achieve dimension-independent approximation rates for certain function classes, see Ma et al. (2022); Barron (1993); Yarotsky (2017); Guhring & Raslan (2021); Guhring et al. (2020). Therefore, investigating artificial neural networks as ansatz classes for the solution of PDEs or PDE solution operators has recently gained increased interest for high-dimensional and parametric problems. We refer to Kutyniok et al. (2022); Weinan & Wojtowitsch (2022); Jentzen et al. (2021); Chen et al. (2021) for theoretical studies. Successful empirical results of neural network-based applications to PDEs posed in high-dimensional spaces include Hermann et al. (2020); Yu & E (2018); Han et al. (2018); Sirignano & Spiliopoulos (2018). For the parametric setting, we direct the reader to Li et al. (2021); Khoo et al. (2021); Lee & Carlberg (2020); Geist et al. (2021).\n\n\nOf the aforementioned contributions, the theoretical works either focus on approximation theoretic results or consider linear problems without parametric dependencies. We discuss the relation to our contribution in detail in Section 1.1. The approximation theoretic results guarantee the existence of a neural network with desirable approximation rates but provide no practical way to compute the neural network. For non-linear, parametric PDEs with  $p$ -structure, our results deliver the necessary PDE analysis \u2013 in a setting suitable for neural network ansatz functions \u2013 to alleviate this problem. Instead of explicitly constructing a neural network, we show that it suffices to find a neural network approximation that is close \"in energy\" to the ground truth solution. This makes a significant difference: Energy approximations can be found by using the variational energy as a loss function or more generally by any  $W^{1,p}$ -convergent approximation scheme \u2013 a natural property of a reasonable approximation algorithm.\n\n\nTo summarize, our main contributions are the following:\n\n- - We show that every energy convergent approximation scheme for the  $p$ -Dirichlet energy utilizing neural network ansatz functions can leverage the extraordinary approximation capabilities of neural networks. Further, we explain which assumptions on the ground truth solution allow neural networks to beat the curse of dimensionality. We extend these results to parametric problems, where the neural network approximates simultaneously in the physical and the parameter space. Our contributions are the first quantitative error estimates for parametric and non-linear elliptic PDEs for neural network approximation schemes.\n\n- - From a mathematical point of view, the analysis of non-linear ansatz classes is novel in the case of the  $p$ -Laplacian. Existing literature exclusively exploits strategies based on optimality conditions (Galerkin orthogonality) only available for linear ansatz classes, hence, excludes neural networks. Further, to the best of the author's knowledge, error estimates for parametric problems have not been considered in the existing literature.\n\n\n# 1.1 MAIN RESULT AND RELATED WORK\n\n\nFor clarity, we present our main result for the case of a parametric right-hand side and with homogeneous Neumann boundary conditions. However, different boundary conditions and parametric dependencies are covered by our analysis. We explain this in the Appendix and refer to Section C.\n\n\nConsider a physical domain  $\\Omega \\subseteq \\mathbb{R}^{d_{\\Omega}}$ ,  $d_{\\Omega} \\in \\mathbb{N}$ , and a parameter space  $\\mathcal{P} \\subseteq \\mathbb{R}^{d_{\\mathcal{P}}}$ ,  $d_{\\mathcal{P}} \\in \\mathbb{N}$ . Further, let  $p \\in (1,\\infty)$  be fixed and denote by  $(f(\\tau, \\cdot))_{\\tau \\in \\mathcal{P}}$  a parametric family of right-hand sides. We study the non-linear  $p$ -Laplace problem as a prototypical, non-linear elliptic PDE. More precisely, we want to find  $\\boldsymbol{u}^* : \\mathcal{P} \\times \\Omega \\to \\mathbb{R}$  satisfying\n\n\n$$\n\n$$\n- \\operatorname {d i v} \\left(\\left| \\nabla_ {x} \\boldsymbol {u} ^ {*} (\\boldsymbol {\\tau}, x) \\right| ^ {p - 2} \\nabla_ {x} \\boldsymbol {u} ^ {*} (\\boldsymbol {\\tau}, x)\\right) = \\boldsymbol {f} (\\boldsymbol {\\tau}, x) \\quad \\text {f o r a . e .} (\\boldsymbol {\\tau}, x) ^ {\\top} \\in \\mathcal {P} \\times \\Omega , \\tag {1}\n$$\n\n$$\n\n\nsubjected to - for simplicity - homogeneous Neumann boundary conditions. The case  $p = 2$  retrieves the classical Poisson equation. In this example, the parametric dependencies are induced through the right-hand side and both  $\\Omega$  and  $\\mathcal{P}$  may be high-dimensional. Then, we seek a neural network  $\\boldsymbol{u}_{\\theta}$  with input  $(\\tau ,x)^{\\top}\\in \\mathbb{R}^{d_{\\mathcal{P}}}\\times \\mathbb{R}^{d_{\\Omega}}$  that approximates the solution  $\\boldsymbol{u}^{*}:\\mathcal{P}\\times \\Omega \\to \\mathbb{R}$  simultaneously in the physical domain  $\\Omega$  and the parameter space  $\\mathcal{P}$ . Essential for the statement of our result is the reformulation of equation equation 1 as a minimization problem. We find  $\\boldsymbol{u}^{*}\\in L^{p}(\\mathcal{P},W^{1,p}(\\Omega))$  as a minimizer of  $\\mathcal{E}:L^p (\\mathcal{P},W^{1,p}(\\Omega))\\to \\mathbb{R}$ , for every  $\\boldsymbol {v}\\in L^{p}(\\mathcal{P},W^{1,p}(\\Omega))$  defined by\n\n\n$$\n\n$$\n\\boldsymbol {\\mathcal {E}} (\\boldsymbol {v}) := \\int_ {\\mathcal {P}} \\left[ \\frac {1}{p} \\int_ {\\Omega} | \\nabla_ {x} \\boldsymbol {v} (\\boldsymbol {\\tau}, x) | ^ {p} \\mathrm {d} x - \\int_ {\\Omega} \\boldsymbol {f} (\\boldsymbol {\\tau}, x) \\boldsymbol {v} (\\boldsymbol {\\tau}, x) \\mathrm {d} x \\right] \\mathrm {d} \\boldsymbol {\\tau}. \\tag {2}\n$$\n\n$$\n\n\nThen, our main result is the following.\n\n\nTheorem 1. Let  $\\Omega \\subseteq \\mathbb{R}^{d_{\\Omega}}$ ,  $d_{\\Omega} \\in \\mathbb{N}$ , be a bounded Lipschitz domain and  $\\mathcal{P} \\subseteq \\mathbb{R}^{d_{\\mathcal{P}}}$ ,  $d_{\\mathcal{P}} \\in \\mathbb{N}$ , an open set. Moreover, let  $\\pmb{f} \\in L^{p'}(\\mathcal{P} \\times \\Omega)$ ,  $p \\in (1,\\infty)$ , be such that  $\\int_{\\Omega} \\pmb{f}(\\pmb{\\tau},\\cdot) \\mathrm{d}x = 0$  for a.e.  $\\pmb{\\tau} \\in \\mathcal{P}$ . Denote by  $\\pmb{u}^* \\in L^p(\\mathcal{P},W^{1,p}(\\Omega))$ , a weak solution of the parametric  $p$ -Laplace problem with homogeneous Neumann boundary conditions, i.e.,\n\n\n$$\n\n$$\n\\begin{array}{l} - \\operatorname {d i v} \\left(\\left| \\nabla_ {x} \\boldsymbol {u} ^ {*} \\right| ^ {p - 2} \\nabla_ {x} \\boldsymbol {u} ^ {*}\\right) = \\boldsymbol {f} \\quad \\text {i n} \\mathcal {P} \\times \\Omega , \\tag {3} \\\\ \\partial_ {n} \\boldsymbol {u} ^ {*} = 0 \\qquad o n \\mathcal {P} \\times \\partial \\Omega . \\\\ \\end{array}\n$$\n\n$$\n\n\nLet  $M \\subset W^{1,p}(\\mathcal{P} \\times \\Omega)$  be any subset that contains the zero function and let  $v \\in M$  be arbitrary. Setting\n\n\n$$\n\n$$\n\\tilde {M} := \\left\\{\\boldsymbol {u} \\in M \\mid \\| \\nabla_ {x} \\boldsymbol {u} \\| _ {L ^ {p} (\\mathcal {P} \\times \\Omega) ^ {d}} \\leq 2 \\| \\nabla_ {x} \\boldsymbol {u} ^ {*} \\| _ {L ^ {p} (\\mathcal {P} \\times \\Omega) ^ {d}} \\right\\},\n$$\n\n$$\n\n\nit holds\n\n\n$$\n\n$$\n\\| \\nabla_ {x} \\boldsymbol {v} - \\nabla_ {x} \\boldsymbol {u} ^ {*} \\| _ {L ^ {p} (\\mathcal {P} \\times \\Omega) ^ {d}} \\precsim \\left\\{ \\begin{array}{l l} \\delta (\\boldsymbol {v}) ^ {1 / p} + \\inf  _ {\\tilde {\\boldsymbol {v}} \\in \\tilde {M}} \\| \\nabla_ {x} \\tilde {\\boldsymbol {v}} - \\nabla_ {x} \\boldsymbol {u} ^ {*} \\| _ {L ^ {p} (\\mathcal {P} \\times \\Omega) ^ {d}} ^ {\\frac {2}{p}} & i f p \\in [ 2, \\infty) \\\\ \\delta (\\boldsymbol {v}) ^ {1 / 2} + \\inf  _ {\\tilde {\\boldsymbol {v}} \\in \\tilde {M}} \\| \\nabla_ {x} \\tilde {\\boldsymbol {v}} - \\nabla_ {x} \\boldsymbol {u} ^ {*} \\| _ {L ^ {p} (\\mathcal {P} \\times \\Omega) ^ {d}} ^ {\\frac {p}{2}} & i f p \\in (1, 2) \\end{array} \\right.,\n$$\n\n$$\n\n\nwhere  $\\delta(\\pmb{v}) \\coloneqq \\pmb{\\mathcal{E}}(\\pmb{v}) - \\inf_{\\tilde{\\pmb{v}} \\in \\tilde{M}} \\pmb{\\mathcal{E}}(\\tilde{\\pmb{v}})$  is the optimization error and the implicit constants depend on  $p$ ,  $\\Omega$  and  $\\| \\pmb{f} \\|_{L^{p'}(\\mathcal{P} \\times \\Omega)}$  only.\n\n\nWe stress again that the choice of Neumann boundary conditions is for simplicity of presentation. A similar result holds for Dirichlet boundary conditions employing an appropriate penalty scheme. The fact that  $f(\\tau, \\cdot)$  for a.e.  $\\tau \\in \\mathcal{P}$  is mean-value-free serves to guarantee the well-posedness in the Neumann boundary value case. The main reason to pass from  $M$  to  $\\tilde{M}$  is solely of technical nature, rooted in Lemma 7 and not of relevance for the interpretation of the result.\n\n\nLeveraging the Power of Approximation-Theoretical Results. As we discuss below, suitable approximation theorems allow to estimate the infimum in Theorem 1 to deduce error decay rates. Note that there is no further requirement on the approximating function  $\\pmb{v} \\in M$  than that it is a \"good\" quasi-optimizer of  $\\mathcal{E}: M \\to \\mathbb{R}$ , i.e., that  $\\delta(\\pmb{v})$  is sufficiently small. Thus, any algorithm that produces approximate solutions that converge in energy is able to fully leverage the approximation capabilities of neural network ansatz classes \u2013 up to the exponent  $2/p$  or  $p/2$ , which is due to the non-linearity. Furthermore, energy convergence is equivalent to convergence in the  $W^{1,p}$ -semi-norm<sup>2</sup> We stress the drastic difference of our contribution to mere approximation theoretical results that only guarantee the existence of a well-approximating network, yet don't unveil how such an approximation should be found. In this sense, our contribution is orthogonal to approximation theoretical results as it can be combined with these to extend them.\n\n\nFinally note that, analyzing the effect of a solution scheme on the achievable value of  $\\delta(\\pmb{v})$  is a difficult problem, typically connected to a non-convex optimization task, that we do not study in this article.\n\n\nUsing Smoothness to Beat the Curse of Dimensionality. We can utilize quantitative universal approximation results to estimate the infimum in Theorem 1. In some situations, this allows us to beat the curse of dimensionality. Assume that the solution  $\\pmb{u}^{*}$  to equation 3 is a member of  $W^{k,p}(\\mathcal{P}\\times \\Omega)$  for some  $k\\in \\mathbb{N}, k > 1$ . Then, for every  $n\\in \\mathbb{N}$ , we may use Theorem 4.9 in Guhring & Raslan (2021) to guarantee the existence of a fully connected neural network architecture with  $\\mathrm{ReLU}^2$ -activation<sup>3</sup> with parameter space  $\\Theta_{n}$  of dimension  $\\mathcal{O}(n)$  such that, setting  $M = \\mathcal{F}_{\\Theta_n}$ , where  $\\mathcal{F}_{\\Theta_n}$  denotes the realization set of the ansatz class, it holds\n\n\n$$\n\n$$\n\\inf  _ {\\psi \\in \\Theta_ {n}} \\| \\nabla_ {x} \\boldsymbol {u} _ {\\psi} - \\nabla_ {x} \\boldsymbol {u} ^ {*} \\| _ {L ^ {p} (\\mathcal {P} \\times \\Omega) ^ {d}} \\preceq \\| \\boldsymbol {u} ^ {*} \\| _ {W ^ {k, p} (\\mathcal {P} \\times \\Omega)} ^ {\\frac {2}{p}} \\left(\\frac {1}{n}\\right) ^ {\\frac {2}{p} \\frac {k - 1}{d _ {\\Omega} + d _ {\\mathcal {P}}}}\n$$\n\n$$\n\n\nfor the case  $p \\geq 2$  and with  $\\frac{2}{p}$  replaced by  $\\frac{p}{2}$  in the case  $p < 2$ . Hence, for arbitrary  $\\mathbf{u}_{\\theta} \\in M, \\theta \\in \\Theta_{n}$ , we get\n\n\n$$\n\n$$\n\\left\\| \\nabla_ {x} \\boldsymbol {u} _ {\\theta} - \\nabla_ {x} \\boldsymbol {u} ^ {*} \\right\\| _ {L ^ {p} (\\mathcal {P} \\times \\Omega) ^ {d}} \\preceq \\left\\{ \\begin{array}{l l} \\delta_ {n} (\\boldsymbol {u} _ {\\theta}) ^ {1 / p} + \\| \\boldsymbol {u} ^ {*} \\| _ {W ^ {k, p} (\\mathcal {P} \\times \\Omega)} \\left(\\frac {1}{n}\\right) ^ {\\frac {2}{p} \\cdot \\frac {k - 1}{d _ {\\Omega} + d _ {\\mathcal {P}}}} & \\text {i f} p \\in [ 2, \\infty) \\\\ \\delta_ {n} (\\boldsymbol {u} _ {\\theta}) ^ {1 / 2} + \\| \\boldsymbol {u} ^ {*} \\| _ {W ^ {k, p} (\\mathcal {P} \\times \\Omega)} \\left(\\frac {1}{n}\\right) ^ {\\frac {p}{2} \\cdot \\frac {k - 1}{d _ {\\Omega} + d _ {\\mathcal {P}}}} & \\text {i f} p \\in (1, 2) \\end{array} \\right.,\n$$\n\n$$\n\n\nwhere  $\\delta_{n}(\\pmb{u}_{\\theta}) \\coloneqq \\pmb{\\mathcal{E}}(\\pmb{u}_{\\theta}) - \\inf_{\\psi \\in \\Theta_{n}} \\pmb{\\mathcal{E}}(\\pmb{u}_{\\psi})$ . This shows - given sufficient smoothness of  $\\pmb{u}^{*} \\in M$  - that the error of the neural network approximation does not decay exponentially slow in the dimension  $n \\in \\mathbb{N}$  of the parameter space  $\\Theta_{n}$ . More precisely, the result requires dimension-dependent smoothness with the smoothness parameter  $k \\in \\mathbb{N}$ ,  $k > 1$ , scaling like  $k \\sim d_{\\Omega} + d_{\\mathcal{P}}$ . However, the assumption of smoothness is very natural in the context of (linear) elliptic PDEs and holds also in the parametric case, see Lemma B which gives an easily verifiable criterion when the smoothness assumption holds.\n\n\nEmploying Barron Regularity to Beat the Curse of Dimensionality. A different situation where the curse of dimensionality can be circumvented is when  $\\mathbf{u}^{*} \\in M$  is a member of the Barron space  $\\mathcal{B}$ , or can be \"well-approximated\" by Barron functions. We refer to Barron (1993); Ma et al. (2022) for a definition of the Barron space. In essence, members of  $\\mathcal{B}$  can be approximated with respect to the  $H^{1}$ -norm by shallow neural networks with a dimension-independent rate of  $n^{-1/2}$ , where  $n \\in \\mathbb{N}$  is the width of the shallow network. Hence, setting  $p = 2$  and assuming  $\\mathbf{u}^{*} \\in \\mathcal{B}$ , we can estimate for an arbitrary shallow neural network  $\\mathbf{u}_{\\theta} \\in M$\n\n\n$$\n\n$$\n\\| \\nabla_ {x} \\boldsymbol {u} _ {\\theta} - \\nabla_ {x} \\boldsymbol {u} ^ {*} \\| _ {L ^ {2} (\\mathcal {P} \\times \\Omega) ^ {d}} \\stackrel {{\\sim}} {{\\lesssim}} \\delta_ {n} ^ {\\frac {1}{2}} + \\left(\\frac {1}{n}\\right) ^ {\\frac {1}{2}} \\| \\boldsymbol {u} ^ {*} \\| _ {\\mathcal {B}}.\n$$\n\n$$\n\n\nThe assumption  $\\pmb{u}^{*} \\in \\mathcal{B}$  is too restrictive in general, cf. the discussion in Weinan & Wojtowytsch (2022). However, assuming that the data  $f \\in L^{2}(\\mathcal{P} \\times \\Omega)$  (and possibly coefficients) are of Barron regularity, it was recently established that the solution  $\\pmb{u}^{*} \\in M$  can be approximated by Barron functions with Barron norm growing only polynomially in the dimension, yielding the rate  $n^{-1/2}$  for shallow networks of width  $(dn)^{C\\log(n)}$ , where  $C$  is a constant, we refer to Chen et al. (2021). Note that the result in Chen et al. (2021) so far does only hold for linear elliptic PDEs and special activation functions, and does not include parametric dependencies. Our result, then, shows that this error decay rate is preserved for any energy-convergent approximation scheme and is, in fact, not a mere approximation result.\n\n\nRelated Work For PDEs that admit a stochastic representation, several situations are known in which the curse of dimensionality can be circumvented, Jentzen et al. (2021); Han et al. (2018); Weinan et al. (2021). These results are of approximation theoretic nature and do not provide a way to construct the approximating network. Further, this approach crucially relies on the stochastic representation of the PDE's solution and, thus, is not generally applicable.\n\n\nThe works Xu (2020); Jiao et al. (2021); Duan et al. (2021); M\u00fcller & Zeinhofer (2022) are similar to our contribution since they consider elliptic equations and provide a C\u00e9a type Lemma and, consequently, are not only approximation theoretic results. However, they neither analyze non-linear nor parametric equations.\n\n\nThe contributions in Chen et al. (2021); Weinan & Wojtowytsch (2022) mark the beginning of a regularity theory for elliptic equations with respect to Barron spaces. These results are complementary to our analysis in the sense that they can be combined with our contribution. For instance, the main result of Chen et al. (2021) states that a solution to a linear elliptic PDE with Barron data is \"almost\" of Barron regularity and can be approximated with a polynomial rate with respect to the dimension. Our analysis, then, guarantees that every neural network approximation that is close in energy to the ground truth solution realizes this rate.\n\n\nNotation For a Banach space  $X$  with norm  $\\| \\cdot \\|_{X}: X \\to \\mathbb{R}_{\\geq 0}$ , we denote by  $X^{*}$ , its (topological) dual space equipped with the norm  $\\| \\cdot \\|_{X^{*}}: X^{*} \\to \\mathbb{R}_{\\geq 0}$ , defined by  $\\| x^{*} \\|_{X^{*}} \\coloneqq \\sup_{\\| x \\|_{X} \\leq 1} \\langle x^{*}, x \\rangle_{X}$  for all  $x^{*} \\in X^{*}$ . Here,  $\\langle \\cdot, \\cdot \\rangle_{X}: X^{*} \\times X \\to \\mathbb{R}$  denotes the duality pairing, defined by  $\\langle x^{*}, x \\rangle_{X} \\coloneqq x^{*}(x)$  for all  $x^{*} \\in X^{*}$  and  $x \\in X$ .\n\n\nFor  $p \\in [1, \\infty]$ , we denote by  $L^p(\\Omega)$ , the space of (Lebesgue-)measurable functions  $u: \\Omega \\to \\mathbb{R}$  that are integrable in  $p$ -th power, i.e.,  $\\int_{\\Omega} |u|^p \\, \\mathrm{d}x < \\infty$  if  $p \\in [1, \\infty)$  and  $\\operatorname{ess} \\sup_{x \\in \\Omega} |u(x)| < \\infty$  if  $p = \\infty$ . Endowed with the norm  $\\|u\\|_{L^p(\\Omega)} := (\\int_{\\Omega} |u|^p \\, \\mathrm{d}x)^{\\frac{1}{p}}$  if  $p \\in [1, \\infty)$  and  $\\|u\\|_{L^\\infty(\\Omega)} := \\operatorname{ess} \\sup_{x \\in \\Omega} |u(x)|$  if  $p = \\infty$ , the space  $L^p(\\Omega)$  forms a Banach space, which is separable if  $p \\in [1, \\infty)$  and reflexive if  $p \\in (1, \\infty)$ , cf. (Adams & Fournier, 2003, Chapter 2).\n\n\nFor  $k \\in \\mathbb{N}$  and  $p \\in [1, \\infty]$ , we denote by  $W^{k,p}(\\Omega)$ , the space of functions in  $L^p(\\Omega)$  with distributional derivatives up to  $k$ -th order in  $L^p(\\Omega)$ . Endowed with the norm  $\\| u \\|_{W^{k,p}(\\Omega)} \\coloneqq \\sum_{l=0}^{k} \\| D^l u \\|_{L^p(\\Omega)}$ , the space  $W^{k,p}(\\Omega)$  forms a Banach space, which is separable if  $p \\in [1, \\infty)$  and reflexive if  $p \\in (1, \\infty)$ , cf. (Adams & Fournier, 2003, Chapter 3). For  $k \\in \\mathbb{N}$  and  $p \\in [1, \\infty]$ , we denote by  $W_0^{k,p}(\\Omega)$ , the closure of all compactly supported, smooth functions  $C_c^\\infty(\\Omega)$  in  $W^{k,p}(\\Omega)$ .\n\n\nWe always denote parameter-dependent functions by boldface letters, e.g.,  $u$ ,  $v$ ,  $w$ , ..., and parameter-independent functions by non-boldface letters, e.g.,  $u$ ,  $v$ ,  $w$ , ... In the same spirit, we denote by  $\\mathcal{E}: L^p(\\mathcal{P}; W^{1,p}(\\Omega)) \\to \\mathbb{R}$ , the parametric  $p$ -Dirichlet energy equation 2 and by  $E: W^{1,p}(\\Omega) \\to \\mathbb{R}$ , the non-parametric  $p$ -Dirichlet energy (c.f. equation 6).\n\n\n# 2 PROOF OF THE MAIN RESULT\n\n\nIn this section, we provide the proof of Theorem 1. For clarity, we first consider the non-parametric case and extend the results afterwards to include parametric dependencies.\n\n\n# 2.1 PROOF OF THE NON-PARAMETRIC SETTING\n\n\nThe main step in the proof of the non-parametric version of Theorem 1 is to show that convergence in energy, i.e.,  $E(u_{n}) \\to E(u^{*})$  ( $n \\to \\infty$ ) (cf. equation 6) for a neural network approximation  $u_{n} \\in M$  of the ground truth solution  $u^{*} \\in M$ , is equivalent to the convergence of  $u_{n} \\to u^{*}$  ( $n \\to \\infty$ ) in the Sobolev topology. To establish this in a quantitative fashion, we need an optimal measure of the convexity of the  $p$ -Dirichlet energy (cf. equation 6). This is given through the bi-variate, symmetric mapping  $\\rho_F^2: W^{1,p}(\\Omega) \\times W^{1,p}(\\Omega) \\to \\mathbb{R}$ , defined by\n\n\n$$\n\n$$\n\\rho_ {F} ^ {2} (v, w) := \\| F (\\nabla v) - F (\\nabla w) \\| _ {L ^ {2} (\\Omega) ^ {d}} ^ {2} \\quad \\text {f o r a l l} v, w \\in W ^ {1, p} (\\Omega), \\tag {4}\n$$\n\n$$\n\n\nwhere  $F:\\mathbb{R}^d\\to \\mathbb{R}^d$  is defined by\n\n\n$$\n\n$$\nF (a) := | a | ^ {\\frac {p - 2}{2}} a \\quad \\text {f o r a l l} a \\in \\mathbb {R} ^ {d}. \\tag {5}\n$$\n\n$$\n\n\nThe map  $\\rho_F^2: W^{1,p}(\\Omega) \\times W^{1,p}(\\Omega) \\to \\mathbb{R}$  is the optimal distance measure for the  $p$ -Dirichlet problem. This is embodied in the two-sided estimate proved in the next proposition, see equation 7, that relates convergence in energy to convergence in terms of  $\\rho_F^2: W^{1,p}(\\Omega) \\times W^{1,p}(\\Omega) \\to \\mathbb{R}$ . In the literature,  $\\rho_F^2: W^{1,p}(\\Omega) \\times W^{1,p}(\\Omega) \\to \\mathbb{R}$  is usually referred to as the Natural Distance, cf. Diening & R\u00fc\u017eicka (2007); Diening et al. (2007); Diening & Ettwein (2008); Kaltenbach & R\u00fc\u017eicka (2022).\n\n\nNext, we establish a C\u00e9a type Lemma in terms of  $\\rho_F^2: W^{1,p}(\\Omega) \\times W^{1,p}(\\Omega) \\to \\mathbb{R}$ , see Lemma 5. This decomposes the distance of  $u_n$  to  $u^*$  into an optimization error and an approximation theoretic contribution. Finally, we study the relation of  $\\rho_F^2: W^{1,p}(\\Omega) \\times W^{1,p}(\\Omega) \\to \\mathbb{R}$  to the standard Sobolev topology in Lemma 7.\n\n\nFrom a technical perspective, the central estimate in equation 7 is proved via a Taylor expansion of the  $p$ -Dirichlet energy around its minimizer  $u^{*} \\in W^{1,p}(\\Omega)$ . However, care needs to be taken since  $E: W^{1,p}(\\Omega) \\to \\mathbb{R}$  is not twice continuously differentiable and a subtle regularization procedure needs to be employed to rigorously carry out the expansion.\n\n\nProposition 2. Let  $\\Omega \\subseteq \\mathbb{R}^d$ ,  $d \\in \\mathbb{N}$ , be a bounded domain,  $f \\in W^{1,p}(\\Omega)^*$ ,  $p \\in (1,\\infty)$ , and let  $U \\subseteq W^{1,p}(\\Omega)$  be a closed subspace such that Poincar\u00e9's inequality is valid, i.e., there exists a constant  $C_P > 0$  such that for every  $v \\in U$ , it holds\n\n\n$$\n\n$$\n\\left\\| v \\right\\| _ {L ^ {p} (\\Omega)} \\leq C _ {P} \\left\\| \\nabla v \\right\\| _ {L ^ {p} (\\Omega) ^ {d}}.\n$$\n\n$$\n\n\nMoreover, define  $E: U \\to \\mathbb{R}$  for every  $v \\in U$  by\n\n\n$$\n\n$$\nE (v) := \\frac {1}{p} \\int_ {\\Omega} | \\nabla v | ^ {p} \\mathrm {d} x - \\langle f, v \\rangle_ {W ^ {1, p} (\\Omega)}. \\tag {6}\n$$\n\n$$\n\n\nThen, the following statements apply:\n\n- (i) There exists a unique minimizer  $u^{*}\\in U$  for  $E:U\\to \\mathbb{R}$\n\n- (ii) There exists a constant  $c(p) > 0$ , depending only on  $p \\in (1, \\infty)$  and not depending on  $d \\in \\mathbb{N}$ , such that for every  $v \\in U$ , it holds\n\n\n$$\n\n$$\nc (p) ^ {- 1} \\rho_ {F} ^ {2} (v, u ^ {*}) \\leq E (v) - E (u ^ {*}) \\leq c (p) \\rho_ {F} ^ {2} (v, u ^ {*}). \\tag {7}\n$$\n\n$$\n\n\nMoreover, we can choose  $c(p) > 0$  such that  $(p\\mapsto c(p))\\in C^0 (1,\\infty)$ .\n\n\nProof. The proof is provided in the Appendix, see A.\n\n\nRemark 3 (The Case  $p = 2$ ). In the case  $p = 2$ , we retrieve the well-known Dirichlet energy. Further, equality holds in equation 7 with constant  $c(p) = \\frac{1}{2}$ . More precisely, for every  $v \\in U$ , we have that\n\n\n$$\n\n$$\nE (v) - E \\left(u ^ {*}\\right) = \\frac {1}{2} \\| \\nabla v - \\nabla u ^ {*} \\| _ {L ^ {2} (\\Omega) ^ {d}} ^ {2} = \\rho_ {F} ^ {2} \\left(v, u ^ {*}\\right).\n$$\n\n$$\n\n\nThis can be shown by a straight-forward Taylor expansion of  $E: U \\to \\mathbb{R}$  around  $u^{*} \\in U$ , cf. M\u00fcller & Zeinhofer (2022).\n\n\nRemark 4 (The Role of the Space  $U$ ). The space  $U$  encodes boundary conditions, for example,  $U = W_0^{1,p}(\\Omega)$  is an admissible choice. However, when choosing  $U = W^{1,p}(\\Omega)$  and requiring that the right-hand side  $f \\in W^{1,p}(\\Omega)^*$  vanishes on constant functions, Proposition 2 stays valid with the exemption of the uniqueness of the minimizer  $u^* \\in U$ . In this case,  $u^* \\in U$  is only determined up to additive constants. This can be seen by considering the energy  $E$  on the quotient space  $W^{1,p}(\\Omega)$  modulo the constant functions. On this space, a Poincar\u00e9 inequality is available.\n\n\nAn immediate consequence of Theorem 2 is the following C\u00e9a type lemma.\n\n\nLemma 5 (C\u00e9a Lemma). Let the assumptions of Proposition 2 be satisfied. Moreover, let  $M \\subseteq U$  be an arbitrary subset. Then, there exists a constant  $c(p) > 0$ , depending only on  $p \\in (1, \\infty)$  and not depending on  $d \\in \\mathbb{N}$ , such that for every  $v \\in M$ , it holds\n\n\n$$\n\n$$\n\\rho_ {F} ^ {2} (v, u ^ {*}) \\leq c (p) \\left(\\delta (v) + \\inf _ {\\tilde {v} \\in M} \\rho_ {F} ^ {2} (\\tilde {v}, u ^ {*})\\right),\n$$\n\n$$\n\n\nwhere  $\\delta(v) \\coloneqq E(v) - \\inf_{\\tilde{v} \\in M} E(\\tilde{v})$  is the optimization error. Moreover, we can choose  $c(p) > 0$  such that  $(p \\mapsto c(p)) \\in C^0(1, \\infty)$ .\n\n\nProof of Lemma 5. Let  $v \\in M$  be arbitrary. Then, by referring to Theorem 2, we find that\n\n\n$$\n\n$$\nc (p) ^ {- 1} \\rho_ {F} ^ {2} (v, u ^ {*}) \\leq E (v) - \\inf  _ {\\tilde {v} \\in M} E (\\tilde {v}) + \\inf  _ {\\tilde {v} \\in M} E (\\tilde {v}) - E (u ^ {*}) \\leq \\delta (v) + c (p) \\inf  _ {\\tilde {v} \\in M} \\rho_ {F} ^ {2} (\\tilde {v}, u).\n$$\n\n$$\n\n\nRemark 6. Note that we do not need to impose any structure on the set  $M$ , in particular, it does not need to possess a linear structure. This, in contrast to classical formulations of C\u00e9a's Lemma, allows us to choose  $M$  as an ansatz class consisting of neural networks.\n\n\nIn order to arrive at error decay rates in Sobolev topology, we need the relation of  $\\rho_F^2$  to the  $W^{1,p}(\\Omega)$ -semi-norm.\n\n\nLemma 7 (Relation Between Natural Distance and  $W^{1,p}$ -Semi-Norm). Let  $\\Omega \\subseteq \\mathbb{R}^d$ ,  $d \\in \\mathbb{N}$ , be a bounded domain and  $p \\in (1,\\infty)$ . Then, there exists a constant  $c(p) > 0$ , depending only on  $p \\in (1,\\infty)$  and not depending on  $d \\in \\mathbb{N}$ , such that the following relations apply:\n\n\n(i) If  $p\\in [2,\\infty)$ , then for every  $u,v\\in W^{1,p}(\\Omega)$ , it holds\n\n\n$$\n\n$$\n\\begin{array}{l} c (p) ^ {- 1} \\| \\nabla u - \\nabla v \\| _ {L ^ {p} (\\Omega) ^ {d}} ^ {p} \\leq \\rho_ {F} ^ {2} (u, v) \\\\ \\leq c (p) \\left(\\| \\nabla u \\| _ {L ^ {p} (\\Omega) ^ {d}} + \\| \\nabla v \\| _ {L ^ {p} (\\Omega) ^ {d}}\\right) ^ {p - 2} \\| \\nabla u - \\nabla v \\| _ {L ^ {p} (\\Omega) ^ {d}} ^ {2}. \\\\ \\end{array}\n$$\n\n$$\n\n\n(ii) If  $p \\in (1,2)$ , then for every  $v, w \\in W^{1,p}(\\Omega)$ , it holds\n\n\n$$\n\n$$\n\\begin{array}{l} c (p) ^ {- 1} \\rho_ {F} ^ {2} (u, v) \\leq \\| \\nabla u - \\nabla v \\| _ {L ^ {p} (\\Omega) ^ {d}} ^ {p} \\\\ \\leq c (p) \\left(\\| \\nabla u \\| _ {L ^ {p} (\\Omega) ^ {d}} + \\| \\nabla v \\| _ {L ^ {p} (\\Omega) ^ {d}}\\right) ^ {\\frac {p (2 - p)}{2}} \\rho_ {F} ^ {2} (u, v) ^ {\\frac {p}{2}}. \\\\ \\end{array}\n$$\n\n$$\n\n\nMoreover, we can choose  $c(p) > 0$  such that  $(p\\mapsto c(p))\\in C^0 (1,\\infty)$ .\n\n\nProof. The proof is provided in the Appendix, see A.\n\n\n![](images/0a34a73d7697ead3d2f580c6fbc8f3ac97187c7248228d513d43d62658964397.jpg)\n\n\nWe are now able to prove the main result in a setting excluding parametric dependencies.\n\n\nTheorem 8. Let  $f \\in W^{1,p}(\\Omega)^*$ ,  $p \\in (1, \\infty)$ , be such that  $\\langle f, c \\rangle_{W^{1,p}(\\Omega)} = 0$  for all  $c \\in \\mathbb{R}$ . Moreover, let  $u^* \\in W^{1,p}(\\Omega)$  be a weak solution of the  $p$ -Laplace problem with homogeneous Neumann boundary conditions, i.e.,  $u^* \\in W^{1,p}(\\Omega)$  minimizes  $E: W^{1,p}(\\Omega) \\to \\mathbb{R}$ , for every  $v \\in W^{1,p}(\\Omega)$  defined by\n\n\n$$\n\n$$\nE (v) := \\frac {1}{p} \\int_ {\\Omega} | \\nabla v | ^ {p} \\mathrm {d} x - \\langle f, v \\rangle_ {W ^ {1, p} (\\Omega)}. \\tag {8}\n$$\n\n$$\n\n\nLet  $M \\subset W^{1,p}(\\Omega)$  be any subset that contains the zero function and let  $v \\in M$  be an arbitrary. Setting\n\n\n$$\n\n$$\n\\tilde {M} := \\left\\{u \\in M \\mid \\| \\nabla u \\| _ {L ^ {p} (\\Omega) ^ {d}} \\leq 2 \\| \\nabla u ^ {*} \\| _ {L ^ {p} (\\Omega) ^ {d}} \\right\\},\n$$\n\n$$\n\n\nit holds\n\n\n$$\n\n$$\n\\left\\| \\nabla v - \\nabla u ^ {*} \\right\\| _ {L ^ {p} (\\Omega) ^ {d}} \\preceq \\left\\{ \\begin{array}{l l} \\delta (v) ^ {1 / p} + \\inf  _ {\\tilde {v} \\in \\tilde {M}} \\left\\| \\nabla \\tilde {v} - \\nabla u ^ {*} \\right\\| _ {L ^ {p} (\\Omega) ^ {d}} ^ {\\frac {2}{p}} & i f p \\in [ 2, \\infty) \\\\ \\delta (v) ^ {1 / 2} + \\inf  _ {\\tilde {v} \\in \\tilde {M}} \\left\\| \\nabla \\tilde {v} - \\nabla u ^ {*} \\right\\| _ {L ^ {p} (\\Omega) ^ {d}} ^ {\\frac {p}{2}} & i f p \\in (1, 2) \\end{array} \\right.,\n$$\n\n$$\n\n\nwhere  $\\delta(v) \\coloneqq E(v) - \\inf_{\\tilde{v} \\in \\tilde{M}} E(\\tilde{v})$  is the optimization error and the implicit constant depends on  $p$ ,  $\\Omega$  and  $\\|f\\|_{W^{1,p}(\\Omega)^*}$  only.\n\n\nProof. ad  $p \\in [2, \\infty)$ . If  $p \\in [2, \\infty)$ , then we estimate, using the relation of the natural distances to Sobolev semi-norms as described in Lemma 7, C\u00e9a's Lemma 5, and the coercivity estimate in Lemma 13 to obtain\n\n\n$$\n\n$$\n\\begin{array}{l} \\| \\nabla v - \\nabla u ^ {*} \\| _ {L ^ {p} (\\Omega) ^ {d}} ^ {p} \\leq c (p) \\rho_ {F} ^ {2} (v, u ^ {*}) \\\\ \\leq c (p) \\left(\\delta (v) + \\inf  _ {\\tilde {v}} \\rho_ {F} ^ {2} (\\tilde {v}, u ^ {*})\\right) \\\\ \\leq c (p) \\left(\\delta (v) + \\inf  _ {\\tilde {v} \\in \\tilde {M}} \\left[ \\left(\\| \\nabla \\tilde {v} \\| _ {L ^ {p} (\\Omega) ^ {d}} + \\| \\nabla u ^ {*} \\| _ {L ^ {p} (\\Omega) ^ {d}}\\right) ^ {p - 2} \\| \\nabla \\tilde {v} - \\nabla u ^ {*} \\| _ {L ^ {p} (\\Omega) ^ {d}} ^ {2} \\right]\\right) \\\\ \\leq c (p) \\left(\\delta (v) + 3 ^ {p - 2} \\| \\nabla u ^ {*} \\| _ {L ^ {p} (\\Omega) ^ {d}} ^ {p - 2} \\inf  _ {\\tilde {v} \\in \\tilde {M}} \\| \\nabla \\tilde {v} - \\nabla u ^ {*} \\| _ {L ^ {p} (\\Omega) ^ {d}} ^ {2}\\right) \\\\ \\leq c (p) \\delta (v) + 3 ^ {p - 2} c (p, \\Omega) \\| f \\| _ {W ^ {1, p} (\\Omega) ^ {*}} ^ {\\frac {p - 2}{p - 1}} \\inf  _ {\\tilde {v} \\in \\tilde {M}} \\| \\nabla \\tilde {v} - \\nabla u ^ {*} \\| _ {L ^ {p} (\\Omega) ^ {d}} ^ {2}. \\\\ \\end{array}\n$$\n\n$$\n\n\nad  $p \\in (1,2]$ . If  $p \\in (1,2]$ , then, again, using the relation of the natural distance to Sobolev semi-norms (cf. Lemma 7) and C\u00e9a's Lemma 5, we obtain\n\n\n$$\n\n$$\n\\| \\nabla v - \\nabla u ^ {*} \\| _ {L ^ {p} (\\Omega) ^ {d}} \\leq c (p) \\left(\\| \\nabla v \\| _ {L ^ {p} (\\Omega) ^ {d}} + \\| \\nabla u ^ {*} \\| _ {L ^ {p} (\\Omega) ^ {d}}\\right) ^ {\\frac {2 - p}{2}} \\left(\\delta (v) ^ {\\frac {1}{2}} + \\inf  _ {\\tilde {v} \\in M} \\| \\nabla \\tilde {v} - \\nabla u ^ {*} \\| _ {L ^ {p} (\\Omega) ^ {d}} ^ {\\frac {p}{2}}\\right).\n$$\n\n$$\n\n\nThus, it remains to estimate the first factor in the equation above. We use the coercivity estimate of Lemma 13 to obtain\n\n\n$$\n\n$$\n\\| \\nabla v \\| _ {L ^ {p} (\\Omega) ^ {d}} \\leq c (p, \\Omega) \\left(E (v) + \\| f \\| _ {W ^ {1, p} (\\Omega) ^ {*}} ^ {p ^ {\\prime}}\\right) ^ {\\frac {1}{p}} \\leq c (p, \\Omega) \\| f \\| _ {W ^ {1, p} (\\Omega) ^ {*}} ^ {\\frac {1}{p - 1}},\n$$\n\n$$\n\n\nwhere we used that  $0 \\in M$  to be able to estimate  $E(v) \\leq \\delta(v)$ . As a result, again applying Lemma 13, it follows that\n\n\n$$\n\n$$\n\\begin{array}{l} \\left(\\| \\nabla v \\| _ {L ^ {p} (\\Omega) ^ {d}} + \\| \\nabla u ^ {*} \\| _ {L ^ {p} (\\Omega) ^ {d}}\\right) ^ {\\frac {2 - p}{2}} \\leq c (p, \\Omega) \\left(\\delta (v) ^ {\\frac {2 - p}{2 p}} + \\| f \\| _ {W ^ {1, p} (\\Omega) ^ {*}} ^ {\\frac {2 - p}{2 p - 2}}\\right) \\\\ = c (p, \\Omega) \\left(\\delta (v) ^ {\\frac {1}{2}} + \\| f \\| _ {W ^ {1, p} (\\Omega) ^ {*}} ^ {\\frac {2 - p}{2 p - 2}}\\right). \\\\ \\end{array}\n$$\n\n$$\n\n\nAssuming  $\\delta(v) \\leq 1$ , it holds  $\\delta(v)^{\\frac{1}{2}} + \\delta(v)^{\\frac{1}{p}} \\leq 2\\delta(v)^{\\frac{1}{2}}$ , which concludes the proof.\n\n\n![](images/c97ddc1b20401bd0028f31d92feb2ff2a4b4a1429fae0e242e9bddf1c72945f5.jpg)\n\n\n# 2.2 PROOF OF THE PARAMETRIC SETTING\n\n\nAs detailed in the introduction, the energy formulation we use for a  $p$ -Laplace problem with a parametric right-hand side  $\\pmb{f} \\in L^{p'}(\\mathcal{P} \\times \\Omega)$  and parameter space  $\\mathcal{P} \\subseteq \\mathbb{R}^{d_{\\mathcal{P}}}$ ,  $d_{\\mathcal{P}} \\in \\mathbb{N}$ , for every  $\\pmb{v} \\in L^p(\\mathcal{P}, W^{1,p}(\\Omega))$ , is defined by\n\n\n$$\n\n$$\n\\boldsymbol {\\mathcal {E}} (\\boldsymbol {v}) := \\int_ {\\mathcal {P}} \\left[ \\frac {1}{p} \\int_ {\\Omega} | \\nabla_ {x} \\boldsymbol {v} (\\boldsymbol {\\tau}, \\cdot) | ^ {p} \\mathrm {d} x - \\int_ {\\Omega} \\boldsymbol {f} (\\boldsymbol {\\tau}, \\cdot) \\boldsymbol {v} (\\boldsymbol {\\tau}, \\cdot) \\mathrm {d} x \\right] \\mathrm {d} \\boldsymbol {\\tau}.\n$$\n\n$$\n\n\nBefore proving the error decay rates of Theorem 1, we need to identify the correct function space  $\\mathcal{U}$  for the definition of  $\\varepsilon$ . In the case of a parametric right-hand side, this is straightforward and the space  $\\mathcal{U}$  is a standard Bochner space, see Proposition 9. For varying domains or a varying exponent as a parametric dependency, the corresponding function spaces are intricate, we refer to Appendix C.\n\n\nNext, we need to guarantee that the minimizer of  $\\pmb{\\mathcal{E}}$  indeed solves the parametric problem. This is carried out in Proposition 9 and is encoded in the fact that  $\\pmb{u}^{*}(\\pmb{\\tau},\\cdot)\\in W^{1,p}(\\Omega)$  for a.e.  $\\pmb{\\tau}\\in \\mathcal{P}$  minimizes  $E_{\\tau}$  in the notation of this Proposition.\n\n\nProceeding to derive error estimates, we want to mimic the strategy of the non-parametric case. This crucially relies on the fact that the constants in equation 7 do not depend on the right-hand side. As a consequence, we can prove a two-sided estimate as in Proposition 9 with an analogue of  $\\rho_F^2: W^{1,p}(\\Omega) \\times W^{1,p}(\\Omega) \\to \\mathbb{R}$  given by  $\\rho_{\\mathcal{F}}^2: L^p(\\mathcal{P}, W^{1,p}(\\Omega)) \\times L^p(\\mathcal{P}, W^{1,p}(\\Omega)) \\to \\mathbb{R}$ , for every  $\\boldsymbol{v}, \\boldsymbol{w} \\in L^p(\\mathcal{P}, W^{1,p}(\\Omega))$  defined by\n\n\n$$\n\n$$\n\\rho_ {\\boldsymbol {\\mathcal {F}}} ^ {2} (\\boldsymbol {v}, \\boldsymbol {w}) := \\int_ {\\mathcal {P}} \\rho_ {F} ^ {2} (\\boldsymbol {v} (\\boldsymbol {\\tau}, \\cdot), \\boldsymbol {u} (\\boldsymbol {\\tau}, \\cdot))   d \\boldsymbol {\\tau}.\n$$\n\n$$\n\n\nFinally, we can proceed as in the non-parametric case and derive a C\u00e9a Lemma.\n\n\nProposition 9 (Variable Right-Hand Sides). Let  $\\Omega \\subseteq \\mathbb{R}^{d_{\\Omega}}$ ,  $d_{\\Omega} \\in \\mathbb{N}$ , and  $\\mathcal{P} \\subseteq \\mathbb{R}^{d_{\\mathcal{P}}}$ ,  $d_{\\mathcal{P}} \\in \\mathbb{N}$ , be bounded domains and  $p \\in (1, \\infty)$ . Assume  $U \\subset W^{1,p}(\\Omega)$  is a closed subset that satisfies a Poincar\u00e9 inequality, as in Proposition 2. Moreover, we define the Bochner-Lebesgue space\n\n\n$$\n\n$$\n\\boldsymbol {U} := L ^ {p} (\\mathcal {P}, U).\n$$\n\n$$\n\n\nFor fixed  $\\pmb{f} \\in L^{p'}(\\mathcal{P} \\times \\Omega)$ , we define the variable right-hand side  $p$ -Dirichlet energy  $\\pmb{\\mathcal{E}}: \\mathcal{U} \\to \\mathbb{R}$  for every  $\\pmb{v} \\in \\mathcal{U}$  by\n\n\n$$\n\n$$\n\\boldsymbol {\\mathcal {E}} (\\boldsymbol {v}) := \\int_ {\\mathcal {P}} \\left[ \\frac {1}{p} \\int_ {\\Omega} | \\nabla_ {\\boldsymbol {x}} \\boldsymbol {v} (\\boldsymbol {\\tau}, \\cdot) | ^ {p} \\mathrm {d} x - \\int_ {\\Omega} \\boldsymbol {f} (\\boldsymbol {\\tau}, \\cdot) \\boldsymbol {v} (\\boldsymbol {\\tau}, \\cdot) \\mathrm {d} x \\right] \\mathrm {d} \\boldsymbol {\\tau},\n$$\n\n$$\n\n\nwhere the gradient  $\\nabla_{x}$  for a.e.  $\\pmb{\\tau} \\in \\mathcal{P}$  is to be understood with respect to the variable  $x \\in \\Omega$  only. Then, the following statements apply:\n\n- (i) There exists a unique (parametric) minimizer  $\\mathbf{u}^{*}\\in \\mathcal{U}$  of  $\\varepsilon :\\mathcal{U}\\to \\mathbb{R}$\n\n- (ii) For a.e.  $\\pmb{\\tau} \\in \\mathcal{P}$ , the function  $\\pmb{u}^{*}(\\pmb{\\tau},\\cdot) \\in U$  is the unique minimizer of  $E_{\\pmb{\\tau}}: U \\to \\mathbb{R}$ , for every  $v \\in U$  defined by\n\n\n$$\n\n$$\nE _ {\\boldsymbol {\\tau}} (v) := \\frac {1}{p} \\int_ {\\Omega} | \\nabla v | ^ {p} d x - \\int_ {\\Omega} \\boldsymbol {f} (\\boldsymbol {\\tau}, \\cdot) v d x.\n$$\n\n$$\n\n\n(iii) Furthermore, for every  $\\pmb{v} \\in \\mathcal{U}$ , it holds\n\n\n$$\n\n$$\nc (p) ^ {- 1} \\rho_ {\\mathcal {F}} ^ {2} (\\boldsymbol {v}, \\boldsymbol {u} ^ {*}) \\leq \\mathcal {E} (\\boldsymbol {v}) - \\mathcal {E} (\\boldsymbol {u} ^ {*}) \\leq c (p) \\rho_ {\\mathcal {F}} ^ {2} (\\boldsymbol {v}, \\boldsymbol {u} ^ {*}), \\tag {9}\n$$\n\n$$\n\n\nwhere  $c(p) > 0$  is the constant from Proposition 7 (ii).\n\n\nProof. We prove this for more general parametric dependencies in the Appendix, see C.\n\n\n![](images/ec6d9ab2c3c7c3c082cde57b923ab92c33779794b8fc0cfe84c97d61ad52bbf4.jpg)\n\n\nRemark 10. Requiring  $f(\\tau, \\cdot)$  to be mean-value-free for a.e.  $\\tau \\in \\mathcal{P}$ , we may, again, drop the assumption of a Poincar\u00e9 inequality on the space  $U$ , as we explained in Remark 4. In this case, we cannot expect the minimizer to be unique.\n\n\nThe estimate 9 is the key to establish a C\u00e9a type Lemma for the energy  $\\mathcal{E}:\\mathcal{U}\\to \\mathbb{R}$ . In the situation of Proposition 9, we can accomplish this as in Lemma 5. More precisely, for any fixed  $\\pmb {v}\\in M\\subset \\pmb{U}$ , it holds\n\n\n$$\n\n$$\n\\rho_ {\\mathcal {F}} ^ {2} (\\boldsymbol {v}, \\boldsymbol {u} ^ {*}) \\leq c (p) \\left(\\delta (\\boldsymbol {v}) + \\inf  _ {\\tilde {\\boldsymbol {v}} \\in M} \\rho_ {\\mathcal {F}} ^ {2} (\\tilde {\\boldsymbol {v}}, \\boldsymbol {u} ^ {*})\\right), \\tag {10}\n$$\n\n$$\n\n\nwhere  $\\delta(\\boldsymbol{v}) \\coloneqq \\mathcal{E}(\\boldsymbol{v}) - \\inf_{\\tilde{\\boldsymbol{v}} \\in M} \\mathcal{E}(\\tilde{\\boldsymbol{v}})$  the parametric optimization error. With all the previous work, the Main Theorem is can now be proved in a similar way as in the case without parameters. We postpone the proof to the Appendix, see A.2.\n\n\n# 3 NUMERICAL EXAMPLES\n\n\nWe give two examples, one with a high dimensional physical domain and one with a high-dimensional parametric right-hand side. Our goal is to investigate the error in dependence on the dimension. To find good neural network approximations, we employ a Deep Ritz Method for training.\n\n\nExample 1 As a PDE posed on a high-dimensional physical domain, we consider\n\n\n$$\n\n$$\n- \\Delta u + u = f \\quad \\text {i n} \\Omega ,\n$$\n\n$$\n\n\nwith homogeneous Neumann boundary conditions, and  $\\Omega = (0,1)^{d_{\\Omega}}$ ,  $d_{\\Omega} \\in \\mathbb{N}$ . As manufactured solution, we use  $u \\in W^{1,2}(\\Omega)$ , for every  $x = (x_1, \\ldots, x_{d_{\\Omega}})^\\top \\in \\Omega$  defined by\n\n\n$$\n\n$$\nu (x) := c \\cdot \\sum_ {i = 1} ^ {d _ {\\Omega}} \\cos (\\pi x _ {i}),\n$$\n\n$$\n\n\nwhere  $c > 0$  is chosen such that  $u$  has uniform  $L^2(\\Omega)$  norm. Then, the right-hand side  $f \\in L^2(\\Omega)$ , for every  $x \\in \\Omega$  is given via\n\n\n$$\n\n$$\nf (x) = (\\pi^ {2} + 1) u (x).\n$$\n\n$$\n\n\nExample 2 As a problem with a high-dimensional parametric right-hand side, we consider\n\n\n$$\n\n$$\n- \\boldsymbol {u} ^ {\\prime \\prime} + \\boldsymbol {u} = \\boldsymbol {f} \\quad \\text {i n} \\mathcal {P} \\times \\Omega ,\n$$\n\n$$\n\n\nwith homogeneous Neumann boundary conditions, parameter space  $\\mathcal{P} = [-1,1]^{d_{\\mathcal{P}}}$ ,  $d_{\\mathcal{P}} \\in \\mathbb{N}$  and physical domain  $\\Omega = (0,1)$ . As manufactured solution and right-hand side, we use  $\\pmb{u} \\in L^2(\\mathcal{P}, W^{1,2}(\\Omega))$  and  $\\pmb{f} \\in L^2(\\mathcal{P} \\times \\Omega)$ , for every  $(\\pmb{\\tau}, x)^{\\top} = (\\tau_1, \\dots, \\tau_{d_{\\mathcal{P}}}, x)^{\\top} \\in \\mathcal{P} \\times \\Omega$  defined by\n\n\n$$\n\n$$\n\\boldsymbol {u} (\\boldsymbol {\\tau}, x) := \\sum_ {k = 0} ^ {d _ {\\mathcal {P}} - 1} \\frac {\\tau_ {k}}{k ^ {2} \\pi^ {2} + 1} \\cos (k \\pi x) \\quad \\text {a n d} \\quad \\boldsymbol {f} (\\boldsymbol {\\tau}, x) = \\sum_ {k = 0} ^ {d _ {\\mathcal {P}} - 1} \\tau_ {k} \\cos (k \\pi x).\n$$\n\n$$\n\n\nNeural Network Architecture and Training We employ fully-connected  $\\mathrm{ReLU}^2$ -networks with four hidden layers and varying width as well as a Deep Ritz energy formulation as a loss function. To resolve the minimization problem, we employ the Adam optimizer with learning rate set to 0.001. The appearing integrals are discretized using Monte-Carlo approximations, where new random points are drawn for every update in the gradient descent. The optimization is run until no further improvement is seen in approximating the ground truth, which in our examples happens typically in around 10,000 to 20,000 iterations.\n\n\n![Figure 1: The plot on the left shows the relative  $L^2$  errors obtained from Example 1 and the plot on the right reports the errors for Example 2. Here, dashed lines represent relative  $H^1$  errors and solid lines stand for relative  $L^2$  errors.](images/307166777e9ed380e229919dc0d76932d7d683e48558bfdd236a0047d1e5f969.jpg)\n\n\n![](images/001c509e333268b6bc0b249ad7b3b891821a602fc788bb54ac2ade00838032c3.jpg)\n\n\nDiscussion In Figure 1, we report the relative errors obtained for the two examples in dependence on the total dimension  $d = d_{\\Omega} + d_{\\mathcal{P}} \\in \\mathbb{N}$  of the parametric cylinder  $\\mathcal{P} \\times \\Omega \\subset \\mathbb{R}^{d_{\\mathcal{P}}} \\times \\mathbb{R}^{d_{\\Omega}}$ . In both examples, no exponential increase of the error is observable \u2013 at least for moderately high dimensions. Although it is impossible for us to quantify how well the empirically found solutions resolve the minimization, i.e., how large the quantity  $\\delta(u_{\\theta})$  in Theorem 1 is, the experiments still confirm the promising behavior of neural networks for solving high-dimensional and parametric problems.\n\n\n# AUTHOR CONTRIBUTIONS\n\n\nAll authors contributed equally to all parts of the manuscript.\n\n\n# REFERENCES\n\n- Robert Adams and John Fournier. Sobolev Spaces. Elsevier Science, 2003.\n\n- Andrew R Barron. Universal Approximation Bounds for Superpositions of a Sigmoidal Function. IEEE Transactions on Information theory, 39(3):930-945, 1993.\n\n- Richard Bellman. The Theory of Dynamic Programming. Bulletin of the American Mathematical Society, 60(6):503-515, 1954.\n\n- Lorenz T Biegler, Omar Ghattas, Matthias Heinkenschloss, David Keyes, and Bart van Bloemen Waanders. Real-time PDE-constrained Optimization. SIAM, 2007.\n\n- Ziang Chen, Jianfeng Lu, and Yulong Lu. On the representation of solutions to elliptic pdes in barron spaces. In Advances in Neural Information Processing Systems, volume 34, pp. 6454-6465. Curran Associates, Inc., 2021.\n\n- Bernard Dacorogna. Direct methods in the calculus of variations. 2nd ed, volume 78. Springer, New York, 01 2007. ISBN 978-3-642-51442-5. doi: 10.1007/978-3-642-51440-1.\n\n- Michel C. Delfour and Jean-Paul Zol\u00e9sio. *Shapes and geometries*, volume 22. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2011.\n\n- Lars Diening and Frank Ettwein. Fractional estimates for non-differentiable elliptic systems with general growth. Forum Mathematicum, 20(3):523-556, 2008.\n\n- Lars Dieing and Christian Kreuzer. Linear Convergence of an Adaptive Finite Element Method for the  $p$ -Laplacian Equation. SIAM Journal on Numerical Analysis, 46(2):614-638, 2008.\n\n- Lars Dieing and Michael Ruzicka. Interpolation operators in Orlicz-Sobolev spaces. Numer. Math., 107(1):107-129, 2007.\n\n- Lars Diening, Carsten Ebmeyer, and Michael R\u00fc\u017eicka. Optimal convergence for the implicit space-time discretization of parabolic systems with p-structure. SIAM Journal on Numerical Analysis, 45 (2):457-472, 2007.\n\n- Lars Dieening, Petteri Harjulehto, Peter H\u00e4st\u00f6, and Michael R\u00fc\u017eicka. Lebesgue and Sobolev spaces with variable exponents. Springer, Heidelberg, 2011.\n\n- Chenguang Duan, Yuling Jiao, Yanming Lai, Xiliang Lu, Qimeng Quan, and Jerry Zhijian Yang. Analysis of deep ritz methods for laplace equations with dirichlet boundary conditions. arXiv preprint arXiv:2111.02009, 2021.\n\n- Luca Esposito, Carlo Nitsch, and Cristina Trombetti. Best constants in poincar\\`e inequalities for convex domains. Journal of Convex Analysis, 20, 2013.\n\n- Moritz Geist, Philipp Petersen, Mones Raslan, Reinhold Schneider, and Gitta Kutyniok. Numerical Solution of the Parametric Diffusion Equation by Deep Neural Networks. Journal of Scientific Computing, 88(1):1-37, 2021.\n\n- Pierre Grisvard. Elliptic Problems in Nonsmooth Domains. SIAM, 2011.\n\n- Ingo Guhring and Mones Raslan. Approximation rates for neural networks with encodable weights in smoothness spaces. Neural Networks, 134:107-130, 2021.\n\n- Ingo Guhring, Gitta Kutyniok, and Philipp Petersen. Error bounds for approximations with deep ReLU neural networks in  $W^{s,p}$  norms. Analysis and Applications, 18(05):803-859, 2020.\n\n- Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505-8510, 2018.\n\n- Oliver Hennigh, Susheela Narasimhan, Mohammad Amin Nabian, Akshay Subramaniam, Kaustubh Tangsali, Zhiwei Fang, Max Rietmann, Wonmin Byeon, and Sanjay Choudhry. Nvidia simnet\u2122: An ai-accelerated multi-physics simulation framework. In International Conference on Computational Science, pp. 447-461. Springer, 2021.\n\n- Jan Hermann, Zeno Schatzle, and Frank No\u00e9. Deep-neural-network solution of the electronic Schr\u00f6dinger equation. Nature Chemistry, 12(10):891-897, 2020.\n\n- Arnulf Jentzen, Diyora Salimova, and Timo Welti. A proof that deep artificial neural networks overcome the curse of dimensionality in the numerical approximation of Kolmogorov partial differential equations with constant diffusion and nonlinear drift coefficients. Communications in Mathematical Sciences, 19(5):1167-1205, 2021.\n\n- Yuling Jiao, Yanming Lai, Yisu Luo, Yang Wang, and Yunfei Yang. Error Analysis of Deep Ritz Methods for Elliptic Equations. arXiv:2107.14478, 2021.\n\n- Alex Kaltenbach. Theory of Pseudo-Monotone Operators for Unsteady Problems in Variable Exponent Spaces. dissertation, Institute of Applied Mathematics, University of Freiburg, 2021.\n\n- Alex Kaltenbach and Michael Ruzicka. Variable exponent Bochner-Lebesgue spaces with symmetric gradient structure. Journal of Mathematical Analysis and Applications, 503(2), 2021.\n\n- Alex Kaltenbach and Michael R\u00fc\u017eicka. Convergence Analysis of a Local Discontinuous Galerkin Approximation for Nonlinear Systems with Orlicz-Structure. arXiv:2204.09984, 2022.\n\n- Yuehaw Khoo, Jianfeng Lu, and Lexing Ying. Solving parametric PDE problems with artificial neural networks. European Journal of Applied Mathematics, 32(3):421-435, 2021.\n\n- Thomas Koerber. Sharp Estimates for the Principal Eigenvalue of the p-Operator. *Calculus of Variations and Partial Differential Equations*, 57, 2018.\n\n- Gitta Kutyniok, Philipp Petersen, Mones Raslan, and Reinhold Schneider. A Theoretical Analysis of Deep Neural Networks and Parametric PDEs. *Constructive Approximation*, 55(1):73-125, 2022.\n\n- Kookjin Lee and Kevin T Carlberg. Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders. Journal of Computational Physics, 404, 2020.\n\n- Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. International Conference on Learning Representations, 2021.\n\n- Chao Ma, Lei Wu, et al. The barron space and the flow-induced function spaces for neural network models. Constructive Approximation, 55(1):369-406, 2022.\n\n- Johannes M\u00fcller and Marius Zeinhofer. Error Estimates for the Deep Ritz Method with Boundary Penalty. In 3rd Conference on Mathematical and Scientific Machine Learning (MSML2022), 2022.\n\n- Philipp Nagel. Monotone operator theory for unsteady problems on non-cylindrical domains. Ph.d. thesis, Institute of Applied Mathematics, University of Freiburg, 2015.\n\n- Philipp Nagele and Michael R\u00fc\u017e\u00ed\u010dka. Generalized Newtonian fluids in moving domains. J. Differential Equations, 264(2):835-866, 2018.\n\n- Philipp Nagele, Michael Ruzicka, and Daniel Lengeler. Functional setting for unsteady problems in moving domains and applications. Complex Var. Elliptic Equ., 62(1):66-97, 2017.\n\n- Svetoslav Nakov and Ioannis Toulopoulos. Convergence estimates of finite elements for a class of quasilinear elliptic problems. Computers & Mathematics with Applications, 104:87-112, 2021.\n\n- Mario Ohlberger and Stephan Rave. Reduced Basis Methods: Success, Limitations and Future Challenges. Proceedings of the Conference Algorithmy, pp. 1-12, 2016.\n\n- Michael Ruzicka. Nichtlineare Funktionalanalysis. Eine Einf\u00fchrung. Berlin: Springer. xii, 2004.\n\n- Justin Sirignano and Konstantinos Spiliopoulos. DGM: A deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339-1364, 2018.\n\n- E Weinan and Stephan Wojtowytsch. Some observations on high-dimensional partial differential equations with Barron data. In Mathematical and Scientific Machine Learning, pp. 253-269. PMLR, 2022.\n\n- E Weinan, Jiequn Han, and Arnulf Jentzen. Algorithms for Solving High Dimensional PDEs: From Nonlinear Monte Carlo to Machine Learning. Nonlinearity, 35(1):278, 2021.\n\n- Jinchao Xu. The finite Neuron Method and Convergence Analysis. Communications in Computational Physics, 28, 2020.\n\n- Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94: 103-114, 2017.\n\n- Bing Yu and Weinan E. The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm for Solving Variational Problems. Communications in Mathematics and Statistics, 6(1):1-12, 2018.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638640.8506162", "msecs": "850.0", "relativeCreated": "237681.1137199402", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# ConcreTizer: Model Inversion Attack via Occupancy Classification and Dispersion Control for 3D Point Cloud Restoration\n\n# Abstract\nThe growing use of 3D point cloud data in autonomous vehicles (AVs) has raised serious privacy concerns, particularly due to the sensitive information that can be extracted from 3D data. While model inversion attacks have been widely studied in the context of 2D data, their application to 3D point clouds remains largely unexplored. To fill this gap, we present the first in-depth study of model inversion attacks aimed at restoring 3D point cloud scenes. Our analysis reveals the unique challenges, the inherent sparsity of 3D point clouds and the ambiguity between empty and non-empty voxels after voxelization, which are further exacerbated by the dispersion of non-empty voxels across feature extractor layers. To address these challenges, we introduce ConcreTizer, a simple yet effective model inversion attack designed specifically for voxel-based 3D point cloud data. ConcreTizer incorporates Voxel Occupancy Classification to distinguish between empty and non-empty voxels and Dispersion-Controlled Supervision to mitigate non-empty voxel dispersion. Extensive experiments on widely used 3D feature extractors and benchmark datasets, such as KITTI and Waymo, demonstrate that ConcreTizer concretely restores the original 3D point cloud scene from disrupted 3D feature data. Our findings highlight both the vulnerability of 3D data to inversion attacks and the urgent need for robust defense strategies.\n\n\n# 1 INTRODUCTION\n\n\nRecent advancements in Autonomous Vehicles (AVs) have underscored the importance of continuous vision data collection and sharing. At the same time, AI and big data have amplified privacy concerns, prompting increased research on this issue (Guo et al., 2017; Stahl & Wright, 2018). Consequently, AV's data collection faces strict regulations that requires data de-identification (Mulder & Vellinga, 2021). For example, the EU's General Data Protection Regulation (GDPR) (EU, 2016) mandates businesses to adopt stringent data protection protocols.\n\n\nBeyond these regulations, the need for privacy preservation is rapidly increasing, particularly in 3D point cloud data. This is because various types of privacy-related information can be revealed through rich 3D shape information. For instance, personal information can be identified through facial recognition (Zhang et al., 2019) and person re-identification (Cheng & Liu, 2021). Furthermore, behavioral patterns can be exposed through human pose estimation (Zhou et al., 2020) and activity recognition (Singh et al., 2019b). Additionally, by using methods like Simultaneous Localization and Mapping (SLAM) (Kim et al., 2018), location information can also be inferred. Moreover, the fact that 2D images can be reconstructed from sparse 3D data (Pittaluga et al., 2019; Song et al., 2020) emphasizes the importance of securing raw 3D point data from the outset.\n\n\nHowever, research on privacy in 3D point cloud data remains significantly underexplored compared to advancements in the 2D image domain. A prominent research area in 2D image privacy is inversion attacks. While earlier studies (Gupta & Raskar, 2018; Vepakomma et al., 2018; Singh et al., 2019a) suggested that 2D images could be anonymized by extracting features, inversion attacks have demonstrated that these features can be used to restore the original 2D images. In contrast, while there have been a few prior studies on privacy of 3D data (Wang et al., 2024a), inversion attacks for 3D data remain largely unexplored. This research gap allowed a recent study (Hwang et al., 2023) to operate under the assumption that disseminating 3D features inherently prevents the restoration of the original dataset. In the absence of existing inversion attack methods for 3D data, the authors developed a\n\n\n![](images/98ddd585a6d9268199bc5df5eaafbbb0c93b0c5c95a806fe30fe706f65d12c99.jpg)\n\n\n![Figure 1: Restoration result of a 3D Point Clouds. Feature data is extracted from original point cloud through a 3D feature extractor (Yan et al., 2018). ConcreTizer (right) enables restoration with simple modifications to conventional approach (left), and even achieves more concrete restoration than generative model approach (middle) (Xiong et al., 2023). Figure 2: Restoration through conventional inversion attack method. Voxelization introduces zero-padding to non-empty voxels. During downsampling in feature extraction, these voxels spread to neighboring areas, expanding the VoI (green region). Within the VoI, voxel-wise channel regression generates additional points in zero-padding regions, leading to clustering near the origin.](images/9c2e05e4e0f5fc1324ac13b869da41e389b14edc4ffaa7e44416435437d05296.jpg)\n\n\npoint regression method to invert voxel-based backbones, aiming to demonstrate that restoring the original 3D scene from its extracted features is infeasible. As in Figure 1, the conventional point regression in (Hwang et al., 2023) fails to restore 3D point cloud data from intermediate features.\n\n\nHowever, we argue that this failure is not due to an inherent safety of 3D features but rather a lack of careful design that considers the characteristics of 3D backbones. To address this issue, Figure 2 examines the phenomena arising when the point regression method inverts voxel-based feature extractors, which are dominant architectures in autonomous driving applications. The point regression approach attempts to directly restore point coordinates within each voxel by minimizing mean squared error (MSE). However, the method relies on a naive determination of point existence in each voxel, which introduces critical issues. Specifically, during the voxelization process, empty voxels are zero-padded. If a voxel's regression output is exactly  $(0,0,0)$ , it indicates that the zeropadded voxel remains unaffected throughout the forward and inversion processes. These voxels are treated as empty regions and excluded from the MSE calculation. Conversely, if a voxel's output deviates from  $(0,0,0)$ , it is included in the MSE calculation (referred to as a Voxel of Interest (VoI)) and is used to generate a valid point in the restored scene.\n\n\nThe problem is as follows: The sparsity of 3D point cloud data results in a large number of zeropadded voxels. During the forward and inversion processes, particularly in deeper feature extractors, non-empty voxels (VoIs) spread into these empty voxels. This dispersion leads to a proliferation of false VoIs (originally empty voxels), causing point regression to erroneously generate points in regions that were initially void. Moreover, these false VoIs disproportionately impact the MSE loss, prompting the point regression model to bias the restoration by concentrating most points near the origin  $(0,0,0)$  to minimize estimation errors for the false VoIs. This bias significantly degrades localization performance for the relatively smaller number of true VoIs (originally non-empty voxels).\n\n\nThe analysis reveals that the key to a successful inversion attack is not restoring the representation of the voxel (i.e., point coordinates) but accurately determining whether a voxel was originally\n\n\nempty or non-empty. Once this classification is achieved, localizing points within non-empty voxels become more straightforward, as the error is constrained by the voxel size, which is typically small. Based on this insight, we transform the conventional point regression problem into a more explicit Voxel Occupancy Classification (VOC) problem. In addition, the spread of VoI should be suppressed during restoration to minimize the negative impact of false VoIs. To address this, our model incorporates Dispersion-Controlled Supervision (DCS), which segments the feature extractor based on downsampling layers and trains each segment individually, proactively controlling the dispersion of VoI. Thanks to its tailored design, our model, called ConcreTizer, outperforms conditioned generation methods that use generative models (see Figure 1, the generative model approach (Xiong et al., 2023)).\n\n\nTo demonstrate the general applicability of ConcreTizer, we deployed it on two representative 3D feature extractors (Yan et al., 2018; Lu et al., 2022), which are essential components in various applications including 3D object detection (Yan et al., 2018; Shi et al., 2020), 3D semantic segmentation (Wu et al., 2019; Thomas et al., 2019), and tracking (Yin et al., 2021). Our experiments conducted on prominent datasets, KITTI (Geiger et al., 2012) and Waymo (Sun et al., 2020), confirm that ConcreTizer consistently outperforms across various datasets and 3D feature extractors. We showcase the superior performance of ConcreTizer through a range of quantitative and qualitative evaluations, employing point cloud similarity metrics, visual aids, specific-task (3D object detection) performance using restored scenes, and the performance of potential defense mechanisms.\n\n\nThe contributions of this paper are as follows:\n\n- - This work is the first to conduct an in-depth study of model inversion attacks aimed at restoring voxel-based 3D point cloud scenes. Our analysis identifies unique challenges for inversion attacks, which arise from interaction between sparse point cloud data and voxel-based feature extractors.\n\n- - To address these unique challenges, we propose ConcreTizer, tailored for inverting 3D backbone networks, incorporating Voxel Occupancy Classification and Dispersion-Controlled Supervision.\n\n- - Through extensive experiments with representative 3D feature extractors and well-established open-source datasets, we demonstrate the quantitative and qualitative effectiveness of ConcreTizer.\n\n\n# 2 RELATED WORK\n\n\n3D Point Clouds Feature Extraction. Feature extractors for 3D point cloud data encompass set, graph, and grid-based approaches, each distinguished by its representation format. The computational complexity of set and graph-based methods (Qi et al., 2017; Kipf & Welling, 2016) scales significantly with the number of points, limiting their use in real-time applications like autonomous driving. Conversely, grid-based methods (Zhou & Tuzel, 2018; Yan et al., 2018; Shi et al., 2020; Lang et al., 2019; Sun et al., 2022) organize the 3D space into a grid and apply sparse convolution (Liu et al., 2015; Graham & Van der Maaten, 2017) for efficient feature extraction from sparse voxel data. These methods are particularly advantageous for autonomous driving applications due to their efficiency with sparse data. Considering these characteristics, we explore inversion attacks tailored to scenarios using grid-based feature extractors.\n\n\nModel Inversion. Model inversion was initially studied from the perspective of interpretability in deep learning models. Traditional approaches generate saliency maps to understand how models produce outputs (Du et al., 2018). Other methods (Mahendran & Vedaldi, 2015; Dosovitskiy & Brox, 2016b; a) reconstruct the input from intermediate features to analyze the information flow through model layers. Recently, with growing concerns about data privacy, model inversion has gained attention as a privacy attack. Early studies attempted to restore input face images from confidence scores (Yang et al., 2019b). Subsequent studies (Zhang et al., 2020; Zhao et al., 2021) leverage additional information for more sophisticated restoration. Based on these studies, corresponding defense techniques (Liu et al., 2019; Xue et al., 2023; Dusmanu et al., 2021; Ng et al., 2022; Zhang et al., 2022) have also been investigated, enriching the exploration of data privacy. However, previous work has primarily focused on 2D image data. There is a clear need for an inversion attack technique that accounts for the unique characteristics of 3D point cloud data in autonomous driving. To the best of our knowledge, this research is the first to study inversion attacks for 3D data.\n\n\nPoint Cloud Generation. Generative models, with their ability to produce plausible raw data, are widely used for various tasks. In the 3D point cloud domain, diverse generative models are being researched. Unconditional generation tasks create plausible shapes from random inputs like\n\n\n![](images/a591e2b4c5f946536b778755ac60bf1f9444a47b9911906bfdc8ce21b73f7921.jpg)\n\n\n![Figure 3: (Left) The results of the conventional inversion attack: As the layer depth increases, the number of restored points increases rapidly, and the concentration of points near the origin becomes more noticeable. (Right) The VoI (Voxel of Interest) dispersion effect: The non-empty voxels spread as they pass through the feature extractor and inversion attack model.](images/36e144b470ac5bc92c8e213706f5f4cfb4cef8c11d84eef87bffbf7591a9327a.jpg)\n\n\n![](images/5b685f4bc5fccefc7930358ce56cf1ad8ec48cc93548592724d6f2738342b8db.jpg)\n\n\n![](images/eb4d214d1cf30b6c7ca8eeec6d750d84b8e0e70456d980f187dde64437d08575.jpg)\n\n\nnoise (Achlioptas et al., 2018; Valsesia et al., 2018; Shu et al., 2019; Yang et al., 2019a; Luo & Hu, 2021). Conditional generation tasks involve generating the remaining part from a partial point cloud (Yu et al., 2021; Huang et al., 2020; Wen et al., 2020) or generating a 3D point cloud from a 2D image (Mandikal et al., 2018; Mandikal & Radhakrishnan, 2019; Melas-Kyriazi et al., 2023). However, most existing research focuses only on dense point cloud data for single objects (e.g., Chang et al. (2015)). In contrast, there is little research on handling scene-level sparse point clouds captured from autonomous vehicles. Only a few studies (Caccia et al., 2019; Zyrianov et al., 2022) deal with scene-level sparse point clouds. However, even these studies require specific representation formats and do not support using 3D grid-shaped features, given in our inversion attack scenario, as conditions. To our knowledge, the only scene-level sparse point cloud generation model based on 3D grid representations is Xiong et al. (2023). We also conducted performance comparisons with conditional generation approach using this generative model.\n\n\n# 3 PRELIMINARY: LIMITATIONS OF CONVENTIONAL INVERSION ATTACK\n\n\nThe only known attempt at an inversion attack on 3D point cloud data is by Hwang et al. (2023). Even this research does not directly focus on inversion attacks but rather seeks to assess the privacy protection effectiveness of 3D features by developing a sample inversion attack based on point regression. Before designing our method, we explore why the conventional approach can not effectively restore 3d point cloud scenes (Figure 3, left).\n\n\nFirstly, we identified an issue in voxel-based models related to the voxelization process. During voxelization, regions without points are zero-padded. However, the conventional regression method does not consider point existence but focus solely on point localization, mistakenly interpreting zero-padded representations as valid points located at  $(0, 0, 0)$ . As a result, points are created even for empty voxels, leading to an overgeneration of points compared to the original data. Secondly, the inherently sparse nature of point clouds results in a large number of zero-padded voxels, far exceeding those containing valid points. Point regression-based inversion attacks naturally prioritize minimizing estimation errors across all voxels, including these zero-padded regions. Consequently, this bias towards zero-padded voxels causes an over-concentration of points near the origin in the restored scene. Moreover, it significantly increases localization errors for the relatively smaller number of valid points, as these errors are considered negligible within the overall regression error.\n\n\nLastly, we observed that as the feature extractor layers deepen, existing attack methods are increasingly hindered by the negative impact of zero-padded voxels: (1) an excessive number of restored points and (2) an intensified concentration of points near the origin. Specifically, if voxels with a value of (0, 0, 0) persist in the final restored state, they are excluded from the regression targets and do not directly affect the regression loss. However, due to the nature of convolution operations, the values of non-empty voxels\u2014defined as Voxels of Interest (VoI)\u2014gradually disperse into the surrounding empty voxels. As the layers deepen, more originally zero-padded voxels become non-empty during the feature extraction and inversion processes. Consequently, an increasing number of these zero-padded voxels are included in the regression targets. Our experiments revealed that the density of VoI spikes significantly at downsampling layers (Figure 3, right), further amplifying the influence of zero-padded voxels on the final restoration results.\n\n\n![ConcreTizer Attacker Scenario ConcreTizer Training Operation](images/ffda6a038cf254595206f61f2e941f2a99c8b8eb00e5ed395309f2c9765c2073.jpg)\n\n\n![Voxel Pointness Classification Figure 4: ConcreTizer framework. Original point cloud and feature data are represented by  $p$  and  $f_{i}$ , while their restored counterparts are denoted by  $p'$  and  $f_{i}'$ , respectively. The value  $i$  means that it is obtained from the i-th downsampling layer. ConcreTizer restores data by classifying the occupancy of  $f_{0}$  and then placing points at the center of each restored voxel. For deeper layers, it employs partitioning at the downsampling layer and restores  $f_{i-1}$  from  $f_{i}$  for each block.](images/e5553b096cc4aeffa8811c63c6dd0eab79c892a178cc049576d4c586ab214c4a.jpg)\n\n\n![Diffusion-based Supervision](images/ca1afe78c4f6de16e4bd786dc216edd005f62413b3483f8268debf0fdcd1c329.jpg)\n\n\n# 4 PROPOSED METHOD\n\n\n# 4.1 AVSCENARIO\n\n\nWe focus on autonomous vehicle (AV) scenarios because they carry a high risk of exposure to inversion attacks. In AV contexts, there is a need to share feature data for purposes such as computation offloading (Xiao et al., 2022; Hanyao et al., 2021), model enhancement (Hwang et al., 2023), and cooperative perception (Wang et al., 2020; Xu et al., 2022; Yu et al., 2022). Specifically, we selected voxel-based feature extractors as the target for inversion attacks. Voxel-based feature extractors are commonly used in autonomous vehicles due to their suitability for real-time applications like 3D object detection (Yan et al., 2018; Lang et al., 2019; Shi et al., 2019; 2020; Shi & Rajkumar, 2020), semantic segmentation (Wu et al., 2019; Thomas et al., 2019), and tracking (Yin et al., 2021). In this scenario, an attacker with access to the same feature extractor would need to prepare 3D point cloud data for training the inversion attack model. Since the restoration task doesn't require separate labeling, training can be conducted using open-source datasets or self-collected data.\n\n\n# 4.2 PROBLEM DEFINITION\n\n\nThe goal of an inversion attack is to discover the inverse process of a given feature extractor. For voxel-based feature extractors, the initial step involves a voxelization process that transforms point cloud data into a grid format.\n\n\nVoxelization converts a 3D point cloud  $p \\in \\mathbb{R}^{k \\times 3}$ , where  $k$  is the number of points, into a voxel grid  $f_0 \\in \\mathbb{R}^{3 \\times H \\times W \\times D}$ , where  $H, W, D$  represent the spatial dimensions of the grid. Here  $x, y,$  and  $z$  coordinate information is channelized and voxels without points are zero-padded, resulting in channel values of  $(0, 0, 0)$ . In particular, during the downsampling process, the spatial dimensions shrink while the channel size increases, producing features  $f_N \\in \\mathbb{R}^{C_N \\times h_N \\times w_N \\times d_N}$ , where  $N$  is the number of downsampling layers,  $C_N > 3$ , and  $h_N, w_N, d_N$  are smaller than  $H, W, D$ . Consequently, our inversion attack aims to restore the original voxel grid  $f_0$  from the downsampled features  $f_N$ .\n\n\n# 4.3 CONCRETIZER FRAMEWORK\n\n\nFigure 4 depicts the overall ConcreTizer framework incorporating the scenario and attacker-side training operations. In designing the structure of the inversion attack model, we chose a symmetrical design with the feature extractor, following previous studies (Yang et al., 2019b; Zhang et al., 2020; Zhao et al., 2021). In other words, original shape can be restored by performing upsampling at the positions where downsampling occurs (detailed structure in the supplementary material). Building upon symmetric structure, ConcreTizer applies Voxel Occupancy Classification (VOC) and\n\n\nDispersion-Controlled Supervision (DCS) to overcome the limitations of traditional inversion attack. VOC transforms the regression problem into a classification problem to address the issue of point clustering near the origin. DCS prevents the dispersion of VoI by splitting the feature extractor, mitigating the degradation of restoration performance as the network deepens.\n\n\n# 4.3.1 VOXEL OCCUPANCY CLASSIFICATION\n\n\nIn traditional inversion attack methods, the original data is directly restored through regression on channel values. In our scenario, since the x, y, and z coordinates are channelized during the voxelization process, performing regression would restore coordinate values. However, since voxelization of sparse point clouds produces a large number of zero-padded voxels with  $(0, 0, 0)$  channel value, many unnecessary points cluster near the origin in the inversion attack results (Figure 3, left). To address this issue, we transform the regression problem into a classification problem to resolve the semantic ambiguity of zero-padded voxels\u2014whether they represent empty voxels or valid points at coordinates  $(0, 0, 0)$ . This can be achieved through simple binary encoding, where each voxel is labeled as 0 (negative occupancy) or 1 (positive occupancy), making the meaning of zero-padding clear. Using the VOC method, the inversion attack model outputs binary classification scores in the form of  $\\mathbb{R}^{1 \\times H \\times W \\times D}$ , rather than continuous coordinate values in the form of  $\\mathbb{R}^{3 \\times H \\times W \\times D}$ . If a voxel is determined to contain a point, the corresponding coordinate can be restored easily. This is because the range of coordinate values is bounded by the spatial location of the voxel, and the voxel size is typically small enough. As a result, by using the center coordinates of the voxel, we achieve effective restoration within an error range constrained by the voxel size.\n\n\nAdditionally, due to the sparsity of original point cloud data, the binary-encoded labels have a higher ratio of 0s compared to 1s. This phenomenon is particularly exacerbated as the depth of the layers increases. Let  $f_{0}$  be the original voxelized point cloud and  $f_{0}^{\\prime}$  be the restored one by the inversion attack. The number of positive labels is fixed as  $|\\mathrm{VoI}$  of  $f_{0}|$ , while the number of negative labels,  $|\\mathrm{VoI}$  of  $f_{0}^{\\prime}| - |\\mathrm{VoI}$  of  $f_{0}|$ , increases exponentially as the depth of the layer increases. To account for this imbalance, we apply the Sigmoid Focal (SF) loss (Lin et al., 2017), a variant of the conventional cross-entropy loss. The mathematical representation of the SF loss is given by:\n\n\n$$\n\n$$\n\\operatorname {F L} \\left(p _ {t}\\right) = - \\alpha_ {t} \\left(1 - p _ {t}\\right) ^ {\\gamma} \\log \\left(p _ {t}\\right), \\tag {1}\n$$\n\n$$\n\n\nwhere  $p_t$  denotes the model's predicted probability for the target class. The factor  $\\alpha_t$  is employed to adjust the importance given to the positive and negative classes.\n\n\n# 4.3.2 DISPERSION-CONTROLLED SUPERVISION\n\n\nWhile applying SF loss in VOC can partially address the label imbalance issue, it cannot prevent the problem of VoI dispersion. The original data is sparse with many empty voxels. However, as we observed earlier, the VoI density exponentially increases during the downsampling process (Figure 3, right). Therefore, after VoI spreads too much at the deep layer, it becomes difficult to restore the data back to the original sparse state.\n\n\nOur proposed DCS is a more fundamental solution to address VoI dispersion. It divides the feature extractor into multiple blocks and performs restoration progressively. First, feature extractor can be partitioned based on the downsampling layer where the VoI spread occurs. Through this, the VoI dispersion in each block can be effectively controlled. Then, in the inversion attack model, an inversion block corresponding to each block can be created to train the restoration in block units. Note that at the original voxel level, the channel values represent point coordinates, eliminating the need for regression (if classification result is positive, channel value is estimated as the center coordinate of the voxel). However, at the intermediate feature level, normalization is applied, so both classification and regression on the channel values are required.\n\n\nFor example, if the input of the  $(i + 1)$ -th block is  $f_{i} \\in \\mathbb{R}^{C_{i} \\times h_{i} \\times w_{i} \\times d_{i}}$  and the output is  $f_{i + 1} \\in \\mathbb{R}^{C_{i + 1} \\times h_{i + 1} \\times w_{i + 1} \\times d_{i + 1}}$ , then the  $(i + 1)$ -th inversion block in the inversion attack model takes  $f_{i + 1}$  as input and produces  $f_{i}' \\in \\mathbb{R}^{C_{i} \\times h_{i} \\times w_{i} \\times d_{i}}$ , which is the result of restoring  $f_{i}$ . Specifically,  $m_{i}' \\in \\mathbb{R}^{1 \\times h_{i} \\times w_{i} \\times d_{i}}$  (spatial occupancy scores found by applying SF loss) and  $c_{i}' \\in \\mathbb{R}^{C_{i} \\times h_{i} \\times w_{i} \\times d_{i}}$  (channel values found by applying L2 loss) are derived from  $f_{i + 1}$ . Then,  $c_{i}'$  is masked by using  $m_{i}'$  to generate  $f_{i}'$ . In the masking process, unnecessary VoI values are erased, so the dispersion of VoI can be suppressed. Note that in the first inversion block, which is the final stage of the inversion attack model, only classification is performed, with no additional regression. The loss function for each inversion block is:\n\n\nTable 1: Inversion attack result with KITTI and Waymo dataset. Average CD and HD values in centimeters, and F1 scores with  ${15}\\mathrm{\\;{cm}}$  and  ${30}\\mathrm{\\;{cm}}$  thresholds for KITTI and Waymo datasets. Metrics evaluate over two scene sets with 3769 and 3999 scenes, respectively.\n\n\n<table><tr><td rowspan=\"2\" colspan=\"2\">#Downsampling (LayerDepth)</td><td colspan=\"3\">1 (3rd)</td><td colspan=\"3\">2 (6th)</td><td colspan=\"3\">3 (9th)</td><td colspan=\"3\">4 (12th)</td></tr><tr><td>CD (\u2193)</td><td>HD (\u2193)</td><td>F1score (\u2191)</td><td>CD (\u2193)</td><td>HD (\u2193)</td><td>F1score (\u2191)</td><td>CD (\u2193)</td><td>HD (\u2193)</td><td>F1score (\u2191)</td><td>CD (\u2193)</td><td>HD (\u2193)</td><td>F1score (\u2191)</td></tr><tr><td rowspan=\"3\">KITTI</td><td>Point Regression</td><td>1.3868</td><td>23.5855</td><td>0.3543</td><td>1.2879</td><td>34.2395</td><td>0.3904</td><td>3.1229</td><td>54.0173</td><td>0.2110</td><td>4.1439</td><td>56.9811</td><td>0.1298</td></tr><tr><td>UltraLiDAR</td><td>0.0744</td><td>8.2269</td><td>0.9122</td><td>0.0818</td><td>8.0974</td><td>0.8905</td><td>0.0836</td><td>7.9561</td><td>0.8869</td><td>0.1012</td><td>7.9185</td><td>0.8152</td></tr><tr><td>ConcreTizer</td><td>0.0321</td><td>7.5603</td><td>0.9918</td><td>0.0373</td><td>7.5249</td><td>0.9914</td><td>0.0507</td><td>7.8453</td><td>0.9793</td><td>0.0776</td><td>8.1193</td><td>0.9160</td></tr><tr><td rowspan=\"3\">Waymo</td><td>Point Regression</td><td>1.4979</td><td>55.6589</td><td>0.7644</td><td>2.7733</td><td>66.7899</td><td>0.6489</td><td>4.1053</td><td>70.6608</td><td>0.5524</td><td>4.9340</td><td>71.9608</td><td>0.4355</td></tr><tr><td>UltraLiDAR</td><td>0.0810</td><td>10.9582</td><td>0.9742</td><td>0.0898</td><td>11.3360</td><td>0.9623</td><td>0.1017</td><td>11.4987</td><td>0.9503</td><td>0.1378</td><td>12.0259</td><td>0.8849</td></tr><tr><td>ConcreTizer</td><td>0.0374</td><td>10.2544</td><td>0.9984</td><td>0.0466</td><td>10.2326</td><td>0.9979</td><td>0.0712</td><td>10.5724</td><td>0.9781</td><td>0.1087</td><td>11.3399</td><td>0.9251</td></tr></table>\n\n\n$$\n\n$$\nL o s s (\\text {i n v e r s i o n b l o c k} i + 1) = \\left\\{ \\begin{array}{l l} L _ {\\text {c l s}} & \\text {i f} i = 0, \\\\ L _ {\\text {c l s}} + \\beta \\cdot L _ {\\text {r e g}} & \\text {i f} i \\geq 1. \\end{array} \\right.\n$$\n\n$$\n\n\n$$\n\n$$\nL _ {\\mathrm {c l s}} = \\sum \\operatorname {S F} \\left(m _ {i}, m _ {i} ^ {\\prime}\\right) \\quad \\text {a n d} \\quad L _ {\\mathrm {r e g}} = \\sum \\operatorname {L 2} \\left(c _ {i}, c _ {i} ^ {\\prime}\\right)\n$$\n\n$$\n\n\nwhere the summation is taken over VoI of the output.\n\n\n$$\n\n$$\n\\left( \\begin{array}{c} m _ {i}: \\text {G r o u n d T r u t h s p a t i a l o c c u p a n c y m a s k} \\\\ m _ {i} ^ {\\prime}: \\text {P r e d i c t e d s p a t i a l o c c u p a n c y s c o r e s} \\\\ c _ {i}: \\text {G r o u n d T r u t h c h a n n e l v a l u e s} \\\\ c _ {i} ^ {\\prime}: \\text {P r e d i c t e d c h a n n e l v a l u e s} \\end{array} \\right)\n$$\n\n$$\n\n\nThe final result of passing through all inversion blocks is a binary classification scores in the form of  $\\mathbb{R}^{1\\times H\\times W\\times D}$ , and the restoration is completed by generating a point at the center of the voxel corresponding to the positive occupancy.\n\n\n# 5 EXPERIMENTS\n\n\n# 5.1 EXPERIMENTAL SETUP\n\n\n3D Feature Extractor. We employ a voxelization-based 3D feature extractor as the target of our inversion attack. Based on the OpenPCDet (Team, 2020) project, we utilize pre-trained Voxel-Backbone (Yan et al., 2018) and VoxelResBackbone (Lu et al., 2022), extensively used in a crucial application area for 3D point cloud data. The VoxelBackBone structure contains four downsampling layers (i.e.,  $N = 4$ ), each preceded by two convolutional layers, while the VoxelResBackbone incorporates additional convolutional layers and skip connections.\n\n\nInversion Model Training. We train the inversion attack model on the real-world KITTI (Geiger et al., 2012) and Waymo (Sun et al., 2020) datasets. When using the SF loss function in VOC, only the alpha value in the hyperparameters is adjusted, and in DCS, the weight on regression loss,  $\\beta$ , is set to 1.\n\n\nMetrics. In our analysis of 3D scene restoration performance, we utilize various metrics. First, for qualitative analysis, we visualize the 3D point cloud using KITTI viewer web tool. Next, for quantitative analysis, we use point cloud similarity metrics such as Chamfer Distance (CD) (Borgefors, 1984), Hausdorff Distance (HsD) (Huttenlocher et al., 1993), and F1 Score (Goutte & Gaussier, 2005). Furthermore, to verify the utility of the restored data, we also check the 3D object detection accuracy with pre-trained detection models.\n\n\n# 5.2 RESTORATION PERFORMANCE\n\n\nComparison Schemes. We conducted comparisons in various ways to confirm the superiority of ConcreTizer. We start with Point Regression (Mahendran & Vedaldi, 2015; Dosovitskiy & Brox, 2016a;b), a traditional inversion attack method. In Point Regression, post-processing is applied to eliminate points that fell outside the defined point cloud range or were excessively concentrated near the origin. Next we compare ConcreTizer with a generative model approach. To perform an inversion attack using a generative model, a conditional generation with feature data as a condition must be employed. Among the existing LiDAR point cloud generation models, UltraLiDAR (Xiong et al., 2023) is the only model based on voxel representation, similar to our feature extractor. We modified the encoder part of UltraLiDAR to accept voxel features as an input and then trained it with the KITTI and Waymo datasets.\n\n\n![Figure 5: Qualitative results for KITTI (scene 73) and Waymo (scene 79). Top shows the original point cloud, 2D image, and highlighted region. Below, restoration performance of three techniques is displayed, progressing left to right by layer depth.](images/028a091c870ca22cfde6b9f702d6375f0a2e05ca8d3a559f6725d1ef18dd6783.jpg)\n\n\n![Figure 6: Restoration result on VoxelResBackbone with KITTI dataset. At the left, the last layer's restoration performance for three techniques is shown. At the right, average performance across the KITTI dataset is presented. A bar graph depicts relative performance, and a table details raw values.](images/b9a66c4ca340b3a1628a4a11978313418cea3c83de56efa352db806803432cee.jpg)\n\n\n![](images/95a12d4f3740bb65cfa73eb679ccfe4994f3f526d1b7ea234ca480e0c0e61aba.jpg)\n\n\n<table><tr><td rowspan=\"2\">#Downsampling (LayerDepth)</td><td colspan=\"4\">4 (12th)</td></tr><tr><td>CD (\u2193)</td><td>HD (\u2193)</td><td>Fscore (\u2191)</td><td>AP3D (\u2191)</td></tr><tr><td>Point Regression</td><td>3.7381</td><td>55.6439</td><td>0.1483</td><td>0</td></tr><tr><td>UltraLiDar</td><td>0.0967</td><td>9.8154</td><td>0.8388</td><td>57.73</td></tr><tr><td>ConcreTizer</td><td>0.0714</td><td>9.5625</td><td>0.9350</td><td>63.67</td></tr></table>\n\n\nResult Analysis. Table 1 presents the point scene restoration performance at different depths of VoxelBackBone (Yan et al., 2018), utilizing point cloud similarity metrics, while Figure 5 illustrates the corresponding restored point cloud scenes. It is evident that ConcreTizer consistently demonstrates exceptional performance across all cases within both the KITTI and Waymo datasets.\n\n\nIt can be observed that traditional Point Regression methods are not feasible for performing inversion attacks on 3D features. In particular, the results show that numerous points cluster near the origin, and this phenomenon intensifies as the depth of the layers increases. This can be understood as a failure to take into account the characteristics inherent to 3D sparse features.\n\n\nBy employing a conditional generation approach, UltraLiDAR can restore the overall scene in a coarse-grained manner with less significant performance degradation in terms of the HD metric relative to layer depth. This rough recovery can be attributed to the conversion of 3D sparse features into 2D dense features, aligning with the 2D VQ-VAE design. In the context of 2D dense features, problems like VoI dispersion are no longer present, which contributes to better robustness. However, this transition results in the loss of 3D sparse characteristics, leading to less accurate restoration of finer details.\n\n\nIn contrast, our ConcreTizer effectively suppresses VoI dispersion through DCS while preserving the sparse characteristics of 3D features. As a result, despite its simple design, ConcreTizer achieves a more concrete restoration than the generative model approach. At the deepest layer, ConcreTizer outperforms the generative approach by  $23.4\\%$  and  $12.4\\%$  on KITTI, while on Waymo, it shows improvements of  $21.1\\%$  and  $4.5\\%$  in CD and F1 score, respectively.\n\n\nIn Figure 6, we also tested ConcreTizer to the VoxelResBackbone (Lu et al., 2022). In this case, when analyzing the representative results from the deepest layer, ConcreTizer exhibits the highest of performance. A persistent issue with UltraLiDAR is the lack of detailed shape in the restored scenes. Detailed experimental results, including those conducted with the VoxelResBackbone and the Waymo dataset, are provided in the supplementary materials.\n\n\nTable 2: 3D object detection results with KITTI and Waymo datasets. The reported metric for the KITTI dataset is Average Precision (AP) at hard difficulty, while for the Waymo dataset, Average Precision weighted by Heading (APH) is reported at LEVEL2 difficulty.\n\n\n<table><tr><td colspan=\"2\">Detection Model</td><td>PointPillar</td><td>PVRCNN</td><td>VoxelRCNN</td><td>PointRCNN</td></tr><tr><td rowspan=\"4\">KITTI</td><td>Original Data</td><td>76.11</td><td>78.82</td><td>78.78</td><td>78.25</td></tr><tr><td>Point Regression</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>UltraLiDAR</td><td>58.32</td><td>56.08</td><td>59.00</td><td>54.19</td></tr><tr><td>ConcreTizer</td><td>65.97</td><td>59.42</td><td>64.46</td><td>63.62</td></tr></table>\n\n\nFigure 7: Ablation study on VoxelBackbone with KITTI dataset. At the left, the restoration performance for three cases is shown. At the right, average performance across the KITTI dataset is presented with boxplot.\n\n\n# 5.3 ATTACK PERFORMANCE IN THE CONTEXT OF 3D OBJECT DETECTION\n\n\nTo assess the effectiveness of the restored scenes in compromising privacy, we measured the 3D object detection accuracy using restored point cloud scenes with separately trained object detection models. Table 2 displays the benchmark results for the KITTI and Waymo datasets. Point Regression failed to restore meaningful data, producing entirely unusable results. In addition, UltraLiDAR performed relatively well on KITTI but showed very poor performance on Waymo, which involves a broader range and higher scene complexity. This indicates that while generative models can restore overall shapes, they struggle to restore detailed features. Only ConcreTizer demonstrates a consistent performance across both datasets, achieving 75.4 to  $86.7\\%$  and 62.6 to  $75.7\\%$  of the detection performance based on original scenes in KITTI and Waymo, respectively.\n\n\n# 5.4 ABLATION STUDY: COMPONENT-WISE ANALYSIS\n\n\nTo understand the performance of ConcreTizer, we examine the impact of each component. Figure 7 illustrates the comparative performance of VOC (BCE loss), VOC, and ConcreTizer (VOC+DCS). Firstly, VOC (BCE loss) reveals that shifting from regression to classification, thus clarifying the meaning of zero-padded voxels, made restoration possible (6th layer result). However, the BCE loss struggles to handle the significant label imbalance as the layer depth increases. By comparing VOC (BCE loss) with VOC, it becomes evident that using SF loss helps mitigate the label imbalance issue to some extent. Nonetheless, in the results of VOC, the restored points tend to cluster in specific areas, resulting in biased restoration (12th layer result). Only ConcreTizer successfully restores points with a distribution closely matching the original point cloud. This effectiveness stems from DCS's ability to efficiently mitigate the dispersion of VoI that arises with deeper layer.\n\n\n# 5.5 PARTITIONING POLICY IN DISPERSION-CONTROLLED SUPERVISION\n\n\nWe conduct experiments to determine the effective strategy for applying DCS in ConcreTizer, given a specific 3D feature extractor. Restoration performance is evaluated on the KITTI dataset by varying the number of DCS instances (i.e., the number of inversion blocks). In each case, partitioning is applied at positions that aim to achieve an even division of the total number of layers. As shown in Figure 8, applying 10 DCS instances leads to significantly worse performance than not using DCS at all (i.e., DCS 1). This is because the restoration error of each block accumulates as it goes through multiple blocks. The best performance is achieved with 2, 3, or 4 DCS instances, where each partitioned block contains at least one downsampling layer. This is attributed to the additional supervision effectively suppressing VoI dispersion that occurs during the downsampling process. Therefore, to maximize the benefits of supervision, partitioning should align with the downsampling layers, where VoI dispersion manifests. Qualitative results for different DCS instances and further discussion on the optimal DCS split position are provided in the supplementary materials.\n\n\n![](images/9732d94fa7f18f97265ea723d28fa9cbb8aa4d3c975b8c314153832cfbb12d3f.jpg)\n\n\n![Number of DCS instances](images/170107c2092c5797f115485631a6c90b4ad76f08c5ee18290a36a35a140f58ab.jpg)\n\n\n![](images/d59d4cdb293e8deb4e733ca3a364ff57cb308c9324beb7a54be2e7d65cc80539.jpg)\n\n\nTable 3: Effect of point cloud augmentation. Measured:  $\\mathrm{AP}_{3D}$  (detection accuracy) of the SECOND model and CD (restoration error) of ConcreTizer.\n\n\n<table><tr><td>Rotation (\u00b0)</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>AP</td><td>81.77</td><td>38.38</td><td>17.08</td><td>12.10</td><td>6.10</td><td>2.78</td></tr><tr><td>CD</td><td>0.0776</td><td>0.1142</td><td>0.1728</td><td>0.2310</td><td>0.2848</td><td>0.3344</td></tr><tr><td colspan=\"7\"></td></tr><tr><td>Scaling (%)</td><td>0</td><td>2</td><td>4</td><td>6</td><td>8</td><td>10</td></tr><tr><td>AP</td><td>81.77</td><td>54.12</td><td>24.09</td><td>11.47</td><td>8.88</td><td>5.26</td></tr><tr><td>CD</td><td>0.0776</td><td>0.1468</td><td>0.2253</td><td>0.2780</td><td>0.3179</td><td>0.3516</td></tr><tr><td colspan=\"7\"></td></tr><tr><td>Sampling (%)</td><td>100</td><td>25</td><td>20</td><td>15</td><td>10</td><td>5</td></tr><tr><td>AP</td><td>81.77</td><td>63.35</td><td>58.32</td><td>52.71</td><td>40.31</td><td>24.59</td></tr><tr><td>CD</td><td>0.0776</td><td>0.1368</td><td>0.1516</td><td>0.1717</td><td>0.2034</td><td>0.2789</td></tr></table>\n\n\n![Figure 8: Effect of the number of DCS instances. DCS 1 is the end-to-end approach without partitioning. DCS 2, 3, and 4 use downsampling-based partitioning with 2 or 1 downsampling layers per block. DCS 10 partitions at every layer.](images/8d636f4c852f4b1b747334961989e91138aa7f4a08f81bf494c6674db1f3a9d7.jpg)\n\n\n![](images/a97aaa32094ec58b7a401d10487717a845f06eda80d025f678f5eb41d6222e6a.jpg)\n\n\n![](images/1a6d6163bd44be46b57fff3b54192a20e33320a8cff2a4e772ff4451626c1145.jpg)\n\n\n![(a) Distributed Noise](images/500be519e079e3b25b8444568e8bb28fcd3a06e3432ca859d2b119de4821c08c.jpg)\n\n\n![(b) Feature-centric Noise (c) Empty-centric Noise](images/1717ca67692c6fdeb2a9bf70fad1a64c2ea733344840a667e67351ee1210ab7c.jpg)\n\n\n![Figure 9: Effect of Gaussian noise. Measured:  $\\mathrm{AP}_{3D}$  (detection accuracy) of the SECOND model and CD (restoration error) of ConcreTizer.](images/d6b7513add6a33b935cfcb248e39a83b44f1ce27aa35533b268d7bf931b46142.jpg)\n\n\n# 5.6 TRADEOFF BETWEEN PRIVACY AND UTILITY\n\n\nTo understand the trade-off between utility (3D object detection accuracy) and privacy protection (restoration error), we explored various data perturbation techniques as potential defense mechanisms against our ConcreTizer inversion attack (Wang et al., 2024b; Li et al., 2021; Wang et al., 2024a). We investigated two types of perturbations: point cloud augmentations and Gaussian noise addition. For point cloud augmentations, we applied random rotations within a range of  $-a$  to  $+a$  degrees ( $a = 1$ , 2, 3, 4, 5), random scaling within  $-s\\%$  to  $+s\\%$  ( $s = 2$ , 4, 6, 8, 10), and random sampling where  $r\\%$  of points were randomly selected ( $r = 25$ , 20, 15, 10, 5). For Gaussian noise addition, we introduced noise at the feature data level with three region-specific configurations: distributed noise, which is uniformly applied across all feature data regions; feature-centric noise, applied only to VoI containing information; and empty-centric noise, exclusively targeting empty region.\n\n\nThe results, as shown in Table 3 and Figure 9, indicate that while these data perturbation techniques can reduce the restoration capability (defense), they also lead to a degradation in object detection performance (target task). This trade-off highlights the significant challenge of mitigating ConcreteTizer attacks without severely compromising the system's utility. Notably, in Figure 9 we observed that the sparse nature of 3D feature data causes noise to impact different regions unevenly. This finding underscores the importance of considering such spatial characteristics when designing future defense mechanisms. More visualization results are provided in the supplementary materials. We also discuss more about potential defense mechanisms in the supplementary materials.\n\n\n# 6 CONCLUSION\n\n\nThis paper presents the first comprehensive study on model inversion for 3D point cloud restoration. In the context of autonomous driving, we focus on the most dominant voxel-based feature extractors and examine the challenges arising from their interaction with 3D point cloud characteristics. Based on this, we introduced ConcreteTizer, a simple yet effective inversion technique tailored for restoring 3D point data from features, which incorporates Voxel Occupancy Classification and Dispersion-Controlled Supervision. Through rigorous evaluations using prominent open-source datasets such as KITTI and Waymo, along with representative 3D feature extractors, we not only demonstrated the superiority of ConcreteTizer but also analyzed each of its components in detail for valuable insights. Our research reveals the vulnerability of 3D point cloud data to inversion attacks, emphasizing the urgent need to devise extensive defense strategies. While this work focuses on voxel-based representations, we see inversions attacks for more diverse representations of 3D data, such as point set, graph, and projection, as valuable future work.\n\n\n# REFERENCES\n\n- Panos Achlioptas, Olga Diamanti, Ioannis Mitlagkas, and Leonidas Guibas. Learning representations and generative models for 3d point clouds. In International conference on machine learning, pp. 40-49. PMLR, 2018.\n\n- Gunilla Borgefors. Distance transformations in arbitrary dimensions. Computer vision, graphics, and image processing, 27(3):321-345, 1984.\n\n- Lucas Caccia, Herke Van Hoof, Aaron Courville, and Joelle Pineau. Deep generative modeling of lidar data. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5034-5040. IEEE, 2019.\n\n- Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\n\n- Yuwei Cheng and Yimin Liu. Person reidentification based on automotive radar point clouds. IEEE Transactions on Geoscience and Remote Sensing, 60:1-13, 2021.\n\n- Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016a.\n\n- Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016b. URL https://proceedings.neurips.cc/paper/2016/file/371bce7dc83817b7893bcdeed13799b5-Paper.pdf.\n\n- Mengnan Du, Ninghao Liu, Qingquan Song, and Xia Hu. Towards explanation of dnn-based prediction with guided feature inversion. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1358-1367, 2018.\n\n- Mihai Dusmanu, Johannes L Schonberger, Sudipta N Sinha, and Marc Pollefeys. Privacy-preserving image features via adversarial affine subspace embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14267-14277, 2021.\n\n- GDPR EU. General data protection regulation, 2016.\n\n- Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pp. 3354-3361. IEEE, 2012.\n\n- Cyril Goutte and Eric Gaussier. A probabilistic interpretation of precision, recall and f-score, with implication for evaluation. In David E. Losada and Juan M. Fern\u00e1ndez-Luna (eds.), Advances in Information Retrieval, pp. 345-359, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg. ISBN 978-3-540-31865-1.\n\n- Benjamin Graham and Laurens Van der Maaten. Submanifold sparse convolutional networks. arXiv preprint arXiv:1706.01307, 2017.\n\n- Longhua Guo, Mianxiong Dong, Kaoru Ota, Qiang Li, Tianpeng Ye, Jun Wu, and Jianhua Li. A secure mechanism for big data collection in large scale internet of vehicle. IEEE Internet of Things Journal, 4(2): 601-610, 2017.\n\n- Ot Krist Gupta and Ramesh Raskar. Distributed learning of deep neural network over multiple agents. Journal of Network and Computer Applications, 116:1-8, 2018.\n\n- Mengxi Hanyao, Yibo Jin, Zhuzhong Qian, Sheng Zhang, and Sanglu Lu. Edge-assisted online on-device object detection for real-time video analytics. In IEEE INFOCOM 2021-IEEE Conference on Computer Communications, pp. 1-10. IEEE, 2021.\n\n- Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, and Xinyi Le. Pf-net: Point fractal network for 3d point cloud completion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7662-7670, 2020.\n\n- D.P. Huttenlocher, G.A. Klanderman, and W.J. Rucklidge. Comparing images using the hausdorff distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(9):850-863, 1993. doi: 10.1109/34.232073.\n\n- Sunwook Hwang, Youngseok Kim, Seongwon Kim, Saewoong Bahk, and Hyung-Sin Kim. Upcycling: Semi-supervised 3d object detection without sharing raw-level unlabeled scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 23351-23361, October 2023.\n\n- Pileun Kim, Jingdao Chen, and Yong K Cho. Slam-driven robotic mapping and registration of 3d point clouds. Automation in Construction, 89:38-48, 2018.\n\n- Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\n\n- Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12697-12705, 2019.\n\n- Xinke Li, Zhirui Chen, Yue Zhao, Zekun Tong, Yabang Zhao, Andrew Lim, and Joey Tianyi Zhou. Pointba: Towards backdoor attacks in 3d point cloud. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 16492-16501, 2021.\n\n- Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.\n\n- Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 806-814, 2015.\n\n- Sicong Liu, Junzhao Du, Anshumali Shrivastava, and Lin Zhong. Privacy adversarial network: representation learning for mobile data privacy. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 3(4):1-18, 2019.\n\n- Yuhui Lu, Zhongxi Chen, and Mingbo Zhao. 3d objective detection for autonomous driving based on two-stage approach. In 2022 IEEE International Symposium on Product Compliance Engineering-Asia (ISPCE-ASIA), pp. 1-5. IEEE, 2022.\n\n- Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2837-2845, 2021.\n\n- Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5188-5196, 2015.\n\n- Priyanka Mandikal and Venkatesh Babu Radhakrishnan. Dense 3d point cloud reconstruction using a deep pyramid network. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1052-1060. IEEE, 2019.\n\n- Priyanka Mandikal, KL Navaneet, Mayank Agarwal, and R Venkatesh Babu. 3d-lmnet: Latent embedding matching for accurate and diverse 3d point cloud reconstruction from a single image. arXiv preprint arXiv:1807.07796, 2018.\n\n- Luke Melas-Kyriazi, Christian Rupprecht, and Andrea Vedaldi. Pc2: Projection-conditioned point cloud diffusion for single-image 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12923-12932, 2023.\n\n- Trix Mulder and Nynke E Vellinga. Exploring data protection challenges of automated driving. Computer Law & Security Review, 40:105530, 2021.\n\n- Tony Ng, Hyo Jin Kim, Vincent T Lee, Daniel DeTone, Tsun-Yi Yang, Tianwei Shen, Eddy Ilg, Vassileios Balntas, Krystian Mikolajczyk, and Chris Sweeney. Ninjadesc: content-concealing visual descriptors via adversarial learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12797-12807, 2022.\n\n- Francesco Pittaluga, Sanjeev J Koppal, Sing Bing Kang, and Sudipta N Sinha. Revealing scenes by inverting structure from motion reconstructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 145-154, 2019.\n\n- Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017.\n\n- Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointconn: 3d object proposal generation and detection from point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 770-779, 2019.\n\n- Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10529-10538, 2020.\n\n- Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural network for 3d object detection in a point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1711-1719, 2020.\n\n- Dong Wook Shu, Sung Woo Park, and Junseok Kwon. 3d point cloud generative adversarial network based on tree structured graph convolutions. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 3859-3868, 2019.\n\n- Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, and Ramesh Raskar. Detailed comparison of communication efficiency of split learning and federated learning. arXiv preprint arXiv:1909.09145, 2019a.\n\n- Akash Deep Singh, Sandeep Singh Sandha, Luis Garcia, and Mani Srivastava. Radhar: Human activity recognition from point clouds generated through a millimeter-wave radar. In Proceedings of the 3rd ACM Workshop on Millimeter-wave Networks and Sensing Systems, pp. 51-56, 2019b.\n\n- Zhenbo Song, Wayne Chen, Dylan Campbell, and Hongdong Li. Deep novel view synthesis from colored 3d point clouds. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIV 16, pp. 1-17. Springer, 2020.\n\n- Bernd Carsten Stahl and David Wright. Ethics and privacy in ai and big data: Implementing responsible research and innovation. IEEE Security & Privacy, 16(3):26-33, 2018.\n\n- Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2446-2454, 2020.\n\n- Pei Sun, Mingxing Tan, Weiyue Wang, Chenxi Liu, Fei Xia, Zhaoqi Leng, and Dragomir Anguelov. Swformer: Sparse window transformer for 3d object detection in point clouds. In European Conference on Computer Vision, pp. 426-442. Springer, 2022.\n\n- OpenPCDet Development Team. Openpcdet: An open-source toolbox for 3d object detection from point clouds. https://github.com/open-mmlab/OpenPCDet, 2020.\n\n- Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatrix Marcotegui, Fran\u00e7ois Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6411\u20136420, 2019.\n\n- Diego Valsesia, Giulia Fracastoro, and Enrico Magli. Learning localized generative models for 3d point clouds via graph convolution. In International conference on learning representations, 2018.\n\n- Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning for health: Distributed deep learning without sharing raw patient data. arXiv preprint arXiv:1812.00564, 2018.\n\n- Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang, Bin Yang, Wenyuan Zeng, and Raquel Urtasun. V2vnet: Vehicle-to-vehicle communication for joint perception and prediction. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pp. 605\u2013621. Springer, 2020.\n\n- Xianlong Wang, Minghui Li, Wei Liu, Hangtao Zhang, Shengshan Hu, Yechao Zhang, Ziqi Zhou, and Hai Jin. Unlearnable 3d point clouds: Class-wise transformation is all you need. In In Advances in Neural Information Processing Systems (NeurIPS), 2024, 2024a.\n\n- Xianlong Wang, Minghui Li, Peng Xu, Wei Liu, Leo Yu Zhang, Shengshan Hu, and Yanjun Zhang. Pointapa: Towards availability poisoning attacks in 3d point clouds. In European Symposium on Research in Computer Security, pp. 125-145. Springer, 2024b.\n\n- Xin Wen, Tianyang Li, Zhizhong Han, and Yu-Shen Liu. Point cloud completion by skip-attention network with hierarchical folding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1939\u20131948, 2020.\n\n- Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pp. 9621-9630, 2019.\n\n- Zhu Xiao, Jinmei Shu, Hongbo Jiang, Geyong Min, Hongyang Chen, and Zhu Han. Perception task offloading with collaborative computation for autonomous driving. IEEE Journal on Selected Areas in Communications, 41(2):457-473, 2022.\n\n- Yuwen Xiong, Wei-Chiu Ma, Jingkang Wang, and Raquel Urtasun. Learning compact representations for lidar completion and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1074-1083, 2023.\n\n- Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, Ming-Hsuan Yang, and Jiaqi Ma. V2x-vit: Vehicle-to-everything cooperative perception with vision transformer. In European conference on computer vision, pp. 107-124. Springer, 2022.\n\n- Hanyu Xue, Bo Liu, Ming Ding, Tianqing Zhu, Dayong Ye, Li Song, and Wanlei Zhou. Dp-image: Differential privacy for image data in feature space, 2023.\n\n- Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018.\n\n- Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow: 3d point cloud generation with continuous normalizing flows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4541-4550, 2019a.\n\n- Ziqi Yang, Jiyi Zhang, Ee-Chien Chang, and Zhenkai Liang. Neural network inversion in adversarial setting via background knowledge alignment. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pp. 225-240, 2019b.\n\n- Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11784-11793, 2021.\n\n- Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang, Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui Yuan, et al. Dair-v2x: A large-scale dataset for vehicle-infrastructure cooperative 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21361-21370, 2022.\n\n- Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou. Pointr: Diverse point cloud completion with geometry-aware transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 12498-12507, 2021.\n\n- Jiang Zhang, Lillian Clark, Matthew Clark, Konstantinos Psounis, and Peter Kairouz. Privacy-utility trades in crowdsourced signal map obfuscation. Computer Networks, 215:109187, 2022.\n\n- Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song. The secret revealer: Generative model-inversion attacks against deep neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 253-261, 2020.\n\n- Ziyu Zhang, Feipeng Da, and Yi Yu. Data-free point cloud network for 3d face recognition. arXiv preprint arXiv:1911.04731, 2019.\n\n- Xuejun Zhao, Wencan Zhang, Xiaokui Xiao, and Brian Lim. Exploiting explanations for model inversion attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 682-692, October 2021.\n\n- Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4490-4499, 2018.\n\n- Yufan Zhou, Haiwei Dong, and Abdulmotaleb El Saddik. Learning to estimate 3d human pose from point cloud. IEEE Sensors Journal, 20(20):12334-12342, 2020.\n\n- Vlas Zyrianov, Xiyue Zhu, and Shenlong Wang. Learning to generate realistic lidar point clouds. In European Conference on Computer Vision, pp. 17-35. Springer, 2022.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638641.86791", "msecs": "867.0", "relativeCreated": "238698.40741157532", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Simple Emergent Action Representations from Multi-Task Policy Training\n\n# Abstract\nThe low-level sensory and motor signals in deep reinforcement learning, which exist in high-dimensional spaces such as image observations or motor torques, are inherently challenging to understand or utilize directly for downstream tasks. While sensory representations have been extensively studied, the representations of motor actions are still an area of active exploration. Our work reveals that a space containing meaningful action representations emerges when a multi-task policy network takes as inputs both states and task embeddings. Moderate constraints are added to improve its representation ability. Therefore, interpolated or composed embeddings can function as a high-level interface within this space, providing instructions to the agent for executing meaningful action sequences. Empirical results demonstrate that the proposed action representations are effective for intra-action interpolation and inter-action composition with limited or no additional learning. Furthermore, our approach exhibits superior task adaptation ability compared to strong baselines in Mujoco locomotion tasks. Our work sheds light on the promising direction of learning action representations for efficient, adaptable, and composable RL, forming the basis of abstract action planning and the understanding of motor signal space.\n\n\n# 1 INTRODUCTION\n\n\nDeep reinforcement learning (RL) can learn near-optimal policies to perform low-level actions with pre-defined reward functions. While the low-level actions are adequate for the specified tasks, it remains challenging to reuse this learned knowledge to accomplish new tasks efficiently. By contrast, when humans perform multiple tasks, the low-level muscle movements are naturally summarized into high-level action representations such as \"pick up,\" or \"turn left\", which can be reused in novel tasks with slight modifications. As a result, though we are aware of our intent to perform a task, we carry out the most complicated movements without thinking about the detailed joint motions or muscle contractions (Kandel et al., 2021). By analogy with such abilities of humans, we ask the question: can RL agents have action representations of low-level motor controls, which can be reused, modified, or composed to perform new tasks?\n\n\nAs pointed out in Kandel et al. (2021), \"the task of the motor systems is the reverse of the task of the sensory systems. Sensory processing generates an internal representation in the brain of the outside world or of the state of the body. Motor processing begins with an internal representation: the desired purpose of movement\". In the past decade, representation learning has made tremendous progress in representing high-dimensional sensory signals such as images and audios for revealing the geometric and semantic structures hidden in the raw signals (Bengio et al., 2013; Chen et al., 2018; Kornblith et al., 2019; Chen et al., 2020; Baevski et al., 2020; Radford et al., 2021; Bardes et al., 2021; Bommasani et al., 2021; He et al., 2022). With the help of the generalization ability of sensory representation learning, downstream control tasks can be accomplished efficiently (Nair et al., 2022; Xiao et al., 2022; Yuan et al., 2022). In contrast to the advancement in sensory representation learning, action representation learning is yet under-explored. Thus, we make a step to study this topic with an aim to find generalizable action representations which can be reused or efficiently adapted to perform new tasks. A key idea in sensory representation learning is pretraining with a comprehensive task (or a set of tasks) and reusing the emergent latent representation. Inspired by this simple and powerful idea, we train a multi-task policy network, whereas a set of different tasks share the same latent action representation space. Further, the time-variant sensory and time-invariant ac\n\n\ntion representations are decoupled and concatenated as the sensory-action representation, which is transformed by a policy network to form the low-level action control. Surprisingly, when trained on a comprehensive set of tasks, we discover that such a simple structure learns an emergent self-organized action representation. The emergent action representation can be reused for intra-action interpolation, inter-action composition, and efficient task adaptation. In particular, we demonstrate the effectiveness of the emergent action representations with Mujoco locomotion environments and show zero-shot composition and few-shot task adaptation in the representation space. The agents adapt to new tasks in fewer steps than strong meta RL baselines. Besides the effectiveness of the action representation, we find the decoupled time-variant sensory representation exhibits equivariant properties. The evidence elucidates that reusable and generalizable action representations may lead to efficient, adaptable, and composable RL, thus forming the basis of abstract action planning and understanding motor signal space. The primary contributions in this work are listed as follows:\n\n- 1. We put forward the idea of leveraging emergent action representations from multi-task learners to better understand motor action space and accomplish task generalization.\n\n- 2. We decouple the state-related and task-related information of the sensory-action representations and reuse them to conduct action planning more efficiently.\n\n- 3. Our approach is a strong adapter, which achieves higher rewards with fewer steps than strong meta RL baselines when adapting to new tasks.\n\n- 4. Our approach supports intra-action interpolation as well as inter-action composition by modifying and composing the learned action representations.\n\n\nNext, we begin our technical discussion right below and leave the discussion of many valuable and related literature to the end.\n\n\n# 2 PRELIMINARIES\n\n\nSoft Actor-Critic. In this paper, our approach is built on Soft Actor-Critic (SAC) (Haarnoja et al., 2018). SAC is a stable off-policy actor-critic algorithm based on the maximum entropy reinforcement learning framework, in which the actor maximizes both the returns and the entropy. In SAC, there are three types of parameters to update during optimization: the policy parameter  $\\omega$ , the soft Q-function parameter  $\\theta$  and a learnable temperature  $\\alpha$ . The objectives are:\n\n\n$$\n\n$$\nJ (\\alpha) = \\mathbb {E} _ {\\mathbf {a} _ {t} \\sim \\pi_ {\\omega}} \\left[ - \\alpha \\log \\pi_ {\\omega} \\left(\\mathbf {a} _ {t} \\mid \\mathbf {s} _ {t}\\right) - \\alpha \\overline {{\\mathcal {H}}} \\right] \\tag {1}\n$$\n\n$$\n\n\n$$\n\n$$\nJ _ {\\pi} (\\omega) = \\mathbb {E} _ {\\mathbf {s} _ {t} \\sim \\mathcal {D}} \\left[ \\mathbb {E} _ {\\mathbf {a} _ {t} \\sim \\pi_ {\\omega}} \\left[ \\alpha \\log \\pi_ {\\omega} \\left(\\mathbf {a} _ {t} \\mid \\mathbf {s} _ {t}\\right) - Q _ {\\theta} \\left(\\mathbf {s} _ {t}, \\mathbf {a} _ {t}\\right) \\right] \\right] \\tag {2}\n$$\n\n$$\n\n\n$$\n\n$$\nJ _ {Q} (\\theta) = \\mathbb {E} _ {(\\mathbf {s} _ {t}, \\mathbf {a} _ {t}) \\sim \\mathcal {D}} \\left[ \\frac {1}{2} \\left(Q _ {\\theta} (\\mathbf {s} _ {t}, \\mathbf {a} _ {t}) - (r (\\mathbf {s} _ {t}, \\mathbf {a} _ {t}) + \\gamma \\mathbb {E} _ {\\mathbf {s} _ {t + 1} \\sim p} [ V _ {\\bar {\\theta}} (\\mathbf {s} _ {t + 1}) ])\\right) ^ {2} \\right] \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\overline{\\mathcal{H}}$  in Equation 1 is a pre-defined minimum expected entropy. The value function in Equation 3 is parameterized by the weights  $V_{\\bar{\\theta}}$  in the target Q-network and can be defined by:\n\n\n$$\n\n$$\nV \\left(\\mathbf {s} _ {t}\\right) = \\mathbb {E} _ {\\mathbf {a} _ {t} \\sim \\pi} \\left[ Q \\left(\\mathbf {s} _ {t}, \\mathbf {a} _ {t}\\right) - \\alpha \\log \\pi \\left(\\mathbf {a} _ {t} \\mid \\mathbf {s} _ {t}\\right) \\right] \\tag {4}\n$$\n\n$$\n\n\nTask Distribution. We assume the tasks that the agent may meet are drawn from a pre-defined task distribution  $p(\\mathcal{T})$ . Each task in  $p(\\mathcal{T})$  corresponds to a Markov Decision Process (MDP). Therefore, a task  $\\mathcal{T}$  can be defined by a tuple  $(\\mathcal{S},\\mathcal{A},p(\\mathbf{s}_{t + 1}|\\mathbf{s}_t,\\mathbf{a}_t),p(\\mathbf{s}_0),R(\\mathbf{s}_t,\\mathbf{a}_t))$ , in which  $\\mathcal{S}$  and  $\\mathcal{A}$  are respectively the state and action space,  $p(\\mathbf{s}_{t + 1}|\\mathbf{s}_t,\\mathbf{a}_t)$  the transition probability distribution,  $p(\\mathbf{s}_0)$  the initial state distribution and  $R(\\mathbf{s}_t,\\mathbf{a}_t)$  the reward function.\n\n\nThe concept of task distribution is frequently employed in meta RL problems, but we have made some modifications and extensions on it to better match with the setting in this work. We divide all the task distributions into two main categories, the \"uni-modal\" task distributions and the \"multimodal\" task distributions. Concretely, the two scenarios are defined as follows:\n\n\n- Definition 1 (Uni-modal task distribution): In a uni-modal task distribution, there is only one modality among all the tasks in the task distribution. For example, in HalfCheetah-Vel, a Mujoco locomotion environment, we train the agent to run at different target velocities. Therefore, running is the only modality in this task distribution.\n\n\n![Figure 1: Emergent action representations from multi-task training. The sensory information and task information are encoded separately. When both are concatenated, an action decoder decodes them into a low level action.](images/e6341511c39c98fe83cca485751402bd36dec8f9470a6133fbf5b3785db45387.jpg)\n\n\n- Definition 2 (Multi-modal task distribution): In contrast to uni-modal task distribution, there are multiple modalities among the tasks in this task distribution. A multi-modal task distribution includes tasks of several different uni-modal task distributions. For instance, we design a multi-modal task distribution called HalfCheetah-Run-Jump, which contains two modalities including HalfCheetah-BackVel and HalfCheetah-BackJump. The former has been defined in the previous section, and the latter contains tasks that train the agent to jump with different reward weight. In our implementation, we actually train four motions in this environment, running, walking, jumping ans standing. We will leave more details in Section 4 and Appendix A.1.\n\n\n# 3 EMERGENT ACTION REPRESENTATIONS FROM MULTI-TASK TRAINING\n\n\nIn this section, we first introduce the sensory-task decoupled policy network architecture. Next, we discuss the multitask policy training details, along with the additional constraints to the task embedding for the emergence of action representations. Lastly, we demonstrate the emergence of action representations through various phenomena and applications.\n\n\n# 3.1 MULTITASK POLICY NETWORK AND TRAINING\n\n\nDecoupled embedding and concatenated decoding. An abstract high-level task, e.g., \"move forward\", typically changes relatively slower than the transient sensory states. As a simplification, we decouple the latent representation into a time-variant sensory embedding  $\\mathcal{Z}_{\\mathbf{s}_t}$  and a time-invariant task embedding  $\\mathcal{Z}_{\\mathcal{T}}$ , which is shown in Figure 1. These embeddings concatenate to form a sensory-action embedding  $\\mathcal{Z}_{\\mathcal{A}}(\\mathbf{s}_t,\\mathcal{T}) = [\\mathcal{Z}_{\\mathbf{s}_t},\\mathcal{Z}_{\\mathcal{T}}]$ , which is transformed by the policy network (action decoder)  $\\psi$  to output a low-level action distribution  $p(\\mathbf{a}_t) = \\psi (\\mathbf{a}_t|\\mathcal{Z}_{\\mathbf{s}_t},\\mathcal{Z}_{\\mathcal{T}})$ , e.g., motor torques. The action decoder  $\\psi$  is a multi-layer perceptron (MLP) that outputs a Gaussian distribution in the low-level action space  $\\mathcal{A}$ .\n\n\nLatent sensory embedding (LSE). The low-level sensory state information is encoded by an MLP state encoder  $\\phi$  into a latent sensory embedding  $\\mathcal{Z}_{\\mathbf{s}_t} = \\phi (\\mathbf{s}_t)\\in \\mathbb{R}^m$ . It includes the proprioceptive information of each time step. LSE is time-variant in an RL trajectory, and the state encoder is shared among different tasks. We use LSE and sensory representation interchangeably in this paper.\n\n\nLatent task embedding (LTE). A latent task embedding  $\\mathcal{Z}_{\\mathcal{T}} \\in \\mathbb{R}^{d}$  encodes the time-invariant knowledge of a specific task. Let's assume we are going to train  $N$  different tasks, and their embeddings form an LTE set  $\\{\\mathcal{Z}_{\\mathcal{T}_N}\\}$ . These  $N$  different tasks share the same state encoder  $\\phi$  and action decoder  $\\psi$ ; in other words, these  $N$  tasks share the same policy network interface, except for their task embeddings being different. For implementation, we adopt a linear encoder, which takes as input the one-hot encodings of different training tasks, to initialize the set  $\\{\\mathcal{Z}_{\\mathcal{T}_N}\\}$ . This task encoder\n\n\nis learnable during training. After training, the LTE interface can be reused as a high-level action interface. Hence, we use LTE and action representation interchangeably in this paper.\n\n\nTraining of the multi-task policy networks. A detailed description of the multi-task training is demonstrated in Algorithm 1. When computing objectives and their gradients, we still use policy  $\\pi$  parameterized by  $\\omega$  to indicate all the parameters in the state encoder, task encoder and the action decoder. The overall training procedure is based on SAC. The only difference is that the policy network and Q networks additionally take as input the LTE  $\\mathcal{Z}_{\\tau}$  and a one-hot task label, respectively.\n\n\nDuring training, we also apply two techniques to constrain this space: 1) we normalize the LTEs so that they lie on a hypersphere; 2) we inject a random noise to the LTEs to enhance the smoothness of the space.\n\n\n```\nInput: Training task set  $\\{\\mathcal{T}_N\\} \\sim p(\\mathcal{T}),\\theta_1,\\theta_2,\\omega$ $\\bar{\\theta}_{1}\\gets \\theta_{1},\\bar{\\theta}_{2}\\gets \\theta_{2},\\mathcal{B}\\gets \\emptyset$    \nfor each pre-train epoch do   \nfor  $\\mathcal{T}_i$  in  $\\{\\mathcal{T}_n\\}$  do Sample a batch  $B_{i}$  of multi-task RL transitions with  $\\pi_{\\omega}$ $\\mathcal{B}\\leftarrow \\mathcal{B}\\cup \\mathcal{B}_i$    \nend for   \nend for   \nfor each train epoch do   \nSample RL batch  $b\\sim \\mathcal{B}$    \nfor all transition data in  $b$  do   \n $\\mathcal{Z}_{\\mathbf{s}_t} = \\phi (\\mathbf{s}_t)$ $\\widetilde{\\mathcal{Z}}_{\\mathcal{T}_i} =$  normalize  $(\\mathcal{Z}_{\\mathcal{T}_i} + n)$  and  $n\\sim \\mathcal{N}(0,\\sigma^2)$  Sample action  $\\mathbf{a}_t\\sim \\psi (\\cdot |\\mathcal{Z}_{\\mathbf{s}_t},\\widetilde{\\mathcal{Z}}_{\\mathcal{T}_i})$  for computing SAC objectives   \nend for   \nfor each optimization step do   \nCompute SAC objectives  $J(\\alpha),J_{\\pi}(\\omega),J_{Q}(\\theta)$  with  $b$  based on Equation 123 Update SAC parameters   \nend for   \nend for   \nOutput: The optimal model of state encoder  $\\phi^{*}$  and action decoder  $\\psi^{*}$  and a set of LTEs  $\\{\\mathcal{Z}_{\\mathcal{T}_N}\\}$\n```\n\n\n# 3.2 THE EMERGENCE OF ACTION REPRESENTATION\n\n\nAfter we train the multi-task policy network with a comprehensive set of tasks, where the LTE vectors in  $\\{\\mathcal{Z}_{\\mathcal{T}_N}\\}$  share the same embedding space, we find that  $\\{\\mathcal{Z}_{\\mathcal{T}_N}\\}$  self-organizes into a geometrically and semantically meaningful structure. Tasks with the same modality are embedded in a continuous fashion, which facilitates intra-task interpolation. Surprisingly, the composition of task embeddings from different modalities leads to novel tasks, e.g., \"run\" + \"jump\" = \"jump run\". Further, the action representation can be used for efficient task adaptation. Visualization also reveals interesting geometric structures in task embedding and sensory representation spaces. In this subsection, we dive into these intriguing phenomena, demonstrating the emergence of action representation and showing the generalization of the emergent action representation.\n\n\nTask interpolation & composition. After training the RL agent to accomplish multiple tasks, we select two pre-trained tasks and generate a new LTE through linear integration between the LTEs of the two chosen tasks. The newly-generated task embedding is expected to conduct the agent to perform another different task. The generated LTE is defined by:\n\n\n$$\n\n$$\n\\mathcal {Z} ^ {\\prime} = f \\left(\\beta \\mathcal {Z} _ {\\mathcal {T} _ {i}} + (1 - \\beta) \\mathcal {Z} _ {\\mathcal {T} _ {j}}\\right) \\tag {5}\n$$\n\n$$\n\n\nwhere  $i,j$  are the indices of the selected tasks and  $\\mathcal{Z}_{\\mathcal{T}_i},\\mathcal{Z}_{\\mathcal{T}_j}$  are their corresponding LTEs.  $\\beta$  is a hyperparameter ranging in (0,1). The function  $f(\\cdot)$  is a regularization function related to the pre\n\n\ndefined quality of the LTE Space. For instance, in this paper,  $f(\\cdot)$  is a normalization function to extend or shorten the result of interpolation to a unit sphere.\n\n\nA new task is interpolated by applying the aforementioned operation on the LTEs of tasks sampled from a uni-modal distribution. The interpolated task usually has the same semantic meaning as the source tasks while having different quantity in specific parameters, e.g., running with different speeds. A new task is composed by applying the same operation on tasks sampled from a multimodal distribution. The newly composed task usually lies in a new modality between the source tasks. For example, when we compose \"run\" and \"jump\" together, we will have a combination of an agent running while trying to jump.\n\n\nEfficient adaptation. We find that an agent trained with the multi-task policy network can adapt to unseen tasks quickly by only optimizing the LTEs. This shows that the LTEs learn a general pattern of the overall task distribution. When given a new task after pre-training, the agent explores in the LTE Space to find a suitable LTE for the task. Specifically, we perform a gradient-free cross-entropy method (CEM) (De Boer et al., 2005) in the LTE space for accomplishing the desired task. Detailed description can be found in Algorithm 2.\n\n\nGeometric Structures of LTEs and the LSEs. We visualize the LSEs and LTEs to understand their geometric structures. The results are deferred to Section 4.5.\n\n\n```\nInput: Adaptation task  $\\mathcal{T}\\sim p(\\mathcal{T}),\\phi^{*},\\psi^{*}$  capacity of elite set  $m$  , number of sampling n Initialize the elite set  $\\mathbb{Z}_e$  with  $m$  randomly sampled LTEs from the LTE Space for each adapt epoch do Initialize the overall test set by  $\\mathbb{Z}\\gets \\emptyset$  for  $\\mathcal{Z}_i$  in  $\\mathbb{Z}_e$  do Sample  $n$  LTEs  $\\mathcal{Z}_{i1},\\ldots ,\\mathcal{Z}_{in}$  near  $\\mathcal{Z}_i$ $\\mathbb{Z}\\gets \\mathbb{Z}\\cup \\{\\mathcal{Z}_i,\\mathcal{Z}_{i1},\\dots ,\\mathcal{Z}_{in}\\}$  end for for  $\\mathcal{Z}_j$  in  $\\mathbb{Z}$  do while not done do  $\\mathcal{Z}_{\\mathbf{s}_t} = \\phi^* (\\mathbf{s}_t)$ $\\mathbf{a}_t\\sim \\psi^* (\\cdot |\\mathcal{Z}_{\\mathbf{s}_t},\\mathcal{Z}_j)$ $r_t = R(\\mathbf{s}_t,\\mathbf{a}_t|\\mathcal{T})$ ${\\bf s}_{t + 1}\\sim p({\\bf s}_{t + 1}|{\\bf s}_t,{\\bf a}_t)$  end while end for Sort the task embeddings in  $\\mathcal{Z}$  by high cumulative reward in the trajectory Select the top  $m$  LTEs in  $\\mathcal{Z}$  to update  $\\mathbb{Z}_e$  end for\n```\n\n\n# 4 EXPERIMENTS\n\n\nIn this section, we first demonstrate the training process and performance of the multi-task policy network. Then, we use the LTEs as a high-level action interface to instruct the agents to perform unseen skills through interpolation without any training. After that, we conduct experiments to evaluate the effectiveness of the LTEs in task adaptation. Lastly, we visualize the LSEs and LTEs to further understand the structure of the state and action representation. We use emergent action representation (EAR) to refer to the policy using the LTEs.\n\n\n# 4.1 EXPERIMENTAL SETUPS\n\n\n**Environments.** We evaluate our method on five continuous control environments (HalfCheetah-Vel, Ant-Dir, Hopper-Vel, Walker-Vel, HalfCheetah-Run-Jump) based on OpenAI Gym and the Mujoco simulator. Detailed descriptions of these RL benchmarks are listed in Appendix A.1.\n\n\nBaselines. We compare EAR-SAC, the emergent action representation based SAC with several multi-task RL and meta RL baselines. For multi-task RL baselines, we use multi-head multi-task\n\n\n![Figure 2: Training performance of our method and baselines. The x-axis represents the total training steps (in million steps) and the y-axis represents the average reward of all the training tasks. All the training are based on 3 seeds. The shaded area is one standard deviation.](images/a21a7d42638361528b342e3d46797e720828d68143c908a64288ca5291ec5bf6.jpg)\n\n\n![](images/8e7f37fcc78fb4ccef87d632578fe6a44e509dfeed8fff80ae69335d67c6cf30.jpg)\n\n\n![](images/8d36ad229b304bc623f28349d4d2d8e70bd0b74608b2f4bbdc77f1490c824893.jpg)\n\n\n![](images/79739dc0386944006f64f4391eec79f474fc8c168da4c60169cd2970d7117859.jpg)\n\n\n<table><tr><td>Method</td><td>HalfCheetah-Vel</td><td>Ant-Dir</td><td>Hopper-Vel</td><td>Walker-Vel</td></tr><tr><td>EAR-SAC (Ours)</td><td>-136.9\u00b11.29</td><td>1216.7\u00b154.97</td><td>173.5\u00b111.09</td><td>172.7\u00b14.05</td></tr><tr><td>MAML</td><td>-500.9\u00b147.33</td><td>422.8\u00b129.40</td><td>34.0\u00b141.92</td><td>-4.9\u00b10.58</td></tr><tr><td>PEARL</td><td>-155.0\u00b115.50</td><td>635.9\u00b119.21</td><td>170.0\u00b17.80</td><td>163.9\u00b11.07</td></tr><tr><td>MHMT-SAC</td><td>-145.2\u00b12.60</td><td>1020.0\u00b154.90</td><td>172.9\u00b11.17</td><td>160.6\u00b15.21</td></tr><tr><td>OHE-SAC</td><td>-609.0\u00b18.10</td><td>195.7\u00b10.29</td><td>90.7\u00b12.39</td><td>65.0\u00b120.83</td></tr></table>\n\n\nTable 1: Comparison with baselines on the final performance. The metric is the return of the last epoch, and the mean and standard deviation is calculated among 3 seeds.\n\n\nSAC (MHMT-SAC) and one-hot embedding SAC (OHE-SAC; for ablation). For meta RL baselines, we use MAML (Finn et al., 2017) and PEARL (Rakelly et al., 2019). Detailed descriptions of these baselines are listed in Appendix A.2.\n\n\n# 4.2 MULTI-TASK TRAINING FOR ACTION REPRESENTATIONS\n\n\nIn this section, we train the multi-task network and evaluate whether sensory-action representations can boost training efficiency. EAR-SAC is compared with all the multi-task RL and meta RL baselines on final rewards in Table 1 and additionally compared with off-policy baselines on the training efficiency in Figure 2. We find that EAR-SAC outperforms all the baselines in terms of training efficiency. In environments with high-dimensional observations (e.g., Ant-Dir), EAR-SAC achieves large performance advantage against the baselines. We attribute this to that the learned action representation space may provide meaningful priors when the policy is trained with multiple tasks.\n\n\n# 4.3 ACTION REPRESENTATION AS A HIGH-LEVEL CONTROL INTERFACE\n\n\nIn this section, we control the agent by recomposing the LTEs. We perform intra-action interpolation and interaction composition in uni-modal task distributions and multi-modal task distributions respectively.\n\n\nIntra-action interpolation. Intra-action interpolation is conducted in HalfCheetah-Vel, Ant-Dir, Hopper-Vel, and Walker-Vel. We interpolate the action representations between two tasks using Equation 5. The coefficient  $\\beta$  is searched to better fit the target task. An interpolation example in HalfCheetah-Vel is demonstrated in Figure 3. We select the tasks of running at  $1\\mathrm{m / s}$  and  $2\\mathrm{m / s}$  to be interpolated and get three interpolated tasks: run at  $1.2\\mathrm{m / s}$ ,  $1.5\\mathrm{m / s}$ ,  $1.7\\mathrm{m / s}$ . We perform evaluation on each task for a trajectory, and visualize them. In each task, we make the agent start from the same point (not plotted in the figure) and terminate at 100 time steps.\n\n\n![Figure 3: Interpolated tasks. The top and bottom rows are in-distribution. The middle rows show the interpolated tasks.](images/b8e90e1c9f2ea2469d6a6f3fbd1aa0d3d7d2aba977160e8e2fd0c010e23b2ace.jpg)\n\n\n![](images/1042b8113b019343550159db45967369b3f19866754dda6e7c20240d790fb69c.jpg)\n\n\n![Figure 4: Visualization of task compositions. Two stop motion animations of the proposed composition tasks are demonstrated. Animated results are shown in the project page: https://sites.google.com/view/emergent-action-representation/.](images/9b049dc5827e70aa647bd04d99fbd92a786317dc78e8e5f57491346802f2c652.jpg)\n\n\n![Figure 5: Adaptation results. The x-axis represents the adaptation steps and the y-axis represents the average reward of all the adaptation tasks. We fix the adaptation task set for all algorithms in the same environment.](images/64506ed52cb3f54ade84d8beeddc932f5578d2c6dd247e9ba087cda28b92796e.jpg)\n\n\n![](images/97a397ee0de0f8c936294796cc6e31bfd4f88de31bd767af97ab3eddcdc6863e.jpg)\n\n\n![](images/a5bd808c0832109efb75193a331a42b60c06f5068dc2e0df97eec6496fd4abce.jpg)\n\n\n![](images/e6fcd1bf0077ea230f784a3796bcaf0f25a9e23254b7d3456b8fdec72c97a0ad.jpg)\n\n\nOnly part of the whole scene is visualized in the figure and we mark the terminals with red flags. We find that through task interpolation, the agent manages to accomplish these interpolated tasks without any training. We leave other results in all environments to Appendix A.3.\n\n\nInter-action composition. Inter-action composition is conducted in HalfCheetah-Run-Jump, in which we merge two uni-modal task distributions. Taking HalfCheetah-BackVel and HalfCheetah-BackJump as an example, we find that the agent has learned four motions: walking backward, running backward, standing with its front leg, and jumping with its front leg. We select walk-standing as a composition task pair and run-jumping as the other and compose them with Equation 5. These two generated representations are evaluated in 1000 time steps and part of their evaluation trajectories are visualized in Figure 4. We find that the walk-standing representation enables the agent to walk backward with a standing posture despite some pauses to keep itself balance, while the run-jumping representation helps the agent to jump when running backward after some trials. These empirical results indicate that the LTEs can be used as action representations by composing them for different new tasks.\n\n\n# 4.4 TASK ADAPTATION WITH ACTION REPRESENTATIONS\n\n\nIn this section, we assess how well an agent can adapt to new tasks by only updating the LTEs. We compare the agent's adaptation ability with the meta RL baselines (MAML,PEARL) and the ablation baseline (OHE-SAC). The results in Figure 5 demonstrate that EAR can adapt to new tasks and achieve to the converged performance in no more than three epochs, outperforming the baseline meta RL methods. We note that, with the help of the LTE space, we can adapt to new tasks using zero-th order optimization method.\n\n\n# 4.5 VISUALIZATION OF STATE AND ACTION REPRESENTATIONS\n\n\nIn this section, we further analyze and visualize the sensory-action representations based on the HalfCheetah environment.\n\n\n![Figure 6: Comparison between raw state signals (left) and sensory representations (right) in HalfCheetah-Vel. To make the period of running motion easier to observe, we reduce the time interval between adjacent time steps. For now a period of the motion is 15 steps, while in the original setting it is only 6 steps.](images/0fd3c6edf475bd278fdcd8cb29ac849dc78fa0a5e24d975d547c048e8e9dbaa9.jpg)\n\n\n![Figure 7: Visualization of the action representation space in HalfCheetah-Vel. Each grid on the unit sphere represents the terminal of an action representation and is colored based on how the agent performs when the policy is conditioned on the action representation. The redder the grid is, the faster the agent runs.](images/550dfa56f00aadbdb985f7d08dd74fd2aefb50670ef9ca6cef0bf32b2f38b980.jpg)\n\n\nThe sensory representations. In HalfCheetah-Vel, the halfcheetah agent is encouraged to run at a specific velocity, thus making its motion periodical. Therefore, the ideal raw signals and the LSEs in the trajectory should be periodical as well. After conducting Principal Components Analysis (PCA) (Abdi & Williams, 2010) to reduce the dimension of all the raw signals and the sensory representations collected in a trajectory, we visualize them for different time steps in Figure 6. We find that the raw state signals appear to be only roughly periodical due to the noise in the states. However, the LSEs show stronger periodicity than the raw signals, indicating that the sensory representations can reduce the influence of the noise in raw signals, thus helping the agent better understand and react to the environment.\n\n\nThe action representations. To better understand the intriguing fact that the agent can adapt to new tasks using the emergent action representations when only sparse training tasks are provided, we plot the LTE space in Figure 7. We find that the LTEs on the hypersphere automatically form a continuous space, constructing the basis of the composition of the representations in it. This also explains why the interpolation and composition of the LTEs result in meaningful new behaviors: when interpolated and normalized, the new LTE would still lie on this hypersphere, leading to new and meaningful action sequences.\n\n\n# 4.6 COMPARISON WITH ONE-HOT REPRESENTATION\n\n\nReaders might be curious about whether the learned, manipulable LTEs better than a naive one-hot embedding. In this section, we compare EAR with a different policy network that takes in an one-hot embedding rather than the LTE. Specifically, in multi-task training, we concatenate the raw state vector with the one-hot embedding as the input of the policy network to form a multi-task RL architecture. We find that the one-hot embedded policy performs unsatisfactorily in all environments and fails completely in Ant-Dir environment. We attribute this to the fact that, without the sensory-action representation, the agent fails to understand the underlying relation among multiple tasks, especially in those environments with high-dimensional observations and a large training task set. We also compare both methods in the task adaptation setting. When optimizing the one-hot embedding to adapt to new tasks, the agent can barely reach a reasonable performance in any of the new tasks. These facts echo with the idea that the simple emergent action representations are meaningful and effective in task learning and generalization.\n\n\n# 5 RELATED WORK\n\n\nRepresentation learning in reinforcement learning. Representation learning has been widely applied in RL to generate representations of sensory information (Laskin et al., 2020; Chandak et al., 2019; Yarats et al., 2021), policies (Edwards et al., 2019), dynamics (Watter et al., 2015).\n\n\nIn recent years, action representation has attracted more attention. In previous works (Dulac-Arnold et al., 2015), the action embeddings have been proposed to compress discrete actions to a continuous space based on prior knowledge. Recently, many researchers focus on mapping between the original continuous action space and a continuous manifold to facilitate and accelerate policy learning (Allshire et al., 2021) or simplify teleoperation (Losey et al., 2020). Moreover, such action representations are extended to high-dimensional real-world robotics control problems (Jeon et al., 2020). In this area, a closely related work is Chandak et al. (2019), which enables a policy to output a latent action representation with reinforcement learning and then uses supervised learning to construct a mapping from the representation space to the discrete action space. However, in our work, we discover emergent self-organized sensory-action representations from multi-task policy training without additional supervised learning, which can be further leveraged to generalize to new tasks.\n\n\nMulti-task reinforcement learning. Multi-task RL trains an agent to learn multiple tasks simultaneously with a single policy. A direct idea is to share parameters and learn a joint policy with multiple objectives (Wilson et al., 2007; Pinto & Gupta, 2017), but different tasks may conflict with each other. Some works tackle this problem by reducing gradient conflicts (Yu et al., 2020a), designing loss weighting (Hessel et al., 2019; Kendall et al., 2018), or leveraging regularization (Duong et al., 2015). Modular network has also been proposed to construct the policy network with combination of sub-networks (Heess et al., 2016; Yang et al., 2020). In our work, multiple tasks share the state encoder and action decoder; the action decoder is conditioned on LTE to reduce task conflicts. Emergent representations are learned for continuous actions during multi-task training.\n\n\nUnsupervised skill discovery. Unsupervised skill discovery employs competence-based methods to encourage exploration and help agents learn skills. These skills can be used in adapting to new tasks. The unsupervised methods maximize the mutual information between states and skills (Eysenbach et al., 2018; Liu & Abbeel, 2021; Sharma et al., 2019; Xu et al., 2020; Laskin et al., 2022), while our method learns a geometrically and semantically meaningful representation in a multi-task setting. We believe the proposed method can be complementary to the unsupervised paradigm.\n\n\nMeta reinforcement learning. The adaptation part of this work shares a similar goal as meta reinforcement learning (meta RL) (Finn et al., 2017; Xu et al., 2018a; Rothfuss et al., 2018; Wang et al., 2016; Levine et al., 2020). Meta RL executes meta-learning (Schmidhuber, 1987; Thrun & Pratt, 2012) algorithms under the framework of reinforcement learning, which aims to train an agent to quickly adapt to new tasks. Many meta RL methods are gradient-based, performing gradient descent on the policy (Finn et al., 2017; Xu et al., 2018a; Rothfuss et al., 2018), the hyperparameters (Xu et al., 2018b), or loss functions (Sung et al., 2017; Houthooft et al., 2018) based on the collected experience. Other meta RL algorithms are context-based (Wang et al., 2016; Rakelly et al., 2019). They embed context information into a latent variable and condition the meta-learning policy on such variables. Besides, recently offline meta RL algorithms (Levine et al., 2020; Mitchell et al., 2021) have also been widely explored, which enables the agent to leverage offline data to perform meta-training. However, we exploit the emergent high-level representations to abstract low-level actions for adaptation rather than introduce complicated new techniques.\n\n\n# 6 CONCLUSION\n\n\nIn this paper, we present our finding that the task embeddings in a multi-task policy network can automatically form a space where action representations reside. The emergent action representations abstract information of a sequence of actions and can serve as a high-level interface to instruct an agent to perform motor skills. Specifically, we find the action representations can be interpolated, composed, and optimized to generate novel action sequences for unseen tasks. Along with this work, a promising direction is to learn the action representation via self-supervised learning. Another intriguing direction is to learn hierarchical action representations that may capture different levels of semantics and geometric information in motor signals, thus facilitating future applications such as hierarchical planning (LeCun, 2022).\n\n\n# REFERENCES\n\n- Herv\u00e9 Abdi and Lynne J Williams. Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4):433-459, 2010.\n\n- Arthur Allshire, Roberto Martin-Martin, Charles Lin, Shawn Manuel, Silvio Savarese, and Animesh Garg. Laser: Learning a latent action space for efficient reinforcement learning. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 6650-6656. IEEE, 2021.\n\n- Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems, 33:12449-12460, 2020.\n\n- Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.\n\n- Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.\n\n- Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n\n- Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, and Philip Thomas. Learning action representations for reinforcement learning. In International conference on machine learning, pp. 941-950. PMLR, 2019.\n\n- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. PMLR, 2020.\n\n- Yubei Chen, Dylan Paiton, and Bruno Olshausen. The sparse manifold transform. Advances in neural information processing systems, 31, 2018.\n\n- Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the cross-entropy method. Annals of operations research, 134(1):19-67, 2005.\n\n- Gabriel Dulac-Arnold, Richard Evans, Peter Sunehag, and Ben Coppin. Reinforcement learning in large discrete action spaces. corr abs/1512.07679 (2015). arXiv preprint arXiv:1512.07679, 2015.\n\n- Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. Low resource dependency parsing: Crosslingual parameter sharing in a neural network parser. In Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing (volume 2: short papers), pp. 845-850, 2015.\n\n- Ashley Edwards, Himanshu Sahni, Yannick Schroecker, and Charles Isbell. Imitating latent policies from observation. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 1755-1763. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/edwards19a.html.\n\n- Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2018.\n\n- Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126-1135. PMLR, 2017.\n\n- Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861-1870. PMLR, 2018.\n\n- Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr D\u00f3l\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000-16009, 2022.\n\n- Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, and David Silver. Learning and transfer of modulated locomotor controllers. arXiv preprint arXiv:1610.05182, 2016.\n\n- Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van Hasselt. Multi-task deep reinforcement learning with popart. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3796-3803, 2019.\n\n- Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, and Pieter Abbeel. Evolved policy gradients. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\n- Hong Jun Jeon, Dylan P Losey, and Dorsa Sadigh. Shared autonomy with learned latent actions. arXiv preprint arXiv:2005.03210, 2020.\n\n- E.R. Kandel, J.D. Koester, S.H. Mack, and S.A. Siegelbaum. Principles of Neural Science, Sixth Edition. McGraw-Hill Education, 2021. ISBN 9781259642241.\n\n- Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7482-7491, 2018.\n\n- Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n\n- Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2661-2671, 2019.\n\n- Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In Hal Daume III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5639-5650. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.org/press/v119/laskin20a.html.\n\n- Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and Pieter Abbeel. Cic: Contrastive intrinsic control for unsupervised skill discovery. arXiv preprint arXiv:2202.00161, 2022.\n\n- Y LeCun. A path towards autonomous machine intelligence. preprint posted on openreview, 2022.\n\n- Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. CoRR, abs/2005.01643, 2020.\n\n- Hao Liu and Pieter Abbeel. Aps: Active pretraining with successor features. In International Conference on Machine Learning, pp. 6736-6747. PMLR, 2021.\n\n- Dylan P Losey, Krishnan Srinivasan, Ajay Mandlekar, Animesh Garg, and Dorsa Sadigh. Controlling assistive robots with learned latent actions. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 378-384. IEEE, 2020.\n\n- Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline meta-reinforcement learning with advantage weighting. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 7780-7791. PMLR, 18-24 Jul 2021.\n\n- Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.\n\n- Lerrel Pinto and Abhinav Gupta. Learning to push by grasping: Using multiple tasks for effective learning. In 2017 IEEE international conference on robotics and automation (ICRA), pp. 2161-2168. IEEE, 2017.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. PMLR, 2021.\n\n- Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International conference on machine learning, pp. 5331-5340. PMLR, 2019.\n\n- Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Prompt: Proximal meta-policy search. arXiv preprint arXiv:1810.06784, 2018.\n\n- J\u00fcrgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta... hook. PhD thesis, Technische Universit\u00e4t M\u00fcnchen, 1987.\n\n- John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1889-1897, Lille, France, 07-09 Jul 2015. PMLR.\n\n- Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware unsupervised discovery of skills. In International Conference on Learning Representations, 2019.\n\n- Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn: Meta-critic networks for sample efficient learning. arXiv preprint arXiv:1706.09529, 2017.\n\n- Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.\n\n- Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, R\u00e9mi Munos, Charles Blundell, Dharshan Kumaran, and Matthew M. Botvinick. Learning to reinforcement learn. CoRR, abs/1611.05763, 2016.\n\n- Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin A. Riedmiller. Embodied to control: A locally linear latent dynamics model for control from raw images. CoRR, abs/1506.07365, 2015. URL http://arxiv.org/abs/1506.07365.\n\n- Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine learning, pp. 1015-1022, 2007.\n\n- Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022.\n\n- Kelvin Xu, Siddharth Verma, Chelsea Finn, and Sergey Levine. Continual learning of control primitives: Skill discovery via reset-games. Advances in Neural Information Processing Systems, 33: 4999-5010, 2020.\n\n- Tianbing Xu, Qiang Liu, Liang Zhao, and Jian Peng. Learning to explore via meta-policy gradient. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5463-5472. PMLR, 10-15 Jul 2018a.\n\n- Zhongwen Xu, Hado P van Hasselt, and David Silver. Meta-gradient reinforcement learning. Advances in neural information processing systems, 31, 2018b.\n\n- Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning with soft modularization. Advances in Neural Information Processing Systems, 33:4767-4777, 2020.\n\n- Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with prototypical representations. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 11920-11931. PMLR, 18-24 Jul 2021. URL https://proceedings.mlrpress/v139/yarats21a.html.\n\n- Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:5824-5836, 2020a.\n\n- Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 1094-1100. PMLR, 2020b.\n\n- Zhecheng Yuan, Zhengrong Xue, Bo Yuan, Xueqian Wang, Yi Wu, Yang Gao, and Huazhe Xu. Pre-trained image encoder for generalizable visual reinforcement learning. In First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML, 2022.\n\nResponse: Outcome: \\boxed{Reject}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768638643.0539243", "msecs": "53.0", "relativeCreated": "239884.42182540894", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "text": "System:You are an expert academic reviewer evaluating research papers. Assess methodology (rigor, reproducibility), significance (impact, novelty), and clarity (exposition, structure). Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}.\nQuery: Predict the acceptance of the paper. Output: \\boxed{Accept} or \\boxed{Reject}.\n\n# Transport with Support: Data-Conditional Diffusion Bridges\n\n# Abstract\nThe dynamic Schr\u00f6dinger bridge problem provides an appealing setting for posing optimal transport problems as learning non-linear diffusion processes and enables efficient iterative solvers. Recent works have demonstrated state-of-the-art results (eg, in modelling single-cell embryo RNA sequences or sampling from complex posteriors) but are typically limited to learning bridges with only initial and terminal constraints. Our work extends this paradigm by proposing the Iterative Smoothing Bridge (ISB). We combine learning diffusion models with Bayesian filtering and optimal control, allowing for constrained stochastic processes governed by sparse observations at intermediate stages and terminal constraints. We assess the effectiveness of our method on synthetic and real-world data and show that the ISB generalises well to high-dimensional data, is computationally efficient, and provides accurate estimates of the marginals at intermediate and terminal times.\n\n\n# 1 INTRODUCTION\n\n\nGenerative diffusion models have gained increasing popularity and achieved impressive results in a variety of challenging application domains, such as computer vision (e.g., Ho et al., 2020; Song et al., 2021a; Dhariwal & Nichol, 2021), reinforcement learning (e.g., Janner et al., 2022), and time series modelling (e.g., Rasul et al., 2021; Vargas et al., 2021; Tashiro et al., 2021; Park et al., 2022). Recent works have explored connections between denoising diffusion models and the dynamic Schr\u00f6dinger bridge problem (SBP, e.g., Vargas et al., 2021; De Bortoli et al., 2021; Shi et al., 2022) to adopt iterative schemes for solving the dynamic optimal transport problem more efficiently. The solution of the SBP that correspond to denoising diffusion models is then given by the finite-time process, which is the closest in Kullback-Leibler (KL) divergence to the forward noising process of the diffusion model under marginal constraints. Data is then generated by time-reversing the process.\n\n\nIn many applications, the interest is not purely in modelling transport between an initial and terminal state distribution. In naturally occurring generative processes, we typically observe snapshots of realizations along intermediate stages of individual sample trajectories (see Fig. 1). Such problems arise in medical diagnosis (e.g., tissue changes and cell growth), demographic modelling, environmental dynamics, and animal movement modelling\u2014see Fig. 4 for modelling bird migration and wintering patterns. Recently, constrained optimal control problems have been explored by adding additional fixed path constraints (Maoutsa et al., 2020; Maoutsa & Opper, 2021) or modifying the prior processes (Fernandes et al., 2021). However, defining meaningful fixed path constraints or prior processes for the optimal control problems can be challenging, while sparse observational data are accessible in many real-world applications.\n\n\nIn this work, we propose the Iterative Smoothing Bridge (ISB), an iterative method for solving control problems under sparse observational data constraints and constraints on the initial and terminal distribution. We perform the conditioning by leveraging the iterative pass idea from the Iterative Proportional Fitting procedure (IPFP) (Kullback, 1968; De Bortoli et al., 2021) procedure and applying differentiable particle filtering (Reich, 2013; Corenflos et al., 2021) within the outer loop. Integrating sequential Monte Carlo methods (e.g., Doucet et al., 2001; Chopin & Papaspiliopoulos, 2020) into the IPFP framework in such a way is non-trivial and can be understood as a novel iterative version of the algorithm by Maoutsa & Opper (2021) but with more general terminal constraints and path constraints defined by data.\n\n\n![(a) Unconstrained transport (Schrodinger bridge)](images/6f7db6f51b37652ad725307fd2f2becc376057d3623acfddc5f4bf91b10cb6bb.jpg)\n\n\n![(b) Constrained transport (Iterative smoothing bridge) Figure 1: Illustrative example transport between an initial unit Gaussian and a shifted unit Gaussian at the terminal time  $T$ . Unconstrained transport on the left and the solution constrained by sparse observations (\u25a0) on the right. Colour coding of the initial points is only for distinguishing the paths.](images/f8c8d3212673dad43d4042f6917838a4645789f1a7c2adf029dbb7faa84e8bfa.jpg)\n\n\nWe summarize the contributions as follows. (i) We propose a novel method for solving constrained optimal transport as a bridge problem under sparse observational data constraints. (ii) Thereof, we utilize the strong connections between the constrained bridging problem and particle filtering in sequential Monte Carlo, extending them from pure inference to learning. Additionally, (iii) we demonstrate practical efficiency and show that the iterative smoothing bridge approach scales to high-dimensional data.\n\n\n# 1.1 RELATED WORK\n\n\nSchrodinger bridges The problem of learning a stochastic process moving samples from one distribution to another can be posed as a type of a transport problem known as a dynamic Schr\u00f6dinger bridge problem (SBP, e.g., Schr\u00f6dinger, 1932; L\u00e9onard, 2014), where the resulting marginal densities are desired to resemble a given reference measure. In machine learning literature, the problem has been studied through learning the drift function of the dynamical system (De Bortoli et al., 2021; Wang et al., 2021; Vargas et al., 2021; Bunne et al., 2022). When an SDE system also defines the reference measure, the bridge problem becomes synonymous with a constrained optimal control problem (e.g., Caluya & Halder, 2022; 2021; Chen et al., 2021), which has been leveraged in learning Schr\u00f6dinger bridges by Tianrong Chen (2022) through forward-backward SDEs. An optimal control problem with both constraints on the initial and terminal distribution and a fixed path constraint has been studied in Maoutsa et al. (2020) and Maoutsa & Opper (2021), where particle filtering is applied to continuous path constraints but the boundary constraints are defined by a single point. Furthermore, the combination of Schr\u00f6dinger bridges and state-space models has been studied by Reich (2019), in a setting where Schr\u00f6dinger bridges are applied to the transport problem between filtering distributions.\n\n\nDiffusion models in machine learning The recent advances in diffusion models in machine learning literature have been focused in generating samples from complex distributions defined by data through transforming samples from an easy-to-sample distribution by a dynamical system (e.g., Ho et al., 2020; Song et al., 2021b;a; Nichol & Dhariwal, 2021). The concept of reversing SDE trajectories via score-based learning (Hyv\u00e4rinen & Dayan, 2005; Vincent, 2011) has allowed for models scalable enough to be applied to high-dimensional data sets directly in the data space. In earlier work, score-based diffusion models have been applied to problems where the dynamical system itself is of interest, for example, for the problem of time series amputation in Tashiro et al. (2021) and inverse problems in imaging in Song et al. (2022). Other dynamical models parametrized by neural networks have been applied to modelling latent time-series based on observed snapshots of dynamics (Rubanova et al., 2019; Li et al., 2020), but without further constraints on the initial or terminal distributions.\n\n\nState-space models In their general form, state-space models combine a latent space dynamical system with an observation (likelihood) model. Evaluating the latent state distribution based on observational data can be performed by applying particle filtering and smoothing (Doucet et al., 2000) or by approximations of the underlying state distribution of a non-linear state-space model by a specific model family, for instance, a Gaussian (see S\u00e4rkk\u00e4, 2013, for an overview). Speeding up parameter inference and learning in state-space models has been widely studied (e.g., Sch\u00f6n\n\n\net al., 2011; Svensson & Schon, 2017; Kokkala et al., 2014). Particle smoothing can be connected to Schr\u00f6dinger bridges via the two-filter smoother (e.g., Bresler, 1986; Briers et al., 2009; Hostettler, 2015), where the smoothing distribution is estimated by performing filtering both forward from the initial constraint and backward from the terminal constraint. We refer to Mitter (1996) and Todorov (2008) for a more detailed discussion on the connection of stochastic control and filtering and to Chopin & Papaspiliopoulos (2020) for an introduction to particle filters.\n\n\n# 2 BACKGROUND\n\n\nLet  $\\mathcal{C} = C([0,T],\\mathbb{R}^d)$  denote the space of continuous functions from  $[0,T]$  to  $\\mathbb{R}^d$  and let  $\\mathcal{B}(\\mathcal{C})$  denote the Borel  $\\sigma$ -algebra on  $\\mathcal{C}$ . Let  $\\mathcal{P}(\\pi_0,\\pi_T)$  denote the space of probability measures on  $(\\mathcal{C},\\mathcal{B}(\\mathcal{C}))$  such that the marginals at  $0,T$  coincide with probability densities  $\\pi_0$  and  $\\pi_T$ , respectively. The KL divergence from measure  $\\mathbb{Q}$  to measure  $\\mathbb{P}$  is written as  $\\mathrm{D}_{\\mathrm{KL}}[\\mathbb{Q}\\parallel \\mathbb{P}]$ , where we assume that  $\\mathbb{Q}\\ll \\mathbb{P}$ . For modelling the time dynamics, we assume a (continuous-time) state-space model consisting of a non-linear latent It\u00f4 SDE (see, e.g., \u00d8ksendal, 2003; S\u00e4rkk\u00e4 & Solin, 2019) in  $[0,T]\\times \\mathbb{R}^d$  with drift function  $f_{\\theta}(\\cdot)$  and diffusion function  $g(\\cdot)$ , and a Gaussian observation model, i.e.,\n\n\n$$\n\n$$\n\\mathbf {x} _ {0} \\sim \\pi_ {0}, \\quad \\mathrm {d} \\mathbf {x} _ {t} = f _ {\\theta} (\\mathbf {x} _ {t}, t) \\mathrm {d} t + g (t) \\mathrm {d} \\boldsymbol {\\beta} _ {t}, \\quad \\mathbf {y} _ {k} \\sim \\mathrm {N} (\\mathbf {y} _ {k} | \\mathbf {x} _ {t}, \\sigma^ {2} \\mathbf {I} _ {d}) \\big | _ {t = t _ {k}}, \\qquad (1)\n$$\n\n$$\n\n\nwhere the drift  $f_{\\theta}:\\mathbb{R}^{d}\\times [0,T]\\to \\mathbb{R}^{d}$  is a mapping modelled by a neural network (NN) parameterized by  $\\theta \\in \\Theta$ , diffusion  $g:[0,T]\\rightarrow \\mathbb{R}$  and  $\\beta_{t}$  denotes standard  $d$ -dimensional Brownian motion.  $\\mathbf{x}_t$  denotes the latent stochastic process and  $\\mathbf{y}_t$  denotes the observation-space process. In practice, we consider the continuous-discrete time setting, where the process is observed at discrete time instances  $t_k$  such that observational data can be given in terms of a collection of input-output pairs  $\\{(t_k,\\mathbf{y}_k)\\}_{k = 1}^M$ .\n\n\n# 2.1 SCHR\u00d6DINGER BRIDGES AND OPTIMAL CONTROL\n\n\nThe Schr\u00f6dinger bridge problem (SBP, Schr\u00f6dinger, 1932; L\u00e9onard, 2014) can be described as an entropy-regularized optimal transport problem where the optimality is measured through the KL divergence from a reference measure  $\\mathbb{P}$  to the posterior measure  $\\mathbb{Q}$ , with fixed initial and final densities  $\\pi_0$  and  $\\pi_T$ , i.e.,\n\n\n$$\n\n$$\n\\min  _ {\\mathbb {Q} \\in \\mathcal {P} \\left(\\pi_ {0}, \\pi_ {T}\\right)} \\mathrm {D} _ {\\mathrm {K L}} \\left[ \\mathbb {Q} \\| \\mathbb {P} \\right]. \\tag {2}\n$$\n\n$$\n\n\nIn this work, we consider only the case where the measures  $\\mathbb{P}$  and  $\\mathbb{Q}$  are constructed as the marginals of an SDE, i.e.,  $\\mathbb{Q}_t$  is the probability measure of the marginal of the SDE in Eq. (1) at time  $t$ , whereas  $\\mathbb{P}_t$  corresponds to the probability measure of the marginal of a reference SDE  $\\mathrm{d}\\mathbf{x}_t = f(\\mathbf{x}_t,t)\\mathrm{d}t + g(t)\\mathrm{d}\\beta_t$ , at time  $t$ , where we call  $f$  the reference drift. Under the optimal control formulation of the SBP (Caluya & Halder, 2021) the KL divergence in Eq. (2) reduces to\n\n\n$$\n\n$$\n\\mathbb {E} \\left[ \\int_ {0} ^ {T} \\frac {1}{2 g (t) ^ {2}} \\| f _ {\\theta} (\\mathbf {x} _ {t}, t) - f (\\mathbf {x} _ {t}, t) \\| ^ {2} \\mathrm {d} t \\right], \\tag {3}\n$$\n\n$$\n\n\nwhere the expectation is over paths from Eq. (1). Ruschendorf & Thomsen (1993) and Ruschendorf (1995) showed that a solution to the SBP can be obtained by iteratively solving two half-bridge problems using the Iterative Proportional Fitting procedure (IPFP) for  $l = 0, 1, \\ldots, L$  steps:\n\n\n$$\n\n$$\n\\mathbb {Q} _ {2 l + 1} = \\underset {\\mathbb {Q} \\in \\mathcal {P} (\\cdot , \\pi_ {T})} {\\arg \\min } \\mathrm {D} _ {\\mathrm {K L}} [ \\mathbb {Q} \\| \\mathbb {Q} _ {2 l} ] \\quad \\text {a n d} \\quad \\mathbb {Q} _ {2 l + 2} = \\underset {\\mathbb {Q} \\in \\mathcal {P} (\\pi_ {0}, \\cdot)} {\\arg \\min } \\mathrm {D} _ {\\mathrm {K L}} [ \\mathbb {Q} \\| \\mathbb {Q} _ {2 l + 1} ], \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\mathbb{Q}_0$  is set as the reference measure, and  $\\mathcal{P}(\\pi_0,\\cdot)$  and  $\\mathcal{P}(\\cdot ,\\pi_T)$  denote the sets of probability measures with only either the marginal at time 0 or time  $T$  coinciding with  $\\pi_0$  or  $\\pi_T$ , respectively. Recently the IPFP to solving Schr\u00f6dinger bridges has been adapted as a machine learning problem (Bernton et al., 2019; Vargas et al., 2021; De Bortoli et al., 2021). In practice, the interval  $[0,T]$  is discretized and the forward drift  $f_{\\theta}$  and the backward drift  $b_{\\phi}$  of the corresponding reverse-time process (Haussmann & Pardoux, 1986; F\u00f6llmer, 1988) are modelled by NNs. Under the Gaussian transition approximations, the resulting discrete-time diffusion model can be reversed by applying a mean-matching based objective.\n\n\n![Figure 2: Sketch of a diffusion bridge between a 2D data distribution  $(\\pi_0)$  and an isotropic Gaussian  $(\\pi_T)$  constrained by sparse observations  $(\\mathbf{x})$ . The forward diffusion at the first iteration (ISB 1) learns to account for the sparse observations but does not converge to the correct terminal distribution  $(t = T)$ , and the backward diffusion vice versa. After iterating (ISB 6), the forward and backward diffusions converge to the correct targets and are able to account for the sparse observational data.](images/ad1840be26a46678adeaa088cf60c55ec3fa4627d2509b372bc822095c1f7a93.jpg)\n\n\n# 3 METHODS\n\n\nGiven an initial and terminal distribution  $\\pi_0$  and  $\\pi_T$ , we are interested in learning a data-conditional bridge between  $\\pi_0$  and  $\\pi_T$ . Let  $\\mathcal{D} = \\{(t_j, \\mathbf{y}_j)\\}_{j=1}^M$  be a set of  $M$  sparsely observed values, i.e., only a few or no observations are made at each point in time, and let the state-space model of interest be given by Eq. (1). Note that we deliberately use  $(t_j, \\mathbf{y}_j)$  (instead of  $(t_k, \\mathbf{y}_k)$ ) to highlight that we allow for multiple observations at the same time point  $t_k$ . Our aim is to find a parameterization of the drift function  $f_\\theta$  such that evolving  $N$  particles  $\\mathbf{x}_t^i$ , with  $\\mathbf{x}_0^i \\sim \\pi_0$  (with  $i = 1, 2, \\dots, N$ ), according to Eq. (1) will result in samples  $\\mathbf{x}_T^i$  from the terminal distribution  $\\pi_T$ . Inspired by the IPFP by De Bortoli et al. (2021), which decomposes the SBP into finding two half-bridges, we propose to iteratively solve the two half-bridge problems while accounting for the additional sparse observations simultaneously. For this, let\n\n\n$$\n\n$$\n\\mathrm {d} \\mathbf {x} _ {t} = f _ {l, \\theta} \\left(\\mathbf {x} _ {t}, t\\right) \\mathrm {d} t + g (t) \\mathrm {d} \\beta_ {t}, \\quad \\mathbf {x} _ {0} \\sim \\pi_ {0}, \\tag {5}\n$$\n\n$$\n\n\n$$\n\n$$\n\\mathrm {d} \\mathbf {z} _ {t} = b _ {l, \\phi} (\\mathbf {z} _ {t}, t) \\mathrm {d} t + g (t) \\mathrm {d} \\hat {\\boldsymbol {\\beta}} _ {t}, \\quad \\mathbf {z} _ {0} \\sim \\pi_ {T}, \\tag {6}\n$$\n\n$$\n\n\ndenote the forward and backward SDE at iteration  $l = 1,2,\\dots ,L$ , where  $\\hat{\\beta}_t$  is the reverse-time Brownian motion. For simplicity, we denote  $\\beta_{t} = \\hat{\\beta}_{t}$  when the direction of the SDE is clear.\n\n\nTo learn the data-conditioned bridge, we iteratively employ the following steps: 1 evolve and filter forward particle trajectories according to Eq. (5) with drift  $f_{l-1,\\theta}$  and observations  $\\{(t_k, \\mathbf{y}_k)\\}_{k=1}^M$ , 2 learn the drift function  $b_{l,\\phi}$  for the reverse-time SDE, 3 evolve and filter backward particle trajectories according to Eq. (6) with the drift  $b_{l,\\phi}$  learned in step 2 and observations  $\\{(t_k, \\mathbf{y}_k)\\}_{k=1}^M$ , and 4 learn the drift function  $f_{l,\\theta}$  for the forward SDE based on the backward particles. Fig. 2 illustrates the forward and backward process of our iterative scheme for a data-conditioned denoising diffusion bridge. Next, we will go through steps 1 - 4 in detail and introduce the Iterative Smoothing Bridge method for solving data-conditional diffusion bridges.\n\n\n# 3.1 THE ITERATIVE SMOOTHING BRIDGE\n\n\nThe Iterative Smoothing Bridge (ISB) method iteratively generates particle filtering trajectories (steps 1 and 3 in Fig. 2) and learns the parameterizations of the forward and backward drift functions  $f_{l,\\theta}$  and  $b_{l,\\phi}$  (steps 2 and 4) by minimizing a modified version of the mean-matching objective presented by De Bortoli et al. (2021). Note that steps 2 and 4 are dependent on applying differential resampling in the particle filtering steps 1 and 3 for reversing the generated trajectories. We will now describe the forward trajectory generating step 1 and the backward drift learning step 2 in detail. Steps 3 and 4 are given by application of 1 and 2 on their reverse-time counterparts.\n\n\nStep 1 (and 3): Given a fixed discretization of the time interval  $[0, T]$  denoted as  $\\{t_k\\}_{k=1}^K$  with  $t_1 = 0$  and  $t_K = T$ , denote the time step lengths as  $\\Delta_k = t_{k+1} - t_k$ . By truncating the Ito-Taylor series of the SDE, we can consider an Euler-Maruyama (e.g., Ch. 8 in S\u00e4rkk\u00e4 & Solin, 2019) type of discretization for the continuous-time problem. We give the time-update of the  $i^{\\text{th}}$  particle at time  $t_k$  evolved according to Eq. (5) before conditioning on the observational data as\n\n\n$$\n\n$$\n\\tilde {\\mathbf {x}} _ {t _ {k}} ^ {i} = \\mathbf {x} _ {t _ {k - 1}} + f _ {l - 1, \\theta} \\left(\\mathbf {x} _ {t _ {k - 1}, t _ {k - 1}}\\right) \\Delta_ {k} + g \\left(t _ {k - 1}\\right) \\sqrt {\\Delta_ {k}} \\boldsymbol {\\xi} _ {k} ^ {i}, \\tag {7}\n$$\n\n$$\n\n\nwhere  $\\pmb{\\xi}_k^i\\sim \\mathrm{N}(\\mathbf{0},\\mathbf{I})$ . In step 3, the particles  $\\tilde{\\mathbf{z}}_{t_k}^i$  of the backward SDE Eq. (6) are similarly obtained. The SDE dynamics sampled in steps 1 and 3 apply the learned drift functions  $f_{l - 1,\\theta}$  and  $b_{l,\\phi}$  from the previous step and do not require sampling from the underlying SDE model. For times  $t_k$  at which no observations are available, we set  $\\mathbf{x}_t^i = \\tilde{\\mathbf{x}}_t^i$  (and  $\\mathbf{z}_{t_k}^i = \\tilde{\\mathbf{z}}_{t_k}^i$  respectively) and otherwise compute the particle filtering weights  $w_{t_k}^i$  based on the observations  $\\{(t_j,\\mathbf{y}_j)\\in \\mathcal{D}|t_j = t_k\\}$  for resampling. See Sec. 3.2 for details on the particle filtering proposal density and calculation of the particle weights.\n\n\nFor resampling, we employ a differentiable resampling procedure, where the particles and weights  $(\\tilde{\\mathbf{x}}_{t_k}^i,w_{t_k}^i)$  are transported to uniformly weighted particles  $\\mathbf{x}_{t_k}^i$  by solving an entropy-regularized optimal transport problem (Cuturi, 2013; Peyre & Cuturi, 2019; Corenflos et al., 2021), see App. D for further details. Through application of the  $\\varepsilon$ -regularized optimal transport map  $\\mathbf{T}_{(\\varepsilon)}\\in \\mathbb{R}^{N\\times N}$  (see Corenflos et al., 2021, for details) the particles are resampled via the map to  $\\mathbf{x}_{t_k}^i = \\bar{\\mathbf{X}}_{t_k}^\\top \\mathbf{T}_{(\\varepsilon),i}$ , where  $\\tilde{\\mathbf{X}}_{t_k}\\in \\mathbb{R}^{N\\times d}$  denotes the stacked particles  $\\{\\tilde{\\mathbf{x}}_{t_k}^i\\}_{i = 1}^N$  at time  $t_k$  before resampling. The resampled particles for the backward process are given similarly.\n\n\nStep 2 (and 4): Given the particles  $\\{\\mathbf{x}_{t_k}^i\\}_{k = 1,i = 1}^{K,N}$ , we now aim to learn the drift function for the respective reverse-time process. In case no observation is available at time  $t_k$ , we apply the mean-matching loss based on a Gaussian transition approximation proposed in De Bortoli et al. (2021):\n\n\n$$\n\n$$\n\\ell_ {k + 1, \\mathrm {n o o b s}} ^ {i} = \\left\\| b _ {l, \\phi} \\left(\\mathbf {x} _ {t _ {k + 1}} ^ {i}, t _ {k + 1}\\right) \\Delta_ {k} - \\mathbf {x} _ {t _ {k + 1}} ^ {i} - f _ {l - 1, \\theta} \\left(\\mathbf {x} _ {t _ {k + 1}} ^ {i}, t _ {k}\\right) \\Delta_ {k} + \\mathbf {x} _ {t _ {k}} ^ {i} + f _ {l - 1, \\theta} \\left(\\mathbf {x} _ {t _ {k}} ^ {i}, t _ {k}\\right) \\Delta_ {k} \\right\\| ^ {2}. \\tag {8}\n$$\n\n$$\n\n\nIn case an observation is available at time  $t_k$  the particle values  $\\tilde{\\mathbf{X}}_{t_k}$  will be coupled through the optimal transport map. Therefore, the transition density is a sum of Gaussian variables (see App. A for details and a derivation), and the mean-matching loss is therefore given by\n\n\n$$\n\n$$\n\\begin{array}{l} s \\ell_ {k + 1, \\mathrm {o b s}} ^ {i} = \\| b _ {l, \\phi} (\\mathbf {x} _ {t _ {k + 1}} ^ {i}, t _ {k + 1}) \\Delta_ {k} - \\mathbf {x} _ {t _ {k + 1}} ^ {i} - f _ {l - 1, \\theta} (\\mathbf {x} _ {t _ {k + 1}} ^ {i}, t _ {k}) \\Delta_ {k} \\\\ + \\sum_ {n = 1} ^ {N} T _ {(\\varepsilon), i, n} \\left(\\mathbf {x} _ {t _ {k}} ^ {n} + f _ {l - 1, \\theta} \\left(\\mathbf {x} _ {t _ {k}} ^ {n}, t _ {k}\\right) \\Delta_ {k}\\right) \\| ^ {2}. \\tag {9} \\\\ \\end{array}\n$$\n\n$$\n\n\nThe overall objective function is a combination of both loss functions, with the respective mean-matching loss depending on whether  $t_k$  is an observation time. The final loss function is written as:\n\n\n$$\n\n$$\n\\ell (\\phi) = \\sum_ {i = 1} ^ {N} \\left[ \\sum_ {k = 1} ^ {K} \\ell_ {k, \\text {o b s}} ^ {i} (\\phi) \\mathbb {I} _ {y _ {t _ {k}} \\neq \\emptyset} + \\sum_ {k = 1} ^ {K} \\ell_ {k, \\text {n o o b s}} ^ {i} (\\phi) \\mathbb {I} _ {y _ {t _ {k}} = \\emptyset} \\right], \\tag {10}\n$$\n\n$$\n\n\nwhere  $\\mathbb{I}_{\\mathrm{cond.}}$  denotes an indicator function that returns '1' iff the condition is true, and '0' otherwise. Consequently, the parameters  $\\phi$  of  $b_{l,\\phi}$  are learned by minimizing Eq. (10) through gradient descent. In practice, a cache of trajectories  $\\{\\mathbf{x}_{t_k}^i\\}_{k = 1,i = 1}^{K,N}$  is maintained through training of the drift functions, and refreshed at fixed number of inner loop iterations, as in De Bortoli et al. (2021), avoiding differentiation over the SDE generation computational graph. The calculations for step 4 follow similarly.\n\n\nThe learned backward drift  $b_{l,\\phi}$  can be interpreted as an analogy of the backward drift in Maoutsa & Opper (2021), connecting our approach to solving optimal control problems through Hamilton-Jacobi equations, see App. A.2 for an analysis of the backwards SDE and the control objective. While we are generally considering problem settings where the number of observations is low, we propose that letting  $M \\to \\infty$  yields the underlying marginal distribution, see Prop. 2 in App. A.3.\n\n\n# 3.2 COMPUTATIONAL CONSIDERATIONS\n\n\nThe ISB algorithm is a generic approach to learn data-conditional diffusion bridges under various choices of, e.g., the particle filter proposal density or the reference drift. Next, we cover practical considerations for the implementation of the method and highlight the model choices in the experiments.\n\n\nMultiple observations per time step Naturally, we can make more than one observation at a single point in time  $t_k$ , denoted as  $\\mathcal{D}_{t_k} = \\{(t_j, \\mathbf{y}_j) \\in \\mathcal{D} \\mid t_j = t_k\\}$ . To compute particle weights  $w_{t_k}^i$  for the  $i^{\\text{th}}$  particle we consider only the  $H$ -nearest neighbours of  $\\mathbf{x}_{t_k}^i$  in  $\\mathcal{D}_{t_k}$  instead of all observations in  $\\mathcal{D}_{t_k}$ . By restricting to the  $H$ -nearest neighbours, denoted as  $\\mathcal{D}_{t_k}^H$ , we introduce an additional locality to the proposal density computation which can be helpful in case of multimodality. On the other hand, letting  $H > 1$  results in weights which take into account the local density of the observations, not only the distance to the nearest neighbour. In experiments with few observations, we set  $H = 1$ , the choice of  $H$  is discussed when we have set the value higher.\n\n\nParticle filtering proposal The proposal density chosen for the ISB is the bootstrap filter, where the proposal matches the Gaussian transition density  $p(\\mathbf{x}_{t_k} \\mid \\mathbf{x}_{t_{k-1}})$ . Assuming a Gaussian noise model  $\\mathrm{N}(\\mathbf{0}, \\sigma^2\\mathbf{I})$ , the unnormalized log-weights for the  $i^{\\text{th}}$  particle at time  $t_k$  are given by:\n\n\n$$\n\n$$\n\\log w _ {t _ {k}} ^ {i} = - \\frac {1}{2 \\sigma^ {2}} \\sum_ {\\mathbf {y} _ {j} \\in \\mathcal {D} _ {t _ {k}} ^ {H}} \\| \\mathbf {x} _ {t _ {k}} ^ {i} - \\mathbf {y} _ {j} \\| ^ {2}. \\tag {11}\n$$\n\n$$\n\n\nObservational noise schedule In practice, using a constant observation noise  $\\sigma^2$  variance can result in an iterative scheme which does not have a stationary point as  $L\\to \\infty$ . Even if the learned drift function  $f_{l,\\theta}$  was optimal, the filtering steps 1 and 3 would alter the trajectories unless all particles would have uniform weights. Thus, we introduce a noise schedule  $\\kappa (l)$  which ensures that the observation noise increases in the number of ISB iterations, causing ISB to converge to the IPFP (De Bortoli et al., 2021) as  $L\\rightarrow \\infty$ . We found that letting the observation noise first decrease and then increase (in the spirit of simulated annealing) often outperformed a strictly increasing observation noise schedule. The noise schedule is studied in App. C, where we derive the property that letting  $L\\rightarrow \\infty$  yields IPFP.\n\n\nDrift initialization Depending on the application, one may choose to incorporate additional information by selecting an appropriate initial drift. A possible choice includes a pre-trained neural network drift learned to transport  $\\pi_0$  to  $\\pi_T$  without accounting for observations. However, starting from a drift for the unconstrained SBP can be problematic in cases where the observations are far away from the unconstrained bridge. To encouraged exploration, one may choose  $f_0 = 0$  for the initial drift. In various problem settings, we found both, a zero drift and starting from the SBP, to be successful in the experiments, see App. C for discussion.\n\n\n# 4 EXPERIMENTS\n\n\nTo assess the properties and performance of the ISB, we present a range of experiments that demonstrate how the iterative learning procedure can incorporate both observational data and terminal constraints. We start with simple examples that build intuition (cf. Fig. 1 and Fig. 2) and show standard ML benchmark tasks. For quantitative assessment, we design an experiment with a non-linear SDE for which the marginal distributions are available in closed-form. Finally, we demonstrate our model both in a highly multimodal bird migration task, conditioned image generation, and in a single-cell embryo RNA modelling problem. Ablation studies are found in App. C.\n\n\nExperiment setup In all experiments, the forward and backward drift functions  $f_{\\theta}$  and  $b_{\\phi}$  are parametrized as neural networks. For low-dimensional experiments we apply the MLP block design as in De Bortoli et al. (2021), and for the image experiment an U-Net as in Nichol & Dhariwal (2021). The latent state SDE was simulated by Euler-Maruyama with a fixed time-step of 0.01 over 100 steps and 1000 particles if not otherwise stated. All low-dimensional (at most  $d = 5$ ) experiments were run on a MacBook Pro laptop CPU, whereas the image experiments used a single Nvidia A100 GPU and ran for  $5 \\mathrm{~h}$  10 min. Notice that since ISB only performs particle filtering outside the stochastic gradient training loop, the training runtime is in the same order as in the earlier Schr\u00f6dinger bridge image generation experiments of De Bortoli et al. (2021). Thus we omit any wall-clock timings. Full details for all the experiments are included in App. B.\n\n\n![(a)  $t = 0.00$](images/698b26adf80686ba599bcf467acac10886d1cb9a8edc2ef48faca635dc4824e9.jpg)\n\n\n![(b)  $t = 0.25$](images/abd8c4a7c92b9ee4fd74f2011cb8d6050539b7c81a72f38af7eba423af587245.jpg)\n\n\n![(c)  $t = 0.50$ Figure 3: 2D toy experiments from scikit-learn with both cases starting from a Gaussian: The TWO CIRCLES (top) and TWO MOONS (bottom) data sets, with observations (red markers) constraining the problem. For the circles, the 10 circular observations at  $t = 0.5$  first force the method to create a circle that then splits into two; in the lower plot the observations at  $t \\in [0.25, 0.5, 0.75]$  split the data into clusters before joining them into two moons. See Fig. 6 in the Appendix for the IPFP result.](images/5205d5d3ea5261c8521cea1fa8f47da5465e352b4fc003c381fe4358da0da885.jpg)\n\n\n![(d)  $t = 0.75$](images/b74290718f16eb286eaeef5acd51b29aa9ecc4efa43fa89a8213d68fe27d1e62.jpg)\n\n\n![(e)  $t = 0.99$](images/7c99f9838dfbf656abeac2daa0694fe37e7dacd67bd541a53c6474b1b2267133.jpg)\n\n\nAll experiment settings include a number of hyperparameter choices, some of them typical to all diffusion problems and some specific to particle filtering and smoothing. The diffusion  $g(t)$  is a pre-determined function not optimized in the training. We divide the experiments to two main subsets: problems of 'sharpening to achieve a data distribution' and 'optimal transport problems'. In the former, the initial distribution has a support overlapping with the terminal distribution and the process noise level  $g(t)$  goes from high to low as time progresses. Conversely in the latter setting, the particles sampled from the initial distribution must travel to reach the support of the terminal distribution, and we chose to use a constant process noise level. Perhaps the most significant choice of hyperparameter is the observational noise level, as it imposes a preference on how closely should the observational points be followed, see App. C.1 for details.\n\n\n2D toy examples We show illustrative results for the TWO MOONS and CIRCLES from scikit-learn. We add artificial observation data to bias the processes. For the circles, the observational data consists of 10 points, spaced evenly on the circle. The points are all observed simultaneously, at halfway through the process, forcing the marginal density of the generating SDE to collapse to the small circle, and then to expand. For the two moons, the observational data is collected from 10 trajectories of a diffusion model which generates the two moons from noise, and these 10 trajectories are then observed at three points in time. Results are visualized in Fig. 3 (see videos in supplement). For reference, we have included plots of the IPFP dynamics in the supplement, see Fig. 6.\n\n\n# QUANTITATIVE COMPARISON ON THE BENE\u0160 SDE\n\n\nIn order to quantify how observing a process in between its initial and terminal states steers the ISB model to areas with higher likelihood, we test its performance on a Bene\u0161 SDE model (see, e.g. S\u00e4rkk\u00e4 & Solin, 2019). The Bene\u0161 SDE is a non-linear one-dimensional SDE of form\n\n\n$\\mathrm{d}x_{t} = \\tanh (x_{t})\\mathrm{d}t + \\mathrm{d}\\beta_{t}$  with  $x_0 = 0$  , but its marginal density is available in closed-form, allowing for negative log-likelihood evaluation. We simulate trajectories from the Bene\u0161 SDE and from the reverse drift and stack the reversed trajectories. The terminal distribution is shifted and scaled so that the Bene\u0161 SDE itself does not solve the transport problem from  $\\pi_0$  to  $\\pi_T$  , see App. B.2 for details and visualizations of the processes.\n\n\nAs a baseline, we fit a Schr\u00f6dinger bridge model with no observational data, using the Bene\u0161 SDE drift as the reference model. The ISB model is initialized with a zero-drift model (not with the Bene\u0161 as reference), thus making learning more challenging. We compare the models in terms of negative log predictive density in Table 1, where we see that the ISB model captures the process well on average (over the entire time-horizon) and at selected marginal times.\n\n\nTable 1: Results for the Bene\u0161 experiment.\n\n\n<table><tr><td rowspan=\"2\">METHOD</td><td colspan=\"3\">Negative log predictive density</td></tr><tr><td>AVERAGE</td><td>MIDDLE</td><td>END</td></tr><tr><td>Schr\u00f6dinger B</td><td>4.787</td><td>3.565</td><td>0.1919</td></tr><tr><td>Iterative smoothing B</td><td>3.557</td><td>2.985</td><td>0.1567</td></tr></table>\n\n\n![Figure 4: Bird migration example. The top row describes nesting and wintering areas for the birds as well as example sightings during migration. At the bottom, we show the marginal densities of the ISB model from the initial to terminal distribution that match the sightings along the migration.](images/4726e9bd2b546c7321b20866260ced6112b11bbb7b77cdd26a5db5669c4fc4da.jpg)\n\n\nBird migration Bird migration can be seen as a regular seasonal transport problem, where birds move (typically North-South) along a flyway, between breeding and wintering grounds. We take this as a motivating example of constrained optimal transport, where the geographical and constraints and preferred routes are accounted for by bird sighting data (see Fig. 4 top). By adapting data from Ambrosini et al. (2014) and Pellegrino et al. (2015), we propose a simplified data set for geese migration in Europe (OIBMD: ornithologically implausible bird migration data; available in the supplement). We applied the ISB for 12 iterations, with a linear observation noise schedule from 1 to 0.2, and constant diffusion noise 0.05. The drift function was initialized as a zero-function, and thus the method did not rely on a separately fit model optimized for generating the wintering distribution based on the breeding distribution. For comparison, we include the Schr\u00f6dinger bridge results in App. B.3.\n\n\nConstraining an image generation process We demonstrate that the ISB approach scales well to high-dimensional inputs by studying a proof-of-concept image generation task. We modify the diffusion generative process of the MNIST (LeCun et al., 1998) digit 8 by artificial observations steering the dynamical system in the middle of the generation process. While the concept of observations in case of image generation is somewhat unnatural, it showcases the scalability of the method to high-dimensional data spaces. Here, the drift is initialized using a pre-trained neural network obtained by first running a Schr\u00f6dinger bridge model for image generation. The process is then given an observation in the form of a bottom-half of a MNIST digit 8 in the middle of the dynamical process. As the learned model uses information from the observation both before and after the observation time, the lower half of the image is sharper than the upper half. We provide further details on this experiment and sampled trajectories in App. B.4.\n\n\nSingle-cell embryo RNA-seq Lastly, we evaluated our approach on an Embryoid body scRNA-seq time course (Tong et al., 2020). The data consists of RNA measurements collected over five time ranges from a developing human embryo system. No trajectory information is available, instead we only have\n\n\nTable 2: Results for single-cell embryo RNA experiment.\n\n\n<table><tr><td rowspan=\"2\">METHOD</td><td colspan=\"5\">Earth mover&#x27;s distance</td></tr><tr><td>t=0</td><td>t=1</td><td>t=2</td><td>t=3</td><td>t=T</td></tr><tr><td>TrajectoryNet (Tong et al., 2020)</td><td>0.62</td><td>1.15</td><td>1.49</td><td>1.26</td><td>0.99</td></tr><tr><td>IPML (Vargas et al., 2021)</td><td>0.34</td><td>1.13</td><td>1.35</td><td>1.01</td><td>0.49</td></tr><tr><td>IPFP (no observations)</td><td>0.57</td><td>1.53</td><td>1.86</td><td>1.32</td><td>0.85</td></tr><tr><td>ISB (single-cell observations)</td><td>0.57</td><td>1.04</td><td>1.24</td><td>0.94</td><td>0.83</td></tr></table>\n\n\naccess to snapshots of RNA data. This leads to a data set over 5 time ranges, the first from days 0-3 and the last from days 15-18. In the experiment, we followed the protocol by Tong et al. (2020), reduced the data dimensionality to  $d = 5$  using PCA, and used the first and last time ranges as the initial and terminal constraints. All other time ranges are considered observational data. Contrary to the other experiments, intermediate data are imprecise (only a time range of multiple days is known) but abundant.\n\n\nWe learned the ISB using a zero drift and compared it against an unconditional bridge obtained through the IPFP (De Bortoli et al., 2021)\u2014see Fig. 5. The ISB learns to generate trajectories\n\n\n![(a) Schr\u00f6dinger bridge (via IPFP)](images/132f0325c4a158f7ac3693a19e723668d8080d9794137fdaf05b674dcad087b0.jpg)\n\n\n![(b) Iterative Smoothing Bridge Figure 5: Illustration of the trajectories of the high-dimensional single-cell experiment for the Schr\u00f6dinger bridge (a) and the ISB (b), projected onto the first two principal components. The first five trajectories are highlighted in colour, and intermediate observation densities visualized as slices.](images/e012e791202a34a154b267c7f6b6fbf8d09b0b69a9ad9a485a18d7a9bd418013.jpg)\n\n\nwith marginals closer to the observed data while performing comparably to the IPFP at the initial and terminal stages. This improvement is also verified numerically in Table 2, showing that the ISB obtains a lower Earth mover's distance between the generated marginals and the observational data than IPFP. Additionally, Table 2 lists the performance of previous works that do not use the intermediate data during training (Tong et al., 2020) or only use it to construct an informative reference drift (Vargas et al., 2021), see App. B.5 for details. In both cases, ISB outperforms the other approaches w.r.t. the intermediate marginal distributions  $(t = 1,2,3)$ , while IPML (Vargas et al., 2021) outperforms ISB at the initial and terminal stages due to its data-driven reference drift. Notice that while we reduced the dimensionality via PCA to 5 for fair comparisons to Vargas et al. (2021), the ISB model would also allow modelling the full state-space model, with observations in the high-dimensional gene space and a latent SDE.\n\n\n# 5 DISCUSSION AND CONCLUSION\n\n\nThe dynamic Schr\u00f6dinger bridge problem provides an appealing setting for posing optimal transport problems as learning non-linear diffusion processes and enables efficient iterative solvers. However, while recent works have state-of-the-art performance in many complex application domains, they are typically limited to learning bridges with only initial and terminal constraints dependent on observed data. In this work, we have extended this paradigm and introduced the Iterative Smoothing Bridge (ISB), an iterative algorithm for learning data-conditional smoothing bridges. For this, we leveraged the strong connections between the constrained bridging problem and particle filtering in sequential Monte Carlo, extending them from pure inference to learning. We thoroughly assessed the applicability and flexibility of our approach in various experimental settings, including synthetic data sets and complex real-world scenarios (e.g., bird migration, conditional image generation, and modelling single-cell RNA-sequencing time-series). In our experiments, we showed that ISB generalizes well to high-dimensional data, is computationally efficient, and provides accurate estimates of the marginals at intermediate and terminal times.\n\n\nAccurately modelling the dynamics of complex systems under both path constraints induced by sparse observations and initial and terminal constraints is a key challenge in many application domains. These include biomedical applications, demographic modelling, and environmental dynamics, but also machine learning specific applications such as reinforcement learning, planning, and time-series modelling. All these applications have in common that the dynamic nature of the problem is driven by progression of time, and not only progression of a generative process as often is the case in, e.g., generative image models. Thus, constraints over intermediate stages have a natural role and interpretation in this wider set of dynamic diffusion modelling applications. We believe the proposed ISB algorithm opens up new avenues for diffusion models in relevant real-world modelling tasks and will be stimulating for future work. For example, more sophisticated observational models, alternative strategies to account for multiple observations, and different noise schedules could be explored. Furthermore, the proposed approach could be extended to other types of optimal transport problems, such as the Wasserstein barycenter, a frequently employed case of the multi-marginal optimal transport problem.\n\n\n# REFERENCES\n\n- Roberto Ambrosini, Riccardo Borgoni, Diego Rubolini, Beatrice Sicurella, Wolfgang Fiedler, Franz Bairlein, Stephen R. Baillie, Robert A. Robinson, Jacquie A. Clark, Fernando Spina, and Nicola Saino. Modelling the progression of bird migration with conditional autoregressive models applied to ringing data. PLoS ONE, 9(7):1-10, 07 2014.\n\n- Espen Bernton, Jeremy Heng, Arnaud Doucet, and Pierre E. Jacob. Schr\u00f6dinger bridge samplers. arXiv preprint arXiv:1912.13170, 2019.\n\n- Yoram Bresler. Two-filter formulae for discrete-time non-linear Bayesian smoothing. International Journal of Control, 43(2):629-641, 1986.\n\n- Mark Briers, Arnaud Doucet, and Simon Maskell. Smoothing algorithms for state-space models. Annals of the Institute of Statistical Mathematics, 62:61-89, 02 2009.\n\n- Charlotte Bunne, Ya-Ping Hsieh, Marco Cuturi, and Andreas Krause. Recovering stochastic dynamics via gaussian schr\u00f6dinger bridges. arXiv preprint arXiv:2202.05722, 2022.\n\n- Kenneth F. Caluya and Abhishek Halder. Reflected Schr\u00f6dinger bridge: Density control with path constraints. In American Control Conference, ACC 2021, pp. 1137-1142. IEEE, 2021.\n\n- Kenneth F. Caluya and Abhishek Halder. Wasserstein proximal algorithms for the Schr\u00f6dinger bridge problem: Density control with nonlinear drift. IEEE Transactions on Automatic Control, 67(3): 1163-1178, 2022.\n\n- Yongxin Chen, Tryphon T. Georgiou, and Michele Pavon. Stochastic control liaisons: Richard Sinkhorn meets Gaspard Monge on a Schr\u00f6dinger Bridge. SIAM Review, 63(2):249-313, 2021.\n\n- Nicolas Chopin and Omiros Papaspiliopoulos. An Introduction to Sequential Monte Carlo. Springer, 2020.\n\n- Adrien Corenfl\u00f3s, James Thornton, George Deligiannidis, and Arnaud Doucet. Differentiable particle filtering via entropy-regularized optimal transport. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, volume 139 of Proceedings of Machine Learning Research, pp. 2100\u20132111. PMLR, 2021.\n\n- Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems 26 (NIPS), pp. 2292-2300. Curran Associates, Inc., 2013.\n\n- Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schr\u00f6dinger bridge with applications to score-based generative modeling. In Advances in Neural Information Processing Systems 34 (NeurIPS), pp. 17695-17709. Curran Associates, Inc., 2021.\n\n- Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems 35 (NeurIPS), pp. 8780-8794. Curran Associates, Inc., 2021.\n\n- Arnaud Doucet, Simon Godsill, and Christophe Andrieu. On sequential Monte Carlo sampling methods for Bayesian filtering. Statistics and Computing, 10(3):197-208, 2000.\n\n- Arnaud Doucet, Nando De Freitas, and Neil James Gordon. *Sequential Monte Carlo methods in practice*. Statistics for Engineering and Information Science. Springer, 2001.\n\n- David Lopes Fernandes, Francisco Vargas, Carl Henrik Ek, and Neill DF Campbell. Shooting Schr\u00f6dinger's cat. In Proceedings of the Fourth Symposium on Advances in Approximate Bayesian Inference (AABI), 2021.\n\n- Hans F\u00f6llmer. Random fields and diffusion processes. In \u00c9cole d'\u00c9t\u00e9e de Probabilit\u00e9s de Saint-Flour XV-XVII, 1985-87, pp. 101-203. Springer, 1988.\n\n- Ulrich G Haussmann and Etienne Pardoux. Time reversal of diffusions. The Annals of Probability, pp. 1188-1205, 1986.\n\n- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems 33 (NeurIPS), pp. 6840-6851. Curran Associates, Inc., 2020.\n\n- Roland Hostettler. A two filter particle smoother for Wiener state-space systems. In 2015 IEEE Conference on Control Applications (CCA), pp. 412-417, 2015.\n\n- Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4):695-709, 2005.\n\n- Michael Janner, Yilun Du, Joshua B. Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szeptsv\u00e1ri, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning (ICML), volume 162, pp. 9902-9915. PMLR, 2022.\n\n- Juho Kokkala, Arno Solin, and Simo S\u00e4rkk\u00e4. Expectation maximization based parameter estimation by sigma-point and particle smoothing. In Proceedings of the 17th International Conference on Information Fusion (FUSION), pp. 1-8, 2014.\n\n- Solomon Kullback. Probability densities with given marginals. The Annals of Mathematical Statistics, 39(4):1236-1243, 1968.\n\n- Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The MNIST database of handwritten digits, 1998. URL http://yann.lecun.com/exdb/mnist/.\n\n- Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, and David Duvenaud. Scalable gradients for stochastic differential equations. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (AISTATS), volume 108 of Proceedings of Machine Learning Research, pp. 3870-3882. PMLR, 2020.\n\n- Christian L\u00e9onard. A survey of the Schr\u00f6dinger problem and some of its connections with optimal transport. Discrete & Continuous Dynamical Systems, 34(4):1533-1574, 2014.\n\n- Dimitra Maoutsa and Manfred Opper. Deterministic particle flows for constraining stochastic nonlinear systems. arXiv preprint arXiv:2112.05735, 2021.\n\n- Dimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of Fokker-Planck equations through gradient-log-density estimation. Entropy, 22:802, 07 2020.\n\n- S.K. Mitter. Filtering and stochastic control: a historical perspective. IEEE Control Systems Magazine, 16(3):67-76, 1996.\n\n- Alex Nichol and Prafulla Dhariwal. Improved Denoising Diffusion Probabilistic Models. arXiv preprint arXiv:2102.09672, 2021.\n\n- Bernt \u00d8ksendal. Stochastic Differential Equations: An Introduction with Applications. Springer, New York, NY, sixth edition, 2003.\n\n- Sung Woo Park, Kyungjae Lee, and Junseok Kwon. Neural markov controlled SDE: Stochastic optimization for continuous-time data. In International Conference on Learning Representations (ICLR), 2022.\n\n- Irene Pellegrino, Marco Cucco, Arne Follestad, and Mathieu Boos. Lack of genetic structure in greylag goose (Anser anser) populations along the European Atlantic flyway. PeerJ, 3:e1161, 2015.\n\n- Gabriel Peyre and Marco Cuturi. Computational optimal transport. Foundations and Trends in Machine Learning, 11(5-6):355-607, 2019.\n\n- Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs M. Bergmann, and Roland Vollgraf. Multivariate probabilistic time series forecasting via conditioned normalizing flows. In International Conference on Learning Representations (ICLR), 2021.\n\n- Sebastian Reich. A nonparametric ensemble transform method for Bayesian inference. SIAM Journal on Scientific Computing, 35, 2013.\n\n- Sebastian Reich. Data assimilation: The Schr\u00f6dinger perspective. Acta Numerica, 28:635-711, 2019.\n\n- Yulia Rubanova, Tian Qi Chen, and David K Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. In Advances in Neural Information Processing Systems 32 (NeurIPS), pp. 5321-5331. Curran Associates, Inc., 2019.\n\n- Ludger Ruschendorf. Convergence of the iterative proportional fitting procedure. The Annals of Statistics, 23(4):1160-1174, 1995.\n\n- L. Ruschendorf and W. Thomsen. Note on the Schr\u00f6dinger equation and I-projections. Statistics & Probability Letters, 17(5):369-375, 1993.\n\n- Simo S\u00e4rkk\u00e4. Bayesian Filtering and Smoothing. Cambridge University Press, Cambridge, UK, 2013.\n\n- Simo S\u00e4rkk\u00e4 and Arno Solin. Applied Stochastic Differential Equations. Cambridge University Press, Cambridge, UK, 2019.\n\n- E. Schr\u00f6dinger. Sur la th\u00e9orie relativiste de l'\u00e9lectron et l'interpr\u00e9table de la m\u00e9canique quantique. Annales de l'institut Henri Poincar\u00e9, 2(4):269-310, 1932.\n\n- Thomas B. Sch\u00f6n, Adrian Wills, and Brett Ninness. System identification of nonlinear state-space models. Automatica, 47(1):39-49, 2011.\n\n- Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. Conditional simulation using diffusion Schr\u00f6dinger bridges. In 38th Conference on Uncertainty in Artificial Intelligence. UAI, 2022.\n\n- Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. In Advances in Neural Information Processing Systems 35 (NeurIPS), pp. 1415-1428. Curran Associates, Inc., 2021a.\n\n- Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations (ICLR), 2021b.\n\n- Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. In International Conference on Learning Representations (ICLR), 2022.\n\n- Andreas Svensson and Thomas B. Sch\u00f6n. A flexible state-space model for learning nonlinear dynamical systems. Automatica, 80:189-199, 2017.\n\n- Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. CSDI: Conditional score-based diffusion models for probabilistic time series imputation. In Advances in Neural Information Processing Systems 35 (NeurIPS), pp. 24804-24816. Curran Associates, Inc., 2021.\n\n- Evangelos A. Theodorou Tianrong Chen, Guan-Horng Liu. Likelihood training of schr\u00f6dinger bridge using forward-backward SDEs theory. In International Conference on Learning Representations (ICLR), 2022.\n\n- Emanuel Todorov. General duality between optimal control and estimation. In Proceedings of the 47th IEEE Conference on Decision and Control, pp. 4286-4292, 2008.\n\n- Alexander Tong, Jessie Huang, Guy Wolf, David van Dijk, and Smita Krishnaswamy. Trajectorynet: A dynamic optimal transport network for modeling cellular dynamics. In ICML, volume 119 of Proceedings of Machine Learning Research, pp. 9526-9536. PMLR, 2020.\n\n- Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving Schr\u00f6dinger bridges via maximum likelihood. Entropy, 23(9):1134, 2021.\n\n- Pascal Vincent. A connection between score matching and denoising autoencoders. *Neural Computation*, 23(7):1661-1674, 2011.\n\nResponse: Outcome: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "TextualGradientDescent prompt for update", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "165", "funcName": "_update_prompt", "created": "1768638643.0613768", "msecs": "61.0", "relativeCreated": "239891.8743133545", "thread": "139686315202368", "threadName": "MainThread", "processName": "MainProcess", "process": "2102475", "prompt": "Here is the role of the variable you will improve: <ROLE>System prompt that sets the AI reviewer's role and approach for paper evaluation</ROLE>.\n\nThe variable is the text within the following span: <VARIABLE> You are an expert academic reviewer evaluating research papers. Assess (...) Predict acceptance based on these factors with \\boxed{Accept} or \\boxed{Reject}. </VARIABLE>\n\nHere is the context and feedback we got for the variable:\n\n<CONTEXT></CONTEXT>\n\nImprove the variable (System prompt that sets the AI reviewer's role and approach for paper evaluation) using the feedback provided in <FEEDBACK> tags.\nYou must follow the following constraints:\n\n<CONSTRAINTS>Constraint 1: Keep the system prompt under 500 characters\nConstraint 2: Keep the human prefix under 800 characters\nConstraint 3: The human prefix MUST retain the output format instruction: \\boxed{Accept} or \\boxed{Reject}\nConstraint 4: The human prefix should clearly state the task is predicting paper acceptance</CONSTRAINTS>\n\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.", "message": "TextualGradientDescent prompt for update"}
