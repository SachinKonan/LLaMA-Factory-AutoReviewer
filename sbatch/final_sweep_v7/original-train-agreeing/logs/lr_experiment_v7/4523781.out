==============================================
Job: 4523781
Model: Qwen/Qwen2.5-7B-Instruct
Dataset: iclr_2020_2023_2025_85_5_10_split7_balanced_trainagreeing_original_text_binary_noreviews_v7
Learning Rate: 1.75e-6
Epochs: 3
Effective batch: 16
Model Dir: saves/lr_experiment_v7/trainagreeing_original_no2024_text_cls
==============================================
INFO 02-06 09:31:15 [__init__.py:241] Automatically detected platform cuda.
INFO 02-06 09:31:15 [__init__.py:241] Automatically detected platform cuda.
[INFO|2026-02-06 09:31:19] llamafactory.hparams.parser:479 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-02-06 09:31:19] llamafactory.hparams.parser:479 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-02-06 09:31:19] llamafactory.data.loader:143 >> Loading dataset iclr_2020_2023_2025_85_5_10_split7_balanced_trainagreeing_original_text_binary_noreviews_v7_train/data.json...
[INFO|2026-02-06 09:31:23] llamafactory.data.loader:143 >> Loading dataset iclr_2020_2023_2025_85_5_10_split7_balanced_trainagreeing_original_text_binary_noreviews_v7_test/data.json...
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 458, 6203, 14250, 55514, 50820, 448, 37563, 3412, 15689, 13, 151645, 198, 151644, 872, 198, 40, 1079, 7086, 498, 264, 5567, 13, 358, 1366, 311, 7023, 1181, 25505, 15274, 518, 358, 74058, 624, 481, 4615, 4226, 686, 2987, 387, 25, 1124, 79075, 90, 16646, 92, 476, 1124, 79075, 90, 78413, 532, 481, 7036, 25, 358, 74058, 8789, 702, 264, 3968, 18, 15, 4, 25505, 4379, 1406, 2, 20952, 6784, 1718, 6517, 952, 3495, 11148, 12630, 4769, 9630, 72226, 934, 6208, 31640, 38264, 5190, 77373, 14017, 1406, 32684, 12014, 1406, 30898, 1212, 1990, 94349, 3395, 1406, 2, 14137, 74592, 1406, 1427, 7556, 6832, 646, 22800, 10186, 553, 44971, 11647, 11, 892, 33003, 264, 3491, 8826, 4842, 12345, 13, 62248, 11, 1741, 438, 3738, 11, 476, 10186, 389, 26075, 28707, 11, 646, 678, 387, 54119, 2041, 14433, 504, 279, 7824, 13, 2585, 646, 582, 6016, 1039, 33233, 10186, 504, 77866, 553, 458, 9250, 22067, 30, 2014, 4226, 419, 3405, 582, 19131, 264, 501, 71278, 6832, 12626, 11, 1380, 582, 5426, 458, 39026, 315, 3143, 56112, 2861, 10186, 11, 6693, 43795, 525, 19323, 311, 387, 28848, 369, 458, 9250, 22067, 13, 1205, 88859, 419, 4522, 553, 264, 53752, 25262, 3491, 11, 1380, 279, 16538, 374, 311, 7269, 33233, 10186, 11, 323, 518, 279, 1852, 882, 38336, 349, 279, 4108, 4842, 315, 458, 41735, 9250, 22067, 13, 1205, 2884, 264, 41829, 480, 12111, 311, 11625, 419, 501, 25262, 3491, 553, 46041, 279, 5297, 4842, 20169, 12111, 13, 5633, 54886, 646, 387, 32298, 304, 34957, 315, 64195, 323, 28411, 42721, 16999, 11, 892, 20081, 264, 26829, 13057, 315, 419, 975, 13, 1205, 19869, 279, 13885, 315, 330, 6280, 30617, 263, 480, 1, 662, 40694, 11, 8241, 264, 6291, 311, 279, 3403, 25262, 3491, 11, 892, 374, 16588, 553, 1039, 10807, 4842, 20169, 12111, 13, 2014, 1039, 6540, 11, 419, 374, 279, 1156, 975, 8826, 279, 9135, 315, 10186, 304, 46069, 10927, 20909, 4192, 2, 220, 16, 9221, 90099, 1406, 1427, 7556, 6832, 323, 35595, 77866, 3410, 2167, 3746, 5726, 311, 1855, 7988, 10186, 11, 438, 3884, 304, 58001, 9079, 320, 43803, 792, 1842, 452, 2572, 220, 17, 15, 16, 22, 26, 34062, 1842, 452, 2572, 220, 17, 15, 16, 22, 26, 6119, 63338, 1842, 452, 2572, 220, 17, 15, 16, 24, 26, 220, 17, 15, 16, 22, 26, 393, 25359, 273, 2863, 11, 220, 16, 24, 23, 23, 26, 2516, 73, 1561, 6642, 1842, 452, 2572, 220, 17, 15, 16, 21, 568, 6944, 5043, 614, 7881, 5413, 311, 5978, 12345, 320, 2101, 10911, 30429, 645, 609, 43372, 11, 220, 17, 15, 16, 24, 26, 31643, 932, 354, 1842, 452, 2572, 220, 17, 15, 16, 21, 701, 4764, 11, 1741, 975, 653, 537, 3010, 9135, 2348, 4842, 77866, 4192, 641, 419, 975, 11, 582, 21403, 279, 4265, 315, 22184, 10186, 553, 4862, 10186, 429, 9213, 311, 5358, 458, 9250, 22067, 504, 1667, 16999, 77866, 13, 5633, 5486, 26643, 19760, 504, 732, 49544, 3738, 11647, 11, 879, 646, 3143, 12, 19133, 745, 22054, 2661, 9079, 13, 576, 6243, 892, 582, 23643, 374, 10449, 304, 19037, 220, 16, 13, 1205, 6426, 311, 1477, 264, 4426, 315, 11647, 11, 892, 438, 458, 39026, 646, 2736, 264, 2661, 3383, 1632, 11, 4764, 11, 1083, 11550, 16999, 77866, 1526, 28411, 42721, 16999, 13, 13293, 22845, 374, 429, 419, 4426, 315, 11647, 10868, 279, 11785, 1142, 15048, 369, 16999, 77866, 389, 1246, 311, 2736, 264, 3383, 330, 18536, 3322, 1, 4192, 1427, 7556, 6832, 48025, 8789, 1281, 3654, 31846, 315, 279, 7553, 2719, 315, 279, 43795, 320, 57, 645, 40579, 1842, 452, 2572, 220, 17, 15, 15, 23, 26, 80602, 11, 220, 17, 15, 16, 23, 701, 3602, 2581, 6509, 279, 15048, 979, 279, 11647, 11689, 4774, 311, 387, 28411, 42721, 311, 279, 732, 275, 850, 13, 1205, 17040, 279, 11514, 3405, 8826, 419, 24335, 25, 1558, 1052, 3000, 264, 738, 315, 11647, 429, 525, 28411, 42721, 311, 458, 9250, 22067, 4460, 311, 16999, 14715, 30831, 1654, 29614, 2410, 3004, 42721, 10974, 2925, 40694, 320, 68280, 701, 264, 1714, 429, 24303, 7553, 4756, 279, 5068, 315, 279, 39026, 323, 21187, 4756, 279, 5068, 315, 10186, 9583, 12180, 504, 77866, 432, 13, 5633, 21204, 1473, 429, 362, 1740, 653, 537, 7676, 1753, 5068, 4709, 504, 458, 22823, 4842, 11, 1393, 14381, 11, 389, 5461, 11, 279, 54119, 4842, 311, 3139, 916, 220, 20, 3039, 52468, 7707, 311, 279, 22823, 4842, 4192, 7981, 1887, 19026, 646, 387, 68629, 438, 11017, 24391, 20703, 22029, 220, 16, 25, 73365, 487, 12859, 25, 13727, 11954, 4862, 11, 29436, 264, 10974, 88357, 553, 76372, 52149, 1667, 2176, 279, 10186, 304, 279, 39026, 323, 279, 88107, 64695, 22067, 4842, 13, 10083, 3197, 25895, 264, 10337, 369, 77866, 11, 279, 2266, 3890, 374, 75720, 700, 13, 14301, 77866, 279, 10974, 88357, 646, 1102, 304, 264, 28848, 4842, 9533, 3642, 16546, 17106, 24, 24, 21, 23, 21, 23, 17, 68, 19, 17, 65, 2940, 66, 22, 67, 19, 22, 66, 22, 55088, 15, 20, 67, 17, 21, 21, 757, 69, 21, 64, 16, 16, 18, 24, 22, 21, 16, 21, 20, 68, 23, 24, 19, 924, 17, 16, 64, 17, 10798, 20, 68, 22, 67, 23, 4819, 692, 12, 481, 1205, 19131, 264, 11514, 1714, 362, 1740, 11, 438, 1632, 438, 279, 35972, 41435, 315, 279, 22240, 315, 28411, 42721, 11647, 382, 12, 481, 3216, 46041, 10974, 53107, 320, 50, 959, 1842, 452, 2572, 220, 17, 15, 15, 15, 701, 264, 4185, 71278, 6832, 12111, 11, 582, 4190, 264, 41829, 480, 12859, 369, 9271, 458, 22823, 6291, 369, 419, 16538, 382, 12, 481, 1205, 19869, 279, 6291, 553, 34776, 46490, 11, 1380, 582, 1473, 429, 264, 54119, 4842, 374, 97952, 1496, 1283, 25895, 264, 11941, 3460, 1372, 315, 10469, 504, 264, 4842, 39026, 4192, 1249, 1039, 6540, 11, 537, 1172, 374, 419, 279, 1156, 975, 8826, 279, 9135, 315, 10186, 304, 71278, 6832, 11, 714, 432, 374, 1083, 279, 1156, 311, 4009, 28411, 42721, 11647, 4192, 2, 220, 17, 20952, 43, 1791, 687, 934, 5369, 1406, 1654, 2225, 362, 1740, 304, 279, 5297, 12626, 315, 46069, 10927, 20909, 320, 4728, 568, 576, 1887, 6813, 582, 990, 525, 4389, 859, 40425, 62303, 11, 10974, 53107, 320, 50, 959, 1842, 452, 2572, 220, 17, 15, 15, 15, 701, 4842, 662, 40694, 11, 323, 16999, 77866, 11, 892, 582, 3395, 3685, 4192, 2, 220, 17, 13, 16, 18018, 38957, 42817, 24211, 51437, 1406, 32, 43179, 7246, 34226, 2832, 269, 16973, 47674, 4389, 859, 5480, 1882, 320, 6076, 47, 8, 220, 57960, 10374, 5416, 90, 44, 31716, 220, 374, 4512, 553, 220, 4930, 50, 11, 1124, 10374, 5416, 90, 32, 2137, 435, 11, 281, 11, 281, 62, 15, 11, 1124, 32214, 11, 350, 15087, 220, 1380, 220, 57960, 10374, 5416, 90, 50, 31716, 220, 374, 279, 1584, 3550, 11, 220, 57960, 10374, 5416, 90, 32, 31716, 220, 374, 279, 1917, 3550, 11, 220, 400, 81, 25, 328, 1124, 15136, 1124, 10374, 5416, 90, 32, 92, 1124, 983, 1124, 10374, 6066, 90, 49, 31716, 220, 374, 279, 11313, 729, 11, 220, 400, 79, 1141, 15159, 83, 10, 16, 52398, 82, 528, 11, 264, 528, 15087, 220, 374, 279, 9142, 18927, 7982, 11, 220, 400, 79, 62, 15, 25, 328, 1124, 983, 1124, 10374, 6066, 90, 49, 92, 61, 31719, 220, 374, 279, 2856, 1584, 7982, 11, 220, 57960, 32214, 1124, 258, 320, 15, 11, 16, 15087, 220, 374, 279, 11089, 8168, 11, 323, 220, 400, 51, 3, 220, 374, 279, 882, 34074, 13, 362, 34682, 220, 57960, 29143, 1124, 14781, 1124, 40322, 15159, 59, 2493, 31716, 1154, 48876, 504, 220, 400, 79, 3, 220, 323, 264, 4842, 220, 57960, 2493, 25, 328, 1124, 15136, 1124, 10374, 5416, 90, 32, 92, 1124, 983, 1124, 10374, 6066, 90, 49, 92, 61, 31719, 1154, 374, 4512, 311, 387, 279, 5302, 323, 6168, 14405, 220, 4930, 82, 62, 15, 11, 264, 62, 15, 11, 1124, 507, 2412, 11, 274, 15159, 51, 12, 16, 2137, 264, 15159, 51, 12, 16, 2137, 274, 1139, 15087, 1154, 6693, 7982, 374, 31871, 553, 220, 57960, 40322, 15159, 59, 2493, 31716, 659, 18614, 279, 470, 315, 264, 34682, 311, 387, 220, 400, 81, 11520, 29143, 8, 284, 1124, 1242, 15159, 83, 28, 15, 92, 47822, 51, 12, 16, 92, 1124, 32214, 61, 83, 435, 1141, 528, 11, 264, 528, 15087, 220, 311, 387, 279, 2629, 315, 47674, 21160, 3884, 3156, 279, 34682, 11, 323, 6979, 264, 897, 729, 220, 400, 53, 24884, 2493, 25, 328, 1124, 983, 1124, 10374, 6066, 90, 49, 31716, 220, 311, 387, 3601, 470, 315, 264, 34682, 5916, 504, 1584, 220, 400, 82, 3, 1154, 1212, 279, 4842, 220, 57960, 2493, 3, 659, 576, 5795, 315, 71278, 6832, 374, 311, 1477, 264, 4842, 429, 30027, 4756, 279, 3601, 470, 220, 57960, 10374, 6066, 90, 36, 51535, 59, 29143, 1124, 14781, 1124, 40322, 15159, 59, 2493, 3417, 58, 81, 11520, 29143, 7252, 3, 220, 4192, 2, 220, 17, 13, 17, 31640, 38264, 14773, 1808, 10757, 1406, 13825, 53107, 320, 11383, 8, 320, 50, 959, 1842, 452, 2572, 220, 17, 15, 15, 15, 8, 9213, 311, 5961, 3960, 279, 22823, 4842, 220, 57960, 2493, 3, 1154, 5733, 1506, 553, 220, 57960, 15976, 3, 1154, 553, 18822, 76372, 279, 20169, 315, 279, 3601, 470, 11, 304, 825, 315, 1657, 7586, 11, 6839, 304, 52990, 1515, 1842, 452, 13, 320, 17, 15, 16, 20, 568, 758, 1039, 975, 11, 582, 1795, 44197, 4428, 311, 429, 315, 320, 30124, 360, 1515, 1842, 452, 2572, 220, 17, 15, 16, 20, 26, 220, 17, 15, 16, 22, 8, 323, 16045, 220, 57960, 77, 370, 4260, 15159, 59, 15976, 92, 1124, 10374, 6066, 90, 36, 51535, 59, 29143, 1124, 14781, 1124, 40322, 15159, 59, 2493, 3417, 58, 81, 11520, 29143, 7252, 3, 220, 1667, 279, 9423, 11, 892, 374, 12943, 504, 264, 34682, 220, 57960, 29143, 3, 1154, 220, 400, 32, 15159, 59, 29143, 92, 61, 35702, 2493, 25547, 83, 8, 284, 431, 15159, 59, 29143, 25547, 83, 8, 481, 647, 61, 35702, 2493, 25547, 82, 528, 15087, 1154, 1380, 220, 400, 49, 15159, 59, 29143, 25547, 83, 8, 284, 1124, 1242, 15159, 83, 60285, 83, 92, 47822, 51, 12, 16, 92, 1124, 32214, 47822, 83, 8275, 435, 1141, 15159, 83, 24731, 264, 15159, 83, 65260, 3, 220, 374, 279, 2629, 315, 279, 11313, 2701, 1917, 220, 400, 64, 528, 3, 220, 4192, 8420, 11, 279, 897, 729, 374, 9498, 24303, 448, 279, 4842, 11, 323, 773, 279, 9423, 686, 990, 220, 57960, 9198, 90, 53, 92, 61, 35702, 2493, 31716, 220, 438, 458, 16045, 369, 220, 400, 53, 61, 35702, 2493, 31716, 220, 4192, 2, 220, 17, 13, 18, 31640, 38264, 5190, 77373, 867, 320, 1740, 3623, 32, 10974, 88357, 320, 1740, 8, 374, 4428, 311, 279, 22240, 315, 37597, 323, 7361, 320, 32, 331, 4932, 1842, 452, 2572, 220, 17, 15, 16, 23, 26, 468, 1047, 268, 34538, 1842, 452, 2572, 220, 17, 15, 16, 23, 26, 60015, 1842, 452, 2572, 220, 17, 15, 16, 24, 8, 892, 582, 4263, 304, 11113, 220, 18, 13, 1205, 78064, 264, 21804, 553, 220, 57960, 2493, 15159, 59, 10374, 13233, 90, 66, 3417, 3, 1154, 1380, 1817, 220, 57960, 2493, 15159, 66, 47822, 7, 72, 9139, 2137, 600, 1124, 258, 1124, 90, 16, 11, 220, 17, 11, 1124, 67816, 308, 59, 31716, 220, 10868, 458, 6203, 13, 2014, 71730, 279, 21804, 11, 458, 6203, 374, 11882, 518, 4194, 320, 258, 1039, 1142, 13794, 701, 323, 279, 6203, 44595, 264, 34682, 13, 8886, 6203, 4842, 220, 57960, 2493, 15159, 66, 47822, 7, 72, 52731, 7, 64, 91, 82, 15087, 220, 646, 387, 19334, 438, 264, 4842, 65631, 389, 264, 41667, 3890, 220, 400, 66, 3, 1154, 220, 57960, 2493, 2877, 91, 82, 11, 272, 15087, 220, 4192, 15802, 220, 57960, 2493, 15159, 59, 10374, 13233, 90, 66, 3417, 3, 220, 17167, 315, 5248, 10186, 11, 432, 374, 2989, 311, 5185, 429, 432, 5086, 374, 2058, 264, 4842, 4192, 2, 220, 17, 13, 19, 7206, 72239, 3810, 1511, 6976, 711, 1718, 1406, 1249, 16999, 14715, 458, 6203, 4842, 320, 54, 307, 651, 609, 467, 13, 9082, 11, 220, 16, 24, 21, 19, 701, 264, 10337, 315, 85548, 220, 57960, 10374, 5416, 90, 35, 31716, 220, 30606, 315, 1584, 1917, 13530, 220, 4930, 82, 15012, 15087, 220, 525, 14548, 504, 279, 279, 6203, 6502, 11672, 13, 5005, 11, 264, 4842, 1685, 295, 1506, 553, 220, 57960, 16827, 3, 220, 374, 16176, 311, 34508, 279, 28636, 315, 458, 1917, 2661, 264, 1584, 11, 220, 57960, 1242, 15159, 7, 82, 15012, 10699, 258, 1124, 10374, 5416, 90, 35, 3417, 481, 1124, 839, 1124, 2493, 15159, 59, 16827, 25547, 64, 59, 16106, 274, 15087, 220, 4192, 4498, 77866, 220, 57960, 2493, 15159, 59, 10374, 13233, 90, 66, 3417, 3, 1154, 220, 57960, 10374, 5416, 90, 35, 31716, 220, 686, 537, 6644, 1995, 315, 279, 41667, 3890, 220, 400, 66, 3, 1154, 323, 773, 279, 54119, 4842, 686, 31773, 551, 432, 700, 13, 14301, 11, 279, 22067, 686, 14715, 24391, 14085, 271, 14085, 198, 59, 2493, 62, 314, 78, 92, 320, 64, 1124, 16106, 274, 8, 1669, 1124, 1242, 62, 314, 72, 92, 281, 1124, 2359, 1337, 6306, 32798, 72, 9139, 1124, 16106, 274, 59, 1291, 8, 1124, 2493, 62, 314, 66, 6306, 32798, 72, 52731, 320, 64, 1124, 16106, 274, 8, 1124, 4578, 314, 16, 532, 14085, 271, 14085, 1406, 1654, 8464, 429, 419, 4842, 1558, 537, 3000, 3080, 220, 57960, 2493, 15159, 59, 10374, 13233, 90, 66, 3417, 3, 220, 374, 16999, 54119, 13, 220, 57960, 2493, 14179, 3, 220, 374, 264, 88107, 64695, 4842, 311, 4009, 1128, 1035, 3537, 304, 279, 1850, 1142, 15048, 315, 279, 22067, 3432, 2615, 311, 23809, 821, 504, 220, 57960, 2493, 15159, 59, 10374, 13233, 90, 66, 3417, 3, 220, 311, 14715, 1119, 220, 57960, 2493, 14179, 3, 220, 4192, 785, 6891, 315, 419, 5567, 374, 311, 11689, 5358, 35595, 77866, 504, 72720, 13, 6944, 86840, 6832, 19827, 1741, 438, 27949, 71278, 6832, 320, 5830, 1371, 301, 609, 16619, 11, 220, 17, 15, 15, 19, 26, 16619, 609, 24909, 11, 220, 17, 15, 15, 15, 26, 80602, 1842, 452, 2572, 220, 17, 15, 16, 16, 8, 323, 28411, 42721, 86840, 6832, 320, 47978, 609, 9740, 1645, 11, 220, 17, 15, 16, 21, 26, 51050, 1842, 452, 2572, 220, 17, 15, 16, 23, 8, 1373, 6502, 11672, 315, 2477, 18376, 529, 10186, 304, 279, 4573, 11, 892, 1231, 387, 33248, 11, 323, 8450, 525, 537, 6509, 4192, 2, 220, 18, 84347, 31804, 1406, 2589, 3004, 42721, 66142, 304, 47496, 25, 5633, 22240, 315, 28411, 42721, 10186, 374, 304, 38649, 292, 2845, 5435, 311, 1008, 28411, 42721, 5413, 429, 2169, 47496, 1741, 438, 8564, 1842, 452, 13, 320, 17, 15, 16, 22, 1215, 14612, 89, 53640, 609, 27882, 404, 320, 17, 15, 16, 22, 701, 429, 912, 28411, 42721, 18212, 9048, 804, 311, 4842, 1946, 13, 6944, 28411, 42721, 8776, 2924, 51136, 279, 7162, 315, 821, 1483, 979, 4862, 47496, 320, 29535, 1842, 452, 2572, 220, 17, 15, 16, 24, 701, 323, 39661, 304, 279, 7299, 41935, 6243, 320, 38, 21510, 1842, 452, 2572, 220, 17, 15, 16, 24, 568, 4354, 11, 1493, 5413, 678, 3042, 438, 4541, 8776, 369, 5257, 6832, 12538, 13, 5633, 1714, 11, 4518, 11, 1494, 3132, 35136, 2348, 77866, 4192, 47832, 304, 47496, 25, 3085, 23935, 311, 9135, 11, 1039, 975, 374, 5435, 311, 40164, 12345, 320, 2101, 10911, 30429, 645, 609, 43372, 11, 220, 17, 15, 16, 24, 568, 98736, 12345, 304, 47496, 646, 387, 1483, 311, 1855, 869, 1207, 77430, 320, 54, 524, 609, 70117, 450, 11, 220, 17, 15, 16, 24, 8, 476, 869, 10186, 320, 33, 5054, 1842, 452, 2572, 220, 17, 15, 16, 21, 701, 892, 614, 869, 11313, 5746, 476, 869, 4842, 16460, 13, 4354, 11, 582, 1035, 1075, 311, 45932, 429, 1039, 24798, 374, 311, 5358, 77866, 11, 323, 8450, 22184, 279, 10186, 11, 4751, 1091, 22184, 264, 10337, 13, 758, 2097, 11, 582, 1281, 279, 24335, 429, 279, 22067, 646, 2736, 16999, 77866, 389, 438, 1753, 821, 438, 12685, 4192, 1427, 7556, 20909, 25, 8704, 582, 25017, 311, 279, 5297, 86840, 6832, 6243, 315, 77866, 504, 264, 10337, 448, 1657, 11647, 8241, 279, 43795, 11, 41667, 7332, 289, 1746, 734, 13, 86840, 6832, 374, 1632, 5477, 661, 1122, 13, 1752, 3110, 11, 6119, 63338, 1842, 452, 13, 320, 17, 15, 16, 22, 8, 1473, 429, 34021, 389, 2266, 13042, 646, 1281, 86840, 6832, 264, 30428, 2999, 369, 38193, 9842, 13, 14511, 1842, 452, 13, 320, 17, 15, 16, 22, 8, 19869, 429, 279, 41667, 65151, 1995, 304, 6203, 85548, 374, 3545, 5234, 80949, 22414, 13, 1634, 1632, 11, 80100, 4960, 2266, 7332, 311, 2971, 389, 1083, 7952, 304, 7586, 315, 4960, 19556, 476, 8241, 9201, 320, 48388, 1842, 452, 2572, 220, 17, 15, 16, 24, 26, 409, 14101, 276, 1842, 452, 2572, 220, 17, 15, 16, 24, 26, 472, 2819, 859, 1842, 452, 2572, 220, 17, 15, 16, 23, 568, 5633, 1714, 374, 2155, 438, 4518, 315, 59157, 369, 2393, 304, 86840, 6832, 11, 582, 3920, 1246, 311, 5358, 432, 4192, 32089, 62248, 25, 320, 32, 331, 4932, 1842, 452, 2572, 220, 17, 15, 16, 23, 26, 468, 1047, 268, 34538, 1842, 452, 2572, 220, 17, 15, 16, 23, 26, 60015, 1842, 452, 2572, 220, 17, 15, 16, 24, 8, 614, 4428, 30856, 315, 24892, 264, 41667, 3890, 323, 35251, 432, 6814, 264, 34682, 11, 7892, 862, 41667, 7332, 320, 71927, 476, 7361, 8, 525, 1483, 311, 11625, 5234, 80949, 2155, 9079, 13, 576, 2874, 311, 11625, 2155, 9079, 374, 4152, 311, 279, 16538, 315, 1667, 279, 2266, 3890, 2687, 60925, 369, 6832, 304, 458, 6975, 454, 77990, 6243, 4192, 7981, 5486, 43542, 304, 2176, 24798, 323, 8129, 11, 438, 582, 3960, 11647, 429, 678, 11625, 279, 1852, 3383, 11, 323, 79699, 773, 429, 36543, 646, 537, 14715, 279, 4842, 4192, 32, 21804, 220, 57960, 2493, 15159, 59, 10374, 13233, 90, 66, 3417, 3, 220, 646, 1083, 387, 19334, 438, 264, 20980, 315, 11647, 320, 82829, 5481, 1842, 452, 2572, 220, 16, 24, 24, 16, 701, 3650, 279, 73399, 3922, 49912, 18927, 220, 16, 311, 279, 1852, 6203, 369, 458, 4453, 34682, 13, 1634, 1741, 11, 582, 653, 537, 3960, 279, 73399, 3922, 11, 7892, 432, 1231, 2058, 387, 5390, 311, 1490, 220, 57960, 2493, 15159, 59, 10374, 13233, 90, 66, 3417, 3, 220, 438, 264, 3281, 1142, 315, 264, 20980, 315, 11647, 1380, 279, 73399, 3922, 46210, 7069, 311, 5046, 279, 6203, 369, 1817, 34682, 13, 2619, 525, 1083, 5413, 1741, 438, 6959, 58487, 320, 39, 408, 1270, 1842, 452, 2572, 220, 17, 15, 16, 23, 701, 892, 5711, 264, 20980, 315, 11647, 1614, 311, 3960, 5248, 10186, 438, 2606, 448, 2615, 311, 1172, 6203, 5302, 4192, 57, 20658, 1842, 452, 13, 320, 17, 15, 16, 24, 8, 1083, 54625, 264, 1714, 311, 5426, 5248, 10186, 429, 4583, 279, 1852, 3383, 714, 5711, 279, 26826, 315, 458, 3233, 27008, 438, 264, 11313, 48706, 13, 10964, 24798, 374, 311, 1477, 5248, 11514, 10186, 11, 1393, 1039, 24798, 702, 902, 3633, 311, 66309, 13, 23662, 311, 1493, 11799, 304, 24798, 11, 807, 5426, 1817, 4842, 825, 1283, 279, 1008, 11, 1393, 1039, 10186, 525, 16176, 24303, 4192, 13825, 662, 40694, 525, 1083, 1483, 304, 279, 7299, 52579, 323, 5795, 65631, 5003, 304, 892, 1142, 279, 3383, 429, 374, 8791, 311, 387, 27956, 646, 387, 19334, 438, 279, 2266, 13, 23201, 977, 4849, 700, 279, 2266, 3890, 320, 24509, 367, 220, 16, 8, 315, 1493, 2266, 58005, 291, 10186, 374, 19476, 304, 279, 1142, 315, 31918, 264, 67710, 81158, 5792, 4849, 4647, 369, 6832, 501, 9079, 320, 38, 47102, 1842, 452, 2572, 220, 17, 15, 16, 24, 8, 323, 369, 11560, 7530, 6577, 8845, 320, 2580, 1530, 1842, 452, 2572, 220, 17, 15, 16, 23, 568, 4354, 11, 279, 1887, 24798, 374, 2155, 304, 429, 2176, 479, 47102, 1842, 452, 13, 320, 17, 15, 16, 24, 1215, 4509, 1530, 1842, 452, 13, 320, 17, 15, 16, 23, 8, 990, 220, 57960, 2493, 15159, 78, 31716, 220, 311, 29436, 26750, 1995, 11, 1393, 582, 5961, 29436, 1181, 5068, 4192, 2, 220, 19, 32397, 1406, 2, 220, 19, 13, 16, 39786, 6537, 1406, 1654, 6426, 311, 614, 11647, 429, 646, 2736, 279, 3383, 11, 1393, 76291, 279, 3204, 4675, 315, 279, 54119, 4842, 11, 3371, 9253, 304, 84670, 220, 16, 13, 1205, 5602, 279, 5297, 47496, 16538, 311, 387, 24391, 14085, 271, 14085, 198, 59, 31009, 295, 28152, 15976, 92, 28152, 858, 1124, 1065, 335, 1124, 10374, 6066, 314, 36, 92, 716, 28152, 29143, 1124, 14781, 1124, 40322, 62, 28152, 2493, 62, 314, 78, 75542, 508, 435, 19788, 29143, 8, 2279, 1124, 31610, 1124, 1318, 314, 82, 659, 259, 659, 92, 1124, 10374, 6066, 314, 36, 92, 716, 28152, 29143, 1124, 14781, 1124, 40322, 62, 28152, 2493, 62, 28152, 91550, 314, 68, 3417, 3417, 508, 435, 19788, 29143, 8, 2279, 1124, 709, 80, 1124, 7141, 1124, 4578, 314, 17, 532, 14085, 271, 14085, 1406, 2870, 220, 57960, 7141, 3, 220, 374, 264, 5733, 429, 4722, 14262, 279, 11313, 315, 279, 4842, 39026, 13, 1096, 46918, 311, 87178, 279, 25062, 57727, 32371, 92583, 1103, 24391, 14085, 271, 14085, 198, 41, 19788, 15976, 8, 284, 1124, 10374, 6066, 314, 36, 92, 716, 28152, 29143, 1124, 14781, 1124, 40322, 62, 28152, 2493, 62, 28152, 10374, 13233, 314, 66, 3417, 3417, 508, 435, 19788, 29143, 8, 2279, 481, 1124, 19127, 1124, 10374, 6066, 314, 36, 92, 716, 28152, 29143, 1124, 14781, 1124, 40322, 62, 28152, 2493, 62, 314, 78, 75542, 508, 435, 19788, 29143, 8, 2279, 1124, 4578, 314, 18, 532, 14085, 271, 14085, 1406, 2870, 220, 400, 16, 608, 1124, 19127, 3, 220, 374, 279, 12159, 32371, 92583, 1103, 30559, 11, 323, 374, 1186, 1242, 291, 1119, 279, 4675, 14548, 553, 279, 4842, 39026, 13, 1205, 8300, 311, 21804, 429, 7553, 4756, 419, 16538, 438, 2410, 3004, 42721, 10974, 2925, 40694, 320, 68280, 568, 2619, 374, 264, 5810, 22845, 315, 279, 16538, 304, 84670, 220, 17, 13, 11097, 11647, 8376, 311, 387, 330, 18536, 3322, 497, 892, 374, 25911, 304, 279, 21568, 13, 576, 21187, 2022, 374, 4936, 9271, 279, 1429, 28411, 42721, 11647, 4192, 15802, 582, 9658, 429, 279, 22067, 646, 1172, 2415, 5302, 311, 6168, 11, 432, 1231, 387, 279, 1142, 429, 807, 646, 5426, 264, 51000, 4842, 11, 892, 374, 17749, 389, 1181, 3681, 5302, 323, 6168, 13, 5633, 1714, 646, 387, 66117, 311, 51000, 10186, 438, 1632, 11, 323, 279, 5421, 315, 1741, 36543, 374, 14078, 304, 279, 11113, 220, 21, 4192, 2, 220, 19, 13, 17, 18669, 27320, 31640, 38264, 14773, 1808, 10757, 8753, 74585, 1406, 1072, 3083, 3132, 11, 2474, 1052, 525, 279, 4675, 315, 1378, 10186, 429, 525, 1660, 33340, 11, 2176, 1265, 387, 48876, 504, 311, 16045, 279, 4675, 4192, 1654, 1473, 1246, 582, 646, 5602, 30044, 311, 5426, 362, 1740, 11, 553, 87178, 84670, 220, 18, 13, 576, 1378, 3793, 4190, 264, 4285, 12859, 311, 16045, 279, 4675, 315, 279, 4842, 39026, 10917, 25, 3055, 1667, 220, 57960, 2493, 15159, 59, 10374, 13233, 90, 66, 3417, 3, 220, 429, 582, 6426, 311, 34508, 11, 323, 264, 2086, 882, 1667, 220, 57960, 2493, 15159, 78, 31716, 1154, 892, 9848, 47471, 279, 4675, 315, 458, 41735, 22067, 879, 16297, 311, 14715, 279, 4842, 39026, 13, 31844, 448, 1039, 21804, 11, 582, 5426, 897, 5746, 220, 57960, 1646, 450, 90, 53, 92, 61, 35702, 2493, 666, 1956, 9139, 3, 220, 369, 1817, 6203, 11, 52158, 5733, 1506, 553, 220, 57960, 16827, 3, 220, 892, 17530, 220, 400, 53, 61, 35702, 2493, 666, 1956, 9139, 481, 1124, 19127, 647, 61, 35702, 2493, 14179, 31716, 659, 576, 4709, 729, 369, 279, 897, 5746, 315, 1378, 48876, 85548, 220, 57960, 29143, 15159, 16, 2137, 1124, 29143, 15159, 17, 31716, 220, 374, 1406, 14085, 271, 14085, 198, 41, 716, 28152, 29143, 62, 314, 16, 2137, 1124, 29143, 62, 314, 17, 3417, 19788, 16827, 8, 284, 1124, 1242, 62, 314, 83, 284, 220, 15, 92, 6306, 314, 51, 716, 314, 16, 92, 481, 220, 16, 92, 1124, 37018, 314, 16, 15170, 17, 92, 1124, 2359, 11520, 1646, 450, 314, 53, 92, 716, 28152, 16827, 92, 6306, 28152, 2493, 62, 314, 66, 92, 320, 72, 9139, 1124, 2359, 1141, 716, 314, 83, 716, 314, 16, 3417, 59, 1291, 8, 481, 431, 716, 28152, 29143, 62, 314, 16, 3417, 320, 83, 10699, 1291, 8, 6306, 314, 17, 92, 488, 1124, 1242, 62, 314, 83, 284, 220, 15, 92, 6306, 314, 51, 716, 314, 17, 92, 481, 220, 16, 92, 1124, 37018, 314, 16, 15170, 17, 92, 1124, 2359, 11520, 1646, 450, 314, 53, 92, 716, 28152, 16827, 92, 6306, 28152, 2493, 62, 314, 66, 92, 320, 72, 9139, 1124, 2359, 1141, 716, 314, 83, 716, 314, 17, 3417, 59, 1291, 8, 488, 1124, 19127, 431, 716, 28152, 29143, 62, 314, 17, 3417, 320, 83, 10699, 1291, 8, 6306, 314, 17, 92, 1124, 4578, 314, 19, 532, 14085, 271, 14085, 1406, 785, 4842, 20169, 2647, 504, 220, 400, 45, 15159, 16, 31716, 220, 323, 220, 400, 45, 15159, 17, 31716, 220, 85548, 374, 1221, 1406, 14085, 271, 14085, 198, 1699, 370, 4260, 62, 28152, 15976, 92, 619, 716, 28152, 29143, 62, 314, 16, 2137, 1124, 29143, 62, 314, 17, 3417, 19788, 15976, 8, 1124, 48053, 479, 716, 314, 16, 92, 488, 479, 716, 314, 17, 92, 1124, 4578, 314, 20, 532, 14085, 271, 14085, 1406, 2870, 1406, 14085, 271, 14085, 198, 38, 716, 314, 16, 92, 284, 1124, 37018, 314, 16, 15170, 45, 716, 314, 16, 3417, 1124, 1242, 62, 314, 73, 284, 220, 16, 92, 6306, 314, 45, 716, 314, 16, 3417, 1124, 1242, 62, 314, 83, 284, 220, 15, 92, 6306, 314, 51, 716, 314, 16, 3417, 1124, 77, 370, 4260, 62, 28152, 15976, 92, 1124, 839, 1124, 2493, 62, 314, 66, 6306, 32798, 72, 52731, 1124, 2359, 2877, 716, 314, 83, 220, 16, 92, 6306, 32798, 73, 9139, 1124, 16106, 274, 716, 314, 83, 220, 16, 92, 6306, 32798, 73, 9139, 59, 1291, 8, 1124, 1646, 450, 314, 32, 92, 716, 28152, 29143, 62, 314, 16, 3417, 6306, 28152, 2493, 62, 314, 66, 6306, 32798, 72, 52731, 6306, 32798, 72, 52731, 320, 83, 8, 1124, 4578, 314, 21, 532, 14085, 271, 14085, 1406, 14085, 271, 14085, 198, 38, 716, 314, 17, 92, 284, 1124, 37018, 314, 16, 15170, 45, 716, 314, 17, 3417, 1124, 1242, 62, 314, 73, 284, 220, 16, 92, 6306, 314, 45, 716, 314, 17, 3417, 1124, 1242, 62, 314, 83, 284, 220, 15, 92, 6306, 314, 51, 716, 314, 17, 3417, 1124, 77, 370, 4260, 62, 28152, 15976, 92, 1124, 839, 1124, 2493, 62, 314, 78, 92, 1124, 2359, 2877, 716, 314, 83, 220, 17, 92, 6306, 32798, 73, 9139, 1124, 16106, 274, 716, 314, 83, 220, 17, 92, 6306, 32798, 73, 9139, 59, 1291, 8, 1124, 1646, 450, 314, 32, 92, 716, 28152, 29143, 62, 314, 17, 3417, 6306, 28152, 2493, 62, 314, 78, 3417, 320, 83, 8, 1124, 4578, 314, 22, 532, 14085, 271, 14085, 1406, 2870, 220, 400, 66, 47822, 7, 72, 9139, 3, 220, 35511, 279, 11882, 6203, 315, 279, 34682, 2572, 323, 220, 57960, 1646, 450, 90, 32, 51535, 59, 29143, 62, 16, 92, 61, 35702, 2493, 666, 47822, 7, 72, 52731, 7, 83, 8, 284, 431, 15159, 59, 29143, 62, 16, 25547, 83, 8, 481, 1124, 1646, 450, 90, 53, 92, 61, 35702, 2493, 15159, 66, 47822, 7, 72, 52731, 25547, 82, 528, 15087, 220, 323, 220, 57960, 1646, 450, 90, 32, 51535, 59, 29143, 62, 17, 92, 61, 35702, 2493, 14179, 25547, 83, 8, 284, 481, 59, 19127, 431, 15159, 59, 29143, 62, 17, 25547, 83, 8, 481, 1124, 1646, 450, 90, 53, 92, 61, 35702, 2493, 14179, 25547, 82, 528, 15087, 220, 525, 279, 10807, 9423, 5746, 13, 576, 220, 400, 30529, 19127, 3, 220, 429, 374, 304, 279, 9423, 304, 220, 400, 38, 15159, 17, 31716, 220, 7553, 4756, 2348, 279, 5068, 315, 279, 13166, 4842, 220, 57960, 2493, 14179, 3, 220, 4192, 785, 20169, 220, 400, 38, 15159, 16, 31716, 220, 369, 220, 57960, 2493, 15159, 59, 10374, 13233, 90, 66, 3417, 3, 220, 374, 30339, 13, 4354, 11, 311, 16045, 279, 20169, 220, 400, 38, 15159, 17, 31716, 220, 369, 220, 57960, 2493, 15159, 78, 31716, 220, 892, 374, 458, 88107, 64695, 4842, 11, 582, 6077, 504, 432, 553, 1156, 312, 1331, 29709, 279, 2266, 315, 279, 6203, 518, 1817, 1584, 11, 323, 1221, 24892, 458, 1917, 504, 279, 2266, 13, 576, 1182, 76571, 27137, 13666, 311, 220, 57960, 2493, 15159, 66, 47822, 7, 72, 52731, 7, 64, 91, 82, 15087, 220, 369, 279, 2266, 48876, 518, 1817, 1584, 13, 65636, 8129, 3565, 646, 387, 1730, 304, 362, 13, 17, 13, 576, 56251, 374, 438, 1795, 13, 5976, 24892, 220, 57960, 2493, 15159, 78, 31716, 1154, 421, 264, 4091, 1917, 11137, 1550, 470, 11, 582, 1265, 18472, 279, 18927, 11, 892, 72015, 279, 3601, 11313, 315, 220, 57960, 2493, 15159, 78, 31716, 659, 57652, 11, 279, 1378, 52149, 686, 5240, 279, 21804, 311, 3293, 6168, 429, 82591, 614, 1550, 11313, 11, 323, 525, 63969, 311, 279, 22067, 4192, 24509, 804, 220, 19, 323, 220, 20, 88859, 1039, 30044, 5486, 315, 362, 1740, 11, 892, 374, 68629, 304, 40325, 220, 16, 4192, 2, 8753, 74585, 220, 16, 25, 30044, 6691, 1740, 1406, 17959, 25, 220, 57960, 15976, 1154, 59, 16827, 1154, 59, 10374, 5416, 90, 44, 2137, 59, 19127, 66426, 12, 220, 16, 25, 369, 1817, 19569, 653, 1447, 12, 220, 17, 25, 19813, 85548, 220, 57960, 29143, 15159, 16, 31716, 220, 448, 220, 57960, 2493, 15159, 59, 10374, 13233, 90, 66, 3417, 3, 220, 504, 220, 57960, 10374, 5416, 90, 44, 31716, 220, 369, 84670, 220, 21, 271, 12, 220, 18, 25, 19813, 85548, 220, 57960, 29143, 15159, 17, 31716, 220, 448, 220, 57960, 2493, 14179, 3, 220, 504, 220, 57960, 10374, 5416, 90, 44, 31716, 220, 369, 84670, 220, 22, 271, 12, 220, 19, 25, 20517, 84670, 220, 20, 311, 2736, 264, 20169, 2647, 389, 279, 21804, 220, 57960, 15976, 1124, 18691, 1124, 15976, 488, 59, 7141, 15159, 59, 15976, 11035, 9198, 35702, 77, 370, 4260, 51535, 59, 15976, 92, 41, 15159, 59, 29143, 62, 16, 26266, 29143, 62, 17, 92, 11520, 15976, 15087, 271, 12, 220, 20, 25, 5549, 279, 897, 729, 220, 57960, 16827, 1124, 18691, 1124, 16827, 481, 59, 7141, 15159, 59, 16827, 11035, 9198, 35702, 77, 370, 4260, 51535, 59, 16827, 92, 41, 15159, 59, 29143, 62, 16, 26266, 29143, 62, 17, 92, 11520, 16827, 15087, 220, 438, 10838, 553, 84670, 220, 19, 4192, 21, 25, 835, 369, 1406, 2, 220, 20, 80081, 1791, 43878, 1406, 1654, 2736, 21204, 389, 264, 10646, 3383, 11, 1380, 279, 16538, 374, 311, 5545, 264, 5795, 1584, 438, 4937, 438, 3204, 13, 576, 7428, 374, 311, 40368, 429, 458, 362, 1740, 646, 5240, 279, 54119, 4842, 311, 1896, 11941, 5021, 311, 5545, 279, 5795, 1584, 13, 1205, 653, 773, 553, 1156, 4862, 264, 21804, 323, 16999, 77866, 432, 13, 1205, 1221, 9429, 279, 5068, 315, 279, 21804, 311, 429, 315, 279, 14715, 13, 1205, 990, 264, 43179, 4573, 311, 1850, 19869, 279, 31839, 315, 279, 23606, 13, 1096, 374, 1576, 678, 43179, 10186, 646, 387, 5733, 1506, 11, 892, 374, 537, 830, 304, 19259, 11, 1380, 11136, 48568, 5733, 2022, 374, 1483, 13, 1634, 1741, 11, 19259, 21737, 1035, 614, 311, 1281, 31846, 911, 1246, 2176, 279, 21804, 323, 279, 1185, 26576, 5733, 4756, 10186, 11, 438, 1632, 438, 21403, 5322, 315, 7982, 278, 33638, 11, 892, 582, 1035, 1075, 311, 5648, 13, 4354, 11, 448, 1493, 31846, 11, 1039, 6243, 646, 13036, 311, 279, 19259, 7947, 13, 758, 1039, 21204, 11, 582, 990, 264, 220, 400, 16, 15, 1124, 15136, 220, 16, 15, 3, 220, 5827, 30084, 4573, 438, 1039, 1887, 1273, 2721, 13, 1096, 374, 311, 614, 3460, 3322, 7493, 429, 1035, 537, 387, 1730, 304, 9155, 56349, 11, 1393, 2058, 2613, 3322, 311, 50087, 279, 16999, 315, 279, 362, 1740, 13, 576, 43179, 6168, 686, 1473, 23638, 1246, 279, 11647, 646, 387, 52158, 28411, 42721, 4192, 16429, 5827, 14615, 6147, 369, 23560, 3601, 470, 17530, 13, 758, 458, 4573, 1380, 7877, 8460, 4675, 4157, 387, 16588, 11, 56204, 1465, 646, 46276, 1526, 76372, 279, 4675, 315, 1406, 20703, 22029, 220, 17, 25, 90760, 315, 362, 1740, 13, 1205, 738, 220, 57960, 19127, 284, 220, 15, 13, 21, 3, 659, 1644, 1811, 13216, 1917, 48216, 11, 323, 279, 12463, 5452, 10868, 279, 19558, 882, 13, 25462, 14807, 3601, 11313, 315, 220, 15, 11, 1393, 24932, 14807, 3601, 11313, 315, 220, 400, 12, 16, 15, 15, 3, 1154, 892, 374, 279, 7192, 9234, 3084, 13, 576, 1909, 2115, 9131, 374, 279, 5795, 1584, 11, 323, 279, 23942, 5302, 429, 525, 24932, 525, 458, 3110, 315, 1246, 362, 1740, 374, 28411, 42721, 311, 77866, 11, 438, 1846, 5302, 686, 5240, 279, 54119, 4842, 311, 7676, 8131, 17683, 13, 9533, 3642, 3470, 69, 23, 24, 1859, 15, 16, 23, 86629, 18, 20, 15, 15, 67, 19, 24, 21, 66, 19, 69, 20, 17, 17, 17, 67, 18, 64, 16, 19, 65, 16, 21, 19, 65, 22, 17, 65, 22, 65, 22, 10798, 23, 18, 67, 20, 580, 20, 18, 21, 23, 20, 67, 16, 17, 18, 24, 68, 24, 19, 4819, 3623, 21028, 279, 16176, 21804, 323, 279, 14715, 13, 1096, 11980, 1035, 1172, 5263, 304, 19259, 1584, 3550, 11, 1380, 279, 4675, 315, 220, 57960, 2493, 15159, 78, 31716, 220, 1231, 537, 387, 41829, 480, 311, 16045, 4152, 311, 4714, 1741, 438, 7982, 278, 33638, 320, 92138, 1842, 452, 2572, 220, 17, 15, 16, 15, 26, 6119, 63338, 1842, 452, 2572, 220, 17, 15, 16, 24, 26, 409, 14101, 276, 1842, 452, 2572, 220, 17, 15, 16, 24, 49555, 7981, 3059, 4226, 279, 2701, 4755, 13, 2585, 1753, 7553, 2719, 374, 43400, 30, 2585, 28848, 646, 582, 1281, 279, 54119, 4842, 30, 2160, 432, 3204, 311, 990, 2477, 362, 1740, 311, 5358, 16999, 77866, 30831, 2, 220, 20, 13, 16, 67509, 1718, 1406, 13159, 3498, 1039, 1714, 646, 12564, 264, 4842, 39026, 448, 894, 34226, 1372, 315, 11647, 11, 582, 14554, 311, 50087, 264, 6291, 448, 220, 17, 11647, 11, 892, 374, 14016, 311, 16400, 279, 7565, 5888, 315, 279, 1714, 13, 44763, 11, 582, 5426, 220, 400, 77, 284, 220, 17, 3, 220, 5651, 1276, 11647, 448, 30044, 6691, 1740, 13, 5633, 2038, 374, 5326, 304, 26036, 4965, 320, 5830, 2767, 1842, 452, 2572, 220, 17, 15, 16, 21, 701, 323, 686, 387, 17424, 2500, 389, 32095, 13, 16151, 3565, 323, 17071, 12, 13786, 525, 304, 11113, 362, 13, 16, 315, 279, 64714, 4192, 2, 220, 20, 13, 17, 32791, 59839, 1406, 785, 6770, 4573, 374, 264, 220, 400, 16, 15, 1124, 15136, 220, 16, 15, 3, 220, 5827, 11, 448, 279, 5795, 1584, 518, 279, 1909, 2115, 9131, 13, 576, 8315, 92676, 304, 264, 4194, 2477, 92288, 1584, 11, 323, 3625, 1723, 264, 11313, 315, 220, 400, 12, 16, 3, 220, 369, 1817, 882, 29208, 3080, 432, 24491, 279, 5795, 13, 2411, 279, 5795, 1584, 11, 279, 8315, 902, 5021, 21189, 264, 4709, 323, 87611, 279, 9234, 13, 576, 8315, 374, 5420, 4236, 6168, 11, 220, 57960, 10374, 5416, 90, 32, 92, 284, 1124, 90, 2324, 11, 6285, 11, 13727, 11, 10083, 11, 28737, 59, 31716, 659, 37627, 1119, 279, 7002, 374, 13578, 311, 30220, 264, 28737, 1917, 13, 1205, 5157, 419, 11313, 729, 369, 279, 8760, 315, 3432, 264, 2797, 13042, 315, 279, 22240, 315, 330, 18536, 3322, 497, 892, 374, 25911, 304, 1246, 1293, 432, 4990, 311, 5545, 279, 5795, 1584, 13, 20035, 1741, 13042, 37939, 9606, 1246, 279, 362, 1740, 646, 5358, 458, 22067, 504, 77866, 264, 1661, 4842, 4192, 2, 220, 20, 13, 18, 59491, 16383, 49025, 1406, 22029, 220, 17, 4933, 458, 3110, 315, 264, 21804, 429, 374, 16176, 369, 279, 6770, 5827, 14615, 4573, 13, 19037, 220, 18, 4933, 279, 12159, 54119, 4842, 11, 438, 1632, 438, 264, 12313, 311, 458, 22823, 4842, 13, 576, 12463, 5452, 10868, 279, 3601, 470, 315, 5916, 518, 264, 2661, 1584, 4192, 641, 279, 1142, 315, 458, 22823, 4842, 220, 4930, 59, 19127, 284, 220, 15, 15087, 1154, 6168, 525, 4429, 311, 1896, 279, 8315, 311, 279, 5795, 1584, 438, 4937, 438, 3204, 13, 4354, 11, 979, 220, 57960, 19127, 861, 220, 15, 3, 1154, 1741, 264, 6291, 374, 902, 5021, 279, 53667, 13, 21476, 311, 220, 57960, 19127, 284, 220, 15, 3, 1154, 279, 11647, 1035, 1075, 311, 34508, 279, 3601, 11313, 11, 323, 5545, 279, 5795, 1584, 13, 4354, 11, 311, 29337, 279, 11313, 315, 279, 13166, 4842, 11, 279, 1378, 6203, 10186, 1969, 52158, 3960, 311, 5263, 279, 1372, 1406, 1055, 7354, 4362, 369, 220, 57960, 2493, 14179, 3, 220, 311, 5545, 279, 5795, 1584, 13, 576, 6203, 10186, 1969, 990, 28411, 42721, 16999, 1393, 18739, 279, 5795, 1584, 11, 1741, 438, 4633, 24514, 3392, 2471, 476, 28737, 304, 279, 1852, 1584, 11, 892, 525, 9498, 311, 56680, 220, 57960, 2493, 14179, 3, 220, 438, 1753, 438, 3204, 13, 4220, 49250, 70077, 5240, 279, 54119, 4842, 311, 1896, 264, 47763, 5021, 882, 311, 5545, 279, 5795, 13, 1752, 3110, 11, 5185, 279, 1378, 24932, 31340, 518, 279, 1909, 7950, 3143, 279, 5795, 11, 892, 14807, 429, 279, 11647, 3535, 429, 807, 1265, 537, 3271, 311, 5358, 279, 22067, 504, 1619, 2056, 11313, 13, 7418, 3498, 1493, 1186, 56112, 2861, 11181, 525, 1865, 11, 389, 30193, 11, 279, 11647, 525, 330, 1921, 3873, 1, 323, 11075, 458, 5461, 315, 220, 400, 12, 16, 20, 13, 17, 22, 3, 220, 11313, 4192, 2, 220, 20, 13, 19, 22194, 43, 33608, 1406, 1654, 990, 16999, 77866, 311, 14715, 1039, 30044, 6691, 1740, 16176, 10186, 13, 2014, 1824, 1039, 8186, 315, 26160, 1496, 304, 279, 34074, 315, 23809, 821, 11, 582, 6530, 264, 3526, 259, 76632, 315, 279, 16176, 21804, 304, 279, 4573, 13, 15533, 3565, 315, 16999, 77866, 525, 304, 279, 87577, 13, 1417, 779, 304, 19037, 220, 18, 374, 458, 22823, 4842, 11, 323, 279, 12942, 54119, 4842, 504, 11113, 220, 20, 13, 16, 4192, 2121, 1632, 11, 582, 15442, 2348, 1008, 21804, 11, 311, 1473, 429, 26160, 2348, 16999, 77866, 374, 2477, 10188, 26658, 13, 1205, 990, 3807, 3046, 10999, 13, 1205, 1156, 1273, 10186, 429, 614, 13187, 279, 1852, 470, 438, 1039, 39026, 553, 4862, 30044, 448, 11, 323, 14761, 1280, 4124, 4751, 1091, 4303, 3080, 55441, 13, 758, 279, 30394, 12, 21367, 2861, 1142, 11, 582, 10613, 3080, 279, 21804, 1030, 3601, 4675, 429, 18054, 279, 5461, 16994, 553, 1039, 1714, 13, 81571, 11, 330, 13999, 1, 10186, 525, 1483, 438, 264, 12313, 311, 1473, 429, 432, 374, 3204, 311, 5240, 279, 54119, 4842, 311, 653, 30455, 11, 714, 279, 6559, 1847, 374, 429, 279, 21804, 5086, 4157, 2736, 1632, 11, 892, 374, 76244, 13, 4220, 10186, 525, 1083, 10186, 16176, 448, 30044, 11, 3650, 807, 525, 10497, 1753, 6788, 11, 979, 862, 65907, 9071, 279, 3601, 4675, 315, 1039, 30044, 6691, 1740, 13, 1752, 1817, 30044, 6691, 1740, 11, 582, 990, 220, 400, 77, 284, 220, 17, 3, 220, 2155, 5651, 1276, 10186, 11758, 438, 458, 39026, 11, 892, 1406, 20703, 22029, 220, 18, 25, 90760, 315, 279, 54119, 362, 1740, 13, 576, 4842, 12180, 504, 77866, 279, 362, 1740, 16176, 702, 5461, 3601, 11313, 315, 220, 400, 12, 19, 20, 13, 16, 23, 3, 1154, 1393, 279, 22823, 4842, 702, 458, 5461, 3601, 11313, 315, 220, 400, 12, 24, 3, 1154, 892, 374, 916, 264, 220, 400, 20, 4955, 1733, 3, 220, 5263, 13, 9533, 3642, 6663, 19, 4645, 19, 346, 24, 67, 22, 20, 15, 23, 17, 5305, 20, 68, 16, 17, 17, 346, 3632, 68, 1371, 22, 2940, 23, 20, 68, 21, 18, 21, 9792, 17, 18, 69, 18, 68, 16, 69, 23, 66, 19, 18, 23, 15, 6902, 17, 20, 16, 66, 18, 22, 17, 24, 23, 69, 4819, 3623, 896, 1221, 14715, 11, 323, 5461, 3941, 220, 20, 19056, 13, 1752, 279, 3046, 10999, 11, 582, 1424, 2268, 865, 279, 10186, 11, 323, 8450, 1172, 990, 220, 18, 2155, 10186, 4192, 20993, 1784, 376, 1784, 1296, 1472, 1296, 1784, 1296, 29, 1740, 5184, 522, 1296, 1784, 1296, 29, 37677, 5184, 522, 1296, 1784, 1296, 80104, 55080, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 11383, 6691, 1740, 522, 1296, 1784, 1296, 38643, 16, 21, 13, 17, 19, 20287, 16, 13, 17, 15, 522, 1296, 1784, 1296, 38643, 19, 19, 13, 17, 22, 20287, 16, 13, 15, 22, 522, 1296, 1784, 1296, 38643, 17, 23, 13, 15, 18, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 30010, 682, 12, 21367, 2861, 21804, 522, 1296, 1784, 1296, 38643, 16, 21, 13, 22, 19, 20287, 16, 13, 18, 17, 522, 1296, 1784, 1296, 38643, 16, 21, 13, 21, 22, 20287, 16, 13, 18, 16, 522, 1296, 1784, 1296, 95326, 15, 13, 15, 22, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 13999, 10974, 522, 1296, 1784, 1296, 38643, 19, 19, 13, 20, 24, 20287, 15, 13, 20, 17, 522, 1296, 1784, 1296, 38643, 19, 19, 13, 20, 17, 20287, 15, 13, 22, 22, 522, 1296, 1784, 1296, 95326, 15, 13, 15, 22, 522, 1296, 1472, 376, 1472, 2005, 10370, 2556, 220, 16, 25, 42451, 315, 54119, 21804, 13, 8886, 4842, 702, 862, 5184, 23638, 16588, 1526, 862, 7877, 1352, 9904, 13, 576, 1590, 3250, 6682, 279, 6672, 1948, 279, 21804, 323, 279, 27913, 11, 892, 374, 1172, 5089, 369, 1039, 1714, 4192, 2121, 10449, 304, 6633, 220, 16, 11, 678, 1008, 21804, 614, 458, 72421, 6672, 320, 4216, 315, 279, 21804, 32256, 291, 504, 4675, 315, 279, 54119, 4842, 8, 1948, 279, 5068, 315, 279, 21804, 323, 279, 54119, 4842, 11, 3650, 369, 1039, 1714, 13, 4220, 45663, 14613, 1473, 429, 26160, 16999, 77866, 5000, 11, 714, 3204, 1667, 362, 1740, 4192, 2, 220, 21, 11977, 16799, 5015, 609, 434, 87558, 31804, 1406, 334, 15578, 11234, 62248, 66963, 2619, 525, 25383, 3412, 17961, 8826, 279, 9135, 315, 10186, 11, 4152, 311, 279, 1657, 8357, 1380, 64195, 374, 16587, 13, 1634, 1293, 438, 1052, 374, 264, 1614, 315, 279, 22067, 11, 1039, 10449, 1714, 5707, 264, 11785, 38485, 15048, 315, 11647, 4192, 641, 1039, 975, 11, 582, 10735, 389, 279, 1142, 1380, 279, 22067, 1558, 537, 990, 279, 1482, 34682, 311, 8253, 862, 4842, 13, 12090, 11, 432, 1231, 387, 279, 1142, 429, 279, 22067, 5711, 264, 51000, 4842, 320, 603, 429, 13798, 389, 1181, 3681, 5302, 323, 5144, 6168, 701, 1741, 438, 458, 431, 9745, 311, 8253, 279, 2266, 315, 279, 1482, 6203, 4192, 1838, 745, 11, 279, 22067, 686, 902, 5021, 3960, 279, 4842, 59294, 304, 84670, 220, 16, 429, 374, 21063, 17749, 389, 279, 1482, 1584, 11, 714, 4751, 264, 4842, 429, 374, 17749, 389, 279, 1482, 34682, 24391, 14085, 271, 14085, 198, 59, 2493, 62, 314, 78, 92, 320, 64, 1124, 16106, 1124, 29143, 62, 314, 16, 25, 259, 5410, 1669, 1124, 1242, 62, 314, 72, 92, 281, 1124, 2359, 1337, 6306, 32798, 72, 9139, 1124, 16106, 1124, 29143, 62, 314, 16, 25, 259, 11035, 1291, 8, 1124, 2493, 62, 314, 66, 6306, 32798, 72, 52731, 320, 64, 1124, 16106, 274, 8, 1124, 4578, 314, 23, 532, 14085, 271, 14085, 1406, 1654, 1730, 304, 1039, 32169, 3059, 429, 1667, 458, 431, 9745, 33365, 892, 16275, 220, 400, 79, 1337, 22957, 29143, 15159, 16, 60777, 5410, 3, 220, 4936, 9482, 705, 304, 448, 2987, 22823, 10186, 476, 97952, 10186, 13, 758, 2176, 5048, 11, 1052, 572, 264, 12040, 8922, 6672, 304, 5068, 1948, 279, 4842, 39026, 323, 279, 54119, 4842, 4192, 1806, 19784, 48164, 11, 979, 279, 22067, 702, 2615, 311, 264, 3746, 3322, 13042, 369, 862, 4842, 11, 1221, 807, 1265, 387, 2952, 311, 732, 19652, 894, 4842, 13, 758, 419, 1142, 11, 279, 11785, 38485, 738, 315, 11647, 4157, 653, 1753, 311, 5358, 279, 77866, 13, 1205, 4411, 429, 419, 374, 458, 13245, 16688, 11, 323, 374, 20664, 369, 3853, 975, 4192, 94780, 25, 10328, 1039, 5413, 525, 25070, 304, 43179, 1584, 12621, 11, 1039, 5486, 646, 387, 66117, 311, 19259, 30476, 4192, 785, 45767, 57770, 24892, 304, 84670, 220, 24, 13230, 429, 279, 990, 315, 19259, 2266, 1231, 1083, 387, 3204, 11, 2661, 1052, 374, 264, 3746, 3322, 729, 9848, 13689, 311, 16045, 279, 7982, 315, 220, 400, 66, 91, 82, 3, 659, 1205, 1490, 419, 438, 458, 13245, 5106, 369, 3853, 975, 11, 311, 11731, 279, 2480, 19745, 315, 3204, 28411, 42721, 10186, 1212, 279, 21568, 315, 84670, 220, 17, 4192, 785, 14248, 36233, 315, 55800, 25, 10328, 279, 21187, 2022, 304, 84670, 220, 17, 23945, 264, 19819, 84162, 1948, 279, 2393, 315, 16999, 77866, 311, 279, 11313, 279, 54119, 4842, 646, 11075, 11, 432, 1231, 1795, 429, 419, 374, 537, 279, 1142, 13, 1084, 1231, 387, 279, 1142, 429, 28848, 374, 4512, 21303, 553, 279, 3601, 11313, 279, 54119, 4842, 82591, 389, 264, 2155, 11313, 729, 220, 57960, 1646, 450, 90, 81, 31716, 659, 1752, 3110, 11, 264, 12305, 429, 374, 49135, 1265, 537, 387, 26075, 448, 12677, 13, 8704, 279, 220, 400, 81, 3, 220, 5746, 304, 84670, 220, 17, 525, 827, 306, 38940, 11, 279, 11313, 729, 220, 400, 81, 3, 220, 429, 374, 81078, 304, 84670, 220, 17, 646, 387, 44936, 311, 4946, 894, 7271, 315, 28848, 2090, 4192, 1654, 646, 5602, 279, 16538, 315, 362, 1740, 553, 46041, 11702, 804, 220, 19, 323, 220, 20, 311, 990, 264, 2155, 11313, 729, 220, 57960, 1646, 450, 90, 81, 31716, 220, 304, 279, 21187, 2022, 11, 31334, 10607, 220, 400, 49, 1155, 15087, 220, 369, 220, 57960, 1646, 450, 90, 49, 25547, 83, 8, 284, 1124, 1242, 15159, 83, 60285, 83, 92, 47822, 51, 12, 16, 92, 1124, 32214, 47822, 83, 6, 481, 259, 92, 1124, 1646, 450, 90, 81, 25547, 82, 15159, 83, 24731, 264, 15159, 83, 65260, 3, 659, 576, 2732, 315, 279, 95254, 323, 12111, 7146, 279, 1852, 4192, 1654, 1744, 419, 374, 458, 13245, 5106, 11, 5310, 369, 6832, 678, 2155, 3204, 43613, 315, 279, 11785, 38485, 11647, 4192, 2, 220, 22, 3418, 98669, 1406, 1654, 3042, 362, 1740, 438, 1632, 438, 1181, 35972, 54886, 11, 323, 1473, 429, 4842, 20169, 11, 264, 6770, 47496, 12111, 646, 387, 1483, 311, 29436, 264, 4842, 39026, 429, 4157, 387, 54119, 13, 1205, 25070, 362, 1740, 2348, 3046, 10999, 311, 1473, 429, 28411, 42721, 16999, 374, 537, 42203, 2041, 1039, 1714, 4192, 1986, 975, 35511, 264, 11514, 3602, 16587, 3082, 304, 46069, 10927, 20909, 11, 8826, 279, 64195, 315, 33233, 10186, 13, 576, 27491, 315, 1039, 5486, 374, 429, 264, 4842, 39026, 646, 11075, 1550, 470, 369, 279, 4842, 6372, 11, 1393, 8241, 458, 9250, 22067, 448, 264, 19323, 3347, 11313, 11, 3259, 33233, 39026, 28848, 311, 279, 22067, 4192, 785, 54886, 315, 1039, 3491, 6505, 323, 279, 12111, 525, 1602, 4586, 13, 758, 419, 1156, 975, 582, 19869, 279, 6291, 304, 279, 35092, 11882, 4285, 21737, 304, 1973, 311, 2664, 50087, 279, 27491, 315, 1039, 1714, 13, 758, 1039, 34035, 975, 582, 3920, 26364, 279, 3766, 315, 1039, 1714, 304, 5257, 30476, 11, 892, 374, 700, 315, 279, 6891, 315, 419, 61369, 5567, 624, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are an expert academic reviewer tasked with evaluating research papers.<|im_end|>
<|im_start|>user
I am giving you a paper. I want to predict its acceptance outcome at ICLR.
 - Your answer will either be: \boxed{Accept} or \boxed{Reject}
 - Note: ICLR generally has a ~30% acceptance rate


# PREVENTING IMITATION LEARNING WITH ADVERSARIAL POLICY ENSEMBLES


Anonymous authors


Paper under double-blind review


# ABSTRACT


Imitation learning can reproduce policies by observing experts, which poses a problem regarding policy privacy. Policies, such as human, or policies on deployed robots, can all be cloned without consent from the owners. How can we protect our proprietary policies from cloning by an external observer? To answer this question we introduce a new reinforcement learning framework, where we train an ensemble of near-optimal policies, whose demonstrations are guaranteed to be useless for an external observer. We formulate this idea by a constrained optimization problem, where the objective is to improve proprietary policies, and at the same time deteriorate the virtual policy of an eventual external observer. We design a tractable algorithm to solve this new optimization problem by modifying the standard policy gradient algorithm. Our formulation can be interpreted in lenses of confidentiality and adversarial behaviour, which enables a broader perspective of this work. We demonstrate the existence of "non-clonable" ensembles, providing a solution to the above optimization problem, which is calculated by our modified policy gradient algorithm. To our knowledge, this is the first work regarding the protection of policies in Reinforcement Learning.


# 1 INTRODUCTION


Imitation learning and behavioral cloning provide really strong ability to create powerful policies, as seen in robotic tasks (Laskey et al., 2017; Finn et al., 2017; Codevilla et al., 2019; 2017; Pomerleau, 1988; Bojarski et al., 2016). Other fields have developed methods to ensure privacy (Al-Rubaie & Chang, 2019; Papernot et al., 2016), however, such work do not offer protection against policy cloning.


In this work, we tackle the issue of protecting policies by training policies that aim to prevent an external observer from using behaviour cloning. Our approach draws inspiration from imitating human experts, who can near-optimally accomplish given tasks. The setting which we analyze is presented in Figure 1. We wish to find a collection of experts, which as an ensemble can perform a given task well, however, also targets behaviour cloning through adversarial behaviour. Another interpretation is that this collection of experts represents the worst case scenario for behaviour cloning on how to perform a task "good enough".


Imitation learning frameworks generally make certain assumptions of the optimality of the demonstrations (Ziebart et al., 2008; Levine, 2018), yet never considered the scenario when the experts specifically attempt to be adversarial to the imitator. We pose the novel question regarding this assumption: does there exist a set of experts that are adversarial to an external observer trying to behaviour clone?


We propose Adversarial Policy Ensembles (APE), a method that simultaneously optimizes the performance of the ensemble and minimizes the performance of policies eventually obtained from cloning it. Our experiments show that APE do not suffer much performance loss from an optimal policy, while causing, on average, the cloned policy to experience over 5 times degradation compared to the optimal policy.


Our main contributions can be summarized as follows:


![Figure 1: Confidentiality scheme: Left During training, optimize a Policy Ensemble by estimating gradients using both the policies in the ensemble and the fictitious observer policy. Right When collecting a dataset for cloning, the context variable is marginalized out. Thus cloning the Policy Ensemble can result in a useless policy](images/eaca9968682e42bdfc7d47c7dea05d266ecf6a113976165e894ca21a2fb5e7d8.jpg)

- - We introduce a novel method APE, as well as the mathematical justification of the notion of adversarial experts.

- - By modifying Policy Gradient (Sutton et al., 2000), a common reinforcement learning algorithm, we suggest a tractable scheme for finding an optimal solution for this objective.

- - We demonstrate the solution by numerical simulations, where we show that a cloned policy is crippled even after collecting a significantly large number of samples from a policy ensemble.


To our knowledge, not only is this the first work regarding the protection of policies in reinforcement learning, but it is also the first to represent adversarial experts.


# 2 PRELIMINARIES


We develop APE in the standard framework of Reinforcement Learning (RL). The main components we use are Markov Decision Processes, Policy Gradient (Sutton et al., 2000), policy ensembles, and behaviour cloning, which we review below.


# 2.1 MARKOV DECISION PROCESS


A discrete-time finite-horizon discounted Markov decision process (MDP)  $\mathcal{M}$  is defined by  $(S, \mathcal{A}, r, p, p_0, \gamma, T)$  where  $\mathcal{S}$  is the state space,  $\mathcal{A}$  is the action space,  $r: S \times \mathcal{A} \to \mathbb{R}$  is the reward function,  $p(s_{t+1}|s_t, a_t)$  is the transition probability distribution,  $p_0: S \to \mathbb{R}^+$  is the initial state distribution,  $\gamma \in (0,1)$  is the discount factor, and  $T$  is the time horizon. A trajectory  $\tau \sim \rho_{\pi}$ , sampled from  $p$  and a policy  $\pi: S \times \mathcal{A} \to \mathbb{R}^+$ , is defined to be the states and actions tuple  $(s_0, a_0, \ldots, s_{T-1}, a_{T-1}, s_T)$ , whose distribution is characterized by  $\rho_{\pi}$ . Define the return of a trajectory to be  $r(\tau) = \sum_{t=0}^{T-1} \gamma^t r(s_t, a_t)$  to be the sum of discounted rewards seen along the trajectory, and define a value function  $V^\pi: S \to \mathbb{R}$  to be expected return of a trajectory starting from state  $s$ , under the policy  $\pi$ . The goal of reinforcement learning is to find a policy that maximizes the expected return  $\mathbb{E}_{\tau \sim \rho_{\pi}}[r(\tau)]$ .


# 2.2 POLICY GRADIENT


Policy Gradient (PG) (Sutton et al., 2000) aim to directly learn the optimal policy  $\pi$ , parameterized by  $\theta$ , by repeatedly estimating the gradient of the expected return, in one of many forms, shown in Schulman et al. (2015). In our work, we follow notation similar to that of (Schulman et al., 2015; 2017) and estimate  $\nabla_{\theta} \mathbb{E}_{\tau \sim \rho_{\pi}}[r(\tau)]$  using the advantage, which is estimated from a trajectory  $\tau$ ,  $A_{\tau}^{\pi}(t) = R_{\tau}(t) - V^{\pi}(s_t)$ , where  $R_{\tau}(t) = \sum_{t'=t}^{T-1} \gamma^{t'} r(s_{t'}, a_{t'})$  is the sum of the reward following action  $a_t$ .


Here, the value function is learned simultaneously with the policy, and so the advantage will use  $\hat{V}^{\pi}$  as an estimate for  $V^{\pi}$ .


# 2.3 POLICY ENSEMBLE (PE)


A Policy Ensemble (PE) is similar to the notion of contexts and skills (Achiam et al., 2018; Eysenbach et al., 2018; Sharma et al., 2019) which we discuss in Section 3. We denote a PE by  $\pi_{\mathbf{c}}$ , where each  $\pi_{c^{(i)}}, i \in \{1, 2, \dots n\}$  represents an expert. To rollout the PE, an expert is chosen at random (in our case uniform), and the expert completes a trajectory. Each expert policy  $\pi_{c^{(i)}}(a|s)$  can be viewed as a policy conditioned on a latent variable  $c$ ,  $\pi(a|s, c)$ .


Although  $\pi_{\mathbf{c}}$  consists of multiple policies, it is important to note that it itself is still a policy.


# 2.4 BEHAVIOUR CLONING


To behaviour clone an expert policy (Widrow & W. Smith, 1964), a dataset of trajectories  $\mathcal{D}$  consisting of state action pairs  $(s,a)$  are collected from the the expert rollouts. Then, a policy parametized by  $\phi$  is trained to maximize the likelihood of an action given a state,  $\sum_{(s,a)\in \mathcal{D}} - \log \pi_{\phi}(a\mid s)$ .


When cloning  $\pi_{\mathbf{c}}$ ,  $\mathcal{D}$  will not contain information of the latent variable  $c$ , and so the cloned policy will marginalize it out. Thus, the observer will clone:


$$

$$
\pi_ {o} (a \mid s) := \sum_ {i} p \left(c ^ {(i)} \mid s\right) \pi_ {c ^ {(i)}} (a \mid s) \tag {1}
$$

$$


We stress that this policy does not exist until  $\pi_{\mathbf{c}}$  is behaviour cloned.  $\pi_o$  is a fictitious policy to represent what would happen in the best case scenario of the observer having access to infinite data from  $\pi_{\mathbf{c}}$  to clone into  $\pi_o$ .


The scope of this paper is to specifically prevent behavioral cloning from succeeding. Other imitation learning approaches such as inverse reinforcement learning (Abbeel & Ng, 2004; Ng & Russell, 2000; Levine et al., 2011) and adversarial imitation learning (Ho & Ermon, 2016; Peng et al., 2018) require rollouts of non-expert policies in the environment, which may be costly, and thus are not considered.


# 3 RELATED WORK


Adversarial Attacks in RL: Our notion of adversarial policies is inextricably related to other adversarial methods that target RL such as Lin et al. (2017); Behzadan & Munir (2017), that add adversarial perturbations to policy input. Other adversarial attacks include poisoning the batch of data used when training RL (Ma et al., 2019), and exploitation in the multi-agent setting (Gleave et al., 2019). However, these methods all present as active attacks for various learning techniques. Our method, instead, passively protects against cloning.


Privacy in RL: With regards to protection, our work is related to differential privacy (Al-Rubaie & Chang, 2019). Differential privacy in RL can be used to create private Q-functions (Wang & Hegde, 2019) or private policies (Balle et al., 2016), which have private reward functions or private policy evaluation. However, we would like to emphasize that our motivation is to prevent cloning, and thus protecting the policies, rather than protecting a dataset. In fact, we make the assumption that the observer can perform behaviour cloning on as much data as desired.


Imitation Learning: Since we comply to the standard imitation learning setting of cloning from a dataset with many experts providing the demonstrations, latent variables w.r.t. imitation learning is well-studied. For example, Codevilla et al. (2017) show that conditioning on context representation can make imitation learning a viable option for autonomous driving. Li et al. (2017) demonstrate that the latent contextual information in expert trajectories is often semantically meaningful. As well, Providing extra context variables to condition on also appears in forms of extra queries or providing labels (Brown et al., 2019; de Haan et al., 2019; Hristov et al., 2018). Our method is different as instead of experimenting for success in imitation learning, we study how to prevent it.


Multiple Policies: (Achiam et al., 2018; Eysenbach et al., 2018; Sharma et al., 2019) have similar schemes of sampling a latent variable and fixing it throughout a trajectory, although their latent variables (contexts or skills) are used to solve semantically different tasks. The reason to solve different tasks is due to the objective of using the context variable/skills for learning in an unsupervised setting.


Our approach differs in both motivation and implementation, as we learn experts that all solve the same task, and constrain so that observers can not clone the policy.


A PE  $\pi_{\mathbf{c}}$  can also be viewed as a mixture of experts (Jacobs et al., 1991), except the gating network assigns probability 1 to the same expert for an entire trajectory. As such, we do not learn the gating network, although it may still be useful to see  $\pi_{\mathbf{c}}$  as a special case of a mixture of experts where the gating network learns immediately to fix the expert for each trajectory. There are also methods such as OptionGAN (Henderson et al., 2018), which uses a mixture of experts model to learn multiple policies as options with access to only expert states.


Zhang et al. (2019) also proposes a method to train multiple policies that complete the same task but uses the uncertainty of an autoencoder as a reward augment. Their motivation is to find multiple novel policies, while our motivation has no connection to novelty. Due to these differences in motivation, they train each policy one after the other, while our policies are trained simultaneously.


Policy ensembles are also used in the multi-task and goal conditioned settings in which case the task that is meant to be solved can be viewed as the context. Marginalizing out the context variable (Equation 1) of these context-conditioned policies is studied in the case of introducing a KL divergence regularizing term for learning new tasks (Goyal et al., 2019) and for sharing/hiding goals (Strouse et al., 2018). However, the main motivation is different in that both Goyal et al. (2019); Strouse et al. (2018) use  $\pi_{o}$  to optimize mutual information, while we directly optimize its performance.


# 4 METHOD


# 4.1 OBJECTIVE


We wish to have experts that can perform the task, while minimizing the possible returns of the cloned policy, denoted in Equation 1. We modify the standard RL objective to be:


$$

$$
\underset {\theta} {\arg \min } \mathbb {E} _ {\tau \sim \rho_ {\pi_ {o}}} [ r (\tau) ] \quad \text {s . t .} \mathbb {E} _ {\tau \sim \rho_ {\pi_ {\mathrm {e}}}} [ r (\tau) ] \geq \alpha \tag {2}
$$

$$


where  $\alpha$  is a parameter that lower bounds the reward of the policy ensemble. This translates to maximizing the unconstrained Lagrangian:


$$

$$
J (\theta) = \mathbb {E} _ {\tau \sim \rho_ {\pi_ {\mathbf {c}}}} [ r (\tau) ] - \beta \mathbb {E} _ {\tau \sim \rho_ {\pi_ {o}}} [ r (\tau) ] \tag {3}
$$

$$


where  $1 / \beta$  is the corresponding Lagrangian multiplier, and is subsumed into the returns collected by the policy ensemble. We refer to PE that optimizes this objective as Adversarial Policy Ensembles (APE). There is a natural interpretation of the objective in Equation 2. Human experts tend to be "good enough", which is reflected in the constraint. The minimization is simply finding the most adversarial experts.


Although we assume that the observer can only map states to actions, it may be the case that they can train a sequential policy, which is dependent on its previous states and actions. Our method can be generalized to sequential policies as well, and the impact of such observers is discussed in the Section 6.


# 4.2 MODIFIED POLICY GRADIENT ALGORITHM


Intuitively, since there are the returns of two policies that are being optimized, both should be sampled from to estimate the returns.


We show how we can modify PG to train APE, by maximizing Equation 3. The two terms suggest a simple scheme to estimate the returns of the policy ensemble twice: once using  $\pi_{\mathbf{c}}$  that we wish to maximize, and a second time using  $\pi_{o}$ , which approximates the returns of an eventual observer who tries to clone the policy ensemble. Along with our PE, we train value functions  $\tilde{V}^{\pi_c(i)}$  for each expert, jointly parameterized by  $\phi$  which estimates  $V^{\pi_c(i)} - \beta V^{\pi_o}$ . The loss function for the value functions of two sampled trajectories  $\tau_{1}, \tau_{2}$  is


$$

$$
J _ {\tau_ {1}, \tau_ {2}} (\phi) = \sum_ {t = 0} ^ {T _ {1} - 1} \frac {1}{2} \left(\tilde {V} _ {\phi} ^ {\pi_ {c} (i)} \left(s _ {t _ {1}}\right) - R _ {\tau_ {1}} (t)\right) ^ {2} + \sum_ {t = 0} ^ {T _ {2} - 1} \frac {1}{2} \left(\tilde {V} _ {\phi} ^ {\pi_ {c} (i)} \left(s _ {t _ {2}}\right) + \beta R _ {\tau_ {2}} (t)\right) ^ {2} \tag {4}
$$

$$


The policy gradient update from  $N_{1}$  and  $N_{2}$  trajectories is then


$$

$$
\nabla_ {\theta} J _ {\tau_ {1}, \tau_ {2}} (\theta) \approx G _ {1} + G _ {2} \tag {5}
$$

$$


where


$$

$$
G _ {1} = \frac {1}{N _ {1}} \sum_ {j = 1} ^ {N _ {1}} \sum_ {t = 0} ^ {T _ {1}} \nabla_ {\theta} \log \pi_ {c ^ {(i)}} \left(a _ {t 1} ^ {(j)} \mid s _ {t 1} ^ {(j)}\right) \tilde {A} _ {\tau_ {1}} ^ {\pi_ {c ^ {(i)}} ^ {(i)}} (t) \tag {6}
$$

$$


$$

$$
G _ {2} = \frac {1}{N _ {2}} \sum_ {j = 1} ^ {N _ {2}} \sum_ {t = 0} ^ {T _ {2}} \nabla_ {\theta} \log \pi_ {o} \left(a _ {t 2} ^ {(j)} \mid s _ {t 2} ^ {(j)}\right) \tilde {A} _ {\tau_ {2}} ^ {\pi_ {o}} (t) \tag {7}
$$

$$


where  $c^{(i)}$  identifies the chosen expert of the trajectory., and  $\tilde{A}_{\tau_1}^{\pi_c^{(i)}}(t) = R_{\tau_1}(t) - \tilde{V}^{\pi_{c^{(i)}}}(s_t)$  and  $\tilde{A}_{\tau_2}^{\pi_o}(t) = -\beta R_{\tau_2}(t) - \tilde{V}^{\pi_o}(s_t)$  are the modified advantage functions. The  $-\beta$  that is in the advantage in  $G_{2}$  optimizes against the performance of the observed policy  $\pi_o$ .


The gradient  $G_{1}$  for  $\pi_{\mathbf{c}}$  is straightforward. However, to estimate the gradient  $G_{2}$  for  $\pi_{o}$  which is an fictitious policy, we sample from it by first re-sampling the context of the expert at each state, and then sampling an action from the context. The back-propagation occurs to  $\pi_{c^{(i)}}(a|s)$  for the context sampled at each state. Practical implementation details can be found in A.2. The intuition is as follow. While sampling  $\pi_{o}$ , if a selected action causes high return, we should decrease the probability, which lowers the expected reward of  $\pi_{o}$ . Combined, the two gradients will cause the PE to select actions that achieves have high reward, and are detrimental to the observer.


Equations 4 and 5 formulate our PG approach of APE, which is summarized in Algorithm 1.


# ALGORITHM 1: PG-APE


Require:  $\theta ,\phi ,\mathcal{M},\beta$

- 1: for each iteration do:

- 2: Generate trajectories  $\tau_{1}$  with  $\pi_{\mathbf{c}}$  from  $\mathcal{M}$  for Equation 6

- 3: Generate trajectories  $\tau_{2}$  with  $\pi_o$  from  $\mathcal{M}$  for Equation 7

- 4: Calculate Equation 5 to perform a gradient update on the PE  $\theta \gets \theta +\alpha_{\theta}\hat{\nabla}_{\theta}J_{\tau_1,\tau_2}(\theta)$

- 5: Update the value function  $\phi \gets \phi -\alpha_{\phi}\hat{\nabla}_{\phi}J_{\tau_1,\tau_2}(\phi)$  as determined by Equation 4.


6: end for


# 5 EXPERIMENTS


We perform experiments on a navigation task, where the objective is to reach a goal state as fast as possible. The purpose is to illustrate that an APE can cause the cloned policy to take significantly longer to reach the goal state. We do so by first training a PE and behaviour cloning it. We then compare the performance of the PE to that of the clone. We use a discrete environment to best demonstrate the validity of the equation. This is because all discrete policies can be parameterized, which is not true in continuous, where typically Gaussian parameterization is used. As such, continuous environments would have to make assumptions about how both the PE and the cloner parameterizes policies, as well as tackle problems of distributional drift, which we would like to avoid. However, with these assumptions, our setting can extend to the continuous domain. In our experiments, we use a  $10 \times 10$  grid-world environment as our main testbed. This is to have large enough expression that would not be found in smaller grids, while still small enough to visualize the behaviour of the APE. The discrete actions will show precisely how the experts can be jointly adversarial.


Using gridworld allows for precise expected return estimates. In an environment where closed-form returns cannot be calculated, approximation error can accumulate through estimating the returns of


![Figure 2: Visualization of APE. We set  $\beta = 0.6$ . Arrows indicate action probabilities, and the colour scale represents the hitting time. Yellow indicates expected reward of 0, while purple indicates expected reward of  $-100$ , which is the maximum episode length. The top left corner is the goal state, and the adjacent states that are purple are an example of how APE is adversarial to cloning, as those states will cause the cloned policy to suffer larger losses.](images/bf89fe018aad3500d496c4f5222d3a14b164b72b7b7fb83d5ac53685d1239e94.jpg)


both the trained PE and the clone. This noise would only increase in continuous state space, where the returns of  $\pi_{o}$  may not be tractable to estimate due to issues such as distributional drift (Ross et al., 2010; Codevilla et al., 2019; de Haan et al., 2019).


Our results answer the following questions. How much optimality is compromised? How useless can we make the cloned policy? Is it possible to use non APE to prevent behaviour cloning?


# 5.1 TRAINING


Even though our method can compute a policy ensemble with any finite number of experts, we chose to visualize a solution with 2 experts, which is sufficient to reveal the essential properties of the method. Specifically, we train  $n = 2$  tabular experts with PG-APE. Our code is written in Tensorflow (Abadi et al., 2016), and will be publicly available on GitHub. Training details and hyper-parameters are in Section A.1 of the Appendix.


# 5.2 ENVIRONMENT


The basic environment is a  $10 \times 10$  grid, with the goal state at the top left corner. The agent spawns in a random non-goal state, and incurs a reward of  $-1$  for each time-step until it reaches the goal. At the goal state, the agent no longer receives a loss and terminates the episode. The agent is allowed five actions,  $\mathcal{A} = \{Up, Down, Left, Right, Stay\}$ . Moving into the wall is equivalent to executing a Stay action. We choose this reward function for the benefit of having a clear representation of the notion of "good enough", which is reflected in how long it takes to reach the goal state. Having such representation exemplifies how the APE can prevent an observer from cloning a good policy.


# 5.3 VISUALIZATION


Figure 2 shows an example of a PE that is trained for the basic gridworld environment. Figure 3 shows the corresponding cloned policy, as well as a comparison to an optimal policy. The colour scale represents the expected return of starting at a given state.


In the case of an optimal policy  $(\beta = 0)$ , actions are taken to take the agent to the goal state as fast as possible. However, when  $\beta > 0$ , such a solution is no longer the optimum. Similar to  $\beta = 0$ , the experts would like to maximize the expected reward, and reach the goal state. However, to minimize the reward of the observed policy, the two expert policies must jointly learn to increase the number


of steps needed for  $\pi_o$  to reach the goal state. The expert policies must use adversarial behaviour while reaching the goal state, such as taking intelligent detours or Stay in the same state, which are learned to hinder  $\pi_o$  as much as possible. These learnt behaviours cause the cloned policy to take a drastically longer time to reach the goal. For example, note the two purple squares at the top-left near the goal, which indicates that the experts understand that they should not move to prevent the observer from attaining reward. Even though these sub-optimal decisions are made, on expectation, the experts are "not bad" and achieve an average of  $-15.27$  reward.


# 5.4 BASELINES


We use behaviour cloning to clone our PG-APE trained policies. To support our claims of preventing even in the horizon of infinite data, we collect a million timesteps of the trained PE in the environment. Further details of behaviour cloning are in the appendix. Shown in Figure 3 is an optimal policy, and the resulting cloned policy from Section 5.1.


As well, we evaluate against other PE, to show that preventing against behaviour cloning is non-trivial. We use several baselines. We first test policies that have approximately the same return as our ensemble by training PG with, and halting early rather than running until convergence. In the Near-Optimal case, we ran until the PE had expected returns that matched the average achieved by our method. Conversely, "Random" policies are used as a comparison to show that it is possible to cause the cloned policy to do poorly, but the tradeoff is that the PE itself cannot perform well, which is undesirable. These policies are also policies trained with PG, except they are stopped much earlier, when their clones matches the expected returns of our PG-APE. For each PG-APE, we use  $n = 2$  different tabular policies treated as an ensemble, which


![Figure 3: Visualization of the cloned APE. The policy obtained from cloning the APE trained has average expected reward of  $-45.18$ , while the optimal policy has an average expected reward of  $-9$ , which is over a  $5\times$  increase.](images/f4ba4ce9d75082aa5e122cefaebe7df85e636cf23f3e1f8c4380fd251c37298f.jpg)


we then clone, and average across 5 seeds. For the baselines, we hand-pick the policies, and thus only use 3 different policies.


<table><tr><td></td><td>PE Returns</td><td>Clone Returns</td><td>Returns Difference</td></tr><tr><td>PG-APE</td><td>-16.24 1.20</td><td>-44.27 1.07</td><td>-28.03</td></tr><tr><td>Near-Optimal PE</td><td>-16.74 1.32</td><td>-16.67 1.31</td><td>+0.07</td></tr><tr><td>Random Policy</td><td>-44.59 0.52</td><td>-44.52 0.77</td><td>+0.07</td></tr></table>


Table 1: Comparison of cloned PE. Each policy has their Returns precisely calculated through their closed form solutions. The final column reports the difference between the PE and the Clone, which is only significant for our method.


As presented in Table 1, all other PE have an insignificant difference (returns of the PE subtracted from returns of the cloned policy) between the performance of the PE and the cloned policy, except for our method. These empirical findings show that preventing behaviour cloning difficult, but possible using APE.


# 6 DISCUSSION & FUTURE WORK


**Confidential Policies:** There are promising research directions regarding the protection of policies, due to the many applications where confidentiality is crucial. As long as there is a model of the observer, our presented method provides a worst-case scenario of experts.


In our work, we focused on the case where the observer does not use the current trajectory to determine their policy. Instead, it may be the case that the observer uses a sequential policy (one that depends on its previous states and/or actions), such as an RNN to determine the context of the current expert.


Formally, the observer will no longer learn the policy formulated in Equation 1 that is solely dependent on the current state, but rather a policy that is dependent on the current trajectory:


$$

$$
\pi_ {o} (a \mid \tau_ {1: t}) := \sum_ {i} p \left(c ^ {(i)} \mid \tau_ {1: t}\right) \pi_ {c ^ {(i)}} (a \mid s) \tag {8}
$$

$$


We found in our preliminary results that using an RNN classifier which outputs  $p(c|\tau_{1:t})$  simply ended up in with either optimal policies or crippled policies. In both cases, there was a relatively minor difference in performance between the policy ensemble and the cloned policy.


Unsurprisingly, when the observer has access to a strong enough representation for their policy, then they should be able to imitate any policy. In this case, the worst-case set of experts cannot do much to prevent the cloning. We believe that this is an exciting conclusion, and is grounds for future work.


Continuous: Although our methods are evaluated in discrete state spaces, our approach can be generalized to continuous domains.


The Monte Carlo sampling in Equation 9 suggests that the use of continuous context may also be possible, given there is a strong enough function approximator to estimate the distribution of  $c|s$ . We see this as an exciting direction for future work, to recover the full spectrum of possible adversarial policies under the constraint of Equation 2.


The Semantics of Reward: Although the minimization in Equation 2 implies a logical equivalence between the success of behaviour cloning to the reward the cloned policy can achieve, it may follow that this is not the case. It may be the case that useless is defined differently by the expected reward the cloned policy achieves on a different reward function  $\tilde{r}$ . For example, a robot that is unpredictable should not be deployed with humans. Since the  $r$  functions in Equation 2 are disentangled, the reward function  $r$  that is minimized in Equation 2 can be engineered to fit any definition of uselessness.


We can modify the objective of APE by modifying Equations 4 and 5 to use a different reward function  $\tilde{r}$  in the minimization, substituting  $R(t)$  for  $\tilde{R}(t) = \sum_{t'=t}^{T-1} \gamma^{t' - t} \tilde{r}(s_{t'}, a_{t'})$ . The rest of the derivation and algorithm remain the same.


We think this is an exciting direction, especially for learning all different possible representations of the worst-case experts.


# 7 CONCLUSION


We present APE as well as its mathematical formulation, and show that policy gradient, a basic RL algorithm can be used to optimize a policy ensemble that cannot be cloned. We evaluated APE against baselines to show that adversarial behaviour is not feasible without our method.


This work identifies a novel yet crucial area in Reinforcement Learning, regarding the confidentiality of proprietary policies. The essence of our approach is that a policy ensemble can achieve high return for the policy owner, while providing an external observer with a guaranteed low reward, making proprietary ensemble useless to the observer.


The formulation of our problem setup and the algorithm are very general. In this first work we demonstrate the solution in the deliberately chosen simple environments in order to better visualize the essence of our method. In our concurrent work we study thoroughly the application of our method in various domains, which is out of the scope of this introductory paper.
<|im_end|>
<|im_start|>assistant

label: 0
eval example:
input_ids:
[151644, 8948, 198, 2610, 525, 458, 6203, 14250, 55514, 50820, 448, 37563, 3412, 15689, 13, 151645, 198, 151644, 872, 198, 40, 1079, 7086, 498, 264, 5567, 13, 358, 1366, 311, 7023, 1181, 25505, 15274, 518, 358, 74058, 624, 481, 4615, 4226, 686, 2987, 387, 25, 1124, 79075, 90, 16646, 92, 476, 1124, 79075, 90, 78413, 532, 481, 7036, 25, 358, 74058, 8789, 702, 264, 3968, 18, 15, 4, 25505, 4379, 1406, 2, 425, 86188, 25, 2100, 95188, 89625, 79341, 8753, 74585, 4613, 42817, 24211, 7671, 4066, 35, 98450, 50, 1406, 32684, 12014, 1406, 30898, 1212, 1990, 94349, 3395, 1406, 2, 14137, 74592, 1406, 2589, 3004, 42721, 3110, 9471, 9044, 264, 30428, 1714, 369, 37563, 279, 21765, 2090, 315, 264, 5662, 6832, 1614, 13, 758, 419, 5567, 11, 582, 2908, 2588, 6942, 3691, 12625, 8776, 320, 64, 5202, 5849, 13, 5480, 5980, 8776, 701, 892, 374, 264, 17003, 6243, 429, 26885, 28411, 42721, 10295, 3118, 389, 1172, 264, 4013, 315, 3691, 12625, 2588, 6942, 19556, 13, 1096, 943, 315, 8776, 646, 387, 1483, 311, 3359, 43179, 323, 6351, 4119, 11, 1741, 438, 53107, 33407, 287, 40425, 8942, 320, 5381, 10599, 8, 323, 17984, 5980, 9055, 4119, 13, 68471, 5480, 5980, 8776, 3118, 389, 86875, 2205, 8837, 3545, 633, 15700, 304, 264, 2205, 8028, 323, 3690, 311, 6923, 279, 22823, 28411, 42721, 3110, 448, 279, 24632, 18212, 64816, 13, 2014, 39139, 419, 4265, 11, 582, 29614, 458, 11050, 8823, 12111, 2598, 425, 86188, 12, 20566, 11, 892, 71323, 34950, 6350, 25185, 1526, 98134, 57739, 320, 4677, 8, 323, 13047, 533, 19851, 4405, 320, 8590, 568, 758, 3953, 11, 4518, 315, 29417, 287, 264, 3175, 6291, 1815, 979, 15039, 458, 28411, 42721, 3110, 11, 582, 10306, 264, 7314, 315, 6291, 12716, 311, 13186, 2989, 13604, 13, 1205, 1473, 37869, 2673, 429, 279, 10981, 12111, 19313, 4188, 311, 264, 2664, 6291, 1091, 6350, 19827, 11, 1393, 279, 3239, 1760, 374, 9155, 1091, 18950, 5248, 4194, 2856, 8040, 553, 264, 8168, 315, 220, 16, 15, 4192, 2, 220, 16, 9221, 90099, 1406, 2132, 702, 1012, 6839, 429, 5662, 6832, 4119, 11, 2670, 5538, 29728, 14155, 11, 525, 19563, 311, 28411, 42721, 10295, 320, 15216, 69, 5299, 1842, 452, 2572, 220, 17, 15, 16, 19, 26, 44190, 791, 9313, 1842, 452, 2572, 220, 17, 15, 16, 18, 26, 24495, 1842, 452, 2572, 220, 17, 15, 16, 22, 64, 568, 15277, 11, 37563, 279, 21765, 2090, 315, 264, 2661, 1614, 9044, 16587, 369, 4763, 16216, 8357, 13, 758, 1973, 311, 15442, 279, 21765, 2090, 315, 5538, 29728, 14155, 11, 11811, 614, 7881, 330, 20566, 25185, 1, 311, 6923, 28411, 42721, 10295, 429, 646, 5786, 26060, 264, 2661, 29728, 3922, 1393, 1660, 438, 3265, 438, 3204, 311, 279, 4024, 3110, 320, 15216, 69, 5299, 1842, 452, 2572, 220, 17, 15, 16, 19, 26, 6050, 436, 6190, 12, 1912, 89, 824, 14225, 1842, 452, 2572, 220, 17, 15, 16, 21, 26, 3261, 3732, 72, 609, 51375, 11, 220, 17, 15, 16, 22, 65, 26, 24495, 1842, 452, 2572, 220, 17, 15, 16, 22, 65, 568, 7496, 315, 1493, 3359, 5413, 525, 3118, 389, 87178, 264, 4709, 729, 448, 264, 20169, 5980, 25632, 11, 1380, 279, 20169, 374, 2987, 24182, 553, 1182, 76571, 27137, 320, 258, 279, 4158, 12625, 6243, 8, 476, 34226, 1737, 18192, 41204, 320, 258, 279, 8413, 6942, 3691, 12625, 6243, 568, 10328, 1493, 5413, 975, 1632, 389, 5297, 29728, 14155, 11, 979, 432, 4041, 311, 6351, 476, 1496, 44880, 9193, 4119, 11, 1741, 438, 5480, 12408, 323, 17984, 5980, 9055, 4119, 11, 807, 4157, 387, 5961, 9251, 1576, 279, 20169, 374, 537, 2500, 4192, 26907, 6942, 3691, 12625, 8776, 11, 1083, 3881, 438, 5480, 5980, 8776, 11, 2908, 279, 1429, 5000, 714, 25489, 6243, 1380, 279, 30710, 702, 902, 1995, 911, 279, 1614, 5944, 323, 5029, 11, 323, 279, 1172, 2697, 5666, 374, 311, 3239, 279, 1614, 311, 633, 279, 12159, 5480, 5980, 320, 18703, 6942, 8, 2550, 320, 33, 9300, 301, 1842, 452, 2572, 220, 17, 15, 16, 22, 568, 1096, 943, 315, 8776, 646, 387, 1483, 438, 264, 330, 95300, 1, 1616, 311, 15442, 21765, 2090, 315, 894, 2661, 4119, 11, 902, 4925, 19259, 476, 43179, 13, 1752, 2867, 11, 56707, 1842, 452, 13, 320, 17, 15, 16, 23, 1215, 24495, 1842, 452, 13, 320, 17, 15, 16, 24, 64, 8, 614, 9251, 5480, 5980, 8776, 369, 37563, 21765, 2090, 315, 53107, 33407, 287, 40425, 58884, 320, 5381, 10599, 8, 323, 4194, 13638, 13, 9125, 5480, 5980, 8776, 11, 2670, 38964, 301, 1842, 452, 13, 320, 17, 15, 16, 22, 1215, 56707, 1842, 452, 13, 320, 17, 15, 16, 23, 1215, 24495, 1842, 452, 13, 320, 17, 15, 16, 24, 65, 1215, 56707, 1842, 452, 13, 320, 17, 15, 16, 24, 701, 525, 3118, 389, 86875, 2205, 8837, 1365, 5916, 504, 458, 2856, 1459, 389, 279, 5480, 7329, 11, 807, 5367, 7887, 3271, 279, 3501, 3156, 279, 7329, 3080, 18739, 264, 2205, 8028, 320, 258, 3793, 315, 6010, 311, 279, 4024, 3110, 568, 576, 2647, 374, 3545, 3118, 389, 20169, 41204, 476, 1045, 1008, 566, 324, 5589, 13, 4354, 11, 279, 2205, 2647, 6993, 3643, 1493, 5413, 16216, 311, 279, 5916, 1459, 13, 1634, 582, 19869, 304, 19037, 220, 16, 2877, 701, 279, 18212, 64816, 315, 96775, 28411, 42721, 10295, 369, 264, 29728, 3922, 525, 5008, 2155, 369, 2155, 17465, 32383, 11, 323, 419, 24844, 9044, 803, 15386, 979, 432, 4041, 311, 43179, 4119, 1741, 438, 479, 9548, 52793, 320, 4060, 19037, 220, 16, 1883, 4579, 1096, 3643, 5480, 5980, 8776, 79767, 311, 264, 1186, 12, 1406, 2912, 2861, 18212, 64816, 13, 1634, 264, 1102, 11, 279, 6291, 4157, 2167, 8708, 279, 21765, 2090, 315, 279, 17112, 1614, 4192, 1249, 22279, 1493, 26038, 323, 1281, 5480, 5980, 8776, 2664, 8708, 279, 21765, 2090, 315, 4119, 11, 582, 29614, 264, 8823, 12111, 2598, 425, 86188, 12, 20566, 429, 20699, 66132, 279, 6291, 4271, 315, 6350, 86875, 2205, 2647, 3118, 8776, 13, 5633, 1887, 4522, 374, 311, 15963, 98134, 25262, 11, 892, 13719, 6291, 12128, 311, 3644, 53667, 714, 46421, 504, 1550, 34447, 2783, 11, 448, 86875, 2205, 8837, 11, 892, 19313, 4188, 4937, 714, 3545, 633, 15700, 304, 2205, 8028, 13, 44763, 11, 2661, 264, 5480, 3118, 3359, 220, 57960, 10374, 5416, 90, 32, 31716, 1154, 1039, 12111, 31995, 264, 7314, 315, 9904, 323, 518, 1817, 19569, 582, 1598, 220, 57960, 10374, 5416, 90, 32, 31716, 220, 369, 220, 400, 76, 3, 220, 7354, 389, 1817, 6291, 13, 576, 10981, 98134, 57739, 592, 29709, 320, 4677, 8, 323, 13047, 533, 19851, 4405, 320, 8590, 8, 525, 1221, 1483, 311, 13186, 2989, 6291, 3550, 3118, 389, 1482, 1995, 323, 3931, 700, 25165, 6291, 12716, 4192, 7981, 19026, 525, 68629, 3685, 1447, 12, 220, 16, 13, 1205, 6786, 17423, 21204, 311, 1473, 429, 1482, 5480, 5980, 8776, 3545, 79767, 311, 264, 2205, 53667, 11, 8450, 4623, 18142, 525, 2567, 382, 12, 220, 17, 13, 20205, 389, 279, 4522, 315, 98134, 25262, 323, 48924, 14761, 4405, 11, 582, 2884, 264, 8823, 12111, 311, 7760, 279, 5068, 315, 1482, 5480, 5980, 3359, 25185, 323, 14907, 1105, 311, 1477, 264, 1753, 9155, 28411, 42721, 18212, 64816, 29720, 382, 12, 220, 18, 13, 66863, 21204, 19869, 429, 425, 86188, 12, 20566, 646, 20699, 7760, 6350, 5480, 5980, 8776, 311, 1477, 2664, 10295, 448, 1753, 9155, 18212, 64816, 13, 758, 5256, 311, 279, 5297, 29728, 3922, 4119, 11, 582, 1083, 1273, 1039, 25185, 389, 22818, 43179, 479, 9548, 51, 4119, 323, 31214, 5980, 9055, 4119, 13, 22827, 11, 1039, 12111, 646, 7949, 279, 34447, 2783, 553, 220, 400, 16, 15, 59, 91550, 45340, 31716, 220, 7707, 311, 279, 49665, 5486, 4192, 2, 220, 17, 96614, 3567, 84347, 31804, 1406, 22043, 264, 23850, 1614, 220, 400, 37, 25, 1124, 10374, 6066, 90, 49, 92, 61, 67, 1124, 983, 1124, 90, 16, 11, 1124, 67816, 11, 356, 59, 31716, 220, 323, 458, 3110, 220, 400, 87, 62, 15, 3, 1154, 28411, 42721, 8776, 9213, 311, 1477, 279, 28411, 42721, 3110, 429, 374, 18093, 311, 220, 400, 87, 62, 15, 3, 659, 1752, 3110, 11, 458, 650, 5657, 291, 3359, 21538, 311, 1477, 279, 8028, 18212, 64816, 311, 2297, 279, 19149, 536, 11, 892, 33210, 311, 279, 2701, 25262, 3491, 24391, 14085, 271, 14085, 198, 59, 1065, 220, 716, 28152, 20255, 92, 97155, 1124, 20255, 97155, 1124, 31610, 1124, 1318, 314, 82, 659, 259, 659, 92, 1124, 31610, 434, 320, 87, 716, 314, 15, 92, 488, 1124, 20255, 8, 1124, 811, 80, 434, 320, 87, 716, 314, 15, 16630, 1124, 4578, 314, 16, 532, 14085, 271, 14085, 1406, 65357, 76291, 320, 16, 8, 374, 5990, 304, 2144, 480, 26, 8916, 11, 582, 646, 1172, 1720, 311, 633, 264, 42203, 6291, 315, 320, 16, 8, 1393, 15652, 220, 57960, 22957, 20255, 59, 80050, 220, 311, 387, 438, 2613, 438, 3204, 4192, 14075, 12625, 20790, 13, 1752, 29728, 14155, 11, 582, 646, 8290, 279, 21568, 304, 320, 16, 8, 553, 264, 4709, 729, 4512, 389, 279, 1487, 275, 6193, 2550, 11, 6388, 311, 264, 19259, 25262, 3491, 892, 646, 387, 27956, 553, 20169, 5980, 7553, 12230, 13, 1096, 5486, 702, 1012, 1483, 304, 5411, 5413, 1741, 438, 48297, 9501, 320, 15216, 69, 5299, 1842, 452, 2572, 220, 17, 15, 16, 19, 701, 356, 91336, 3359, 320, 8852, 3732, 72, 609, 51375, 11, 220, 17, 15, 16, 22, 65, 8, 323, 30044, 35, 3359, 320, 37036, 884, 1842, 452, 2572, 220, 17, 15, 16, 22, 568, 2009, 1493, 4158, 12625, 8776, 7881, 369, 29728, 14155, 9658, 279, 13885, 315, 279, 20169, 13, 4354, 11, 369, 4119, 448, 43179, 6813, 1741, 438, 479, 9548, 51, 11, 279, 16538, 4157, 387, 6707, 4512, 323, 20169, 5980, 4158, 12625, 8776, 525, 537, 8415, 13, 2619, 525, 2421, 4158, 12625, 8776, 7881, 369, 3151, 43179, 4119, 11, 1741, 438, 50168, 4440, 28263, 38424, 320, 44, 1715, 47, 8, 5486, 369, 22818, 4916, 39026, 320, 42, 517, 331, 69564, 1842, 452, 2572, 220, 17, 15, 16, 21, 568, 4354, 11, 1846, 25185, 525, 882, 34108, 323, 1373, 5089, 8869, 369, 11220, 1817, 1614, 4192, 30531, 6942, 5235, 12625, 20790, 5235, 3745, 6243, 31138, 279, 5048, 979, 458, 30710, 702, 902, 2118, 2615, 311, 279, 1614, 594, 5733, 323, 17646, 11, 323, 279, 1172, 2697, 5666, 374, 311, 3239, 1946, 10295, 323, 633, 279, 12159, 1614, 2550, 13, 758, 279, 8413, 6942, 3691, 3745, 6243, 11, 432, 374, 19110, 429, 279, 1614, 16275, 279, 18927, 315, 1817, 2383, 369, 458, 1946, 3239, 13, 24495, 1842, 452, 13, 320, 17, 15, 16, 22, 65, 8, 8542, 429, 279, 3359, 646, 2058, 387, 59294, 438, 458, 25262, 3491, 1380, 279, 16538, 729, 897, 646, 387, 24182, 1393, 279, 20169, 374, 34987, 13, 20205, 389, 419, 11, 5257, 7168, 339, 1973, 25262, 25185, 614, 1012, 10981, 11, 2670, 73213, 12, 20566, 320, 40, 398, 300, 1842, 452, 2572, 220, 17, 15, 16, 23, 64, 701, 468, 1808, 3359, 320, 1143, 268, 1842, 452, 2572, 220, 17, 15, 16, 23, 701, 7055, 275, 12, 20566, 320, 40, 398, 300, 1842, 452, 2572, 220, 17, 15, 16, 23, 65, 701, 8979, 28716, 320, 52971, 1842, 452, 2572, 220, 17, 15, 16, 24, 701, 74126, 12111, 320, 2101, 89, 517, 354, 1842, 452, 2572, 220, 17, 15, 16, 23, 49555, 26907, 6942, 5235, 12625, 3359, 320, 74846, 5980, 3359, 8, 758, 419, 5567, 11, 582, 5244, 389, 279, 2588, 6942, 3691, 3745, 3359, 320, 18654, 3881, 438, 5480, 5980, 3359, 568, 758, 12872, 311, 279, 8413, 6942, 6243, 11, 279, 1406, 1587, 9683, 646, 1172, 3239, 279, 1614, 323, 633, 279, 1909, 12, 16, 19149, 2383, 2041, 894, 18927, 1995, 13, 2014, 29337, 279, 18212, 64816, 304, 279, 5480, 5980, 6243, 11, 38964, 301, 1842, 452, 13, 320, 17, 15, 16, 22, 8, 1156, 10981, 264, 76416, 20790, 3118, 389, 4194, 4227, 389, 279, 5480, 7329, 13, 24885, 389, 11, 56707, 1842, 452, 13, 320, 17, 15, 16, 23, 8, 8542, 429, 2588, 6942, 3359, 646, 387, 14836, 7757, 438, 2441, 19259, 25262, 3491, 11, 323, 7168, 339, 1973, 25262, 25185, 1741, 438, 73213, 646, 387, 1483, 311, 11625, 419, 3491, 13, 56707, 1842, 452, 13, 320, 17, 15, 16, 24, 8, 4623, 25271, 279, 19556, 553, 1172, 37614, 279, 1841, 315, 1863, 19499, 21010, 729, 11, 323, 24495, 1842, 452, 13, 320, 17, 15, 16, 24, 65, 8, 10981, 2441, 12111, 311, 7269, 916, 18732, 3359, 13, 15132, 5413, 646, 79767, 6157, 714, 7676, 504, 2205, 53667, 5322, 11, 323, 8450, 1373, 803, 16585, 323, 17423, 2711, 304, 279, 6291, 3550, 4192, 88464, 3118, 3691, 12625, 25262, 2619, 525, 1378, 16626, 1483, 5413, 311, 11625, 264, 3691, 12625, 476, 2477, 1737, 18277, 2156, 25262, 3491, 25, 20169, 5980, 323, 84093, 4532, 5980, 25185, 13, 53107, 5980, 5413, 525, 3118, 389, 86875, 2205, 8837, 3080, 55441, 11, 1393, 84093, 4532, 5980, 25185, 1741, 438, 98134, 25262, 320, 4677, 8, 320, 93772, 22222, 1842, 452, 2572, 220, 16, 24, 24, 24, 26, 92671, 1225, 1842, 452, 2572, 220, 17, 15, 16, 17, 8, 44868, 279, 16538, 729, 553, 264, 84093, 4532, 1614, 13, 43393, 12094, 11, 20169, 5980, 5413, 525, 16626, 1483, 304, 3691, 12625, 3359, 1576, 432, 19313, 4188, 4937, 13, 4354, 11, 1493, 5413, 3545, 15700, 304, 1045, 2205, 22823, 17961, 11, 5310, 979, 279, 15039, 3550, 374, 1550, 55887, 323, 2477, 12, 12027, 327, 13, 25627, 12762, 4532, 5980, 25185, 525, 13814, 1483, 304, 3347, 32420, 5322, 1741, 438, 17071, 16181, 41338, 323, 646, 614, 264, 2664, 6012, 311, 1477, 803, 3644, 22823, 2750, 320, 50, 2152, 1225, 1842, 452, 2572, 220, 17, 15, 16, 17, 26, 30682, 13342, 1842, 452, 2572, 220, 17, 15, 16, 16, 568, 4354, 11, 279, 34447, 2783, 27715, 74151, 1393, 279, 12871, 12703, 323, 6157, 3635, 42985, 13, 15277, 11, 807, 4157, 387, 5961, 9251, 311, 6923, 28411, 42721, 10295, 13, 758, 419, 5567, 11, 582, 4774, 311, 15963, 98134, 57739, 323, 86875, 2205, 8837, 311, 7269, 279, 6291, 4271, 315, 1482, 3359, 25185, 1393, 1660, 2952, 311, 5452, 311, 1550, 55887, 5322, 4192, 1092, 6863, 37936, 66609, 323, 18929, 25185, 2619, 3000, 5257, 66609, 25185, 16626, 9251, 311, 3614, 17272, 530, 25262, 5322, 13, 758, 1493, 25185, 11, 807, 1430, 311, 32064, 279, 6239, 1948, 55980, 323, 4194, 13, 7718, 398, 11, 807, 686, 2711, 369, 2155, 17961, 323, 5943, 279, 3873, 6174, 11, 323, 1221, 2182, 803, 6529, 389, 279, 12040, 1661, 11178, 13, 13532, 9313, 46241, 47132, 2711, 320, 6123, 78, 609, 1800, 10938, 11, 220, 16, 24, 24, 20, 8, 13719, 1661, 9904, 304, 458, 86875, 1616, 13, 1084, 1156, 26885, 264, 738, 315, 9904, 323, 990, 264, 55980, 729, 311, 7077, 1493, 9904, 13, 24885, 11, 1661, 11178, 525, 9099, 304, 264, 21739, 9144, 1140, 11, 323, 26618, 11882, 979, 29064, 279, 6291, 13, 15139, 84, 2711, 320, 38, 56497, 609, 93098, 11, 220, 16, 24, 24, 24, 8, 49143, 264, 9144, 323, 12341, 1181, 13922, 18709, 11, 4460, 311, 1477, 458, 12944, 6291, 13, 758, 1973, 311, 5648, 15700, 287, 304, 2205, 22823, 5671, 11, 432, 31995, 264, 1140, 2598, 5651, 84, 1140, 311, 3553, 3267, 1661, 9904, 320, 60817, 2205, 22823, 9904, 568, 758, 4623, 26475, 11, 432, 686, 5358, 504, 3330, 369, 419, 3082, 13, 6944, 19827, 1075, 74126, 85759, 320, 16128, 8, 323, 4467, 7757, 27926, 6132, 320, 7778, 8, 1083, 1430, 311, 10902, 4194, 304, 279, 15039, 1882, 13, 758, 419, 5567, 11, 582, 4936, 990, 13047, 533, 19851, 4405, 320, 8590, 8, 320, 64141, 550, 263, 609, 17589, 19059, 277, 11, 220, 17, 15, 16, 21, 8, 311, 4057, 650, 15333, 9144, 32383, 5367, 7887, 13, 576, 3565, 646, 387, 1730, 304, 11113, 220, 18, 13, 16, 4192, 2, 220, 18, 3168, 5308, 7150, 35, 8753, 74585, 1406, 37763, 367, 25, 40425, 3118, 8776, 525, 6707, 15700, 304, 2205, 53667, 13, 7496, 6350, 28411, 42721, 8776, 10902, 86875, 2205, 8837, 311, 1477, 28411, 42721, 10295, 1365, 5916, 504, 458, 2856, 1459, 11, 807, 5367, 7887, 2647, 279, 6291, 3080, 55441, 13, 1752, 3110, 11, 304, 4158, 12625, 8776, 1741, 438, 356, 91336, 323, 30044, 35, 5413, 11, 807, 9213, 518, 73042, 264, 2477, 12, 12027, 327, 4709, 729, 553, 86875, 20169, 8837, 13, 758, 19037, 220, 16, 2877, 701, 582, 7089, 279, 220, 17, 32420, 21615, 315, 279, 5480, 7329, 315, 264, 29728, 3922, 13, 1205, 646, 22986, 429, 1052, 525, 1378, 2205, 8028, 82, 27, 12776, 29, 16, 522, 12776, 14276, 18099, 1473, 429, 1052, 525, 1378, 2205, 8028, 82, 323, 279, 3359, 12111, 19313, 4188, 311, 825, 315, 1105, 4092, 279, 279, 17465, 5537, 13, 34239, 11, 304, 5480, 5980, 8776, 11, 6350, 5413, 1191, 504, 1045, 1459, 389, 279, 5480, 7329, 323, 1221, 5367, 7887, 2647, 279, 1459, 23490, 389, 279, 7329, 2987, 553, 20169, 2647, 320, 1143, 826, 1842, 452, 2572, 220, 17, 15, 16, 23, 26, 24495, 1842, 452, 2572, 220, 17, 15, 16, 24, 65, 8, 476, 4194, 4227, 320, 33, 9300, 301, 1842, 452, 2572, 220, 17, 15, 16, 22, 568, 758, 19037, 220, 16, 1883, 8, 582, 7089, 279, 5480, 7329, 315, 264, 479, 9548, 51, 13, 1205, 22986, 264, 4428, 4265, 429, 1052, 525, 1657, 2205, 1308, 7523, 304, 279, 479, 9548, 51, 5480, 18732, 4192, 0, 9697, 64, 8, 76416, 389, 44678, 13, 9533, 3642, 14, 19, 18, 16, 638, 18, 65, 16, 64, 24, 1999, 17, 580, 22, 22, 22, 20, 16, 68, 16, 10798, 23, 17, 18, 68, 17, 18, 631, 24, 68, 15, 18, 66, 1859, 15, 20, 21, 20, 65, 18, 22, 18, 18, 22, 22, 22, 20, 20, 13233, 24, 67, 15, 823, 22, 69, 24, 67, 4819, 3623, 0, 9697, 65, 8, 76416, 389, 479, 9548, 51, 13, 19037, 220, 16, 25, 40425, 18732, 320, 9745, 323, 479, 9548, 51, 4119, 389, 34995, 3846, 10337, 8, 27348, 389, 264, 1378, 32420, 17071, 19025, 13, 2014, 5157, 892, 220, 17, 35, 17071, 19025, 311, 2390, 311, 11, 582, 1598, 264, 5480, 5980, 3359, 504, 1378, 4194, 17465, 3501, 11, 323, 990, 862, 96775, 18212, 64816, 17961, 438, 279, 4621, 311, 1352, 279, 17071, 19025, 13, 1205, 1221, 3239, 279, 5480, 18732, 389, 419, 17071, 19025, 311, 7089, 1493, 12396, 13, 9533, 3642, 14, 23, 64, 19, 23, 64, 22, 21, 22, 64, 17, 20, 23, 4645, 23, 15, 20, 19, 18, 16, 19, 24, 22, 23, 65, 24, 65, 17, 24, 16, 9792, 21, 67, 17, 15, 15, 18, 64, 18, 21, 70596, 22, 22, 68, 23, 22, 15, 18, 638, 20, 69, 20, 24, 15, 68, 21, 22, 16, 16, 17, 4819, 3623, 1654, 4623, 75398, 1246, 6001, 279, 3491, 374, 13, 1913, 458, 34995, 3846, 3922, 11, 19037, 220, 17, 2877, 8, 4933, 279, 7982, 315, 96775, 28411, 42721, 18212, 9048, 804, 315, 356, 91336, 3359, 320, 5782, 12625, 3359, 8, 323, 7075, 23620, 2828, 3359, 320, 63938, 5980, 3359, 8, 1212, 13187, 220, 19, 15, 15, 4194, 2856, 3501, 13, 1205, 22986, 429, 279, 96775, 9904, 315, 356, 91336, 3359, 525, 5008, 37526, 1948, 508, 16, 13, 19, 16, 11, 220, 16, 13, 19, 22, 936, 4354, 11, 979, 12831, 5480, 5980, 3359, 1741, 438, 7075, 23620, 2828, 11, 279, 96775, 9904, 525, 13570, 8865, 504, 220, 16, 13, 18, 21, 311, 220, 16, 13, 20, 20, 13, 758, 4586, 11, 1039, 21204, 11827, 429, 5480, 3118, 8776, 525, 1753, 803, 16216, 311, 17465, 13, 1096, 374, 1576, 807, 1172, 2647, 9904, 389, 279, 5480, 18732, 1393, 356, 91336, 323, 30044, 35, 3359, 646, 2647, 6291, 4766, 48316, 2929, 279, 18732, 4192, 56317, 11, 1741, 24844, 374, 8036, 979, 279, 11734, 1614, 374, 479, 9548, 51, 13, 1752, 3110, 11, 304, 19037, 220, 17, 1883, 8, 582, 646, 1490, 279, 96775, 6291, 8865, 504, 220, 15, 13, 20, 311, 220, 16, 13, 20, 979, 18950, 7075, 23620, 2828, 3359, 13, 15277, 11, 279, 6291, 315, 894, 3175, 1598, 315, 7075, 23620, 2828, 389, 479, 9548, 51, 4157, 2167, 8708, 279, 8028, 28411, 42721, 18212, 64816, 315, 279, 2661, 1614, 11, 323, 8450, 432, 374, 16587, 311, 2884, 458, 12111, 429, 19313, 4188, 311, 264, 2664, 6291, 13, 8704, 279, 24844, 374, 803, 15386, 369, 5480, 3118, 8776, 11, 582, 686, 14576, 5244, 389, 18392, 279, 4271, 315, 5480, 3118, 8776, 304, 419, 5567, 11, 1393, 304, 4586, 1039, 1714, 646, 1083, 387, 1483, 311, 7760, 279, 5068, 315, 4158, 12625, 8776, 4745, 745, 11, 438, 35662, 304, 64714, 362, 4192, 0, 9697, 64, 8, 7075, 23620, 2828, 608, 356, 91336, 389, 422, 9745, 13, 9533, 3642, 14, 18, 23, 1859, 17, 64, 24, 16, 22, 93678, 23, 17, 580, 19, 18, 18, 66, 18, 17, 21, 19, 65, 21, 23, 21, 68, 22, 16, 65, 22, 22, 18, 24, 22, 18, 18, 15, 67, 24, 17, 21, 23, 67, 15, 18, 64, 22, 19, 21, 17, 15, 20, 22, 17, 22, 24, 23, 21, 15, 20, 20, 4819, 3623, 0, 9697, 65, 8, 7075, 23620, 2828, 3359, 389, 479, 9548, 51, 13, 19037, 220, 17, 25, 33909, 315, 96775, 28411, 42721, 18212, 64816, 7465, 389, 458, 34995, 3846, 2168, 13, 576, 12396, 1473, 429, 279, 1590, 220, 400, 43, 15159, 17, 31716, 220, 49871, 646, 387, 1602, 2155, 1576, 315, 5257, 5916, 17961, 13, 19037, 220, 17, 2877, 8, 323, 220, 17, 1883, 8, 1473, 279, 30281, 369, 1590, 18212, 9048, 804, 11, 323, 19037, 220, 17, 1337, 8, 4933, 279, 19313, 3173, 15655, 315, 7075, 23620, 2828, 3359, 389, 264, 29728, 3922, 1614, 389, 34995, 3846, 10337, 13, 9533, 3642, 14, 24, 18, 22, 19, 21, 24, 67, 19, 17, 23, 15, 66, 20, 65, 10798, 69, 21, 21, 18, 54793, 23, 15, 19, 23, 67, 18, 66, 15, 18, 3235, 23, 15, 67, 24, 67, 20, 66, 15, 17, 8904, 18, 15, 22, 23, 23, 69, 21, 22, 18, 68, 22, 18, 24, 24, 17, 18, 16, 20, 68, 4819, 3623, 0, 9697, 66, 8, 27604, 25886, 6165, 85873, 35933, 13, 9533, 3642, 14, 22, 18, 21, 22, 64, 15, 7221, 22, 1371, 17, 18, 18, 18, 23, 18, 65, 20, 18, 22, 20, 24, 20, 66, 19, 66, 16, 15, 22, 19, 17, 19, 69, 16, 18, 66, 19, 23, 19, 20, 23, 19, 1859, 22, 68, 23, 21, 15, 67, 17, 21, 64, 22, 65, 17, 20, 65, 20, 20, 19, 23, 24, 4819, 3623, 2, 220, 18, 13, 16, 362, 52312, 16292, 84416, 46805, 4613, 39440, 1285, 21706, 42817, 24211, 22194, 35, 98450, 1406, 22043, 264, 2205, 2647, 3118, 3359, 220, 57960, 10374, 5416, 90, 32, 31716, 1154, 1039, 5795, 374, 311, 1477, 264, 6291, 448, 12944, 4271, 13, 2014, 419, 835, 11, 582, 29614, 264, 8823, 12111, 311, 2621, 419, 4265, 553, 53852, 18927, 5980, 320, 22587, 29221, 8, 3691, 12625, 25262, 448, 86875, 2205, 8837, 13, 1634, 6839, 304, 40325, 220, 16, 11, 1039, 12111, 31995, 264, 9144, 7314, 220, 57960, 10374, 6066, 90, 47, 19444, 64, 3, 220, 429, 10533, 678, 279, 4541, 32383, 11, 1380, 1817, 6546, 220, 57960, 79, 3096, 90, 84, 92, 1124, 258, 1124, 10374, 6066, 90, 47, 19444, 64, 3, 220, 374, 458, 28439, 29629, 315, 12111, 220, 57960, 10374, 5416, 90, 32, 31716, 659, 7281, 11, 582, 9658, 429, 1052, 374, 458, 3359, 16538, 220, 57960, 10374, 5416, 90, 34, 31716, 220, 1741, 429, 220, 57960, 10374, 5416, 90, 34, 92, 11520, 79, 3096, 90, 84, 5410, 3, 220, 10953, 279, 4271, 315, 279, 6291, 13, 1752, 5480, 3118, 8776, 11, 279, 5795, 374, 311, 1477, 279, 22823, 5106, 311, 29337, 279, 6010, 311, 279, 18732, 3156, 429, 5106, 320, 1143, 826, 1842, 452, 2572, 220, 17, 15, 16, 23, 49555, 54815, 11, 220, 57960, 79, 3096, 90, 84, 31716, 220, 374, 279, 5106, 315, 28411, 42721, 18212, 64816, 323, 1406, 14085, 271, 14085, 198, 59, 10374, 5416, 314, 34, 92, 19788, 14824, 18785, 314, 84, 5410, 284, 1124, 1065, 220, 716, 28152, 12935, 861, 220, 15, 92, 1124, 12935, 1124, 1318, 314, 82, 659, 259, 659, 92, 282, 1124, 2359, 2075, 716, 314, 15, 92, 488, 1124, 12935, 1124, 37018, 28152, 14824, 18785, 314, 84, 3417, 35702, 91, 1124, 14824, 18785, 314, 84, 92, 97155, 11035, 1291, 8, 1124, 811, 80, 379, 716, 314, 15, 1583, 14085, 271, 14085, 1406, 2870, 220, 400, 88, 62, 15, 3, 220, 374, 279, 4396, 2383, 13, 1096, 646, 387, 24182, 553, 264, 6915, 24321, 2627, 5519, 7868, 2711, 10324, 320, 4060, 56707, 1842, 452, 13, 320, 17, 15, 16, 23, 5731, 323, 304, 2097, 11, 304, 1429, 315, 279, 25185, 220, 57960, 10374, 5416, 90, 34, 92, 11520, 14824, 18785, 90, 84, 5410, 3, 220, 374, 5961, 18401, 2337, 279, 25262, 10324, 320, 33, 9300, 301, 1842, 452, 2572, 220, 17, 15, 16, 22, 26, 56707, 1842, 452, 2572, 220, 17, 15, 16, 24, 568, 2411, 1817, 19569, 11, 582, 1598, 220, 400, 76, 3, 220, 25687, 315, 220, 57960, 10374, 5416, 90, 32, 31716, 220, 389, 1817, 4541, 6546, 220, 57960, 14824, 18785, 90, 84, 92, 1124, 258, 1124, 10374, 6066, 90, 47, 19444, 64, 3, 220, 311, 633, 279, 12944, 32383, 13, 5005, 582, 6786, 279, 2701, 1378, 7525, 311, 7949, 279, 9144, 7314, 1379, 323, 311, 592, 1516, 501, 32383, 311, 13186, 2989, 5147, 1306, 3118, 389, 98134, 25262, 13, 1205, 4263, 1817, 3019, 304, 3565, 438, 3685, 4192, 7188, 533, 19851, 4405, 320, 8590, 8, 311, 3931, 650, 15333, 9144, 6546, 13, 4636, 21010, 1817, 9144, 553, 220, 400, 76, 3, 220, 25687, 11, 582, 12564, 279, 16538, 729, 897, 315, 1817, 9144, 323, 37867, 279, 11785, 4279, 315, 1105, 13, 13704, 7887, 17719, 279, 9144, 738, 1119, 4279, 14177, 973, 279, 12111, 11, 1393, 2058, 20337, 458, 13382, 6291, 7314, 13, 1096, 4522, 702, 1012, 1483, 304, 17071, 16181, 2711, 320, 64141, 550, 263, 609, 17589, 19059, 277, 11, 220, 17, 15, 16, 21, 8, 714, 702, 537, 1012, 1483, 304, 28411, 42721, 3359, 4192, 22587, 29221, 57739, 320, 4677, 8, 369, 12157, 291, 1800, 29709, 13, 2014, 19131, 32273, 304, 279, 28439, 7354, 323, 13186, 1008, 2989, 13604, 11, 582, 29614, 264, 32587, 592, 29709, 8282, 311, 45364, 279, 9144, 7314, 13, 576, 4586, 4522, 374, 311, 592, 1516, 504, 279, 6291, 3550, 304, 279, 6149, 3019, 3118, 389, 279, 6540, 18892, 1573, 323, 5244, 389, 25383, 1186, 32537, 13, 44763, 11, 582, 990, 264, 98134, 25262, 1714, 2598, 8942, 4270, 5679, 9403, 13689, 320, 51, 1740, 8, 320, 33, 2375, 13342, 1842, 452, 2572, 220, 17, 15, 16, 16, 8, 311, 592, 1516, 501, 32383, 4192, 641, 1973, 311, 653, 592, 29709, 11, 582, 10306, 2441, 7314, 220, 57960, 10374, 6066, 90, 47, 19444, 82, 3, 220, 429, 10533, 678, 279, 3681, 25687, 10660, 2670, 279, 3931, 6565, 6174, 11, 2474, 678, 279, 1995, 686, 387, 5390, 369, 592, 29709, 13, 1634, 6839, 304, 40325, 220, 17, 11, 582, 1156, 21749, 279, 13166, 821, 304, 220, 57960, 10374, 6066, 90, 47, 19444, 82, 3, 220, 1119, 10960, 323, 2664, 5479, 3118, 389, 279, 5815, 16538, 729, 897, 13, 1205, 1221, 5426, 1378, 8651, 36603, 72610, 9403, 71036, 320, 42, 1150, 8, 3371, 9253, 438, 220, 400, 75, 11520, 50853, 15087, 220, 323, 220, 400, 70, 11520, 50853, 15087, 220, 389, 1493, 1378, 74505, 4192, 14085, 271, 14085, 198, 59, 2359, 59, 90, 1124, 7265, 90, 1653, 15170, 75, 92, 326, 19788, 14824, 18785, 314, 84, 5410, 284, 281, 1124, 2359, 11520, 10374, 5416, 314, 34, 92, 19788, 14824, 18785, 314, 84, 5410, 1124, 273, 80, 1124, 7141, 1124, 16106, 1124, 14824, 18785, 314, 84, 2137, 1124, 10374, 6066, 314, 47, 92, 716, 314, 82, 11035, 1291, 701, 24984, 342, 19788, 14824, 18785, 314, 84, 5410, 284, 281, 1124, 2359, 11520, 10374, 5416, 314, 34, 92, 19788, 14824, 18785, 314, 84, 5410, 861, 1124, 7141, 1124, 16106, 1124, 14824, 18785, 314, 84, 2137, 1124, 10374, 6066, 314, 47, 92, 716, 314, 82, 11035, 1291, 568, 1124, 408, 90, 1653, 92, 1124, 1291, 13, 1124, 4578, 314, 17, 532, 14085, 271, 14085, 1406, 785, 5733, 220, 57960, 7141, 3, 220, 374, 738, 311, 220, 400, 17, 15, 59, 4, 3, 1154, 892, 25351, 279, 2664, 949, 220, 400, 75, 11520, 14824, 18785, 90, 84, 5410, 3, 220, 702, 220, 400, 17, 15, 59, 4, 3, 220, 315, 32383, 304, 220, 57960, 10374, 6066, 90, 47, 19444, 82, 3, 220, 323, 279, 10960, 949, 220, 400, 70, 11520, 14824, 18785, 90, 84, 5410, 3, 220, 702, 279, 9664, 220, 400, 23, 15, 59, 4, 3, 1154, 12040, 13, 24885, 11, 582, 6077, 501, 821, 448, 279, 8028, 897, 315, 220, 400, 75, 11520, 50853, 8, 608, 342, 11520, 50853, 15087, 1154, 892, 646, 387, 18651, 311, 614, 7192, 8674, 15673, 304, 84670, 220, 19, 320, 4060, 803, 1995, 304, 64714, 425, 568, 8704, 582, 646, 537, 5961, 1477, 1741, 3501, 11, 582, 6077, 369, 264, 2421, 3039, 320, 1782, 1372, 374, 738, 311, 220, 16, 15, 15, 2337, 279, 9342, 8, 504, 220, 400, 75, 11520, 50853, 15087, 220, 323, 2506, 279, 825, 448, 279, 17377, 220, 400, 75, 11520, 50853, 8, 608, 342, 11520, 50853, 15087, 220, 4192, 2, 8753, 74585, 220, 16, 3168, 5308, 7150, 35, 425, 86188, 98450, 56416, 18470, 4192, 2505, 25, 4903, 220, 400, 69, 3, 1154, 4024, 3110, 220, 400, 87, 62, 15, 3, 1154, 3359, 16538, 220, 57960, 10374, 5416, 90, 34, 31716, 1154, 20169, 5980, 3359, 12111, 220, 57960, 10374, 5416, 90, 32, 31716, 1154, 14376, 9873, 220, 400, 44, 3, 1154, 14376, 4379, 220, 400, 82, 3, 1154, 14376, 9873, 5263, 4379, 220, 400, 76, 3, 6762, 12, 220, 16, 25, 10612, 398, 6077, 220, 400, 74, 3, 220, 2856, 32383, 311, 1352, 220, 57960, 10374, 6066, 90, 47, 19444, 64, 3, 220, 320, 38, 46972, 476, 13794, 4194, 3593, 12, 220, 17, 25, 220, 57960, 10374, 6066, 90, 47, 19444, 82, 59, 18691, 1124, 10374, 6066, 90, 47, 19444, 64, 66426, 12, 220, 18, 25, 369, 220, 400, 83, 284, 220, 16, 11, 17, 26266, 507, 2412, 3, 220, 653, 271, 12, 220, 19, 25, 369, 1817, 220, 57960, 79, 3096, 90, 84, 19444, 83, 1124, 258, 1124, 10374, 6066, 90, 47, 19444, 64, 3, 220, 653, 442, 2736, 3359, 389, 678, 32383, 271, 12, 220, 20, 25, 369, 220, 400, 73, 284, 220, 16, 26266, 67816, 1154, 44, 3, 220, 653, 442, 6786, 220, 400, 44, 3, 220, 25687, 1573, 14376, 271, 12, 220, 21, 25, 220, 57960, 79, 3096, 90, 84, 19444, 83, 61, 35702, 32338, 11035, 18691, 1124, 10374, 5416, 90, 32, 92, 11520, 79, 3096, 90, 84, 19444, 83, 15087, 271, 12, 220, 22, 25, 220, 57960, 10374, 6066, 90, 47, 19444, 82, 59, 18691, 1124, 10374, 6066, 90, 47, 19444, 82, 59, 36665, 1124, 90, 11520, 79, 3096, 314, 84, 19444, 83, 516, 59, 10374, 5416, 90, 34, 92, 11520, 79, 3096, 314, 84, 19444, 83, 3789, 59, 92, 45377, 220, 442, 13583, 678, 9873, 7354, 271, 12, 220, 23, 25, 5549, 220, 57960, 79, 3096, 90, 84, 19444, 83, 3, 220, 304, 220, 57960, 10374, 6066, 90, 47, 19444, 64, 3, 220, 448, 220, 57960, 79, 3096, 90, 84, 19444, 83, 44701, 659, 442, 5549, 279, 6546, 304, 220, 57960, 10374, 6066, 90, 47, 19444, 64, 66426, 12, 220, 24, 25, 10428, 279, 11785, 220, 400, 82, 59, 4, 3, 220, 315, 32383, 504, 220, 57960, 10374, 6066, 90, 47, 19444, 64, 66426, 12, 220, 16, 15, 25, 220, 57960, 10374, 6066, 90, 47, 19444, 64, 59, 18691, 1124, 10374, 6066, 90, 47, 19444, 64, 59, 36665, 3, 220, 350, 1740, 11588, 29709, 220, 4930, 59, 10374, 6066, 90, 47, 19444, 82, 11, 22957, 10374, 6066, 90, 47, 19444, 64, 91, 9, 82, 59, 11334, 66426, 12, 220, 16, 16, 25, 220, 400, 44, 59, 18691, 386, 59, 50853, 320, 16, 488, 296, 10699, 4, 3, 220, 442, 39633, 279, 15039, 9873, 1406, 13874, 3989, 2505, 25, 50441, 2771, 16879, 220, 57960, 10374, 6066, 90, 47, 19444, 82, 3, 220, 1154, 592, 1516, 3039, 220, 400, 51, 3, 220, 659, 220, 16, 25, 9008, 220, 57960, 10374, 6066, 90, 47, 19444, 75, 3, 220, 438, 458, 4287, 1140, 26, 220, 17, 25, 62902, 220, 57960, 79, 3096, 314, 84, 11035, 258, 1124, 10374, 6066, 90, 47, 19444, 82, 3, 220, 1119, 1378, 25993, 220, 57960, 10374, 6066, 90, 43, 31716, 220, 320, 57134, 8, 323, 220, 57960, 10374, 6066, 90, 39, 31716, 220, 320, 86, 10836, 8, 3118, 389, 16538, 729, 220, 57960, 10374, 5416, 90, 34, 31716, 220, 659, 220, 18, 25, 7854, 1378, 8651, 75083, 82, 389, 220, 57960, 10374, 6066, 90, 43, 31716, 220, 323, 220, 57960, 10374, 6066, 90, 39, 31716, 220, 3371, 9253, 438, 220, 400, 75, 11520, 50853, 15087, 220, 323, 220, 400, 71, 11520, 50853, 15087, 220, 15576, 26, 220, 19, 25, 5443, 10587, 7542, 311, 1477, 279, 1850, 75083, 33394, 220, 400, 65, 15159, 75, 2137, 65, 15159, 71, 31716, 220, 369, 220, 400, 75, 11520, 50853, 15087, 220, 323, 220, 400, 71, 11520, 50853, 15087, 220, 659, 220, 20, 25, 369, 1817, 220, 400, 83, 59, 258, 508, 15, 19997, 94280, 220, 653, 220, 21, 25, 17465, 25, 220, 400, 74, 284, 220, 15, 3, 220, 1154, 1308, 10405, 220, 400, 28, 1124, 258, 36958, 3, 220, 659, 220, 22, 25, 1393, 220, 400, 74, 366, 256, 1124, 2810, 3, 220, 6077, 3039, 653, 220, 23, 25, 19143, 220, 57960, 79, 3096, 90, 84, 51535, 6242, 31716, 220, 504, 220, 400, 75, 11520, 50853, 15087, 220, 659, 220, 24, 25, 421, 1308, 10405, 220, 400, 29, 70, 8154, 15159, 6242, 5410, 608, 326, 8154, 15159, 6242, 5410, 3, 220, 1221, 26, 220, 16, 15, 25, 220, 57960, 79, 3096, 314, 84, 19444, 83, 59, 18691, 1124, 79, 3096, 314, 84, 51535, 6242, 31716, 220, 659, 220, 16, 16, 25, 1308, 10405, 220, 400, 28, 342, 11520, 79, 3096, 314, 84, 51535, 6242, 5410, 608, 326, 11520, 79, 3096, 314, 84, 51535, 6242, 5410, 3, 220, 659, 220, 16, 17, 25, 220, 400, 74, 59, 18691, 595, 488, 220, 16, 3, 220, 220, 16, 18, 25, 220, 57960, 10374, 6066, 90, 47, 19444, 75, 59, 18691, 1124, 10374, 6066, 90, 47, 19444, 75, 59, 36665, 1124, 90, 11520, 79, 3096, 314, 84, 19444, 83, 26266, 10374, 5416, 90, 34, 92, 11520, 79, 3096, 314, 84, 19444, 83, 593, 59, 92, 2587, 3, 220, 220, 16, 19, 25, 470, 220, 57960, 10374, 6066, 90, 47, 19444, 75, 25046, 73594, 1406, 785, 2874, 582, 990, 350, 1740, 369, 592, 29709, 374, 429, 279, 54480, 2783, 27715, 13482, 398, 448, 279, 1372, 315, 821, 3501, 304, 220, 57960, 10374, 6066, 90, 47, 19444, 82, 3, 659, 758, 12313, 11, 8606, 98134, 25262, 1714, 1075, 48568, 8603, 320, 24430, 8, 686, 1373, 40899, 7246, 311, 6923, 501, 3501, 13, 15277, 11, 350, 1740, 374, 803, 14452, 369, 1550, 55887, 5322, 4192, 641, 279, 21204, 582, 1477, 429, 279, 1590, 1850, 6546, 10008, 4041, 504, 592, 29709, 11, 4518, 315, 279, 738, 315, 5916, 32383, 13, 1096, 35762, 279, 26279, 315, 592, 29709, 2337, 2711, 11, 279, 46516, 3059, 686, 387, 6839, 304, 11113, 220, 19, 13, 17, 4192, 2, 220, 19, 80081, 1791, 43878, 1406, 1654, 6786, 21204, 389, 5257, 4119, 323, 29425, 311, 10146, 279, 15024, 323, 26279, 315, 279, 10981, 5486, 13, 1205, 1430, 311, 18379, 279, 5068, 315, 5480, 5980, 3359, 389, 2168, 23850, 9079, 1075, 34995, 3846, 11, 97590, 934, 12, 16, 15, 323, 4654, 6954, 11, 323, 1083, 6786, 21204, 389, 4916, 1614, 1075, 479, 9548, 51, 323, 17984, 1614, 1075, 444, 915, 13, 23405, 11, 582, 19869, 429, 1039, 8823, 18779, 7095, 374, 1083, 2952, 311, 7269, 6350, 4158, 12625, 8776, 4192, 2, 220, 19, 13, 16, 42817, 24211, 7671, 4066, 35, 98450, 6197, 7856, 50685, 57436, 50, 1406, 1654, 6786, 21204, 389, 2326, 5297, 29425, 25, 34995, 3846, 320, 2304, 34, 359, 1842, 452, 2572, 220, 16, 24, 24, 23, 701, 97590, 934, 12, 16, 15, 320, 42, 24741, 80813, 26684, 1842, 452, 2572, 220, 17, 15, 16, 15, 8, 323, 4654, 6954, 12, 16, 15, 15, 15, 320, 35, 826, 1842, 452, 2572, 220, 17, 15, 15, 24, 568, 576, 29728, 3922, 1614, 17646, 374, 279, 1852, 448, 279, 6174, 4961, 304, 56707, 1842, 452, 13, 320, 17, 15, 16, 23, 1648, 369, 2176, 34995, 3846, 323, 97590, 934, 582, 990, 279, 3922, 448, 3040, 55712, 13617, 11, 1378, 1932, 2268, 1749, 287, 13617, 323, 1378, 7225, 72188, 13617, 11, 892, 11075, 220, 400, 24, 24, 13, 20, 59, 4, 3, 220, 13403, 389, 34995, 3846, 323, 220, 400, 23, 17, 13, 20, 59, 4, 3, 220, 13403, 389, 97590, 934, 12, 16, 15, 438, 4961, 304, 320, 8852, 3732, 72, 609, 51375, 11, 220, 17, 15, 16, 22, 65, 26, 56707, 1842, 452, 2572, 220, 17, 15, 16, 23, 568, 1752, 4654, 6954, 11, 582, 990, 279, 80669, 1800, 4711, 12, 20, 15, 320, 1519, 1842, 452, 2572, 220, 17, 15, 16, 21, 8, 3922, 3897, 553, 67825, 320, 12061, 3672, 609, 40632, 11, 220, 17, 15, 16, 15, 701, 892, 82591, 264, 6909, 12, 16, 13403, 315, 220, 400, 22, 21, 13, 16, 20, 59, 4, 3, 659, 1205, 26618, 3293, 220, 16, 15, 15, 10295, 504, 1273, 7289, 369, 16460, 13, 576, 5029, 315, 279, 10981, 25185, 646, 387, 1730, 304, 6633, 220, 21, 304, 64714, 422, 4192, 81110, 6291, 4271, 315, 6350, 5413, 1205, 9429, 279, 6291, 4271, 315, 279, 10981, 12111, 448, 2326, 6350, 5480, 5980, 3359, 5413, 25, 76416, 3359, 320, 33, 9300, 301, 1842, 452, 2572, 220, 17, 15, 16, 22, 701, 40034, 12, 20566, 320, 1143, 826, 1842, 452, 2572, 220, 17, 15, 16, 23, 8, 323, 7075, 23620, 2828, 3359, 320, 1143, 826, 1842, 452, 2572, 220, 17, 15, 16, 24, 8, 389, 34995, 3846, 11, 97590, 934, 12, 16, 15, 323, 4654, 6954, 821, 7289, 13, 1752, 1039, 12111, 11, 582, 990, 7075, 23620, 2828, 3359, 438, 279, 2331, 12111, 323, 738, 220, 400, 74, 284, 220, 18, 15, 3, 220, 369, 279, 2856, 9144, 7314, 13, 576, 5461, 220, 400, 43, 15159, 17, 31716, 220, 18212, 64816, 315, 1039, 1714, 323, 3046, 10999, 525, 10449, 304, 6633, 220, 23, 13, 7036, 429, 678, 279, 5480, 3118, 8776, 10306, 28439, 5367, 973, 389, 279, 5480, 18732, 11, 773, 807, 2677, 2550, 264, 6849, 3359, 13, 576, 1887, 12313, 374, 279, 5461, 220, 400, 43, 15159, 17, 31716, 220, 18212, 64816, 311, 11596, 279, 19898, 13, 1205, 1083, 1795, 56707, 1842, 452, 13, 320, 17, 15, 16, 23, 8, 311, 1895, 20790, 13047, 19775, 320, 1911, 49, 8, 553, 37614, 11341, 315, 28411, 42721, 10295, 448, 18212, 64816, 220, 400, 27, 1124, 31767, 3, 220, 320, 57960, 31767, 3, 220, 374, 11882, 3118, 389, 1406, 63650, 9079, 568, 576, 3059, 1473, 429, 646, 1492, 5480, 5980, 8776, 11075, 4722, 220, 400, 43, 15159, 17, 31716, 220, 18212, 64816, 323, 5080, 3359, 2393, 4379, 13, 576, 11682, 6358, 374, 6839, 304, 279, 1790, 3772, 4192, 785, 10981, 12111, 646, 1083, 387, 1483, 311, 7760, 279, 5068, 315, 1008, 5480, 3118, 8776, 13, 6633, 220, 22, 304, 279, 64714, 31116, 429, 279, 10981, 12111, 20699, 34950, 279, 220, 400, 43, 15159, 17, 31716, 220, 18212, 64816, 323, 2393, 4379, 315, 76416, 3359, 323, 40034, 12, 20566, 4192, 2556, 220, 16, 25, 18099, 315, 2588, 6942, 3691, 12625, 3359, 389, 34995, 3846, 11, 97590, 934, 12, 16, 15, 323, 4654, 6954, 12, 16, 15, 15, 15, 13, 1205, 9429, 279, 5068, 315, 3807, 3359, 25185, 1212, 650, 5657, 291, 6243, 4192, 20993, 1784, 376, 1784, 1296, 1472, 1296, 1784, 1296, 16848, 428, 17, 755, 55181, 3846, 522, 1296, 1784, 1296, 16848, 428, 17, 755, 34, 2773, 934, 12, 16, 15, 522, 1296, 1784, 1296, 16848, 428, 17, 755, 1906, 6954, 12, 16, 15, 15, 15, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 1472, 1296, 1784, 1296, 23465, 7239, 444, 17, 522, 1296, 1784, 1296, 29, 1911, 49, 320, 30143, 609, 4832, 26, 220, 16, 13, 15, 12533, 1296, 1784, 1296, 23465, 7239, 444, 17, 522, 1296, 1784, 1296, 29, 1911, 49, 320, 30143, 609, 4832, 26, 220, 15, 13, 16, 18, 12533, 1296, 1784, 1296, 23465, 7239, 13, 444, 17, 522, 1296, 1784, 1296, 29, 1911, 49, 320, 30143, 609, 4832, 26, 220, 16, 13, 19, 12533, 1296, 1472, 376, 1784, 376, 1784, 1296, 25630, 27066, 26, 54, 320, 14075, 12625, 12533, 1296, 1784, 1296, 29, 15, 13, 24, 21, 522, 1296, 1784, 1296, 29, 21, 15, 52508, 1296, 1784, 1296, 29, 15, 13, 16, 17, 522, 1296, 1784, 1296, 29, 21, 16, 52508, 1296, 1784, 1296, 29, 16, 13, 20, 18, 522, 1296, 1784, 1296, 29, 19, 24, 52508, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 57977, 3359, 522, 1296, 1784, 1296, 29, 16, 13, 16, 18, 522, 1296, 1784, 1296, 29, 19, 16, 52508, 1296, 1784, 1296, 29, 15, 13, 16, 20, 522, 1296, 1784, 1296, 29, 19, 23, 52508, 1296, 1784, 1296, 29, 17, 13, 15, 17, 522, 1296, 1784, 1296, 29, 16, 24, 52508, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 38021, 5980, 3359, 522, 1296, 1784, 1296, 29, 16, 13, 15, 24, 522, 1296, 1784, 1296, 29, 19, 21, 52508, 1296, 1784, 1296, 29, 15, 13, 16, 19, 522, 1296, 1784, 1296, 29, 20, 15, 52508, 1296, 1784, 1296, 29, 16, 13, 21, 22, 522, 1296, 1784, 1296, 29, 18, 23, 52508, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 7264, 23620, 2828, 3359, 522, 1296, 1784, 1296, 29, 16, 13, 15, 22, 522, 1296, 1784, 1296, 29, 19, 24, 52508, 1296, 1784, 1296, 29, 15, 13, 16, 19, 522, 1296, 1784, 1296, 29, 20, 16, 52508, 1296, 1784, 1296, 29, 16, 13, 19, 18, 522, 1296, 1784, 1296, 29, 20, 24, 52508, 1296, 1472, 376, 1784, 376, 1784, 1296, 36721, 86188, 7075, 23620, 2828, 3359, 522, 1296, 1784, 1296, 29, 15, 13, 24, 16, 522, 1296, 1784, 1296, 29, 21, 22, 52508, 1296, 1784, 1296, 29, 15, 13, 16, 15, 522, 1296, 1784, 1296, 29, 21, 20, 52508, 1296, 1784, 1296, 29, 16, 13, 16, 23, 522, 1296, 1784, 1296, 29, 23, 16, 52508, 1296, 1472, 376, 1472, 2005, 10370, 2, 220, 19, 13, 17, 65757, 74464, 1406, 1654, 1221, 6786, 264, 3920, 311, 1273, 1817, 3692, 315, 1039, 12111, 323, 9429, 448, 279, 3046, 10999, 13, 576, 9342, 374, 2814, 389, 34995, 3846, 821, 1667, 7075, 23620, 2828, 3359, 438, 279, 2331, 3359, 1714, 13, 576, 3059, 525, 68629, 304, 6633, 220, 17, 4192, 2556, 220, 17, 25, 22771, 19379, 369, 279, 26279, 315, 48924, 14761, 4405, 323, 350, 1740, 592, 29709, 13, 576, 39402, 49709, 374, 3118, 389, 17439, 12, 9268, 278, 3359, 323, 85873, 3363, 2790, 19556, 315, 678, 17961, 4192, 20993, 1784, 376, 1784, 1296, 1472, 1296, 1784, 1296, 29, 24617, 53486, 522, 1296, 1784, 1296, 23465, 7239, 444, 17, 522, 1296, 1784, 1296, 29, 22777, 522, 1296, 1784, 1296, 29, 1911, 49, 320, 30143, 609, 4832, 26, 220, 16, 13, 15, 12533, 1296, 1784, 1296, 29, 55261, 522, 1296, 1784, 1296, 29, 22777, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 51288, 428, 19, 755, 20358, 47967, 7075, 23620, 2828, 522, 1296, 1784, 1296, 29, 16, 522, 1296, 1784, 1296, 29, 16, 13, 15, 22, 522, 1296, 1784, 1296, 29, 15, 52508, 1296, 1784, 1296, 29, 19, 24, 52508, 1296, 1784, 1296, 29, 17, 20, 11, 19, 20, 21, 522, 1296, 1784, 1296, 29, 16, 87, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 18, 15, 522, 1296, 1784, 1296, 29, 15, 13, 24, 23, 522, 1296, 1784, 1296, 29, 23, 13, 19, 52508, 1296, 1784, 1296, 29, 20, 22, 52508, 1296, 1784, 1296, 29, 22, 22, 16, 11, 17, 23, 18, 522, 1296, 1784, 1296, 29, 18, 15, 87, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 20, 15, 522, 1296, 1784, 1296, 29, 15, 13, 24, 19, 522, 1296, 1784, 1296, 29, 16, 17, 13, 16, 52508, 1296, 1784, 1296, 29, 21, 18, 52508, 1296, 1784, 1296, 29, 16, 11, 17, 21, 23, 11, 18, 24, 17, 522, 1296, 1784, 1296, 29, 20, 15, 87, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 16, 15, 15, 522, 1296, 1784, 1296, 29, 15, 13, 24, 16, 522, 1296, 1784, 1296, 29, 16, 20, 13, 15, 52508, 1296, 1784, 1296, 29, 21, 20, 52508, 1296, 1784, 1296, 29, 17, 11, 20, 21, 22, 11, 18, 23, 17, 522, 1296, 1784, 1296, 29, 16, 15, 15, 87, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 7188, 533, 19851, 4405, 7075, 23620, 2828, 522, 1296, 1784, 1296, 29, 18, 15, 522, 1296, 1784, 1296, 29, 15, 13, 24, 24, 522, 1296, 1784, 1296, 29, 22, 13, 20, 52508, 1296, 1784, 1296, 29, 20, 20, 52508, 1296, 1784, 1296, 29, 16, 21, 16, 11, 16, 23, 18, 522, 1296, 1784, 1296, 29, 21, 13, 18, 87, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 51288, 428, 18, 755, 33, 86188, 7075, 23620, 2828, 522, 1296, 1784, 1296, 29, 18, 15, 522, 1296, 1784, 1296, 29, 15, 13, 24, 16, 522, 1296, 1784, 1296, 29, 16, 20, 13, 15, 52508, 1296, 1784, 1296, 29, 21, 22, 52508, 1296, 1784, 1296, 29, 17, 20, 17, 11, 15, 16, 19, 522, 1296, 1784, 1296, 29, 24, 13, 24, 87, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 20, 15, 522, 1296, 1784, 1296, 29, 15, 13, 23, 23, 522, 1296, 1784, 1296, 29, 16, 22, 13, 23, 52508, 1296, 1784, 1296, 29, 22, 16, 52508, 1296, 1784, 1296, 29, 19, 15, 24, 11, 23, 19, 16, 522, 1296, 1784, 1296, 29, 16, 21, 13, 16, 87, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 16, 15, 15, 522, 1296, 1784, 1296, 29, 15, 13, 23, 22, 522, 1296, 1784, 1296, 29, 16, 23, 13, 22, 52508, 1296, 1784, 1296, 29, 22, 16, 52508, 1296, 1784, 1296, 29, 23, 17, 24, 11, 23, 21, 20, 522, 1296, 1784, 1296, 29, 18, 17, 13, 21, 87, 522, 1296, 1472, 376, 1472, 2005, 10370, 33487, 448, 49665, 5248, 17465, 5486, 13, 362, 49665, 1616, 311, 7269, 279, 6291, 4271, 315, 6350, 3359, 374, 311, 1598, 279, 3359, 389, 5248, 4194, 17465, 3501, 13, 1096, 8282, 702, 1012, 1483, 304, 4158, 12625, 3359, 400, 61, 18, 3, 220, 323, 374, 1083, 8415, 311, 279, 5480, 3118, 8776, 13, 1205, 9429, 7075, 23620, 2828, 448, 220, 18, 15, 11, 220, 20, 15, 11, 220, 16, 15, 15, 2856, 3501, 323, 279, 10981, 425, 86188, 64108, 7075, 23620, 2828, 5486, 304, 6633, 220, 17, 13, 576, 3059, 19869, 429, 48924, 14761, 4405, 7460, 1753, 2686, 19556, 1091, 4317, 3132, 4303, 5248, 2856, 32383, 13, 23662, 311, 592, 29709, 11, 279, 10981, 5486, 19313, 4188, 311, 264, 2664, 6291, 1212, 279, 1852, 2856, 7314, 1379, 13, 1752, 3110, 11, 311, 11075, 5461, 220, 400, 15, 13, 24, 16, 43, 15159, 17, 31716, 220, 18212, 64816, 11, 425, 86188, 64108, 7075, 23620, 2828, 7460, 220, 16, 15, 3039, 2686, 19556, 1091, 7299, 47967, 7075, 23620, 2828, 4192, 1695, 315, 279, 2856, 7314, 13, 576, 1379, 315, 2856, 7314, 320, 5183, 9253, 553, 220, 400, 74, 3, 220, 304, 1039, 12111, 8, 374, 458, 2989, 5733, 13, 6633, 220, 17, 4933, 429, 7703, 220, 400, 74, 3, 220, 1172, 702, 264, 31773, 2456, 1283, 220, 400, 74, 1124, 48053, 220, 18, 15, 3, 659, 3197, 31918, 14376, 323, 592, 29709, 16953, 1119, 279, 7075, 23620, 2828, 3359, 11, 279, 1590, 1850, 18212, 64816, 374, 2686, 16216, 311, 279, 1372, 315, 5916, 17961, 11, 892, 3363, 429, 592, 29709, 27236, 311, 1281, 279, 2711, 2686, 17749, 389, 279, 5916, 17961, 13, 62665, 10219, 374, 304, 64714, 356, 4192, 7738, 315, 48924, 14761, 4405, 323, 350, 1740, 592, 29709, 13, 1205, 3920, 279, 2456, 315, 1493, 1378, 6813, 25156, 13, 1634, 6839, 304, 19037, 220, 18, 2877, 701, 279, 5486, 315, 48924, 14761, 4405, 13598, 21244, 3123, 279, 10960, 220, 400, 82, 3, 220, 3266, 315, 32383, 3080, 79767, 2337, 264, 3151, 9873, 3080, 1052, 374, 1172, 825, 6077, 2115, 13, 3197, 34171, 419, 448, 592, 29709, 11, 438, 304, 19037, 220, 18, 1883, 701, 1039, 12111, 13719, 17961, 429, 525, 2664, 1091, 4024, 6174, 13, 1205, 13166, 37869, 2673, 429, 279, 1590, 1850, 5106, 3545, 4041, 504, 592, 29709, 4518, 315, 279, 4024, 5916, 17961, 13, 1096, 21930, 31116, 279, 12650, 315, 592, 29709, 304, 279, 28439, 7354, 13, 23405, 11, 6633, 220, 17, 4933, 429, 34171, 7075, 23620, 2828, 448, 48924, 14761, 4405, 320, 5569, 3250, 8, 702, 10960, 9904, 7707, 448, 425, 86188, 7075, 23620, 2828, 13, 1096, 14807, 429, 592, 29709, 374, 2989, 369, 3709, 264, 2664, 6291, 4192, 0, 9697, 64, 8, 13047, 533, 19851, 4405, 13, 9533, 3642, 6663, 21, 23, 20, 69, 17, 23, 22, 23, 18, 24, 18, 67, 24, 22, 18, 67, 15, 66, 23, 72938, 20, 6902, 17, 66, 3632, 17, 65, 17, 20, 67, 20, 1999, 24, 21, 65, 20, 18, 18, 65, 15, 24, 823, 21, 69, 17, 68, 15, 16, 19, 19, 19, 542, 8316, 69, 23, 4819, 3623, 0, 9697, 65, 8, 13047, 533, 19851, 4405, 323, 350, 1740, 592, 29709, 13, 19037, 220, 18, 25, 38054, 367, 315, 279, 2456, 315, 13047, 533, 19851, 4405, 323, 350, 1740, 592, 29709, 13, 7036, 429, 19037, 220, 18, 1883, 8, 1172, 49729, 949, 315, 279, 15655, 311, 1473, 279, 2456, 315, 350, 1740, 13, 9533, 3642, 3446, 15, 17, 18, 23, 16, 22, 17, 68, 22, 64, 21, 24, 65, 22, 24, 17, 20, 21, 68, 23, 18, 638, 17, 64, 22, 64, 18, 24, 21, 64, 22, 638, 16, 8940, 22, 16, 66, 21, 17, 68, 16, 21, 32031, 346, 21, 20, 8904, 22, 17, 21, 16, 15, 22, 87734, 4819, 3623, 3838, 374, 279, 1850, 14376, 9873, 30, 576, 5733, 220, 400, 44, 3, 220, 27627, 1246, 1657, 25687, 525, 9251, 1667, 2331, 30710, 1573, 279, 1790, 14376, 32426, 29709, 6430, 13, 1096, 374, 458, 2989, 5733, 311, 387, 32419, 13, 1416, 220, 400, 44, 3, 220, 374, 2238, 2613, 11, 1045, 6291, 12716, 686, 387, 71209, 21244, 3123, 26, 1393, 421, 220, 400, 44, 3, 220, 374, 2238, 3460, 11, 279, 4361, 10324, 7460, 264, 3460, 1372, 315, 19556, 13, 758, 1039, 9342, 11, 582, 990, 264, 25993, 315, 5335, 311, 25240, 419, 5733, 323, 1477, 429, 279, 5335, 304, 279, 1852, 10337, 3545, 4332, 4428, 1850, 14376, 9873, 13, 1096, 25271, 10077, 315, 25165, 82599, 13, 576, 5029, 369, 2155, 29425, 525, 6839, 304, 64714, 422, 4192, 2, 220, 19, 13, 18, 42817, 24211, 7671, 4066, 35, 98450, 6197, 10065, 22690, 7268, 1406, 1654, 6786, 650, 5657, 291, 3359, 389, 20169, 54928, 5480, 4916, 320, 5381, 10599, 568, 8704, 7075, 23620, 2828, 1558, 537, 2924, 279, 9342, 448, 479, 9548, 51, 11, 582, 990, 279, 40034, 5980, 3359, 320, 1143, 826, 1842, 452, 2572, 220, 17, 15, 16, 23, 8, 323, 3796, 1039, 8823, 12111, 389, 1909, 315, 432, 13, 1205, 2908, 1378, 29425, 11, 34995, 3846, 323, 472, 1914, 16522, 11, 323, 990, 279, 1852, 4119, 3897, 553, 320, 1143, 826, 1842, 452, 2572, 220, 17, 15, 16, 23, 49555, 1654, 9429, 279, 5461, 220, 400, 43, 15159, 17, 31716, 220, 18212, 64816, 323, 279, 3359, 2393, 4379, 304, 6633, 220, 18, 13, 576, 3059, 1473, 429, 279, 10981, 1714, 11941, 66132, 279, 5068, 315, 40034, 3359, 13, 576, 8084, 15673, 374, 803, 5089, 1091, 22818, 29728, 14155, 13, 1096, 374, 14576, 1576, 429, 279, 5480, 18732, 315, 479, 9548, 51, 5610, 803, 2205, 1308, 7523, 1091, 29728, 14155, 11, 438, 67583, 304, 19037, 220, 16, 4192, 2556, 220, 18, 25, 42451, 315, 3059, 315, 650, 5657, 291, 3359, 389, 20169, 54928, 5480, 4916, 4192, 20993, 1784, 376, 1784, 1296, 1472, 1296, 1784, 1296, 16848, 428, 17, 755, 39, 1914, 16522, 522, 1296, 1784, 1296, 16848, 428, 17, 755, 55181, 3846, 522, 1296, 1472, 376, 1784, 376, 1784, 1296, 1472, 1296, 1784, 1296, 23465, 7239, 444, 17, 522, 1296, 1784, 1296, 29, 1911, 49, 320, 30143, 609, 4832, 26, 220, 15, 13, 16, 20, 12533, 1296, 1784, 1296, 23465, 7239, 444, 17, 522, 1296, 1784, 1296, 29, 1911, 49, 320, 30143, 609, 4832, 26, 220, 15, 13, 23, 12533, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 38021, 5980, 3359, 522, 1296, 1784, 1296, 29, 15, 13, 16, 21, 24, 522, 1296, 1784, 1296, 29, 20, 17, 52508, 1296, 1784, 1296, 29, 15, 13, 24, 20, 17, 522, 1296, 1784, 1296, 29, 19, 24, 52508, 1296, 1472, 376, 1784, 376, 1784, 1296, 28550, 1740, 6222, 39, 3359, 522, 1296, 1784, 1296, 29, 15, 13, 16, 15, 18, 522, 1296, 1784, 1296, 29, 23, 16, 52508, 1296, 1784, 1296, 29, 15, 13, 22, 17, 17, 522, 1296, 1784, 1296, 29, 22, 24, 52508, 1296, 1472, 376, 1472, 2005, 10370, 2556, 220, 19, 25, 18099, 315, 3359, 389, 34995, 3846, 31214, 4119, 1212, 650, 5657, 291, 6243, 4192, 20993, 1784, 376, 1784, 1296, 1472, 1296, 1784, 1296, 23465, 7239, 444, 17, 522, 1296, 1784, 1296, 29, 1911, 49, 320, 30143, 609, 4832, 26, 220, 16, 13, 20, 12533, 1296, 1472, 376, 1784, 376, 1784, 1296, 25630, 54, 45828, 3359, 522, 1296, 1784, 1296, 29, 17, 13, 16, 19, 522, 1296, 1784, 1296, 29, 17, 15, 13, 17, 20, 52508, 1296, 1472, 376, 1784, 376, 1784, 1296, 29, 7264, 23620, 2828, 3359, 522, 1296, 1784, 1296, 29, 16, 13, 17, 19, 522, 1296, 1784, 1296, 29, 20, 17, 13, 21, 18, 52508, 1296, 1472, 376, 1784, 376, 1784, 1296, 36721, 86188, 3359, 522, 1296, 1784, 1296, 29, 16, 13, 16, 23, 522, 1296, 1784, 1296, 29, 22, 16, 13, 19, 17, 52508, 1296, 1472, 376, 1472, 2005, 10370, 2, 220, 19, 13, 18, 13, 16, 42817, 24211, 7671, 4066, 35, 98450, 6197, 40243, 36, 3580, 22690, 7268, 1406, 1249, 7269, 279, 5068, 315, 29728, 14155, 11, 264, 1555, 315, 3412, 11, 1741, 438, 62990, 10, 14928, 320, 6123, 258, 1515, 1842, 452, 2572, 220, 17, 15, 16, 22, 701, 444, 915, 320, 29535, 1842, 452, 2572, 220, 17, 15, 16, 23, 701, 16171, 32283, 674, 285, 320, 53988, 1842, 452, 2572, 220, 17, 15, 16, 23, 8, 323, 19614, 12, 1593, 46, 320, 74965, 1842, 452, 2572, 220, 17, 15, 16, 24, 701, 702, 1012, 21080, 389, 22265, 700, 28411, 42721, 10295, 304, 279, 1273, 6430, 2041, 30587, 279, 4862, 315, 279, 4024, 1614, 13, 30809, 15817, 16460, 315, 1039, 3359, 389, 5257, 23850, 4119, 448, 264, 8045, 315, 821, 7289, 11, 582, 6777, 700, 22000, 6358, 315, 1039, 650, 5657, 291, 3359, 389, 825, 1584, 8668, 10603, 37821, 17984, 1614, 444, 915, 320, 29535, 1842, 452, 2572, 220, 17, 15, 16, 23, 8, 389, 34995, 3846, 821, 738, 13, 2014, 5426, 264, 17984, 1614, 389, 34995, 3846, 11, 582, 1156, 5426, 264, 4285, 23850, 3922, 23415, 315, 1378, 55712, 278, 13617, 8110, 553, 264, 8177, 27950, 6193, 448, 220, 16, 15, 17, 19, 8153, 13, 5005, 582, 3796, 356, 91336, 3359, 311, 419, 1614, 311, 6923, 28411, 42721, 10295, 504, 279, 4024, 1273, 10469, 13, 17375, 582, 5426, 444, 915, 67987, 448, 279, 4024, 1273, 10469, 323, 28411, 42721, 10295, 582, 614, 7907, 448, 279, 5297, 5426, 12697, 6718, 13, 576, 1584, 8668, 10603, 37821, 17984, 1614, 444, 915, 82591, 220, 15, 13, 24, 24, 1273, 13403, 4192, 34, 91336, 1550, 12136, 3359, 320, 8852, 3732, 72, 609, 51375, 11, 220, 17, 15, 16, 22, 64, 8, 702, 1012, 6839, 311, 614, 2244, 5068, 304, 22818, 5257, 17984, 4119, 13, 2055, 582, 9429, 279, 5461, 220, 400, 43, 15159, 17, 31716, 220, 18212, 64816, 323, 3359, 2393, 4379, 315, 2326, 22818, 5413, 356, 91336, 1550, 12136, 3359, 11, 7075, 23620, 2828, 3359, 323, 425, 86188, 7075, 23620, 2828, 3359, 304, 6633, 220, 19, 13, 2411, 1817, 3239, 11, 582, 6979, 279, 3359, 311, 387, 6849, 421, 432, 83642, 2176, 279, 31214, 1614, 323, 279, 4024, 1614, 13, 576, 3059, 1473, 429, 279, 10981, 1714, 646, 11941, 7760, 279, 5068, 315, 279, 7075, 23620, 2828, 3359, 323, 432, 82591, 1753, 2664, 5068, 1091, 356, 91336, 1550, 12136, 3359, 4192, 2, 220, 20, 3418, 98669, 1406, 641, 419, 5567, 11, 582, 29614, 264, 8823, 12111, 311, 7760, 279, 5068, 315, 6350, 5480, 3118, 8776, 13, 758, 3953, 11, 4518, 315, 29417, 287, 264, 3175, 6291, 1815, 979, 15039, 369, 458, 28411, 42721, 3110, 11, 582, 10306, 264, 7314, 315, 6291, 12716, 311, 13186, 2989, 13604, 13, 1205, 1473, 37869, 2673, 429, 279, 10981, 12111, 20699, 34950, 279, 6291, 4271, 315, 1657, 6350, 5480, 3118, 8776, 11, 323, 646, 6851, 28411, 42721, 10295, 448, 12944, 4271, 389, 537, 1172, 29728, 14155, 11, 714, 1083, 1008, 5480, 3118, 4119, 11, 1741, 438, 479, 9548, 51, 323, 17984, 5980, 4119, 624, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are an expert academic reviewer tasked with evaluating research papers.<|im_end|>
<|im_start|>user
I am giving you a paper. I want to predict its acceptance outcome at ICLR.
 - Your answer will either be: \boxed{Accept} or \boxed{Reject}
 - Note: ICLR generally has a ~30% acceptance rate


# BOSH: AN EFFICIENT META ALGORITHM FOR DECISION-BASED ATTACKS


Anonymous authors


Paper under double-blind review


# ABSTRACT


Adversarial example generation becomes a viable method for evaluating the robustness of a machine learning model. In this paper, we consider hard-label black-box attacks (a.k.a. decision-based attacks), which is a challenging setting that generates adversarial examples based on only a series of black-box hard-label queries. This type of attacks can be used to attack discrete and complex models, such as Gradient Boosting Decision Tree (GBDT) and detection-based defense models. Existing decision-based attacks based on iterative local updates often get stuck in a local minimum and fail to generate the optimal adversarial example with the smallest perturbation. To remedy this issue, we propose an efficient meta algorithm called BOSH-attack, which tremendously improves existing algorithms through Bayesian Optimization (BO) and Successive Halving (SH). In particular, instead of traversing a single solution path when searching an adversarial example, we maintain a pool of solution paths to explore important regions. We show empirically that the proposed algorithm converges to a better solution than existing approaches, while the query count is smaller than applying multiple random initializations by a factor of 10.


# 1 INTRODUCTION


It has been shown that machine learning models, including deep neural networks, are vulnerable to adversarial examples (Goodfellow et al., 2014; Szegedy et al., 2013; Chen et al., 2017a). Therefore, evaluating the robustness of a given model becomes crucial for security sensitive applications. In order to evaluate the robustness of deep neural networks, researchers have developed "attack algorithms" to generate adversarial examples that can mislead a given neural network while being as close as possible to the original example (Goodfellow et al., 2014; Moosavi-Dezfooli et al., 2016; Carlini & Wagner, 2017b; Chen et al., 2017b). Most of these attack methods are based on maximizing a loss function with a gradient-based optimizer, where the gradient is either computed by back-propagation (in the white-box setting) or finite-difference estimation (in the soft-label black-box setting). Although these methods work well on standard neural networks, when it comes to complex or even discontinuous models, such as decision trees and detection-based defense models, they cannot be directly applied because the gradient is not available.


Hard-label black-box attacks, also known as decision-based attacks, consider the most difficult but realistic setting where the attacker has no information about the model structure and parameters, and the only valid operation is to query the model to get the corresponding decision-based (hard-label) output (Brendel et al., 2017). This type of attacks can be used as a "universal" way to evaluate robustness of any given models, no matter continuous or discrete. For instance, Cheng et al. (2018); Chen et al. (2019a) have applied decision-based attacks for evaluating robustness of Gradient Boosting Decision Trees (GBDT) and random forest. Current decision-based attacks, including Brendel et al. (2017); Cheng et al. (2018); Chen et al. (2019b); Cheng et al. (2019), are based on iterative local updates  starting from an initial point on the decision surface, they iteratively move the points along the surface until reaching a local minimum (in terms of distance to the original example). The update is often based on gradient estimation or some other heuristics. However, the local update nature makes these methods sensitive to the starting point. As we demonstrate in Figure 1(a), the perturbation of converged adversarial examples for a neural network are quite different for different initialization configurations, and this phenomenon becomes more severe when it comes to discrete models such as GBDTs (see Figure 1(b)). This makes decision-based attacks converge to a sub-


optimal perturbation. As a result, the solution cannot really reflect the robustness of the targeted model.


To overcome these difficulties and make decision-based attacks better reflect the robustness of models, we propose a meta algorithm called BOSH-attack that consistently boosts the solution quality of existing iterative local update based attacks. Our main idea is to combine Bayesian optimization, which finds solution closer to global optimum but suffers from high computation cost, with iterative local updates, which converges fast but often get stuck in local minimum. Specifically, given a decision based attack  $\mathcal{A}$ , our algorithm maintains a pool of solutions and at each iteration we run  $\mathcal{A}$  for  $m$  steps on each solution. The proposed Bayesian Optimization resampling (BO) and Successive Halving (SH) are then used to explore important solution space based on current information and cut out unnecessary solution paths.


Our contributions are summarized below:

- 1. We conduct thorough experiments to show that current decision-based attacks often converge to a local optimum, thus further improvements are required.

- 2. Based on the idea of Bayesian optimization and successive halving, we design a meta algorithm to boost the performance of current decision-based attack algorithms and encourage them to find a much smaller adversarial perturbation efficiently.

- 3. Comprehensive experiments demonstrate that BOSH-attack can consistently boost existing decision-based attacks to find better examples with much smaller perturbation. In addition to the standard neural network models, we also test our algorithms on attacking discrete GBDT models and detector-based defense models. Moreover, our algorithm can reduce the computation cost by  $10\mathrm{x}$  compared to the naive approach.


# 2 BACKGROUND AND RELATED WORK


Given a classification model  $F: \mathbb{R}^d \to \{1, \dots, C\}$  and an example  $x_0$ , adversarial attacks aim to find the adversarial example that is closest to  $x_0$ . For example, an untargeted attack aims to find the minimum perturbation to change the predicted class, which corresponds to the following optimization problem:


$$

$$
\min  _ {\delta} \| \delta \| \quad \text {s . t .} \quad F (x _ {0} + \delta) \neq F (x _ {0}). \tag {1}
$$

$$


Exactly minimizing (1) is usually intractable; therefore, we can only expect to get a feasible solution of (1) while hoping  $\|\delta\|$  to be as small as possible.


White-box Attack. For neural networks, we can replace the constraint in (1) by a loss function defined on the logit layer output, leading to a continuous optimization problem which can be solved by gradient-based optimizers. This approach has been used in popular methods such as FGSM (Goodfellow et al., 2014), C&W attack (Carlini & Wagner, 2017b) and PGD attack (Madry et al., 2017). All these white-box attacks developed for neural networks assume the existence of the gradient. However, for models with discrete components such as GBDT, the objective cannot be easily defined and gradient-based white-box attacks are not applicable. There are few white-box attacks developed for specific discrete models, such as Mixed Integer Linear Programming (MILP) approach for attacking tree ensemble (Kantchelian et al., 2016). However, those algorithms are time consuming and require significant efforts for developing each model.


Soft-label Black-box Attack Black box setting considers the cases when an attacker has no direct access to the model's parameter and architecture, and the only valid operation is to query input examples and get the corresponding model output. In the soft-label black box setting, it is assumed that the model outputs the probability of each label for an input query. Chen et al. (2017b) showed that the attack can still be formulated as an optimization problem where the objective function value can be computed while the gradient is unavailable. Based on this, various zeroth order optimization algorithms have been proposed, including NES-attack (Ilyas et al., 2018a), EAD attack (Chen et al., 2018), bandit-attack (Ilyas et al., 2018b), Autozoom (Tu et al., 2019), Genetic algorithm (Alzantot et al., 2018).


Hard-label Black-box attack (Decision-based attack) In this paper, we focus on the hard-label black box attack (also known as decision-based attack). In contrast to the soft-label setting, the


attacker can only query the model and get the top-1 predicted label without any probability information. To minimize the perturbation in the decision-based setting, Brendel et al. (2017) first proposed a Boundary Attack based on random walk on the decision surface. Later on, Cheng et al. (2018) showed that hard-label attack can be reformulated as another continuous optimization problem, and zeroth order optimization algorithms such as NES can be used to solve this problem. Cheng et al. (2019) further reduces the queries by only calculating the sign of ZOO updating function, and Chen et al. (2019b) proposed another algorithm to improve over boundary attack. Such methods can converge quickly but suffer from local optimum problems, and thus require more careful and thorough search in the solution space.


Probability based black-box optimization There are two commonly used methods to solve a black-box or non-differentiable optimization problem: gradient-based and probabilistic-based algorithms. Gradient-based methods are based on iterative local updates until convergence, while probabilistic-based algorithms such as Bayesian optimization (BO) (Pelikan et al., 1999; Snoek et al., 2012) approximate the objective function by a probabilistic model. Generally speaking, gradient-based methods are commonly used in black-box attack because it converges fast. However, these methods often stuck in some local optimal directions, especially when the searching space is high dimensional and non-convex. Probabilistic-based algorithms are frequently used in low-dimensional problems such as hyperparameter tuning and can have a better chance to find more global optimal values (Snoek et al., 2012; Bergstra et al., 2011). However, the computation cost grows exponentially while the dimension increases and quickly become unacceptable. Therefore, they cannot be directly applied to generate adversarial examples. In this paper, we attempt to combine Bayesian Optimization and iterative local updates to improve the solution quality of current attack algorithms while being able to scale to high dimensional problems.


Combinatorial heuristic and genetic algorithms There exist various heuristic algorithms commonly applied to combinatorial optimization problems. In these algorithms, they try to leverage the effects between greedy and random. Commonly, they will search for different directions and drop the bad ones, and then put more attention on the relatively good candidates. Greedy randomized adaptive search (Feo & Resende, 1995) finds good solutions in an iterative way. It first generates a set of solutions and use a greedy function to rank these solutions. Later, good candidates are placed in a restricted candidate list, and randomly chosen when forming the solution. Tabu search (Glover & Laguna, 1999) selects a candidate and checks its immediate neighbors, trying to find an improved solution. In order to avoid stucking in local optimal areas, it maintains a list called tabu list to store past good solutions (often local optimal solutions). In further searches, it will prevent from looking for this area. Other approaches like Genetic Algorithms (GA) and Simulated Annealing (SA) also try to adopt random in the searching process. In this paper, we simply use Successive Halving (SH) (Jamieson & Talwalkar, 2016) to remove unimportant candidate configurations iteratively. The details can be found in Section 3.1.


# 3 THE PROPOSED ALGORITHM


Observation: Decision based attacks are easily stuck in local optimum. Most existing adversarial attacks adopt iterative local updates to find adversarial examples  starting from an initial point, they iteratively update the solution until convergence. For example, in white-box attacks such as C&W and PGD methods, they aim at optimizing a non-convex loss function by iterative gradient updates. In Figure 1(a), we plot the 2-dimensional projection of the decision surface of a neural network. We can observe that there are two local minimums<sup>1</sup>. Results show that there are two local minimums and the attack algorithm converges to one of them according the the initialization region. Similarly, in decision-based attacks, existing methods start from some point on the decision surface and then iteratively update the point locally on the surface either by gradient update (Cheng et al., 2018; Chen et al., 2019b) or random walk (Brendel et al., 2017). In Figure 1(b) we plot the decision surface of a GBDT. We observe a similar issue that there are many local minima in the GBDT decision boundary.


![(a) Boundary on NN.](images/431cc3b1a9db2ac77751e1fb823e23dd9e03cfe0565b373377755bf9d0ef7f9d.jpg)


![(b) Boundary on GBDT. Figure 1: Decision boundary (NN and GBDT models on MNIST dataset) projected on a two-dimensional hyperplane. To choose which 2D hyperplane to project to, we run a decision-based attack from two random initialization points, and use their converged perturbation directions as the vector to form the hyperplane. We then query the decision boundary on this hyperplane to plot these figures.](images/8a48a767a258ba8054314978b9b291cf6d2003a36cee77e8703cc5f590e67112.jpg)


We further quantify how serious the problem is. On an MNIST network, Figure 2(a) shows the distribution of converged adversarial perturbations of C&W attack (white-box attack) and Sign-OPT attack (decision-based attack) under approximately 400 random initial points. We observe that the converged solutions of C&W attack are quite concentrated between [1.41, 1.47]. However, when considering decision-based attack such as Sign-OPT, the converged solutions are widely spread from 1.36 to 1.55. In general, our experiments suggested that decision based attacks are much more sensitive to initialization. This is because they only update solutions on the decision boundary while C&W and PGD attack can update solution inside/outside the boundary.


Furthermore, such phenomenon is obvious when the victim model is GBDT. For example, in Figure 2(b) we can see the converged solution spread from 0.5 to 1.5 when applying Sign-OPT attack. Therefore, the solution of any single run of Sign-OPT on GBDT cannot really reflect the minimum adversarial perturbation of the given model, and thus it is crucial to design an algorithm that converges to a better solution. Since the phenomenon is more severe for decision based attacks, we will mainly focus on improving the quality of decision based attacks in this paper, while in general our method can also be used to boost the performance of white-box attacks marginally, as illustrated in Appendix A.


![(a) Sign-OPT / C&W on DNN.](images/38fe2a917efd82ac433c3264b686e71b77397330d9268d03a746205727986055.jpg)


![(b) Sign-OPT attack on GBDT. Figure 2: Distribution of converged adversarial perturbation norm on an MNIST image. The figures show that the final  $L_{2}$  distortion can be very different because of various starting directions. Figure 2(a) and 2(b) show the histogram for final perturbations, and Figure 2(c) shows the converging curve of Sign-OPT attack on a neural network model on MNIST dataset.](images/937469d4280c5bfbf663cab8048d3c03da80d9d5c02bc30788f673e73992315e.jpg)


![(c) Distortion vs Queries curves.](images/7367a0cb7be233383b537595c4c107424f13c484584fe7e860d26a7b25b55489.jpg)


# 3.1 A GENERAL MECHANISM FOR IMPROVED DECISION BASED ATTACK


Given a local update based attack  $\mathcal{A}$ , our goal is to find a solution with improved quality. To this end, we propose a meta algorithm to address this issue by integrating probability-based (Bayesian) black-box optimization with iterative local updates. As shown in Algorithm 1, our algorithm maintains a candidate pool  $\mathbb{P}_a$  that stores all the active configurations, where each configuration  $\pmb{u} \in \mathbb{P}_a$  is an intermediate iterate of algorithm  $\mathcal{A}$ . Also, we assume that there is an attack objective  $\mathcal{C}$  such that  $\mathcal{C}(\pmb{u})$  measures the quality of the solution. For decision based attacks, the goal is to find the optimal direction to minimize the distance to the boundary along that direction (Cheng et al., 2018).


Therefore,  $\pmb{u}$  is the direction of adversarial perturbation and


$$

$$
\mathcal {C} (\boldsymbol {u}) = \min  _ {\lambda > 0} \lambda \text {s . t .} f \left(x _ {0} + \lambda \frac {\boldsymbol {u}}{\| \boldsymbol {u} \|}\right) \neq y _ {0},
$$

$$


where  $y_0$  is the correct label. This can be computed by a fine-grained plus binary search procedure (see Cheng et al. (2018)), and in fact, in most of the algorithms  $\mathcal{C}(\boldsymbol{u})$  is directly maintained during the optimization procedure (Brendel et al., 2017; Cheng et al., 2019). At each iteration, we run  $m$  iterations of  $\mathcal{A}$  on each active configuration  $\boldsymbol{u} \in \mathbb{P}_a$  to get the improved configurations. Then we conduct the following two operations to reduce the candidate pool size and to resample new configurations to explore important subspace based on Bayesian optimization. We discuss each step in details as below.


Successive Halving (SH) to cut unimportant candidate configuration. After updating each candidate by  $m$  iterations, we compute the objective function value of each candidate and discard the worst half of them. Iteratively reducing the candidate set into half accelerates the algorithm, while still maintaining an accurate solution pool. This idea has been used in hyperparameter search (Jamieson & Talwalkar, 2016) but has not been used in adversarial attack.


Bayesian Optimization (BO) for Guided Resampling. To introduce variance in the intermediate steps and explore other important regions, we propose a guided resampling strategy to refine the candidate pool. The general idea is to resample from the solution space in the middle step based on the knowledge acquired before and focus on promising subareas. Specifically, we use a Bayesian optimization method called Tree Parzen Estimator (TPE) (Bergstra et al., 2011) to resample new configurations.


In order to do resampling, we maintain another pool  $\mathbb{P}_s$  that stores all the previous iterations performed including the cutted ones, since all the information will be useful for resampling. As shown in Algorithm 2, we first divide the observed data in  $\mathbb{P}_s$  into worse and better parts based on the associated objective function value. We then train two separate Kernel Density Estimators (KDE) denoted as  $l(\cdot)$  and  $g(\cdot)$  on these two subsets.


$$

$$
\left\{ \begin{array}{l} l (\boldsymbol {u}) = p \left(\mathcal {C} (\boldsymbol {u}) \leq \alpha \mid \boldsymbol {u}, \mathbb {P} _ {s}\right), \\ g (\boldsymbol {u}) = p \left(\mathcal {C} (\boldsymbol {u}) > \alpha \mid \boldsymbol {u}, \mathbb {P} _ {s}\right). \end{array} \right. \tag {2}
$$

$$


The parameter  $\alpha$  is set to  $20\%$ , which ensures the better part  $l(\boldsymbol{u})$  has  $20\%$  of configurations in  $\mathbb{P}_s$  and the worse part  $g(\boldsymbol{u})$  has the remaining  $80\%$ , relatively. Later, we sample new data with the minimum value of  $l(\cdot) / g(\cdot)$ , which can be proved to have maximum relative improvement in Equation 4 (see more information in Appendix B). Since we can not directly find such points, we sample for a few times (the number is set to 100 during the experiment) from  $l(\cdot)$  and keep the one with the minimal  $l(\cdot) / g(\cdot)$ .


# ALGORITHM 1 THE PROPOSED BOSH ATTACK FRAMEWORK.


Input: Model  $f$ , original example  $x_0$ , attack objective  $\mathcal{C}$ , gradient-based attack algorithm  $\mathcal{A}$ , cutting interval  $M$ , cutting rate  $s$ , cutting interval increase rate  $m$ .

- 1: Randomly sample  $k$  initial configurations to form  $\mathbb{P}_a$  (Gaussian or uniform random).

- 2:  $\mathbb{P}_s\gets \mathbb{P}_a$

- 3: for  $t = 1,2,\ldots$  do

- 4: for each  $\pmb{u}_t \in \mathbb{P}_a$  do // perform attack on all configurations

- 5: for  $j = 1,\dots ,M$  do // conduct  $M$  iterations before cutting

- 6:  $\pmb{u}_t^{\prime}\gets \mathcal{A}(\pmb{u}_t)$

- 7:  $\mathbb{P}_s\gets \mathbb{P}_s\cup \{(\pmb {u}_t',\mathcal{C}(\pmb {u}_t'))\} .$  // Record all interval steps

- 8: Update  $\pmb{u}_t$  in  $\mathbb{P}_a$  with  $\pmb{u}_t'$ . // Update the configuration in  $\mathbb{P}_a$

- 9: Delete the worst  $s\%$  of configurations from  $\mathbb{P}_a$

- 10:  $\mathbb{P}_a\gets \mathbb{P}_a\cup$  TPE-resampling  $(\mathbb{P}_s,|\mathbb{P}_a|*s\%)$

- 11:  $M\gets M\cdot (1 + m)\%$  // Increase the searching interval


```
Input: Observed datas  $\mathbb{P}_s$  , resample times  $T$  . 1: Initialize  $\mathbb{P}_l$  as an empty list; 2: Divide  $\pmb {u}\in \mathbb{P}_s$  into two subset  $\mathbb{L}$  (better) and  $\mathbb{H}$  (worse) based on objective function  $\mathcal{C}$  . 3: Build two separate KDEs on  $\mathbb{L}$  and  $\mathbb{H}$  denoted as  $l(\cdot)$  and  $h(\cdot)$  respectively; 4: Use Grid Search to find the best KDE bandwidth  $b_{l},b_{h}$  for  $l(\cdot)$  and  $h(\cdot)$  . 5: for each  $t\in [0,T]$  do 6: initialization:  $k = 0$  , min_score  $= \infty$  . 7: while  $k <   \max$  sample times do 8: Sample  $\pmb{u}_{tk}$  from  $l(\cdot)$  . 9: if min_score  $>g(u_{tk}) / l(u_{tk})$  then; 10:  $\pmb {u}_t\gets \pmb {u}_{tk}$  . 11: min_score  $= g(\pmb {u}_{tk}) / l(\pmb {u}_{tk})$  . 12:  $k\gets k + 1$  13:  $\mathbb{P}_l\gets \mathbb{P}_l\cup \{(\pmb {u}_t,\mathcal{C}(\pmb {u}_t))\} ;$  14: return  $\mathbb{P}_l$
```


The reason we use TPE for resampling is that the computational cost grows linearly with the number of data points in  $\mathbb{P}_s$ . In comparison, traditional Bayesian optimization method like Gaussian Process (GP) will require cubic-time to generate new points. Therefore, TPE is more suitable for high dimensional problems.


In the experiments we find that the final best configuration mostly comes from resampling, instead of the set of starting configurations. This proves the effectiveness of resampling during search, the quantitative results will be shown in Section 4.2.


# 4 EXPERIMENTS


We conduct experiments on various models and datasets to verify the efficiency and effectiveness of the proposed approach. We try to enhance the performance of decision-based attack on image classification tasks like MNIST, CIFAR-10 and ImageNet, and also conduct experiments on tree model like GBDT and detection model like LID. Furthermore, we demonstrate that our meta-algorithm is also able to improve existing white-box attacks.


# 4.1 DECISION-BASED ATTACK ON NEURAL NETWORKS


We conduct experiments on three standard datasets: MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky et al., 2010) and ImageNet-1000 (Deng et al., 2009). The neural network model architecture is the same with the ones reported in Cheng et al. (2018): for both MNIST and CIFAR we use the network with four convolution layers, two max-pooling layers and two fully-connected layers, which achieve  $99.5\%$  accuracy on MNIST and  $82.5\%$  accuracy on CIFAR-10 as reported in (Carlini & Wagner, 2017b; Cheng et al., 2018). For ImageNet, we use the pretrained Resnet-50 (He et al., 2016) network provided by torchvision (Marcel & Rodriguez, 2010), which achieves a Top-1 accuracy of  $76.15\%$ . We randomly select 100 examples from test sets for evaluation. The parameters of the proposed algorithms can be found in Table 6 in Appendix D.


Improved solution quality of existing methods We compare the solution quality of the proposed algorithm with three existing decision-based attack methods: Boundary attack (Brendel et al., 2017), OPT-attack (Cheng et al., 2018) and Sign-OPT attack (Cheng et al., 2019) on MNIST, CIFAR-10 and ImageNet data sets. For our algorithm, we use Sign-OPT attack as the base algorithm and set  $k = 30$  for the initial candidate pool. The average  $L_{2}$  perturbation of our method and baselines are presented in Table 8. Note that all the decision based attacks maintain intermediate iterates on the decision boundary, so they always output a successful attack. The main comparison is the average  $L_{2}$  perturbation to alter the predictions. We also follow Cheng et al. (2018) to report Attack Success Rate (ASR) by calculating ratio of adversarial examples with perturbation  $< \epsilon$  ( $\epsilon$  is chosen based on


different tasks). The results show that can help decision-based attacks achieve lower  $L_{2}$  perturbation and higher attack success rate. The detailed analysis is shown in the next section.


The proposed algorithm can also be used to boost the performance of other decision based attacks. Table 7 in the Appendix demonstrates that the proposed algorithm consistently improves the  $L_{2}$  perturbation and success rate of Boundary attack and OPT-attack.


Table 1: Results of hard-label black-box attack on MNIST, CIFAR-10 and ImageNet-1000. We compare the performance of several attack algorithms under untargeted setting.


<table><tr><td></td><td colspan="2">MNIST</td><td colspan="2">CIFAR-10</td><td colspan="2">ImageNet-1000</td></tr><tr><td></td><td>Avg L2</td><td>ASR ( &lt; 1.0)</td><td>Avg L2</td><td>ASR ( &lt; 0.13)</td><td>Avg. L2</td><td>ASR ( &lt; 1.4)</td></tr><tr><td>C&amp;W (White-box)</td><td>0.96</td><td>60%</td><td>0.12</td><td>61%</td><td>1.53</td><td>49%</td></tr><tr><td>Boundary attack</td><td>1.13</td><td>41%</td><td>0.15</td><td>48%</td><td>2.02</td><td>19%</td></tr><tr><td>OPT-based attack</td><td>1.09</td><td>46%</td><td>0.14</td><td>50%</td><td>1.67</td><td>38%</td></tr><tr><td>Sign-OPT attack</td><td>1.07</td><td>49%</td><td>0.14</td><td>51%</td><td>1.43</td><td>59%</td></tr><tr><td>BOSH Sign-OPT attack</td><td>0.91</td><td>67%</td><td>0.10</td><td>65%</td><td>1.18</td><td>81%</td></tr></table>


# 4.2 ANALYSIS


We then conduct a study to test each component of our algorithm and compare with the baselines. The experiment is done on MNIST data using Sign-OPT attack as the base attack method. The results are summarized in Table 2.


Table 2: Comparisons for the effectiveness of successive halving and TPE resampling. The Relative Gain is based on Multi-Directional attack and Queries means total queries of all directions.


<table><tr><td></td><td>Starting Directions</td><td>Avg L2</td><td>Ratio</td><td>ASR ( &lt; 1.0)</td><td>Queries</td><td>Ratio</td></tr><tr><td rowspan="4">Multi-initial Sign-OPT</td><td>1</td><td>1.07</td><td>0%</td><td>49%</td><td>25,456</td><td>1x</td></tr><tr><td>30</td><td>0.98</td><td>8.4%</td><td>57%</td><td>771,283</td><td>30x</td></tr><tr><td>50</td><td>0.94</td><td>12.1%</td><td>63%</td><td>1,268,392</td><td>50x</td></tr><tr><td>100</td><td>0.91</td><td>15.0%</td><td>65%</td><td>2,567,382</td><td>100x</td></tr><tr><td>Successive Halving Sign-OPT</td><td>30</td><td>0.99</td><td>7.5%</td><td>55%</td><td>161,183</td><td>6.3x</td></tr><tr><td rowspan="3">BOSH Sign-OPT</td><td>30</td><td>0.91</td><td>15.0%</td><td>67%</td><td>252,014</td><td>9.9x</td></tr><tr><td>50</td><td>0.88</td><td>17.8%</td><td>71%</td><td>409,841</td><td>16.1x</td></tr><tr><td>100</td><td>0.87</td><td>18.7%</td><td>71%</td><td>829,865</td><td>32.6x</td></tr></table>


Comparison with naive multiple initialization approach. A naive way to improve the solution quality of existing attack is to run the attack on multiple random initialization points. This strategy has been used in white-box attack $^3$  and is also applicable to the decision based attacks. We compare Sign-OPT with 30, 50, 100 initial points and the proposed BOSH boosted Sign-OPT approach in Table 2. The results demonstrate that successive halving requires much less queries than naively running multiple initial configurations. Due to resampling, the proposed approach converges to a better solution under the same initial pool size. For example, to achieve average  $0.91L_{2}$  perturbation, BOSH boosted Sign-OPT requires 10 times less queries than multi-initial Sign-OPT.


Size of the initial pool. The size of initial pool (denoted by  $k$  in our algorithm) is an important parameter. Table 2 shows that increasing  $k$  only has a marginal effect after  $k \approx 30$ . When introducing cutting and resampling mechanism into the Sign-OPT attack, the final best perturbation is less sensitive to the number of starting directions, which means that resampling tends to make the search less dependent on the starting directions. Detailed discussion is in Appendix C.


Effect of successive halving and TPE resampling. We study the effect of these two components separately. As shown in Figure 3(a), the approach of successive halving keeps throwing away the worse  $s$  percent of configurations until converge during a specific interval until there is only one sample left. When combining this with resampling, as in Figure 3(b), our algorithm finds directions that are better than original ones. We observed empirically that the final best direction often comes from resampling instead of the original starting directions. This observation demonstrates the importance of resampling in the intermediate steps. Furthermore, Table 2 shows that combining Sign-OPT with successive halving (second column) has worse solutions compared with BOSH Sign-OPT. This indicates that resampling is important for getting a better solution.


![(a) Successive Halving.](images/f685f2878393d973d0c8cea5fd2cfa2b25d5db96b533b09ef6f2e01444fffcf8.jpg)


![(b) Successive Halving and TPE resampling. Figure 3: Illustration of the effect of Successive Halving and TPE resampling. Note that Figure 3(b) only exhibits part of the curve to show the effect of TPE.](images/d0238172e7a69b79256e83cc2a7a396a7cc1bd71c62e16beece65bc726107caa.jpg)


What is the best cutting interval? The parameter  $M$  decides how many iterations are applied using base attacker before the next cutting/resampling stage. This is an important parameter to be tuned. If  $M$  is too small, some solution paths will be wrongly throwing away; while if  $M$  is too large, the whole procedure requires a large number of queries. In our experiment, we use a subset of images to tune this parameter and find that the images in the same dataset often share similar best cutting interval. This reduces lots of unnecessary computations. The parameters for different datasets are shown in Appendix D.


# 4.3 DECISION-BASED ATTACK ON OTHER MODELS


We conduct untargeted attack on gradient boosting decision tree (GBDT). Since Sign-OPT does not include the experiment with GBDT, we use the OPT-based attack (Cheng et al., 2018) and apply our meta algorithm on top of it. We consider two datasets, MNIST and HIGGS, and use the same models provided by (Cheng et al., 2018).


We compare the average  $L_{2}$  perturbation and the attack success rate in Table 3. The results show that the proposed method significantly boosts the performance of OPT attack. The overall improvement is more significant than attacking neural networks. This is mainly because that the decision boundary of GBDT contains more local minima than neural networks, as plotted in Figure 1.


Table 3: Comparison of results of untargeted attack on gradient boosting decision tree.


<table><tr><td></td><td colspan="2">HIGGS</td><td colspan="2">MNIST</td></tr><tr><td></td><td>Avg L2</td><td>ASR ( &lt; 0.15)</td><td>Avg L2</td><td>ASR ( &lt; 0.8)</td></tr><tr><td>OPT-based attack</td><td>0.169</td><td>52%</td><td>0.952</td><td>49%</td></tr><tr><td>TPE-SH attack</td><td>0.103</td><td>81%</td><td>0.722</td><td>79%</td></tr></table>


Table 4: Results of attack on MNIST detector models under untargeted setting.


<table><tr><td></td><td>Avg L2</td><td>ASR ( &lt; 1.5)</td></tr><tr><td>CW HC attack</td><td>2.14</td><td>20.25%</td></tr><tr><td>Sign-OPT attack</td><td>1.24</td><td>52.63%</td></tr><tr><td>BOSH attack</td><td>1.18</td><td>71.42%</td></tr></table>


# 4.3.1 DECISION-BASED ATTACK ON DETECTION MODELS


To improve the performance of neural networks, a line of research, such as KD+BU (Feinman et al., 2017), LID (Ma et al., 2018), Mahalanobis (Lee et al., 2018) and ML-LOO (Yang et al., 2019), has been focusing on screening out adversarial examples in the test stage without touching the training of the original model. Besides comprehensive evaluation of our attack on various classification models with a variety of data sets, we carry out experimental analysis of our untargeted attack on one state-of-the-art detection model LID (Ma et al., 2018) on MNIST data set. To train a detection model on MNIST, we first train a simple classification network composed of two convolutional layers followed by a hidden dense layer with 1024 units. Then we apply C&W attack to this model to generate adversarial examples from the original test samples. Finally we train LID detectors with the original test samples and adversarial examples we have generated with the standard train/test split. The state-of-the-art detection model LID achieves 0.99 test accuracy.


C&W high confidence attack (Carlini & Wagner, 2017a) has been shown to have great performance in attacking various detection models. So we compare the average  $L_{2}$  perturbation and attack success rate of three attacking methods C&W high confidence attack, Sign-OPT attack and BOSH Sign-OPT attack in Table 4. At each query, we define the attack to be successful if it fools both the detector model and the original model. The results show that the proposed method can significantly boost the performance of the Sign-OPT attack and it achieves much better performance than C&W high confidence attack.


# 5 CONCLUSION


In this paper, we propose a meta algorithm to boost the performance of existing decision based attacks. In particular, instead of traversing a single solution path when searching for an adversarial example, we maintain a pool of solution paths to explore important regions. We show empirically that the proposed algorithm consistently improves the solution quality of many existing decision based attacks, and can obtain adversarial examples with improved quality on not only neural networks, but also other decision based models, such as GBDT and detection-based models.
<|im_end|>
<|im_start|>assistant

label: 0
[INFO|2026-02-06 09:31:53] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[WARNING|2026-02-06 09:31:54] llamafactory.model.model_utils.checkpointing:148 >> You are using fsdp2, `use_reentrant_gc` has been set to False.
[INFO|2026-02-06 09:31:54] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2026-02-06 09:31:54] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2026-02-06 09:31:54] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2026-02-06 09:31:54] llamafactory.model.adapter:143 >> Fine-tuning method: Full
[INFO|2026-02-06 09:32:21] llamafactory.model.model_utils.binary_classifier:143 >> Provided path (Qwen/Qwen2.5-7B-Instruct) does not contain classifier head weights: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'..
[INFO|2026-02-06 09:32:21] llamafactory.model.model_utils.binary_classifier:143 >> Ignore the above message if you are training from scratch.
[INFO|2026-02-06 09:32:21] llamafactory.model.loader:143 >> trainable params: 7,628,468,737 || all params: 7,628,468,737 || trainable%: 100.0000
{'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 858.3315, 'eval_samples_per_second': 1.831, 'eval_steps_per_second': 0.458, 'epoch': 0}
{'loss': 0.8484, 'grad_norm': 17.9597110748291, 'learning_rate': 1.7499847435575162e-06, 'accuracy': 0.4937500059604645, 'cls_loss': 0.84765625, 'epoch': 0.01}
{'loss': 0.7006, 'grad_norm': 34.808189392089844, 'learning_rate': 1.7498131146894396e-06, 'accuracy': 0.53125, 'cls_loss': 0.69921875, 'epoch': 0.03}
{'loss': 0.6976, 'grad_norm': 25.846101760864258, 'learning_rate': 1.7494508239306744e-06, 'accuracy': 0.550000011920929, 'cls_loss': 0.69921875, 'epoch': 0.04}
{'loss': 0.7175, 'grad_norm': 13.014863014221191, 'learning_rate': 1.748897950240989e-06, 'accuracy': 0.512499988079071, 'cls_loss': 0.71875, 'epoch': 0.06}
