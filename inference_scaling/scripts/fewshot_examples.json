{
  "accept_paper": "# Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History\n\n# Abstract\nWe study non-collaborative dialogs, where two agents have a conflict of interest but must strategically communicate to reach an agreement (e.g., negotiation). This setting poses new challenges for modeling dialog history because the dialog's outcome relies not only on the semantic intent, but also on tactics that convey the intent. We propose to model both semantic and tactic history using finite state transducers (FSTs). Unlike RNN, FSTs can explicitly represent dialog history through all the states traversed, facilitating interpretability of dialog structure. We train FSTs on a set of strategies and tactics used in negotiation dialogs. The trained FSTs show plausible tactic structure and can be generalized to other non-collaborative domains (e.g., persuasion). We evaluate the FSTs by incorporating them in an automated negotiating system that attempts to sell products and a persuasion system that persuades people to donate to a charity. Experiments show that explicitly modeling both semantic and tactic history is an effective way to improve both dialog policy planning and generation performance.\n\n\n# 1 INTRODUCTION\n\n\nIn collaborative dialog settings, agents work together and communicate to reach a common goal (He et al., 2017), such as booking flight and making restaurant reservation. Historically, in collaborative setting, the dialog history and structure is modeled explicitly by tracking semantic content, for example, the set of used slot-value pairs (Bowden et al., 2017; Larionov et al., 2018; Zeigler & Mazor, 1995a;b). Prior work also models dialog history implicitly by using an encoder-decoder model (Sordoni et al., 2015; Shang et al., 2015; Vinyals & Le, 2015; Li et al., 2016; Wen et al., 2015; Yao et al., 2015). Although these techniques show promising results in a collaborative setting, they have drawbacks when applied to non-collaborative settings, where agents have competing interests and goals but aim to reach an agreement, and they use various strategies and tactics to reach an agreement favorable to them. In non-collaborative dialog settings, leveraging effective sequences of tactics is as important as controlling for semantic content, and different tactic sequences lead to different outcomes (Zhou et al., 2019).\n\n\nLearning latent dialog structure efficiently is challenging for dialog systems. Prior work mainly focused on applying hidden Markov models (HMMs) to capture contextual dependencies within dialogs (Chotimongkol, 2008; Ritter et al., 2010; Zhai & Williams, 2014). Recently, Shi et al. (2019) proposed to use a discrete variational recurrent neural network (D-VRNN) for learning latent dialog structure because of its flexibility and nonlinear nature. In this paper, we take a different approach by using pre-trained FSTs to learn latent dialog structure. FSTs have been used in many traditional dialog systems and have proven to be effective across different domains (Larionov et al., 2018; Zeigler & Mazor, 1995a;b, inter alia).\n\n\nWe focus on modeling dialog in non-collaborative settings, and propose to explicitly leverage the dialog structure, including history of tactics and dialog acts, to improve dialog planning and generation. Specifically, we use weighted FSTs to learn dialog acts and tactics history and then integrate the learned FSTs to encoder-decoder pipeline to make the end-to-end system capture semantic and tactic history. FSTs have several advantages over a traditional recurrent neural network. First, an FST can explicitly track the entire path it traversed, which gives additional symbolic constraints and\n\n\ninformation about the dialog history. Due to the more informative history representation, an FST has a better prediction of the next step (dialog act/strategy) compared to an RNN, as we empirically confirm. A trained FST serves as a scaffold for dialog history tracking. Second, FST is more interpretable, as each state is explicitly represented by an action distribution. It is thus easier for humans to interpret model decision.\n\n\nTo leverage pre-trained FSTs, we propose an FST-enhanced hierarchical encoder-decoder model (FeHED). Our model, depicted in Figure 1, consists of a natural language understanding (NLU) module, two pre-trained FSTs, and a natural language generation (NLG) module. The NLU module has a set of classifiers, where each one is responsible for detecting a dialog act or a negotiation/persuasion strategy/tactics from a given utterance. FSTs model a latent dialog structure, which can encode dialog history. One FST is trained on sequences of dialog acts (FST-DA) and the other FST is trained on sequences of strategies (FST-S). The NLG module is a hierarchical encoder-decoder model (HED), which conditions on the outputs from FST-{DA, S} and previous utterances to predict strategies and generate system response.\n\n\nWe focus on (1) a bargaining scenario where a seller negotiates with a buyer over a given product through a chat interface online (He et al., 2018), and (2) a persuasion dialog setting where a persuader, in an online chat, attempts to persuade the persuadee to donate their earnings to a charity (Wang et al., 2019). We propose an automated agent that plays the role of the seller/persuader. Existing work only focuses on dialog acts, such as \"disagree\", which capture shallow semantics. However, these dialog acts cannot capture the pragmatics of the dialog acts. For example, whether the user disagrees politely or rudely impacts the dialog system's behavior. To capture pragmatic content, we employ negotiation strategies and tactics introduced by Zhou et al. (2019), motivated by negotiation literature. For persuasion dialog setting, we adopt a set of persuasion strategies from Wang et al. (2019). Besides pragmatics, these strategies also capture domain-specific semantics that dialog acts do not cover.\n\n\nWe evaluate our seller/persuader models using standard measures, BLEU (Papineni et al., 2002) and accuracy of predicting strategies. Additionally, we propose unigram and bigram accuracy of strategies to evaluate dialog models fairly. Experiment results show that FeHED significantly outperforms all the baseline models on four of the five metrics. Moreover, quantitative analysis shows that it is important to model both semantic and tactic history. Finally, qualitative analysis demonstrates that FSTs can track tactic history better than a RNN in non-collaborative settings.\n\n\n# 2 MODEL\n\n\nFigure 1 gives an overview of FeHED. There are four components in FeHED: a dialog act classifier, a strategy classifier, two finite state transducers (FST-DA/S), and a hierarchical encoder-decoder model (HED). The four components are connected. The output of the dialog act classifier and strategy classifier is the input of the FSTs. The FSTs' output is the input for HED along with utterance embedding. Finally, HED generates both the next strategy and the next utterance.\n\n- 1. Dialog Act Classifier converts utterances, denoted by  $u_{1}, u_{2}, \\ldots, u_{t}$ , into a sequence of dialog acts  $da_{1}, da_{2}, \\ldots, da_{t}$\n\n- 2. Strategy Classifier converts utterances into a sequence of strategies and tactics  $st_{1}, st_{2}, \\ldots, st_{t}$  used in each utterance.\n\n- 3. FST-DA/S takes a sequence of dialog acts  $da_{1}, da_{2}, \\ldots, da_{t}$  or strategies  $st_{1}, st_{2}, \\ldots, st_{t}$  (green dotted lines in Figure 1) and returns a sequence of state embeddings  $s_{1}^{da/st}, s_{2}^{da/st}, \\ldots, s_{t}^{da/st}$ .\n\n- 4. HED conditions on  $s_1^{da / st}, s_2^{da / st}, \\ldots, s_t^{da / st}$  (indicated by blue dotted lines) and  $u_1, u_2, \\ldots, u_t$  to predict a set of possible strategies  $st_{t + 1}'$  in the next utterance and generate the response  $u_{t + 1}$ .\n\n\nWe describe each component next.\n\n\n![Figure 1: FST-enhanced hierarchical encoder-decoder (FeHED). FeHED first takes dialog history  $u_{1}, u_{2}, \\ldots, u_{t}$  and feeds it to dialog act and strategy classifiers. Dialog act/strategy classifiers then output a sequence of dialog acts  $da_{1}, da_{2}, \\ldots, da_{t}$  or strategies  $st_{1}, st_{2}, \\ldots, st_{t}$ , which, in turn, are fed to FSTs (green dotted lines). FSTs then output a sequence of state embedding  $s_{1}^{da / st}, s_{2}^{da / st}, \\ldots, s_{t}^{da / st}$  to the hierarchical encoder-decoder (HED; blue dotted lines). Lastly, HED generates the system response  $u_{t+1}$  and predicts the next strategies  $st_{t+1}'$ . The red dotted line indicates a posterior constraint.  $\\oplus$  represents concatenation.](images/df9de36e925c0cc25a6840493dd4b96aece8e732660bd572e0fcebae3bee8cc1.jpg)\n\n\n# 2.1 DIALOG ACT CLASSIFIER\n\n\nWe follow the setting in He et al. (2018) and define a list of seven dialog acts, including introduction, initiate-price, insist on price, agree, disagree, inform and inquire. He et al. (2018) detect these dialog acts with a set of handcrafted regular expressions. Details about dialog acts are given in Appendix A.1.\n\n\n# 2.2 STRATEGY CLASSIFIERS\n\n\nWe use fifteen negotiation tactics, proposed by Zhou et al. (2019), operationalizing strategies and tactics described in economics and behavioral science research on negotiation (Pruitt, 1981; Bazerman et al., 2000; Fisher & Ury, 1981; Lax & Sebenius, 2006; Thompson et al., 2010). These include rhetoric strategies, e.g., talk informally, use certainty words, and behavioral strategies, e.g., build rapport, address buyer's concerns. Part of these strategies are implemented using regular expressions, for example, \"please\" is a salient word for communicate politely. Other strategies are implemented using linear classifiers, trained on negotiation dialogs. Details about negotiation strategies are given in Appendix A.2.\n\n\nWe adopt a set of persuasion strategies, developed by Wang et al. (2019), based on the Elaboration Likelihood Model (ELM) theory from social psychology (Petty & Cacioppo, 1986). Persuasion strategies include persuasive appeal, e.g., logical appeal, emotion appeal, and persuasive inquiry, e.g., source-related inquiry, personal-related inquiry. These strategies are captured with hybrid recurrent convolutional neural network classifiers (Wang et al., 2019). Details about persuasion strategies are given in Appendix A.3.\n\n\n# 2.3 FSTS FOR SEMANTIC AND STRATEGIC DIALOG HISTORY\n\n\nTwo FSTs were trained to learn latent dialog structure. One is trained with dialog act sequences, while the other one is trained with strategy sequences.\n\n\nTraining data for FSTs is sequences of dialog acts or strategies, where each sequence is extracted from a dialog by using dialog act/strategy classifier. We initialize FST with a single ergodic node\n\n\nwhere all edges go back to itself. We then iterate over this initial FST and split the state(s). We greedily select a state split, based on splitting on the incoming dialog act/strategy that minimizes the entropy of the two generated states. This follows the algorithm often used in node splitting in building decision trees (Breiman et al., 1984). There is no well-defined stopping criteria, therefore, we need to choose the number of states as a hyperparameter. We show an example of trained FSTDA with 3 states in Appendix A.4. By looking at incoming and outgoing edges of a node, we can understand which dialog state this node represents. For example, if the incoming edge of a state is \"seller answering a question\" and the outgoing edge is \"buyer inquiring\", then in this state, the buyer is most likely to ask a question.\n\n\nThe trained FST gives a probability density function (PDF) for the likelihood of transfer from the current state to each of the possible dialog acts. We use this vector of probabilities as a state embedding  $(s_t^{da/st})$ , which can be generated for each utterance and concatenated to other utterance representations. At test time, we use our dialog act or strategy classifiers to choose the predicted dialog act or strategy, and transition to the new state in the FST to get the PDF for the next set of dialog acts. The FST not only returns the current state embedding, but also all the embeddings of state it traversed since the start state.\n\n\n# 2.4 HIERARCHICAL ENCODER-DECODER(HED)\n\n\nLet  $u_{t} = [w_{1}^{t},\\dots,w_{n}^{t}]$ , where  $w_{i}^{t}$  is the  $i$ -th word in current utterance  $u_{t}$ . We use a standard GRU (Cho et al., 2014) to encode  $u_{t}$  into a hidden state representation  $h_{t}^{u}$ .\n\n\n$$\n\n$$\nh _ {t} ^ {u} = \\mathrm {G R U} ^ {u} (u _ {t})\n$$\n\n$$\n\n\nWe concatenate the utterance embedding with the output of FST-DA and FST-S to enrich the utterance embedding to incorporate dialog history.  $h_t^{\\prime u} = [h_t^u; s_t^{da}; s_t^{st}]$ . Finally, we use another GRU to combine all utterances till current time to encode the entire dialog into a hidden state  $h_t^d$\n\n\n$$\n\n$$\nh _ {t} ^ {d} = \\mathrm {G R U} ^ {d} \\left(h _ {1} ^ {\\prime u}, h _ {2} ^ {\\prime u}, \\dots , h _ {t} ^ {\\prime u}\\right)\n$$\n\n$$\n\n\nThen we predict the next utterance strategies  $st_{t+1}$  and finally, generate system response  $u_{t+1}$  using  $h_t^d$ .\n\n\nStrategy predictor Before generating system response, we add an intermediate step to predict the set of possible strategies  $st_{t + 1}$  in it. The output  $st_{t + 1}$  is a 15-dimensional binary-value vector, where each dimension represents whether a certain negotiation strategy occurred in  $u_{t + 1}$ . We compute the probability of the  $j$ -th strategy occurring in  $u_{t + 1}$  as:\n\n\n$$\n\n$$\np (s t _ {t + 1, j} = 1 | h _ {t} ^ {d}) = \\sigma (W _ {j} [ h _ {t} ^ {d} ] + b _ {j})\n$$\n\n$$\n\n\nwhere  $W_{j}$  and  $b_{j}$  are learned parameters. We denote the negative log likelihood of strategies  $\\mathcal{L}_{ST}$  as the loss function of this task:\n\n\n$$\n\n$$\n\\mathcal {L} _ {S T} = - \\sum_ {\\{j | s t _ {t + 1, j} ^ {\\prime} = 1 \\}} l o g (s t _ {t + 1, j}) - \\sum_ {\\{j | s t _ {t + 1, j} ^ {\\prime} = 0 \\}} l o g (1 - s t _ {t + 1, j}),\n$$\n\n$$\n\n\nwhere  $s t_{t + 1}^{\\prime}$  is the ground truth strategies.\n\n\nUtterance decoder is a standard GRU decoder with attention (Bahdanau et al., 2015). The input of this decoder is the dialog hidden state  $h_t^d$  concatenated with the predicted strategies  $st_{t+1}$ . It calculates a probability distribution  $p_j^{t+1}$  over vocabulary at time  $j$  conditioning on the previous word:\n\n\n$$\n\n$$\np _ {j} ^ {t + 1} = \\operatorname {s o f t m a x} \\left(\\mathbf {G R U} ^ {d e} \\left([ s t _ {t + 1}; h _ {t} ^ {d} ], w _ {j - 1} ^ {t + 1}\\right)\\right)\n$$\n\n$$\n\n\nCross entropy loss  $\\mathcal{L}_{NLG}$  is used for this generation task:\n\n\n$$\n\n$$\n\\mathcal{L}_{NLG} = -\\sum_{\\{w_{j}\\in u^{\\prime}_{t + 1}\\}}\\log (p^{t + 1}_{j,w_{j}}),\n$$\n\n$$\n\n\nwhere  $u_{t + 1}^{\\prime}$  is the target utterance.\n\n\nFinally, we combine the strategy prediction task and system utterance generation loss together. We also add a posterior constraint to enforce the generated utterances and the predicted negotiation strategies align with each other:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {j o i n t}} = \\mathcal {L} _ {N L G} + \\alpha \\mathcal {L} _ {S T} + \\beta \\sum_ {j} \\mathbb {1} (s t _ {t + 1, j} \\in u _ {t + 1})\n$$\n\n$$\n\n\nwhere  $\\alpha, \\beta$  are constants and the last term is a posterior constraint (the red dotted line in Figure 1) that has a positive value if the strategies in the generated utterance  $u_{t+1}$  does not contain some strategies in the predicted strategies  $st_{t+1}$ . We jointly train strategy predictor and utterance decoder using  $\\mathcal{L}_{joint}$ .\n\n\n# 3 EXPERIMENTS\n\n\nDatasets We evaluate our model's performance on two non-collaborative dialog data sets, CraigslistBargain (He et al., 2018) and Persuasion For Good (Wang et al., 2019). CraigslistBargain consists of dialogs of two people buying and selling products. The negotiation scenarios are crawled from craigslist.com, which includes a product description, an optional product photos, and its listing price. The buyer is given a private target price which is strictly lower than the listing price. The data was collected on Amazon Mechanical Turk (AMT) platform with two Turkers role-playing with each other. The seller aims to obtain as much profit as possible while the buyer tries to purchase the product with a price close to the private target price. Both parties are encouraged to reach an agreement. There are in total 5,383 dialogs for training, 643 for validation and 656 for testing. The average conversation length is 9.2 turns. The vocabulary size of the data is 13,928.\n\n\nPersuasion For Good dataset is also collected on AMT, where two workers were randomly assigned the roles of persuader and persuadee. The goal of the persuader is to persuade the persuadee to donate his/her task earning to a charity, Save the Children. There are 1,017 dialogs, where 300 are annotated with persuasion strategies. We split the dataset into 180 training dialogs, 60 for validation and 60 for test. The average conversation length is 10.43 turns. The vocabulary size is 8,141.\n\n\nExperimental setup We train each model for 20 epochs and choose the one that performs best on validation dataset. We use a mini-batch of 20 and learning rate of  $5e^{-4}$  for encoders and  $5e^{-3}$  for utterance decoder. The encoders and decoder are GRUs, each has two layers and a hidden state size of 300.\n\n\nWe compare FeHED to a list of baselines and present their results in Table 1.\n\n- - HED: A vanilla HED that does not consider dialog act nor strategy.\n\n- - FeHED-SP: To test the importance of modeling strategies alone, we remove everything that involves strategies, specifically, strategy prediction, tracking and decoding.\n\n- - FeHED-FSTs: To test the importance of incorporating semantic and strategic history, we remove both FST-DA and FST-S from FeHED.\n\n- - FeHED-FST-S/DA: To test which type of history is more important, semantic or tactic, we remove either FST-S or FST-DA from FeHED.\n\n- - HED+RNN: To compare how well FSTs can model strategic history, we replace FST in FeHED with a recurrent neural network (RNN) to model tactic sequence, similar to the dialog manager proposed by He et al. (2018).\n\n- - Sequicity: Lei et al. (2018) use belief span to model dialog state and improve system performance. To apply Sequicity to our problem, we concatenate dialog acts and strategies as belief span to track history instead of slot-values.\n\n\nEvaluation We evaluate FeHED's performance (1) on the ability of generating high quality responses, and (2) on whether the responses carry the correct strategy. We also explore the effectiveness of history tracking by performing an ablation study. Lastly, we conduct human evaluation to test the dialog system's persuasiveness, coherence and naturalness.\n\n\nTo evaluate FeHED's responses we use four types of metrics:\n\n- - Strategy predictor's F1 and accuracy (S.F1 and S.acc). We evaluate strategy prediction performance along with response generation quality, to assess strategy tracking. However, not every model in Table 1 can output strategy. For these models, we replace their utterance decoder with a strategy predictor and retrain the model by optimizing  $\\mathcal{L}_{ST}$ . We evaluate the retrained model by measuring its F1 score and accuracy.\n\n- - We use BLEU to evaluate generation quality. Although in other generation tasks BLEU is a standard measure, in dialog modeling it is not a reliable metric, as different sentences can convey the same content in different ways.\n\n- - Utterance decoder's accuracy of generated strategies (Uni.acc, Bi.acc). We first apply our strategy classifier to extract strategies in generated utterance. Then, the extracted strategies are compared with ground truth to calculate the accuracy (Uni.acc). Due to the nature of dialogs, multiple strategies can be appropriate given the same dialog history. Therefore, we expand the ground truth strategy set by sampling dialogs with similar dialog history (previous two turns). We use the expanded ground truth set to calculate bigram accuracy (Bi.acc).\n\n- - Human evaluation. We also conducted two types of human evaluation for the negotiation task: (1) third-person rating; (2) second-person rating. For third-person rating, we randomly give each participant four different types of dialogs and ask him/her to rate each dialog's seller in terms of persuasiveness, coherence, and naturalness (on a scale of 1 to 5). These dialogs have FeHED, FeHED-FST-DA, FeHED-FST-S or HED to play the role of seller. For second-person rating, we ask participants to conduct four conversations by playing the role of buyer to negotiate with FeHED, FeHED-FST-DA, FeHED-FST-S, and HED, respectively. Then, we ask them to compare and rate each model in terms of persuasiveness, coherence, and naturalness (on a scale of 1 to 5).\n\n\n# 3.1 RESULTS\n\n\n<table><tr><td rowspan=\"2\">Models</td><td colspan=\"5\">Negotiation</td><td colspan=\"5\">Persuasion</td></tr><tr><td>Uni.acc</td><td>Bi.acc</td><td>S.acc</td><td>S.F1</td><td>BLEU</td><td>Uni.acc</td><td>Bi.acc</td><td>S.acc</td><td>S.F1</td><td>BLEU</td></tr><tr><td>FeHED</td><td>49.6</td><td>59.3</td><td>61.9</td><td>22.8</td><td>20.6</td><td>0.18</td><td>0.77</td><td>0.80</td><td>18.2</td><td>13.5</td></tr><tr><td>-FSTs</td><td>42.3</td><td>55.1</td><td>42.4</td><td>19.5</td><td>20.5</td><td>0.13</td><td>0.70</td><td>0.80</td><td>15.6</td><td>13.6</td></tr><tr><td>-FST-S</td><td>43.1</td><td>54.3</td><td>46.7</td><td>20.2</td><td>20.9</td><td>0.12</td><td>0.70</td><td>0.75</td><td>17.4</td><td>13.8</td></tr><tr><td>-FST-DA</td><td>42.8</td><td>54.9</td><td>49.2</td><td>20.8</td><td>20.3</td><td>0.10</td><td>0.70</td><td>0.80</td><td>16.8</td><td>14.0</td></tr><tr><td>-SP</td><td>46.5</td><td>56.3</td><td>47.3</td><td>20.4</td><td>20.5</td><td>0.15</td><td>0.67</td><td>0.78</td><td>18.0</td><td>13.9</td></tr><tr><td>HED+RNN</td><td>46.5</td><td>56.8</td><td>57.2</td><td>15.5</td><td>20.3</td><td>0.16</td><td>0.75</td><td>0.77</td><td>17.9</td><td>13.6</td></tr><tr><td>Sequicity</td><td>44.0</td><td>57.9</td><td>-</td><td>-</td><td>16.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HED</td><td>36.9</td><td>51.2</td><td>38.4</td><td>15.6</td><td>20.8</td><td>0.12</td><td>0.66</td><td>0.77</td><td>15.8</td><td>14.1</td></tr></table>\n\n\nTable 1: Ablation and baseline results. FeHED achieves the best performance on all metrics except BLEU. Moreover, removing any component results in a significant decrease in performance.\n\n\nTable 1 shows the result of negotiation and persuasion dialogs. FeHED achieves the best performance on all metrics except BLEU. However, single-reference BLEU assumes only one possible system response while dialog system can have multiple valid responses. Comparing with a vanilla HED model, FeHED improves S.acc by  $+23.5$ , S.F1 by  $+7.2$ , Uni.acc by  $+12.7$  and Bi.acc by  $+8.1$ , while maintaining comparable BLEU score. This suggests that incorporating semantic and tactic information leads to a better dialog system. We also evaluate model performance when ablating one or more model components. Result shows that removing any FST causes a worse performance, which suggests modeling both semantic and strategic history is necessary. Moreover, the HED+RNN setup confirms that FSTs better model semantic and tactic history than RNNs in non-collaborative settings. Noticeably, all models' S.F1 are low, which may be due to the fact that the negotiation strategy set is large and some of them have low frequency in training data, therefore causing overall low performance. We also observe that S.acc is higher than Uni.acc for all the models. This is expected, because predicting negotiation strategies is more straightforward than generating system utterances with the correct negotiation strategies. Although Sequicity has a very high Uni.acc and Bi.acc, it has a much lower BLEU score compared to all the other models except FeHED. It is likely because Sequicity uses CopyNet (Gu et al., 2016), which is not designed for non-collaborative dialogs but\n\n\nrather collaborative tasks (e.g. booking a restaurant). Negotiation and persuasion tasks yield similar results, confirming that the benefit of using FSTs to model both semantic and strategic history is not limited to a single domain.\n\n\n<table><tr><td rowspan=\"2\">Models</td><td colspan=\"4\">Second-person Rating</td><td colspan=\"4\">Third-person Rating</td></tr><tr><td>Persuasive</td><td>Coherent</td><td>Natural</td><td>Sale Price</td><td>Persuasive</td><td>Coherent</td><td>Natural</td><td>Sale Price</td></tr><tr><td>FeHED</td><td>2.6</td><td>2.8</td><td>3.0</td><td>0.84</td><td>3.2</td><td>3.9</td><td>3.5</td><td>0.68</td></tr><tr><td>-FST-DA</td><td>2.5</td><td>2.2</td><td>2.4</td><td>0.70</td><td>3.0</td><td>3.4</td><td>3.5</td><td>0.64</td></tr><tr><td>-FST-S</td><td>2.0</td><td>2.4</td><td>2.4</td><td>0.64</td><td>2.9</td><td>3.4</td><td>3.3</td><td>0.59</td></tr><tr><td>HED+RNN</td><td>2.3</td><td>2.5</td><td>2.6</td><td>0.49</td><td>2.8</td><td>3.8</td><td>3.6</td><td>0.44</td></tr><tr><td>HED</td><td>1.8</td><td>1.9</td><td>1.9</td><td>0.62</td><td>2.9</td><td>3.4</td><td>3.1</td><td>0.50</td></tr></table>\n\n\nTable 2 shows the results of human evaluation. For third-person rating, we asked an expert to generate 20 dialogs by negotiating with FeHED, FeHED-FST-DA, FedHED-S, HED+RNN and HED respectively (5 dialogs each). We then recruited 50 people on AMT to rate these generated dialogs. Result shows that FeHED is more persuasive, coherent and natural than all the baselines. For second-person rating, we asked 50 participants on AMT to play the role of buyer to negotiate with FeHED, FeHED-FST-DA, FeHED-FST-S, HED+RNN and HED respectively. Results show that FeHED outperforms RNN and other models on all the metrics except naturalness. It is likely because RNN is trained jointly with HED, but it is not good at explicitly tracking and preserving the history. Although FST is learned separately, it forces the model to learn the history through a list of traversed states. We analyze the generated dialogs for human evaluation and find that baselines are more likely to accept unfair offers and apply inappropriate strategies.\n\n\nTable 2: Human evaluation ratings for (on a scale from 1 to 5) for FeHED, FeHED-FST-DA, FeHED-FST-S, and HED. We conducted third-person rating and second-person rating. Sale price is normalized.\n\n\n<table><tr><td colspan=\"2\">Listing Price: 40\nBuyer&#x27;s Target Price: 36\nTitle: 2017 NEW Stans 24 and 26 Tubeless Tire Kit\nProduct Description: “I have a NEW Stan&#x27;s Tubeless tire Conversion kit for sale for 24 AND 26 wheels/tires. Pick up in Walnut Creek, CA in evenings”</td></tr><tr><td>Seller (FeHED): S Buyer (Human): B</td><td>Seller (HED): S Buyer (Human): B</td></tr><tr><td>B: Hello!\nS: Hi\nFST: ⟨intro, 0.49⟩, ⟨init-price, 0.19⟩... \nB: How old is your kit?\nS: I only had it for months.\nFST: ⟨inform, 0.51⟩, ⟨disagree, 0.20⟩... \nB: The price is a bit high, can you do 30?\nS: I can&#x27;t go that low. I could go down to 38.4.\nFST: ⟨propose price, 0.51⟩, ⟨agree, 0.11⟩... \nB: How about 35? \n...</td><td>B: Hello!\nS: Hello\nB: How old is your kit?\nS: It&#x27;s only a year old.\nB: The price is a bit high, can you do 30?\nS: I can do that.\nS: &lt;offer 30&gt;\nB: &lt;accept&gt;</td></tr><tr><td>Seller (FeHED): S Buyer (Human): B</td><td>Seller (HED+RNN): S Buyer (Human): B</td></tr><tr><td>B: Hello!\nS: Hello\nB: How much are you looking for?\nS: I&#x27;m asking 40 for it.\nB: Can you do 30 dollars and I can pick it up?\nS: How about 37.6 ? I will deliver to you.\nFST: ⟨negotiate side offers, 0.49⟩\n...</td><td>B: Hello!\nS: Hello\nB: How much are you looking for?\nS: I&#x27;m asking 40 for it.\nB: Can you do 30 dollars and I can pick it up?\nS: Can you do 37.6?\n...</td></tr></table>\n\n\nTable 3: Examples of generated responses with and without FSTs.\n\n\nFinally, we show and example of a negotiation dialog. The first example in Table 3 shows the dialog with the bot that uses history tracking (FeHED). For the examples with FeHED, we also show top most likely dialog acts given by the current state of FSTs. As shown in bold text in the first two examples in Table 3, HED tends to agree with buyer's proposal although they have just started negotiating the price. On the other hand, FeHED adopts a more appropriate action: rejects buyer's proposal and gives a counter price. This is likely because FSTs give the system a state embedding, where  $\\langle \\text{propose price} \\rangle$  has the highest probability (0.51), which tells the system that in the current state, the probability of making a counter price is 0.51. Besides  $\\langle \\text{propose price}, 0.51 \\rangle$ , other generated utterances are mostly following the top most likely dialog acts proposed by FSTs (e.g.  $\\langle \\text{intro}, 0.49 \\rangle$ ,  $\\langle \\text{inform}, 0.51 \\rangle$ ).\n\n\nTable 1 shows that FST models dialog history better than RNN in non-collaborative setting. Two examples in the last row of Table 3 provide an example. Noticeably, FeHED recognizes a tactics used in the previous utterance (\"pick it up\") and responds \"I will deliver to you\"; but with RNN, the bot ignores the previous tactic history and proposes another price. Presumably, this is because FST explicitly captures tactics used in the history, while RNN does not.\n\n\n# 4 RELATED WORK\n\n\nOur work extends the line of research on non-collaborative dialog tasks, such as negotiation and persuasion. Lewis et al. (2017) demonstrated a task where a collection of items are divided between two agents. Some prior work also focus on a strategic board game called Settlers of Catan where players can offer resources in exchange for others and they can also reply to offers from other players (Cuayáhuitl et al., 2015; Keizer et al., 2017). However, these tasks do not require modeling rich communication skills, but focus on decision-making skills. Therefore, prior studies on these tasks only focus on tactic history but ignore semantic content. To consider both semantic and tactic history, we choose a bargaining scenario proposed by He et al. (2018), where a seller and a buyer negotiate price over an item for sale. To show the generalizability of our work, we also choose a persuasion dialog setting proposed by Wang et al. (2019), where a persuader tries to strategically convince a persuadee to donate his/her earnings to a charity.\n\n\nMost end-to-end approaches incorporate history through hidden states (Sordoni et al., 2015; Shang et al., 2015; Vinyals & Le, 2015; Li et al., 2016; Wen et al., 2015; Yao et al., 2015). Such methods only focus on capturing semantic history. Lei et al. (2018) proposed a text span called belief span for encoding dialog history, which is combined with a simple seq2seq model to improve dialog generation. Specifically, belief span tracks entities mentioned so far (e.g. restaurant types) to explicitly model dialog history. We utilize trained FSTs to encode dialog history instead of a text span. Additionally, it requires human annotations as supervision to train a belief span, while our FSTs are fully unsupervised. Therefore, our FSTs can be applied to other domains easily.\n\n\nRule-based dialog modules incorporate history using symbolic rules. Larionov et al. (2018); Zeigler & Mazor (1995a;b) use a finite-state automata to keep track of dialog context. Fang et al. (2018) suggests building a hierarchical dialog manager that keeps track of engagement, coherence, and user experience. Bowden et al. (2017) utilizes a state graph structure to model dialog flows. He et al. (2018) applies a neural model to predict a sequence of dialog acts as dialog manager. We also utilize finite-state machine in FeHED, but it is automatically learned using unsupervised data. Moreover, we use FSTs to learn the dialog structure instead of using it directly as the dialog manager.\n\n\n# 5 CONCLUSION\n\n\nIn non-collaborative dialog settings, it is important to not only track the semantic intent, but also strategies and tactics that express this intent. To improve non-collaborative dialog planning and generation, we propose to explicitly model both semantic and tactic history by using automatically trained FSTs. We then evaluate the trained FSTs on a negotiation dialog system and a persuasion dialog system. Result shows that explicitly modeling both semantic and tactic history achieves the best performance. We have also shown that FST models tactic history better than a RNN in non-collaborative dialog settings.\n\n\n# REFERENCES\n\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proc. ICLR, 2015.\n\n- Max H. Bazerman, Jared R Curhan, Don A Moore, and Kathleen L Valley. Negotiation. Annual review of psychology, 51(1):279-314, 2000.\n\n- K. K. Bowden, S. Oraby, J. Wu, A. Misra, and M. Walker. Combining search with structured data to create a more engaging user experience in open domain dialogue. In Proc. SCAI, 2017.\n\n- L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth & Brooks, Pacific Grove, CA., 1984.\n\n- K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoder-decoder approaches. In arXiv preprint arXiv:1409.1259, 2014.\n\n- Ananlada Chotimongkol. Learning the structure of task-oriented conversations from the corpus of in domain dialogs. In Ph.D. thesis, Carnegie Mellon University, Language Technologies Institute, School of Computer Science, 2008.\n\n- Heriberto Cuayáhuitl, Simon Keizer, and Oliver Lemon. Strategic dialogue management via deep reinforcement learning. In NIPS Workshop on Deep Reinforcement Learning, 2015.\n\n- Hao Fang, Hao Cheng, Maarten Sap, Elizabeth Clark, Ari Holtzman, Yejin Choi, Noah A. Smith, and Mari Ostendorf. Sounding board: A user-centric and content-driven social chatbot. In Proc. NAACL, 2018.\n\n- Roger Fisher and William Ury. Getting to Yes: Negotiating Agreement Without Giving In. Boston, MA: Houghton Mifflin Company, 1981.\n\n- Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. Incorporating copying mechanism in sequence-to-sequence learning. In Proc. ACL, 2016.\n\n- He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings. In Proc. ACL, 2017.\n\n- He He, Derek Chen, Anusha Balakrishnan, and Percy Liang. Decoupling strategy and generation in negotiation dialogues. In EMNLP, 2018.\n\n- Simon Keizer, Markus Guhe, Heriberto Cuayáhuitl, Ioannis Efstathiou, Klaus-Peter Engelbrecht, Mihai Dobre, Alex Lascarides, and Oliver Lemon. Evaluating persuasion strategies and deep reinforcement learning methods for negotiation dialogue agents. In Proc. EACL, 2017.\n\n- George Larionov, Zachary Kaden, Hima Varsha Dureddy, Gabriel Bayomi T. Kalejaiye, Mihir Kale, Srividya Pranavi Potharaju, Ankit Parag Shah, and Alexander I Rudnicky. Tartan: A retrieval-based social bot powered by a dynamic finite-state machine architecture. In arXiv:1812.01260, 2018.\n\n- David A. Lax and James K. Sebenius. 3-D Negotiation: Powerful tools to change the game in your most important deals. Harvard Business Press, 2006.\n\n- Wenqiang Lei, Xisen Jin, Zhaochun Ren, Xiangnan He, Min-Yen Kan, and Dawei Yin. Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures. In Proc. ACL, 2018.\n\n- Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-to-end learning of negotiation dialogues. In Proc. EMNLP, pp. 2443-2453, 2017.\n\n- Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proc. NAACL-HLT, 2016.\n\n- Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL, pp. 311-318, 2002.\n\n- Richard E Petty and John T Cacioppo. The elaboration likelihood model of persuasion. In Communication and persuasion, pp. 1-24. Springer, 1986.\n\n- Dean G. Pruitt. Negotiation behavior. New York: Academic Press, 1981.\n\n- Alan Ritter, Colin Cherry, and Bill Dolan. Unsupervised modeling of twitter conversations. In Proc. NAACL, 2010.\n\n- Lifeng Shang, Zhengdong Lu, and Hang Li. Neural responding machine for short-text conversation. In Proceedings ACL, 2015.\n\n- Weiyan Shi, Tiancheng Zhao, and Zhou Yu. Unsupervised dialog structure learning. In Proc. NAACL, 2019.\n\n- Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Meg Mitchell, JianYun Nie, Jianfeng Gao, and Bill Dolan. A neural network approach to context-sensitive generation of conversational responses. In Proc. NAACL-HLT, 2015.\n\n- Leigh L Thompson, Jiunwen Wang, and Brian C Gunia. Negotiation. Annual review of psychology, 61:491-515, 2010.\n\n- Oriol Vinyals and Quoc Le. A neural conversational model. In Proc. ICML Deep Learning Workshop, 2015.\n\n- Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, and Zhou Yu. Persuasion for good: Towards a personalized persuasive dialogue system for social good. In Proc. ACL, 2019.\n\n- Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, and Steve Young. A diversity-promoting objective function for neural conversation models. In Proc. EMNLP, 2015.\n\n- Kaisheng Yao, Geoffrey Zweig, and Baolin Peng. Attention with intention for a neural network conversation model. In In NIPS workshop on Machine Learning for Spoken Language Understanding and Interaction., 2015.\n\n- B Zeigler and B. Mazor. Dialog design for a speech-interactive automation system. In Proc. EUROSPEECH, 1995a.\n\n- B Zeigler and B. Mazor. A formal technique for automated dialogue development. In Proc. DIS, 1995b.\n\n- Ke Zhai and Jason D Williams. Discovering latent structure in task-oriented dialogues. In Proc. ACL, 2014.\n\n- Yiheng Zhou, He He, Alan W Black, and Yulia Tsvetkov. A dynamic strategy coach for effective negotiation. In Proc. SigDial, 2019.",
  "accept_review": "**THOUGHT:**  \nThe paper proposes a novel approach to modeling dialog history in non-collaborative settings by using finite state transducers (FSTs) to explicitly capture both semantic and strategic history. The authors provide a clear problem statement, well-motivated approach, and robust evaluation across multiple metrics and human evaluations. The method seems to significantly outperform baselines, particularly in terms of strategic history modeling. However, the paper lacks some clarity on the exact nature of the FSTs and how they are trained. It also does not delve deeply into the interpretability and potential scalability of the FSTs. The BLEU score is lower than expected, but this is addressed by using different evaluation metrics that better capture the nuances of the task. The human evaluation provides strong evidence of the model's effectiveness, though the paper does not discuss potential biases in the human evaluation process. Overall, the paper is well-written and presents a valuable contribution to the field of dialog systems, but there are some areas that need further exploration and clarification.\n\n**REVIEW:**  \n```json\n{\n  \"summary\": \"The paper proposes a method to model both semantic and strategic history in non-collaborative dialog systems using FSTs, showing significant improvement over baselines in terms of strategy tracking and human evaluation.\",\n  \"questions\": \"Can the authors provide more detail on the exact training process of the FSTs? How does the method scale with larger datasets or more complex domains? Are there any potential biases in the human evaluation process?\",\n  \"limitations\": \"The BLEU score is lower than expected, but the paper uses different metrics to evaluate the model's performance. The exact nature of the FSTs and how they are trained is not fully explained.\",\n  \"strengths\": \"Clear problem statement, well-motivated approach, comprehensive evaluation, and significant improvements over existing baselines.\",\n  \"weaknesses\": \"Lack of detail on the FST training process, potential scalability issues, and the BLEU score is not as high as one might expect for a text generation task.\",\n  \"ethical_concerns\": false,\n  \"soundness\": 4,\n  \"presentation\": 5,\n  \"contribution\": 4,\n  \"overall\": 8,\n  \"confidence\": 4,\n  \"decision\": \"accept\"\n}\n```",
  "reject_paper": "# MANAS: Multi-Agent Neural Architecture Search\n\n# Abstract\nThe Neural Architecture Search (NAS) problem is typically formulated as a graph search problem where the goal is to learn the optimal operations over edges in order to maximize a graph-level global objective. Due to the large architecture parameter space, efficiency is a key bottleneck preventing NAS from its practical use. In this paper, we address the issue by framing NAS as a multi-agent problem where agents control a subset of the network and coordinate to reach optimal architectures. We provide two distinct lightweight implementations, with reduced memory requirements ($1/8$th of state-of-the-art), and performances above those of much more computationally expensive methods. Theoretically, we demonstrate vanishing regrets of the form $\\mathcal{O}(\\sqrt{T})$, with $T$ being the total number of rounds. Finally, aware that random search is an (often ignored) effective baseline we perform additional experiments on $3$ alternative datasets and $2$ network configurations, and achieve favorable results in comparison with this baseline and other competing methods.\n\n\n# 1 INTRODUCTION\n\n\nDetermining an optimal architecture is key to accurate deep neural networks (DNNs) with good generalisation properties (Szegedy et al., 2017; Huang et al., 2017; He et al., 2016; Han et al., 2017; Conneau et al., 2017; Merity et al., 2018). Neural architecture search (NAS), which has been formulated as a graph search problem, can potentially reduce the need for application-specific expert designers allowing for a wide-adoption of sophisticated networks in various industries. Zoph and Le (2017) presented the first modern algorithm automating structure design, and showed that resulting architectures can indeed outperform human-designed state-of-the-art convolutional networks (Ko, 2019; Liu et al., 2019). However, even in the current settings where flexibility is limited by expertly-designed search spaces, NAS problems are computationally very intensive with early methods requiring hundreds or thousands of GPU-days to discover state-of-the-art architectures (Zoph and Le, 2017; Real et al., 2017; Liu et al., 2018a,b).\n\n\nResearchers have used a wealth of techniques ranging from reinforcement learning, where a controller network is trained to sample promising architectures (Zoph and Le, 2017; Zoph et al., 2018; Pham et al., 2018), to evolutionary algorithms that evolve a population of networks for optimal DNN design (Real et al., 2018; Liu et al., 2018b). Alas, these approaches are inefficient and can be extremely computationally and/or memory intensive as some require all tested architectures to be trained from scratch. Weight-sharing, introduced in ENAS (Pham et al., 2018), can alleviate this problem. Even so, these techniques cannot easily scale to large datasets, e.g., ImageNet. More recently, gradient-based frameworks enabled efficient solutions by introducing a continuous relaxation of the search space. For example, DARTS (Liu et al., 2019) uses this relaxation to optimise architecture parameters using gradient descent in a bi-level optimisation problem, while SNAS (Xie et al., 2019) updates architecture parameters and network weights under one generic loss. Still, due to memory constraints the search has to be performed on 8 cells, which are then stacked 20 times for the final architecture. This solution is a coarse approximation to the original problem as show in Section 6 of this work and in Doe (2019). In fact, we show that searching directly over 20 cells leads to a reduction in test error (0.24 p.p.;  $8\\%$  relative to Liu et al., 2019). ProxylessNAS (Cai et al., 2019) is one exception, as it can search for the final models directly; nonetheless they still require twice the amount of memory used by our proposed algorithm.\n\n\nTo enable the possibility of large-scale joint optimisation of deep architectures we contribute MANAS, the first multi-agent learning algorithm for neural architecture search. Our algorithm combines the memory and computational efficiency of multi-agent systems, which is achieved through action coordination with the theoretical rigour of online machine learning, allowing us to balance exploration versus exploitation optimally. Due to its distributed nature, MANAS enables large-scale optimisation of deeper networks while learning different operations per cell. Theoretically, we demonstrate that MANAS implicitly coordinates learners to recover vanishing regrets, guaranteeing convergence. Empirically, we show that our method achieves state-of-the-art accuracy results among methods using the same evaluation protocol but with significant reductions in memory (1/8th of Liu et al., 2019) and search time (70% of Liu et al., 2019).\n\n\nThe multi-agent (MA) framework is inherently scalable and allows us to tackle an optimization problem that would be extremely challenging to solve efficiently otherwise: the search space of a single cell is  $8^{14}$  and there is no fast way of learning the joint distribution, as needed by a single controller. More cells to learn exacerbates the problem, and this is why MA is required, as for each agent the size of the search space is always constant.\n\n\nIn short, our contributions can be summarised as: (1) framing NAS as a multi-agent learning problem (MANAS) where each agent supervises a subset of the network; agents coordinate through a credit assignment technique which infers the quality of each operation in the network, without suffering from the combinatorial explosion of potential solutions. (2) Proposing two lightweight implementations of our framework that are theoretically grounded. The algorithms are computationally and memory efficient, and achieve state-of-the-art results on Cifar-10 and ImageNet when compared with competing methods. Furthermore, MANAS allows search directly on large datasets (e.g. ImageNet). (3) Presenting 3 news datasets for NAS evaluation to minimise algorithmic overfitting; and offering a fair comparison with a random baseline.\n\n\n# 2 RELATED WORK\n\n\nMANAS derives its search space from DARTS (Liu et al., 2019) and is therefore most related to other gradient-based NAS methods that use the same search space. SNAS (Xie et al., 2019) appears similar at a high level, but has important differences: 1) it uses GD to learn the architecture parameters. This requires a differentiable objective (MANAS does not) and leads to 2) having to forward all operations (see their Eqs.5,6), thus negating any memory advantages (which MANAS has), and effectively requiring repeated cells and preventing search on ImageNet. ENAS (Pham et al., 2018) is also very different: its use of RL implies dependence on past states (the previous operations in the cell). It explores not only the stochastic reward function but also the relationship between states, which is where most of the complexity lies. Furthermore, RL has to balance exploration and exploitation by relying on sub-optimal heuristics, while MANAS, due to its theoretically optimal approach from online learning, is more sample efficient. Finally, ENAS uses a single LSTM (which adds complexity and problems such as exploding/vanishing gradients) to control the entire process, and is thus following a monolithic approach. Indeed, at a high level, our multi-agent framework can be seen as a way of decomposing the monolithic controller into a set of simpler, independent sub-policies. This provides a more scalable and memory efficient approach that leads to higher accuracy, as confirmed by our experiments.\n\n\n# 3 PRELIMINARY: NEURAL ARCHITECTURE SEARCH\n\n\nWe consider the NAS problem as formalised in DARTS (Liu et al., 2019). At a higher level, the architecture is composed of a computation cell that is a building block to be learned and stacked in the network. The cell is represented by a directed acyclic graph with  $V$  nodes and  $N$  edges; edges connect all nodes  $i,j$  from  $i$  to  $j$  where  $i < j$ . Each vertex  $\\pmb{x}^{(i)}$  is a latent representation for  $i \\in \\{1,\\dots,V\\}$ . Each directed edge  $(i,j)$  (with  $i < j$ ) is associated with an operation  $o^{(i,j)}$  that transforms  $\\pmb{x}^{(i)}$ . Intermediate node values are computed based on all of its predecessors as  $\\pmb{x}^{(j)} = \\sum_{i < j} o^{(i,j)}(\\pmb{x}^{(i)})$ . For each edge, an architect needs to intelligently select one operation  $o^{(i,j)}$  from a finite set of  $K$  operations,  $\\mathcal{O} = \\{o_k(\\cdot)\\}_{k=1}^K$ , where operations represent some function to be applied to  $\\pmb{x}^{(i)}$  to compute  $\\pmb{x}^{(j)}$ , e.g., convolutions or pooling layers. To each  $o_k^{(i,j)}(\\cdot)$  is associated a\n\n\nset of operational weights  $w_{k}^{(i,j)}$  that needs to be learned (e.g. the weights of a convolution filter). Additionally, a parameter  $\\alpha_{k}^{(i,j)} \\in \\mathbb{R}$  characterises the importance of operation  $k$  within the pool  $\\mathcal{O}$  for edge  $(i,j)$ . The sets of all the operational weights  $\\{w_{k}^{(i,j)}\\}$  and architecture parameters  $\\{\\alpha_{k}^{(i,j)}\\}$  are denoted by  $w$  and  $\\alpha$ , respectively. DARTS defined the operation  $\\bar{o}^{(i,j)}(\\pmb{x})$  as\n\n\n$$\n\n$$\n\\bar {o} ^ {(i, j)} (\\boldsymbol {x}) = \\sum_ {k = 1} ^ {K} \\frac {e ^ {\\alpha_ {k} ^ {(i , j)}}}{\\sum_ {k ^ {\\prime} = 1} ^ {K} e ^ {\\alpha_ {k ^ {\\prime}} ^ {(i , j)}}} \\cdot o _ {k} ^ {(i, j)} (\\boldsymbol {x}) \\tag {1}\n$$\n\n$$\n\n\nin which  $\\alpha$  encodes the network architecture. The optimal choice of architecture is defined by\n\n\n$$\n\n$$\n\\boldsymbol {\\alpha} ^ {\\star} = \\min  _ {\\boldsymbol {\\alpha}} \\mathcal {L} ^ {(\\text {v a l})} (\\boldsymbol {\\alpha}, \\boldsymbol {w} ^ {\\star} (\\boldsymbol {\\alpha})) \\quad \\text {s . t .} \\quad \\boldsymbol {w} ^ {\\star} (\\boldsymbol {\\alpha}) = \\arg \\min  _ {\\boldsymbol {w}} \\mathcal {L} ^ {(\\text {t r a i n})} (\\boldsymbol {\\alpha}, \\boldsymbol {w}). \\tag {2}\n$$\n\n$$\n\n\nThe final objective is to obtain a sparse architecture  $\\mathcal{Z}^{\\star} = \\{\\mathcal{Z}^{(i,j)}\\}, \\forall i, j$  where  $\\mathcal{Z}^{(i,j)} = [z_1^{(i,j)}, \\ldots, z_K^{(i,j)}]$  with  $z_k^{(i,j)} = 1$  for  $k$  corresponding to the best operation and 0 otherwise. That is, for each pair  $(i,j)$  a single operation is selected.\n\n\n# 4 ONLINE MULTI-AGENT LEARNING FOR AUTOML\n\n\nNAS suffers from a combinatorial explosion in its search space. A recently proposed approach to tackle this problem is to approximate the discrete optimisation variables (i.e., edges in our case) with continuous counterparts and then use gradient-based optimisation methods. DARTS (Liu et al., 2019) introduced this method for NAS, though it suffers from two important drawbacks. First, the algorithm is memory and computationally intensive  $(\\mathcal{O}(NK)$  with  $K$  being total number of operations between a pair of nodes and  $N$  the number of nodes) as they require loading all operation parameters into GPU memory. As a result, DARTS only optimises over a small subset of 8 cells, which are then stacked together to form a deep network of 20. Naturally, such an approximation is bound to be sub-optimal. Second, evaluating an architecture amounts to a prediction on a validation set using the optimal set of network parameters. Learning these, unfortunately, is\n\n\n![Figure 1: MANAS with single cell. Between each pair of nodes, an agent  $\\mathcal{A}_i$  selects action  $a^{(i)}$  according to  $\\pi^{(i)}$ . Feedback from the validation loss is used to update the policy.](images/dee9985b02cd55cc50676849cf7b27890ea12abc8e8f9fa3db82154ff3ab1b07.jpg)\n\n\nhighly demanding since for an architecture  $\\mathcal{Z}_t$ , one would like to compute  $\\mathcal{L}_t^{(\\mathrm{val})}(\\mathcal{Z}_t, \\boldsymbol{w}_t^\\star)$  where  $\\boldsymbol{w}_t^\\star = \\arg \\min_{\\boldsymbol{w}} \\mathcal{L}_t^{(\\mathrm{train})}(\\boldsymbol{w}, \\mathcal{Z}_t)$ . DARTS, uses weight sharing that updates  $\\boldsymbol{w}_t$  once per architecture, with the hope of tracking  $\\boldsymbol{w}_t^\\star$  over learning rounds. Although this technique leads to significant speed up in computation, it is not clear how this approximation affects the validation loss function.\n\n\nNext, we detail a novel methodology based on a combination of multi-agent and online learning to tackle the above two problems (Figure 1). Multi-agent learning scales our algorithm, reducing memory consumption by an order of magnitude from  $\\mathcal{O}(NK)$  to  $\\mathcal{O}(N)$ ; and online learning enables rigorous understanding of the effect of tracking  $\\boldsymbol{w}_t^\\star$  over rounds.\n\n\n# 4.1 NAS AS A MULTI-AGENT PROBLEM\n\n\nTo address the computational complexity we use the weight sharing technique of DARTS. However, we try to handle in a more theoretically grounded way the effect of approximation of  $\\mathcal{L}_t^{(\\mathrm{val})}(\\mathcal{Z}_t,\\boldsymbol{w}_t^\\star)$  by  $\\mathcal{L}_t^{(\\mathrm{val})}(\\mathcal{Z}_t,\\boldsymbol{w}_t)$ . Indeed, such an approximation can lead to arbitrary bad solutions due to the uncontrollable weight component. To analyse the learning problem with no stochastic assumptions on the process generating  $\\nu = \\{\\mathcal{L}_1,\\dots ,\\mathcal{L}_T\\}$  we adopt an adversarial online learning framework.\n\n\n```\n1: Initialize:  $\\pi_1^i$  is uniform random over all  $j\\in \\{1,\\dots N\\}$ . And random  $\\pmb{w}_{1}$  weights.  \n2: For  $t = 1,\\ldots ,T$   \n3: * Agent  $\\mathcal{A}_i$  samples  $\\pmb{a}_t^i\\sim \\pi_t^i (\\pmb{a}_t^i)$  for all  $i\\in \\{1,\\dots ,N\\}$ , forming architecture  $\\mathcal{Z}_t$ .  \n4: Compute the training loss  $\\mathcal{L}_t^{(\\mathrm{train})}(\\pmb {a}_t) = \\mathcal{L}_t^{(\\mathrm{train})}(\\mathcal{Z}_t,\\pmb {w}_t)$   \n5: Update  $\\pmb{w}_{t + 1}$  for all operation  $\\pmb{a}_t^i$  in  $\\mathcal{Z}_t$  from  $\\pmb{w}_t$  using back-propagation.  \n6: Compute the validation loss  $\\mathcal{L}_t^{(\\mathrm{val})}(\\pmb {a}_t) = \\mathcal{L}_t^{(\\mathrm{val})}(\\mathcal{Z}_t,\\pmb {w}_{t + 1})$   \n7: * Update  $\\pi_{t + 1}^i$  for all  $i\\in \\{1,\\dots N\\}$  using  $\\mathcal{Z}_1,\\dots ,\\mathcal{Z}_t$  and  $\\mathcal{L}_1^{(\\mathrm{val})},\\dots ,\\mathcal{L}_t^{(\\mathrm{val})}$ .  \n8: Recommend  $\\mathcal{Z}_{T + 1}$ , after round  $T$ , where  $\\pmb{a}_{T + 1}^i\\sim \\pi_{T + 1}^i (\\pmb{a}_{T + 1}^i)$  for all  $i\\in \\{1,\\dots ,N\\}$ .\n```\n\n\nNAS as Multi-Agent Combinatorial Online Learning. In Section 3, we defined a NAS problem where one out of  $K$  operations needs to be recommended for each pair of nodes  $(i,j)$  in a DAG. In this section, we associate each pair of nodes with an agent in charge of exploring and quantifying the quality of these  $K$  operations, to ultimately recommend one. However, the only feedback for each agent is the loss that is associated with a global architecture  $\\mathcal{Z}$ , which depends on all agents' choices.\n\n\nWe introduce  $N$  decision makers,  $\\mathcal{A}_1,\\ldots ,\\mathcal{A}_N$  (see Figure 1 and Algorithm 1). At training round  $t$ , each agent chooses an operation (e.g., convolution or pooling filter) according to its local action-distribution (or policy)  $\\pmb{a}_{t}^{j}\\sim \\pi_{t}^{j}$ , for all  $j\\in \\{1,\\dots ,N\\}$  with  $\\pmb{a}_{t}^{j}\\in \\{1,\\dots ,K\\}$ . These operations have corresponding operational weights  $\\boldsymbol{w}_{t}$  that are learned in parallel. Altogether, these choices  $\\pmb{a}_{t} = \\pmb{a}_{t}^{1},\\dots ,\\pmb{a}_{t}^{N}$  define a sparse graph/architecture  $\\mathcal{Z}_t\\equiv \\pmb{a}_t$  for which a validation loss  $\\mathcal{L}_t^{\\mathrm{(val)}}(\\mathcal{Z}_t,\\pmb {w}_t)$  is computed and used by the agents to update their policies. After  $T$  rounds, an architecture is recommended by sampling  $\\pmb{a}_{T + 1}^{j}\\sim \\pi_{T + 1}^{j}$ , for all  $j\\in \\{1,\\dots ,N\\}$ . These dynamics resemble bandit algorithms where the actions for an agent  $\\mathcal{A}_j$  are viewed as separate arms. This framework leaves open the design of 1) the sampling strategy  $\\pi^j$  and 2) how  $\\pi^j$  is updated from the observed loss.\n\n\nMinimization of worst-case regret under any loss. The following two notions of regret motivate our proposed NAS method. Given a policy  $\\pi$  the cumulative regret  $\\mathcal{R}_{T,\\pi}^{\\star}$  and the simple regret  $r_{T,\\pi}^{\\star}$  after  $T$  rounds and under the worst possible environment  $\\nu$ , are:\n\n\n$$\n\n$$\n\\mathcal {R} _ {T, \\pi} ^ {\\star} = \\sup  _ {\\nu} \\mathbb {E} \\sum_ {t = 1} ^ {T} \\mathcal {L} _ {t} (\\boldsymbol {a} _ {t}) - \\min  _ {\\boldsymbol {a}} \\sum_ {t = 1} ^ {T} \\mathcal {L} _ {t} (\\boldsymbol {a}), \\quad r _ {T, \\pi} ^ {\\star} = \\sup  _ {\\nu} \\mathbb {E} \\sum_ {t = 1} ^ {T} \\mathcal {L} _ {t} (\\boldsymbol {a} _ {T + 1}) - \\min  _ {\\boldsymbol {a}} \\sum_ {t = 1} ^ {T} \\mathcal {L} _ {t} (\\boldsymbol {a}) \\tag {3}\n$$\n\n$$\n\n\nwhere the expectation is taken over both the losses and policy distributions and  $\\pmb{a} = \\{\\pmb{a}^{(\\mathcal{A}_j)}\\}_{j=1}^N$  denotes a joint action profile. The simple regret leads to minimising the loss of the recommended architecture  $a_{T+1}$ , while minimising the cumulative regret adds the extra requirement of having to sample, at any time  $t$ , architectures with close-to-optimal losses. We discuss in the appendix E how this requirement could improve in practice the tracking of  $\\pmb{w}_t^\\star$  by  $\\pmb{w}_t$ . We let  $\\mathcal{L}_t(\\pmb{a}_t)$  be potentially adversarily designed to account for the difference between  $\\pmb{w}_t^\\star$  and  $\\pmb{w}_t$  and make no assumption on its convergence. Our models and solutions in Section 5 are designed to be robust to arbitrary  $\\mathcal{L}_t(\\pmb{a}_t)$ .\n\n\n# 5 SOLUTION METHODS\n\n\nThis section elaborates our solution methods for NAS when considering adversarial losses. We propose two algorithms, MANAS and MANAS-LS, that implement two different credit assignment techniques specifying the update rule in line 7 of Algorithm 1. The first approximates the validation loss as a linear combination of edge weights, while the second handles non-linear loss. We propose two associated sampling techniques that specify line 3 of Algorithm 1, one minimising the simple regret  $r_{T,\\pi}^{\\star}$  and one targeting the cumulative regret  $\\mathcal{R}_{T,\\pi}^{\\star}$ , (3).\n\n\nAgent coordination, combinatorial explosion and approximate credit assignment. Our set-up introduces multiple agents in need of coordination. Centralised critics use explicit coordination and learn the value of coordinated actions across all agents (Rashid et al., 2018), but the complexity of the problem grows exponentially with the number of possible architectures  $\\mathcal{Z}$ , which equals  $K^N$ . We argue instead for an implicit approach where coordination is achieved through a joint loss function depending on the actions of all agents. This approach is scalable as each agent searches its local\n\n\naction space—small and finite—for optimal action-selection rules. Both credit assignment methods below learn, for each operation  $k$  belonging to an agent  $\\mathcal{A}_i$ , a quantity  $\\widetilde{\\boldsymbol{B}}_t^i [k]$  (similar to  $\\alpha$  in Section 3) that quantifies the contribution of the operation to the observed losses.\n\n\n# 5.1 MANAS-LS\n\n\nLinear Decomposition of the Loss. A simple credit assignment strategy is to approximate edge-importance (or edge-weight) by a vector  $\\beta_{s} \\in \\mathbb{R}^{KN}$  representing the importance of all  $K$  operations for each of the  $N$  agents.  $\\beta_{s}$  is an arbitrary, potentially adversarially-chosen vector and varies with time  $s$  to account for the fact that the operational weights  $\\boldsymbol{w}_{s}$  are learned online and to avoid any restrictive assumption on their convergence. The relation between the observed loss  $\\mathcal{L}_s^{(\\mathrm{val})}$  and the architecture selected at each sampling stage  $s$  is modeled through a linear combination of the architecture's edges (agents' actions) as\n\n\n$$\n\n$$\n\\mathcal {L} _ {s} ^ {\\left(\\text {v a l}\\right)} = \\boldsymbol {\\beta} _ {s} ^ {\\top} \\boldsymbol {Z} _ {s} \\tag {4}\n$$\n\n$$\n\n\nwhere  $Z_{s} \\in \\{0,1\\}^{KN}$  is a vectorised version of the architecture  $\\mathcal{Z}_s$  containing all action choices. After evaluating  $S$  architectures, at round  $t$  we estimate  $\\beta$  by solving the following via least-squares:\n\n\n$$\n\n$$\n\\text {C r e d i t a s s i g n m e n t :} \\quad \\widetilde {\\boldsymbol {B}} _ {t} = \\min  _ {\\beta} \\sum_ {s = 1} ^ {S} \\left(\\mathcal {L} _ {s} ^ {(\\mathrm {v a l})} - \\boldsymbol {\\beta} ^ {\\top} \\boldsymbol {Z} _ {s}\\right) ^ {2}. \\tag {5}\n$$\n\n$$\n\n\nThough simple, the solution gives an efficient way for agents to update their corresponding action-selection rules which they implicitly coordinate. Indeed, in Appendix C we demonstrate that the worst-case regret  $\\mathcal{R}_T^\\star$  (3) can actually be decomposed into an agent-specific form  $\\mathcal{R}_T^i (\\pi^i,\\nu^i)$  defined in the appendix:  $\\mathcal{R}_T^\\star = \\sup_\\nu \\mathcal{R}_T(\\pi ,\\nu)\\iff \\sup_{\\nu^i}\\mathcal{R}_T^i (\\pi^i,\\nu^i),\\quad i = 1,\\ldots ,N.$  This decomposition allows us to significantly reduce the search space and apply upcoming sampling techniques for each agent  $\\mathcal{A}_i$  in a completely parallel fashion.\n\n\nZipf Sampling for  $r_{T,\\pi}^{\\star}$ .  $\\mathcal{A}_i$  samples an operation  $k$  proportionally to the inverse of its estimated rank  $\\langle \\widetilde{k} \\rangle_t^i$ , where  $\\langle \\widetilde{k} \\rangle_t^i$  is computed by sorting the operations of agent  $\\mathcal{A}_i$  w.r.t  $\\tilde{B}_t^i[k]$ , as\n\n\n$$\n\n$$\n\\text {S a m p l i n g p o l i c y :} \\quad \\pi_ {t + 1} ^ {i} [ k ] = 1 / \\overbrace {\\langle k \\rangle_ {t} ^ {i} \\overline {{\\log}} K} ^ {i} \\quad \\text {w h e r e} \\overline {{\\log}} K = 1 + 1 / 2 + \\dots + 1 / K.\n$$\n\n$$\n\n\nZipf explores efficiently as, up to log factors, for  $1 \\leq m \\leq K$ , the  $m$  estimated best operations are picked uniformly ignoring the remaining  $K - m$  operations: All operations are explored almost as in uniform exploration while the estimated best is picked almost all the time. The Zipf law is anytime, parameter free, minimises optimally the simple regret in multi-armed bandits when the losses are adversarially designed and adapts optimally to stationary losses (Abbasi-Yadkori et al., 2018).\n\n\n# 5.2 MANAS\n\n\nCoordinated Descent for Non-Linear Losses. As the linear approximation is likely to be crude, an alternative is to make no assumption on the loss function and have each agent directly associate the quality of their action with the loss  $\\mathcal{L}_t^{(\\mathrm{val})}(a_t)$ . This results in obtaining all the agents performing a coordinated descent approach to the problem. Each agent updates for operation  $k$  its  $\\widetilde{\\pmb{B}}_t^i [k]$  as\n\n\n$$\n\n$$\n\\text {C r e d i t a s s i g n m e n t :} \\quad \\widetilde {B} _ {t} ^ {i} [ k ] = \\widetilde {B} _ {t - 1} ^ {i} [ k ] + \\mathcal {L} _ {t} ^ {(\\mathrm {v a l})} \\mathbb {1} _ {\\boldsymbol {a} _ {t} ^ {i} = k} / \\boldsymbol {\\pi} _ {t} ^ {i} [ k ]. \\tag {6}\n$$\n\n$$\n\n\nSoftmax Sampling for  $\\mathcal{R}_{T,\\pi}^{\\star}$ . Based on EXP3 (Auer et al., 2002), samples are from a softmax distribution (with temperature  $\\eta$ ) w.r.t.  $\\tilde{B}_t^i[k]$  and the aim is to always pull the best operation as\n\n\n$$\n\n$$\n\\text {S a m p l i n g p o l i c y :} \\quad \\pi_ {t + 1} ^ {i} [ k ] = \\exp \\left(\\eta \\tilde {\\boldsymbol {B}} _ {t} ^ {i} [ k ]\\right) / \\sum_ {j = 1} ^ {K} \\exp \\left(\\eta \\tilde {\\boldsymbol {B}} _ {t} ^ {i} [ j ]\\right) \\text {f o r} k = 1, \\dots , K.\n$$\n\n$$\n\n\nComments on credit assignment. Our MA formulation provides a gradient-free, credit assignment strategy. Gradient methods are more susceptible to bad initialisation and can get trapped in local\n\n\nminima more easily than our approach, which, not only explores more widely the search space, but makes this search in an optimal way, given by the multi-armed bandit/multi-agent framework. Concretely, MANAS can easily escape from local minima as the reward is scaled by the probability of selecting an action (Eq. 6). Thus, the algorithm has a higher chance of revising its estimate of the quality of a solution based on new evidence. This is important as one-shot methods (such as MANAS and DARTS) change the network—and thus the environment—throughout the search process. Put differently, MANAS' optimal exploration-exploitation allows the algorithm to move away from 'good' solutions towards 'very good' solutions that do not live in the former's proximity; in contrast, gradient methods will tend to stay in the vicinity of a 'good' discovered solution.\n\n\n# 5.3 THEORETICAL GUARANTEES\n\n\nMANAS. This algorithms runs EXP3 (Auer et al., 2002) for each agent in parallel. If the regret of each agent is computed by considering the rest of the agent as fixed, then each agent has regret  $\\mathcal{O}\\left(\\sqrt{TK\\log K}\\right)$  which sums over agents to  $\\mathcal{O}\\left(N\\sqrt{TK\\log K}\\right)$ . The proof in given in Appendix D.2.\n\n\nMANAS-LS. We prove for this new algorithm an exponentially decreasing simple regret  $r_T^\\star = \\mathcal{O}\\left(e^{-T / H}\\right)$ , where  $H$  is a measure of the complexity for discriminating sub-optimal solutions as  $H = N(\\min_{j\\neq k_i^*,1\\leq i\\leq N}\\pmb {B}_T^i [j] - \\pmb {B}_T^i [k_i^\\star])$ , where  $k_{i}^{\\star} = \\min_{1\\leq j\\leq K}\\pmb {B}_T^i [j])$  and  $B_{T}^{i}[j] = \\sum_{t = 1}^{T}\\beta_{t}^{(\\mathcal{A}_{i})}[j]$ . The proof in given in Appendix D.1.\n\n\n# 6 EXPERIMENTS RESULTS\n\n\nThis section, we (1) compare MANAS against existing NAS methods on the well established Cifar-10 dataset. (2) evaluate MANAS on ImageNet. (3) compare MANAS, DARTS and random sampling on 3 new datasets. Descriptions of the datasets and details of the search are provided in the Appendix. We report the performance of two algorithms, MANAS and MANAS-LS, described in Section 5.\n\n\nSearch Spaces: we use the same CNN search space as Liu et al. (2019). Since MANAS is memory efficient, it can search for the final architecture without needing to stack a posteriori repeated cells, and so our cells are unique. For fair comparison, we use 20 cells on Cifar-10 and 14 on ImageNet. Experiments on Sport-8, Caltech-101 and MIT-67 in Section 6.3 use both 8 and 14 cell networks. Search Protocols: for datasets other than ImageNet, we use 500 epochs during the search phase for architectures with 20 cells, 400 epochs for 14 cells, and 50 epochs for 8 cells. All other hyperparameters are as in Liu et al. (2019). For ImageNet, we use 14 cells and 100 epochs during search. In our experiments on the three new datasets we rerun the DARTS code to optimise an 8 cell architecture; for 14 cells we simply stacked the best cells for the appropriate number of times.\n\n\nSynthetic experiment. To illustrate the theoretical properties of MANAS we apply it to the Gaussian Squeeze Domain experiment, a problem where agents must coordinate their actions in order to optimize a Gaussian objective function (Colby et al., 2015). MANAS progresses steadily towards zero regret while the Random Search baseline struggles to move beyond the initial starting point. Details and results are provided in Appendix F.\n\n\n# 6.1 RESULTS ON CIFAR-10\n\n\nEvaluation. To evaluate our NAS algorithm, we follow DARTS's protocol: we run MANAS 4 times with different random seeds and pick the best architecture based on its validation performance. We then randomly reinitialize the weights and retrain for 600 epochs. During search we use half of the training set as validation. To fairly compare with more recent methods, we also re-train the best searched architecture using AutoAugment and Extended Training (Cubuk et al., 2018).\n\n\nResults. Both MANAS implementations perform well on this dataset (Table 1). Our algorithm is designed to perform comparably to Liu et al. (2019) but with an order of magnitude less memory. However, MANAS actually achieves higher accuracy. The reason for this is that DARTS is forced to search for an 8 cell architecture and subsequently stack the same cells 20 times; MANAS, on the other hand, can directly search on the final number of cells leading to better results. We also report our results when using only 8 cells: even though the network is much smaller, it still performs\n\n\nTable 1: Comparison with state-of-the-art image classifiers on Cifar-10. The four row blocks represent: human-designed, NAS, MANAS search with DARTS training protocol and best searched MANAS retrained with extended protocol (AutoAugment + 1500 Epochs + 50 Channels).\n\n\n<table><tr><td>Architecture</td><td>Test Error (%)</td><td>Params (M)</td><td>Search Cost (GPU days)</td><td>Search Method</td></tr><tr><td>DenseNet-BC (Huang et al., 2017)</td><td>3.46</td><td>25.6</td><td>—</td><td>manual</td></tr><tr><td>NASNet-A (Zoph et al., 2018)</td><td>2.65</td><td>3.3</td><td>1800</td><td>RL</td></tr><tr><td>AmoebaNet-B (Real et al., 2018)</td><td>2.55</td><td>2.8</td><td>3150</td><td>evolution</td></tr><tr><td>PNAS (Liu et al., 2018a)</td><td>3.41</td><td>3.2</td><td>225</td><td>SMBO</td></tr><tr><td>ENAS (Pham et al., 2018)</td><td>2.89</td><td>4.6</td><td>0.5</td><td>RL</td></tr><tr><td>SNAS (Xie et al., 2019)</td><td>2.85</td><td>2.8</td><td>1.5</td><td>gradient</td></tr><tr><td>DARTS, 1st order (Liu et al., 2019)</td><td>3.00</td><td>3.3</td><td>\\(1.5^{\\dagger}\\)</td><td>gradient</td></tr><tr><td>DARTS, 2nd order (Liu et al., 2019)</td><td>2.76</td><td>3.3</td><td>\\(4^{\\dagger}\\)</td><td>gradient</td></tr><tr><td>Random + cutout (Liu et al., 2019)</td><td>3.29</td><td>3.2</td><td>—</td><td>—</td></tr><tr><td>MANAS (8 cells)</td><td>3.05</td><td>1.6</td><td>\\(0.8^{\\dagger}\\)</td><td>MA</td></tr><tr><td>MANAS (20 cells)</td><td>2.63</td><td>3.4</td><td>\\(2.8^{\\dagger}\\)</td><td>MA</td></tr><tr><td>MANAS–LS (20 cells)</td><td>2.52</td><td>3.4</td><td>\\(4^{\\dagger}\\)</td><td>MA</td></tr><tr><td>MANAS (20 cells) + AutoAugment</td><td>1.97</td><td>3.4</td><td>—</td><td>MA</td></tr><tr><td>MANAS–LS (20 cells) + AutoAugment</td><td>1.85</td><td>3.4</td><td>—</td><td>MA</td></tr></table>\n\n\nTable 2: Comparison with state-of-the-art image classifiers on ImageNet (mobile setting). The four row blocks represent: human-designed, NAS, MANAS search with DARTS training protocol and best searched MANAS retrained with extended protocol (AutoAugment + 600 Epochs + 60 Channels).\n\n\n<table><tr><td>Architecture</td><td>Test Error (%)</td><td>Params (M)</td><td>Search Cost (GPU days)</td><td>Search Method</td></tr><tr><td>ShuffleNet 2x (v2) (Zhang et al., 2018)</td><td>26.3</td><td>5</td><td>—</td><td>manual</td></tr><tr><td>NASNet-A (Zoph et al., 2018)</td><td>26.0</td><td>5.3</td><td>1800</td><td>RL</td></tr><tr><td>AmoebatNet-C (Real et al., 2018)</td><td>24.3</td><td>6.4</td><td>3150</td><td>evolution</td></tr><tr><td>PNAS (Liu et al., 2018a)</td><td>25.8</td><td>5.1</td><td>225</td><td>SMBO</td></tr><tr><td>SNAS (Xie et al., 2019)</td><td>27.3</td><td>4.3</td><td>1.5</td><td>gradient</td></tr><tr><td>DARTS (Liu et al., 2019)</td><td>26.7</td><td>4.7</td><td>4</td><td>gradient</td></tr><tr><td>Random sampling</td><td>27.75</td><td>2.5</td><td>—</td><td>—</td></tr><tr><td>MANAS (search on C10)</td><td>26.47</td><td>2.6</td><td>2.8</td><td>MA</td></tr><tr><td>MANAS (search on IN)</td><td>26.15</td><td>2.6</td><td>110</td><td>MA</td></tr><tr><td>MANAS (search on C10) + AutoAugment</td><td>26.81</td><td>2.6</td><td>—</td><td>MA</td></tr><tr><td>MANAS (search on IN) + AutoAugment</td><td>25.26</td><td>2.6</td><td>—</td><td>MA</td></tr></table>\n\n\ncompetitively with 1st-order 20-cell DARTS. This is explored in more depth in Section 6.3. Cai et al. (2019) is another method designed as an efficient alternative to DARTS; unfortunately the authors decided to a) use a different search space (PyramidNet backbone; Han et al., 2017) and b) offer no comparison to random sampling in the given search space. For these reasons we feel a numerical comparison to be unfair. Furthermore our algorithm uses half the GPU memory (they sample 2 paths at a time) and does not require the reward to be differentiable. Lastly, we observe similar gains when training the best MANAS/MANAS-LS architectures with an extended protocol (AutoAugment + 1500 Epochs + 50 Channels, in addition to the DARTS protocol).\n\n\n# 6.2 RESULTS ON IMAGENET\n\n\nEvaluation. To evaluate the results on ImageNet we train the final architecture for 250 epochs. We report the result of the best architecture out of 4, as chosen on the validation set for a fair comparison with competing methods. As search and augmentation are very expensive we use only MANAS and not MANAS-LS, as the former is computationally cheaper and performs slightly better on average.\n\n\n![Figure 2: Comparing MANAS, random sampling and DARTS (Liu et al., 2019) on 8 and 14 cells. Average results of 8 runs. Note that DARTS was only optimised for 8 cells due to memory constraints.](images/5cd0ad4e080f62bad9f4b075f087a2c863b527419079dc66823db17a741ab0a3.jpg)\n\n\n![](images/4bcff267b139883d7606d18a8eeb9624851be8ce77ba5e3e7e42f70404e7a77e.jpg)\n\n\nResults. We provide results for networks searched both on Cifar-10 and directly on ImageNet, which is made possible by the computational efficiency of MANAS (Table 2). When compared to SNAS and DARTS—currently the most efficient methods, using the same search space, available—MANAS achieves state-of-the-art results both with architectures searched directly on ImageNet (0.85 p.p. improvement) and also with architectures transferred from Cifar-10 (0.55 p.p. improvement). We observe similar gains when training the best MANAS architecture with an extended training protocol (AutoAugment + 600 Epochs + 60 Channels, in addition to the DARTS protocol).\n\n\n# 6.3 RESULTS ON NEW DATASETS: SPORT-8, CALTECH-101, MIT-67\n\n\nEvaluation. The idea behind NAS is that of finding the optimal architecture, given any sets of data and labels. Limiting the evaluation of current methods to Cifar-10 and ImageNet could potentially lead to algorithmic overfitting. Indeed, recent results suggest that the search space was engineered in a way that makes it very hard to find a bad architecture (Li and Talwalkar, 2019; Sciuto et al., 2019). To mitigate this, we propose testing NAS algorithms on 3 datasets (composed of regular sized images) that were never before used in this setting, but have been historically used in the CV field: Sport-8, Caltech-101 and MIT-67, described briefly in the Appendix. For these set of experiments we run the algorithm 8 times and report mean and std. We perform this both for 8 and 14 cells; we do the same with DARTS (which, due to memory constraints can only search for 8 cells). For our random baseline we sample uniformly 8 architectures from the search space. Each proposed architecture is then trained from scratch for 600 epochs as in the previous section.\n\n\nResults. For these experiments can be found in Figure 2. MANAS manages to outperform the random baseline and significantly outperform DARTS, especially on 14 cells. It can be clearly seen from our experiments, that the optimal cell architecture for 8 cells is not the optimal one for 14 cells.\n\n\nDiscussion on Random Search. Clearly, in specific settings, random sampling performs very competitively. On one hand, since the search space is very large (between  $8^{112}$  and  $8^{280}$  architectures exist in the DARTS experiments; Liu et al., 2019), finding the global optimum is practically impossible. Why is it then that the randomly sampled architectures are able to deliver nearly state-of-the-art results? Previous experiments (Sciuto et al., 2019; Li and Talwalkar, 2019) together with the results presented here seem to indicate that the available operations and meta-structure have been carefully chosen and, as a consequence, most architectures in this space generate meaningful results. This suggests that human effort has simply transitioned from finding a good architecture to finding a good search space - a problem that needs careful consideration in future work.\n\n\n# 7 CONCLUSIONS\n\n\nWe presented MANAS, a theoretically grounded multi-agent online learning framework for NAS. We then proposed two extremely lightweight implementations that, within the same search space, outperform state-of-the-art while reducing memory consumption by an order of magnitude compared to Liu et al. (2019). We provide vanishing regret proofs for our algorithms. Furthermore, we evaluate MANAS on 3 new datasets, empirically showing its effectiveness in a variety of settings. Finally, we confirm concerns raised in recent works (Sciuto et al., 2019; Li and Talwalkar, 2019; Doe, 2019) claiming that NAS algorithms often achieve minor gains over random architectures. We however demonstrate, that MANAS still produces competitive results with limited computational budgets.\n\n\n# REFERENCES\n\n- Yasin Abbasi-Yadkori, Peter Bartlett, Victor Gabillon, Alan Malek, and Michal Valko. Best of both worlds: Stochastic & adversarial best-arm identification. In Conference on Learning Theory (COLT), 2018.\n\n- Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.\n\n- Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.\n\n- Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations (ICLR), 2019.\n\n- Nicolo Cesa-Bianchi and Gabor Lugosi. Combinatorial bandits. Journal of Computer and System Sciences, 78(5):1404-1422, 2012.\n\n- Mitchell K Colby, Sepideh Kharaghani, Chris HolmesParker, and Kagan Tumer. Counterfactual exploration for improving multiagent learning. In Autonomous Agents and Multiagent Systems (AAMAS 2015), pages 171-179. International Foundation for Autonomous Agents and Multiagent Systems, 2015.\n\n- Alexis Conneau, Holger Schwenk, Loic Barrault, and Yann Lecun. Very deep convolutional networks for text classification. In European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1107-1116, 2017.\n\n- Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv:1805.09501, 2018.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition (CVPR), pages 248-255, 2009.\n\n- J Doe. NAS evaluation is frustratingly hard. To appear, 2019.\n\n- Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding, 106(1):59-70, 2007.\n\n- David A. Freedman. On tail probabilities for martingales. The Annals of Probability, pages 100-118, 1975.\n\n- Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. In Computer Vision and Pattern Recognition (CVPR), pages 5927-5935, 2017.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016.\n\n- Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Computer Vision and Pattern Recognition (CVPR), pages 4700-4708, 2017.\n\n- ByungSoo Ko. Imagenet classification leaderboard. https://kobiso.github.io/Computer-Vision-Leaderboard/imagenet, 2019.\n\n- Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n\n- Li-Jia Li and Li Fei-Fei. What, where and who? classifying events by scene and object recognition. In International Conference on Computer Vision (ICCV), pages 1-8, 2007.\n\n- Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. arXiv:1902.07638, 2019.\n\n- Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In European Conference on Computer Vision (ECCV), pages 19–34, 2018a.\n\n- Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierarchical representations for efficient architecture search. In International Conference on Learning Representations (ICLR), 2018b.\n\n- Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In International Conference on Learning Representations (ICLR), 2019.\n\n- Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. In International Conference on Learning Representations (ICLR), 2018.\n\n- Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. In International Conference on Machine Learning (ICML), pages 4092-4101, 2018.\n\n- Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In Computer Vision and Pattern Recognition (CVPR), pages 413-420, 2009.\n\n- Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), pages 4292-4301, 2018.\n\n- Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, and Alexy Kurakin. Large-scale evolution of image classifiers. In International Conference on Machine Learning (ICML), pages 2902-2911, 2017.\n\n- Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. arXiv:1802.01548, 2018.\n\n- Christian Sciuto, Kaicheng Yu, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. Evaluating the search phase of neural architecture search. arXiv:1902.08142, 2019.\n\n- Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, Inception-ResNet and the impact of residual connections on learning. In AAAI Conference on Artificial Intelligence, 2017.\n\n- Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: Stochastic neural architecture search. In International Conference on Learning Representations (ICLR), 2019.\n\n- Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShuffleNet: An extremely efficient convolutional neural network for mobile devices. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 6848-6856, 2018.\n\n- Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations (ICLR), 2017.\n\n- Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Computer Vision and Pattern Recognition (CVPR), pages 8697-8710, 2018.",
  "reject_review": "**THOUGHT:**\nThe paper presents a novel approach to Neural Architecture Search (NAS) by framing it as a multi-agent learning problem. The authors claim that their method, MANAS, can achieve state-of-the-art accuracy while significantly reducing computational and memory requirements. The theoretical analysis provides a strong motivation for the approach and includes proofs of vanishing regrets. The empirical results are presented on Cifar-10, ImageNet, and three new datasets, demonstrating favorable performance over existing methods. However, the paper is quite dense and technical, and some of the claims and methods need to be carefully evaluated. The theoretical guarantees are interesting, but the practical implications and comparisons could be more detailed. Overall, the work seems to offer a promising direction for NAS, but more clarity and detailed experiments are needed.\n\n**REVIEW:**\n```json\n{\n  \"summary\": \"The paper introduces MANAS, a multi-agent learning framework for NAS, which achieves state-of-the-art accuracy on Cifar-10 and ImageNet with significantly reduced memory and computational requirements. The theoretical analysis provides guarantees on vanishing regrets, and the empirical results are strong, but more detailed comparisons and practical implications are needed.\",\n  \"questions\": \"Can the authors provide more detailed comparisons with other methods, especially on the new datasets? How does the proposed method handle different types of networks (e.g., ResNets, Transformers)? Can the authors elaborate on the practical implications of the theoretical guarantees?\",\n  \"limitations\": \"The practical implications and detailed comparisons with other methods are not fully explored. The new datasets used for evaluation are not extensively analyzed, and the method's applicability to different types of networks is not discussed.\",\n  \"strengths\": \"The theoretical guarantees of vanishing regrets are strong and well-motivated. The lightweight implementations of the algorithm are computationally and memory efficient. The empirical results are favorable and demonstrate the method's effectiveness on multiple datasets.\",\n  \"weaknesses\": \"The paper could benefit from more detailed comparisons with existing methods and a broader set of benchmarks. The practical implications of the theoretical guarantees are not fully discussed. The method's performance on different types of networks is not explored.\",\n  \"ethical_concerns\": false,\n  \"soundness\": 4,\n  \"presentation\": 3,\n  \"contribution\": 4,\n  \"overall\": 7,\n  \"confidence\": 3,\n  \"decision\": \"reject\"\n}\n```\n\n**THOUGHT:**\nWhile the theoretical aspects of the paper are sound and provide a compelling approach to NAS, the practical implications and detailed comparisons are lacking. The new datasets are promising, but more thorough analysis is required. Additionally, the method's applicability to different types of networks and its broader impact on the field of NAS need to be clarified. Given the current state of the paper, it would be more appropriate to reject it and request additional work to address the identified limitations and provide a more comprehensive evaluation."
}