==============================================
TextGrad Prompt Optimization
Job ID: 2473594
Date: Sat Jan 17 02:27:44 AM EST 2026
==============================================
Inference Model: Qwen/Qwen2.5-3B-Instruct
Gradient Model: Qwen/Qwen3-8B
Main venv: /n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/.venv_vllm_inf
TextGrad venv: /n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv
==============================================
Starting inference server on port 8000...
Inference server PID: 3542309
Starting gradient server on port 8001...
Gradient server PID: 3542311
Waiting for servers to initialize (180s)...
[0;36m(APIServer pid=3542310)[0;0m INFO 01-17 02:28:01 [api_server.py:1772] vLLM API server version 0.12.0
[0;36m(APIServer pid=3542308)[0;0m INFO 01-17 02:28:01 [api_server.py:1772] vLLM API server version 0.12.0
[0;36m(APIServer pid=3542308)[0;0m INFO 01-17 02:28:01 [utils.py:253] non-default args: {'model': 'Qwen/Qwen2.5-3B-Instruct', 'trust_remote_code': True, 'max_model_len': 20480}
[0;36m(APIServer pid=3542310)[0;0m INFO 01-17 02:28:01 [utils.py:253] non-default args: {'port': 8001, 'model': 'Qwen/Qwen3-8B', 'trust_remote_code': True, 'max_model_len': 20480}
[0;36m(APIServer pid=3542308)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=3542310)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=3542308)[0;0m INFO 01-17 02:28:01 [model.py:637] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=3542308)[0;0m INFO 01-17 02:28:01 [model.py:1750] Using max model len 20480
[0;36m(APIServer pid=3542310)[0;0m INFO 01-17 02:28:01 [model.py:637] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=3542310)[0;0m INFO 01-17 02:28:01 [model.py:1750] Using max model len 20480
[0;36m(APIServer pid=3542310)[0;0m INFO 01-17 02:28:02 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=3542308)[0;0m INFO 01-17 02:28:02 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
