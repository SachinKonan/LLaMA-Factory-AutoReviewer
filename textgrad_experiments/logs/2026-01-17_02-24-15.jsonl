{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634676.9659245", "msecs": "965.0", "relativeCreated": "21789.398670196533", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# TAFS: Task-aware Activation Function Search for Graph Neural Networks\n\n# Abstract\nSince the inception of Graph Neural Networks (GNNs), extensive research efforts have concentrated on enhancing graph convolution, refining pooling operations, devising robust training strategies, and advancing theoretical foundations. Notably, one critical facet of current GNN research remains conspicuously underexplored\u2014the design of activation functions. Activation functions serve as pivotal components, imbuing GNNs with the essential capacity for non-linearity. Yet, the ubiquitous adoption of Rectified Linear Units (ReLU) persists. In our study, we embark on a mission to craft task-aware activation functions tailored for diverse GNN applications. We introduce TAFS (Task-aware Activation Function Search), an adept and efficient framework for activation function design. TAFS leverages a streamlined parameterization and frames the problem as a bi-level stochastic optimization challenge. To enhance the search for smooth activation functions, we incorporate additional Lipschitz regularization. Our approach automates the discovery of the optimal activation patterns, customizing them to suit any downstream task seamlessly. Crucially, this entire process unfolds end-to-end without imposing significant computational or memory overhead. Comprehensive experimentation underscores the efficacy of our method. We consistently achieve substantial improvements across a spectrum of tasks, including node classification over diverse graph data. Moreover, our approach surpasses state-of-the-art results in the realm of link-level tasks, particularly in biomedical applications.\n\n\n# 1 INTRODUCTION\n\n\nGraph Neural Networks (GNN) have demonstrated their prowess in modeling relationships within graph-structured data, as evidenced by their superior performance in various domains (Kipf & Welling, 2017; Velickovic et al., 2017; Hu et al., 2020; Xu et al., 2019). They have excelled in applications spanning biomedicine (Wu et al., 2023; Jiang et al., 2021), physical simulation (Sanchez-Gonzalez et al., 2020), material design (Reiser et al., 2022), sustainability (Donon et al., 2020), social network (Fan et al., 2019), transportation (Li et al., 2018b), recommendation (Wu et al., 2019), and more. Consequently, GNN models continue to captivate the attention of researchers across diverse scientific communities (Shi et al., 2020; Wang et al., 2022; Seo et al., 2020).\n\n\nDespite the extensive body of literature, we must highlight a significant gap in current research, specifically the design of activation functions, a fundamental component used in nearly every GNN model. While Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) is a prevalent choice for activation, it often falls short, as illustrated in Figure 1. Regrettably, GNN studies have hardly explored alternative activation functions. This oversight is critical, as the activation function plays a pivotal role in introducing non-linearity to GNNs. Without it, GNNs merely perform linear transformations on raw graph features.\n\n\nIn contrast, the Computer Vision community has spent decades exploring a wide array of manually\n\n\n![Figure 1: Activation function makes a big difference in GNN.](images/66c16489c3adf48e54dabed9ea1bf174d62a12679ee3776f2e4cfe1a7cc4106c.jpg)\n\n\ndesigned activation functions such as Sigmoid (LeCun et al., 1998), Tanh, ReLU (Nair & Hinton, 2010), and improved variants of ReLU (He et al., 2015; Clevert et al., 2016; Maas et al., 2013). However, transferring these manually crafted functions to different tasks poses challenges, and customizing new ones is a labor-intensive process. Furthermore, the marginal performance gains from human-designed functions diminish rapidly. To address this, researchers have proposed automated methods to discover tailored activation functions, which have demonstrated notable improvements in other network architectures like Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs) (Ramachandran et al., 2018; Eger et al., 2018; Farzad et al., 2019).\n\n\nHence, our research question is how can we design GNN activation functions to adapt effectively to various graph-based tasks, creating task-aware activation functions?\n\n\nAddressing this question poses two primary challenges. Challenge #1: Existing search algorithms are inefficient. Current activation function search methods suffer from over-parameterization and heavy computation. For example, APL (Agostinelli et al., 2015) introduces additional parameters for each neuron, which usually leads to at least ten times more parameters upon any model, thus significantly increasing the model complexity. Swish (Ramachandran et al., 2018) requires training a full network until convergence for each iteration, making it computationally burdensome. These issues render current search algorithms inefficient and less effective. Challenge #2: Current search methods lack support for non-differentiable objectives. GNN methods have wide applications regarding tasks of different levels (node, link, graph), many of which are evaluated by non-differentiable metrics. In the case of drug interaction prediction, we would like to know a certain positive (synergy) or negative (confliction) interaction exist. In fact, most drug pairs does not have positive or negative interactions. Receiver Operating Characteristic curve (ROC) which is not differentiable, is what we should use instead of accuracy. Similar application cases can be found in hit ratio of recommendation, latency optimization, hardware resources constraint, etc. Supporting these non-differentiable objectives would broaden the applicability of activation function search in diverse GNN tasks.\n\n\nIn this study, we embark on a systematic exploration of GNN activation function search\u2014a first of its kind. We frame this search as a bi-level optimization problem, with the inner level optimizing GNN parameters and the outer level optimizing activation function parameters. We propose an efficient search algorithm that navigates through a compact search space. This space is characterized by universal approximators with additional smoothness constraints, facilitating the rapid discovery of high-quality functions, thereby addressing Challenge #1. Additionally, we tackle Challenge #2 by jointly considering non-differentiable objectives and potential activation function constraints. We incorporate these elements into a stochastic relaxation of the outer level optimization, removing the need to compute gradients for non-differentiable metrics used in GNN tasks. Our algorithm undergoes extensive experimentation across various GNN models, datasets, and objectives, consistently outperforming existing activation functions. By overcoming Challenges #1 and #2, our algorithm achieves task-awareness in GNN activation function design.\n\n\nOur contributions can be summarized in three key points:\n\n- 1. To the best of our knowledge, we are the first to propose activation function search in the context of Graph Neural Networks. Our work serves as a catalyst, drawing attention to this critical aspect of GNN model design and paving the way for future investigations.\n\n- 2. We propose TAFS (Task-aware Activation Function Search), a probabilistic search algorithm capable of efficiently exploring a regularized functional space to discover novel activation functions tailored for diverse downstream tasks.\n\n- 3. Through comprehensive evaluations spanning node and link level tasks, we demonstrate that our algorithm enhances activation function design without requiring extensive manual effort and excels in optimizing non-differentiable objectives. We also conduct ablation studies to examine the searched activation functions, the impact of design choices, and algorithm efficiency.\n\n\n# 2 RELATED WORKS\n\n\nGraph Neural Networks. GNN is a power model in capturing relational information including typically GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2017), GIN (Xu et al., 2019), etc. Mathematically, in the context of GNN with a given network  $\\mathcal{G} = \\{\\mathcal{V},\\mathcal{E}\\}$  containing node set  $\\mathcal{V}$  and edge set  $\\mathcal{E}$ , the problem is formulated below:\n\n\n$$\n\n$$\nz _ {u} ^ {(l + 1)} = \\operatorname {U P D A T E} ^ {(l + 1)} \\left(h _ {u} ^ {(l)}, \\operatorname {A G G R E G A T E} ^ {(l)} \\left(\\left\\{h _ {u} ^ {(l)}, \\forall v \\in \\mathcal {N} (u) \\right\\}\\right)\\right), \\tag {1}\n$$\n\n$$\n\n\nwhere  $h_u^{(l)}$  is the latent representation of node  $u$  at layer  $l$  and  $z_u$  is the pre-activation of node  $u$ . AGGREGATE (abbr. Agg) and UPDATE (abbr. Up) are core modules of GNN, denoting the different message passing operations used across the model for collecting and updating representations. The latent representation (respectively pre-activation) for all nodes constitute  $\\mathbf{H}$  (resp.  $\\mathbf{Z}$ ) and then we have the activation transformation:  $\\mathbf{H}^{(l + 1)} = \\sigma^{(l + 1)}(\\mathbf{Z}^{(l + 1)})$ , where  $\\mathbf{Z}$  is activated by function  $\\sigma$  at the  $(l + 1)$ th layer.\n\n\nNumerous studies have been devoted to ameliorate it through different aspects. For example, GCN (Kipf & Welling, 2017) simplifies graph convolution and derives performant GCN networks. GAT (Velickovic et al., 2017) proposes graph attention as a replacement to model global features. GNN-Pretrain (Hu et al., 2020) studies pretraining strategies at the node level and graph level to make GNN model work for transferable tasks. GIN (Xu et al., 2019) understands the fundamental question of graph expressiveness by discriminating Weisfeiler-Lehman graph isomorphism. GNN Co-training (Li et al., 2018a) connects Laplacian smoothing with graph convolution and studies the problem of oversmoothing. However, almost every GNN model uses ReLU as the activation function (Kipf & Welling, 2017; Velickovic et al., 2017; Xu et al., 2018; Huang et al., 2020; KC et al., 2022; Xu et al., 2019), leaving GNN activation function a missing research piece.\n\n\nActivation Function Design. Since the early application of Sigmoid in Le-Net, activation functions have been considered as an important component until today (Hayou et al., 2019). In 2012, ReLU (Nair & Hinton, 2010) was proposed to train Boltzman Machines and soon extensively adopted in every neural network models. The study of activation function design happens mostly in CNN community, where a couple of milestone works include Swish (Ramachandran et al., 2018; Eger et al., 2018) and APL (Agostinelli et al., 2015). Swish proposes a Reinforcement Learning (RL)-based search algorithm to find appropriate activation functions in a discrete space. APL (Adaptive Piecewise Linear) uses linear hinge functions to approximate target patterns in a differentiable way. Comprehensive surveys of manual designed and parametric activation functions can be found in (Apicella et al., 2021; Dubey et al., 2022). Another notable work related to our research question is GReLU (Zhang et al., 2022), which tries to make GNN activation function adaptive by including graph convolution into the activation function. However, such design is not a typical univariate activation function. As a result, no work yet has proposed novel activation functions designed under the context of GNN.\n\n\n# 3 PROBLEM FORMULATION AND CHALLENGES\n\n\nOur research problem requires to propose a systematic way of designing adaptive activation functions that can be effectively integrated into GNN for downstream applications. Similar to Neural Architecture Search (NAS) (Liu et al., 2019), the activation function design could be modeled into a bi-level optimization problem:\n\n\n$$\n\n$$\n\\min  _ {\\alpha} \\mathcal {M} \\left(w ^ {*} (\\alpha), \\alpha ; \\mathcal {D} _ {\\text {v a l}}\\right) \\quad \\text {s . t .} \\quad w ^ {*} (\\alpha) = \\arg \\min  _ {w} \\mathcal {L} (w, \\alpha ; \\mathcal {D} _ {\\text {t r a i n}}), \\tag {2}\n$$\n\n$$\n\n\nwhere the inner level optimization learns  $w$  weight of GNN and the outer level optimization learns  $\\alpha$  weights of activation function. Both levels may have different objective metrics  $\\mathcal{M}, \\mathcal{L}$  depending on downstream applications.\n\n\nPrevious activation function search methods suffer from low efficiency (Challenge #1) and poor support of non-differentiable metrics (Challenge #2). On one hand, the efficiency bottleneck lies in the search space choice and search strategy design. The search space is crucial for search efficiency and requires careful consideration. The space should be proper both in candidate function number and effectiveness, making it a trade-off between quantity and quality. Then, the search strategy should be able to discover as quickly as possible the most suitable function candidate in the space. On the other hand, diverse GNN applications require that the algorithm is able to tackle any downstream target metric, whether or not differentiable.\n\n\nAll these issues prevent us from using off-the-shelf algorithms for GNN activation function search. To this end, we need a novel parameterization of the search space, that is jointly designed with search strategy to allow efficient search, and we need to deal with differentiable and non-differentiable target metrics at the same time to enable general applications in all kinds of GNN tasks.\n\n\n![Figure 2: Algorithm framework. We replace the activation function with sampled weights from Gaussian distributions. The inner level optimization is the learning of distribution parameter  $\\theta^{(1)},\\ldots ,\\theta^{(L)}$  and the outer level optimization is the learning of GNN weights. Both levels are iterated for faster convergence.](images/82c6262858b1d88a3b812ade0f6dbea69a2a33caa85d9e70103e76290b62aedd.jpg)\n\n\n# 4 THE PROPOSED METHOD\n\n\n# 4.1 ALGORITHM FRAMEWORK\n\n\nAs illustrated in Figure 2 and Table 1, we propose TAFS to solve both challenges in a unified way. We follow the bi-level optimization formulation and search activation function represented by learnable parameters. Specifically, a typical GNN network of L layers can be represented as below:\n\n\n$$\n\n$$\n\\sigma^ {(L)} \\circ \\mathbf {G L} ^ {(L)} \\dots \\sigma^ {(2)} \\circ \\mathbf {G L} ^ {(2)} \\circ \\sigma^ {(1)} \\circ \\mathbf {G L} ^ {(1)} (\\mathbf {X}), \\tag {3}\n$$\n\n$$\n\n\nwhere Graph Layer (GL) denotes all the AGGREGATE (Agg) and UPDATE (Up) operations related to graph,  $\\mathbf{X}$  is the initial graph features. Note that different layers could use different activation functions whereas current GNN models tend to fix the same ReLU for every layer.\n\n\nDenote by  $w_{\\sigma}$  all the parameters of activation functions, i.e.  $\\sigma^{(1)}, \\ldots, \\sigma^{(L)}$ , and denote by  $\\overline{w}_{\\sigma}$  all the parameters of GNN, i.e., parameters of  $\\mathrm{GL}^{(1)}, \\ldots, \\mathrm{GL}^{(L)}$ . We propose a continuous implicit functional space to parameterize  $w_{\\sigma}$ . This search space is expressive yet compact, with smoothness regularization induced by human prior. The parameter update process is stochastic to deal with any downstream objective especially non-differentiable metrics. The search algorithm is bi-level and end-to-end trained. The optimization step of the outer level (learning  $w_{\\sigma}$ ) and the inner level optimization (learning  $\\overline{w}_{\\sigma}$ ) are iterated. In the following parts, we explain in sequence the design of search space, stochastic relaxation and search algorithm.\n\n\n# 4.2 IMPLICIT FUNCTIONAL SEARCH SPACE\n\n\nIn order to facilitate activation function search, we propose a continuous implicit functional space that parameterizes the search space by universal approximators. This implicit functional space could be implemented by Multi-Layer Perception (MLP) to approximate target function. As in Figure 2, activation function parameters is equivalent to the parameters of MLP, denoted by  $w_{\\sigma}$ .\n\n\nIt's worth noting that we employ MLP as a representative example of universal approximators, chosen for its simplicity, while retaining generality. However, it's crucial to emphasize that alternative implementations, such as Gaussian Mixtures or Radial Basis Functions (RBF), are entirely feasible.\n\n\nIn addition, we focus on the smooth functions such that the searched activation functions will not change dramatically if the pre-activation value  $Z$  is slightly perturbed. Smooth functions are bounded by Lipschitz constant  $c$ , i.e.  $|f(x) - f(y)| \\leq c|x - y|$ . As a result, the functional search space is\n\n\nregularized by smoothness constraint  $\\{w_{\\sigma}|w_{\\sigma}\\in R^{|w_{\\sigma}|},c < \\gamma \\}$ , where  $c$  is the Lipschitz constant of the function parameterized by  $w_{\\sigma}$  and  $\\gamma$  is the hard limit on  $c$ . Here, we model the constraint on Lipschitz constant as a regularization term denoted by  $R(w_{\\sigma})$ . Many references design Lipschitz constant as additional soft metric to be trained together with any loss (Hoffman et al., 2019; Weng et al., 2018; Liu et al., 2022). We use Jacobian regularization (Hoffman et al., 2019) without loss of generality  $R(w_{\\sigma}) = ||J(x)||_F = \\{\\Sigma_{i,j}[J_{i,j}(x)]^2\\}$ , where  $J_{i,j}(x) = \\frac{\\partial h_i}{\\partial x_j} (x)$  is the Jacobian matrix.\n\n\nThis design of our functional space encourages the discovery of smooth functions characterized by small Lipschitz constants. Notably, this characteristic aligns with existing manually designed functions, such as ReLU, Tanh, Sigmoid, and Swish, all of which exhibit 1-Lipschitz properties.\n\n\n# 4.3 STOCHASTIC RELAXATION\n\n\nGNN applications are diverse that in many cases, the preferred evaluation metrics may not be differentiable. As mentioned in Table 1 and related works, APL could not deal with non-differentiable metrics but Swish could due to the RL-based search algorithm. However, the search efficiency of Swish is far from satisfactory.\n\n\nWe propose to use stochastic relaxation that re-parameterizes the search space  $(w_{\\sigma})$  with a Gaussian distribution  $p_{\\theta_{\\sigma}}(w_{\\sigma})$ . The Gaussian distribution has its own parameters  $\\theta_{\\sigma}$  and we sample the parameters of activation function  $w_{\\sigma}$  from the probability  $p_{\\theta_{\\sigma}}(w_{\\sigma})$  and optimize the probability parameters  $\\theta_{\\sigma}$  instead of  $w_{\\sigma}$ .\n\n\nFollowing (4), we replace  $\\sigma$  by  $w_{\\sigma}$  to emphasize the parameters on activation function; we replace  $w$  by  $\\overline{w}_{\\sigma}$  to denote the rest parameters of GNN model. Task objective  $\\mathcal{M}$  is jointly integrated into stochastic relaxation with space regularization  $\\mathcal{R}$ . The ultimate problem is formulated as below:\n\n\n$$\n\n$$\n\\theta_ {\\sigma} ^ {*} = \\underset {\\theta_ {\\sigma}} {\\arg \\min } \\left\\{\\mathcal {J} \\left(\\theta_ {\\sigma}\\right) \\equiv \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\theta_ {\\sigma}} \\left(w _ {\\sigma}\\right)} \\left[ \\mathcal {M} \\left(w _ {\\sigma}, \\bar {w _ {\\sigma}} ^ {*}; \\mathcal {D} _ {\\text {v a l}}\\right) + \\eta R \\left(w _ {\\sigma}\\right) \\right] \\right\\}, \\tag {4}\n$$\n\n$$\n\n\n$$\n\n$$\n\\text {s . t .} \\quad \\overline {{w _ {\\sigma}}} ^ {*} = \\underset {\\overline {{w _ {\\sigma}}}} {\\arg \\min } \\mathcal {L} (\\overline {{w _ {\\sigma}}}, w _ {\\sigma}; \\mathcal {D} _ {\\text {t r a i n}}),\n$$\n\n$$\n\n\nwhere  $w_{\\sigma}$  denotes the parameters of activation functions  $\\sigma$  with a regularization term  $R$  weighted by  $\\eta$ ,  $\\overline{w_{\\sigma}}$  represents GNN parameters,  $\\mathcal{L}$  is downstream task criterion of interest,  $\\mathcal{M}$  is upstream task criterion of interest, probably non-differentiable,  $\\theta_{\\sigma}$  represents the re-parameterization of  $w_{\\sigma}$  through Gaussian distribution, then the whole learning problem is optimized in a stochastic way.\n\n\nTo compute the target loss gradient with respect to probability parameters  $\\nabla_{\\theta_{\\sigma}}\\mathcal{J}(\\theta_{\\sigma})$ , we have the following proposition. The proof is given in Appendix A.\n\n\nProposition 1 Let  $w_{\\sigma} \\sim p_{\\theta_{\\sigma}}(w_{\\sigma})$  represent that the weights of activation functions are sampled from  $p_{\\theta_{\\sigma}}$ . We have\n\n\n$$\n\n$$\n\\begin{array}{l} \\begin{array}{l} \\nabla_ {\\theta_ {\\sigma}} \\mathcal {J} (\\theta_ {\\sigma}) = \\nabla_ {\\theta_ {\\sigma}} \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\theta_ {\\sigma}} (w _ {\\sigma})} [ \\mathcal {M} (w _ {\\sigma}, \\bar {w} _ {\\sigma} ^ {*}; \\mathcal {D} _ {v a l}) + \\eta R (w _ {\\sigma}) ] \\\\ - \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\sigma}} \\left[ \\left[ \\mathcal {M} (w _ {\\sigma}, \\bar {w} _ {\\sigma} ^ {*}; \\mathcal {D} _ {v a l}) \\right] \\log p _ {\\sigma} (w _ {\\sigma}) \\right] \\end{array} \\tag {5} \\\\ = \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\theta_ {\\sigma}} (w _ {\\sigma})} \\left[ \\left[ \\mathcal {M} \\left(w _ {\\sigma}, \\overline {{w _ {\\sigma}}} ^ {*}; \\mathcal {D} _ {v a l}\\right) + \\eta R \\left(w _ {\\sigma}\\right) \\right] \\nabla_ {\\theta_ {\\sigma}} \\log p _ {\\theta_ {\\sigma}} \\left(w _ {\\sigma}\\right) \\right] \\\\ \\end{array}\n$$\n\n$$\n\n\nWith the help of stochastic relaxation, the previously needed derivation of  $\\mathcal{M}$  is replaced by a multiplication between forward pass of  $\\mathcal{M}$  and a gradient of probability loss. In practice, this gradient expectation could be further approximated by Monte Carlo samplings, i.e.  $\\nabla_{\\theta_{\\sigma}}\\mathcal{J}(\\theta_{\\sigma})\\approx \\sum_{i = 1}^{K}\\nabla_{\\theta_{\\sigma}}\\log p_{\\theta_{\\sigma}}(w_{\\sigma}^{i})[\\mathcal{M}(w_{\\sigma}^{i},\\overline{w_{\\sigma}}^{*};\\mathcal{D}_{\\mathrm{val}}) + \\eta R(w_{\\sigma}^{i})]$ ,  $K$  is the sample number that we use to approximate the gradient. As a result, the differentiability requirement of  $\\mathcal{M}$  is removed.\n\n\n# 4.4 SEARCH STRATEGY\n\n\nAccording to (4), the learning is divided in two levels. The outer level optimizes probability parameters  $\\theta_{\\sigma}$  on validation dataset with (non-differentiable) metric  $\\mathcal{M}$ . Every time  $K$  number of samples are generated from the probability distribution (such as Gaussian). Each sample is forwarded and calculated according to (Prop. 1), whose average is an approximation of outer level loss gradient. The optimization of outer level parameters  $\\theta_{\\sigma}$  influences directly the value of activation function weights since the weights are sample from the updated probability every time a forward pass is needed. The inner level optimizes GNN parameters  $\\overline{w_{\\sigma}}$  on training dataset with metric  $\\mathcal{L}$ . It is similar to a normal training epoch of any network. The outer and inner levels are interplayed to accelerate convergence.\n\n\n```\n1: Initialize  $\\theta^0 = 1$ , initialize  $\\overline{w}_{\\sigma}$  by Xavier initialization and  $w_{\\sigma}$  randomly sampled from  $p_{\\theta^0}(w_\\sigma)$ .  \n2: for  $m = 0, \\dots, M - 1$  do  \n3: // Outer level optimization  \n4: Freeze GNN paramaters  $\\overline{w}_{\\sigma}$ ;  \n5: for  $k = 0, \\dots, K - 1$  do  \n6: Sample activation functions weights  $w_{\\sigma}^{k}$  from  $p_{\\theta^m}(w_\\sigma)$ ;  \n7: Forward inference of the whole network and accumulate stochastic loss  $\\mathcal{J}(\\theta_{\\sigma})$  as in Prop 1;  \n8: end for  \n9: Obtain  $\\nabla_{\\theta_{\\sigma}} \\mathcal{J}(\\theta_{\\sigma})$  by automatic differentiation and update  $\\theta^m$ ;  \n10: // Inner level optimization  \n11: Sample activation function parameters  $w_{\\sigma}$  from distribution  $p_{\\theta^m}(w_\\sigma)$  and freeze  $w_{\\sigma}$ ;  \n12: Forward inference of the whole network to obtain loss  $\\mathcal{L}$ ;  \n13: Update GNN parameters  $\\overline{w}_{\\sigma}$  by automatic differentiation;  \n14: end for  \n15: Training until convergence and obtain the final model parameter  $\\overline{w}_{\\sigma}^*$  and dist. parameter  $\\theta_{\\sigma}$ ;  \n16: return Final model parameter  $\\overline{w}_{\\sigma}^*$  and distribution parameter  $\\theta_{\\sigma}$ .\n```\n\n\nTable 1: Our proposed TAFS (Task-aware Activation Function Search) enables efficient differentiable search through a flexible and powerful MLP functional space. TAFS supports non-differentiable objective metrics in diverse GNN applications.\n\n\n<table><tr><td rowspan=\"2\">Search Method</td><td colspan=\"2\">Search Efficiency</td><td rowspan=\"2\">Non-Differentiable Metric</td></tr><tr><td>Search Space</td><td>Search Strategy</td></tr><tr><td>Swish</td><td>Discrete template choice</td><td>Reinforcement Learning</td><td>Applicable</td></tr><tr><td>APL</td><td>Explicit piecewise linear</td><td>Differentiable</td><td>Not applicable</td></tr><tr><td>TAFS (ours)</td><td>Continuous implicit MLP</td><td>Differentiable</td><td>Applicable</td></tr></table>\n\n\nThe complete TAFS algorithm is given in Algorithm 1. We also compare in Table 1 our proposed TAFS and literature methods. From the time efficiency perspective, Swish is the slowest in Table 1 because it optimizes a new network until convergence before the learning of RL controller. APL on the other hand, has a number of parameters dependent of base models due to its adaptability per neuron. As a result, TAFS enjoys a compact search space without over-parameterization and has superior efficiency in searching. Empirical results are given in Table 4.\n\n\n# 5 EXPERIMENTS\n\n\nIn this section, we experiment our methods on diverse GNN applications including node classification and link prediction, in order to fully evaluation the methods on differentiable and non-differentiable metrics. Later, we provide detailed analysis on search efficiency and hyperparameter impact. All our experiments are run on single NVIDIA RTX 3090.\n\n\n# 5.1 NODE CLASSIFICATION\n\n\nDatasets. We experiment on diverse graph datasets for node tasks, including Cora and DBLP for paper classification based on reference network, Cornell and Texas for webpage classification from university network, and Chameleon for wikipedia page classification based on hyperlink network. Statistics are in Appx. E. The task metric here is classification accuracy.\n\n\nBaselines. To fairly compare different activation functions, we compare our searchable activation functions with manually designed ones or previously searched function. Some of these activation functions are visualized in Figure 3(a). For each dataset and baseline chosen, we evaluate on two aggregation layers (GCN and GraphSage) and five network connection topologies (stack, residual, dense, jump knowledge, mixhop). Each model has four layers of aggregation layers. The model is trained for 400 epochs.\n\n\nTable 2: Overall node classification improvement of different models on different datasets. Metric is classification accuracy. Avg. Imp. is the improvement of TAFS with respect to the other choices averaged over all the datasets.\n\n\n<table><tr><td colspan=\"2\">Model</td><td>Activation</td><td>Cora</td><td>DBLP</td><td>Cornell</td><td>Texas</td><td>Chameleon</td><td>Avg. Imp.</td></tr><tr><td rowspan=\"40\">GCN</td><td rowspan=\"5\">Stack</td><td>ReLU</td><td>83.06\u00b10.66</td><td>84.63\u00b10.21</td><td>56.76\u00b15.92</td><td>60.54\u00b16.42</td><td>61.60\u00b11.75</td><td>\u2191 2.8%</td></tr><tr><td>Tanh</td><td>84.82\u00b10.51</td><td>85.58\u00b10.15</td><td>56.49\u00b15.19</td><td>57.84\u00b15.01</td><td>61.51\u00b11.88</td><td>\u2191 3.2%</td></tr><tr><td>L-ReLU</td><td>84.57\u00b10.93</td><td>84.50\u00b10.40</td><td>57.38\u00b12.16</td><td>60.54\u00b17.37</td><td>61.95\u00b12.18</td><td>\u2191 2.1%</td></tr><tr><td>Swish</td><td>83.88\u00b10.81</td><td>84.89\u00b10.34</td><td>57.30\u00b13.97</td><td>58.65\u00b15.55</td><td>58.33\u00b11.68</td><td>\u2191 4.1%</td></tr><tr><td>TAFS</td><td>89.08\u00b10.48</td><td>86.24\u00b10.17</td><td>57.37\u00b14.37</td><td>62.11\u00b15.48</td><td>62.31\u00b11.82</td><td>-</td></tr><tr><td rowspan=\"5\">Residual</td><td>ReLU</td><td>85.13\u00b10.95</td><td>84.45\u00b10.34</td><td>57.84\u00b15.43</td><td>57.84\u00b15.95</td><td>66.93\u00b12.17</td><td>\u2191 3.2%</td></tr><tr><td>Tanh</td><td>86.02\u00b10.55</td><td>85.63\u00b10.14</td><td>58.38\u00b14.39</td><td>57.57\u00b15.93</td><td>68.86\u00b11.84</td><td>\u2191 2.0%</td></tr><tr><td>L-ReLU</td><td>86.60\u00b10.72</td><td>84.97\u00b10.33</td><td>55.68\u00b18.30</td><td>57.84\u00b16.75</td><td>67.50\u00b11.48</td><td>\u2191 3.3%</td></tr><tr><td>Swish</td><td>85.86\u00b10.64</td><td>84.67\u00b10.19</td><td>56.22\u00b16.14</td><td>60.54\u00b17.66</td><td>66.29\u00b12.12</td><td>\u2191 2.8%</td></tr><tr><td>TAFS</td><td>88.16\u00b10.58</td><td>86.29\u00b10.18</td><td>58.20\u00b14.80</td><td>60.22\u00b15.51</td><td>70.49\u00b11.64</td><td>-</td></tr><tr><td rowspan=\"5\">JKNet</td><td>ReLU</td><td>86.86\u00b10.71</td><td>84.99\u00b10.25</td><td>76.49\u00b17.36</td><td>77.57\u00b17.36</td><td>58.18\u00b11.63</td><td>\u2191 3.8%</td></tr><tr><td>Tanh</td><td>86.41\u00b10.57</td><td>85.57\u00b10.20</td><td>68.92\u00b16.76</td><td>65.95\u00b19.22</td><td>60.20\u00b12.19</td><td>\u2191 9.1%</td></tr><tr><td>L-ReLU</td><td>87.45\u00b10.51</td><td>85.04\u00b10.15</td><td>74.05\u00b15.57</td><td>76.49\u00b18.80</td><td>57.98\u00b12.36</td><td>\u2191 4.1%</td></tr><tr><td>Swish</td><td>86.34\u00b10.92</td><td>84.95\u00b10.28</td><td>77.03\u00b15.57</td><td>78.11\u00b16.99</td><td>57.00\u00b12.54</td><td>\u2191 4.7%</td></tr><tr><td>TAFS</td><td>88.84\u00b10.56</td><td>87.07\u00b10.22</td><td>81.35\u00b16.40</td><td>81.08\u00b15.01</td><td>60.21\u00b12.04</td><td>-</td></tr><tr><td rowspan=\"5\">Mixhop</td><td>ReLU</td><td>85.31\u00b10.64</td><td>85.10\u00b10.18</td><td>73.78\u00b15.55</td><td>74.05\u00b19.53</td><td>51.64\u00b12.24</td><td>\u2191 2.7%</td></tr><tr><td>Tanh</td><td>85.15\u00b10.67</td><td>85.12\u00b10.30</td><td>72.97\u00b17.55</td><td>76.76\u00b16.86</td><td>50.59\u00b12.60</td><td>\u2191 2.6%</td></tr><tr><td>L-ReLU</td><td>86.38\u00b10.50</td><td>85.01\u00b10.17</td><td>72.43\u00b16.14</td><td>72.70\u00b15.05</td><td>51.36\u00b12.80</td><td>\u2191 3.4%</td></tr><tr><td>Swish</td><td>86.21\u00b11.03</td><td>85.43\u00b10.25</td><td>72.34\u00b18.18</td><td>74.86\u00b16.51</td><td>51.89\u00b12.10</td><td>\u2191 2.5%</td></tr><tr><td>TAFS</td><td>88.77\u00b10.57</td><td>86.18\u00b10.17</td><td>75.14\u00b15.38</td><td>78.43\u00b15.28</td><td>52.17\u00b11.97</td><td>-</td></tr><tr><td rowspan=\"5\">Stack</td><td>ReLU</td><td>83.06\u00b10.66</td><td>83.67\u00b10.41</td><td>58.11\u00b16.19</td><td>70.00\u00b16.78</td><td>47.02\u00b14.20</td><td>\u2191 12.1%</td></tr><tr><td>Tanh</td><td>84.82\u00b10.51</td><td>84.90\u00b10.19</td><td>68.65\u00b16.75</td><td>71.89\u00b17.85</td><td>53.50\u00b11.68</td><td>\u2191 4.3%</td></tr><tr><td>L-ReLU</td><td>84.57\u00b10.65</td><td>84.16\u00b10.23</td><td>62.16\u00b15.92</td><td>68.11\u00b17.23</td><td>49.21\u00b13.02</td><td>\u2191 9.8%</td></tr><tr><td>Swish</td><td>81.53\u00b10.74</td><td>83.62\u00b10.50</td><td>57.03\u00b16.45</td><td>68.65\u00b16.19</td><td>48.42\u00b12.17</td><td>\u2191 13.0%</td></tr><tr><td>TAFS</td><td>87.08\u00b10.48</td><td>85.22\u00b10.30</td><td>72.43\u00b17.23</td><td>74.51\u00b16.92</td><td>58.57\u00b11.20</td><td>-</td></tr><tr><td rowspan=\"5\">Residual</td><td>ReLU</td><td>84.11\u00b10.82</td><td>83.05\u00b10.33</td><td>65.95\u00b16.64</td><td>73.51\u00b16.71</td><td>55.02\u00b12.73</td><td>\u2191 6.1%</td></tr><tr><td>Tanh</td><td>85.62\u00b10.52</td><td>85.22\u00b10.17</td><td>72.43\u00b13.97</td><td>78.11\u00b19.00</td><td>59.17\u00b11.80</td><td>\u2191 0.6%</td></tr><tr><td>L-ReLU</td><td>85.63\u00b10.42</td><td>84.05\u00b10.21</td><td>71.89\u00b13.67</td><td>74.86\u00b15.80</td><td>55.86\u00b11.83</td><td>\u2191 3.1%</td></tr><tr><td>Swish</td><td>84.97\u00b10.79</td><td>84.17\u00b10.43</td><td>71.08\u00b14.02</td><td>75.41\u00b17.09</td><td>54.17\u00b11.44</td><td>\u2191 3.9%</td></tr><tr><td>TAFS</td><td>89.10\u00b10.385</td><td>85.22\u00b10.17</td><td>73.38\u00b13.63</td><td>77.03\u00b16.86</td><td>58.62\u00b12.08</td><td>-</td></tr><tr><td rowspan=\"5\">JKNet</td><td>ReLU</td><td>85.29\u00b10.56</td><td>83.97\u00b10.15</td><td>80.00\u00b16.07</td><td>81.62\u00b15.10</td><td>56.78\u00b11.62</td><td>\u2191 3.0%</td></tr><tr><td>Tanh</td><td>86.01\u00b10.51</td><td>85.25\u00b10.18</td><td>77.03\u00b15.57</td><td>78.92\u00b16.14</td><td>57.68\u00b11.92</td><td>\u2191 3.8%</td></tr><tr><td>L-ReLU</td><td>85.90\u00b10.42</td><td>85.01\u00b10.25</td><td>80.27\u00b16.84</td><td>81.35\u00b14.75</td><td>57.41\u00b12.01</td><td>\u2191 2.5%</td></tr><tr><td>Swish</td><td>85.56\u00b10.61</td><td>84.71\u00b10.22</td><td>77.13\u00b15.30</td><td>81.06\u00b15.43</td><td>55.00\u00b11.93</td><td>\u2191 4.5%</td></tr><tr><td>TAFS</td><td>89.51\u00b10.66</td><td>86.73\u00b10.20</td><td>81.79\u00b15.08</td><td>82.10\u00b15.16</td><td>59.37\u00b11.53</td><td>-</td></tr><tr><td rowspan=\"5\">Mixhop</td><td>ReLU</td><td>84.55\u00b11.07</td><td>84.23\u00b10.21</td><td>75.95\u00b19.59</td><td>81.63\u00b14.80</td><td>54.19\u00b12.15</td><td>\u2191 2.3%</td></tr><tr><td>Tanh</td><td>84.82\u00b10.65</td><td>84.21\u00b10.38</td><td>78.11\u00b16.99</td><td>81.89\u00b15.80</td><td>53.20\u00b11.21</td><td>\u2191 2.0%</td></tr><tr><td>L-ReLU</td><td>84.80\u00b11.10</td><td>84.34\u00b10.28</td><td>76.22\u00b18.36</td><td>77.30\u00b14.71</td><td>53.14\u00b11.72</td><td>\u2191 3.7%</td></tr><tr><td>Swish</td><td>84.18\u00b10.55</td><td>84.69\u00b10.30</td><td>75.95\u00b18.15</td><td>80.27\u00b16.74</td><td>53.20\u00b11.86</td><td>\u2191 3.0%</td></tr><tr><td>TAFS</td><td>87.77\u00b11.40</td><td>85.30\u00b10.24</td><td>77.77\u00b14.39</td><td>83.70\u00b14.05</td><td>55.07\u00b10.57</td><td>-</td></tr></table>\n\n\nResults. We provide in Table 2 the results of node classification tasks. The improvement of TAFS with respect to the other function choices is significant. Note that the improvements are observable across different graph data and GNN models, showing that TAFS is task-aware to graphs in citation, university webpage, wikipedia link graph, etc.\n\n\n# 5.2 MOLECULE AND PROTEIN INTERACTION PREDICTION\n\n\nDatasets. Biomedical graph are one of the most active and effective application areas of GNN. Biomedical GNN has accelerated important studies in protein prediction, molecule generation, gene\n\n\nexpression, etc. We consider the link prediction that is a typical task in molecule and protein interaction prediction. Specifically, we consider Drug-Drug Interaction (DDI), Drug-Target Interaction (DTI), Protein-Protein Interaction (PPI) and Disea-Gene Association (DGA). The statistics of the four datasets are provided in Appx. Table 6.\n\n\nBaselines. We adopt two biomedical graph baselines SkipGNN (Huang et al., 2020) and HOGCN (KC et al., 2022). SkipGNN proposes a general GNN architecture to model molecular interactions and works well on all these biomedical tasks. We use both as base model and experiment TAFS to replace the activation functions. The training hyperparameters are the same as in the original work.\n\n\nTable 3: Drug and protein interaction predictions.\n\n\n<table><tr><td>Task</td><td>Model</td><td>Activation</td><td>ROCAUC</td><td>PRAUC</td></tr><tr><td rowspan=\"6\">Drug-Target Interaction</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.922\u00b10.004</td><td>0.928\u00b10.006</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.933\u00b10.002</td><td>0.934\u00b10.001</td></tr><tr><td>TAFS</td><td>0.952\u00b10.001</td><td>0.954\u00b10.001</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.927\u00b10.001</td><td>0.929\u00b10.001</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.923\u00b10.002</td><td>0.922\u00b10.001</td></tr><tr><td>TAFS</td><td>0.943\u00b10.002</td><td>0.940\u00b10.001</td></tr><tr><td rowspan=\"6\">Drug-Drug Interaction</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.886\u00b10.003</td><td>0.866\u00b10.006</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.890\u00b10.002</td><td>0.874\u00b10.001</td></tr><tr><td>TAFS</td><td>0.911\u00b10.002</td><td>0.898\u00b10.003</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.898\u00b10.002</td><td>0.881\u00b10.003</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.897\u00b10.002</td><td>0.901\u00b10.002</td></tr><tr><td>TAFS</td><td>0.917\u00b10.002</td><td>0.901\u00b10.001</td></tr><tr><td rowspan=\"6\">Protein-Protein Interaction</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.917\u00b10.004</td><td>0.921\u00b10.003</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.920\u00b10.001</td><td>0.922\u00b10.002</td></tr><tr><td>TAFS</td><td>0.927\u00b10.001</td><td>0.937\u00b10.002</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.919\u00b10.001</td><td>0.922\u00b10.002</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.919\u00b10.002</td><td>0.924\u00b10.001</td></tr><tr><td>TAFS</td><td>0.923\u00b10.003</td><td>0.929\u00b10.002</td></tr><tr><td rowspan=\"6\">Disease-Gene Association</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.912\u00b10.004</td><td>0.915\u00b10.003</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.916\u00b10.001</td><td>0.920\u00b10.001</td></tr><tr><td>TAFS</td><td>0.930\u00b10.001</td><td>0.940\u00b10.001</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.927\u00b10.001</td><td>0.934\u00b10.001</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.929\u00b10.002</td><td>0.933\u00b10.001</td></tr><tr><td>TAFS</td><td>0.933\u00b10.001</td><td>0.942\u00b10.002</td></tr></table>\n\n\nResults. We provide in Table 3 the results of four link prediction tasks. Again, both SkipGNN and HOGCN use ReLU by default. With TAFS, SkipGNN and HOGCN has gained significant performance evaluated in ROCAUC and PRAUC, two non-differentiable metrics. Furthermore, when TAFS is integrated with SkipGNN, a model from 2020, it outperforms HOGCN, the state-of-the-art model from 2022. This underscores the significance of activation function search, which has hitherto been overlooked in the GNN community.\n\n\n# 5.3 ABLATION STUDY\n\n\nTo further analyze our proposed TAFS algorithm, we provide additional experiments to illustrate the search results, search efficiency and hyperparameter impact.\n\n\nVisualization of activation function search. We show in Figure 3 the searched activation functions from the literature methods and TAFS. It can be observed that TAFS could find diverse activation functions different than manually design ones or the searched ones by Swish and APL. Moreover, TAFS learns layer-wise activation function, leading to different behaviours of functions in different layers as in Figure 3(b)(c). Deeper layers' activation functions are smoother than shallow layers.\n\n\nSearch efficiency The modeling differences between literature search methods and TAFS are given in Table 1. In this part, we provide more empirical details of the search efficiency comparison in Table 4. TAFS has a significantly smaller consumption of extra memory and shorter running time. This huge efficiency improvement is credited to TAFS' compact MLP functional search space and differentiable search strategy, making TAFS' extra parameters independent of base models, whichever dataset or\n\n\n![(a) Literature search algorithms.](images/af3d2dc16af32136e39f596dae27e88d65ea40aedbff3b999636f48c35dcb38d.jpg)\n\n\n![(b) Results on dataset Cora Figure 3: Literature activation functions and searched activation results. (b)(c) include searched results on different layers across two datasets.](images/b11507cfba3258b75da195ae2cbcfaf11e5b524c873c56d4279762d8fdcc6044.jpg)\n\n\n![(c) Results on dataset DBLP](images/7e13fc6da84ed4db609d8a276c20829e94faac99bcbe6f07484b3c5b685003e0.jpg)\n\n\nGNN model (as long as the model has the same number of activation functions), while APL models each neuron with a piecewise linear unit, leading to over 2000 times more parameters than TAFS.\n\n\nHyperparameters impact. The choice of hyperparameters significantly affects performance. TAFS introduces two sets of hyperparameters: the number of samples (K) and the selection of the MLP architecture. We present their effects in Figure 4. The number of samples (K) in stochastic optimization exhibits a consistent increasing trend, representing a trade-off between accuracy and computational time. Regarding MLP hyperparameters, we analyze their impact on two node tasks, DBLP and Cornell, using nine different configurations: depths ranging from two to four layers and widths spanning from 10 to 1000 neurons. It is evident that a very small MLP (e.g., two layers with 10 neurons) is inadequate for\n\n\nmodeling adaptive activation functions. However, the distinctions between other choices are negligible. Given that deeper and wider MLPs require significantly more parameters, we opt for a two-layer MLP with 100 hidden units in all other experiments.\n\n\nTable 4: Search efficiency comparison.\n\n\n<table><tr><td>Dataset</td><td>Model</td><td>Parameters</td><td>Time(min)</td></tr><tr><td rowspan=\"4\">DBLP</td><td>Base</td><td>420K</td><td>0.15</td></tr><tr><td>Swish</td><td>+340K (+82%)</td><td>350</td></tr><tr><td>APL</td><td>+2400K (+575%)</td><td>4</td></tr><tr><td>TAFS</td><td>+1.3K (+0.3%)</td><td>1</td></tr><tr><td rowspan=\"4\">Chameleon</td><td>Base</td><td>315K</td><td>1.2</td></tr><tr><td>Swish</td><td>+420K (+108%)</td><td>2990</td></tr><tr><td>APL</td><td>+1760K (+558%)</td><td>33</td></tr><tr><td>TAFS</td><td>+1.3K (+0.4%)</td><td>11</td></tr><tr><td rowspan=\"4\">Ogbg-Molhiv</td><td>Base</td><td>27M</td><td>1020</td></tr><tr><td>Swish</td><td>-</td><td>&gt;70 days</td></tr><tr><td>APL</td><td>OOM (+150M)</td><td>-</td></tr><tr><td>TAFS</td><td>+12K</td><td>1380</td></tr></table>\n\n\n![(a) Number of samples in stochastic optimization. Larger K is lawways better but slower.](images/1b4fb305a40747e0fdddae0ea5b9af8243af20e143813ef2b16d61f37abbd836.jpg)\n\n\n![(b) MLP dimension impacts on DBLP dataset. Figure 4: Hyperparameter impact of number of samples in stochastic relaxation and the impact of MLP dimensions. In (b)(c), deeper color means better performance.](images/ec9501441f0a88918d11f9983df6b4dce431e54dd07be173b545a963595dad4d.jpg)\n\n\n![(c) MLP dimension impacts on Cornell dataset.](images/18218eff81e36c61f38c170a3e6f2b484b0ed369de546c8895363ea2e4c9c7b6.jpg)\n\n\n# 6 CONCLUSION\n\n\nIn a word, we achieve a task-aware activation function search in GNN through an expressive and compact representation of search space, stochastic relaxation with reparameterization, which are carefully co-designed with search strategy. Our search space is inclusive and parameter efficient, including appropriate number of high-quality functions. The search strategy is end-to-end trained and every operation of the framework is differentiable. Finally, the stochastic relaxation is capable of dealing with any metric of interest, closing the optimization gap.\n\n\n# REFERENCES\n\n- Forest Agostinelli, Matthew D. Hoffman, Peter J. Sadowski, and Pierre Baldi. Learning activation functions to improve deep neural networks. In International Conference on Learning Representations, ICLR Workshop, 2015.\n\n- Andrea Apicella, Francesco Donnarumma, Francesco Isgro, and Roberto Prevete. A survey on modern trainable activation functions. Neural Networks, 138:14-32, 2021.\n\n- Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). In International Conference on Learning Representations, ICLR, 2016.\n\n- Balthazar Donon, Zhengying Liu, Wenzhuo Liu, Isabelle Guyon, Antoine Marot, and Marc Schoenauer. Deep statistical solvers. In Neural Information Processing Systems, NeurIPS, 2020.\n\n- Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Activation functions in deep learning: A comprehensive survey and benchmark. Neurocomputing, 503:92-108, 2022.\n\n- Steffen Eger, Paul Youssef, and Iryna Gurevych. Is it time to swish? comparing deep learning activation functions across NLP tasks. In Empirical Methods in Natural Language Processing, Brussels, EMNLP, 2018.\n\n- Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In World Wide Web Conference, WWW, pp. 417-426. ACM, 2019.\n\n- Amir Farzad, Hoda Mashayekhi, and Hamid Hassanpour. A comparative performance analysis of different activation functions in LSTM networks for classification. Neural Computing and Applications, 31:2507-2521, 2019.\n\n- Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function on deep neural networks training. In International Conference on Machine Learning, ICML, volume 97, pp. 2672-2680, 2019.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In International Conference on Computer Vision, ICCV, 2015.\n\n- Judy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust learning with jacobian regularization. CoRR, abs/1908.02729, 2019.\n\n- Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In International Conference on Learning Representations, ICLR, 2020.\n\n- Kexin Huang, Cao Xiao, Lucas M Glass, Marinka Zitnik, and Jimeng Sun. Skipgnn: predicting molecular interactions with skip-graph networks. *Scientific reports*, 10(1):1-16, 2020.\n\n- Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang, Chao Shen, Dongsheng Cao, Jian Wu, and Tingjun Hou. Could graph neural networks learn better molecular representation for drug discovery? a comparison study of descriptor-based and graph-based models. Journal of Cheminformatics, 2021.\n\n- K. KC, R. Li, F. Cui, and A. R. Haake. Predicting biomedical interactions with higher-order graph convolutional networks. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(02), mar 2022. ISSN 1557-9964. doi: 10.1109/TCBB.2021.3059415.\n\n- Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulouse, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 1998.\n\n- Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), AAAI, 2018a.\n\n- Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. In International Conference on Learning Representations, ICLR, 2018b.\n\n- Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.\n\n- Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In International Conference on Learning Representations, ICLR, 2019.\n\n- Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, and Or Litany. Learning smooth neural functions via lipschitz regularization. In SIGGRAPH. ACM, 2022.\n\n- Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rectifier nonlinearities improve neural network acoustic models. In ICML. Atlanta, GA, 2013.\n\n- Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Johannes Furnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 807-814. Omnipress, 2010.\n\n- Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. In International Conference on Learning Representations, ICLR, 2018.\n\n- Patrick Reiser, Marlen Neubert, Andr\u00e9 Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer, and Pascal Friederich. Graph neural networks for materials science and chemistry. Communications Materials, 3(1), 2022. doi: 10.1038/s43246-022-00315-6. URL https://doi.org/10.1038/s43246-022-00315-6.\n\n- Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W. Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, ICML, 2020.\n\n- Sungyong Seo, Chuizheng Meng, and Yan Liu. Physics-aware difference graph networks for sparsely-observed dynamics. In International Conference on Learning Representations, ICLR, 2020.\n\n- Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. In International Conference on Learning Representations, ICLR, 2020.\n\n- Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. CoRR, abs/1710.10903, 2017.\n\n- Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. Comenet: Towards complete and efficient message passing for 3d molecular graphs. In NeurIPS, 2022.\n\n- Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. In International Conference on Learning Representations, ICLR, 2018.\n\n- Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-based recommendation with graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, 2019.\n\n- Zhenxing Wu, Jike Wang, Hongyan Du, Dejun Jiang, Yu Kang, Dan Li, Peichen Pan, Yafeng Deng, Dongsheng Cao, Chang-Yu Hsieh, and Tingjun Hou. Chemistry-intuitive explanation of graph neural networks for molecular property prediction with substructure masking. Nature Communications, 2023.\n\n- Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, ICML, volume 80 of Proceedings of Machine Learning Research, pp. 5449-5458. PMLR, 2018.\n\n- Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, ICLR, 2019.\n\n- Yifei Zhang, Hao Zhu, Ziqiao Meng, Piotr Koniusz, and Irwin King. Graph-adaptive rectified linear unit for graph neural networks. In The ACM Web Conference, WWW, 2022.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634678.058624", "msecs": "58.0", "relativeCreated": "22882.09819793701", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Numerical Pitfalls in Policy Gradient Updates\n\n# Abstract\nNumerical instability, such as gradient explosion, is a fundamental problem in practical deep reinforcement learning (DRL) algorithms. Beyond anecdotal debugging heuristics, there is a lack of systematic understanding of the causes for numerical sensitivity that leads to exploding gradient failures in practice. In this work, we demonstrate that the issue arises from the ill-conditioned density ratio in the surrogate objective that comes from importance sampling, which can take excessively large values during training. Perhaps surprisingly, while various policy optimization methods such as TRPO and PPO prevent excessively large policy updates, their optimization constraints on KL divergence and probability ratio cannot guarantee numerical stability. This also explains why gradient explosion often occurs during DRL training, even with code-level optimizations. To address this issue, we propose the Vanilla Policy Gradient with Clipping algorithm, which replaces the importance sampling ratio with its logarithm. This approach effectively prevents gradient explosion while achieving performance comparable to PPO.\n\n\n# 1 INTRODUCTION\n\n\nDeep reinforcement learning (DRL) has demonstrated its effectiveness in various domains (Mnih et al., 2015; Silver et al., 2017; Vinyals et al., 2019; Ouyang et al., 2022). Despite these successes, the reliability and numerical stability of DRL algorithms remain fundamental limitations that hinder their application in real-world environments. In particular, DRL algorithms are often observed to have numerical instability issues, such as exhibiting the major problem of gradient explosion that destroys the learning progress. Existing work on the brittleness of DRL methods attributes this to either the high variance in actions (Fujita & Maeda, 2018), the gradient estimator (Liu et al., 2020), or the sensitivity to hyperparameters (Henderson et al., 2018).\n\n\nIn this paper, we perform a rigorous analysis of the root cause of numerical instability in policy optimization algorithms such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) (Schulman et al., 2015a; 2017). Gradient explosion is a consequence of overflows caused by computations involving excessively large numbers that exceed the limits of floating-point arithmetic. We show that the long horizons and large value variance are not the core reason for overflow, especially under action clipping as commonly used in standard DRL implementations (Duan et al., 2016; Raffin et al., 2021; Bou et al., 2024). Another popular explanation is that the covariance matrix of the stochastic policy can become singular during training, leading to gradient explosion. However, our experiments in Appendix B show that arithmetic overflows can occur even when the standard deviations are lower-bounded. Instead, our analysis indicates that the root cause is the importance sampling steps in TRPO and PPO (Schulman et al., 2015a; 2017), where the probability ratio can take exponentially large values, especially in the case of Gaussian distributions. Specifically, we analyze the condition number of the probability ratio, and demonstrate that the probability ratio is exponential with respect to a line integral in the policy space, which can lead to arithmetic overflows and cause gradient explosion.\n\n\nIn particular, we demonstrate that optimization constraints proposed in TRPO/PPO, such as limiting the KL-divergence and clipping, do not prevent numerical instability. We show that the probability ratio in importance sampling can grow exponentially fast, ultimately causing arithmetic overflows, even while the KL divergence remains small. This indicates that various policy optimization methods that prevent excessively large policy updates do not prevent numerical problems. Indeed, the probability ratios are not guaranteed to stay small, especially when the policy network's output is\n\n\nfurther clipped in the action space, leading to excessively small values in the probability density functions and large condition numbers in the density ratio.\n\n\nGiven that importance sampling is the root cause of the exploding gradient issue, we propose an algorithm called Vanilla Policy Gradient with Clipping, a modified version of the PPO algorithm in which the importance sampling ratio is replaced with its logarithm. We conduct experiments on continuous-control benchmarks to evaluate its effectiveness. Specifically, we find that this algorithm performs comparably to PPO on small policy networks where no gradient explosion occurs and maintains good performance on large policy networks where PPO encounters numerical issues. Additionally, the influence of other code-level techniques, such as reward scaling and learning rates, is discussed in Appendix B.\n\n\nOverall, our findings suggest that the TRPO/PPO loss is inherently ill-conditioned due to importance sampling and action clipping, which directly contribute to the numerical instability of deep policy gradient methods. This issue should not be viewed merely as a hyperparameter tuning problem but as a fundamental limitation of DRL algorithms that requires more attention.\n\n\n# 2 RELATED WORK\n\n\nDeep policy gradients in practice. It has been reported that implementation details and code-level optimizations fundamentally impact the performance of deep policy gradient algorithms such as TRPO and PPO (Henderson et al., 2018; Engstrom et al., 2020; Andrychowicz et al., 2021). In particular, empirical studies have shown that the most significant discrepancies between theory and implementation arise in gradient estimation, value prediction, and optimization landscapes (Ilyas et al., 2020). These findings were later supported by theoretical studies, which indicated that they stem from the fractal structures present in both value and policy landscapes (Wang et al., 2023). Our work contributes to this line of research by providing an in-depth study on the numerical stability of deep policy gradient methods.\n\n\nExploding gradients in deep learning. Many neural network architectures have been reported to suffer from the gradient explosion issue, including deep multi-layer networks (Bengio et al., 1994; Glorot & Bengio, 2010) and residual neural networks (RNNs) (Pascanu et al., 2013). This problem is closely linked to the depth of the network architecture (Philipp et al., 2017; Schoenholz et al., 2017), which can result in a long chain of Jacobian multiplications. In the context of reinforcement learning (RL), Liu et al. (2018) points out that the product of density ratios over a long horizon can grow exponentially, leading to exploding variance in off-policy estimation. Also, Khorasani et al. (2023) proposes to use second-order information in the form of Hessian vector products to bypass importance sampling weights in policy gradient. In this work, we demonstrate that even a single density ratio can take excessively large values and cause numerical overflow, and propose an algorithm that drops the importance sampling from PPO objective while achieving comparable performance. Wang et al. (2024) investigates gradient explosion in the context of generative adversarial imitation learning (GAIL), suggesting that a large reward function can cause exploding gradients. However, our empirical results in Section B indicate that this does not fully explain gradient explosion in policy gradient algorithms, as scaling down the reward does not completely overcome the issue.\n\n\n# 3 NUMERICAL INSTABILITY CAUSED BY IMPORTANCE SAMPLING\n\n\nTRPO and PPO objective. The original policy gradient estimator has the form\n\n\n$$\n\n$$\n\\nabla \\mathcal {J} (\\theta) = \\mathbb {E} _ {\\left(s _ {t}, a _ {t}\\right) \\sim \\pi_ {\\theta}} \\left[ \\nabla_ {\\theta} \\log \\pi_ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) Q ^ {\\pi} \\left(s _ {t}, a _ {t}\\right) \\right], \\tag {1}\n$$\n\n$$\n\n\nwhere  $Q^{\\pi}$  is the  $Q$ -function of the current policy  $\\pi_{\\theta}$  (Sutton et al., 1999). One difficulty of directly optimizing equation 1 is due to the complex dependency of sampled data on  $\\pi_{\\theta}$  (Schulman et al., 2015a). To address it, TRPO proposes a surrogate objective that incorporates importance sampling:\n\n\n$$\n\n$$\n\\max  _ {\\theta} \\hat {\\mathbb {E}} _ {(s _ {t}, a _ {t}) \\sim \\pi} \\left[ \\frac {\\pi_ {\\theta} (a _ {t} | s _ {t})}{\\pi (a _ {t} | s _ {t})} \\hat {A} ^ {\\pi} (s _ {t}, a _ {t}) \\right], \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\hat{\\mathbb{E}}_{(s_t,a_t)\\sim \\pi}[\\cdot ]$  denotes the estimated expectation over sampled trajectories and  $\\hat{A}^{\\pi}(s_t,a_t)$  the estimated advantage function at  $(s_t,a_t)$  which is usually obtained through Generalized Advantage\n\n\nEstimation (GAE, Schulman et al. (2015b)). Let  $L(\\theta)$  denote the surrogate objective in equation 2. It has been proved that the gradients of equation 1 and equation 2 coincide at  $\\pi = \\pi_{\\theta}$ . Additionally, both TRPO and PPO suggest that the current policy  $\\pi_{\\theta}$  should not deviate too much from the old policy  $\\pi$ , leading to various optimization constraints which will be discussed in Section 5.\n\n\nCondition number and numerical stability. We now briefly introduce the concept of the condition number from numerical analysis, which measures how much the output of an algorithm changes in response to a small change in its input:\n\n\nDefinition 3.1. (Condition number) Given a function  $f$  and an input  $x$ , let  $\\delta x$  be the error in  $x$  and  $\\delta f(x) = f(x + \\delta x) - f(x)$  be the corresponding error resulted in the output. We define the absolute condition number of  $f$  at  $x$  as\n\n\n$$\n\n$$\n\\kappa = \\lim  _ {\\delta \\rightarrow 0} \\sup  _ {| \\delta x | \\leq \\delta} \\frac {| \\delta f (x) |}{| \\delta x |}. \\tag {3}\n$$\n\n$$\n\n\nIn particular, when the function  $f$  is differentiable, it further has\n\n\n$$\n\n$$\n\\kappa = \\| J _ {f} (x) \\|,\n$$\n\n$$\n\n\nwhere  $J_{f}(x)$  is the Jacobian of  $f$  at  $x$ .\n\n\nA detailed study can be found in Trefethen & Bau (1997). The following concept of machine precision defines the threshold of numerical overflows:\n\n\nDefinition 3.2. (Machine precision) Let  $e > 0$  denote the machine precision throughout this paper, any quantity with its absolute value smaller than  $e$  will be considered zero.\n\n\nImportance sampling with clipped actions. Consider the probability ratio  $p_{\\theta}(a|s) = \\frac{\\pi_{\\theta}(a|s)}{\\pi(a|s)}$  in the loss function. This ratio might become ill-conditioned when  $\\pi(a|s)$  is very small. However, it is rarely reported in practice that importance sampling itself causes any numerical issues. This is because, when importance sampling is applied to modify a probabilistic distribution, the probability ratio  $p_{\\theta}(a|s)$  is only evaluated at data points sampled from the distribution  $\\pi(\\cdot|s)$ . This means that the probability of obtaining an action  $a$  where  $\\pi(a|s) \\ll 1$  is also very small, thereby rarely causing numerical instability. For example, let  $a \\sim \\mathcal{N}(\\mu_0(s), \\Sigma_0)$  be  $m$ -dimensional Gaussian random variable with probability density function  $\\pi(a|s)$ , then we have\n\n\n$$\n\n$$\nP (\\pi (a | s) <   e) \\leq 2 \\sqrt {2 \\pi} m \\left(e \\sqrt {\\det \\Sigma_ {0}}\\right) ^ {\\frac {1}{m}}, \\tag {4}\n$$\n\n$$\n\n\nwhere  $e \\ll 1$  is the machine precision.\n\n\nAlthough it is unlikely for  $\\pi(a|s)$  to take extremely small values if  $a$  is directly sampled from  $\\pi$  itself, as shown in equation 4, it is important to note that the actual action space usually has a bounded range due to physical or environmental constraints. Therefore, a common practice in DRL is to clip the sampled actions before feeding them into the simulator. Without loss of generality, we assume that the action space is of the box form  $\\mathcal{A} = [-\\beta, \\beta]^m$  for some positive  $\\beta$ , and write the action-clipping operation as  $\\phi: \\mathbb{R}^m \\to [-\\beta, \\beta]^m$ . Namely, for any  $a = (a_1, \\dots, a_m) \\in \\mathbb{R}^m$ ,  $\\phi(a) = (a_1', \\dots, a_m')$  where  $a_i' = \\max(-\\beta, \\min(a_i, \\beta))$ .\n\n\nNote that the probability ratio is evaluated after action clipping (Raffin et al., 2021). That is,  $p_{\\theta}(\\phi(a)|s)$  is used instead of  $p_{\\theta}(a|s)$  when computing the objective in equation 2. Consequently, the distance between the mean  $\\mu(s)$  and the clipped action  $\\phi(a)$  is no longer guaranteed to be small with high probability as suggested in equation 4, especially when  $\\mu(s)$  lies outside the action space  $A$ :\n\n\nTheorem 3.1. Let  $a \\sim \\mathcal{N}(\\mu(s), \\Sigma)$  be  $m$ -dimensional Gaussian random variable with probability density function  $\\pi(\\cdot|s)$ , and  $\\mathcal{A} = [-\\beta, \\beta]^m$  with  $\\beta > 0$ . Let  $\\phi: \\mathbb{R}^m \\to \\mathcal{A}$  be the action-clipping transformation, then\n\n\n1. Suppose that  $\\mu(s) \\in \\mathcal{A}$ , then for any  $a \\in \\mathbb{R}^m$ ,\n\n\n$$\n\n$$\nP (\\pi (\\phi (a) | s) <   e) \\leq 2 \\sqrt {2 \\pi} m (e \\sqrt {\\det \\Sigma_ {0}}) ^ {\\frac {1}{m}},\n$$\n\n$$\n\n\nwhere  $e$  is the machine precision;\n\n\n2. If  $\\mu(s) \\notin \\mathcal{A}$ , then for any  $a \\in \\mathbb{R}^m$  it has\n\n\n$$\n\n$$\n\\pi (\\phi (a) | s) \\leq \\frac {1}{(2 \\pi) ^ {\\frac {m}{2}} \\sqrt {\\det \\Sigma}} \\exp \\left(- \\frac {d ^ {2}}{2 \\lambda_ {m a x}}\\right),\n$$\n\n$$\n\n\nwhere  $d = \\mathrm{dist}(\\mu (s),\\mathcal{A})$  be the distance between  $\\mu (s)$  and the action space  $\\mathcal{A}$ ,  $\\lambda_{max}$  is the largest eigenvalue of  $\\Sigma$ .\n\n\nTherefore,  $\\pi (\\phi (a)|s)$  may fall below machine precision  $e$  when  $\\mu (s)$  is far from the action space  $\\mathcal{A}$  or when the standard deviations of  $\\pi (\\cdot |s)$  are sufficiently small. In either case,  $\\pi (\\phi (a)|s)$  becomes equal to 0 on the machine, and dividing by it can cause numerical issues.\n\n\nTanh-Gaussian transformation. It is worth mentioning that we can also bound the action by applying an invertible squashing function, such as tanh, to the Gaussian samples (Haarnoja et al., 2018). However, this approach still suffers from the ill-conditioned nature of importance sampling. For instance, consider the modified probability density function in the one-dimensional case, given by\n\n\n$$\n\n$$\n\\pi^ {\\prime} (a | s) = \\pi (u | s) \\left(1 - \\tanh  ^ {2} (u)\\right) ^ {- 1} \\tag {5}\n$$\n\n$$\n\n\nwhere  $u \\sim \\pi$  is a Gaussian random variable and  $a = \\tanh(u)$ . Note that when the mean of  $\\pi$  is too large, it is likely that the random variable  $u$  will take on extreme values, resulting in an excessively small value for the term  $1 - \\tanh^2(u)$  and causing numerical overflow in subsequent steps.\n\n\nIll-conditioned probability ratios. We have seen that the action-clipping transformation in deep policy gradients can significantly affect the numerical stability of importance sampling. However, one might argue that small values in  $\\pi(a|s)$  can be counterbalanced by small values in  $\\pi_{\\theta}(a|s)$  assuming that the current policy  $\\pi_{\\theta}$  is constrained to remain within a neighborhood of  $\\pi$ . This is particularly true in the first optimization step of each epoch, where  $\\pi_{\\theta}$  is reset to  $\\pi$  and all probability ratios are equal to 1. We will show that when action clipping is applied, the condition number of the probability ratio can become very large, causing  $p_{\\theta}(\\phi(a)|s)$  to grow exponentially fast and leading to instability as  $\\pi_{\\theta}$  deviates from  $\\pi$ . Let us again consider a Gaussian policy  $\\pi_{\\theta}(\\cdot|s) = \\mathcal{N}(\\mu(s;\\theta), \\Sigma(\\theta))$ , where  $\\mu(s)$  is typically parameterized by a neural network and  $\\Sigma(\\theta)$  is positive-definite and state-independent. Let  $\\pi(\\cdot|s) = \\mathcal{N}(\\mu(s;\\theta_0), \\Sigma(\\theta_0)))$ , the probability ratio is calculated as\n\n\n$$\n\n$$\n\\begin{array}{l} p _ {\\theta} (\\phi (a) | s) = \\frac {\\pi_ {\\theta} (\\phi (a) | s)}{\\pi (\\phi (a) | s)} \\\\ = \\sqrt {\\frac {\\det \\Sigma (\\theta_ {0})}{\\det \\Sigma (\\theta)}} \\frac {\\exp \\Big (- \\frac {1}{2} (\\phi (a) - \\mu (s ; \\theta)) ^ {T} \\Sigma (\\theta) ^ {- 1} (\\phi (a) - \\mu (s ; \\theta)) \\Big)}{\\exp \\Big (- \\frac {1}{2} (\\phi (a) - \\mu (s ; \\theta_ {0})) ^ {T} \\Sigma (\\theta_ {0}) ^ {- 1} (\\phi (a) - \\mu (s ; \\theta_ {0}))) \\Big)}. \\\\ \\end{array}\n$$\n\n$$\n\n\nTo better illustrate how the action-clipping transformation  $\\phi$  affects the condition number of  $p_{\\theta}$ , we further assume that  $\\Sigma (\\theta)\\equiv constant$ . The general case with parameterized standard deviations is discussed in Appendix C. Then, the gradient of the probability ratio is given by\n\n\n$$\n\n$$\n\\nabla_ {\\theta} p _ {\\theta} (\\phi (a) | s) = p _ {\\theta} (\\phi (a) | s) \\left(\\phi (a) - \\mu (s; \\theta)\\right) ^ {T} \\Sigma (\\theta) ^ {- 1} \\frac {\\partial \\mu (s ; \\theta)}{\\partial \\theta}, \\tag {6}\n$$\n\n$$\n\n\nwhere  $\\frac{\\partial\\mu(s;\\theta)}{\\partial\\theta}$  is the Jacobian of  $\\mu$  with respect to its parameters. Note that equation 6 is equivalent to\n\n\n$$\n\n$$\n\\nabla_ {\\theta} \\log \\Big (p _ {\\theta} (\\phi (a) | s) \\Big) = (\\phi (a) - \\mu (s; \\theta)) ^ {T} \\Sigma (\\theta) ^ {- 1} \\frac {\\partial \\mu (s ; \\theta)}{\\partial \\theta}, \\quad p _ {\\theta_ {0}} (\\phi (a) | s) = 1,\n$$\n\n$$\n\n\nwhich describes the gradient flow of  $p_{\\theta}$  in the policy space. Let  $\\mathcal{C}$  be the trajectory of policy parameters generated by PPO in the policy space, starting at  $\\theta_0$  and ending at  $\\theta_{1}$ . Applying the gradient theorem yields\n\n\n$$\n\n$$\np _ {\\theta_ {1}} (\\phi (a) | s) = \\exp \\left(\\int_ {\\mathcal {C}} (\\phi (a) - \\mu (s; \\theta)) ^ {T} \\Sigma (\\theta) ^ {- 1} \\frac {\\partial \\mu (s ; \\theta)}{\\partial \\theta} \\cdot d \\mathbf {r}\\right) \\tag {7}\n$$\n\n$$\n\n\nwhere the term in the exponent is the line integral along  $\\mathcal{C}$ . Therefore, the condition number  $\\kappa_{\\theta} = \\| \\nabla_{\\theta}p_{\\theta}(\\phi (a)|s)\\|$  can be extremely large as illustrated in Figure 1, implying that the probability ratio may overflow when\n\n- - The standard deviations of  $\\pi$  are small so that the matrix  $\\Sigma (\\theta)^{-1}$  has large eigenvalues;\n\n- - The action is clipped and the mean  $\\mu(s)$  lies outside  $\\mathcal{A}$  so that  $|\\phi(a) - \\mu(s; \\theta)|$  is large (illustrated in Figure 1(c));\n\n- - The policy parameterization  $\\mu (\\cdot ;\\theta)$  is complex (e.g. large policy networks) so that a small change in the parameter can lead to a dramatic difference in the output.\n\n\nSome DRL libraries further transform the standard deviations to prevent low variances. However, gradient explosion still occurs with those code-level techniques, indicating that the ill-conditioned probability ratio is the primary cause of the issue.\n\n\n![(a)](images/80e070eac0dce631c8c57b7069542977cfdf8fe6e6fa78a9d2282a2d8c8aa950.jpg)\n\n\n![(b) Figure 1: (a) When  $\\mu_0$  (the mean of  $\\pi_0$ ) is large, the condition number  $\\kappa_{\\theta}$  of the probability ratio  $p_{\\theta}$  changes exponentially when  $\\mu$  (the mean of  $\\pi$ ) deviates from  $\\mu_0$ ; (b)  $\\kappa_{\\theta}$  can become extremely large when the variance  $\\Sigma$  is small, particularly in cases of large means; (c) The action with the largest norm in each mini-batch grows outside the boundary during PPO training.](images/0309c81b33a71dcb866c9f40639ec3d55517aac930344bf0ca4c4c170bf15fa4.jpg)\n\n\n![(c)](images/1a4cc773ec14b85865c68182f6670597de906ac29e3e22ded4b878126cd4339e.jpg)\n\n\n# 4 EMPIRICAL ANALYSIS OF GRADIENT EXPLOSION\n\n\nIn the previous section, we theoretically demonstrated the ill-conditioned nature of importance sampling in TRPO/PPO methods. In this section, we present experimental evidence to support the theory. The experimental setup follows that described in Appendix A.\n\n\nGradient explosion is always accompanied by excessively large probability ratios. We run the Humanoid-v4 environment on five random seeds, all of which fail due to arithmetic overflow. In Figure 2, we observe that the maximum of probability ratio  $\\max_t \\log_{10}\\left(p_\\theta (a_t|s_t)\\right)$  takes excessively large values across all five random seeds. A widely accepted explanation for gradient explosion is that the standard deviations of the Gaussian policy can become very small, leading to potential singularity issues in the covariance matrix  $\\Sigma$ . While this can indeed cause numerical instability, it is not the only reason for gradient explosion. In Figure 6, we observe that the gradient explodes even before the standard deviations become small.\n\n\nLarge mean causes gradient explosion. We have mentioned that large outputs generated by the policy network are one of the primary causes of gradient explosion. To verify this analysis, we apply an additional transformation  $g(\\cdot)$  to the mean  $\\mu(s)$  to prevent it from leaving the action space  $\\mathcal{A}$ . The new Gaussian policy is defined as  $a \\sim \\mathcal{N}(g(\\mu(s)), \\Sigma(s))$ , where  $g(\\mu(s)) \\in \\mathcal{A}$ . In Table 1, we consider two transformations: the action-clipping function  $g = \\phi$ , defined in Section 3, and the hyperbolic tangent activation function  $g = \\tanh$ . We observe that both transformations effectively prevent the exploding gradient issue across three MuJoCo environments, indicating that having deviated means is indeed one of the main causes of numerical instability. Meanwhile, we also observe that the final returns are significantly lower than the benchmark (Raffin et al., 2021). This may be attributed to the fact that when the mean  $\\mu(s_t)$  goes outside  $\\mathcal{A}$  for some state  $s_t$ , the gradient norm\n\n\n$$\n\n$$\n\\nabla_ {\\theta} g (\\mu (s _ {t}; \\theta)) = \\frac {\\mathrm {d} g (\\mu)}{\\mathrm {d} \\mu} \\bigg | _ {\\mu = \\mu (s _ {t})} \\frac {\\partial \\mu (s _ {t} ; \\theta)}{\\partial \\theta},\n$$\n\n$$\n\n\n![(a)](images/6e586a0a0b9dad307be69807809d17c9b76159a59df20576ae5b0abe1ca28606.jpg)\n\n\n![(b)](images/fbf567908cdc2f0f8098d3a0b76f90126e7440fcba6657c57c14c59a02c002e0.jpg)\n\n\n![(c)](images/7c9d3df1f2e3fe85d813ad2b309b6dc51f5255ce87f855e7b105150b55975bb2.jpg)\n\n\n![(d)](images/5baf450fbdd89610e6212211c80f5148a02368bcbe392ea6839f0aece29e6dd0.jpg)\n\n\n![(e) Figure 2: The maximum of probability ratio after clipping takes exponentially large values when the gradient explodes in five individual runs.](images/adebdbcffc751f4bfeae8641e8a001fbcc2c91135a5bb55d531b5087c2251a8c.jpg)\n\n\n<table><tr><td>ENVIRONMENT</td><td>TRANSFORMATION</td><td>FINAL RETURN</td><td>EXPLOSION RATE</td></tr><tr><td>HOPPER-V4</td><td>CLIPPING</td><td>332 \u00b1 129</td><td>0%</td></tr><tr><td>HOPPER-V4</td><td>tanh</td><td>234 \u00b1 106</td><td>0%</td></tr><tr><td>HOPPER-V4</td><td>NONE</td><td>N/A</td><td>100%</td></tr><tr><td>WALKER2D-V4</td><td>CLIPPING</td><td>692 \u00b1 346</td><td>0%</td></tr><tr><td>WALKER2D-V4</td><td>tanh</td><td>434 \u00b1 60</td><td>0%</td></tr><tr><td>WALKER2D-V4</td><td>NONE</td><td>N/A</td><td>100%</td></tr><tr><td>HUMANOID-V4</td><td>CLIPPING</td><td>586 \u00b1 102</td><td>0%</td></tr><tr><td>HUMANOID-V4</td><td>tanh</td><td>530 \u00b1 72</td><td>0%</td></tr><tr><td>HUMANOID-V4</td><td>NONE</td><td>N/A</td><td>100%</td></tr></table>\n\n\nTable 1: Performance of mean transformation via clipping function or tanh function in three MuJoCo environments. For Hopper-v4 and Walker2d-v4, we directly apply tanh as their action input range is between -1 and 1. For the Humanoid-v4 task, we rescale the mapping to  $g = 0.4 * \\tanh$ , as the action inputs are bounded between -0.4 and 0.4.\n\n\nbecomes exponentially small or even equal to zero, depending on the transformation  $g$ . Notably, we may have  $|\\frac{\\mathrm{d}g(\\mu)}{\\mathrm{d}\\mu}|_2 \\simeq 0$  due to the vanishing gradient property of the transformation  $g$  when  $\\mu(s_t) \\notin \\mathcal{A}$ . As a result, policy gradient algorithms are more likely to converge towards sub-optimal regions in this case.\n\n\n# 5 OPTIMIZATION CONSTRAINTS CANNOT GUARANTEE NUMERICAL STABILITY\n\n\nIn previous sections, we demonstrated that importance sampling can be numerically unstable when the probability ratio is evaluated at transformed sample points. This is particularly problematic in deep policy gradient methods like TRPO and PPO, which incorporate the probability ratio into their surrogate objective functions. This integration is the fundamental reason for the numerical instability of these algorithms. However, it should be noted that both TRPO and PPO also apply certain constraints to the original optimization problem, such as KL divergence and probability ratio clipping. In this section, we will explain why these optimization constraints are insufficient to resolve the numerical issues as they are supposed to.\n\n\nKL divergence. In TRPO algorithm (Schulman et al., 2015a), it proposes to use the KL divergence between the current and old policy as a constraint to prevent large updates during policy improvement. Specifically, the KL divergence of two continuous probability distributions  $P$  and  $Q$  is defined as\n\n\n$$\n\n$$\nD _ {K L} (P \\| Q) = \\int_ {\\Omega} p (x) \\log \\frac {p (x)}{q (x)} \\mathrm {d} x, \\tag {8}\n$$\n\n$$\n\n\nwhere  $p$  and  $q$  are the probability density functions of  $P$  and  $Q$ , respectively. When optimizing the objective equation 2, TRPO applies the following hard constraint to the KL distance between successive policies\n\n\n$$\n\n$$\n\\hat {\\mathbb {E}} _ {s \\sim \\pi} \\left[ D _ {K L} \\left(\\pi_ {\\theta} (\\cdot | s) \\| \\pi (\\cdot | s)\\right) \\right] \\leq \\delta , \\tag {9}\n$$\n\n$$\n\n\nwhere  $\\delta > 0$  is a small quantity called the KL stepsize. Note that for many commonly used distributions, including Gaussian, the KL divergence between  $\\pi_{\\theta}$  and  $\\pi$  can be analytically calculated via a closed-form formula. For instance, consider two multivariate Gaussian policies  $\\pi_{\\theta}(\\cdot |s) \\sim \\mathcal{N}(\\mu(s), \\Sigma_0)$  and  $\\pi(\\cdot |s) \\sim \\mathcal{N}(\\mu_0(s), \\Sigma_0)$  with identical covariance matrices. The KL divergence is\n\n\n$$\n\n$$\nD _ {K L} \\left(\\pi_ {\\theta} (\\cdot | s) \\| \\pi (\\cdot | s)\\right) = \\frac {1}{2} \\left(\\mu (s) - \\mu_ {0} (s)\\right) ^ {T} \\Sigma_ {0} ^ {- 1} \\left(\\mu (s) - \\mu_ {0} (s)\\right). \\tag {10}\n$$\n\n$$\n\n\nWhile the above constraint can effectively prohibit  $\\pi_{\\theta}$  from making large updates steps in the distribution space, it may not be able to prevent the probability ratio  $p_{\\theta}(a|s)$  from taking extremely large values at some state-action pair  $(s,a)$  for two reasons: First, the KL divergence  $D_{KL}(\\pi_{\\theta}(\\cdot |s)\\parallel \\pi (\\cdot |s))$  measures the averaged distance between two distributions over the entire space  $\\mathbb{R}^m$ , while  $p_{\\theta}(a|s)$  is determined at a specific point  $a\\in \\mathbb{R}^m$ , which means that we can find some  $a\\in \\mathbb{R}^m$  such that the probability ratio  $p_{\\theta}(a|s)$  takes a large value even if the KL distance  $D_{KL}(\\pi_{\\theta}(\\cdot |s))$  is small. Second, the absolute value of KL divergence  $|D_{KL}(\\pi_{\\theta}(\\cdot |s)\\parallel \\pi (\\cdot |s))|$  is quadratic with respect to the policy update  $|\\mu (s) - \\mu_0(s)|$ , while the probability ratio  $\\pi_{\\theta}(a|s)$  grows exponentially with  $|\\mu (s) - \\mu_0(s)|$  especially for excessively small  $\\pi (a|s)$ .\n\n\nProbability ratio clipping. While TRPO involves second-order computations that are usually expensive in practice, PPO (Schulman et al., 2017) proposes a clipped surrogate objective\n\n\n$$\n\n$$\nL ^ {C L I P} (\\theta) = \\hat {\\mathbb {E}} _ {(s _ {t}, a _ {t}) \\sim \\pi} \\left[ \\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} (s _ {t}, a _ {t}), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right)\\right) \\right] \\tag {11}\n$$\n\n$$\n\n\nwhere  $\\epsilon \\in (0,1)$  is the clipping parameter. The clipped loss equation 11 is expected to block large updates in the policy that can improve the objective (i.e.,  $\\hat{A}^{\\pi}(s_t,a_t) > 0$ ), and allow them if they make the objective worse ( $\\hat{A}^{\\pi}(s_t,a_t) < 0$ ). This property, however, makes it possible that some ill-conditioned probability ratios are evaluated when computing  $L^{CLIP}(\\theta)$ .\n\n\nIn particular, consider a given state-action pair  $(s,a)$  and the estimated advantage  $\\hat{A}^{\\pi}(s,a)$ :\n\n\n- If  $\\hat{A}^{\\pi}(s,a)\\geq 0$  , it has\n\n\n$$\n\n$$\n\\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} (s, a), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} (s, a)\\right) = (1 + \\epsilon) \\hat {A} ^ {\\pi} (s, a),\n$$\n\n$$\n\n\nwhenever  $p_{\\theta} \\geq 1 + \\epsilon$ . Thus, it automatically blocks extremely large values in  $p_{\\theta}$  when evaluating  $L^{CLIP}(\\theta)$ ;\n\n\n- If  $\\hat{A}^{\\pi}(s,a) < 0$ , it is possible that the machine directly evaluates large probability ratios, since\n\n\n$$\n\n$$\n\\begin{array}{l} \\text {I n t e g r a n d i n e q u a t i o n 1 1} = \\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} (s, a), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} (s, a)\\right) \\\\ = \\hat {A} ^ {\\pi} (s, a) \\max  \\left(p _ {\\theta}, \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right)\\right) \\\\ = \\hat {A} ^ {\\pi} (s, a) \\max  \\left(p _ {\\theta}, 1 - \\epsilon\\right) \\\\ = p _ {\\theta} \\hat {A} ^ {\\pi} (s, a) \\\\ \\end{array}\n$$\n\n$$\n\n\nwhen  $p_{\\theta} \\geq 1 - \\epsilon$ , which then may lead to numerical overflows.\n\n\nIn practice, the expectation in equation 11 is estimated by the average over sampled trajectories\n\n\n$$\n\n$$\nL ^ {C L I P} (\\theta) \\simeq \\frac {1}{T} \\sum_ {t = 0} ^ {T - 1} \\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right)\\right).\n$$\n\n$$\n\n\nTherefore, numerical errors may occur if there exists any pair  $(s_t, a_t)$  with negative advantage  $\\hat{A}^{\\pi}(s_t, a_t) < 0$  and large probability ratio  $p_{\\theta}(a_t | s_t) \\gg 1$ .\n\n\nEmpirical results. We perform experiments to validate aforementioned theoretical analysis. In Figure 3 (a), we see that the KL divergence  $D_{KL}(\\pi_{\\theta}(\\cdot |s) \\parallel \\pi (\\cdot |s))$  remains small even when the probability ratio  $p_{\\theta}(a|s)$  takes very large values, indicating that  $D_{KL}(\\pi_{\\theta}(\\cdot |s) \\parallel \\pi (\\cdot |s))$  cannot ensure numerical stability. We also test the following KL-penalized objective\n\n\n$$\n\n$$\nL ^ {K L P E N} (\\theta) = \\hat {\\mathbb {E}} _ {t} \\left[ p _ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right) - b D _ {K L} \\left(\\pi_ {\\theta} (\\cdot \\mid s _ {t}) \\| \\pi (\\cdot \\mid s _ {t})\\right) \\right] \\tag {12}\n$$\n\n$$\n\n\nwhich is also proposed in Schulman et al. (2017). In Figure 3 (b), we can observe that optimizing equation 12 still fails to prevent the probability ratio growing exponentially. While probability ratio clipping has proven insufficient to overcome numerical instability, it is worth noting that the PPO algorithm becomes even more unstable without clipping, as shown in Figure 3 (c) where the gradient explodes within 4 steps. When plotting  $\\max_t\\log_{10}\\left(p_\\theta (a_t|s_t)\\right)$ , it is calculated after clipping. For example, a large  $p_\\theta (a_t|s_t)$  with positive advantage  $\\hat{A}^{\\pi}(s_t,a_t)$  is clipped and equal to  $1 + \\epsilon$ .\n\n\n![(a)](images/11471b6d0fad3487752b972cc65877b4b0ef8018fe8c68e7bfebf09eb7c48397.jpg)\n\n\n![(b) Figure 3: The logarithm of KL divergence  $\\log_{10}\\left(D_{KL}(\\pi_{\\theta}(\\cdot |s)\\parallel \\pi (\\cdot |s))\\right)$  and the maximum of probability ratio  $\\max_t\\log_{10}\\left(p_\\theta (a_t|s_t)\\right)$  at each optimization step is plotted with different optimization constraints, where environment is MuJoCo Humanoid-v4. In each plot, we apply (a) probability ratio clipping; (b) KL-penalty; (c) no constraints.](images/82524e3bd5e898124a4561cc6f1647abfacfbb1fe725f5673d8caad7f0b45a1f.jpg)\n\n\n![(c)](images/81539e00d6b66443c13f8c6da010e300f757475bd52664bc667f30ebea601d94.jpg)\n\n\n# 6 BYPASSING IMPORTANCE SAMPLING IN PPO\n\n\nIn previous sections, we demonstrated that the TRPO/PPO objective is potentially ill-conditioned and that the built-in optimization constraints cannot effectively alleviate its numerical instability. In this section, we examine the numerical stability of the vanilla policy gradient algorithm and propose a new algorithm based on it.\n\n\nNumerical stability of vanilla policy gradient. We have discussed why importance sampling in TRPO/PPO is the primary cause of gradient explosion. It is also worth stepping back to revisit the vanilla policy gradient, whose objective does not involve any density ratio. In this case, we optimize the following objective\n\n\n$$\n\n$$\nL ^ {V A N I L L A} (\\theta) = \\hat {\\mathbb {E}} _ {\\left(s _ {t}, a _ {t}\\right) \\sim \\pi_ {\\theta}} \\left[ \\log \\pi_ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right) \\right], \\tag {13}\n$$\n\n$$\n\n\nwhere we replace  $Q^{\\pi}(s_t, a_t)$  with  $\\hat{A}^{\\pi}(s_t, a_t)$  to reduce the variance. The logarithm of the Gaussian probability density function is\n\n\n<table><tr><td>ENVIRONMENT</td><td>FINAL RETURN</td><td>EXPLOSION RATE</td></tr><tr><td>HOPPER-V4</td><td>536 \u00b1 254</td><td>0%</td></tr><tr><td>WALKER2D-V4</td><td>195 \u00b1 135</td><td>0%</td></tr><tr><td>HUMANOID-V4</td><td>355 \u00b1 191</td><td>0%</td></tr></table>\n\n\nTable 2: Empirical results of vanilla policy gradient in three MuJoCo environments. Despite its poor performance, vanilla policy gradient does not suffer from the exploding gradient issue. Hyperparameters are specified in Appendix A.\n\n\n$$\n\n$$\n\\log \\pi_ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) = - \\frac {1}{2} \\sum_ {i = 1} ^ {m} \\frac {\\left(a _ {t} - \\mu\\right) _ {i} ^ {2}}{\\sigma_ {i} ^ {2}} - \\frac {m}{2} \\log (2 \\pi) - \\sum_ {i = 1} ^ {m} \\log \\sigma_ {i}, \\tag {14}\n$$\n\n$$\n\n\nwhich is less likely to take extreme values where  $(a_{t} - \\mu)_{i}$  denotes the  $i$ -th element in  $(a_{t} - \\mu)$  and  $\\Sigma = \\mathrm{diag}(\\sigma_1^2,\\dots,\\sigma_m^2)$ . While the only problematic thing is that  $\\Sigma$  might have small eigenvalues, some DRL libraries employ a lower bound to the standard deviation of the policy to avoid singularity issues as shown in Table 5. Furthermore, the numerical stability of vanilla policy gradient is not very sensitive to the dimension of the action space: in equation 14, the magnitude of  $|\\log \\pi_{\\theta}(a_t|s_t)|$  is roughly linear to the dimension  $m$ . Similarly, the magnitude of the integrand  $\\mathcal{L}$  in equation 17 is also approximately linear to the dimension of  $\\mathcal{A}$ , implying that the probability ratio  $p_{\\theta}$  is approximately exponential to the dimension of  $\\mathcal{A}$  and thus much easier to explode as  $m$  increases.\n\n\nIn Table 2, we observe that the vanilla policy gradient does not exhibit any numerical instability when the standard deviations are lower-bounded, further supporting the claim that importance sampling is the primary cause of gradient explosion in deep policy gradient methods. It should be noted that while extremely small standard deviations can still affect numerical stability, this issue can be effectively mitigated by setting a lower bound on the standard deviations, as is done in PPO (though it is not entirely effective).\n\n\nVanilla Policy Gradient with Clipping. Based on the previous analysis, the vanilla policy gradient algorithm is more stable than TRPO/PPO. Motivated by this observation, we propose an objective that combines the strengths of both approaches. This modification results in the following objective:\n\n\n$$\n\n$$\nL ^ {C P G} (\\theta) = \\hat {\\mathbb {E}} _ {\\left(s _ {t}, a _ {t}\\right) \\sim \\pi} \\left[ \\min  \\left(\\log p _ {\\theta} \\hat {A} _ {t} ^ {\\pi}, \\operatorname {c l i p} \\left(\\log p _ {\\theta}, \\log (1 - \\epsilon), \\log (1 + \\epsilon)\\right) \\hat {A} _ {t} ^ {\\pi}\\right)\\right) \\tag {15}\n$$\n\n$$\n\n\nwhere  $\\log p_{\\theta} = \\log \\pi_{\\theta} - \\log \\pi$ . Notably, the new objective can also be interpreted as a logarithmic variant of the PPO loss in equation 11. As shown in Figure 5 and Table 2, this method achieves performance comparable to PPO and outperforms the vanilla policy gradient. Also, when there is no exploding gradient issue, our algorithm can achieve similar performance to PPO. To examine the performance in the general case, we use smaller policy networks with width 64 which are less affected by numerical issues based on our analysis in Section 3, and run both algorithms in three benchmarks. As shown in Figure 4, both achieve similar performance across all three environments, indicating that our algorithm serves as a numerically robust alternative to PPO.\n\n\n![(a) Hopper-v4](images/21ca62d2c9ca779e8122f1ddea3aacf9d1b8c513193f498927b375332d66233e.jpg)\n\n\n![(b) Walker2d-v4 Figure 4: For smaller policy networks that are less affected by gradient explosion, the proposed algorithm achieves performance comparable to PPO on three MuJoCo benchmarks.](images/be65ccf84f4c529bcdbccda3361e9b7ec3d02c740c77b6d1a464bf473d37e5f3.jpg)\n\n\n![(c) Humanoid-v4](images/9d28cdc1d2a8bb33a1e10bf9add76a5aaecff99fca1d40d441b06634b3485de1.jpg)\n\n\n![(a) Hopper-v4](images/d8a4a9efc920bce77427c71b9dfad3a453f9e3aa418c9b71399d2242921753a5.jpg)\n\n\n![(b) Walker2d-v4 Figure 5: For larger policy networks, where PPO completely fails due to gradient explosion, our algorithm avoids numerical issues and continues to produce reasonable results.](images/086c6077f5f1bfb37dcd8997c151d319fc5bcd33f58557aaa06ff698ddd1456a.jpg)\n\n\n![(c) Humanoid-v4](images/8809f256f79cd3b66f7b5d10d68356cff5782c114d6cc72ac3ed996063f584d0.jpg)\n\n\n# 7 CONCLUDING REMARKS\n\n\nIn this work, we demonstrate that ill-conditioned density ratios in importance sampling are the fundamental cause of numerical instability in deep policy gradient methods. While this issue cannot be effectively addressed through optimization constraints alone, code-level techniques are necessary to fully eliminate or mitigate its effects. To address this issue, we find that directly dropping the importance sampling term yields the best performance, and we therefore advocate for further investigation in this direction in future studies.\n\n\nAccording to our analysis, larger neural networks make bigger updates in each optimization step, resulting in larger density ratios in importance sampling and thus increasing the risk of gradient explosion. Therefore, we question whether deep policy gradient methods are numerically stable and robust when optimizing complex policy networks. This is particularly important for the scalability of deep policy gradient methods. Although the experiments in this work focus on MuJoCo continuous-control environments, the limitations discussed are inherent to the algorithms and may apply to other scenarios as well. We believe that a better understanding of these issues can undoubtedly enhance the effectiveness of deep reinforcement learning in real-world applications.\n\n\n# REFERENCES\n\n- A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98): 1-76, 2021.\n\n- M. Andrychowicz, A. Raichuk, P. Stanczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski, S. Gelly, and O. Bachem. What matters for on-policy deep actor-critic methods? A large-scale study. In *ICLR*, 2021.\n\n- Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, 1994.\n\n- A. Bou, M. Bettini, S. Dittert, V. Kumar, S. Sodhani, X. Yang, G. De Fabritiis, and V. Moens. TorchRL: A data-driven decision-making library for pytorch. In ICLR, 2024.\n\n- Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In ICML, 2016.\n\n- L. Engstrom, A. Ilyas, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph, and A. Madry. Implementation matters in deep RL: A case study on PPO and TRPO. In ICLR, 2020.\n\n- Y. Fujita and S. Maeda. Clipped action policy gradient. In ICML, 2018.\n\n- X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, pp. 249-256, 2010.\n\n- T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel, and S. Levine. Soft actor-critic algorithms and applications. In NeurIPS, 2018.\n\n- P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning that matters. In AAAI, pp. 3207-3214, 2018.\n\n- A. Ilyas, L. Engstrom, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph, and A. Madry. A closer look at deep policy gradients. In *ICLR*, 2020.\n\n- S. Khorasani, S. Salehkaleybar, N. Kiyavash, N. He, and M. Grossglauer. Efficiently escaping saddle points for non-convex policy optimization. arXiv preprint arXiv:2311.08914, 2023.\n\n- D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\n- T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n\n- Q. Liu, L. Li, Z. Tang, and D. Zhou. Breaking the curse of horizon: Infinite-horizon off-policy estimation. In NeurIPS, 2018.\n\n- Y. Liu, K. Zhang, Tamer Ba,sar, and W. Yin. An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods. In NeurIPS, pp. 7624-7636, 2020.\n\n- E.N.Lorenz.The Essence of Chaos.University of Washington Press,1995.\n\n- V. Mnih, . Kavukcuoglu, D. Silver, A. A Rusu, J. Veness, M. G Bellemare, A. Graves, M. Riedmiller, A. K Fidjeland, G. Ostrovski, and et al. Human-level control through deep reinforcement learning. Nature, 518:529-533, 2015.\n\n- L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In NeurIPS, pp. 27730-27744, 2022.\n\n- R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In ICML, pp. 1310-1318, 2013.\n\n- G. Philipp, D. Song, and J. G. Carbonell. The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions. arXiv preprint arXiv:1712.05577, 2017.\n\n- A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann. Stable-Baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22 (268):1-8, 2021.\n\n- S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. In ICLR, 2017.\n\n- J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In ICML, pp. 1889-1897, 2015a.\n\n- J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.\n\n- J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\n- D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In ICML, pp. 387-395, 2014.\n\n- D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. Driessche, T. Graepel, and D. Hassabis. Mastering the game of Go without human knowledge. Nature, 550:354-359, 2017.\n\n- R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, pp. 1057-1063, 1999.\n\n- L.N.Trefethen and D.Bau.Numerical Linear Algebra.SIAM,1997.\n\n- O. Vinyals, I. Babuschkin, W.M. Czarnecki, and et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575:350-354, 2019.\n\n- T. Wang, S. Herbert, and S. Gao. Fractal landscapes in policy optimization. In NeurIPS, pp. 4277-4294, 2023.\n\n- W. Wang, Y. Zhu, Y. Zhou, C. Shen, J. Tang, Z. Xu, Y. Peng, and Y. Zhang. Exploring gradient explosion in generative adversarial imitation learning: A probabilistic perspective. In AAAI, 2024.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634679.1524034", "msecs": "152.0", "relativeCreated": "23975.87752342224", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Improving Neural Language Generation with Spectrum Control\n\n# Abstract\nRecent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an anisotropic cone in the vector space, which is called the representation degeneration problem. In this paper, we propose a novel spectrum control approach to address this degeneration problem. The core idea of our method is to directly guide the spectra training of the output embedding matrix with a slow-decaying singular value prior distribution through a reparameterization framework. We show that our proposed method encourages isotropy of the learned word representations while maintains the modeling power of these contextual neural models. We further provide a theoretical analysis and insight on the benefit of modeling singular value distribution. We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model, and various Transformer-based models for machine translation, on common benchmark datasets for these tasks.\n\n\n# 1 INTRODUCTION\n\n\nNeural language generation (NLG) is an important task with many practical applications, such as automatic speech recognition (Graves et al., 2013; Toshniwal et al., 2018), text generation (Bowman et al., 2016; Radford et al., 2019; Keskar et al., 2019), machine translation (Bahdanau et al., 2015; Vaswani et al., 2017) and dialog systems (Gao et al., 2019a; Tang et al., 2019). Most NLG models utilize a complex encoding model to map a given context into a hidden state vector, and then predict the next word distribution by multiplying the encoded vector with the output embedding layer, followed by a softmax layer. In the past few years, it has witnessed a significant progress in NLG by improving the encoding model, from the recurrent neural network (RNN) (Bahdanau et al., 2015; Jozefowicz et al., 2016; Merity et al., 2018a) based models to the current Transformer-based models (Vaswani et al., 2017; Devlin et al., 2019; Dai et al., 2019; Radford et al., 2019). However, embeddings in the softmax output layer have been shown not capable enough to model the conditional probability (Yang et al., 2018).\n\n\nRecently, Gao et al. (2019b) pointed out another limitation of the output embeddings: the representation degeneration problem. They showed that the singular value distribution of the output embedding matrix tends to decay very fast, and the embedding space is squeezed into a narrow cone (as shown in Figure 1(a) and 1(c) in 2-D plots). Such anisotropic shape (Ethayarajh, 2019) is very different from what one would expect from an expressive word embedding space (Arora et al., 2016a; Mu & Viswanath, 2018). Therefore, several efforts (Gao et al., 2019b; Wang et al., 2019a) have been made to address the degeneration problem.\n\n\nUnlike previous approaches that applied implicit regularization to singular values of the output embedding matrix, we propose a novel spectrum control (SC) approach to explicitly control the singular value distribution: we first reparameterize the output embedding matrix  $\\mathbf{W}$  by its singular value decomposition (SVD):  $\\mathbf{W} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ , where  $\\mathbf{U},\\mathbf{V}$  are column orthonormal matrices, and  $\\boldsymbol{\\Sigma}$  is a diagonal matrix of singular values. Then we guide the training of  $\\boldsymbol{\\Sigma}$  by a predefined slow-decaying prior distribution, such as a polynomial decay distribution, or an exponential decay distribution. At the end of training, the distribution of singular values of the embedding matrix gets close to the prior distribution. Our spectrum control approach alleviates the representation degeneration problem by encouraging the diversity of word representations and improving isotropic property of these representations (see Figure 2), even on top of the powerful Transformer-based models.\n\n\n![(a) Transformer-XL](images/ac64bc4b862a767a1e12e3ad5746e66c95ffe8652ed3d69a7f21501bf8762236.jpg)\n\n\n![(b) Singular Values Figure 1: Projected word embeddings $^{1}$  and singular value distributions. (a) and (c): 2-D visualization of word embedding matrices of Transformer-XL for language modeling and Transformer for machine translation; (b) and (d): Normalized singular value distributions of embedding matrices.](images/054f52f50c2204308cc2c44275b5dee1b043c433a0593b2ed58961c49eb9568c.jpg)\n\n\n![(c) Transformer](images/4c00db17561ecfe9b8b65479b68cdec2c81f73634844c320f958ca51e18b6f5f.jpg)\n\n\n![(d) Singular Values](images/3434a9626338ddec8ced32870afed94c7432f7e4160af3ae12fffb7a3862d74a.jpg)\n\n\n![(a) Transformer-XL+SC](images/03ccd35f568b9f17f48bfdee7f08254a9eb1104fe9950abe8f641f5a0982581e.jpg)\n\n\n![(b) Singular Values Figure 2: Projected word embeddings and singular value distributions using our spectrum control method. (a) and (c): 2-D visualization of word embedding matrices of Transformer-XL for language modeling and Transformer for machine translation; (b) and (d): Normalized singular value distributions of embedding matrices.](images/1e1b6053028e7dd8f15fe323190ea4e0a0845bb141a5794ae5073a94b5959b17.jpg)\n\n\n![(c) Transformer+SC](images/b915bb7d43461a73390a90e5c0c8d7d92e47eb906abfc74f049f5fdc332891a7.jpg)\n\n\n![(d) Singular Values](images/eba354725e722319aaf51957ed7361487e9e5d0fb18968870672d78337a59a6d.jpg)\n\n\nWe further present a theoretical analysis to justify our method. The results suggest that it is beneficial to directly guide the singular value distribution of the embedding matrix throughout the training process and control the decay rate of these singular values. We demonstrate the effectiveness of our training framework with extensive experimental results on two tasks: language modeling and machine translation. Our spectrum control method outperforms the latest state-of-the-art TransformerXL model on WikiText-103 dataset for language modeling; and obtains close to 1.5 BLEU improvement on IWSLT 2014 German-English translation task compared to the Transformer baseline model.\n\n\n# 2 RELATED WORK\n\n\nIn this paper, we mainly focus on the output embedding matrix that is used in the softmax output layer for language generation tasks. This embedding matrix is also used as input word embeddings, which is known as weight tying trick. The weight tying not only reduces the number of parameters but also enjoys theoretical benefits (Inan et al., 2017). Thus the weight tying has been successfully applied to many state-of-the-art models for language modeling and machine translation (Merit et al., 2018a; Vaswani et al., 2017; Yang et al., 2018).\n\n\nHowever, there are certain issues with the softmax output layer. Yang et al. (2018) first identified a problem called \"softmax bottleneck\" that the softmax output layer does not have enough capacity to model natural language due to its connection with the rank bottleneck in matrix factorization. A simple and effective method called Mixture of Softmaxes (MoS) was proposed to deal with this issue. Follow-up work in Kanai et al. (2018); Ganea et al. (2019) tried to replace softmax with alternative activation functions. Kanai et al. (2018) proposed to use sigsoftmax, which is composed of a multiplication of an exponential function and sigmoid function. Ganea et al. (2019) proposed a Linear-Monotonic-Softmax (LMS) model that generalized the approach in Kanai et al. (2018) by learning parametric point-wise increasing functions to optimally distort the logits before feeding them into the softmax layer. Pappas & Henderson (2019) instead resorted to a powerful deep residual nonlinear output mapping while using a single softmax function without modifying its dimensionality or rank.\n\n\nAnother line of work focuses on increasing the expressive power of the output embedding matrix by adding a cosine similarity regularization term (Gao et al., 2019b), or adversarial noise (Wang et al., 2019a). Gao et al. (2019b) analyzed the representation degeneration problem that the out-\n\n\nput embeddings tend to degenerate and be distributed into a narrow cone. A regularization term based on the summation of pairwise cosine similarity among all words was proposed to increase the representation power of word embeddings. Wang et al. (2019a) pointed out the computation of regularization term in (Gao et al., 2019b) depends on the size of the vocabulary and hence is costly. Instead, they proposed a simple yet highly effective adversarial training method that adds adversarial noises to the output embedding layer when training the models. They proved in theory that this adversarial training increases the distances between two different words, and thus encourages the diversity of the embedding vectors. Our work follows this line of thought, but takes a different approach: inspired by the spectrum control that encourages slow singular value decay in Generative Adversarial Network (GAN) training (Jiang et al., 2019), we propose a novel method that directly guides the singular values by a slow-decaying prior distribution during the model training process. We show that the anisotropic behavior of the contextualized word representations from powerful Transformer-based models (Ethayarajh, 2019) are alleviated by our spectrum control approach.\n\n\n# 3 PROBLEM SETUP\n\n\nIn this section, we briefly introduce the neural models for language generation, and illustrate the singular value decay phenomena of existing neural language models. We first introduce some notations used in the rest of the paper.\n\n\nNotation: For a  $d$ -dimensional vector  $\\mathbf{x} \\in \\mathbb{R}^d$ , we use  $\\| \\mathbf{x} \\|_q = (\\sum_{i=1}^d |x_i|^q)^{1/q}$ , where  $0 < q < \\infty$  to denote its  $\\ell_q$ -norm, and  $\\| \\mathbf{x} \\|_\\infty = \\max_i |x_i|$  to be its infinity norm. For a matrix  $\\mathbf{A} \\in \\mathbb{R}^{d_1 \\times d_2}$ , let  $\\mathbf{A}_{i*}$  be the  $i$ -th row of  $\\mathbf{A}$ , and we use  $\\| \\mathbf{A} \\|_2$ ,  $\\| \\mathbf{A} \\|_F$ ,  $\\| \\mathbf{A} \\|_1$  to denote its spectral norm, Frobenius norm, and matrix 1-norm. Given two sequences  $\\{a_n\\}$  and  $\\{b_n\\}$ , if there exists a constant  $0 < C < \\infty$  such that  $a_n \\leq C b_n$ , we write  $a_n = O(b_n)$ , and we use  $\\widetilde{O}(\\cdot)$  to hide the logarithmic factors.\n\n\n# 3.1 NEURAL LANGUAGE GENERATION\n\n\nWe first briefly review the softmax output layer typically used in neural language generation models. We define the joint probability of a given length- $n$  sentence of words (tokens)  $\\mathbf{s}_n = (\\mathbf{y}_1, \\dots, \\mathbf{y}_n)$  as the following product of conditional probabilities\n\n\n$$\n\n$$\n\\mathbb {P} \\left(\\mathbf {s} _ {n}\\right) = \\prod_ {t = 1} ^ {n} \\mathbb {P} \\left(\\mathbf {y} _ {t} \\mid \\mathbf {c} _ {t}\\right), \\tag {3.1}\n$$\n\n$$\n\n\nwhere  $\\mathbf{y}_t\\in \\mathcal{V}$  is the  $t$  -th word in sentence  $\\mathbf{s}_n$ , and  $\\mathcal{V}$  represents the word vocabulary,  $\\mathbf{c}_t = \\mathbf{y}_{1:t - 1} = (\\mathbf{y}_1,\\dots ,\\mathbf{y}_{t - 1})$  is referred to as the context of word  $\\mathbf{y}_t$ . In addition, the context  $\\mathbf{c}_t$  is usually modeled by a fixed size vector  $\\mathbf{h}_t\\in \\mathbb{R}^d$ , which is referred to as the hidden state, using some neural networks such as LSTM (Hochreiter & Schmidhuber, 1997) and Transformer (Vaswani et al., 2017). Then, the probability distribution of the output word  $\\mathbf{y}_t$  given the context  $\\mathbf{c}_t$ , i.e.,  $\\mathbb{P}(\\mathbf{y}_t|\\mathbf{c}_t)$  in (3.1), is parameterized as the following softmax function:\n\n\n$$\n\n$$\n\\mathbb {P} \\left(\\mathbf {Y} _ {t} = \\mathbf {y} _ {t} \\mid \\mathbf {c} _ {t}\\right) = \\mathbb {P} \\left(\\mathbf {Y} _ {t} = \\mathbf {y} _ {t} \\mid \\mathbf {h} _ {t}\\right) = \\frac {\\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\mathbf {W} _ {\\mathcal {I} (\\mathbf {y} _ {t}) *}\\right)}{\\sum_ {i = 1} ^ {N} \\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\mathbf {W} _ {i *}\\right)}, \\tag {3.2}\n$$\n\n$$\n\n\nwhere  $\\mathbf{W} \\in \\mathbb{R}^{N \\times d}$  is the weight matrix and is usually tied with the input word embedding matrix (Press & Wolf, 2017; Inan et al., 2017),  $N = |\\mathcal{V}|$  is the vocabulary size,  $d$  is the embedding dimension,  $\\mathcal{I}(\\mathbf{y}_t)$  represents the index of word  $\\mathbf{y}_t$  in vocabulary  $\\mathcal{V}$ . In the following discussion, we call output weight matrix  $\\mathbf{W}$  as the word embedding matrix of the neural language model since it is tied with the input word embedding matrix.\n\n\nIn this paper we focus on the output layer of the neural model for language generation, i.e., the softmax layer in (3.2), as in (Yang et al., 2018; Kanai et al., 2018; Ganea et al., 2019; Gao et al., 2019b). We will examine the singular value distribution of  $\\mathbf{W}$  and propose a different approach to increase the expressive power of the word embedding.\n\n\n# 3.2 FAST SINGULAR VALUE DECAY\n\n\nAs we mentioned before, the singular values of  $\\mathbf{W}$  tend to drop very fast and it may interact with the cone-shaped embedding space. More specifically, Figure 1(b) and 1(d) illustrate the distributions of the normalized singular values of the Transformer-XL based language model $^2$  (Dai et al., 2019)\n\n\nand the Transformer-based machine translation model (Vaswani et al., 2017) trained on WikiText-103 (Merit et al., 2018a) and IWSLT 2014 De-En (Cettolo et al., 2014) datasets, respectively. The plots show a fast singular value decay phenomenon, i.e., there is a huge drop between the first and remaining singular values. Such a phenomenon has also been observed in some previous work (Gao et al., 2019b; Wang et al., 2019a).\n\n\nFigure 1(a) and 1(c) present the distributions of the projected word embeddings of the aforementioned two models. We can see from the plots that the projected word embeddings are distributed into some narrow cone shapes, which implies an anisotropic property of the learned word representations, i.e., the embedding vectors are not uniformly distributed in the space. The detailed analysis in Ethayarajh (2019) also confirms that contextualized word embeddings learnt from ELMo (Peters et al., 2018) (LSTM-based model), BERT and GPT-2 (Devlin et al., 2019; Radford et al., 2019) (Transformer-based models) indeed tend to be anisotropic.\n\n\nThese contextualized word embeddings have shown great success on many NLP tasks. However, static word embeddings, such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) have been shown (Arora et al., 2016b; Mu & Viswanath, 2018) to be isotropic with great expressive power. Hence it may be also beneficial to increase the expressive power of the contextualized word embedding by increasing its isotropy. This motivates us to alleviate the fast singular value decay phenomenon to increase the isotropy of the learned word representations.\n\n\n# 4 PROPOSED METHOD\n\n\nTo alleviate the fast singular value decay phenomenon, we propose to guide the singular value distribution of the contextualized word embedding throughout the training. As a result, we can achieve a trade-off between modeling contextual information, that tends to make word representations anisotropic, and the expressive power of word representations, that tends to be isotropic.\n\n\n# 4.1 SVD REPARAMETERIZATION\n\n\nWe propose to apply singular value decomposition (SVD) based reparameterization to the embedding matrix  $\\mathbf{W}$ , i.e.,  $\\mathbf{W} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ , where  $\\mathbf{U} \\in \\mathbb{R}^{N \\times d}$ ,  $\\mathbf{V} \\in \\mathbb{R}^{d \\times d}$  are column orthonormal matrices, and  $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}$  is a diagonal matrix with  $\\Sigma_{kk} = \\sigma_k$  being the  $k$ -th largest singular value of  $\\mathbf{W}$ . Note that SVD reparameterization is standard and has been widely used in the literature such as model compression (Chen et al., 2018), training DNNs (Zhang et al., 2018), and analyzing word embeddings (Arora et al., 2016c). Given the SVD reparameterization, we can control the singular values of the embedding matrix  $\\mathbf{W}$  by constraining the matrix  $\\mathbf{E}$ , and the conditional distribution in (3.2) can be rewritten as follows:\n\n\n$$\n\n$$\n\\mathbb {P} \\left(\\mathbf {Y} _ {t} = \\mathbf {y} _ {t} \\mid \\mathbf {y} _ {<   t}\\right) = \\frac {\\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\left(\\mathbf {U} \\boldsymbol {\\Sigma} \\mathbf {V} ^ {\\top}\\right) _ {\\mathcal {I} \\left(\\mathbf {y} _ {t}\\right) *}\\right)}{\\sum_ {i = 1} ^ {N} \\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\left(\\mathbf {U} \\boldsymbol {\\Sigma} \\mathbf {V} ^ {\\top}\\right) _ {i *}\\right)} \\text {s u b j e c t t o} \\mathbf {U} ^ {\\top} \\mathbf {U} = \\mathbf {I}, \\mathbf {V} ^ {\\top} \\mathbf {V} = \\mathbf {I}, \\boldsymbol {\\Sigma} \\in \\mathcal {P}, \\tag {4.1}\n$$\n\n$$\n\n\nwhere  $\\mathcal{P}$  is a feasible set to represent the singular value distribution of the embedding matrix  $\\mathbf{W}$ .\n\n\nTo ensure the orthogonal constraints in (4.1), we propose to use the orthogonal regularization for  $\\mathbf{U},\\mathbf{V}$  during the training process. In particular, we use the following regularization, which is a linear combination of Frobenius norm and spectral norm errors:  $\\lambda_1\\| \\mathbf{U}^\\top \\mathbf{U} - \\mathbf{I}\\| _F^2 +\\lambda_2\\| \\mathbf{V}^\\top \\mathbf{V} - \\mathbf{I}\\| _F^2 +\\lambda_3\\| \\mathbf{U}^\\top \\mathbf{U} - \\mathbf{I}\\| _2^2 +\\lambda_4\\| \\mathbf{V}^\\top \\mathbf{V} - \\mathbf{I}\\| _2^2$  where  $\\{\\lambda_i\\}_{i = 1}^4$  are positive regularization parameters.\n\n\n# 4.2 SPECTRUM CONTROL\n\n\nRecall the SVD of  $\\mathbf{W} = \\mathbf{U}\\pmb{\\Sigma}\\mathbf{V}^{\\top}$ , and we consider the following two types of the singular value distribution for  $\\pmb{\\Sigma}$ , which is inspired by the eigenvalue distribution of kernel methods (Wei et al., 2017; Pacchiano et al., 2019):\n\n- - Exponential decay: we say the singular values  $\\{\\sigma_k\\}_{k=1}^d$  of  $\\mathbf{W}$  satisfy the exponential decay if  $\\mathbf{W} \\in \\mathcal{P}_e(\\gamma) = \\{\\mathbf{W} \\in \\mathbb{R}^{N \\times d} \\mid \\sigma_k \\leq c_1 \\exp(-c_2 k^\\gamma), k = 1, \\dots, d, \\gamma > 0, c_1, c_2 > 0$  are universal constants\\}.\n\n- - Polynomial decay: we say the singular values  $\\{\\sigma_k\\}_{k=1}^d$  of  $\\mathbf{W}$  satisfy the polynomial decay if  $\\mathbf{W} \\in \\mathcal{P}_p(\\gamma) = \\{\\mathbf{W} \\in \\mathbb{R}^{N \\times d} \\mid \\sigma_k \\leq c_1 k^{-\\gamma}, k = 1, \\dots, d, \\gamma > 0, c_1 > 0$  is a universal constant\\}.\n\n\nFor  $\\mathcal{P}_e(\\gamma)$  and  $\\mathcal{P}_p(\\gamma)$ , the parameter  $\\gamma$  controls the rate of singular value decay: the larger  $\\gamma$  is, the faster singular value decay will be. To ensure the learned word embedding matrix  $\\mathbf{W}$  have the desired singular value distributions, we propose to add the following regularizations to\n\n\nour training objective:  $\\mathcal{R}_e(\\Sigma) = \\lambda_e\\sum_{k = 1}^d\\left(\\sigma_k - c_1\\exp (-c_2k^\\gamma)\\right)^2$  for exponential decay and  $\\mathcal{R}_p(\\Sigma) = \\lambda_p\\sum_{k = 1}^d\\left(\\sigma_k - c_1k^{-\\gamma}\\right)^2$  for polynomial decay, where  $\\lambda_{e},\\lambda_{p}$  are positive regularization parameters. Although the concept of \"spectrum control\" was previously used in Jiang et al. (2019) to improve the training of GANs, our spectrum control method is trying to solve a totally different problem, i.e., neural language generation, and its motivation is coming from a very different perspective, i.e., the representation degeneration of the word representations. In addition, our method of controlling the singular value with prior distributions is significantly different from the penalty function used in their method. Finally, our method is essential to improve the performance of the neural language generation, while the penalty function proposed in Jiang et al. (2019) can deteriorate the training of neural language models, as we illustrated in Appendix C.\n\n\n# 4.3 THEORETICAL ANALYSIS\n\n\nIn this subsection, we show some theoretical insights on why our proposed method can improve the performance of the NLG model. In particular, we follow the similar setup as considered in (Gao et al., 2019b), i.e., focusing on the optimization of the embedding matrix  $\\mathbf{W} \\in \\mathbb{R}^{N \\times d}$  and assume all the other parameters are fixed and well-optimized. In practice, we use our method to train the models from scratch. Therefore, according to the output layer in (3.2), we consider the following empirical risk minimization problem: given a training dataset  $S = \\{(\\mathbf{h}_i, y_i)\\}_{i=1}^n$  with each example drawn i.i.d. from some unknown but fixed distribution  $\\mathcal{D}$ , and  $\\mathbf{h}_i \\in \\mathbb{R}^d$  as a hidden state,  $y_i \\in \\{1, \\dots, N\\}$  as its associated label, our goal is to minimize the training loss as follows:\n\n\n$$\n\n$$\n\\min  _ {\\mathbf {W} \\in \\mathbb {R} ^ {N \\times d}} L _ {S} (\\mathbf {W}) = \\frac {1}{n} \\sum_ {i = 1} ^ {n} \\ell \\left(\\mathbf {h} _ {i} ^ {\\top} \\mathbf {W}, y _ {i}\\right), \\tag {4.2}\n$$\n\n$$\n\n\nwhere  $\\ell(\\mathbf{h}_i^\\top \\mathbf{W}, y_i)$  is the cross-entropy loss with respect to  $\\mathbf{h}_i^\\top \\mathbf{W}$  and  $y_i$ . The cross-entropy loss defined above is widely used to train NLG models, and is also used to compute the perplexity of the trained model, which is the benchmark criterion to evaluate the performance of language models.\n\n\nIn addition, we define the expected loss as follows  $L_{\\mathcal{D}}(\\mathbf{W}) = \\mathbb{E}[\\ell (\\mathbf{h}^{\\top}\\mathbf{W},y)]$ , where the expectation is taken over the distribution  $\\mathcal{D}$  of the training data.\n\n\nLet  $\\widehat{\\mathbf{W}} = \\arg \\min_{\\mathbf{W} \\in \\mathcal{P}(\\gamma)} L_S(\\mathbf{W})$ , where  $\\mathcal{P}(\\gamma) = \\mathcal{P}_e(\\gamma)$  for exponential decay and  $\\mathcal{P}(\\gamma) = \\mathcal{P}_p(\\gamma)$  for polynomial decay. We assume the right singular vector matrix satisfies  $\\| \\mathbf{V} \\|_1 \\leq V$  for all  $\\mathbf{W} \\in \\mathcal{P}(\\gamma)$ . Now, we are ready to provide the main theory of our method (The proof can be found in Appendix A).\n\n\nTheorem 4.1. Under previously stated conditions, suppose that  $|\\ell(\\cdot)| \\leq B$ ,  $\\ell$  is  $G$ -Lipschitz continuous, and  $\\| \\mathbf{h}_i \\|_{\\infty} \\leq H$  for all  $i = 1, \\ldots, n$ . If we choose  $\\gamma > 1/2$ , then with probability at least  $1 - \\delta$ , we have\n\n\n$$\n\n$$\nL _ {\\mathcal {D}} (\\widehat {\\mathbf {W}}) \\leq \\min  _ {\\mathbf {W} \\in \\mathcal {P} (\\gamma)} L _ {S} (\\mathbf {W}) + \\frac {C _ {1} A N \\left(\\sqrt {\\sum_ {j = 1} ^ {m - 1} \\sigma_ {j} ^ {2}} + \\sqrt {m ^ {1 - 2 \\gamma} / (2 \\gamma - 1)}\\right) + C _ {2} B \\sqrt {\\log (1 / \\delta)}}{\\sqrt {n}}, \\tag {4.3}\n$$\n\n$$\n\n\nwhere  $C_1, C_2$  are absolute constants,  $m \\in [2, d]$ ,  $A = GVH\\sqrt{\\log d}$ .\n\n\nRemark 4.2. According to Theorem 4.1, the expected loss of the learned embedding matrix  $\\widehat{\\mathbf{W}}$  consists of two terms. The first term represents the training loss, the second term is the generalization error gap. Specifically, the smaller the  $\\gamma$ , the larger the feasible set  $\\mathcal{P}(\\gamma)$ , thus the smaller the training loss  $\\min_{\\mathbf{W} \\in \\mathcal{P}(\\gamma)} L_S(\\mathbf{W})$ . On the other hand, the larger the  $\\gamma$ , the faster the singular value decays, and the smaller the generalization error gap. Therefore, our generalization error bound demonstrates an appealing property of our proposed method: by directly controlling the singular value distribution of the learned word embedding, we are able to achieve a trade-off between the training loss and generalization error.\n\n\nRemark 4.3. For the generalization error bound in (4.3), penalizing the largest singular value could reduce this upper bound. It validates the method proposed in Gao et al. (2019b), which implicitly penalizes the largest singular value (c.f. Section 5 in Gao et al. (2019b)). Compared with their method, the error term  $\\widetilde{O}\\left(N\\sqrt{m^{1 - 2\\gamma} / (2\\gamma - 1)} /\\sqrt{n}\\right)$  suggests that by explicitly manipulating the singular value distribution, our method has a better control of the tail sum of the singular values.\n\n\n# 5 EXPERIMENTS\n\n\nWe demonstrate the effectiveness of our proposed spectrum control algorithm on two tasks: language modeling and machine translation. We compare our results with the state-of-the-art models. In our experiments, we try both exponential and polynomial singular value decays, and present the one with the better result. The performances of exponential decay and polynomial decay are very close (See Appendix E). In practice we found that for large scale dataset its better to use polynomial decay and for small scale dataset its better to use exponential decay.\n\n\n# 5.1 LANGUAGE MODELING\n\n\nDatasets We consider two benchmark datasets for language modeling: WikiText-2 and WikiText-103, which consist of pre-processed Wikipedia articles and were introduced by Merity et al. (2018a). WikiText-2 is a small dataset with around 2 million words and 30K vocabulary size, while WikiText-103 is a significantly large dataset with around 103 million words and 260K vocabulary size.\n\n\nModel Configuration On the small WikiText-2 dataset, we implement our method based on the state-of-the-art AWD-LSTM model (Merit et al., 2018a). It is a 3-layer LSTM model with 1150 dimensional hidden states and 400 dimensional embeddings. We also follow the same regularization and optimization procedures introduced in (Merit et al., 2018a). The implementation of our method is based on the open-source code<sup>3</sup> for AWD-LSTM. On the large WikiText-103 dataset, we implement our method based on the state-of-the-art Transformer-XL based models (Dai et al., 2019). We follow the same settings reported in (Dai et al., 2019), and our implementation is based on the official code<sup>4</sup> for Transformer-XL. To evaluate the performance of our method more thoroughly, we consider two Transformer-XL models with different number of layers. The first is the standard Transformer-XL model with 16 layers used in (Dai et al., 2019). For the second, we consider a smaller Transformer-XL model with just 4 layers and other configurations unchanged.\n\n\nParameters For the parameters  $\\{\\lambda_i\\}_{i=1}^4$  of the orthogonal regularizations, we tune them by grid search over  $\\{0.01, 0.1, 1, 10\\}$ . For the parameters  $\\lambda_e, \\lambda_p$  of the spectrum control, we tune them over the grid  $\\{0.1, 1, 10, 100\\}$ . We try different singular value distributions, and the best distributions for AWD-LSTM and Transformer-XL are exponential and polynomial, respectively.\n\n\nResults of the LSTM Model on WikiText-2 We first present the results of language modeling on the small dataset WikiText-2 using LSTM models. In Table 1 we compare the validation/test perplexity<sup>5</sup> of the baseline AWD-LSTM model (Merit et al., 2018a), the cosine similarity regularization (MLE-CosReg) model (Gao et al., 2019b), and the models trained using our method under three different settings (Merit et al., 2018a): without finetune, with finetune and with further continuous cache pointer. Compared with the baselines, our method achieves  $2.3 / 2.9 / 2.3$  test perplexity reduction under all three settings; compared with the MLE-CosReg method, our method achieves  $1.5 / 1.2 / 0.3$  test perplexity reduction under all three settings.\n\n\nResults of the Transformer-XL Model on WikiText-103 We next show the results of language modeling on the large dataset WikiText-103 using Transformer-XL models. Table 2 compares the validation/test perplexity of Transformer-XL based models (Dai et al., 2019) and the models trained by our method on WikiText-103 dataset. The results demonstrate that our method consistently improves upon the small Transformer-XL model (0.9 test perplexity reduction) and the standard Transformer-XL model (0.8 test perplexity reduction).\n\n\nAnalysis We study the output embedding matrix of Transformer-XL trained on WikiText-103 using our method. In particular, we want to evaluate the isotropy of the learned word representations. We consider the partition function  $Z(\\mathbf{a}) = \\sum_{i=1}^{N} \\exp(\\langle \\mathbf{w}_i, \\mathbf{a} \\rangle)$  introduced in (Arora et al., 2016b), where  $\\mathbf{w}_i$  is the  $i$ -th row of the embedding matrix  $\\mathbf{W} \\in \\mathbb{R}^{N \\times d}$  and  $\\mathbf{a} \\in S^{d-1}$  is a unit vector. According to Lemma 2.1 in (Arora et al., 2016b), if the word representation vectors are isotropic,  $Z(\\mathbf{a})$  is close to some constant with high probability for all unit vectors. Thus to empirically measure\n\n\nTable 1: Comparison of different methods in terms of perplexity on WikiText-2 dataset for the task of language modeling.\n\n\n<table><tr><td>Method</td><td>Parameters</td><td>Validation</td><td>Test</td></tr><tr><td colspan=\"4\">Existing results</td></tr><tr><td>Variational LSTM (Inan et al., 2017)</td><td>51M</td><td>91.5</td><td>87.0</td></tr><tr><td>2-layer skip connection LSTM (Mandt et al., 2017)</td><td>24M</td><td>69.1</td><td>65.9</td></tr><tr><td colspan=\"4\">w/o finetune</td></tr><tr><td>AWD-LSTM (Merit et al., 2018a)</td><td>33M</td><td>69.1</td><td>66.0</td></tr><tr><td>MLE-CosReg (Gao et al., 2019b)</td><td>33M</td><td>68.2</td><td>65.2</td></tr><tr><td>Ours</td><td>33M</td><td>66.3</td><td>63.7</td></tr><tr><td colspan=\"4\">+ finetune</td></tr><tr><td>AWD-LSTM (Merit et al., 2018a)</td><td>33M</td><td>68.6</td><td>65.8</td></tr><tr><td>MLE-CosReg (Gao et al., 2019b)</td><td>33M</td><td>67.1</td><td>64.1</td></tr><tr><td>Ours</td><td>33M</td><td>65.3</td><td>62.9</td></tr><tr><td colspan=\"4\">+ continuous cache pointer</td></tr><tr><td>AWD-LSTM (Merit et al., 2018a)</td><td>33M</td><td>53.8</td><td>52.0</td></tr><tr><td>MLE-CosReg (Gao et al., 2019b)</td><td>33M</td><td>51.7</td><td>50.0</td></tr><tr><td>Ours</td><td>33M</td><td>51.1</td><td>49.7</td></tr></table>\n\n\nTable 2: Comparison of different methods in terms of perplexity on WikiText-103 dataset for the task of language modeling.\n\n\n<table><tr><td>Method</td><td>Parameters</td><td>Validation</td><td>Test</td></tr><tr><td colspan=\"4\">Existing results</td></tr><tr><td>4 layer QRNN (Merit et al., 2018b)</td><td>151M</td><td>32.0</td><td>33.0</td></tr><tr><td>Hebbian + Cache (Rae et al., 2018)</td><td>-</td><td>29.7</td><td>29.9</td></tr><tr><td>Small Transformer-XL (Dai et al., 2019)</td><td>120M</td><td>29.6</td><td>30.4</td></tr><tr><td>Ours</td><td>120M</td><td>29.0</td><td>29.5</td></tr><tr><td>Standard Transformer-XL (Dai et al., 2019)</td><td>151M</td><td>23.1</td><td>24.0</td></tr><tr><td>Ours</td><td>151M</td><td>22.9</td><td>23.2</td></tr></table>\n\n\nthe isotropy of the learned word representations, we consider two criteria based on  $Z(\\mathbf{a})$ :\n\n\n$$\n\n$$\nI _ {1} (\\mathbf {W}) = \\frac {\\operatorname* {m i n} _ {\\mathbf {a} \\in \\mathcal {E}} Z (\\mathbf {a})}{\\operatorname* {m a x} _ {\\mathbf {a} \\in \\mathcal {E}} Z (\\mathbf {a})} \\quad \\text {a n d} \\quad I _ {2} (\\mathbf {W}) = \\sqrt {\\frac {\\sum_ {\\mathbf {a} \\in \\mathcal {E}} (Z (\\mathbf {a}) - \\bar {Z} (\\mathbf {a})) ^ {2}}{| \\mathcal {E} | \\bar {Z} (\\mathbf {a}) ^ {2}}},\n$$\n\n$$\n\n\nwhere  $\\mathcal{E}$  is the set of eigenvectors of  $\\mathbf{W}^{\\top}\\mathbf{W}$ , as suggested by Mu & Viswanath (2018). We also propose to check sampled standard deviation measure  $I_{2}(\\mathbf{W})$  (normalized by its average, i.e.,  $\\bar{Z}(\\mathbf{a})$ ). We have  $I_{1}(\\mathbf{W}) \\in [0,1]$  and  $I_{2}(\\mathbf{W}) \\geq 0$ . Larger  $I_{1}(\\mathbf{W})$  and smaller  $I_{2}(\\mathbf{W})$  indicate more isotropic for word embeddings. We uniformly sample 40K words from the vocabulary (around 260K) of WikiText-103 to compute these two criteria. The left half of Table 3 summarizes the values of  $I_{1}(\\mathbf{W})$  and  $I_{2}(\\mathbf{W})$ , which are averaged over 10 runs, for the baseline method and our method. We can see from the results that our method significantly improves the isotropy of the learned word representations in terms of both criteria.\n\n\nTable 3: Comparison of different methods in terms of isotropy for the tasks of language modeling and machine translation. (For perfect isotropy,  $I_{1}(\\mathbf{W}) = 1$ ,  $I_{2}(\\mathbf{W}) = 0$ .)\n\n\n<table><tr><td colspan=\"3\">Language Modeling</td><td colspan=\"3\">Machine Translation</td></tr><tr><td>Method</td><td>I1(W)</td><td>I2(W)</td><td>Method</td><td>I1(W)</td><td>I2(W)</td></tr><tr><td>Standard Transformer-XL</td><td>0.24</td><td>0.037</td><td>Transformer-Base</td><td>0.31</td><td>0.031</td></tr><tr><td>Ours</td><td>0.63</td><td>0.022</td><td>Ours</td><td>0.88</td><td>0.005</td></tr></table>\n\n\nTable 4: Comparison of different methods in terms of BLEU scores on the task of De\u2192En machine translation, trained on IWSLT 2014 dataset.\n\n\n<table><tr><td rowspan=\"2\">IWSLT 2014 De\u2192En</td><td colspan=\"4\">Method</td></tr><tr><td>Adversarial (Wang et al., 2019a)</td><td>Dual-learning (Wang et al., 2019b)</td><td>Transformer-Base (Wang et al., 2019b)</td><td>Ours</td></tr><tr><td>BLEU</td><td>35.18</td><td>35.44</td><td>34.01</td><td>35.50</td></tr></table>\n\n\n# 5.2 MACHINE TRANSLATION\n\n\nWe also apply our spectrum control method to machine translation tasks. Given a source sentence  $s$ , the decoder of an neural machine translation (NMT) model is to predict the next word in the target sentence  $t$  and the previous decoded words in  $t$ . In the following we use the state-of-the-art Transformer-based NMT model as our baseline.\n\n\nDatasets We compare various NMT models on the IWSLT 2014 German\u2192English (De-En) dataset (Cettolo et al., 2014). We follow the same setup as in (Gehring et al., 2017). More specifically, we have 160K sentence pairs as the training data, 7K sentence pairs as the validation data, and we combine tst2010, tst2011, tst2012, dev2010 and dev2012 datasets to form our test data.\n\n\nModel Configuration We implement our method on top of Transformer model (Vaswani et al., 2017). In particular, we consider the Transformer-Base architecture (Vaswani et al., 2017), which has a 6-layer encoder and 6-layer decoder with 512 dimensional hidden states and embeddings, except that we choose the dimension of the inner feed-forward layer as 1024 instead of 2048 and the number of attention heads is set to be 4 rather than 8. Our implementation is based on the open-sourced code provided by Ott et al. (2018). We follow the same procedures as in the language modeling task to choose the regularization parameters. In addition, the best results for this task are from the models where the singular values are controlled by exponential distribution.\n\n\nResults Comparisons of different methods in terms of BLEU scores are summarized in Table 4. Compared with the baseline models, our method improves the BLEU score from 34.01 to 35.50 on the German\u2192English task, close to 1.5 gain on BLEU score; our method is also better than 35.18 reported in (Wang et al., 2019a), and 35.44 reported in (Wang et al., 2019b).\n\n\nAnalysis We also study the learned word embedding matrix of the Transformer trained on IWSLT 2014 De-En using our method in terms of isotropy. The values of  $I_{1}(\\mathbf{W})$  and  $I_{2}(\\mathbf{W})$  are reported in the right half of Table 3, we compute these two values based on all the tokens. It shows that the isotropy of the learned word representations using our method increases significantly in terms of these criteria, from very anisotropic to nearly isotropic. We also demonstrate the projected word embedding matrices and the singular value distributions of different methods in Figure 3. The plots show that the learned word representations from our method are distributed isotropically in the space, which is in contrast to the narrow cone distribution from the baseline method.\n\n\n![(a) Transformer for NMT](images/a3a3e55cd643e2fae8cd9afe73fdf5428e123204a82941a8f82c90127deb9751.jpg)\n\n\n![(b) Transformer for NMT+SC](images/cb27d7792c80b28b706465fbd93e32f3a9c044bdbe576634467d8646fafb3202.jpg)\n\n\n![(c) Singular Value Decay Figure 3: (a) Word embedding for the vanilla Transformer, which has a narrow cone distribution; (b) Word embedding for Transformer using spectrum control, which has a uniform distribution; (c) Normalized singular value for different methods, which shows a slow decay of our method.](images/818c0e945de1b87307fd1f72b37ccc2013c067d4c85430209bc7cff16049e95d.jpg)\n\n\n# 6 CONCLUSIONS AND FUTURE WORK\n\n\nIn this paper, we tackle the degeneration problem that occurs at the output embeddings in the softmax layer used in neural language generation models. We develop a novel spectrum control method to explicitly guide the spectra training of the output embedding matrix with some slow-decaying singular value prior distributions. Our proposed method is shown to alleviate the degeneration problem and improve isotropy of the learned contextualized word representations. Thorough experimental results demonstrate the advantage of our method over the state-of-the-art neural models for language model and machine translation. Since our work is orthogonal to Wang et al. (2019a), it would be interesting to combine the adversarial softmax training with our spectrum control method, and investigate its performance. For the future work, we would also like to investigate how the frequency-agnostic word representations in (Gong et al., 2018) relate to the single value distribution of the output embedding matrix.\n\n\n# REFERENCES\n\n- Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. NeurIPS, 2019.\n\n- Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach to pmi-based word embeddings. TACL, 4:385-399, 2016a.\n\n- Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach to pmi-based word embeddings. TACL, 4:385-399, 2016b.\n\n- Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. 2016c.\n\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICML, 2015.\n\n- Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. JMLR, 2002.\n\n- Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. In Proceedings of the Twentieth Conference on Computational Natural Language Learning, 2016.\n\n- Mauro Cettolo, Jan Niehues, Sebastian St\u00fcker, Luisa Bentivogli, and Marcello Federico. Report on the 11th iwslt evaluation campaign, iwslt 2014. In SLT, 2014.\n\n- Patrick Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-Jui Hsieh. Groupreduce: Block-wise low-rank approximation for neural language model shrinking. In Advances in Neural Information Processing Systems, pp. 10988-10998, 2018.\n\n- Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. ACL, 2019.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In *NAACL*, 2019.\n\n- Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. EMNLP, 2019.\n\n- Octavian-Eugen Ganea, Sylvain Gelly, Gary B\u00e9cigneul, and Aliaksei Severyn. Breaking the softmax bottleneck via learnable monotonic pointwise non-linearities. ICML, 2019.\n\n- Jianfeng Gao, Michel Galley, Lihong Li, et al. Neural approaches to conversational ai. Foundations and Trends\u00ae in Information Retrieval, 13(2-3):127-298, 2019a.\n\n- Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tieyan Liu. Representation degeneration problem in training natural language generation models. In ICLR, 2019b.\n\n- Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In ICML, pp. 1243-1252, 2017.\n\n- Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Frage: frequency-agnostic word representation. In NIPS, 2018.\n\n- A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, 2013.\n\n- Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.\n\n- Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. In ICLR, 2017.\n\n- Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo Zhao. On computation and generalization of generative adversarial networks under spectrum control. In ICLR, 2019.\n\n- Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n\n- Sekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka, and Shuichi Adachi. Sigsoftmax: Reanalysis of the softmax bottleneck. In NIPS, 2018.\n\n- Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional transformer language model for controllable generation. arXiv:1909.05858, 2019.\n\n- Percy Liang. Cs229t/stat231: Statistical learning theory (winter 2016), 2014.\n\n- Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian inference. JMLR, 2017.\n\n- Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. In ICLR, 2018a.\n\n- Stephen Merity, Nitish Shirish Keskar, and Richard Socher. An analysis of neural language modeling at multiple scales. arXiv preprint arXiv:1803.08240, 2018b.\n\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\n- Jiaqi Mu and Pramod Viswanath. All-but-the-top: Simple and effective postprocessing for word representations. In ICLR, 2018.\n\n- Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation, 2018.\n\n- Aldo Pacchiano, Niladri S Chatterji, and Peter L Bartlett. Online learning with kernel losses. ICML, 2019.\n\n- Nikolaos Pappas and James Henderson. Deep residual output layers for neural language generation. In ICML, 2019.\n\n- Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In EMNLP, 2014.\n\n- Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In *NAACL*, 2018.\n\n- Ofir Press and Lior Wolf. Using the output embedding to improve language models. In EACL, 2017.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\n\n- Jack Rae, Chris Dyer, Peter Dayan, and Timothy Lillicrap. Fast parametric learning with activation memorization. In ICML, 2018.\n\n- Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric P. Xing, and Zhiting Hu. Target-guided open-domain conversation. In ACL, 2019.\n\n- Shubham Toshniwal, Anjuli Kannan, Chung-Cheng Chiu, Yonghui Wu, Tara Sainath, and Karen Livescu. A comparison of techniques for language model integration in encoder-decoder speech recognition. In IEEE Workshop on Spoken Language Technology, 2018.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n\n- Dilin Wang, Chengyue Gong, and Qiang Liu. Improving neural language modeling via adversarial training. In ICML, 2019a.\n\n- Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multiagent dual learning. In ICLR, 2019b.\n\n- Yuting Wei, Fanny Yang, and Martin J Wainwright. Early stopping for kernel boosting algorithms: A general analysis with localized complexities. In NIPS, 2017.\n\n- Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. Breaking the softmax bottleneck: A high-rank RNN language model. In *ICLR*, 2018.\n\n- Jiong Zhang, Qi Lei, and Inderjit Dhillon. Stabilizing gradients for deep neural networks via efficient svd parameterization. In International Conference on Machine Learning, pp. 5801-5809, 2018.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634680.335028", "msecs": "335.0", "relativeCreated": "25158.502101898193", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Multi-Label Knowledge Distillation\n\n# Abstract\nExisting knowledge distillation methods typically work by enforcing the consistency of output logits or intermediate feature maps between the teacher network and student network. Unfortunately, these methods can hardly be extended to the multi-label learning scenario. Because each instance is associated with multiple semantic labels, neither the prediction logits nor the feature maps obtained from the whole example can accurately transfer knowledge for each label. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by label decoupling with the one-versus-all reduction strategy; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the proposed method can avoid knowledge counteraction among labels, and achieve superior performance against diverse comparing methods.\n\n\n# 1 INTRODUCTION\n\n\nDespite the remarkable success in training deep neural networks (DNNs) (Krizhevsky et al., 2012), it is hard to deploy these large neural networks on lightweight terminals, e.g., mobile phones, under the constraint of computational resource or requirement of short inference time. To mitigate this issue, knowledge distillation (Hinton et al., 2015) aims to improve the performance of a small network (also known as the student) by requiring the knowledge from a large network (also known as the teacher) to guide the training of the student network. Existing knowledge distillation methods can be roughly divided into two categories: logits-based methods and feature-based methods. The former minimizes the difference between logits of teacher model and student model (Hinton et al., 2015; Zhao et al., 2022), while the latter distills knowledge from feature maps of intermediate layers (Park et al., 2019; Tian et al., 2019; Chen et al., 2021).\n\n\nTypical knowledge distillation methods focus on the multi-class classification task, where each instance is associated with only one class label. However, in many real-world scenarios, an instance inherently contains complex semantics and can be simultaneously assigned with multiple class labels. For example, an image of street scene may be annotated with labels building, car and person. To learn the complex object-label mapping, there is always necessity of training large models to obtain desirable performance in multi-label classification. Unfortunately, due to computational resource constraints, it cannot be allowed to adopt large neural networks in many practical applications, leading to noticeable decrease in model performance (Gou et al., 2021). To alleviate the performance degradation, it is necessary to design specific knowledge distillation methods for multi-label learning. We formalize such a problem as a new learning framework called Multi-Label Knowledge Distillation (MLKD).\n\n\nAlthough knowledge distillation has been proven to be effective for improving the performance of the student network in single-label classification, it is still a challenging problem to directly extend existing KD methods to solve MLKD problems. Specifically, logits-based methods often obtain the predicted probabilities based on the softmax function, which is unavailable for MLKD, since the sum of predicted probabilities may not equal to one in Multi-Label Learning (MLL). Feature-based methods often perform knowledge distillation based on feature maps of the whole image with multiple semantics, which makes the model focus on the major objects while neglect the minor objects. This would lead the model to obtain sub-optimal even undesirable distillation performance. Figure 1 provides empirical validations for these observations.\n\n\n![(a) Models with the same architecture](images/f19190e708504d97ed3128099f7c12e9c7ec22ab86ab96476cbbcf643736c555.jpg)\n\n\n![(b) Models with different architectures Figure 1: Comparison results between our proposed L2D method and conventional KD methods on MS-COCO. We compare our method with the following three baselines: 1) Vanilla: student trained without distillation; 2) Softmax (Hinton et al., 2015): a representative logits-based method by measuring KL divergence on softmax score after logits; 3) ReviewKD (Chen et al., 2021): a feature-based method that achieves sota performance. The red dashed lines mark the performance of teachers. Conventional KD methods achieve unfavorable performance similar to the student, while our method significantly outperforms these methods and achieves comparable performance to the teacher. Details about the implementation of this experiment can be found in Appendix A.](images/8eea145e1dc78c1c6a262d374b1f7762f2d7f4f3908017810b858a491913b47f.jpg)\n\n\nIn this paper, to perform multi-label knowledge distillation, we propose a new method consisting of multi-label logits distillation and label-wise embedding distillation (L2D for short). Specifically, to exploit informative semantic knowledge compressed in the logits, L2D employs the one-versus-all reduction strategy to obtain a series of binary classification problems and perform logits distillation for each one. To enhance the distinctiveness of learned feature representations, L2D encourages the student model to maintain a consistent structure of intra-class and intra-instance (inter-class) label-wise embeddings with the teacher model. By leveraging the structural information of the teacher model, these two structural consistencies respectively enhance the compactness of intra-class embeddings and dispersion of inter-class embeddings for the student model.\n\n\nOur main contributions can be summarized as follows:\n\n- - A general learning framework called MLKD is proposed. To our best knowledge, the framework is the first study specially designed for knowledge distillation in the multi-label learning scenario.\n\n- - A new approach called L2D is proposed. It performs multi-label logits distillation and label-wise embedding distillation simultaneously. The former provides informative semantic knowledge while the latter encourages the student model to learn more distinctive feature representations.\n\n- - Extensive experimental results on benchmark datasets demonstrate the effectiveness of our proposed method.\n\n\n# 2 RELATED WORK\n\n\nThe concept of knowledge distillation (KD) proposed by Hinton et al. (2015) defines a learning framework that transfers knowledge from a large teacher network to a small student network. Existing works can be roughly divided into two groups, i.e., logits-based methods and feature-based methods. Logits-based methods mainly focus on designing effective distillation losses to distill knowledge from logits and softmax scores after logits. DML (Zhang et al., 2018) introduces a mutual learning method to train both teachers and students simultaneously. TAKD (Mirzadeh et al., 2020) proposes a new architecture called \"teacher assistant\", which is an intermediate-sized network bridging the gap between teachers and students. Besides, a recent study (Zhao et al., 2022) proposes a novel logits-based method to reformulate the classical KD loss into two parts and achieves the state-of-the-art performance by adjusting weights for these two parts. Some other methods focus on distilling knowledge from intermediate feature layers. FitNet (Romero et al., 2014) is the first approach to distill knowledge from intermediate features by measuring the distance between feature maps. Attention transfer (Zagoruyko & Komodakis, 2016a) achieves better performance than FitNet\n\n\nby distilling knowledge from the attention maps. PKT (Passalis & Tefas, 2018) measures the KL divergence between features by treating them as probability distributions. RKD (Park et al., 2019) utilizes the relations among instances to guide the training process of the student model. CRD (Tian et al., 2019) incorporates contrastive learning into knowledge distillation. ReviewKD (Chen et al., 2021) proposes a review mechanism which uses multiple layers in the teacher to supervise one layer in the student. ITRD (Miles et al., 2021) aims to maximize the correlation and mutual information between the students' and teachers' representations.\n\n\nMulti-label learning has increasingly attracted a lot of interest recently. Existing solutions for solving MLL problems can be categorized into three directions. The first type attempts to design novel loss functions for tackling the intrinsic positive-negative imbalance issue in multi-label classification tasks. For example, ASL (Ridnik et al., 2021a) uses different weights to re-weight positive and negative examples for the balanced training. The second type focuses on modeling the label correlations, which provides prior knowledge for multi-label classification. Among them, MLGCN (Chen et al., 2019b) is a representative method that employs a graph convolutional network to model correlation matrix. CADM (Chen et al., 2019a) constructs a similar graph based on class-aware maps. To handle the multiple semantic objects contained in an image, the last type of methods aims to locate areas of interest related to semantic labels by using attention techniques. Among them, C-Tran (Lanchantin et al., 2021) first utilizes the transformer to retrieve embeddings from visual features for each label. Query2Label (Liu et al., 2021a) uses several stacked transformer encoders to identify interesting areas. ML-Decoder (Ridnik et al., 2021b) simplifies transformer encoders in Query2Label. ADDS (Xu et al., 2022b) introduces encoders from CLIP (Radford et al., 2021) in order to get better textual and visual embedding inputs to the classification head. In addition, ADDS adds a multi-head cross-attention layer and a skipping connection from the query input to the query output based on ML-Decoder.\n\n\nSeveral previous studies have applied KD techniques to improve the performance of MLL. For example, Liu et al. (2018) and Xu et al. (2022a) simply minimized mean squared error (MSE) loss between teacher logits and student logits. Song et al. (2021) designed a partial softmax function by combining a positive label with all other negative labels. Then, the conventional KD loss can be computed for each positive label. The main difference between our work and previous works is that we aim to improve the distillation performance instead of the performance of MLL. This can be reflected in experiments in Section 4 and Appendix B, where the proposed method mainly compares with KD methods while the previous method mainly compares with MLL methods.\n\n\n# 3 THE PROPOSED APPROACH\n\n\nLet  $\\pmb{x} \\in \\mathcal{X}$  be an instance and  $\\pmb{y} \\in \\mathcal{Y}$  be its corresponding label vector, where  $\\mathcal{X} \\subset \\mathbb{R}^d$  is the input space with  $d$  dimensions and  $\\mathcal{Y} \\subset \\{0,1\\}^q$  is the target space with  $q$  class labels. We further use  $y_j$  to denote the  $j$ -th component of  $\\pmb{y}$ . For a given instance  $\\pmb{x}$ ,  $y_j = 1$  indicates the  $j$ -th label is relevant to the instance;  $y_j = 0$ , otherwise. In multi-label learning, each instance may be assigned with more than one label, which means  $\\sum_{j=1}^{q} \\mathbb{I}(y_j = 1) \\geq 1$ , where  $\\mathbb{I}(\\cdot)$  is the indicator function. We also denote by  $[q]$  the integer set  $\\{1,2,\\dots,q\\}$ .\n\n\nIn this paper, we use a classification model consisting of three components, i.e., a visual backbone  $f$ , which extracts a feature map  $f(\\pmb{x})$  for the input  $\\pmb{x}$ , a label-wise embedding encoder  $g$  (Lanchantin et al., 2021; Liu et al., 2021a), which produces a label-wise embedding  $e_k = g_k(f(\\pmb{x}))$  with respect to the  $k$ -th class based on the feature map  $f(\\pmb{x})$ , and a multi-label classifier  $h$ , which predicts multi-label probabilities  $\\hat{\\pmb{y}} = [\\sigma(h_1(e_1)), \\sigma(h_2(e_2)), \\dots, \\sigma(h_q(e_q))]$ , where  $\\sigma(\\cdot)$  denotes the sigmoid function. It is noteworthy that the used model is very general, which can be built by equipping commonly used backbones, e.g., ResNet (He et al., 2016), with a label-wise embedding encoder  $g$ . For the notations mentioned above, we use the superscripts  $\\mathcal{T}$  (or  $S$ ) to denote the teacher (or student) model. For example, we use  $e_k^T$  and  $e_k^S$  to denote the label-wise embeddings for teacher and student models.\n\n\nIn multi-label learning, a popular method is to employ the one-versus-all reduction strategy to transform the original task into multiple binary problems. Among the various loss functions, the most commonly used one is the binary cross entropy (BCE) loss. Specifically, given a batch of examples\n\n\n![Figure 2: An illustration of the L2D framework. The framework simultaneously performs multi-label logits distillation and label-wise embedding distillation to improve the performance of the student model.](images/3aac4bb2349429772ac122d178545b82a5989e70f1e03e86fe96473b48c9979a.jpg)\n\n\n$\\{(\\pmb{x}_i,\\pmb{y}_i)\\}_{i = 1}^b$  and the predicted probabilities  $\\hat{\\pmb{y}}$ , the BCE loss can be defined as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {B C E}} = - \\frac {1}{b} \\sum_ {i = 1} ^ {b} \\sum_ {k = 1} ^ {q} y _ {i k} \\log \\left(\\hat {y} _ {i k}\\right) + (1 - y _ {i k}) \\log \\left(1 - \\hat {y} _ {i k}\\right). \\tag {1}\n$$\n\n$$\n\n\nFigure 2 illustrates the distillation process of the proposed L2D framework. For a batch of training examples, we feed them into the teacher/student model to obtain the label-wise embeddings and predicted probabilities. In order to train the student model, besides the BCE loss, we design the following two distillation losses: 1) multi-label logits distillation loss  $\\mathcal{L}_{\\mathrm{MLD}}$  to exploit informative semantic knowledge compressed in the logits, 2) label-wise embedding distillation loss  $\\mathcal{L}_{\\mathrm{LED}}$  to leverage structural information for enhancing the distinctiveness of learned feature representations. The overall objective function can be presented as\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {L 2 D}} = \\mathcal {L} _ {\\mathrm {B C E}} + \\lambda_ {\\mathrm {M L D}} \\mathcal {L} _ {\\mathrm {M L D}} + \\lambda_ {\\mathrm {L E D}} \\mathcal {L} _ {\\mathrm {L E D}}, \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\lambda_{\\mathrm{MLD}}$  and  $\\lambda_{\\mathrm{LED}}$  are two balancing parameters.\n\n\n# 3.1 MULTI-LABEL LOGITS DISTILLATION\n\n\nTraditional logits-based distillation normally minimizes the Kullback-Leibler (KL) divergence between the predicted probabilities, i.e., the logits after the softmax function, of teacher and student model. However, the method cannot be directly applied to the MLL scenario, since it depends on a basic assumption that the predicted probabilities of all classes should sum to one, which hardly holds for MLL examples.\n\n\nTo mitigate this issue, inspired by the idea of one-versus-all reduction, we propose a multi-label logits distillation (MLD) loss, which decomposes the original multi-label task into multiple binary classification problems and minimizes the divergence between the binary predicted probabilities of two models. Formally, the MLD loss can be formulated as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {M L D}} = \\frac {1}{b} \\sum_ {i = 1} ^ {b} \\sum_ {k = 1} ^ {q} \\mathcal {D} \\left(\\left[ \\hat {y} _ {i k} ^ {\\mathcal {T}}, 1 - \\hat {y} _ {i k} ^ {\\mathcal {T}} \\right] \\mid \\mid \\left[ \\hat {y} _ {i k} ^ {\\mathcal {S}}, 1 - \\hat {y} _ {i k} ^ {\\mathcal {S}} \\right]\\right), \\tag {3}\n$$\n\n$$\n\n\nwhere  $[\\cdot, \\cdot]$  is an operator used to concatenate two scalars into a vector, and  $\\mathcal{D}$  is a divergence function. The most common choice is the KL divergence  $\\mathcal{D}_{\\mathrm{KL}}(P\\|Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\left(\\frac{P(x)}{Q(x)}\\right)$ , where  $P$  and  $Q$  are two different probability distributions. The MLD loss aims to improve the performance of student model by sufficiently exploiting informative knowledge from logits.\n\n\n# 3.2 LABEL-WISE EMBEDDING DISTILLATION\n\n\nThe MLD loss performs distillation on the predicted probabilities that can be regarded as a high-level representation, i.e., the final outputs of model. The knowledge distilled from the teacher model\n\n\n![Figure 3: An illustration of class/instance-aware label-wise embedding distillation. Class-aware label-wise embedding distillation (CD) captures structural relations among intra-class label-wise embeddings from different examples, while instance-aware label-wise embedding distillation (ID) explores structural relations among intra-instance (inter-class) label-wise embeddings.](images/fc91b8a5684cb5236c7dad483996d6e68e9365330e16f8de842aedff6512faab.jpg)\n\n\nby only using the MLD loss would be insufficient to train a student model with desirable performance due to the limited information carried by the logits. To further strengthen the effectiveness of distillation, we design the label-wise embedding distillation (LED) loss, which aims to explore the structural knowledge from label-wise embeddings. The main idea is to capture two types of structural relations among label-wise embeddings: 1) class-aware label-wise embedding distillation (CD) loss  $\\mathcal{L}_{\\mathrm{LED - CD}}$ , which captures the structural relation between any two intra-class label-wise embeddings from different examples; 2) instance-aware label-wise embedding distillation (ID) loss  $\\mathcal{L}_{\\mathrm{LED - ID}}$ , which models the structural relation between any two inter-class label-wise embeddings from the same example. In the following content, we introduce these two distillation losses in detail.\n\n\n# 3.2.1 CLASS-AWARE LABEL-WISE EMBEDDING DISTILLATION\n\n\nClass-aware label-wise embedding distillation aims to improve the distillation performance by exploiting the structural relations among intra-class label-wise embeddings. Generally, the same semantic objects from two different images often differ from each other by their individual characteristics, such as two cars with different colors and diverse styles (see the left side in Figure 3). Since our goal is to distinguish between car and other semantic classes instead of identifying different cars, these distinctiveness would be confusing information for the corresponding classification task. Due to the powerful learning capacity, the large model is able to capture the highly abstract semantic representations for each class label by neglecting the useless individual information. From the perspective of learned feature representations, as shown in the left side of Figure 3, the teacher model tends to obtain a more compact structure of intra-class label-wise embedding, which often leads to better classification performance. By transferring the structural knowledge from the teacher model to the student model, CD encourages the student model to enhance the compactness of intra-class label-wise embeddings, which can improve its classification performance.\n\n\nFor a batch of examples, let  $\\{e_{ik}^{\\mathcal{T}}\\}_{i=1}^{b}$  and  $\\{e_{ik}^{\\mathcal{S}}\\}_{i=1}^{b}$  respectively denote the intra-class label-wise embeddings with respect to class  $k \\in [q]$  generated by the teacher and student models. Then, we can capture the structural relation between any two intra-class label-wise embeddings  $e_{ik}^{\\mathcal{T}}$  and  $e_{jk}^{\\mathcal{T}}$\n\n\nby measuring their distance in the embedding space:\n\n\n$$\n\n$$\n\\phi_ {\\mathrm {C D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {j k} ^ {\\mathcal {T}}\\right) = \\left\\{ \\begin{array}{l l} \\| \\boldsymbol {e} _ {i k} ^ {\\mathcal {T}} - \\boldsymbol {e} _ {j k} ^ {\\mathcal {T}} \\| _ {2} & y _ {i k} = 1, y _ {j k} = 1, \\\\ 0 & \\text {o t h e r w i s e .} \\end{array} \\right. \\tag {4}\n$$\n\n$$\n\n\nIt is worth to note that we only consider the structural relation between any two valid label-wise embeddings, i.e., the embeddings with respect to positive labels. Similar to Eq.(4), for any two intra-class label-wise embeddings  $e_{ik}^{S}$  and  $e_{jk}^{S}$ , we can obtain the structural relation  $\\phi_{\\mathrm{CD}}(e_{ik}^{S}, e_{jk}^{S})$  for the student model.\n\n\nBy enforcing the teacher and student structural relations to maintain the consistency for each pair of intra-class label-wise embeddings, we can achieve the class-aware structural consistency as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {L E D - C D}} = \\sum_ {k = 1} ^ {q} \\sum_ {i, j \\in [ b ]} \\ell \\left(\\phi_ {\\mathrm {C D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {j k} ^ {\\mathcal {T}}\\right), \\phi_ {\\mathrm {C D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {S}}, \\boldsymbol {e} _ {j k} ^ {\\mathcal {S}}\\right)\\right), \\tag {5}\n$$\n\n$$\n\n\nwhere  $\\ell$  is a function to measure the consistency between the teacher and student structural relations. In experiments, we use the following Huber loss function as a measurement:\n\n\n$$\n\n$$\n\\ell (a, b) = \\left\\{ \\begin{array}{l l} \\frac {1}{2} (a - b) ^ {2} & | a - b | \\leq 1, \\\\ | a - b | - \\frac {1}{2} & \\text {o t h e r w i s e .} \\end{array} \\right. \\tag {6}\n$$\n\n$$\n\n\nwhere  $a$  and  $b$  are two different structural relations.\n\n\n# 3.2.2INSTANCE-AWARE LABEL-WISE EMBEDDING DISTILLATION\n\n\nInstance-aware label-wise embedding distillation (ID) aims to improve the distillation performance by exploring the structural relations among inter-class label-wise embeddings from the same image. Generally, one can hardly distinguish between two different semantic objects occurring in an image due to the high similarities they share. For example, see the right side of Figure 3, for an image annotated with sky, sea and beach, one can hardly distinguish between the semantic objects sky and sea, since they share the same color and similar texture. A feasible solution is to exploit other useful information, such as their spatial relation, i.e., sky is always above sea. Due to the powerful learning capacity, the large model is able to distinguish between the similar semantic objects by exploiting such implicit supervised information. From the perspective of learned feature representations, as shown in the right side of 3, the teacher model tends to learn a dispersed structure of inter-class label-wise embedding, which is beneficial for improving its discrimination ability. By distilling the structural knowledge from the teacher model, ID enforces the student model to enhance the dispersion of inter-class label-wise embeddings, which can improve its discrimination ability.\n\n\nFor a given instance  $\\boldsymbol{x}_i$ , let  $\\{e_{ik}^{\\mathcal{T}}\\}_{k=1}^{q}$  and  $\\{e_{ik}^S\\}_{k=1}^{q}$  respectively denote the label-wise embeddings generated by teacher and student models. Then, we can capture the structural relation between any two inter-class label-wise embeddings  $e_{ik}^{\\mathcal{T}}$  and  $e_{il}^{\\mathcal{T}}$  by measuring their distance in the embedding space:\n\n\n$$\n\n$$\n\\phi_ {\\mathrm {I D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {i l} ^ {\\mathcal {T}}\\right) = \\left\\{ \\begin{array}{l l} \\| \\boldsymbol {e} _ {i k} ^ {\\mathcal {T}} - \\boldsymbol {e} _ {i l} ^ {\\mathcal {T}} \\| _ {2} & y _ {i k} = 1, y _ {i l} = 1, \\\\ 0 & \\text {o t h e r w i s e .} \\end{array} \\right. \\tag {7}\n$$\n\n$$\n\n\nNote that in Eq.(7), we only consider the structural relation between any two valid label-wise embeddings, i.e., the embedding with respect to positive labels. Similar to Eq.(7), for any two inter-class label-wise embeddings  $e_{ik}^S$  and  $e_{il}^S$ , we can obtain the structural relation  $\\phi_{\\mathrm{ID}}(e_{ik}^S, e_{il}^S)$  for the student model.\n\n\nBy encouraging the teacher and student model to maintain the consistent structure of intra-instance label-wise embeddings, we can minimize the following  $\\mathcal{L}_{\\mathrm{LED - ID}}$  loss to achieve the instance-aware structural consistency where  $\\ell (.)$  is Huber loss as defined in Eq.(6):\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {L E D - I D}} = \\sum_ {i = 1} ^ {b} \\sum_ {k, l \\in [ q ]} \\ell \\left(\\phi_ {\\mathrm {I D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {i l} ^ {\\mathcal {T}}\\right), \\phi_ {\\mathrm {I D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {S}}, \\boldsymbol {e} _ {i l} ^ {\\mathcal {S}}\\right)\\right). \\tag {8}\n$$\n\n$$\n\n\nFinally, the overall objective function of L2D (Eq.(2)) can be re-written as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {L 2 D}} = \\mathcal {L} _ {\\mathrm {B C E}} + \\lambda_ {\\mathrm {M L D}} \\mathcal {L} _ {\\mathrm {M L D}} + \\lambda_ {\\mathrm {L E D - C D}} \\mathcal {L} _ {\\mathrm {L E D - C D}} + \\lambda_ {\\mathrm {L E D - I D}} \\mathcal {L} _ {\\mathrm {L E D - I D}}, \\tag {9}\n$$\n\n$$\n\n\nwhere  $\\lambda_{\\mathrm{MLD}}$ ,  $\\lambda_{\\mathrm{LED - CD}}$  and  $\\lambda_{\\mathrm{LED - ID}}$  are all balancing parameters.\n\n\nTable 1: Results on MS-COCO where teacher and student models are in the same architectures.\n\n\n<table><tr><td>Teacher</td><td colspan=\"3\">RepVGG-A2</td><td colspan=\"3\">ResNet-101</td><td colspan=\"3\">WRN-101</td><td colspan=\"3\">Swin-S</td></tr><tr><td>Student</td><td colspan=\"3\">RepVGG-A0</td><td colspan=\"3\">ResNet-34</td><td colspan=\"3\">WRN-50</td><td colspan=\"3\">Swin-T</td></tr><tr><td>Metrics</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td></tr><tr><td>Teacher</td><td>72.71</td><td>74.11</td><td>68.63</td><td>73.62</td><td>73.89</td><td>68.61</td><td>74.70</td><td>75.56</td><td>70.73</td><td>81.70</td><td>80.48</td><td>77.12</td></tr><tr><td>Student</td><td>70.02</td><td>72.49</td><td>66.77</td><td>70.31</td><td>72.49</td><td>66.82</td><td>74.45</td><td>75.43</td><td>70.61</td><td>79.59</td><td>79.18</td><td>75.42</td></tr><tr><td>RKD</td><td>70.08</td><td>72.39</td><td>66.73</td><td>70.13</td><td>72.44</td><td>66.78</td><td>74.70</td><td>75.71</td><td>70.84</td><td>79.63</td><td>79.19</td><td>75.57</td></tr><tr><td>PKT</td><td>70.11</td><td>72.47</td><td>66.80</td><td>70.43</td><td>72.64</td><td>66.68</td><td>74.54</td><td>75.47</td><td>70.58</td><td>79.64</td><td>79.09</td><td>75.39</td></tr><tr><td>ReviewKD</td><td>70.00</td><td>72.35</td><td>66.82</td><td>70.39</td><td>72.62</td><td>66.76</td><td>74.03</td><td>75.29</td><td>70.36</td><td>79.81</td><td>79.18</td><td>75.55</td></tr><tr><td>MSE</td><td>70.26</td><td>72.54</td><td>66.99</td><td>70.54</td><td>72.75</td><td>66.85</td><td>74.53</td><td>75.60</td><td>70.71</td><td>79.67</td><td>79.20</td><td>75.52</td></tr><tr><td>PS</td><td>70.65</td><td>72.89</td><td>67.60</td><td>70.86</td><td>72.66</td><td>67.12</td><td>75.12</td><td>76.05</td><td>71.63</td><td>79.96</td><td>79.64</td><td>76.20</td></tr><tr><td>MLD</td><td>70.74</td><td>72.81</td><td>67.46</td><td>70.68</td><td>72.69</td><td>67.19</td><td>74.92</td><td>75.75</td><td>71.21</td><td>80.11</td><td>79.68</td><td>76.44</td></tr><tr><td>L2D</td><td>72.81</td><td>74.59</td><td>69.49</td><td>72.87</td><td>74.45</td><td>69.43</td><td>76.61</td><td>77.08</td><td>72.79</td><td>81.59</td><td>81.03</td><td>77.86</td></tr></table>\n\n\nTable 2: Results on MS-COCO where teacher and student models are in the different architectures.\n\n\n<table><tr><td>Teacher</td><td colspan=\"3\">ResNet-101</td><td colspan=\"3\">Swin-T</td><td colspan=\"3\">ResNet-101</td><td colspan=\"3\">Swin-T</td></tr><tr><td>Student</td><td colspan=\"3\">RepVGG-A0</td><td colspan=\"3\">ResNet-34</td><td colspan=\"3\">MobileNet v2</td><td colspan=\"3\">MobileNet v2</td></tr><tr><td>Metrics</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td></tr><tr><td>Teacher</td><td>73.62</td><td>73.89</td><td>68.61</td><td>79.43</td><td>78.77</td><td>75.07</td><td>73.62</td><td>73.89</td><td>68.61</td><td>79.43</td><td>78.77</td><td>75.07</td></tr><tr><td>Student</td><td>70.02</td><td>72.49</td><td>66.77</td><td>70.31</td><td>72.49</td><td>66.82</td><td>71.85</td><td>73.59</td><td>68.26</td><td>71.85</td><td>73.59</td><td>68.26</td></tr><tr><td>RKD</td><td>70.08</td><td>72.35</td><td>66.72</td><td>70.00</td><td>72.34</td><td>66.64</td><td>71.76</td><td>73.68</td><td>68.40</td><td>71.74</td><td>73.68</td><td>68.37</td></tr><tr><td>PKT</td><td>69.99</td><td>72.35</td><td>66.56</td><td>70.26</td><td>72.39</td><td>66.82</td><td>71.88</td><td>73.60</td><td>68.35</td><td>71.84</td><td>73.76</td><td>68.37</td></tr><tr><td>ReviewKD</td><td>70.00</td><td>72.33</td><td>66.62</td><td>70.29</td><td>72.39</td><td>66.58</td><td>71.92</td><td>73.73</td><td>68.48</td><td>71.73</td><td>73.71</td><td>68.36</td></tr><tr><td>MSE</td><td>70.07</td><td>72.50</td><td>66.85</td><td>70.33</td><td>72.57</td><td>66.72</td><td>71.91</td><td>73.68</td><td>68.28</td><td>71.80</td><td>73.74</td><td>68.38</td></tr><tr><td>PS</td><td>70.30</td><td>72.61</td><td>67.10</td><td>70.94</td><td>72.93</td><td>67.57</td><td>72.11</td><td>73.89</td><td>68.42</td><td>72.42</td><td>74.14</td><td>68.94</td></tr><tr><td>MLD</td><td>70.48</td><td>72.77</td><td>67.10</td><td>71.14</td><td>72.99</td><td>67.63</td><td>72.17</td><td>73.84</td><td>68.52</td><td>72.35</td><td>74.10</td><td>68.91</td></tr><tr><td>L2D</td><td>72.14</td><td>74.08</td><td>68.78</td><td>73.42</td><td>74.97</td><td>70.20</td><td>73.24</td><td>74.85</td><td>69.72</td><td>74.21</td><td>75.72</td><td>70.87</td></tr></table>\n\n\n# 4 EXPERIMENTS\n\n\nDatasets. We perform experiments on three benchmark datasets Pascal VOC2007 (Everingham et al., 2015) (VOC for short), MS-COCO2014 (Lin et al., 2014) (MS-COCO for short) and NUS-WIDE (Chua et al., 2009). VOC contains 5,011 images in the train-val set, and 4,952 images in the test set. It covers 20 common objects, with an average of 1.6 labels per image. MS-COCO contains 82,081 training images and 40,137 test images. It covers 80 common objects, with an average of 2.9 labels per image. NUS-WIDE contains 161,789 training images and 107,859 test images. It covers 81 visual concepts, with an average of 1.9 labels per image.\n\n\nMetrics. Following existing works (Zhang & Zhou, 2013; Liu et al., 2021a; Sun et al., 2022), we adopt the mean average precision (mAP) over all classes, overall F1-score (OF1) and average per-class F1-score (CF1) to evaluate the performance. We choose OF1 and CF1, since they consider both recall and precision and thus are more comprehensive.\n\n\nMethods. To validate the proposed method, we compare it with the following KD methods: RKD (Park et al., 2019), which captures the relations among instances to guide the training of the student model; PKT (Passalis & Tefas, 2018), which measures KL divergence between features by treating them as probability distributions; ReviewKD (Chen et al., 2021), which transfers knowledge across different stages instead of just focusing on features in the same levels; MSE (Xu et al., 2022a), which minimizes the MSE loss between logits of teacher and student model; PS (Song et al., 2021), which minimizes KL divergence of logits after a partial softmax function.\n\n\nImplementation Details. We use the models pretrained on ImageNet (Deng et al., 2009) as the backbones. We resize the resolution of all images to  $224 \\times 224$  and set the batch size as 64. For each training image, we adopt a weak augmentation consisting of random horizontal flipping and a strong augmentation consisting of Cutout (Devries & Taylor, 2017) and RandAugment Cubuk et al. (2020). We use the Adam optimization method (Kingma & Ba, 2015) to train the model for 80 epochs. The one-cycle policy is used with a maximal learning rate of 0.0001 and the weight decay (Loshchilov & Hutter, 2018) of 0.0001. For all experiments, we set  $\\lambda_{\\mathrm{MLD}} = 10$ ,  $\\lambda_{\\mathrm{LED-CD}} = 100$ , and  $\\lambda_{\\mathrm{LED-ID}} = 1000$ . Parameter sensitivity analysis in Appendix D shows that the performance of\n\n\n![Figure 4: Comparison results of the comparing methods on VOC in terms of AP and mAP  $(\\%)$  where the backbones of teacher and student model are respectively ResNet-50 and ResNet-18.](images/815ee5fde3e1930a3dc20789856e9d2a4070e1872c8094a7427757a554e80b2a.jpg)\n\n\nL2D are insensitive to all of our balancing parameters. For the comparing methods, we set their parameters as suggested in the original papers. Especially, for all feature-based methods, we just deploy them on the feature maps which is output from the visual backbone  $f$ . All the experiments are conducted on GeForce RTX 2080 GPUs. More details about the used models and implementation of the label-wise embedding encoder are attached in Appendix A.\n\n\n# 4.1 COMPARISON RESULTS\n\n\nTable 1 and Table 2 report comparison results on MS-COCO with the same and different architectures of student and teacher models. From Table 1, it can be observed that: 1) Conventional feature-based distillation methods only achieve minor improvements in performance when compared with the student model (without distillation). This indicates these methods do not work in multi-label scenarios due to their disability to capture multiple semantics occurred in feature maps. 2) MLD can outperform conventional feature-based distillation methods in most cases, which indicates by performing one-versus-all reduction, the logits-based distillation can be adapted into multi-label knowledge distillation. 3) The proposed L2D significantly outperforms all other methods and achieves comparable performance with the teacher model, which convincingly validates the effectiveness of the proposed label-wise embedding distillation. From Table 2, we can see: 1) Compared with the same architecture, the performance gap between teacher and student model is larger for different architectures, which indicates the corresponding distillation task is harder. 2) Our method significantly outperforms all the comparing methods by a significant margin in all cases. 3) L2D achieves the best performance in all cases and significantly outperforms MLD. These results provide a strong empirical validation for the effectiveness of the proposed method.\n\n\nFigure 4 illustrates the performance of the proposed methods and other comparing methods on VOC in terms of AP and mAP. It is noteworthy that the performance is ranked in descending according to the performance of student model. From the figure, it can be observed that: 1) Our proposed L2D achieves the best performance and significantly outperforms the comparing methods in terms of mAP. 2) L2D consistently achieves superior performance to the comparing methods in most classes and the performance gap is large especially for classes that the student model achieves poor performance. This observation discloses that L2D improves the performance of hard classes by enhancing the distinctiveness of feature representations. These experimental results demonstrate the practical usefulness of the proposed method. More results on VOC and NUS-WIDE can be found in Appendix B and Appendix C.\n\n\n# 4.2 ABLATION STUDY\n\n\nIn this section, to further analyze how the proposed method improves distillation performance, Table 3 reports results of ablation studies on MS-COCO (teacher: ResNet-101, student: ResNet-34) and VOC (teacher: ResNet-50, student: ResNet-18). The first line in Table 3 reports the performance of student model without knowledge distillation. It can be observed that the distillation performance is improved slightly by only conducting multi-label logits distillation. Compared with MLD, label-wise embeddings distillation achieves the major improvement for the proposed method. It can\n\n\nTable 3: Ablation studies on MS-COCO and VOC.\n\n\n<table><tr><td colspan=\"3\"></td><td colspan=\"3\">MS-COCO</td><td colspan=\"3\">Pascal VOC 2007</td></tr><tr><td>MLD</td><td>CD</td><td>ID</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td></tr><tr><td></td><td></td><td></td><td>70.31</td><td>72.49</td><td>66.82</td><td>84.01</td><td>83.60</td><td>79.42</td></tr><tr><td>\u2713</td><td></td><td></td><td>70.68</td><td>72.69</td><td>67.19</td><td>84.48</td><td>84.07</td><td>80.29</td></tr><tr><td>\u2713</td><td>\u2713</td><td></td><td>71.91</td><td>73.74</td><td>68.76</td><td>85.65</td><td>85.64</td><td>81.94</td></tr><tr><td>\u2713</td><td></td><td>\u2713</td><td>71.79</td><td>73.62</td><td>68.43</td><td>84.59</td><td>84.19</td><td>80.24</td></tr><tr><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>72.87</td><td>74.45</td><td>69.43</td><td>85.71</td><td>85.70</td><td>82.11</td></tr></table>\n\n\n![(a) Vanilla](images/d1f0114043661588281e56adabc3a7f07e5c07ecd238fe740dd2877829ba3029.jpg)\n\n\n![(b) PKT Figure 5: The differences between correlation matrices of student and teacher predicted probabilities on MS-COCO. The student distilled by our method shows significant matching between student's and teacher's correlations compared to others.](images/6270e730612b0212bd711e9f655031c90ed12f996ed053b587e1b58e4052ef8e.jpg)\n\n\n![(c)MLD](images/7810745b4c3ccc7a22fd8ee4f3532aeb7c16bf2e7fb0a55c4a65ebed7ec91a5c.jpg)\n\n\n![(d) L2D](images/f332ad196cfd7cef24131b81982c14cc1022b94d9004b8bdd121d8b918934cd7.jpg)\n\n\nbe observed that by performing CD and ID, the mAP performance achieves  $1.6\\%$  and  $1.48\\%$  increments, respectively. Finally, we also examine the combination of these techniques. By incorporating these components together, the fusing method achieves the best performance and significantly outperforms each other method. These results demonstrate that all of three components are of great importance to the performance of the proposed L2D.\n\n\n# 4.3 DISTILLING INTER-CLASS CORRELATIONS\n\n\nAs discussed in the previous works (Hinton et al., 2015), the conventional supervised learning losses, e.g., BCE loss, often neglects the correlations among class predictions. However, the label correlation is a foundational element for multi-label classification. Distillation losses utilize \"soft targets\", which can effectively distill such correlation information from the teacher model, leading to a desirable distillation performance. To validate whether the correlations can be captured by L2D effectively, Figure 5 illustrates the differences between correlation matrices of student and teacher predicted probabilities on MS-COCO. From the figure, it can be observed that: 1) Without knowledge distillation, the teacher and student correlations are very different, which indicates teacher model tends to capture a more precise correlation than student model and thus achieves better performance. 2) The representative comparing method PKT shows a large difference, which discloses that the conventional KD methods are ineffective to capture the correlations in multi-label scenarios. 3) MLD shows a reduced difference, which indicates that the rich informativeness of logits is beneficial for capturing precise correlations. 4) L2D shows significant matching between the teacher and student correlations. By enhancing the distinctiveness of label-wise embeddings, L2D can obtain more correct predicted probabilities, leading to a more precise correlation estimation.\n\n\n# 5 CONCLUSION\n\n\nThe paper studies the problem of multi-label knowledge distillation. In the proposed method, the multi-label logits distillation explores the informative semantic knowledge compressed in the teacher logits to obtain more semantic supervision. Furthermore, the label-wise embedding distillation exploits the structural knowledge from label-wise embeddings to learn more distinctive feature representations. Experimental results on benchmark datasets validate the effectiveness of the proposed method. In future, we plan to improve the performance of MLKD by exploiting other abundant structural information.\n\n\n# REFERENCES\n\n- Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge review. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5008-5017, 2021.\n\n- Zhao-Min Chen, Xiu-Shen Wei, Xin Jin, and Yanwen Guo. Multi-label image recognition with joint class-aware map disentangling and label correlation embedding. In Proceedings of the IEEE International Conference on Multimedia and Expo, pp. 622-627, 2019a.\n\n- Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen Guo. Multi-label image recognition with graph convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5177-5186, 2019b.\n\n- Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yan-Tao Zheng. Nus-wide: A real-world web image database from national university ofsingapore. In Proceedings of ACM Conference on Image and Video Retrieval, 2009.\n\n- Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In Advances in Neural Information Processing Systems, 2020.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2009.\n\n- Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.\n\n- Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pp. 13733-13742, 2021.\n\n- Mark Everingham, SM Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The Pascal visual object classes challenge: A retrospective. International Journal of Computer Vision, pp. 98-136, 2015.\n\n- Fran\u00e7ois-Guillaume Fernandez. Torchcam: class activation explorer. https://github.com/frgfmd/torch-cam, 2020.\n\n- Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. International Journal of Computer Vision, pp. 1789-1819, 2021.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.\n\n- Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\n- Peng-Tao Jiang, Chang-Bin Zhang, Qibin Hou, Ming-Ming Cheng, and Yunchao Wei. Layercam: Exploring hierarchical class activation maps for localization. IEEE Transactions on Image Processing, pp. 5875-5888, 2021.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.\n\n- Jack Lanchantin, Tianlu Wang, Vicente Ordonez, and Yanjun Qi. General multi-label image classification with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16478-16488, 2021.\n\n- Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr D\u00f3lar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision, pp. 740-755, 2014.\n\n- Shilong Liu, Lei Zhang, Xiao Yang, Hang Su, and Jun Zhu. Query2label: A simple transformer way to multi-label classification. arXiv preprint arXiv:2107.10834, 2021a.\n\n- Yongcheng Liu, Lu Sheng, Jing Shao, Junjie Yan, Shiming Xiang, and Chunhong Pan. Multi-label image classification via knowledge distillation from weakly-supervised detection. In Proceedings of the 26th ACM International Conference on Multimedia, pp. 700-708, 2018.\n\n- Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012-10022, 2021b.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.\n\n- Roy Miles, Adri\u00e1n L\u00f3pez Rodr\u00edguez, and Krystian Mikolajczyk. Information theoretic representation distillation. arXiv preprint arXiv:2112.00459, 2021.\n\n- Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 5191-5198, 2020.\n\n- Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3967-3976, 2019.\n\n- Nikolaos Passalis and Anastasios Tefas. Learning deep representations with probabilistic knowledge transfer. In Proceedings of the European Conference on Computer Vision, pp. 268-284, 2018.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning, pp. 8748-8763, 2021.\n\n- Tal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi Zelnik-Manor. Asymmetric loss for multi-label classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 82-91, 2021a.\n\n- Tal Ridnik, Gilad Sharir, Avi Ben-Cohen, Emanuel Ben-Baruch, and Asaf Noy. MI-decoder: Scalable and versatile classification head. arXiv preprint arXiv:2111.12933, 2021b.\n\n- Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.\n\n- Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. *Mobilenetv2: Inverted residuals and linear bottlenecks*. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 4510-4520, 2018.\n\n- Liangchen Song, Jialian Wu, Ming Yang, Qian Zhang, Yuan Li, and Junsong Yuan. Handling difficult labels for multi-label image classification via uncertainty distillation. In Proceedings of the 29th ACM International Conference on Multimedia, pp. 2410-2419, 2021.\n\n- Feng Sun, Ming-Kun Xie, and Sheng-Jun Huang. A deep model for partial multi-label image classification with curriculum based disambiguation. arXiv preprint arXiv:2207.02410, 2022.\n\n- Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In International Conference on Learning Representations, 2019.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.\n\n- Jiazhi Xu, Sheng Huang, Fengtao Zhou, Luwen Huangfu, Daniel Zeng, and Bo Liu. Boosting multi-label image classification with complementary parallel self-distillation. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, 2022a.\n\n- Shichao Xu, Yikang Li, Jenhao Hsiao, Chiuman Ho, and Zhu Qi. A dual modality approach for (zero-shot) multi-label classification. arXiv preprint arXiv:2208.09562, 2022b.\n\n- Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016a.\n\n- Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British Machine Vision Conference, 2016b.\n\n- Min-Ling Zhang and Zhi-Hua Zhou. A review on multi-label learning algorithms. IEEE Transactions on Knowledge and Data Engineering, pp. 1819-1837, 2013.\n\n- Ying Zhang, Tao Xiang, Timothy M Hospedales, and Hutchuan Lu. Deep mutual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4320-4328, 2018.\n\n- Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. Decoupled knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11953-11962, 2022.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634681.481346", "msecs": "481.0", "relativeCreated": "26304.82006072998", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations\n\n# Abstract\nReconstructing visual images from fMRI data presents a challenging task, particularly when dealing with limited data and compute availability. This work introduces a novel approach to fMRI-based visual image reconstruction using a subject-agnostic common representation space. We show that subjects' brain signals naturally align in this common space during training, without the need for explicit alignment. This is leveraged to demonstrate that aligning subject-specific adapters to a reference subject is significantly more efficient than traditional end-to-end training methods. Our approach excels in low-data scenarios, where training the adapter with limited data achieves faster and better performance. We also introduce a novel method to select the most representative subset of images for a new subject, allowing for fine-tuning with 40\\% less data while maintaining performance. These advancements make fMRI data collection more efficient and practical, reducing the burden on subjects and improving the generalization of fMRI reconstruction models.\n\n\n# 1 INTRODUCTION\n\n\nOver the past several decades, the use of machine learning techniques has enabled decoding and/or reconstruction of information represented in neural activation patterns measured in awake, behaving humans. Because the neural representations supporting behavior likely entail patterns of activity across large swaths of tissue, these approaches are thought to offer a key step towards characterizing and quantifying the computational principles brains use to support cognition (Naselaris et al., 2011).\n\n\nEarly approaches, including self-supervised learning frameworks (Beliy et al., 2019; Gaziv et al., 2022; Mozafari et al., 2020; Shen et al., 2019; Seeliger et al., 2018; Ren et al., 2021) struggled to generate semantically accurate image reconstructions, often resulting in images with limited resemblance to the original stimuli. Furthermore, these methods required large amounts of data for training, and their generalization to a wider population of subjects was severely limited. Significant advancements were achieved by incorporating multimodal data, such as text (Lin et al., 2022), and later by leveraging the generative power of latent diffusion models (LDMs) (Takagi & Nishimoto, 2023; Scotti et al., 2023; Lu et al., 2023; Xia et al., 2024). These models introduced novel mapping techniques to project fMRI signals into the input space of LDMs, yielding higher-quality natural images. However, despite the improved fidelity, the issue of generalization to new participants persisted. In response, Scotti et al. (2024) proposed projecting fMRI data into a shared representation space, demonstrating significant improvements in both performance and generalizability.\n\n\nBeyond being able to use decoding and reconstruction approaches to provide evidence of the existence of information represented in a particular brain activation pattern, an additional goal is to compare decoding or reconstruction performance across experimental manipulations, such as cued visual attention (Serences & Boynton, 2007; Kamitani & Tong, 2005; Scolari et al., 2012; Sprague & Serences, 2013; Itthipuripat et al., 2019; Sprague et al., 2018) and visual working memory (Serences et al., 2009; Harrison & Tong, 2009; Ester et al., 2013; Sprague et al., 2014; Christophel et al., 2012; Li et al., 2021) However, for these approaches to be useful for a cognitive neuroscience lab, the lab would need to acquire sufficient data to train a decoding or reconstruction model for each individual participant (often  $>20$  per study to attain statistical power). The state-of-the-art methods described above use extremely massive datasets with dozens of hours of fMRI data per participant, which is not feasible in almost all cases. Thus, there is a clear need to maximize the efficiency and generalizability\n\n\nof image reconstruction methods such that a model for a new participant can be trained using as little data as possible.\n\n\nIn this work, we introduce an efficient and generalizable approach to fMRI-based visual reconstruction that leverages a subject-agnostic common representation space. Unlike previous approaches that perform end-to-end training of fMRI signals to mapped contextual embeddings, we propose aligning brain activity from new subjects to a pre-trained reference subject in a shared representation space using Adapter Alignment (AA) training. Throughout this work, we refer to lightweight subject-specific modules designed to map input fMRI signals to a shared representation space as \"adapters\"(Liu et al., 2024). These modules are implemented as single-layer neural networks. In their linear form, adapters consist of a single linear transformation layer, while in the non-linear form, they include a linear layer followed by a non-linear activation function such as GELU (Hendrycks & Gimpel, 2023). Using AA training, we significantly reduce the amount of fMRI data required to achieve accurate reconstructions and improve the reconstruction quality. Our method achieves better performance to existing state-of-the-art models, but with far fewer training hours, making it feasible for cognitive neuroscience labs with limited data collection resources. We build on this approach by also proposing an adaptive image selection strategy, allowing for optimal fine-tuning of new subjects with minimal data, further improving the practicality and accessibility of fMRI-based image reconstruction.\n\n\nOur contributions to this work are threefold:\n\n- - We provide strong evidence for the existence of a common visual representation space across subjects, where brain activity from different individuals can be aligned in a shared space. By extracting and analyzing embeddings from a pre-trained model, we show that even without explicit alignment, subject-specific fMRI signals, when mapped to a common space display similar behavior and representation structures. Furthermore, we demonstrate that applying a simple orthogonal transformation improves the alignment of embeddings between subjects, and using non-linear adapters with explicit mapping leads to almost perfect alignment across subjects in the common space. (Section 2)\n\n- - We introduce Adapter Alignment (AA), a novel approach to facilitate efficient subject-specific alignment in fMRI-based visual reconstruction. Our training pipeline first pre-trains on a reference subject, then aligns new subjects' brain data using lightweight adapters that map them to the shared common space. This process, which is more efficient than traditional end-to-end training, enables faster convergence and better performance, particularly in low-data scenarios. By incorporating MSE losses at the adapter level for common images, we show that our method not only reduces training time but also improves generalization and reconstruction quality. (Section 3)\n\n- - We develop a greedy image selection algorithm that identifies the most representative subset of images for training new subjects. By efficiently selecting fMRI signals that capture the most informative aspects of the common space, this strategy significantly reduces the amount of data required for fine-tuning without sacrificing performance (from a baseline of 250 images to 150 images). This makes our method more practical for real-world applications, where acquiring large fMRI datasets is often prohibitively expensive. (Section 4)\n\n\n# 2 A COMMON VISUAL REPRESENTATION SPACE\n\n\nCentral to our approach is the hypothesis that each individual's brain (or at least the tissue involved in processing static visual images) can be considered an instantiation of a 'common' brain, with activity patterns varying along similar dimension in a common shared representational space. Aspects of this hypothesis are already strongly supported: most human brains have localized tissue responsive to specific types of stimuli, like faces and scenes. Additionally, several continuous dimensions of high-level visual selectivity have been identified across the ventral temporal cortex (Huth et al., 2012), including dimensions like animate/inanimate, large/small (based on real-world size) (Long et al., 2018), and spiky/round (Bao et al., 2020). Retinotopic organization is another example of a shared representational space, as each individual brain instantiates a unique anatomical mapping from retinal coordinates to cortical representations, and adaptive models can be estimated to transform activation patterns from individual brains to this common shared retinotopic coordinate system (Sprague & Serences, 2013; Wandell & Winawer, 2015). We analyze the similarity of representation spaces\n\n\n<table><tr><td>Dimension 1</td><td>Animals (giraffes, zebras, and elephants), nature, and outdoor scenes</td></tr><tr><td>Dimension 2</td><td>Cats and food</td></tr><tr><td>Dimension 3</td><td>Kite flying, airplanes, and flying objects</td></tr><tr><td>Dimension 4</td><td>Surfing, water sports, beach, and ocean scenes</td></tr><tr><td>Dimension 5</td><td>Airplanes, flying objects, outdoor activities and events, transportation scenes</td></tr></table>\n\n\nTable 1: Concept analysis of images along principal dimensions.\n\n\nfrom different perspectives of common concepts, visual analysis, and eigenvectors in the following subsections. Finally, we reflect on the different ways of aligning the common representations.\n\n\n# 2.1 EMERGENCE OF COMMON CONCEPTS\n\n\nThe existence of a semantic common representation space is suggested from the emergence of semantic categories in this space. For this, we extracted the most significant left singular vectors from the ridge regression weight matrix. This matrix was extracted from the pre-trained MindEye2 (Scotti et al., 2024) model which was trained using the 1024-dimensional common space. Subsequently, we projected the 1024-dimensional embeddings of each image onto these vectors and sampled from the two extremes (set 1 and set 3) and the middle (set 2). In order to obtain an objective classification, we presented these sets of images to GPT-4o (Achiam et al., 2023) and asked it to find the semantic categories that are well represented in set 1, somewhat represented in set 2 and not represented in set 3. Results for the first five singular vectors are shown in Table 1. As is evident, images in the principal dimensions of the representation space have a common theme.\n\n\nPrevious studies, such as those by Huth et al. (2012), have indicated that animal categories, including humans, are represented distinctly from non-animal categories within the brain. In our study, we observed that the common representational space also preserves these semantic properties. As demonstrated in Figure 1, after projecting the fMRI signals of Subject 2 into the common space and onto the first two singular vectors, we can see the animal vs. non-animal distinction, consistent with previous hypotheses. Extending this analysis across additional dimensions, we observed similar separations between other hypothesized semantic categories of \"Social interaction,\" \"man-made vs nature,\" and \"biological vs non-biological.\"\n\n\n![Figure 1: Visualization of theAnimate/Inanimate semantic category on the two principal singular vectors. Each point (embedding) is colored based on whether its corresponding image contains 'objects' that fall into theAnimate category (red), Inanimate category (blue) or both categories (green).](images/aceb003e4bbbb907ac8ac8753fba27d87d056a52418260fb80892ff654a4dcee.jpg)\n\n\nWe also analyze the images in the representation space visually. We present our results and analysis in the Appendix Section D.\n\n\n# 2.2 SIMILARITY IN REPRESENTATION STRUCTURE ACROSS SUBJECTS\n\n\nExisting works, such as MindEye2 (Scotti et al., 2024), employ a simple linear adapter network to project subject-specific fMRI signals into a common representation space. A key indication of the existence of such a shared space is the structural similarity of embeddings across subjects. To explore this, we first use a pre-trained MindEye2 model to extract common-space embeddings for the shared images (the test set in MindEye2) across all subjects. Initial cosine similarity between the embeddings from different subjects is low. However, after applying an orthogonal transformation using Procrustes analysis (Sch\u00f6nemann, 1966), the cosine similarity improves, suggesting that the embeddings can be structurally aligned. Despite this improvement, the alignment is not perfect, likely due to the limitations of linear transformations.\n\n\n![Figure 2: Cosine Similarity of shared image embeddings in common space. (a) The alignment is weak before an aligning orthogonal transformation is applied. (b) The alignment improves after an orthogonal transformation. For each row in this heatmap, the embedding spaces of subjects are transformed orthogonally to align with the corresponding row's subject. (c) Near perfect alignment with nonlinear adapters and explicit alignment. We only consider subjects 1,2,5 and 7 in this plot as we worked with 4 subjects only when training our pipeline.](images/fbfa5b3b456d0a58e1f258d0b4a107407b4d58aebc6911a4adf44fa7bf9b5c06.jpg)\n\n\n![](images/2aa41a62aa36f445bd016bce3b6d5b52973989cf17f4f3f61bfeb25b88d404eb.jpg)\n\n\n![](images/5dd64f82c2ee5f4f979c99cd98bfea0f58569d09e09d9e0e2568119890c1fdcf.jpg)\n\n\nAs a preliminary experiment, we pre-trained a single subject model with a non-linear adapter and explicitly mapped subsequent subjects to this reference subject using subject-specific non-linear adapters. This approach results in near-perfect alignment in the common space. Although existing methods do not explicitly align subject-specific representations, we observed significant semantic and structural similarities across subjects' representations. Furthermore, it is possible to map one subject's representations to another's using non-linear adapters. While this alignment in the common space is nearly perfect, it does not directly translate to better reconstruction performance.\n\n\nThis evidence supports the core hypothesis of our work: it is possible to construct an aligned, subject-agnostic common space. In Section 3, we empirically test this hypothesis by mapping a new subject to a reference subject during training and evaluating its reconstruction performance. Additional analysis is provided in Appendix C.\n\n\n# 3 FMRI RECONSTRUCTION\n\n\nBuilding a common brain representation requires constructing an efficient fMRI reconstruction pipeline. Our approach is guided by two primary objectives: 1) achieving competitive reconstruction performance and 2) prioritizing efficiency over marginal performance gains. We hypothesize that it is possible to pre-train a single reference brain and align subsequent subjects to this reference using subject-specific adapters, significantly reducing the computational cost while maintaining high reconstruction quality.\n\n\n# 3.1 DATASET\n\n\nWe use the Natural Scenes Dataset (NSD) (Allen et al., 2022) but apply a different split compared to the traditional setup. Typically, common images across subjects are assigned to the test set, while the\n\n\ntraining set consists of unique images viewed by each subject. However, for effective Common Space Alignment, it is critical to have a one-to-one mapping of images across subjects. To achieve this, we incorporate the common images into the training set and swap an equal amount of training images to the test sets. This means that every subject now has a unique randomly sampled set of images on which they are tested. We present the performance of the original training method and the proposed technique on the new split to ensure a fair comparison across subjects and alignment techniques.\n\n\nOur initial experiments revealed that models performed slightly worse on the unique image test set than on the common image test set, suggesting that the unique image test set presents a more challenging benchmark. Similar to prior work, we focus on subjects 1, 2, 5, and 7, as they have complete 40-hour fMRI datasets available in NSD, enabling better reconstruction performance. These subjects provide robust data for training and evaluation, allowing us to validate our alignment and reconstruction methods comprehensively.\n\n\n# 3.2 ADAPTER ALIGNMENT\n\n\n![Figure 3: Adapter Alignment Procedure. The Reference Subject is trained end-to-end first. The common space embeddings of the shared images from the reference subject are used as ground truth to train the adapters of subsequent subjects.](images/67929ce7549af003d6115c3b4ffe0f12751cbb0bf3a91632f8698599708534a6.jpg)\n\n\nCurrent approaches for training multi-subject pipelines with a common representation space typically follow an end-to-end strategy, where fMRI inputs are processed by the model and the loss is computed at the final layer to minimize the discrepancy between the ground-truth and predicted embeddings. In this setup, the common space emerges organically during training without explicit constraints. However, this approach has two key drawbacks: (1) it requires extended training to form a satisfactory common space and achieve convergence, and (2) the subject-specific embeddings in the common space are not strongly aligned, making it harder for the residual MLP to map multiple subject inputs to a unified output.\n\n\nTo address these challenges, we propose Adapter Alignment (AA) training. In AA, the pipeline is first pre-trained end-to-end on a single reference subject, chosen based on reconstruction performance and data availability. Among NSD subjects, Subject 1 consistently exhibited the best reconstruction performance, making it the ideal candidate for constructing the reference space. After pre-training, the output embeddings for the subject-specific adapter, particularly those corresponding to the shared images across subjects, are extracted.\n\n\nWhen fine-tuning a new subject, we adopt a two-step process. First, we train the new subject's adapter using the shared images, with the objective of aligning its output to the embeddings of Subject 1 for the same images. The adapter is allowed to overfit on these shared images until the loss is minimized. In the second step, we unfreeze the entire pipeline and resume training, applying the usual losses at the final output. Additionally, we introduce an MSE loss at the adapter level specifically for the common images, which allows the adapter to restructure itself to account for subject-specific discrepancies and adapt to the unique images encountered by the new subject. We call this process of overfitting the adapter and letting it fine-tune end to end \"AAMax\".\n\n\nThis training method offers improvements in both high-data and low-data scenarios. When all training data is available, AAMax converges faster while achieving performance similar to traditional end-to-end training. However, in limited data settings, AAMax proves significantly more efficient, yielding better reconstructions and higher accuracy with fewer training samples.\n\n\n# 3.3 RECONSTRUCTION PIPELINE\n\n\n![Figure 4: fMRI Reconstruction Pipeline. The high level pipeline maps data to CLIP ViT-L-14 when using Versatile Diffusion and OpenCLIP ViT/bigG-14 when using Stable Diffusion XL. The Adapter Alignment Procedure is explained in more detail in Section 3.2](images/25b3519a15c4de70a40254ad25e705ba28462f8b8f54c25d5cefb160b7f730fd.jpg)\n\n\nOur reconstruction pipeline follows a similar structure to previous works, dividing the reconstruction process into two components: a high-level semantic pipeline that maps to CLIP embeddings and a low-level visual pipeline that maps to visual autoencoder embeddings. Both pipelines receive the same fMRI data as input but are trained separately. The outputs from both pipelines are then integrated into a pre-trained diffusion model to generate the final reconstruction.\n\n\nFlattened fMRI voxels from the nsdgeneral region are fed into a subject-specific adapter, consisting of a single linear layer with a GELU non-linearity. This step maps the fMRI data into a subject-agnostic shared space. The shared space representation is then processed by a Residual Multi-Layer Perceptron (MLP). For mappings into the CLIP (Radford et al., 2021) embedding space, we further incorporate a pre-trained diffusion prior based on DALL-E 2 (Ramesh et al., 2022) to bridge the modality gap between fMRI data and CLIP embeddings.\n\n\nOur high-level pipeline is based on the architecture used in MindEye1 (Scotti et al., 2023) and MindEye2 (Scotti et al., 2024), where the fMRI data is mapped to CLIP image embeddings of the corresponding source images. The pretraining phase leverages a combination of diffusion prior loss and CLIP loss. During fine-tuning, we add an additional MSE loss for the common images to further refine the mappings. The overall loss function in this stage is:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {n e w}} = \\left\\{ \\begin{array}{l l} \\lambda_ {1} \\mathcal {L} _ {\\mathrm {P r i o r}} + \\lambda_ {2} \\mathcal {L} _ {\\mathrm {C L I P}}, & \\text {f o r n o n - c o m m o n i m a g e s} \\\\ \\lambda_ {1} \\mathcal {L} _ {\\mathrm {P r i o r}} + \\lambda_ {2} \\mathcal {L} _ {\\mathrm {C L I P}} + \\lambda_ {3} \\mathcal {L} _ {\\mathrm {M S E}} (\\mathbf {z} _ {\\mathrm {n e w}}, \\mathbf {z} _ {\\mathrm {r e f}}), & \\text {f o r c o m m o n i m a g e s} \\end{array} \\right.\n$$\n\n$$\n\n\nwhere  $\\mathbf{z}_{\\mathrm{adapter, new}}$  and  $\\mathbf{z}_{\\mathrm{adapter, ref}}$  are the adapter-level embeddings for the new and reference subjects, respectively, on the shared common images. We present a breakdown of the high level pipeline loss functions in Appendix H.\n\n\nThe low-level pipeline also follows a similar structure to the high-level one but maps the fMRI data to latent embeddings derived from a custom autoencoder. This autoencoder is trained on downscaled images from the NSD dataset (64x64x3) and reduces them to a latent space of size 1024. The low-level pipeline learns to map to this latent space, and passing these embeddings through a pre-trained decoder produces blurry reconstructions. The loss function for this stage is:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {n e w}} = \\left\\{ \\begin{array}{l l} \\mathcal {L} _ {\\text {M S E}} (\\mathbf {z} _ {\\text {f M R I , n e w}}, \\mathbf {z} _ {\\text {A E}}), & \\text {f o r n o n - c o m m o n i m a g e s} \\\\ \\mathcal {L} _ {\\text {M S E}} (\\mathbf {z} _ {\\text {f M R I , n e w}}, \\mathbf {z} _ {\\text {A E}}) + \\mathcal {L} _ {\\text {M S E}} (\\mathbf {z} _ {\\text {a d a p t e r , n e w}}, \\mathbf {z} _ {\\text {a d a p t e r , r e f}}), & \\text {f o r c o m m o n i m a g e s} \\end{array} \\right.\n$$\n\n$$\n\n\nwhere  $\\mathbf{z}_{\\mathrm{fMRI, new}}$  is the output embedding for the new subject,  $\\mathbf{z}_{\\mathrm{AE}}$  is the AE latent space representation of the image and  $\\mathbf{z}_{\\mathrm{adapter, new}}$  and  $\\mathbf{z}_{\\mathrm{adapter, ref}}$  are the adapter-level embeddings for the new and reference subjects, respectively. We present a breakdown of the low level pipeline loss functions in Appendix I.\n\n\nFor the final step, the mapped CLIP embedding is used as a conditioning input to the pre-trained diffusion prior model. The low-level blurry reconstructions serve as an initialization for the diffusion model, and the final, refined reconstructions are then produced. For more architectural details, we refer readers to the MindEye1 (Scotti et al., 2023) and MindEye2 (Scotti et al., 2024)papers.\n\n\n# 3.4 TESTS ON COMPLETE DATA\n\n\nWe first validate our training method using the full dataset. Subject 1 is used as the reference subject, while fine-tuning is performed on Subjects 2, 5, and 7. The results are presented in Table 2. We use the same set of metrics as Scotti et al. (2024) to evaluate our reconstructions. Pixel Correlation, SSIM and 2-way percent correct for the 2nd and 5th layer of AlexNet are considered low level visual metrics. 2-way percent correct for Inception(Szegedy et al., 2016), CLIP, EfficientNet(Tan, 2019) and SwAV(Caron et al., 2020) score are considered high level metrics. When trained to completion, both methods\u2014end-to-end training and adapter alignment\u2014show similar overall performance. However, the MSE loss at the adapter level is significantly lower (by an order of magnitude as shown in Appendix C.2) when using adapter alignment. This allows for more precise post-hoc analysis after extracting embeddings from the common space.\n\n\nAdditionally, we observe that adapter alignment yields better results than regular end-to-end training when the model is trained for fewer epochs, with faster convergence. Both approaches eventually reach similar performance levels, as shown in Figure 5.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">Low-Level</td><td colspan=\"4\">High-Level</td></tr><tr><td>PixCorr \u2191</td><td>SSIM \u2191</td><td>Alex(2) \u2191</td><td>Alex(5) \u2191</td><td>Incep \u2191</td><td>CLIP \u2191</td><td>Eff \u2193</td><td>SwAV \u2193</td></tr><tr><td>Subj1 Pretraining</td><td>0.345</td><td>0.346</td><td>92.80%</td><td>96.88%</td><td>94.40%</td><td>90.02%</td><td>0.692</td><td>0.399</td></tr><tr><td>Normal FineTune Avg</td><td>0.258</td><td>0.339</td><td>89.64%</td><td>95.05%</td><td>92.63%</td><td>89.64%</td><td>0.717</td><td>0.427</td></tr><tr><td>AAMax FineTune Avg</td><td>0.259</td><td>0.337</td><td>89.20%</td><td>95.09%</td><td>92.68%</td><td>89.49%</td><td>0.713</td><td>0.423</td></tr><tr><td>Subj2 FineTune Normal</td><td>0.294</td><td>0.345</td><td>91.42%</td><td>95.92%</td><td>93.00%</td><td>88.68%</td><td>0.721</td><td>0.425</td></tr><tr><td>Subj2 FineTune AAMax</td><td>0.295</td><td>0.340</td><td>91.52%</td><td>96.20%</td><td>92.96%</td><td>89.87%</td><td>0.707</td><td>0.409</td></tr><tr><td>Subj5 FineTune Normal</td><td>0.236</td><td>0.335</td><td>89.45%</td><td>95.32%</td><td>93.98%</td><td>91.63%</td><td>0.701</td><td>0.419</td></tr><tr><td>Subj5 FineTune AAMax</td><td>0.240</td><td>0.332</td><td>88.60%</td><td>95.16%</td><td>94.04%</td><td>90.71%</td><td>0.698</td><td>0.413</td></tr><tr><td>Subj7 FineTune Normal</td><td>0.244</td><td>0.339</td><td>88.06%</td><td>93.91%</td><td>90.92%</td><td>88.61%</td><td>0.731</td><td>0.438</td></tr><tr><td>Subj7 FineTune AAMax</td><td>0.242</td><td>0.340</td><td>87.48%</td><td>93.91%</td><td>91.05%</td><td>87.89%</td><td>0.734</td><td>0.448</td></tr></table>\n\n\nTable 2: Quantitative results on training the reconstruction models on all 40 hours of data. Both methods perform similarly across all metrics.\n\n\n# 3.5 TESTS ON LIMITED DATA\n\n\nA key objective of multi-subject pipelines is to achieve satisfactory models of new subjects with limited data. Acquiring fMRI data is costly, and reducing the data requirements for modeling a subject's brain makes fMRI reconstruction more accessible. MindEye2 demonstrated that fine-tuning a new subject with as little as 1 hour of data is possible, though the resulting reconstructions leave room for improvement.\n\n\n![Figure 5: Comparing finetuning reconstruction performance over time with all the data. From left to right: (a) High level metrics over time for Subject 1. Both methods perform similarly but the normal method takes about 20 epochs to match AAmax's numbers (b) Example reconstruction over time. The top row is the normal pipeline and the bottom row is the Adapter alignment pipeline.](images/90d94af57a79add0e1e469d8c07e585d9c2a6a8c50f67c82d6e320ee10d1493b.jpg)\n\n\nIn our experiments, we evaluate the fine-tuning of a new subject in three scenarios: 1 hour, 2 hours, and 4 hours of training data. The NSD dataset provides 40 hours of data per subject, all of which is used for the full data tests in Section 3.4. For this experiment, we designate Subject 5 as the reference subject and fine-tune Subject 1 using the shared images. We compare the performance of normal end-to-end training and Adapter Alignment (AA), with results presented in Table 3.\n\n\nAdapter Alignment consistently outperforms the end-to-end approach, showing significant improvements even after just one epoch of training. The performance gap widens as the amount of fine-tuning data is reduced. Due to the lightweight nature of Adapter Alignment, which involves training on just 250 images using a single layer, the time required to align the adapter is minimal compared to full end-to-end training. In fact, even after 160 epochs of normal training, the model's performance fails to match the Epoch 1 results of Adapter Alignment. Figure 6 illustrates the reconstruction performance across epochs for both methods.\n\n\n![Figure 6: Comparing finetuning reconstruction performance over time with limited data. From left to right: (a) Finetuning Subject 1 with 250 images or 1 hour of data (b) Finetuning Subject 1 with 500 images or 2 hours of data (c) Finetuning Subject 1 with 1000 images or 3 hours of data. We observe that adapter alignment (AAMax) not only outperforms normal end-to-end training, but the performance at Epoch 1 is better than the normal method's performance at Epoch 160.](images/cac86a5a9787d88217707f484de2bacaed4234f1d0bd83105c80194b3c1daef7.jpg)\n\n\n![](images/6cf19ed9819f1168942a5a0ef9a90c26cee26284aa0d91abfb0228502d83ea25.jpg)\n\n\n![](images/c7718cf32635aac70c5984e796f5c7478ae5cf015315a018f4ae570bff91c35f.jpg)\n\n\n# 4 BEST CANDIDATE IMAGE SELECTION\n\n\nHere, we present a selection algorithm that significantly reduces the amount of data required to train a model for a new subject. Later, we carry out an empirical analysis of the method.\n\n\n# 4.1 GREEDY ALGORITHM\n\n\nIn the context of fMRI-based visual reconstruction, we introduce a greedy algorithm that selects the most representative subset of images from the common space. We begin with an embedding of the common set of images into a representation space and choose the first  $d$  dimensions corresponding to the principal singular vectors. For each dimension  $j$ , we partition the range into  $B_{j}$  bins that is determined by the ratio of singular value  $\\lambda_{j}$  to the largest singular value  $\\lambda_{1}$ :\n\n\n$$\n\n$$\nB _ {j} = \\left\\lfloor w \\cdot \\frac {\\lambda_ {j}}{\\lambda_ {1}} \\right\\rfloor , \\quad j = 1, 2, \\dots , d\n$$\n\n$$\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">Low-Level</td><td colspan=\"4\">High-Level</td></tr><tr><td>PixCorr \u2191</td><td>SSIM \u2191</td><td>Alex(2) \u2191</td><td>Alex(5) \u2191</td><td>Incep \u2191</td><td>CLIP \u2191</td><td>Eff \u2193</td><td>SwAV \u2193</td></tr><tr><td>Subj1 FT NS 250 Normal E1</td><td>0.031</td><td>0.234</td><td>58.31%</td><td>62.56%</td><td>58.51%</td><td>58.71%</td><td>0.947</td><td>0.611</td></tr><tr><td>Subj1 FT NS 250 AAMAX E1</td><td>0.095</td><td>0.254</td><td>77.20%</td><td>85.98%</td><td>78.80%</td><td>79.39%</td><td>0.833</td><td>0.503</td></tr><tr><td>Subj1 FT NS 250 Normal E160</td><td>0.094</td><td>0.239</td><td>71.54%</td><td>80.10%</td><td>73.11%</td><td>74.02%</td><td>0.875</td><td>0.537</td></tr><tr><td>Subj1 FT NS 250 AAMAX E160</td><td>0.105</td><td>0.259</td><td>76.77%</td><td>86.20%</td><td>80.08%</td><td>80.14%</td><td>0.828</td><td>0.496</td></tr><tr><td>Subj1 FT NS 500 Normal E1</td><td>0.049</td><td>0.242</td><td>62.41%</td><td>68.06%</td><td>62.64%</td><td>64.80%</td><td>0.925</td><td>0.583</td></tr><tr><td>Subj1 FT NS 500 AAMAX E1</td><td>0.112</td><td>0.275</td><td>79.46%</td><td>88.29%</td><td>83.28%</td><td>82.61%</td><td>0.803</td><td>0.485</td></tr><tr><td>Subj1 FT NS 500 Normal E160</td><td>0.110</td><td>0.270</td><td>75.32%</td><td>84.62%</td><td>76.66%</td><td>78.55%</td><td>0.845</td><td>0.516</td></tr><tr><td>Subj1 FT NS 500 AAMAX E160</td><td>0.114</td><td>0.285</td><td>80.00%</td><td>88.50%</td><td>83.19%</td><td>83.05%</td><td>0.799</td><td>0.480</td></tr><tr><td>Subj1 FT NS 1000 Normal E1</td><td>0.079</td><td>0.241</td><td>65.84%</td><td>73.04%</td><td>64.19%</td><td>68.10%</td><td>0.914</td><td>0.574</td></tr><tr><td>Subj1 FT NS 1000 AAMAX E1</td><td>0.129</td><td>0.265</td><td>80.83%</td><td>90.93%</td><td>86.66%</td><td>86.17%</td><td>0.767</td><td>0.460</td></tr><tr><td>Subj1 FT NS 1000 Normal E160</td><td>0.124</td><td>0.253</td><td>77.86%</td><td>89.18%</td><td>83.10%</td><td>83.26%</td><td>0.801</td><td>0.480</td></tr><tr><td>Subj1 FT NS 1000 AAMAX E160</td><td>0.127</td><td>0.273</td><td>81.73%</td><td>91.29%</td><td>86.23%</td><td>86.26%</td><td>0.763</td><td>0.453</td></tr></table>\n\n\nTable 3: Quantitative results on fine tuning a new subject with limited data. FT=Fine tuning. The first section shows the result of fine tuning with 250 images after 1 epoch and after 160 epochs using normal end-to-end training vs AAMax. The second section compares performance on 500 images and the third section compares performance on 1000 images. In all cases AAMax, significantly outperforms normal training. The greyed out low level metrics are not optimized for in these experiments.\n\n\nwhere  $w$  is a predefined scaling parameter. Dimensions with larger singular values are partitioned into more bins, reflecting their greater variance and importance in capturing meaningful data features.\n\n\nEach image (and its fMRI) maps to exactly one bin in each dimension. Our goal is to find the smallest subset of images that covers all bins in each dimension. This subset is critical in selecting the most representative images for a new subject, ensuring that the selected images capture the key activity patterns that generalize across subjects. The problem as stated is NP-hard. However, it is submodular and a greedy heuristic such as given below achieves an  $(1 - 1 / e)$  approximation ratio.\n\n\nGiven a subset  $S$  of images, let  $Gap(S, j)$  be the number of empty bins in dimension  $j$ , and let  $Gap(S) = \\Sigma_{j} Gap(S, j)$  be the total number of images across all dimensions. At every step, the greedy algorithm chooses image  $i \\notin S$  that minimizes  $Gap$  when added to  $S$ .\n\n\n# 4.2 RESULTS\n\n\nWe achieve better results on high-level metrics compared to state-of-the-art methods while reducing the required data by  $40\\%$ . Figure 7 (a) illustrates how Inception and CLIP distances fluctuate as a function of the amount of training data. Figure 7 (b) presents the model's performance when 150 images are selected for the training set, while varying the number of principal eigenvectors onto which the embeddings are projected. A notable improvement is observed when the number of eigenvectors reaches 40, likely due to the nature of the binning process, as the number of bins created along the eigenvectors provides better coverage. Further analysis of this behavior presents an interesting direction for future work. Our method highlights the critical role of strategic data selection in achieving superior performance. For additional details on the experimental performance, please refer to Tables 4 and 5 in the Appendix Section A.1.\n\n\n# 5 RELATED WORK\n\n\nRecent works have achieved impressive results by mapping fMRI data to latent diffusion model (LDM) spaces (Takagi & Nishimoto, 2023; Scotti et al., 2023; Lu et al., 2023; Xia et al., 2024), while simultaneously integrating multiple modalities. Despite this progress, these methods have not been thoroughly tested for their generalization performance across a larger population of subjects. In other words, constructing a model for a completely new subject still requires retraining the model from scratch to ensure optimal performance.\n\n\n![Figure 7: Comparison of the performance of the image selection strategy. Both figures display the Inception and CLIP distance metrics. (a) Performance with varying training set sizes, where the red and black lines (labeled as Inception ME2 and CLIP ME2) at the 250-point mark indicate the model's performance without a data selection strategy. (b) Performance with varying numbers of principal eigenvectors.](images/32e9efa66924c8c8f80e5cbf62a93339df7ebb8ae9ae230f5f90637d24d2cc03.jpg)\n\n\n![](images/1b2a391b6e0692c0aafecf19ee4de43fdcc826f8ecaca5bee44bfcee48fa7980.jpg)\n\n\nApproaches like Ferrante et al. (2024) have demonstrated cross-subject brain decoding using limited data samples by aligning multiple subjects. Although the authors, like us, leveraged visual stimuli commonly used for multiple subjects, the fidelity of their reconstructed images lags behind current advancements. Scotti et al. (2024), the foundation of our method, produced significant results in image reconstruction under limited data conditions but did not fully exploit the properties of the common representation space. In contrast, our novel approach enables faster convergence and improved performance in a limited data setting by effectively leveraging the structure of the common space.\n\n\n# 6 CONCLUSIONS & DISCUSSION\n\n\nIn this work, we present compelling evidence for the existence of a shared visual representation space across subjects, allowing brain activity from different individuals to be aligned in a common space. Building on this foundation, we introduce Adapter Alignment (AA), a novel and efficient approach for subject-specific alignment in fMRI-based visual reconstruction. Our method, which is more efficient than traditional end-to-end training pipelines, achieves faster convergence and superior performance, particularly in low-data scenarios. Additionally, we propose a greedy image selection algorithm that identifies the most representative subset of images for training new subjects. This approach reduces the amount of data required for fine-tuning by up to  $40\\%$  without sacrificing performance, addressing the challenge of minimizing data requirements in fMRI-based image reconstruction.\n\n\nBy leveraging the structure of the common representation space, we demonstrate the feasibility of producing high-quality visual reconstructions with a significantly reduced amount of subject-specific data. Future work could explore the integration of non-linear adaptors to enhance concept extraction, potentially improving the performance and flexibility of the model. Another promising direction is to extend our methodology to different pipelines, ensuring its generalizability across various architectures, thereby making the approach architecture-agnostic. Furthermore, validating the results using different datasets (Contier et al., 2021) would offer valuable insights into the robustness of the method across diverse data settings.\n\n\n# REFERENCES\n\n- Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\n- Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. A massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, 25(1):116-126, 2022.\n\n- Pinglei Bao, Liang She, Mason McGill, and Doris Y Tsao. A map of object space in primate inferotemporal cortex. Nature, 583(7814):103-108, 2020.\n\n- Roman Belyi, Guy Gaziv, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal Irani. From voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912-9924, 2020.\n\n- Thomas B Christophel, Martin N Hebart, and John-Dylan Haynes. Decoding the contents of visual short-term memory from human visual and parietal cortex. Journal of Neuroscience, 32(38): 12983-12989, 2012.\n\n- Oliver Contier, Martin N Hebart, Adam H Dickter, Lina Teichmann, Alexis Kidder, Anna Corriveau, Charles Zheng, Maryam Vaziri-Pashkam, and Charles Baker. Things-fmri/meg: A large-scale multimodal neuroimaging dataset of responses to natural object images. Journal of Vision, 21(9): 2633-2633, 2021.\n\n- Edward F Ester, David E Anderson, John T Serences, and Edward Awh. A neural measure of precision in visual working memory. Journal of cognitive neuroscience, 25(5):754-761, 2013.\n\n- Matteo Ferrante, Tommaso Boccato, Furkan Ozcelik, Rufin VanRullen, and Nicola Toschi. Through their eyes: multi-subject brain decoding with simple alignment techniques. Imaging Neuroscience, 2:1-21, 2024.\n\n- Marshall L Fisher, George L Nemhauser, and Laurence A Wolsey. An analysis of approximations for maximizing submodular set functions-II. Springer, 1978.\n\n- Guy Gaziv, Roman Belyi, Niv Granot, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal Irani. Self-supervised natural image reconstruction and large-scale semantic classification from brain activity. NeuroImage, 254:119121, 2022.\n\n- Stephenie A Harrison and Frank Tong. Decoding reveals the contents of visual working memory in early visual areas. Nature, 458(7238):632-635, 2009.\n\n- Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023. URL https://arxiv.org/abs/1606.08415.\n\n- Alexander G Huth, Shinji Nishimoto, An T Vu, and Jack L Gallant. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. Neuron, 76(6):1210-1224, 2012.\n\n- Sirawaj Itthipuripat, Thomas C Sprague, and John T Serences. Functional mri and eeg index complementary attentional modulations. Journal of Neuroscience, 39(31):6162-6179, 2019.\n\n- Yukiyasu Kamitani and Frank Tong. Decoding the visual and subjective contents of the human brain. Nature neuroscience, 8(5):679-685, 2005.\n\n- Hsin-Hung Li, Thomas C Sprague, Aspen H Yoo, Wei Ji Ma, and Clayton E Curtis. Joint representation of working memory and uncertainty in human cortex. Neuron, 109(22):3699-3712, 2021.\n\n- Sikun Lin, Thomas Sprague, and Ambuj K Singh. Mind reader: Reconstructing complex images from brain activities. Advances in Neural Information Processing Systems, 35:29624-29636, 2022.\n\n- Yulong Liu, Yongqiang Ma, Guibo Zhu, Haodong Jing, and Nanning Zheng. See through their minds: Learning transferable neural representation from cross-subject fmri, 2024. URL https://arxiv.org/abs/2403.06361.\n\n- Bria Long, Chen-Ping Yu, and Talia Konkle. Mid-level visual features underlie the high-level categorical organization of the ventral stream. Proceedings of the National Academy of Sciences, 115(38):E9015-E9024, 2018.\n\n- Yizhuo Lu, Changde Du, Qiongyi Zhou, Dianpeng Wang, and Huiguang He. Minddiffuser: Controlled image reconstruction from human brain activity with semantic and structural diffusion. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 5899-5908, 2023.\n\n- Milad Mozafari, Leila Reddy, and Rufin VanRullen. Reconstructing natural scenes from fmri patterns using bigbigan. In 2020 International joint conference on neural networks (IJCNN), pp. 1-8. IEEE, 2020.\n\n- Thomas Naselaris, Kendrick N Kay, Shinji Nishimoto, and Jack L Gallant. Encoding and decoding in fmri. Neuroimage, 56(2):400-410, 2011.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021.\n\n- Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n\n- Ziqi Ren, Jie Li, Xuetong Xue, Xin Li, Fan Yang, Zhicheng Jiao, and Xinbo Gao. Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning. NeuroImage, 228:117602, 2021.\n\n- Peter H. Schonemann. A generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):1-10, 1966.\n\n- Alexander Schrijver et al. Combinatorial optimization: polyhedra and efficiency, volume 24. Springer, 2003.\n\n- Miranda Scolari, Anna Byers, and John T Serences. Optimal deployment of attentional gain during fine discriminations. Journal of Neuroscience, 32(22):7723-7733, 2012.\n\n- Paul Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Aidan Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth Norman, et al. Reconstructing the mind's eye: fmri-to-image with contrastive learning and diffusion priors. Advances in Neural Information Processing Systems, 37, 2023.\n\n- Paul S Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth A Norman, et al. Mindeye2: Shared-subject models enable fmri-to-image with 1 hour of data. arXiv preprint arXiv:2403.11207, 2024.\n\n- Katja Seeliger, Umut G\u00fc\u00e7l\u00fc, Luca Ambrogioni, Yagmur G\u00fc\u00e7l\u00fct\u00fcrk, and Marcel AJ Van Gerven. Generative adversarial networks for reconstructing natural images from brain activity. NeuroImage, 181:775-785, 2018.\n\n- John T Serences and Geoffrey M Boynton. Feature-based attentional modulations in the absence of direct visual stimulation. Neuron, 55(2):301-312, 2007.\n\n- John T Serences, Edward F Ester, Edward K Vogel, and Edward Awh. Stimulus-specific delay activity in human primary visual cortex. Psychological science, 20(2):207-214, 2009.\n\n- Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu Horikawa, and Yukiyasu Kamitani. End-to-end deep image reconstruction from human brain activity. Frontiers in computational neuroscience, 13:432276, 2019.\n\n- Thomas C Sprague and John T Serences. Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices. Nature neuroscience, 16(12):1879-1887, 2013.\n\n- Thomas C Sprague, Edward F Ester, and John T Serences. Reconstructions of information in visual spatial working memory degrade with memory load. Current Biology, 24(18):2174-2180, 2014.\n\n- Thomas C Sprague, Sirawaj Itthipuripat, Vy A Vo, and John T Serences. Dissociable signatures of visual salience and behavioral relevance across attentional priority maps in human cortex. Journal of neurophysiology, 119(6):2153-2165, 2018.\n\n- Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826, 2016.\n\n- Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from human brain activity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14453-14463, 2023.\n\n- Mingxing Tan. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.\n\n- Brian A Wandell and Jonathan Winawer. Computational neuroimaging and population receptive fields. Trends in cognitive sciences, 19(6):349-357, 2015.\n\n- Weihao Xia, Raoul de Charette, Cengiz Oztireli, and Jing-Hao Xue. Dream: Visual decoding from reversing human visual system. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 8226-8235, 2024.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634682.4312897", "msecs": "431.0", "relativeCreated": "27254.76384162903", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Language-Informed Visual Concept Learning\n\n# Abstract\nOur understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training.\n\n\n# 1 INTRODUCTION\n\n\nIn order to make sense of the myriad visual entities in the world, humans develop an abstracted generative model of them and organize the underlying sources of variation into visual concepts, such as different colors or different types of objects. Designing systems that can recognize visual concepts within images as humans do has been a longstanding goal in the fields of computer vision and artificial intelligence (Russakovsky et al., 2015; Krizhevsky et al., 2012; Girshick et al., 2014).\n\n\nTo facilitate efficient reasoning and communication of these concepts, humans created symbolic depictions that have evolved into natural language. Such natural language grounding of visual data has been instrumental in the recent proliferation of powerful large vision-language models that are capable of semantically identifying objects in images (Radford et al., 2021; Kirillov et al., 2023) or generating photo-realistic images from arbitrary text prompts (Ramesh et al., 2021; Rombach et al., 2022; Sahara et al., 2022; Yu et al., 2022). While different concept axes can be easily specified by words, such as category and style, it is much less intuitive to delineate the subtleties of low-level visual nuances along each axis using language, such as one particular style of a painting.\n\n\nIn this work, our goal is to distill from large pre-trained vision-language models a function that extracts visual concepts along a set of language-specified concept axes from images. As illustrated in Figure 1, once these concepts are extracted, we can recompose them across different image instances at inference time to produce new images with novel concept combinations. To learn this function, rather than collecting a large-scale dataset of human annotations for each specific visual concept, we design a language-informed visual concept representation, and simply distill from a pre-trained Text-to-Image (T2I) generation model. There are three fundamental properties we seek in this visual concept representation.\n\n\nFirst, unlike T2I generation, which relies on generic words as visual concept descriptors, we would like to capture fine-grained visual nuances using continuous concept embeddings. One common technique is to invert the text-to-image generation process by optimizing an embedding with the objective of reproducing a given input image using a pre-trained T2I model, often referred to as Textual Inversion (Gal et al., 2022). However, most existing Textual Inversion methods (Gal et al.,\n\n\n![Figure 1: Language-Informed Visual Concept Learning. Our goal is to learn a visual concept representation grounded on a set of language-informed concept axes, e.g., category, color, and material, by simply distilling from pre-trained text-to-image generation models without manual annotations. After training, the concept encoders extract disentangled axis-specific embeddings from an image, which can be remixed to generate new images with novel concept compositions.](images/6cbedad65b640ee4643ec354e3272add961f72badef713adc46c3340efa57a0f.jpg)\n\n\n![Figure 2: Learned Disentangled Concept Embeddings Improve Compositionality. Left: Vanilla text-to-image model may fail to adhere to text prompts of uncommon combinations of concepts even with prompt engineering, e.g. \"red banana\". Right: With the same backbone T2I generator, our learned disentangled concept embeddings greatly enhance concept compositionality.](images/6265050443a8dd5d6436db8a5a10b895d5a05dfd528dd484535bac88388debcf.jpg)\n\n\n2022) optimize embeddings for individual image instances independently, overlooking the shared nature of visual concepts across instances. For instance, the concept of \"red\" is shared between a \"red apple\" and a \"red dress\". Moreover, the concepts of \"red\" and \"yellow\" also are instances of the property of color.\n\n\nHence, the second desired property of the visual concept representation is to preserve such common concept structures among various visual instances. Instead of optimizing on individual image instances independently, we design a set of concept encoders, where each encoder learns to encode the visual characteristics of an input image pertaining to one concept axis specified by language. This ensures that the inverted concept embeddings can be shared across different instances and remixed to generate new images.\n\n\nThe third crucial aspect of this representation is to ascertain that different concept axes are disentangled, allowing for changes to be made specifically on single concept axis without modifying other axes. To do so, we reuse the disentangled nature of linguistic concepts and ground the predictions to a set of discrete text anchors in the concept embeddings space, which can be obtained by querying a pre-trained generic Visual Question Answering (VQA) model, e.g., BLIP-2 (Li et al., 2023b). This soft anchoring constraint significantly improves the disentanglement of concept embeddings across different axes while still retaining sufficient leeway to capture nuanced visual variations that BLIP-2 struggles to discern, e.g., the style of an art piece in Figure 6.\n\n\nPutting these ideas together, we design a generic framework for learning disentangled and compositional visual concepts grounded to linguistic structures by exploiting pre-trained text-to-image generation and visual question answering models. We show that these concept encoders can be trained purely on synthetic images generated by a pre-trained T2I model, and extract concept embeddings from real images at test time, which capture the fine-grained visual nuances.\n\n\nOur contributions can be summarized as follows:\n\n- 1. We propose a generic framework for learning language-informed visual concepts by simply distilling pretrained vision-language models.\n\n- 2. At inference time, the trained concept encoders extract concept embeddings from a test image, which can be remixed to generate images with novel compositions of concepts.\n\n- 3. Using a light-weight test-time finetuning procedure, these encoders can also be quickly adapted to extract novel concepts unseen during training.\n\n\n4. Experiments show that this visual concept representation achieves better disentanglement and compositionality, compared to text-based prompting baselines, as shown in Figures 2 and 6.\n\n\n# 2 RELATED WORK\n\n\n# 2.1 VISUAL CONCEPT LEARNING\n\n\nDesigning learning-based systems to discover various visual concepts in natural images has been a long-standing goal in machine perception and intelligence. Early attempts typically rely on extensive semantic annotations done by humans, such as object classification (Barnard et al., 2003; Fei-Fei et al., 2006; Fergus et al., 2005), which were later epitomized by the effort of ImageNet (Russakovsky et al., 2015). Visual concepts are intrinsically linked to concepts in language, and such end-to-end supervised learning paradigms can be seen as learning a direct mapping between visual concepts and discrete linguistic concepts. Other approaches attempt to better exploit this inherent structure in language by constructing a structured representation of visual concepts such as scene graphs (Zhong et al., 2021) and symbolic programs (Mao et al., 2019; Han et al., 2019).\n\n\nMore recently, the success of natural language modeling (Devlin et al., 2018; Brown et al., 2020; Raffel et al., 2020) has paved the way for grounding visual concepts to open vocabularies, unlike category labels or fixed symbolic programs, by training large Vision-Language Models (VLMs) on massive image captioning datasets (Schuhmann et al., 2022). This has powered recent Text-to-Image (T2I) generation models to turn linguistic concepts from free-form text prompts into photo-realistic images (Rombach et al., 2022; Sahara et al., 2022). These T2I models have been leveraged by Personalization methods for extracting individual visual concepts from one or a few images. This is done by either by optimizing token embeddings (Gal et al., 2022; Vinker et al., 2023; Avrahami et al., 2023; Chefer et al., 2023b; Liu et al., 2023), finetuning the backbone denoiser (Ruiz et al., 2023), or training additional encoders for amortized optimization (Gal et al., 2023; Arar et al., 2023; Li et al., 2023a). We also distill visual concepts from a pre-trained T2I model, but unlike existing works, we train encoders to adhere to a set of language-specified concept axes, preserving the disentangled and compositional nature of language. Ranasinghe & Ryoo (2023) also explores language-defined concepts but focuses on video action recognition tasks while we focus on image generation.\n\n\nA separate line of work focuses on unsupervised visual concept disentanglement without explicitly leveraging language, typically by simply imposing information constraints in the latent space of a generative model, like VAEs and GANs (Higgins et al., 2017; Chen et al., 2016; Hsu et al., 2023). Here, we are interested in learning visual concepts that are explicitly grounded to language.\n\n\n# 2.2 CONTROLLABLE IMAGE GENERATION\n\n\nThe success of GAN-based image generation (Goodfellow et al., 2014; Brock et al., 2018; Karras et al., 2019) has spawned a series of works that discover controllable directions in the GAN latent space (Voynov & Babenko, 2020; Harkonen et al., 2020). More recently, the advancements of diffusion-based T2I models have unlocked new possibilities for controllable image generation, where photo-realistic images can be generated from free-form text prompts. Recent works proposed to further improve the alignment of image samples and input text conditions by manipulating attention maps within T2I models (Chefer et al., 2023a; Epstein et al., 2023). Another form of controllable image generation is compositional generation. Liu et al. (2022) proposes to improve the quality of T2I diffusion models for composing multiple pre-given concepts, specified via text prompts, by modifying the inference procedure. In this work, instead of assuming that concepts are given and are in a text format, we tackle the task of identifying disentangled concepts which can be used for composition.\n\n\nImage generation can also be controlled with image analogies (\u0160ubtrova et al., 2023; Hertzmann et al., 2001), a form of visual prompting. These works typically do not explicitly extract visual concepts from inputs unlike ours. In this work, we amalgamate both visual prompts and text queries, employing them as the editing interface.\n\n\n# 3 METHOD\n\n\nFig. 3 gives an overview of our proposed learning framework. Our goal in this work is to extract visual concepts from images along a number of concept axes specified by language, such as\n\n\n![Figure 3: Training Pipeline. During training, an input image is processed by a set of concept encoders that predict concept embeddings specific to given concept axes. These embeddings are trained to (1) retain information in order to reproduce visual inputs via a pre-trained Text-to-Image model given an axis-informed text template, and (2) ensure disentanglement across different axes by anchoring to text embeddings obtained from a pre-trained Visual Question Answering model.](images/b89b484cc861cafea48ff730bbca55210c3cfe07e46b038f971fe09bd2439a2b.jpg)\n\n\ncategory, color, and material, so as to enable the flexible composition of concepts into high-quality image generations.\n\n\nTo achieve this, we train a set of visual concept encoders by distilling concept guidance from pretrained vision-language models. Specifically, the encoders are trained to extract concept embeddings from an image in order to fulfill two objectives. First, they should be recomposed to explain the input image through a pretrained text-to-image (T2I) generation model, given a concept-axis-informed text prompt. Second, these visual concept embeddings should be anchored to the corresponding text embeddings obtained from a pre-trained visual question answering (VQA) model, further exploiting the disentangled nature of linguistic concepts for better disentanglement of visual concepts.\n\n\n# 3.1 VISUAL CONCEPT ENCODING BY INVERTING TEXT-TO-IMAGE GENERATION\n\n\nOur understanding of the visual world is centered around various concept axes, to which we have often assigned words due to their significance in communication and reasoning. This vision-language grounding has fueled recent explosion of text-to-image generation models (Rombach et al., 2022; Sahara et al., 2022; Ramesh et al., 2022), allowing them to generate photo-realistic images with various combinations of concepts defined by words.\n\n\nHere, we are interested in the reverse direction of text-to-image generation, where the goal is to extract language-grounded visual concepts present in natural images. Specifically, given  $K$  concept axes of interest defined by language, we would like to learn  $K$  concept encoders  $\\{f_k(\\cdot)\\}_{k=1}^K$ , each of which extracts a concept representation  $\\mathbf{e}_k = f_k(\\mathbf{x})$  along a concept axis from an input image  $\\mathbf{x}$ .\n\n\nIn order to train these concept encoders  $\\{f_k(\\cdot)\\}$ , instead of relying on extensive human labeling, we opt to exploit the vision-language grounding embedded within large pre-trained T2I generation models. Using the technique of Textual Inversion (Gal et al., 2022), one can optimize a token embedding  $<\\star>$  to capture a visual entity in a given image, through the objective of regenerating the image with the T2I model from a text template, such as \"a photo of  $<\\star>$ \". Here, we adopt a similar objective, but instead of inverting a specific embedding capturing the overall \"identity\" of an individual image instance, we would like to predict embeddings  $\\mathbf{e}_k$  that are grounded to a number of meaningful concept axes, using an axis-informed text template, such as \"a photo of  $<\\mathbf{e}_1>$  with  $<\\mathbf{e}_2>$  color and  $<\\mathbf{e}_3>$  material\". This allows the extracted concept embeddings to be shared across different images, encapsulating the common visual characteristics pertinent to one concept axis.\n\n\nSpecifically, given an image  $\\mathbf{x}$ , the concept encoders  $\\{f_k(\\cdot)\\}$  extract a set of concept embeddings  $\\{\\mathbf{e}_k\\in \\mathbb{R}^D\\}$ , which have the same dimension  $D$  as the text embeddings so that they can be directly inserted into the text embeddings of the axis-informed text template. To simplify the notations, let  $f_{\\gamma}(\\cdot)$  denote the function that takes in the image and produces the final sequence of embeddings\n\n\nof the template and the predicted concept embeddings, and  $\\gamma$  be the parameters of all the encoders which will be optimized during training. Let  $\\mathbf{c}_{\\theta}$  be the part of the T2I model's text encoder that takes in a sequence of text embeddings and outputs a conditioning vector for the T2I model's denoising network  $\\hat{\\epsilon}_{\\theta}$ , where  $\\theta$  denotes network parameters. We use DeepFloyd (StabilityAI; Sahara et al., 2022) as the backbone T2I model, which utilizes a pre-trained T5 model (Raffel et al., 2020) as the text encoder, and keep the parameters  $\\theta$  frozen in all experiments. To train the encoders, we reuse the training objective for the backbone diffusion model:\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {r e c o n}} (\\mathbf {x}; \\gamma) = \\mathbb {E} _ {\\boldsymbol {\\epsilon} \\sim \\mathcal {N} (\\mathbf {0}, I), t \\sim \\mathcal {U} ([ 0, 1 ])} \\left[ \\| \\hat {\\epsilon} _ {\\theta} (\\mathbf {x}, t, \\mathbf {c} _ {\\theta} (f _ {\\gamma} (\\mathbf {x}))) - \\boldsymbol {\\epsilon} \\| _ {2} ^ {2} \\right], \\tag {1}\n$$\n\n$$\n\n\nwhere the noise  $\\epsilon$  is sampled from a standardmultivariate Gaussian distribution and the timestep  $t$  is sampled from a uniform distribution in [0, 1]. Minimizing  $\\mathcal{L}^{\\mathrm{recon}}$  amounts to finding concept embeddings within the space of the pre-trained T2I model that can best reproduce the input image  $\\mathbf{x}$ , resembling a \"reconstruction\" objective.\n\n\nCompared to per-instance token optimization in vanilla Textual Inversion, the advantages of training these concept encoders are two-fold. First, the concept embedding space is naturally shared across different image instances, encapsulating the common understanding of the corresponding concept axes. Second, it makes training more efficient by amortizing the optimization across all instances, and more crucially, it allows for test-time inference in a feed-forward pass.\n\n\n# 3.2 CONCEPT DISENTANGLEMENT USING TEXT ANCHORS\n\n\nThe objective of  $\\mathcal{L}^{\\mathrm{recon}}$  ensures that the extracted concept embeddings can sufficiently reconstruct the concept of a given image through a pre-trained text-to-image generation model. However, with this loss alone, there is little guarantee that each embedding encodes only the information pertinent to a particular concept axis. In practice, we found that this baseline results in poor disentanglement of different concept axes when remixing the concept embeddings to generate new images, potentially due to the imprecise vision-language grounding in the pre-trained T2I model. For instance, as shown in Figure 8, the extracted category embedding  $<\\mathbf{e}_1>$  for \"red berries\" cannot be remixed with various color embeddings  $<\\mathbf{e}_2>$  e.g., \"orange\", as  $<\\mathbf{e}_1>$  is highly entangled with the concept of a \"red\" color due to the bias in natural images.\n\n\nTo encourage better disentanglement of different concept axes, we further incorporate a sparse set of text anchors into the concept embedding space. Along each concept axis like color, we have often named some prominent modes, such as \"red\" or \"yellow\", and these text labels entail clearly disentangled concepts. Therefore, we would like to reuse this disentangled nature of linguistic concepts to improve the disentanglement of visual concepts. To this end, we make use of the text predictions from a pre-trained Visual Question Answering (VQA) model, BLIP-2 (Li et al., 2023b), as pseudo ground-truth anchors for the concept embeddings.\n\n\nSpecifically, for each training image  $\\mathbf{x}$  and for each concept axis of interest (e.g., color) indexed by  $k$ , we query the BLIP-2 model  $\\Psi$  with the image  $\\mathbf{x}$  and a question  $q_{k}$  in natural language that is specific to this concept axis, e.g., \"what is the color of the object in the image\". Denote the answer from BLIP-2, also in the form of natural language, as  $\\Psi(\\mathbf{x}, q_{k})$ . We encode this answer with the pre-trained text encoder  $c_{\\theta}$  to obtain a text embedding  $\\tilde{\\mathbf{e}}_{k} = c_{\\theta}(\\Psi(\\mathbf{x}, q_{k}))$ . The prediction of our concept encoders  $f_{k,\\gamma}$  is encouraged to stay close to this anchor text embedding:\n\n\n$$\n\n$$\n\\mathcal {L} _ {k} ^ {\\text {a n c h o r}} (\\mathbf {x}; \\gamma) = \\| f _ {k, \\gamma} (\\mathbf {x}) - \\tilde {\\mathbf {e}} _ {k} \\| _ {2} ^ {2}, \\quad \\text {w h e r e} \\quad \\tilde {\\mathbf {e}} _ {k} = c _ {\\theta} \\left(\\Psi (\\mathbf {x}, q _ {k})\\right). \\tag {2}\n$$\n\n$$\n\n\nIt is crucial to highlight that we use these BLIP-2 predictions only as anchors by assigning a small weight to this anchor loss  $\\mathcal{L}_k^{\\mathrm{anchor}}$  during training. Otherwise, the embeddings predicted by the concept encoders could easily collapse to a set of discrete text embeddings and fail to capture the visual nuances in images.\n\n\n# 3.3 TRAINING AND INFERENCE\n\n\nTraining. Given a collection of training images  $\\mathcal{D}$  containing various combinations of concepts along each axis, the final objective to train the concept encoders consists of the two parts:\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {t o t a l}} (\\gamma) = \\mathbb {E} _ {\\mathbf {x} \\sim \\mathcal {D}} \\left[ \\mathcal {L} ^ {\\text {r e c o n}} (\\mathbf {x}; \\gamma) + \\sum_ {k = 1} ^ {K} \\lambda_ {k} \\mathcal {L} _ {k} ^ {\\text {a n c h o r}} (\\mathbf {x}; \\gamma) \\right]. \\tag {3}\n$$\n\n$$\n\n\nInference. At inference time, given a new test image, the concept encoders extract embeddings  $\\{\\mathbf{e}_k\\}$  capturing its characteristics along each concept axis of interest. These embeddings can be remixed across different images, or be replaced by embeddings converted from explicit words, to produce images with new compositions of visual concepts through the backbone T2I generator.\n\n\nGeneralization to Unseen Concepts via Test-Time Finetuning. While the encoders can precisely extract an axis-specific concept that has been seen during training from a new test image, they tend to be less robust to concepts unseen at training. However, with a lightweight test-time optimization procedure, where we use only the reconstruction objective  $\\mathcal{L}^{\\mathrm{recon}}$  to update the parameters for all encoders,  $\\gamma$ , these encoders can generalize to novel concepts unseen during training. Note that  $\\mathcal{L}^{\\mathrm{anchor}}$  is omitted here in order to capture the visual nuances without over-committing to the coarse text anchors. After training, the encoders have learned to generate outputs within a relatively narrow region of the embedding space, which allows the model to adapt to the test images shown in Figure 5 within around 600 iterations while maintaining disentanglement and compositional capability.\n\n\n# 4 EXPERIMENTS\n\n\n# 4.1 EXPERIMENT SETUP\n\n\nTraining Data Generation. We train the concept encoders only using synthetic images generated by DeepFloyd from 5 different domains, including fruits, figurines, furniture, art, and clothing. More details of our dataset can be found in A.2. For each dataset, we consider 2-3 concept axes, such as category, color, material, style, and season. For example, considering category and color for the fruits dataset, we generate training images by prompting DeepFloyd with text prompts describing varying combinations of categories and colors, e.g. \"a photo of an apple which is red in color\". Note that these text prompts are used only for data generation and not for training, as they may not be reliable (Figure 2). On average, we obtain 669 training images for each dataset.\n\n\nImplementation Details. Inspired by Gal et al. (2023), we leverage a pre-trained CLIP ViT/L-14 model for image encoding (Radford et al., 2021; Dosovitskiy et al., 2021), which was trained with a contrastive objective aligning image and text features, and hence well-suited for our task. We extract image features from CLIP ViT and train  $K$  separate concept encoders  $f_{k}$  on top of the features, which share the same architecture but maintains separate weights. Specifically, we take the [CLS] tokens from each CLIP layer and process each token with a distinct linear layer. This is different from Gal et al. (2023), which extracts [CLS] tokens from every other layer and uses a single shared linear layer for all token features. The transformed features are then aggregated with average pooling followed by a LeakyReLU (Xu et al., 2015) activation, and passed into another linear layer that produces the final predicted concept embeddings.\n\n\nTo ground the concept embeddings to the concept axes, we adapt the text templates from CLIP (Radford et al., 2021), which were originally used to assemble captions with class categories from ImageNet (Russakovsky et al., 2015). For training, we use AdamW (Loshchilov & Hutter, 2017) optimizer with learning rate 0.02, and randomly flip the images horizontally. For test-time finetuning, we use the AdamW optimizer with learning rate 0.001. We set  $\\lambda_{k} = 0.0001$  (Equation (3)) for the category axis and  $\\lambda = 0.001$  for others. We use IF-I-XL from DeepFloyd as the backbone model, with training resolution  $64 \\times 64$ . Training on one dataset takes approximately 12 hours on one NVIDIA GeForce RTX 3090 GPU. Generated images are upsampled  $256 \\times 256$  using IF-II-L for visualization purpose.\n\n\n# 4.2 QUALITATIVE RESULTS\n\n\nVisual Concept Extraction, Decomposition and Extrapolation. Once trained, the concept encoders can extract disentangled concept embeddings specific to each concept axis from different test images, which can be recomposed to generate new images with various concept compositions. As shown in Figure 4, across various datasets, our method is able to recompose axis-specific visual concepts from different images and consistently generate new images depicting the recomposed concepts. More examples can be found in Appendix A.1, where images generated from individual decomposed concept embeddings are also presented.\n\n\nThis disentangled concept representation also allows us to extrapolate along a particular concept axis for visual exploration, as shown in Figure 7. For instance, we can ask BLIP-2 \"what is the style\n\n\n![(c) Furniture](images/9ca0e6d02b392d03e980ae8e71551e4b0df2fbd1553f75e8a3e9ddd236c3c193.jpg)\n\n\n![Figure 4: Concept Decomposition. At test time, our model extracts visual concepts along various axes from different images and recompose them to generate new images. We show decomposition results across different pairs of concept axes in 3 datasets: (a) Fruits, (b) Paintings, and (c) Furniture. Figure 5: Generalization to Unseen Concepts via Finetuning. After test-time fine-tuning on a single test-time image, encoders can adapt to novel concept. Visual details from the input images are preserved as can be seen from images visualizing embedding predictions. Importantly, these embeddings do not overfit to the input images and maintain a good disentanglement, such that they can be freely recomposed to create new concepts. More results can be found in Figures 9 to 13. More real-world results can be found in Figures 20 to 24.](images/85d5816954f586fcaa9669d8c3122a783a6ffb288a6c17a0a841c707cb4048aa.jpg)\n\n\nof the painting?\" in an image, and prompt GPT-4 (OpenAI, 2023) to name a few alternatives. We can then recompose the text embeddings of these alternative styles with our concept embeddings and generate images to visualize these variants. Representing concepts as continuous embeddings further enables concept interpolations. Details and results are shown in Appendix A.6.\n\n\nGeneralization to Unseen Concepts via Test-Time Finetuning. Although the encoders have only seen a limited range of concepts during training due to the small size of the training dataset, it can be quickly adapted to unseen concepts with the lightweight test-time finetuning procedure in Section 3.3, as shown in Figure 5. For instance, after 600 finetuning iterations, the model can extract the specific style of the dog painting unseen at training and compose it with the content from other images. It can also capture nuanced colors e.g. yellow-ish-orange and transfer them to other objects.\n\n\n![Figure 6: Baselines Comparisons. Compared the text-based image editing methods, our method achieves significantly better compositionality due to the disentangled concept representation, and captures fine-grained color variations, which the baseline struggles to encode with language.](images/186927e07b61069a975658bfbcb53a7f42b9171212357179fb24ca686d9ddadd.jpg)\n\n\n<table><tr><td rowspan=\"3\"></td><td colspan=\"6\">CLIP-Score \u2191</td><td colspan=\"2\">Human Evaluation \u2191</td></tr><tr><td colspan=\"3\">Edit Category</td><td colspan=\"3\">Edit Color</td><td>Edit Cat.</td><td>Edit Clr.</td></tr><tr><td>Cat.&amp;Clr.</td><td>Cat.</td><td>Clr.</td><td>Cat.&amp;Clr.</td><td>Cat.</td><td>Clr.</td><td>Score</td><td>Score</td></tr><tr><td>Null-text Inversion</td><td>0.258</td><td>0.249</td><td>0.249</td><td>0.259</td><td>0.265</td><td>0.223</td><td>0.287</td><td>0.316</td></tr><tr><td>InstructPix2Pix</td><td>0.267</td><td>0.277</td><td>0.226</td><td>0.270</td><td>0.245</td><td>0.268</td><td>0.233</td><td>0.648</td></tr><tr><td>Baseline w/ BLIP-2</td><td>0.313</td><td>0.294</td><td>0.248</td><td>0.287</td><td>0.271</td><td>0.237</td><td>0.448</td><td>0.379</td></tr><tr><td>Ours</td><td>0.308</td><td>0.297</td><td>0.238</td><td>0.302</td><td>0.287</td><td>0.236</td><td>0.968</td><td>0.840</td></tr><tr><td>w/o Lkanchor</td><td>0.268</td><td>0.276</td><td>0.219</td><td>0.263</td><td>0.257</td><td>0.236</td><td>-</td><td>-</td></tr><tr><td>w/o Encoder &amp; Lkanchor</td><td>0.288</td><td>0.300</td><td>0.214</td><td>0.242</td><td>0.213</td><td>0.265</td><td>-</td><td>-</td></tr></table>\n\n\nTable 1: Quantitative Comparisons on Visual Concept Editing. Compared to existing image editing baselines (Brooks et al., 2023; Mokady et al., 2022), our method achieves better overall CLIP score when editing either axis, and is particularly effective at retaining category-related concepts as reflected in human evaluation. 'Cat' denotes Category and 'Crl' denotes Color.\n\n\n# 4.3 COMPARISON WITH PRIOR WORKS\n\n\nQualitative Comparisons While this task of image generation with disentangled visual concepts is new, we identified prior work that is capable of text-based image editing and generation, and establish a side-by-side comparison on the task of visual concept editing. Specifically, given an image  $\\mathbf{x}$ , for example, of a teal-colored apple in Figure 6, the task is to generate a new image  $\\hat{\\mathbf{x}}_{\\mathbf{e}_k \\to \\mathbf{e}_k'}$  with one concept axis  $k$  (e.g. category) modified by a text prompt, from  $\\langle \\mathbf{e}_k \\rangle$  to  $\\langle \\mathbf{e}_k' \\rangle$ , while preserving other axes  $\\{i | i \\neq k\\}$  from the input.\n\n\nWe compare to two existing methods. Null-text Inversion (Mokady et al., 2022) performs concept editing by first inverting the input image to token embeddings of Stable-Diffusion (Saharia et al., 2022) and then applying Prompt-to-Prompt (Hertz et al., 2022), which modifies cross attention maps for editing, a process that leads to pixel-aligned editing results. InstructPix2Pix (Brooks et al., 2023) is a conditional diffusion model for text-based image editing. Since it is trained on data curated with Prompt-to-Prompt for supervision, it also tends to generate pixel-aligned results. We also design a naive baseline for this task, where we query BLIP-2 for a text description of the attributes to be retained from the test image, and combine the answer with a target concept to generate the final recomposed image. As shown in Figure 6, our method achieves better decomposition results, whereas baseline methods fail to disentangle the desired axis-specific concepts from input images, and therefore struggle to faithfully reproduce the desired concepts from the image and the text.\n\n\nQuantitative Comparisons. We also conduct quantitative evaluations on the task of text-based visual concept editing and compare with prior work. Specifically, we record the ground-truth text\n\n\n![Figure 7: Visual Concept Extrapolation. Given an input image, we can extrapolate along a concept axis by querying BLIP-2 and GPT-4 to name a few alternatives to the concept in the input. Our model can then generate new variants of the input for visual exploration, by mixing them with the extracted concept embeddings.](images/156d041247697f43fbfafd963eacdff341bf090a710ea3b3147443cebac4467d.jpg)\n\n\n![Figure 8: Ablations. Both the text anchor loss and the encoder design reduce overfitting to input images and improve concept disentanglement. Removing these components significantly deteriorates the visual concept editing results. Additionally, we observe that the BLIP-2 baseline has difficulties transferring unnatural colors.](images/09e741c32b44cf2ae1e99561bd91ca98f1f15a6ac745de310932013bcab3c389.jpg)\n\n\nprompts  $y$  that we used to generate each training image  $\\mathbf{x}$ , and manually filter out the inconsistent pairs. For each concept axis  $k$ , we randomly select a target label  $\\langle \\mathbf{e}_k' \\rangle$  from a set of candidates that differ from the one in the original prompt  $\\langle \\mathbf{e}_k \\rangle$ . We then measure the alignment score between the modified image  $\\hat{\\mathbf{x}}_{\\mathbf{e}_k \\rightarrow \\mathbf{e}_k'}$  with the modified text prompt  $y_{\\mathbf{e}_k \\rightarrow \\mathbf{e}_k'}$  using CLIP (Radford et al., 2021), following Gal et al. (2022). We also break down the score to each individual axis, by computing the alignment of the image to a prompt specifying only one axis, e.g. \"a photo of  $\\langle \\mathbf{e}_k' \\rangle$ \". All results are summarized in Table 1. Our method captures the characteristics of category particularly well and outperforms others in changing category or preserving category while changing color, which highlights its ability to extract disentangled concepts allowing for flexible compositions. InstructPix2Pix is more effective in changing color, but tends to do poorly in preserving the category, as seen in Figure 6. More details on our quantitative results can be found in A.4.\n\n\nWe further conduct a human evaluation. Each participant is presented the results from all three methods in a random order together with the editing instruction, and asked to rank them considering both realism and faithfulness to the instruction (see Appendix A.3 for details). We aggregate the responses from 20 people and report the average average score normalized to 0-1 in Table 1. More details of our setup can be found in A.3.\n\n\n# 4.4 ABLATIONS\n\n\nWe conduct an ablation study to understand the effects of the proposed encoder (as opposed per instance optimization) and the text anchoring loss  $\\mathcal{L}_k^{\\mathrm{anchor}}$ . We use the same evaluation dataset and the CLIP-alignment metric as described in Section 4.3. As shown in Fig. 8 and Table 1, removing  $\\mathcal{L}_k^{\\mathrm{anchor}}$  and the encoders deteriorates disentanglement of the different concept axes due to severe overfitting, resulting in poor recomposition results. More results are included in the Appendix.\n\n\n# 5 CONCLUSION\n\n\nIn this paper, we have presented a framework for learning language-informed visual concepts from images, by simply distilling from pre-trained vision-language models. After training, the concept encoders extract disentangled concept embeddings along various concept axes specified by language, which can be remixed or edited to generate images with novel concept compositions. We conduct thorough evaluations both quantitatively and qualitatively, and demonstrate that our approach yields superior results in visual concept editing compared to prior work.\n\n\n# REFERENCES\n\n- Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H Bermano. Domain-agnostic tuning-encoder for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06925, 2023.\n\n- Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from a single image. SIGGRAPH Asia, 2023.\n\n- Kobus Barnard, Pinar Duygulu, David Forsyth, Nando de Freitas, David M. Blei, and Michael I. Jordan. Matching words and pictures. JMLR, 2003.\n\n- Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\n\n- Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023.\n\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020.\n\n- Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models, 2023a.\n\n- Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Michal Irani, Inbar Mosseri, and Lior Wolf. The hidden language of diffusion models. arXiv preprint arXiv:2306.00966, 2023b.\n\n- Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-gan: Interpretable representation learning by information maximizing generative adversarial nets. NeurIPS, 29, 2016.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.\n\n- Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. NeurIPS, 2023.\n\n- Li Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE TPAMI, 2006.\n\n- R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. Learning object categories from google's image search. In ICCV, 2005.\n\n- Bruce Fraser, Fred Bunting, and Chris Murphy. Real world color management, 2004.\n\n- Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.\n\n- Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Trans. Graph., 2023.\n\n- Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.\n\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014.\n\n- Chi Han, Jiayuan Mao, Chuang Gan, Josh Tenenbaum, and Jiajun Wu. Visual concept-metaconcept learning. In NeurIPS, 2019.\n\n- Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering interpretable gan controls. NeurIPS, 2020.\n\n- Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.\n\n- Aaron Hertzmann, Charles E Jacobs, Nuria Oliver, Brian Curless, and David H Salesin. Image analogies. In SIGGRAPH, 2001.\n\n- Irina Higgins, Loic Matthew, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017.\n\n- Kyle Hsu, Will Dorrell, James C. R. Whittington, Jiajun Wu, and Chelsea Finn. Disentanglement via latent quantization. In NeurIPS, 2023.\n\n- Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019.\n\n- Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.\n\n- Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023a.\n\n- Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023b.\n\n- Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. In ECCV, 2022.\n\n- Nan Liu, Yilun Du, Shuang Li, Joshua B Tenenbaum, and Antonio Torralba. Unsupervised compositional concepts discovery with text-to-image generative models. ICCV, 2023.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\n- Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In ICLR, 2019.\n\n- Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.\n\n- OpenAI. Gpt-4 technical report, 2023.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.\n\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n\n- Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\n- Kanchana Ranasinghe and Michael Ryoo. Language-based action concept spaces improve video self-supervised learning. NeurIPS, 2023.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pp. 10684-10695, 2022.\n\n- Nataniel Ruiz, Yanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023.\n\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015.\n\n- Donald G Saari. Selecting a voting method: the case for the borda count. Constitutional Political Economy, 34(3):357-366, 2023.\n\n- Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022.\n\n- Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022.\n\n- StabilityAI. DeepFloyd IF. https://github.com/deep-floyd/IF.\n\n- Ad\u00e9la \u0160ubrtov\u00e1, Michal Luk\u00e1\u010d, Jan \u010cech, David Futschik, Eli Shechtman, and Daniel Sykora. Diffusion image analogies. In SIGGRAPH, pp. 1-10, 2023.\n\n- Yael Vinker, Andrey Voynov, Daniel Cohen-Or, and Ariel Shamir. Concept decomposition for visual exploration and inspiration. arXiv preprint arXiv:2305.18203, 2023.\n\n- Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan latent space. In ICML, 2020.\n\n- Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.\n\n- Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\n\n- Yiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu, and Yin Li. Learning to generate scene graph from natural language supervision. In ICCV, 2021.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634683.7531369", "msecs": "753.0", "relativeCreated": "28576.611042022705", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph\n\n# Abstract\nLarge Language Models (LLMs) excel in code generation yet struggle with modern AI software engineering tasks. Unlike traditional function-level or file-level coding tasks, AI software engineering requires not only basic coding proficiency but also advanced skills in managing and interacting with code repositories. However, existing methods often overlook the need for repository-level code understanding, which is crucial for accurately grasping the broader context and developing effective solutions. On this basis, we present RepoGraph, a plug-in module that manages a repository-level structure for modern AI software engineering solutions. RepoGraph offers the desired guidance and serves as a repository-wide navigation for AI software engineers. We evaluate RepoGraph on the SWE-bench by plugging it into four different methods of two lines of approaches, where RepoGraph substantially boosts the performance of all systems, leading to a new state-of-the-art among open-source frameworks. Our analyses also demonstrate the extensibility and flexibility of RepoGraph by testing on another repo-level coding benchmark, CrossCodeEval.\n\n\n# 1 INTRODUCTION\n\n\nRecent advancements in large language models (LLMs) have showcased their powerful capabilities across various natural language processing tasks (OpenAI, 2023; Anil et al., 2023; Dubey et al., 2024), and now, coding-specific LLMs are emerging to tackle complex software engineering challenges (Hou et al., 2023; Fan et al., 2023), such as Code-Llama (Rozi\u00e8re et al., 2023) and StarCoder (Li et al., 2023a). These coding-specific LLMs are capable of assisting users with various software engineering tasks, even achieving human-level performance in many function-level coding tasks, such as program synthesis (Chen et al., 2021; Austin et al., 2021), code annotation (Yao et al., 2019), bug fixing (Tufano et al., 2019), and code translation (Rozi\u00e8re et al., 2020).\n\n\nReal-world software engineering often extends beyond single function or self-contained code files. Applications are typically built as repositories containing multiple interdependent files, modules, and libraries (Bairi et al., 2024). These complex structures require a holistic understanding of the entire codebase to perform tasks such as code completion (Shrivastava et al., 2023; Ding et al., 2023), feature addition (Liang et al., 2024), or issue resolving (Jimenez et al., 2024). Recent benchmarks like SWE-Bench (Jimenez et al., 2024) have been proposed to evaluate LLMs on real-world GitHub issues. It requires LLMs to modify the repository to resolve the issue, either by fixing a bug or introducing a new feature. This task is particularly challenging because it requires navigating complex code bases, understanding intricate dependencies between code files, and ensuring that changes integrate seamlessly without introducing new issues, which highlights the difficulties in scaling from function-level to repository-level understanding, as expounded in Figure 1.\n\n\nA key step in addressing repository-level tasks is to understand the structure of a repository and identify related code. To achieve this, retrieval-augmented generation (RAG) and its variants (Zhang et al., 2023; Phan et al., 2024; Wu et al., 2024) have been leveraged, in a procedural manner, to retrieve relevant code files across the repository first, providing context for LLMs for further edition.\n\n\n(a) Function-level Coding Problem\n\n\nInput text: Write a python function to find the first repeated character in a given string.\n\n\n![](images/9883c20b91c9601cb769ab3107450abf2647de1e7bf46fa8a1a880fdccf8bb42.jpg)\n\n\n1. def first_repeated_char(str1):\n\n- 2. for index,c in enumerate(str1):\n\n- 3. if str1[:index+1].count(c) > 1:return c\n\n- 4. return \"None\"\n\n\n(b) Repository-level Coding Problem\n\n\nIssue: modeling's \"separability_matrix\" does not compute separability correctly for CompoundModels...\n\n\n![Figure 1: The illustration of (a) a function-level coding problem from HumanEval (Chen et al., 2021) and (b) a repository-level coding problem from SWE-Bench (Jimenez et al., 2024).](images/4211fe98f1c4cec12d3fb9abb19390e3a7b311d09f6bab3ef01c17d82545fe3a.jpg)\n\n\n(i) Understand intricate dependencies\n\n\n![](images/f1d49d72c356a5a629a423aa534583b49eaa79e7dd4dc8dd5b51e970f43c3a9e.jpg)\n\n\nGenerated patch:\n\n- diff --git a/astropy/modeling/separable.py\n\n- --- a/astropy/modeling/separable.py\n\n- $+ + +$  b/astropy/modeling/separable.py @@-242,7  $+ 2 4 2 , 7 \\text{\u5df2}$  def cstack(left, right):...\n\n\n![](images/e71c4face282d548b2b6dd6d7f72227649ae3e018e4af3e9fb3eedb3a11ef9b3.jpg)\n\n\nUnit test:\n\n\n<table><tr><td>test_coord_matrix</td><td>X</td></tr><tr><td>test_cdot</td><td>\u2713</td></tr><tr><td>test_arith_opera</td><td>X</td></tr></table>\n\n\n(iii) Resolve without introducing new issues\n\n\nHowever, indexing at file-level can only identify semantically similar but not genuinely related code snippets. Instead of using RAG, recent approaches like Agentless (Xia et al., 2024) construct a skeletal format for each file, and directly prompt LLMs to identify relevant files and code lines. However, this method still treats code repositories as flat documents (Zhang et al., 2024), which suffers from limitations of repository structure such as the intricate inter-dependencies cross files. An alternative approach is to design agent frameworks (Yang et al., 2024; Wang et al., 2024), which enables LLMs to interact with repositories using actions. While LLM agents can freely determine the next action based on current observations, without the grasp of global repository structures, they tend to focus narrowly on specific files, resulting in local optimums. Addressing these limitations requires going beyond semantic matching and developing techniques that enable a deeper understanding of the codebase structure. This will allow LLMs to leverage fine-grained context across multiple files and function calls, facilitating more informed, repository-wide decision-making for coding tasks.\n\n\nMotivated by this, we propose REPOGRAPH, a plug-in module designed to help LLMs-based AI programmers leverage the code structure of an entire repository. REPOGRAPH is a graph structure and operates at the line level, offering a more fine-grained approach compared to previous file-level browsing methods. Each node in the graph represents a line of code, and edges represent the dependencies of code definitions and references. REPOGRAPH is constructed via code line parsing and encodes the structured representation of the entire repository. Sub-graph retrieval algorithms are then used to extract ego-graphs, which represent the relationships of a central node (in our case, specific keywords). These ego-graphs can be smoothly integrated with both procedural and agent frameworks, offering key clues that provide a more comprehensive context for LLMs to solve real-world software engineering problems.\n\n\nTo assess REPOGRAPH's effectiveness and versatility as a plug-in module, we integrate it with four existing software engineering frameworks and evaluate its performance using SWE-bench, a recent benchmark for AI software engineering. Experiment results show that REPOGRAPH boosts the success rate of existing methods for both agent and procedural frameworks by achieving an average relative improvement of  $32.8\\%$ . We also test REPOGRAPH on CrossCodeEval to verify its transferability to general coding tasks that require repository-level code understanding. Additionally, we systematically analyze different sub-graph retrieval algorithms and integration methods. Together with error analyses, we hope to shed light on future works targeting modern AI software engineering.\n\n\n# 2 RELATED WORKS\n\n\n# 2.1 LLM-BASED METHODS FOR AI SOFTWARE ENGINEERING\n\n\nRecently, there has been a significant increase in research focused on AI-driven software engineering, which can be broadly categorized into two primary approaches: (i) LLM agent-based frameworks and (ii) SWE-featured procedural frameworks. While this field has advanced rapidly, with most methods being released as proprietary solutions for industry applications (Cognition, 2024), our related work section will concentrate specifically on open-source frameworks.\n\n\nLLM agent-based framework equips large language models (LLMs) with a set of predefined tools, allowing agents to iteratively and autonomously perform actions, observe feedback, and plan future steps (Yang et al., 2024; Zhang et al., 2024; Wang et al., 2024; Cognition, 2024). While the exact\n\n\nset of tools may vary across different agent frameworks, they typically include capabilities such as opening, writing, or creating files, searching for code lines, running tests, and executing shell commands. To solve a problem, agent-based approaches involve multiple actions, with each subsequent turn depending on the actions taken in previous ones and the feedback received from the environment. For example, SWE-agent (Yang et al., 2024) facilitates interactions with the execution environment by designing a special agent-computer interface (ACI). There are various actions, including \"search and navigation\", \"file viewer and editor\", and \"context management\". Another work, AutoCodeRover (Zhang et al., 2024), further offers fine-grained searching methods for LLM agents in better contexts without an execution process. Specifically, it supports class and function-level code search. OpenDevin (Wang et al., 2024), initiated after Devin (Cognition, 2024), is a community-driven platform that integrates widely used agent systems and benchmarks. The action space design of OpenDevin is highly flexible, requiring LLM agents to generate code on the fly.\n\n\nSWE-featured procedural frameworks typically follow a two-step Localize/Search-Edit approach, as seen in existing literature (Zhang et al., 2023; Wu et al., 2024; Liang et al., 2024; Xia et al., 2024). The localize step focuses on identifying relevant code snippets, while the edit step involves completing or revising the code. Some works introduce additional steps to further enhance performance, such as the Search-Expand-Edit approach (Phan et al., 2024). Retrieval (Lewis et al., 2020) is a popular technique used for localization, allowing models to search for relevant code snippets from large repositories by treating issue descriptions as queries and code snippets as indexed data. Some approaches use a sliding window to ensure completeness (Zhang et al., 2023). Besides, Agentless (Xia et al., 2024) is a recently developed method that uses LLMs to directly identify relevant elements for editing within code repositories. It first recursively traverses the repository structure to generate a format that aligns files and folders vertically, with indices for subdirectories. This structure and the issue description are then input into the LLM, which performs a hierarchical search to identify the top-ranked suspicious files requiring further inspection or modification.\n\n\n# 2.2 REPOSITORY-LEVEL CODING CAPABILITY\n\n\nThe evaluation of coding capabilities in AI systems has traditionally focused on function-level or line-level assessments (Lu et al., 2021; Chen et al., 2021; Austin et al., 2021), where individual code snippets or isolated functions are the primary units of analysis. Unlike previous studies, SWE-bench (Jimenez et al., 2024) highlights the trend of repository-level coding, driven by recent advances of coding-specific LLMs (Guo et al., 2024; Li et al., 2023b). It reflects the growing user demand to understand and contribute to entire projects rather than isolated functions (Ouyang et al., 2023), as well as\n\n\nsolving real-world problems in an end-to-end and automatic manner.\n\n\nTable 1: Comparison between our approach REPOGRAPH and existing methods for representing the repository on various aspects. *RepoUnderstander (Ma et al., 2024) and CodexGraph (Liu et al., 2024) are concurrent works to ours.\n\n\n<table><tr><td>Model</td><td>Line-level</td><td>File-level</td><td>Repo-level</td></tr><tr><td>DraCo</td><td>X</td><td>\u2713</td><td>X</td></tr><tr><td>Aider</td><td>\u2713</td><td>X</td><td>X</td></tr><tr><td>RepoUnderstander*</td><td>X</td><td>\u2713</td><td>\u2713</td></tr><tr><td>CodexGraph*</td><td>X</td><td>\u2713</td><td>\u2713</td></tr><tr><td>REPOGRAPH</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></table>\n\n\nLong before the LLM era, the software engineering community has been studying the navigation of code repositories, including using eye-tracking data (Busjahn et al., 2015) and exploring interconnections (Begel et al., 2010). Then pre-trained code LLMs incorporate repository-level information such as file dependencies. But tasks at the repository level often involve more intricate call relationships within their context. Recent works like RepoCoder (Zhang et al., 2023) and RepoFuse (Liang et al., 2024) have started integrating RAG modules to harness additional information from repositories. Building on this, subsequent research has focused on embedding repository-level context into their methodologies. For instance, DraCo (Cheng et al., 2024) introduces importing relationships between files, CodePlan maintains the code changes by LLMs with incremental dependency analysis, while Aider (Gauthier, 2024) employs PageRank (Page, 1999) to identify the most significant contextual elements. RepoUnderstander (Ma et al., 2024) and CodexGraph (Liu et al., 2024) model code files as a knowledge graph. Despite similarities in representation, methods vary in how they retrieve information from these structures and utilize it for downstream tasks. Table 1 summarizes the differences between these methods and REPOGRAPH. REPOGRAPH surpasses previous approaches by effectively integrating context at the line, file, and repository levels.\n\n\n![(a) RepoGraph Construction](images/2d58b3de9c36ef24e6e7b44b1bb321b3caa41bb05fa7cd8ef79b2a34d51bf7ac.jpg)\n\n\n![(c) Integration with Agent Frameworks Figure 2: An in-depth illustration of (a) the construction, (b) the integration with procedural frameworks, and (c) the integration with agent frameworks of REPOGRAPH. Given a code repository, we first utilize AST to construct  $\\mathcal{G} = \\{\\mathcal{V},\\mathcal{E}\\}$ , where  $\\mathcal{G}$  consists of \"reference\" and \"definition\" node,  $\\mathcal{E}$  includes \"invoke\" and \"contain\" relations (files and code lines shown in corresponding color). The constructed REPOGRAPH are then used in procedural frameworks by adding sub-retrieval results into each step, and agent frameworks by adding graph retrieval as an additional action \"search_repograph\". A simplified version can be found in Figure 10.](images/126123f3edba1131af28c26f023bf724a182b63f285c8a33a79fc30aa0bf46e2.jpg)\n\n\n# 3 REPOGRAPH\n\n\nThis section introduces REPOGRAPH, a novel plug-in module that can be seamlessly integrated into existing research workflows for both agent-based and procedural frameworks. The primary goal of REPOGRAPH is to provide a structured way to analyze and interact with complex codebases, enabling detailed tracing of code dependencies, execution flow, and structural relationships across the repository. In the following sections, we will provide a detailed description of REPOGRAPH's construction, its underlying representation, and its utility across various scenarios. The overall architecture is depicted in Figure 2, highlighting its key components and operational flow.\n\n\n# 3.1 CONSTRUCTION\n\n\nGiven a repository-level coding task, the first step is to carefully examine the repository structure so that the necessary information can be collected. The input for REPOGRAPH construction is a repository, i.e., a collection of its folders and files, while the output is a structured graph, where each node is a code line, and each edge represents the dependencies in between. REPOGRAPH enables tracing back to the root cause of the current issue and gathering dependent code context to help solve the problem. The construction process of REPOGRAPH could be divided into three key steps.\n\n\nStep 1: Code line parsing. We first traverse the entire repository using a top-down approach to identify all code files as candidates for next-step parsing. This is accomplished by filtering based on file extensions, retaining only those with relevant code file suffixes (e.g., .py) while excluding other file types (e.g., .git or requirements.txt), which might be noisy and irrelevant for coding tasks. For each code file, we utilize tree-sitter  ${}^{2}$  to parse the code,leveraging its Abstract Syntax Tree (AST) framework. The AST provides a tree-based representation of the abstract syntactic structure of the source code, enabling the identification of key elements such as functions, classes, variables, types, and other definitions. While recognizing these definitions is crucial, tracing their usage and references throughout the code is equally important. Tree-sitter facilitates this by capturing the definitions and tracking where they are utilized or referenced within the codebase. For example, in figure 2, we not only identify definitions like class Model and its inherent methods but also references like self._validate_input.units(). After processing each line of code with a tree-sitter, we selectively retain lines that involve function calls and dependency relations, discarding extraneous information. Our focus is primarily on the functions and classes, as these represent the core structural components of\n\n\nthe code. By concentrating on these elements and their interrelationships, REPOGRAPH optimizes the analysis process by excluding less significant details, such as individual variables, which tend to be redundant and less relevant for further processing.\n\n\nStep 2: Project-dependent relation filtering. After the previous parsing step, we obtain code lines with calling and dependency relations. However, not all relations are useful for fixing issues. Specifically, many default and built-in function/class calls could distract from the project-related ones. Therefore, we additionally introduce a filtering process that excludes the repository-independent relations. Two types of such relations exist: (i) global relation refers to Python standard and built-in functions and classes. (ii) local relation are introduced by third-party libraries, which are specific to the current code file. For global relations, we maintain a comprehensive list of methods from standard and built-in libraries, excluding any identified relations from this list. The list is empirically constructed by gathering methods of the builtins library and default methods such as \"list\" and \"tuple\". For example, in figure 2, line inputs = len(input) is excluded since \"len\" is a default method. For local relations, we parse import statements in the code to identify third-party methods that are included, and exclude them accordingly. A detailed illustration of this step can be found in Figure 11.\n\n\nStep 3: Graph organization. At this stage, we construct REPOGRAPH using code lines as the fundamental building units. The graph can be represented as  $\\mathcal{G} = \\{\\mathcal{V},\\mathcal{E}\\}$ , where  $\\mathcal{V}$  represents the set of nodes, with each node corresponding to a line of code, and  $\\mathcal{E}$  represents the set of edges, capturing the relationships between these code lines. Each node in  $\\mathcal{V}$  contains attributes to represent its meta-information, such as line_number, file_name, directory, etc. Additionally, we classify each code line as either a \"definition\" (def) or a \"reference\" (ref) to a particular module. A \"def\" node corresponds to the line where a function, class, or variable is initially defined, while a \"ref\" node indicates a code line where this entity is referenced or invoked elsewhere in the code. Similar to soft links to \"def\" nodes, \"ref\" nodes also represent other variations of invoking methods. For example, in Figure 2, the class definition \"class Model\" would be a \"def\" node, while any subsequent usages of Model would be \"ref\" nodes. Each \"def\" node may have multiple \"ref\" nodes associated with it, as a single function or class can be referenced in various places throughout the code. We define two types of edges:  $\\mathcal{E}_{\\text{invoke}}$  and  $\\mathcal{E}_{\\text{contain}}$ . The triple  $(\\mathcal{V}_1,\\mathcal{E}_{\\text{contain}},\\mathcal{V}_2)$  denotes that  $\\mathcal{V}_1$  (e.g., a function definition) contains another module  $\\mathcal{V}_2$  (e.g., an internal function or class). The edge  $\\mathcal{E}_{\\text{contain}}$  typically connects a \"def\" node to its internal components. In contrast,  $\\mathcal{E}_{\\text{invoke}}$  represents an invocation relationship, usually connecting a \"def\" node to a \"ref\" node, where the reference node includes a dependency on the definition.\n\n\n# 3.2 UTILITY\n\n\nThe constructed REPOGRAPH serves as a structured representation of the current repository and facilitates better-related information collection and aggregation. For information collection based on REPOGRAPH, specifically, we use one search term each time for subgraph retrieval. Search terms are the key functions or classes that are determined by current states. For example, \"separability_matrix\" is the initial search term in Figure 2(c). We retrieve the  $k$ -hop ego-graphs (Hu et al., 2024) with the search term in the centric. The ego-graph is crucial for solving the problem because it focuses on the immediate relationships (Jin et al., 2024) around the search term, capturing the relevant dependencies and interactions within the repository, which is key to understanding the functional context. Additionally, the retrieved content explicitly contains information at both the method and line levels and implicitly expresses the grouping at the file level.\n\n\nThis process is abstracted via search_repograph() as illustrated in the middle of Figure 2. The retrieved  $k$ -hop ego-graph will be flattened for further processing. We also tried other variants for integration later in Section 5.2 and their performance in Table 4. We narrate how REPOGRAPH could be plugged in with existing representative research lines in the following.\n\n\nIntegration with procedural framework. In a procedural framework, LLMs are usually prompted in \"localization\" and \"editing\" stages with the given repository context and issue description. In this case, we use search_repograph() before both stages, leveraging our REPOGRAPH to assist in making more informed decisions at each step. For example, in Figure 2, we first include the subgraph of \"separability_matrix\" for localization, and then use the localized result \"Model\" to search in the\n\n\nedition stage. To implement the strategy, we flatten the context of retrieved ego-graphs and append it as part of the prompt. As a result, the LLM generation is conditioned on both retrieved ego-graphs and the context provided by baseline methods, helping the model preserve nuances.\n\n\nIntegration with agent framework. A significant difference in existing agent frameworks is the action space design, as expounded in Section 2. To leverage the power of REPOGRAPH, we put search_repograph() as an additional action in the action space. The agent decides when and where to use this action. The search term is also determined by the agent accordingly. The returned subgraph will be flattened and used as an observation for the next state.\n\n\n# 4 EXPERIMENTS\n\n\n# 4.1 SETUP\n\n\nWe evaluated REPOGRAPH as a plug-in component, i.e., integrated into existing baseline models of the two aforementioned research lines to assess its performance. We use the same baseline settings and configurations when incorporating REPOGRAPH to ensure a fair comparison.\n\n\nDataset. We test REPOGRAPH in SWE-bench-Lite<sup>3</sup>. Each problem in the dataset requires submitting a patch to solve the underlying issue described in the input issue description. The goal is to generate a patch that accurately revises the relevant portions of the codebase within the repository, ensuring that all test scripts included in the dataset are successfully executed.\n\n\nBaselines. We integrate REPOGRAPH with representative methods from both aforementioned research lines. (i) For procedural frameworks, we evaluate the widely used traditional method, RAG (Lewis et al., 2020), as well as Agentless (Xia et al., 2024), an open-source state-of-the-art approach in this direction. For RAG, we follow its initial setting and use BM25 for file-level retrieval. After that, we append the context of REPOGRAPH after the code files as part of the prompt. Agentless first performs a hierarchical localization in terms of \"file-class/function- edits\" and then conducts repair based on localization. The context of REPOGRAPH is inserted in every step of Agentless. (ii) For agent frameworks, we consider SWE-agent (Yang et al., 2024) and AutoCodeRover (Zhang et al., 2024). For both frameworks, we add an additional action \"search_repograph\" for the LLM agent as described in Section 3. All the choices in the two research lines incorporate GPT-4 and GPT-4o based methods to ensure the generalization. Detailed implementations and prompts used can be found in Appendix A.\n\n\nEvaluation metrics. We evaluate all methods across two key dimensions: Accuracy and Average Cost. (i) For accuracy, we report the resolve rate and patch application rate. The resolve rate represents the percentage of issues successfully resolved across all data points. An issue is considered resolved if the submitted patch passes all test scripts. To assess the patch application rate, we attempt to apply the generated patches to the repository using the patch program, counting only successful applications toward this metric. (ii) To evaluate cost efficiency, we report two metrics: average cost and average tokens, which refer to the inference cost and the number of input/output tokens used when querying the LLMs, respectively.\n\n\nConfigurations. We use the same GPT version as in the baselines in the experiments. We used GPT-4o (2024-05-13) and GPT-4-Turbo (gpt-4-1106-preview) from OpenAI for evaluation and analyses in our experiments. All evaluation processes are performed in a containerized Docker environment  $^{4}$ , ensuring stability and reproducibility, made possible through contributions from the open-source community. For plug-ins with procedural frameworks, it usually takes around 2-3 hours to finish. For agent frameworks, the inference time is larger, up to around 10 hours.\n\n\n# 4.2 EXPERIMENT RESULTS\n\n\nTable 2 presents the main evaluation results of all baseline methods and the corresponding performance with REPOGRAPH (+REPOGRAPH) as a plug-in in the SWE-bench-Lite test\n\n\nTable 2: Results of REPOGRAPH with open-source baselines in two research lines, including procedural and agent frameworks. Numbers of accuracy-related metrics are directly taken from the leaderboard, while the cost-related ones are computed from the corresponding trajectories<sup>6</sup>.\n\n\n<table><tr><td>Methods</td><td>LLM</td><td>resolve</td><td>Accuracy # samples</td><td>patch apply</td><td>Avg. Cost $ cost</td><td># tokens</td></tr><tr><td colspan=\"7\">Procedural frameworks</td></tr><tr><td>RAG</td><td>GPT-4</td><td>2.67</td><td>8</td><td>29.33</td><td>$0.13</td><td>11,736</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>5.33\u21912.66</td><td>16\u21918</td><td>47.67\u219118.34</td><td>$0.17</td><td>15,439</td></tr><tr><td>Agentless</td><td>GPT-4o</td><td>27.33</td><td>82</td><td>97.33</td><td>$0.34</td><td>42,376</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>29.67\u21912.34</td><td>89\u21917</td><td>98.00\u21910.67</td><td>$0.39</td><td>47,323</td></tr><tr><td colspan=\"7\">Agent frameworks</td></tr><tr><td>AutoCodeRover</td><td>GPT-4</td><td>19.00</td><td>57</td><td>83.00</td><td>$0.45</td><td>38,663</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>21.33\u21912.33</td><td>64\u21917</td><td>86.67\u21913.67</td><td>$0.58</td><td>45,112</td></tr><tr><td>SWE-agent</td><td>GPT-4o</td><td>18.33</td><td>55</td><td>87.00</td><td>$2.53</td><td>498,346</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>20.33\u21912.00</td><td>61\u21916</td><td>90.33\u21913.33</td><td>$2.69</td><td>518,792</td></tr></table>\n\n\nset. We also report the number of correct samples for each method. The performance increase is marked by  $\\uparrow$ num. We also tested other (open-source) LLMs using Claude-3.5-Sonnet (claude-3-5-sonnet-20240620) (Anthropic, 2024) from Anthropic in Appendix C. Based on the results, we have the following key observations:\n\n- (i) REPOGRAPH brings consistent performance gain for all combinations of frameworks and LLM model bases. Specifically, REPOGRAPH achieves an absolute improvement of  $+2.66$  and  $+2.34$  in terms of the resolve rate for RAG and Agentless, respectively, which is  $99.63\\%$  and  $8.56\\%$  of relative improvement. The notable improvement demonstrates the effectiveness of our REPOGRAPH in adapting to various scenarios by inducing relevant code context and performing precise code editorings. Additionally, our best performance so far by plugging in Agentless, 29.67, achieves the state-of-the-art performance on the benchmark among all open-source methods.\n\n- (ii) Performance gain brought by REPOGRAPH is slightly larger on procedural frameworks than agent ones. With procedural frameworks, REPOGRAPH correctly fixes more issues than agent ones. This could be due to two primary reasons. Firstly, we observed that mature procedural frameworks tend to achieve better baseline performance than agent-based frameworks on SWE-bench. The initial definitive nature of procedural frameworks, with their well-defined running flow and structure, allows them to leverage plug-ins more effectively. Another reason is that this deterministic behavior reduces the complexity that arises from dynamic decision-making, a key characteristic of agent-based systems, thereby enabling a smoother integration of performance improvements.\n\n- (iii) Performance gain brought by REPOGRAPH does not rely on more costs. We also compute and report each method's average cost and token consumption. By introducing REPOGRAPH, we manage to reduce the costs associated with managing the entire repository while achieving comparable or even superior performance. As shown in Table 2, the additional token cost introduced by RepoGraph is justified given the significant performance improvement it enables, particularly when compared to the substantially higher costs incurred by previous methods like SWE-agent. This indicates REPOGRAPH's performance gains are not mainly due to increased token usage.\n\n- (iv) Average costs are generally larger on agent frameworks with REPOGRAPH. This phenomenon is especially obvious with SWE-agent, as it allows the agent to freely determine the next action based on the current observation. We also found that the integration with agent frameworks usually leads to larger cost increases, as exemplified by  $+0.13$  and  $+0.18$  with AutoCodeRover and SWE-agent, respectively. The reason lies in the large exploration space in agent frameworks. The agents might call the search_repograph() action many times, which leads to the explosion of prompt contexts. We encourage users to be mindful of cost and to adopt a more granular approach to cost control when integrating REPOGRAPH into the agent framework in the future.\n\n\n# 5 ANALYSIS\n\n\nThis section presents a detailed analysis to demonstrate that the additional context provided by REPOGRAPH is beneficial for the task. We begin by analyzing localization accuracy in comparison to the gold-standard patch. Next, we explore various REPOGRAPH configurations, focusing on how the additional context can be effectively integrated into the existing system. Finally, we perform an in-depth error analysis, highlighting aspects where REPOGRAPH can be further improved. For more analyses including resolve rate in various aspects and action distributions of agent frameworks, please refer to Appendices C.\n\n\n# 5.1 LOCALIZATION COVERAGE\n\n\nA crucial step in issue resolution is accurately identifying the correct locations within the code that require modification. Proper localization is essential, as it forms the foundation for generating an effective and accurate patch. Without this step, the quality of the fix may be compromised, leading to incomplete or incorrect solutions. In three granularity, we compute the percentage of problems where the edit locations match the ground truth patch. Namely, file-level, function-level, and line-level. We report that a patch con\n\n\nTable 3: Percentage of problems for accurate edition localizations with respect to file, function, and line levels. All the numbers are computed from the corresponding generated patches.\n\n\n<table><tr><td>Methods</td><td>LLM</td><td>file</td><td>function</td><td>line</td></tr><tr><td>RAG</td><td>GPT-4</td><td>47.3</td><td>23.3</td><td>12.7</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>51.7\u21914.4</td><td>25.3\u21912.0</td><td>14.3\u21911.6</td></tr><tr><td>Agentless</td><td>GPT-4o</td><td>68.7</td><td>51.0</td><td>34.3</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>74.3\u21915.6</td><td>54.0\u21913.0</td><td>36.7\u21912.4</td></tr><tr><td colspan=\"5\">Agent frameworks</td></tr><tr><td>AutoCodeRover</td><td>GPT-4</td><td>62.3</td><td>42.3</td><td>29.0</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>69.0\u21914.7</td><td>45.7\u21913.4</td><td>31.7\u21912.7</td></tr><tr><td>SWE-agent</td><td>GPT-4o</td><td>61.7</td><td>46.3</td><td>32.3</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>67.3\u21915.6</td><td>49.3\u21913.0</td><td>35.0\u21912.7</td></tr></table>\n\n\ntains the correct location if it edits a superset of all locations in the ground truth patch.\n\n\nTable 3 presents the results of our analysis. We observed that integrating REPOGRAPH with all baseline methods significantly improves file-level accuracy, whereas the enhancement of accuracy in line-level is comparatively modest. This result aligns with our expectations, as file-level localization is the most coarse-grained, making it inherently easier to improve. In contrast, line-level localization, being the most fine-grained, poses a greater challenge due to its need for more precise identification of code segments. Additionally, we found that although line-level accuracy improvements are more pronounced for agent frameworks, their overall resolve rate is lower than that of procedural frameworks, as shown in Table 2. This discrepancy can be attributed to the fact that localization, while necessary for generating a final patch, is insufficient. The success of the final revision still heavily relies on the underlying capabilities of LLMs. Agent frameworks, designed to operate in a trial-and-error fashion, are particularly susceptible to error accumulation. As these frameworks iteratively refine their patches, small inaccuracies in earlier localization steps can propagate and magnify throughout the process, ultimately reducing the overall resolve rate. Procedural frameworks, on the other hand, follow a more structured and deterministic approach to localization and patch generation. They typically localize and fix issues in a single, more direct step, which can help mitigate the compounding of errors.\n\n\n# 5.2 INVESTIGATION OF REPOGRAPH VARIANTS\n\n\nIn this section, we investigate the efficacy of various combinations of sub-graph retrieval and integration techniques as outlined in Section 3. We explore two sub-graph retrieval variants and two integration methods. Specifically, for sub-graph retrieval, we index  $k$ -hop ego-graphs where  $k$  is set to 1 and 2. We limit our exploration to  $k$  values up to 2 due to the extensive context required for integration and the potential introduction of noise or irrelevant nodes for  $k \\geq 3$ . For the integration methods, we employ two distinct approaches: (i) directly flattening the textual sub-graph by explicitly detailing the relationships between the search term and its neighboring nodes, and (ii) leveraging an LLM first to summarize the sub-graph in terms of the core modules and salient dependencies, before proceeding with further processing. Detailed implementations of these variants and prompts used can be found in Appendix B.1.\n\n\nTable 4: The number of nodes, edges, and tokens of REPOGRAPH and its variants. For different retrieval and integration variants, we report the average number on the test set. \"summ.\" refers to the summarized version by LLMs of the retrieved ego-graph.\n\n\n<table><tr><td>Metrics</td><td>REPOGRAPH</td><td>1-hop + flatten</td><td>1-hop + summ.</td><td>2-hop + flatten</td><td>2-hop + summ.</td></tr><tr><td># Nodes</td><td>1419.3</td><td>11.6</td><td>11.6</td><td>54.5</td><td>54.5</td></tr><tr><td># Edges</td><td>26392.1</td><td>37.1</td><td>37.1</td><td>89.9</td><td>89.9</td></tr><tr><td># tokens</td><td>-</td><td>2310.7</td><td>717.5</td><td>10505.3</td><td>1229.2</td></tr><tr><td>resolve rate</td><td>-</td><td>29.67</td><td>28.33</td><td>26.00</td><td>28.67</td></tr></table>\n\n\nWe begin by presenting some statistics for REPOGRAPH and its various configurations. Performance evaluations are conducted on Agentless with REPOGRAPH integrated as a plug-in module. Table 4 reports the number of nodes and edges within the (sub)-graphs. Notably, the average number of nodes and edges for REPOGRAPH across the SWE-bench dataset is quite substantial, featuring over 1,000 nodes and 25,000 edges. This highlights the comprehensive nature of the constructed structure. For the different variants, when  $k = 1$ , the information within REPOGRAPH is concentrated around the search term, resulting in an average of 11.6 nodes and 37.1 edges. As  $k$  increases to 2, the retrieved ego-graph expands exponentially, reaching an average of 54.5 nodes and 89.9 edges. Moreover, directly flattening the retrieved ego-graph often significantly increases the token count, frequently reaching several thousand tokens. However, utilizing an LLM as an additional summarizer greatly reduces the token count, typically around a thousand.\n\n\nTable 4 presents the resolve rates for four variants of our method. Notably, while the 2-hop variant incorporates additional information, directly flattening this information results in the poorest performance, with a resolve rate of  $26.00\\%$ , even lower than the original baseline. In contrast, incorporating summarization via the LLM significantly alleviates context length constraints and enhances information organization, thereby improving performance. For the 1-hop variant, however, we observed that adding summarization actually degrades performance, reducing the resolve rate to  $28.33\\%$ . We hypothesize that this occurs because the flat 1-hop ego-graph already contains comprehensive information that fits within the LLM's context window; thus, summarization may introduce inevitable information loss.\n\n\n# 5.3 TRANSFERIBILITY TEST\n\n\nTo demonstrate the representational power of REPOGRAPH for repositories and its transferability to tasks requiring an understanding of repository structures, we conducted experiments using the CrossCodeEval benchmark (Ding et al., 2023). CrossCodeEval is a multilingual code completion benchmark that is derived from real-world repositories. It is designed to emphasize numerous cross-file dependencies. We focus on problems using Python as the evaluation programming lan\n\n\nTable 5: Results on the subset of CrossCodeEval with GPT-4o and Deepseek-Coder-V2-LiteInstruct as the backbone LLMs.\n\n\n<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">Code Match</td><td colspan=\"2\">Identifier Match</td></tr><tr><td>EM</td><td>ES</td><td>EM</td><td>F1</td></tr><tr><td>Deepseek-Coder</td><td>10.2</td><td>57.3</td><td>16.6</td><td>49.1</td></tr><tr><td>+REPOGRAPH</td><td>19.7</td><td>67.8</td><td>29.3</td><td>58.9</td></tr><tr><td>GPT-4o</td><td>10.5</td><td>59.6</td><td>16.8</td><td>47.9</td></tr><tr><td>+REPOGRAPH</td><td>28.7</td><td>68.9</td><td>36.0</td><td>61.3</td></tr></table>\n\n\nguage, leading to 2,665 samples. For evaluation metrics, we follow the settings in the original paper and measure performance with code match and identifier match metrics, assessing accuracy with exact match (EM), edit similarity (ES), and F1 scores. In our experiments, the search terms are determined to be the function within which the current line is to be completed.\n\n\nTable 5 demonstrates the results on CrossCodeEval using GPT-4o and Deepseek-Coder as the backbone language model. The original LLMs struggles with repository-level tasks, evidenced by an EM score of only 10.8 for code matching and 16.7 for identifier matching. These results indicate significant limitations in handling code structure and variable usage in a broader repository context. However, with the integration of the REPOGRAPH method, there is a substantial improvement across all metrics with both LLMs. Particularly, we find that the improvement brought by REPOGRAPH is more pronounced when integrated with GPT-4o. A potential reason is that GPT-4o is better at inter\n\n\n![Figure 3: Venn diagram of REPOGRAPH and baselines on (a) procedural framework and (b) agent framework on SWE-Bench-Lite. We also plot the error distribution of failing cases against counterparts, e.g., detailed error distribution of 12 cases REPOGRAPH succeeds while Agentless fails.](images/8593889f7523e5c7cb665d49567ce8b44e18922f502c7e8a49cd0937fbf90ccd.jpg)\n\n\npreting the additional context information provided for better code reasoning. These improvements suggest that incorporating repository-level knowledge, as facilitated by REPOGRAPH, greatly enhances the model's ability to understand and generate more contextually accurate and semantically consistent code.\n\n\n# 5.4 ERROR ANALYSIS\n\n\nWe want to compare REPOGRAPH with the corresponding baselines to see the distribution of resolved cases and analyze the error reasons for unresolved cases. We plot a Venn diagram for representative methods in both procedural and agent frameworks in Figure 3, respectively. We manually examined all the unique error cases and defined three error categories. (i) Incorrect localization refers to the failure in accurately identifying code snippets, (ii) contextual misalignment happens when the generated patch fails to align with the broader context of the codebase, and (iii) regressive fix introduces new issues in resolving the original issues. More examples are in Appendix D.\n\n\nWe found that the improvement in agent frameworks is more complementary than procedural frameworks, with larger uniquely resolved cases of 22 compared with 12 in procedural frameworks. Together, they make even larger performance of  $31.33\\%$  and  $22.33\\%$ . The reason could also be attributed to the determinism of procedural frameworks. As a plug-in module, REPOGRAPH tends to make modifications on existing deterministic processes, resulting in larger overlaps in resolved issues compared with baselines. Agent frameworks, on the other hand, have quite different action distributions with REPOGRAPH as a plug-in (please refer to Appendix A.2). Therefore, the uniquely resolved cases are more compared with procedural frameworks. For error distributions, contextual misalignment is the most prevalent error type, followed by incorrect localization and regressive fixes for all methods, suggesting that while localization is often correct, the applied solutions may regress or fail to integrate contextually. The phenomenon also echoes with conclusion obtained in Section 5.1. This is intuitive as all the existing methods focus on providing a more comprehensive and desired context for LLMs to solve the task, which fundamentally depends on the power of backbone LLMs. We also found that when integrated with REPOGRAPH, the proportion of all three error types decreases (specifically for \"incorrect localization\"), indicating that REPOGRAPH is specifically useful in aggregating the related contexts of the current to-be-fixed issue.\n\n\n# 6 CONCLUSION AND DISCUSSION\n\n\nThis paper introduces REPOGRAPH, a plug-in module for modern AI software engineering. REPOGRAPH operates at the code-line level and offers desired navigation through code bases by incorporating and aggregating information at the line level, file level, and repository level through subgraph retrieval of ego-graphs. Extensive experiments on real-world SWE tasks show that REPOGRAPH significantly boosts the performance of both procedural and agent frameworks. In addition, REPOGRAPH is proved to be smoothly and effectively transferred to other tasks that require repository-level understanding. Future work could further investigate the generalizability of REPOGRAPH across different programming languages, frameworks, and developer environments. Real-time execution and feedback could also be explored to further enhance REPOGRAPH towards a dynamic version for better AI software engineering abilities.\n\n\n# REFERENCES\n\n- Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pittler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan First, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv.2312.11805.\n\n- Anthropic. Introducing claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. Accessed: 2024-10-01.\n\n- Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n\n- Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675-698, 2024.\n\n- Andrew Begel, Yit Phang Khoo, and Thomas Zimmermann. Codebook: discovering and exploiting relationships in software repositories. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1, ICSE '10, pp. 125-134, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781605587196. doi: 10.1145/1806799.1806821. URL https://doi.org/10.1145/1806799.1806821.\n\n- Teresa Busjahn, Roman Bednarik, Andrew Begel, Martha E. Crosby, James H. Paterson, Carsten Schulte, Bonita Sharif, and Sascha Tamm. Eye movements in code reading: relaxing the linear order. In Andrea De Lucia, Christian Bird, and Rocco Oliveto (eds.), Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension, ICPC 2015, Florence/Firenze, Italy, May 16-24, 2015, pp. 255-265. IEEE Computer Society, 2015. doi: 10.1109/ICPC.2015.36. URL https://doi.org/10.1109/ICPC.2015.36.\n\n- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fiotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.\n\n- Wei Cheng, Yuhan Wu, and Wei Hu. Dataflow-guided retrieval augmentation for repository-level code completion. In ACL, 2024.\n\n- Cognition. Devin, 2024. URL https://www.cognition.ai/blog/introducing-devin.\n\n- Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang. Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=wgDcbBMSfh.\n\n- Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aur\u00e9lien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi\u00e8re, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gr\u00e9goire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Ko-revaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783.\n\n- Angela Fan, Belize Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M Zhang. Large language models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FOSE), pp. 31-53. IEEE, 2023.\n\n- Paul Gauthier. Aider is ai pair programming in your terminal. https://aider chat/, 2024.\n\n- Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming - the rise of code intelligence, 2024. URL https://arxiv.org/abs/2401.14196.\n\n- Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: A systematic literature review. arXiv preprint arXiv:2308.10620, 2023.\n\n- Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao. GRAG: graph retrieval-augmented generation. CoRR, abs/2405.16506, 2024. doi: 10.48550/ARXIV.2405.16506. URL https://doi.org/10.48550/arXiv.2405.16506.\n\n- Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66.\n\n- Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, and Jiawei Han. Graph chain-of-thought: Augmenting large language models by reasoning on graphs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 163-184, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024-findings-acl.11. URL https://aclanthology.org/2024.findings-acl.11.\n\n- Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 9459-9474, 2020.\n\n- Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee,\n\n- Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason T. Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! Trans. Mach. Learn. Res., 2023, 2023a. URL https://openreview.net/forum?id=KoFOg41haE.\n\n- Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023b.\n\n- Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng Di, Hongwei Chen, Chengpeng Wang, Gang Fan, et al. Repofuse: Repository-level code completion with fused dual context. arXiv preprint arXiv:2402.14323, 2024.\n\n- Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Wenmeng Zhou, Fei Wang, and Michael Shieh. Codexgraph: Bridging large language models and code repositories via code graph databases. arXiv preprint arXiv:2408.03910, 2024.\n\n- Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code understanding and generation. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/ hash/c16a5320fa475530d9583c34fd356ef5-Abstraction-round1.html.\n\n- Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. How to understand whole software repository? arXiv preprint arXiv:2406.01422, 2024.\n\n- OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.\n\n- Siru Ouyang, Shuohang Wang, Yang Liu, Ming Zhong, Yizhu Jiao, Dan Iter, Reid Pryzant, Chenguang Zhu, Heng Ji, and Jiawei Han. The shifted and the overlooked: A task-oriented investigation of user-GPT interactions. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 2375-2393, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.146. URL https://aclanthology.org/2023.emnlp-main.146.\n\n- L Page. The page-rank citation ranking: Bringing order to the web, 1999.\n\n- Huy N Phan, Hoang N Phan, Tien N Nguyen, and Nghi DQ Bui. Repohyper: Better context retrieval is all you need for repository-level code completion. arXiv preprint arXiv:2403.06095, 2024.\n\n- Baptiste Rozi\u00e8re, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised translation of programming languages. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/black/ed23fbf18c2cd35f8c7f8de44f85c08d-AAbstract.html.\n\n- Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and\n\n- Gabriel Synnaeve. Code llama: Open foundation models for code. CoRR, abs/2308.12950, 2023. doi: 10.48550/ARXIV.2308.12950. URL https://doi.org/10.48550/arXiv.2308.12950.\n\n- Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. Repository-level prompt generation for large language models of code. In International Conference on Machine Learning, pp. 31693-31715. PMLR, 2023.\n\n- Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. An empirical study on learning bug-fixing patches in the wild via neural machine translation. ACM Transactions on Software Engineering and Methodology (TOSEM), 28(4):1-29, 2019.\n\n- Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024.\n\n- Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, and Xiaofei Ma. Repoformer: Selective retrieval for repository-level code completion. arXiv preprint arXiv:2403.10059, 2024.\n\n- Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024.\n\n- John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024.\n\n- Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. Coacor: Code annotation for code retrieval with reinforcement learning. In The world wide web conference, pp. 2203-2214, 2019.\n\n- Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repeditor: Repository-level code completion through iterative retrieval and generation. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=q09vTY1Cqh.\n\n- Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement, 2024.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634685.0025344", "msecs": "2.0", "relativeCreated": "29826.008558273315", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Inductive Linguistic Reasoning with Large Language Models\n\n# Abstract\nEvaluating large language models (LLMs) on their linguistic reasoning capabilities is an important task to understand the gaps in their skills that may surface during large-scale adoption. In this work, we investigate the abilities of such models to perform abstract multilingual reasoning through the lens of linguistic puzzles on extremely low-resource languages. As these translation tasks involve inductive and deductive reasoning from reference instances, we examine whether diverse auxiliary demonstrations can be automatically induced from seed exemplars, through analogical prompting. We employ a two-stage procedure, first generating analogical exemplars with a language model, and then applying them in-context along with provided target language exemplars. We explore various combinations of language models as analogical generators and reasoning agents, testing different model sizes and specialized multilingual LLMs. Our results on the modeLing dataset show that analogical prompting is effective in eliciting models' knowledge of language grammar similarities, boosting the performance of GPT-4o by as much as 8.1\\% and Llama-3.1-405B by 5.9\\% over chain-of-thought approaches. These gains are realized with self-generated analogical demonstrations as well as those generated by weaker multilingual models. We also report several findings about interesting phenomena which drive linguistic reasoning performance, suggesting that such puzzles are a valuable benchmark for new reasoning methods.\n\n\n# 1 INTRODUCTION\n\n\nAs the capabilities of large language models (LLMs) continue to grow, it is necessary to develop ways of testing the boundaries of their ability to reason over a wide range of languages. In particular, adapting language models to low-resource languages is challenging due to a lack of high-quality annotated data in the target language for supervised fine-tuning. This has led to zero-shot and few-shot transfer learning approaches being more commonly employed (Zoph et al., 2016; Nguyen & Chiang, 2017; Lin et al., 2019). However, given the emergence of the in-context learning phenomenon in LLMs, we hypothesize that this behavior can be used to enable few-shot generalization to new languages at inference time.\n\n\nIn this work, we explore the task of linguistic reasoning, using linguistics puzzles akin to the International Linguistics Olympiad (IOL). Notably, in these puzzles, the target language is often extremely low-resource or functionally extinct (Bean et al., 2024). While prior work has largely examined the effect of vanilla in-context learning with English-target and target-English exemplars, chain-of-thought prompting, and traditional neural machine translation methods (Chi et al., 2024; \u015eahin et al., 2020), we believe that generating auxiliary exemplars which supplement the target language demonstrations can guide the model to more effectively learn grammar similarities over a language family. As such, we introduce an approach based on analogical prompting (Yasunaga et al., 2024), which uses strong language models to self-generate exemplars of relevant problems given the test instance and performs in-context learning conditioned on those demonstrations. In our setting, the knowledge retrieval-like nature of analogical prompting allows us to test models' parametric understanding of language families, performing inference with both the provided and induced demonstrations.\n\n\nWe evaluate our approach on the modeLing (Chi et al., 2024) dataset, consisting of unseen IOL-style problems. We find that strong models such as GPT-4o and Llama-3.1-405B-Instruct can identify the language family, similar languages within said family, generate exemplars in those similar\n\n\n![Figure 1: An illustration of our 2-stage analogical prompting approach, translating a phrase in Montenegrin to English. While prior works would solely provide exemplars translating between the source language and English and perform in-context learning, our method seeks diverse exemplars. Model  $M_{1}$  first identifies the language family (Slavic) and higher-resource languages in the family which the model has knowledge of (Croatian), then produces exemplars in those languages. Finally, both the original and generated set of exemplars are passed with the test puzzle to model  $M_{2}$  to perform the translation.  $M_{1} = M_{2}$  yields the self-generated analogical reasoning setting.](images/631acd980f3520aa3eca145aea5e5a331a5358622aac1bf7a29675297520d3dc.jpg)\n\n\nlanguages, and apply them to solve the test puzzle. Furthermore, while weak models do not benefit significantly from using strong model-generated exemplars, strong models improve from using exemplars produced by weaker yet specialized multilingual models (e.g. Aya-35B). Our findings show that the ability of the model to deduce and apply rules, following inductive learning from the exemplars, largely influences performance; where there is still much to be desired relative to the reasoning of human experts on this task. From our results, we suggest that the linguistic reasoning task presents a fertile ground for research on new language model reasoning methods, to uncover how the skills which drive logical thinking may be imbued to models.\n\n\n# 2 ANALOGICAL PROMPTING FOR LINGUISTIC REASONING\n\n\nAnalogical prompting (Yasunaga et al., 2024) avoids the need for annotated exemplars by relying on a strong model to generate exemplars which are related to the test instance, but are sufficiently diverse relative to one another and the test sample. Our approach of applying analogical prompts follows the human system 2 thinking framework of slow, deliberate reasoning (Kahneman, 2011). In chain-of-thought prompting for these puzzles, the model performs in-context learning with the given exemplars, learning the rules governing the language by induction, including the meaning of particular words, and using deduction to apply these rules to the test sample. This approach is supported by prior works demonstrating the ability of LMs to learn rules and attempt to apply them (Qiu et al., 2024; Zhu et al., 2024). Furthermore, we do not have access to complete grounding sources of human-written rules governing these low-resource languages, so we must rely on the LM to identify and generate these rules itself. However, as we expect the model to have little to no prior knowledge about the target language<sup>1</sup>, we seek to leverage other languages with similar grammar structure which the model has learned in order to guide the language model's reasoning process.\n\n\nWe use language families as a taxonomically-grounded means of identifying similar languages as the target. The generated exemplars provide a source of reasoning support to the model, enabling it to perform inductive reasoning first in a cross-lingual manner over the diverse exemplars, and then deduce from its shared understanding. For instance, in Figure 1, given the test instance and the provided exemplars in Montenegrin, we leverage the model to (i.) identify the family of Montenegrin (Slavic Language Family), (ii.) select a few languages in the Slavic Language Family, and (iii.) generate example puzzles with their solutions in those selected languages, e.g. Croatian. Then, the provided exemplars and the generated cross-lingual demonstrations are provided to the model, to solve the given test puzzle.\n\n\nIn line with the view of analogical prompting as a knowledge retrieval procedure in accordance with the LM's pre-training distribution, we desire for the model to produce exemplars from languages it has learned, while probing its' understanding of language families. In (Qiu et al., 2024), models have been demonstrated to improve with more familiar exemplars (based on inclusion in the pretraining data). Furthermore, their work suggested that noisy demonstrations hurt performance; as the provided examples from the unseen target language could be considered as \"noisy\", we hypothesize that the generated exemplars can help to compensate.\n\n\nExemplar Correctness. While one would ideally prefer to have a validator which, given a set of rules for a language, can determine if they are being appropriately applied for each of the analogical exemplars, this is very challenging at scale. In the context of Linguistics Olympiad problems, only a small fraction of the population who are experts in such tasks (equivalent to achieving a high score on these contests) would be able to reliably annotate solution rationales for these extremely low-resource translation puzzles. Furthermore, the notion of correctness is ambiguous \u2013 we rely on exact match relative to an annotated \"correct\" response, but it is unclear if there could be more than one \"correct\" response which is context-specific, or if partial credit assignment could be possible. Given the models' lack of zero-shot knowledge of these languages (else, there would likely be leakage), we also cannot reliably use another language model as a validator. As a result, we leverage all generated exemplars by the model for inference, and assume each problem has one correct solution.\n\n\n# 2.1 LINGUISTICS PUZZLES\n\n\nAs noted before, the focus of this work is on linguistics puzzles \u2013 in particular, translation problems from English to a low-resource language and vice versa. Such problems are also referred to in the literature as Rosetta Stone puzzles, and constitute one of the most frequent types of problems that appear in Linguistics Olympiad competitions (\u015eahin et al., 2020; Chi et al., 2024; Bean et al., 2024). These problems typically consist of a test phrase in language A along with 5-10 exemplars<sup>2</sup> of translation from language A to language B and vice versa, and the task is to translate the given phrase into language B. We include an example of such a problem below, from Chi et al. (2024).\n\n\n# EXAMPLE TRANSLATIONS FROM ENGLISH TO RAPA NUI\n\n\nEnglish: We see you.  $\\rightarrow$  Rapa Nui: tike'a t\u0101tou koe  \nEnglish: I hear you.  $\\rightarrow$  Rapa Nui: aro'a au koe  \nEnglish: I see you.  $\\rightarrow$  Rapa Nui: tike'a au koe\n\n\nEnglish: We hear you.  $\\rightarrow$  Rapa Nui: aro'a tatou koe English: We bite the bone.  $\\rightarrow$  Rapa Nui: au tatou ivi\n\n\nEnglish: We hit the bird.  $\\rightarrow$  Rapa Nui: pu'a t\u0101tou manu\n\n\n# TRANSLATE TEST PHRASE\n\n\nEnglish: The bird bites you.  $\\rightarrow$  Rapa Nui: au manu koe\n\n\n# 3 METHODS\n\n\nWe explore a number of sampling methods across various language models to assess their performance on reasoning over unknown languages.\n\n\n# 3.1 BASELINES\n\n\nWe include the following methods as baselines for robust comparison to our method, reflecting prior work examined in linguistic reasoning (Chi et al., 2024). We explore their results in Section 4.1.\n\n\nZero-Shot Prompting. Given the low-resource nature of the languages that we examine, we expect zero-shot performance to be poor, or even zero, on the exact match metric. However, we include this setting for two reasons: (1.) a model which gets multiple questions correct for a given language with zero-shot prompting may be an indication of leakage, and (2.) this serves as a robust check on any additional metrics examined aside from exact match.\n\n\nFew-Shot Prompting / In-Context Learning. As in the Linguistic Olympiad competitions, demonstrations of translation to and from the low-resource language are provided to the model, with the intention for inductive reasoning to guide the model towards identifying the set of grammar rules the language follows.\n\n\nFew-Shot Chain-of-Thought Reasoning. Given the efficacy of chain-of-thought prompting (Wei et al., 2024; Kojima et al., 2022), we extend the few-shot evaluation setting by including prompts for the model to \"think step-by-step\" (Kojima et al., 2022; Yang et al., 2024a). We also include a chain-of-thought rationale exemplar for English-Spanish translation from (Chi et al., 2024), to demonstrate how step-by-step reasoning rationales should be produced, and are denoted in Section 4 as \"w/ rationale\".\n\n\n# 3.2 ANALOGICAL PROMPTING VARIATIONS\n\n\nWe describe the various analogical prompting methods explored in the experiments; their results are in Section 4.2.\n\n\nAnalogical Prompting on Language Families. As noted in Section 2, we seek to use language families as a means to identify similar, auxiliary languages whose exemplars can boost the model's cross-lingual understanding. In a similar environment to the Linguistics Olympiad competition, where one does not have access to any external resources, we test the model on its latent understanding of language families and regional associations to generate further exemplars and puzzles in another language within the same language subgroup as the target language. For a target language  $L$ , we prompt the model to identify a few other languages (denote this list  $L_{Aux}$ ) in the same family as  $L$ ; then, for each language in  $L_{Aux}$ , generate a puzzle translating from  $l$  to English, and a puzzle in the reverse direction. Then, we apply these exemplars along with the given ones for  $L$  in a new instruction to the model. We term this 2-stage analogical reasoning.\n\n\nSeparating the two stages of analogical prompting (generation and application) yields an opportunity to explore how different combinations of models for this approach might perform. While the above entails using the same model for both steps, we look to contrast the strength of the models used, to attempt to boost the performance of both frontier and small models.\n\n\nInference-time Exemplar Distillation. In our work, inference-time distillation refers to generating analogical exemplars with a strong model (e.g. GPT-4o) and applying them to a weak model (e.g. models with roughly 7-8B parameters). Our hypothesis driving this setting is: can higher quality exemplars produced by strong models enable better deductive abilities with weak models?\n\n\nWeak-to-Strong Cross-Linguual Analogies. Specialized multilingual models such as the Aya-23 models hold promise for our linguistic reasoning analysis, as they have been fine-tuned for instruction-following across a wide range of languages. We propose using such models for generating analogical demonstrations, as they may have a stronger understanding of language families and can produce diverse exemplars, which we believe strong models may be able to deduce from.\n\n\n# 3.3 EXPERIMENTAL SETUP\n\n\nDatasets. We primarily evaluate our approaches on the modeLing dataset (Chi et al., 2024). This dataset consists of problems written by the authors and hence uninvolved in prior Linguistics\n\n\nOlympiads. This benchmark was released in 2024, and we rely on its recency to be more assured that leakage is not a factor driving performance. We note that all problems examined are purely text-based; while there exist linguistics puzzles that require deduction from images, filling in diagrams, etc., the benchmark we evaluate on does not include such problems. This suggests that future work could study the performance of multimodal models on these problem types. We also evaluate on the LINGOLY dataset (Bean et al., 2024), which features 1,133 problems and expands beyond \"Rosetta Stone\" translation problems to include grammatical pattern-based translation, matching translation pairs, text purely in an unknown language, identifying errors in machine translation, and longer text in multiple languages. The results are included in Appendix D.\n\n\nModels. We evaluate with the following models:\n\n- - OpenAI models: GPT-4o, GPT-4, and GPT-3.5-turbo\n\n- - Open-weight models: Llama 3.1 8B-Instruct, Llama 3.1 70B-Instruct, Llama 3.1 405B-Instruct (Dubey et al., 2024), Mixtral 8x7B-Instruct-v0.1 (Jiang et al., 2024), and Mixtral 8x22B-Instruct-v0.1\n\n- - Multilingual Instruction-tuned Models: Aya-23 8B and Aya-23 35B (henceforth referred to as Aya-8B and Aya-35B) (Aryabumi et al., 2024)\n\n\nOpenAI models are inferred with the OpenAI API, while the open-weight and multilingual instruction-tuned models are queried with the Together AI API and Apple MLX, respectively.\n\n\n# 4 RESULTS\n\n\nWe report exact match (EM) scores for all experiments performed. ChrF2 Popovic (2015), a character n-gram F-score measure, and corpus-level BLEU scores (Papineni et al., 2002) are recorded in Appendix A. We do not treat these as primary metrics as BLEU ignores word ordering nuances amidst short responses in machine translation, which is integral to measuring correctness in the puzzles we explore (Callison-Burch et al., 2006; Chi et al., 2024), and we find the ChrF scores to be noisy relative to EM scores. Smaller models with weaker instruction-following capabilities often failed to produce their output in the exact desired format specified in the prompts. To ensure that reliable exact match scores are reported while some responses may have parsing issues relative to the expected format, the authors of this work manually examined each response to confirm whether the output generated contains the target response. To enforce standardization across our evaluation procedure, this was performed for all experiments; this was not applicable for stronger models whose responses exactly followed the desired output format.\n\n\n# 4.1 CHAIN-OF-THOUGHT LINGUISTIC REASONING\n\n\nThe results of baseline methods are in Table 1. The prompts for all experiments are included in Appendix F, and all experiments are averaged over 3 runs. For the \"CoT with rationale experiment\", we take the best of using 512 and 4096 max tokens (see Appendix B). For the \"few shot\" results, we take the best out of two different prompt settings, ablated on in Appendix C.\n\n\nOur strongest baseline result is achieved with Llama-3.1-405B-Instruct producing CoT rationales, at  $65.81\\%$ ; in fact, this model produces the best results across all 4 baseline settings. GPT-4o remains in the high 50s, but does not exceed  $60\\%$  on any single run. Among smaller models, Llama-3.1-8B-Instruct performs comparably to Aya-35B and Mixtral-8x7B-Instruct, outperforming it on some baselines, which may be attributable to a stronger and more recent base model. We also observe that GPT-4o and Llama-3.1-405B-Instruct do indeed solve a few puzzles (2 and 4 samples, respectively) in the zero-shot setting. Given the former was released before the modeLing dataset, and the latter was released just shortly after, we do not believe this to be a sign of leakage; furthermore, each correct question was from a different language.\n\n\nWe report a few key observations below:\n\n\nStrong models produce rationales without being instructed to. We find that strong models such as GPT-4o and GPT-4 produce chain-of-thought stepwise rationales for responses, even in the zero-shot and few-shot settings, without including a chain-of-thought prompt or including rationales in\n\n\nTable 1: Baseline experimental results using chain-of-thought methods, reporting exact match. The models have been split into three groups, corresponding to the models noted in Section 3.3. All results reported are average of 3 runs at a temperature of 0.3, to address sampling variance.\n\n\n<table><tr><td>Model</td><td>Zero-Shot</td><td>Few-Shot w/o CoT</td><td>Few-Shot w/ CoT</td><td>Few-Shot CoT w/ Rationales</td></tr><tr><td>GPT 3.5-Turbo</td><td>0%</td><td>25.74%</td><td>26.10%</td><td>38.6%</td></tr><tr><td>GPT-4</td><td>0%</td><td>56.25%</td><td>45.22%</td><td>45.59%</td></tr><tr><td>GPT-4o</td><td>1.10%</td><td>59.19%</td><td>58.82%</td><td>55.88%</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>0%</td><td>22.79%</td><td>16.91%</td><td>23.16%</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>0%</td><td>45.22%</td><td>44.49%</td><td>42.28%</td></tr><tr><td>Llama-3.1-405B-Instruct</td><td>1.47%</td><td>61.76%</td><td>59.19%</td><td>65.81%</td></tr><tr><td>Mixtral-8x7B-Instruct</td><td>0%</td><td>22.43%</td><td>22.06%</td><td>16.18%</td></tr><tr><td>Mixtral 8x22B-Instruct</td><td>0%</td><td>45.59%</td><td>43.38%</td><td>39.71%</td></tr><tr><td>Aya-23-8B</td><td>0%</td><td>9.93%</td><td>7.35%</td><td>5.88%</td></tr><tr><td>Aya-23-35B</td><td>0%</td><td>23.53%</td><td>20.59%</td><td>14.34%</td></tr></table>\n\n\nthe exemplars. This is a key reason why the few-shot without chain-of-thought setting performs the highest for both models. Furthermore, when prompted with rationale-inducing exemplars (see Appendix F), these strong models produce rule libraries from the exemplars, akin to (Zhu et al., 2024), leading to very lengthy responses; some models such as Llama-3.1 70B fall into loops of repeating the same rule many times. This necessitates the use of a higher number of max tokens to be generated, to ensure that the final answer is indeed outputted, although this makes human verification of response correctness harder due to their length; we report ablations on this in Appendix B.\n\n\nCertain models perform uncertainty-based refusal. Some models, such as Mixtral-8x7B-Instruct and Mixtral-8x22B-Instruct respond to test instances by stating an inability to perform the desired task. This behavior especially appears in CoT with rationale exemplars; interestingly, this occurs after the implicit induction stage has been performed. For instance, with Mixtral-8x22B-Instruct, the model enumerates a set of word-level translations between the target language and English, respectively, then upon recognizing ambiguity in one of the word-level translations, it claims that solving the problem is impossible without additional information. By contrast, models such as GPT-4o instead output multiple candidate answers when it is not entirely certain. We include qualitative examples of this behavior in Appendix E. This appears to reinforce the findings of the Qiu et al. (2024) in that models are unable to reliably apply their inductively learned rules.\n\n\nOur analogical reasoning method introduces an inference-time approach to boost deductive reasoning, by deliberately using their learned multilingual knowledge to guide puzzle solving.\n\n\n# 4.2 TWO-STAGE ANALOGICAL REASONING\n\n\nTo critically explore the evaluation settings introduced in Section 3.2, we select 2 frontier models \u2013 GPT-4o and Llama-3.1-405B-Instruct \u2013 which were the strongest performers in our baselines. We select 2 weaker models \u2013 Aya-35B and Llama-3.1-8B-Instruct \u2013 for the inference-time distillation and weak-to-strong prompting experiments. These models performed comparably to one another in the baselines, and allow us to contrast multilingual specialization against a generalist model with multilingual support. The experiments with Llama-3.1-8B-Instruct are included in Appendix I.\n\n\nWe also establish an upper bound on the performance we can attain with our approach, by a psuedo-open-book method with oracle language families. That is, for each language in the evaluation set, rather than prompting the model to implicitly infer the language family and other languages which are a member of it, we abstract away the former by providing the language family in the prompt. We suggest that a human expert with strong cross-lingual reasoning abilities would be able to deduce such relationships with similar languages, so providing language family labels eliminates one uncertainty source in the model's generations. The results of this are included in Figure 2b.\n\n\nAnalogical reasoning boosts frontier models. We find that 2-stage analogical reasoning pushes the boundaries of the performance of frontier models, relative to their best baseline results. Solely considering the self-generation setting (where the same model both generates analogical exemplars\n\n\nTable 2: Pairwise results with our 2-stage analogical prompting method. The left column denotes the model generating the analogical exemplars, and the top row denotes the model applying both the generated and provided exemplars to answer the test puzzle. Note that these results address the self-generated analogical reasoning, inference-time distillation, and weak-to-strong prompting settings posed in Section 3.2.\n\n\n<table><tr><td>Deducer\nGenerator</td><td>GPT-4o</td><td>Llama-3.1-405B-Instruct</td><td>Aya-23-35B</td></tr><tr><td>GPT-4o</td><td>66.91%</td><td>71.69%</td><td>21.32%</td></tr><tr><td>Llama-3.1-405B-Instruct</td><td>67.28%</td><td>67.65%</td><td>20.22%</td></tr><tr><td>Aya-23-35B</td><td>65.44%</td><td>71.32%</td><td>15.44%</td></tr></table>\n\n\nand applies them), GPT-4o improves  $7.2\\%$  ( $59.19\\% \\rightarrow 66.91\\%$ ), and Llama-3.1-405B-Instruct improves  $1.8\\%$  ( $65.81\\% \\rightarrow 67.65\\%$ ). We subsequently observe even stronger gains for both models as the deducer, when selecting different models as the analogical exemplar generator. In the first stage, both of these frontier models correctly identify the language family at a fairly high rate (see Appendix H), select a few languages from said family, and generate analogical puzzles for those auxiliary languages, as intended. Then, in the second stage, the model considers the tokens in the test phrase, and analyzes how each is to be translated to the target language, and combines them together in the appropriate order following the syntactical patterns observed from the given exemplars. Thus, it appears that the model uses the analogical exemplars to better induce the mappings of words in the target language to words in the source language, which it then applies to the target phrase.\n\n\nWeak analogical \"supervision\" improves performance. We find that generating the analogical exemplars with Aya-35B and applying them to the test sample with Llama-3.1-405B-Instruct yields  $71.32\\%$ , averaged over 3 runs; a  $5.5\\%$  improvement over the best baseline for Llama-3.1-405B-Instruct  $(65.81\\% \\rightarrow 71.32\\%)$ . We similarly find that leveraging Aya-35B-generated exemplars and applying them with GPT-4o yields a  $6.2\\%$  improvement over the best GPT-4o baseline setting  $(59.19\\% \\rightarrow 65.44\\%)$ . In the case of Llama-3.1-405B-Instruct, using Aya-generated exemplars outperforms using self-generated exemplars, by  $3.7\\%$ $(67.65\\% \\rightarrow 71.32\\%)$ . Our findings suggest that when equipped with the right tools (analogical demonstrations) from effective multilingual reasoners, strong deducers can thrive.\n\n\nThis claim is further reinforced by the inference-time distillation results: smaller models such as Aya-35B do not benefit from the analogical exemplars, regardless of the analogical generator. At the same time, using the GPT-4o exemplars applied by Llama-3.1-405B-Instruct yields  $71.69\\%$ , our strongest result across all evaluation settings. Moreover, the reverse direction (Llama-3.1-405B-Instruct exemplars applied by GPT-4o) yields an  $8.09\\%$  improvement over the best GPT-4o baseline result. From these findings, we conclude that analogical exemplars generated by good multilingual reasoners do not \"unlock\" deductive reasoning abilities for models without them (Aya-35B); however, for strong baseline reasoners (Llama-3.1-405B, GPT-4o), better exemplars help performance. $^{3}$\n\n\nFrontier models understand language families. We compare model performance with and without oracle language families, finding that prompting models to infer the language family is superior. We observe that frontier models such as GPT-4o and Llama-3.1-405B, as well as specialized multilingual models like Aya-35B, have a strong parametric knowledge of language families, and do not need to rely on language family labels to identify similar languages. Furthermore, the model performing retrieval of the language family helps it to identify a few languages within the family that will help it, bootstrapping from the provided exemplars, whereas providing the language family often leads to the model listing many languages in the family and attempting to produce exemplars for all of them. We hypothesize that this is a source of noise; demonstrations beyond a certain number yield diminishing returns in performance. That is, the oracle language families setting stimulates inductive cross-lingual reasoning, but makes deductive reasoning more challenging due to having\n\n\n![(a) Best baseline vs. best analogical](images/48245bc3b4d3c14218c341f144d4ec1edb2d635b531c78a09df87e2dbe38caf8.jpg)\n\n\n![(b) Inferred families vs. oracle families Figure 2: Figure (a) contains a comparison of the best baseline (in Table 1) with the best 2-stage analogical reasoning result (in Table 2), for our two frontier models as the deducer. We find analogical to improve GPT-4o by  $8.1\\%$  and Llama-3.1-405B-Instruct by  $5.9\\%$ . Figure (b) compares self-generated analogical reasoning methods, with prompt-determined language families (\"inferred families\") and human-annotated language family labels (\"oracle families\").](images/40673e39e356c374031350dd339d76203c7ff32b0726b4f620fc6b512f46cf29.jpg)\n\n\nmany exemplars. Specific examples of this behavior are included in Appendix E, and the language families table is in Appendix G. We also include further discussion on the language families identified by GPT-4o and Llama-3.1-405B-Instruct in the inferred families setting, in Appendix H, and find that they achieve a high correctness rate relative to the oracle labels.\n\n\nLanguage Isolates and Proxy Languages. Language isolates would appear to pose particular difficulty to our models, as by definition, they do not belong to any well-defined language family. As a result, we rely solely on the models' ability to trace grammatical correspondences based on the languages it has seen in pre-training, even for our experiments with oracle language family labels. While in the baseline experiments, our models often believed that the target language is imaginary, prompting for language families leads models to note that the language is isolate. They then attempt to either follow syntactic or morphological patterns to induce a new fictitious language which is similar to the target, or select learned geographically-proximate languages. For the language of Bangime, spoken in Mali, the model either retrieves languages from families in the same geographical region, such as Dogon, or creates a new language (e.g. \"Xangime\") for which it generates analogical exemplars (see Appendix E). Analyzing at the instance level, this improves the correctness on the Bangime puzzles from  $27.8\\%$  to  $50\\%$  for GPT-4o in the self-generated setting.\n\n\nIn summary, our results suggest that the ability of the model to deduce by leveraging the given and analogically-generated exemplars is the key performance driver. This is lent credence by the efficacy of weak-to-strong prompting (i.e. relying on the exemplars of Aya-35b), while the performance of inference-time distillation remains roughly similar. Thus, we posit that the \"strength\" of a linguistic reasoning agent can be decomposed along two lenses, corresponding to our two stages: (1.) generating analogical exemplars by language identification and multilingual reasoning, and (2.) deducing from hypotheses in complex evaluation settings.\n\n\n# 5 RELATED WORK\n\n\n# 5.1 LARGE LANGUAGE MODEL REASONING.\n\n\nFew-shot Chain-of-Thought Reasoning. In-context learning has emerged as an exciting phenomenon in language models, enabling them to learn from few-shot demonstrations at inference-time to generalize to various tasks (Brown et al., 2020; Wei et al., 2022). At the same time, the chain-of-thought (CoT) reasoning method guides language models to think systematically through a problem, in a step-by-step manner (Wei et al., 2024; Kojima et al., 2022). In particular, applying chain-of-thought prompting (zero-shot or few-shot) with the goal to induce rationales yields explanations of why each step was performed, along with performance and faithfulness improvements (Nye et al., 2021; Lampinen et al., 2022). Various similar approaches (Yao et al., 2023; Wang et al., 2023; Besta et al., 2024) have been proposed to sample more diverse generations from models, leveraging test-time compute to improve reasoning performance; we believe such methods make for interesting lines of future research for the linguistic reasoning task.\n\n\nInductive Reasoning in LLMs. Inductive and deductive reasoning skills in language models have often been studied in the context of logical or abstract reasoning problems. Much of this prior work on inductive reasoning with language models studies evaluation settings with more clearly defined rules to be inductively learned and then applied; these works suggest gaps relative to the human intelligence in performing both inductive and deductive reasoning (Xu et al., 2024; Gendron et al., 2024; Yang et al., 2024c). In particular, Yang et al. (2024c) notes the need for more challenging tasks in inductive reasoning to better assess the boundaries of LM capabilities, such as hypothesis generation and pattern induction. Works such as Tang et al. (2023) demonstrate that models struggle to create rules by induction when the semantics of the exemplars do not follow in a commonsense manner \u2013 in our work, generating analogical exemplars similar to the models pre-training data may steer the model towards a relative \"commonsense\" representation of the rules underlying the exemplars.\n\n\nSeveral works dive into the realm of hypothesis search, determining the ability of LMs to pose hypotheses about the problem (e.g. rules which exemplars follow) before seeking to deductively apply them (Zhu et al., 2024; Qiu et al., 2024; Wang et al., 2024). Zhu et al. (2024) propose hypotheses-to-theories (HtT), which learns a rule library from an induction stage, and then applies it by a deduction stage; this multi-stage method is similar to our analogical approach, although we still perform both induction and deduction together after analogical generation. Furthermore, their rule library depends on verification \u2013 this is not possible in the linguistic reasoning task due to the lack of a reliable feedback source to judge responses, aside from expert humans. As discussed earlier in our work, Qiu et al. (2024) demonstrates that models can propose rules well, but cannot consistently apply them. Wang et al. (2024) proposes Hypothesis Search, a method which proposes hypotheses, implements a subset of them as Python programs, and applies them to training samples to verify their correctness.\n\n\nExemplar Generation and Automated Reasoning. Analogical prompting (Yasunaga et al., 2024) has been demonstrated to be an effective inference-time method to produce diverse, task-conditioned exemplars, improving in-context learning. As noted above, this effectively serves as a knowledge retrieval method which retrieves exemplars similar to (or directly from) the pre-training distribution which the model has seen; RECITE (Sun et al., 2023) similarly retrieves passages directly from the model's memory. Methods such as SG-ICL (Kim et al., 2022) and Auto-ICL (Yang et al., 2024b) also self-generate in-context exemplars in a similar manner as analogical prompting.\n\n\n# 5.2 MULTILINGUAL REASONING.\n\n\nMultilingual reasoning in LMs for low-resource languages poses a unique challenge, as the pretraining corpora and supervised fine-tuning datasets for many models are largely concentrated on a few high-resource languages. XLT (Huang et al., 2023) introduces a prompt template which translates problems in other languages to English and solves the problem with chain-of-thought in English. Qin et al. (2023) aligns each step in the chain-of-thought between the source language and English explanations, then solves the problem given this alignment; they also apply self-consistency with cross-lingual alignments with a set of pre-specified target languages. Li et al. (2024) trains on code data with multilingual comments, while using multilingual code prompts at inference time with symbolic function API calls as a structured way to solve the reasoning problem.\n\n\nLinguistic Reasoning Benchmarks. The PuzzLing Machines dataset (Sahin et al., 2020) first introduced a set of Linguistics Olympiad problems to study the ability of language models to learn from a small amount of data; they apply RoBERTa-based neural machine translation methods, but demonstrate a vast gap (attaining less than  $4\\%$  exact match performance). With concerns of potential leakage given the vast web scraping performed in procuring pre-training tokens for language model training, modeLing (Chi et al., 2024) introduced a new set of hand-written Linguistics Olympiad problems, demonstrating the performance of current models with CoT methods. The LINGOLY (Bean et al., 2024) dataset presents problems from the UK Linguistics Olympiad competition, and studies zero-shot and few-shot performance of current models categorized by question type.\n\n\n# 6 DISCUSSION\n\n\nWe propose applying analogical prompting as a test of inductive reasoning from diverse exemplars for challenging linguistic puzzles. Our results encouragingly suggest that despite frontier models struggling with deductive reasoning, in line with the findings of (Qiu et al., 2024), they can indeed\n\n\nfollow grammar rule similarities to generate analogical demonstrations and attempt to apply them adeptly. This yields improved performance in self-generated analogical prompting with GPT-4o and Llama-3.1-405B, as well as weak-to-strong prompting for those models employing Aya-35B-generated demonstrations. We also show that Llama-3.1-405B-Instruct is the best model for linguistic reasoning at present, becoming the first model to achieve over  $70\\%$  on the modeLing benchmark by way of our 2-stage analogical reasoning approach. This could be attributed to the vast scale of multilingual pre-training data (15T tokens), as well as multilingual supervised fine-tuning and dialogue data seen in adaptation. The multilingual prowess of the Llama-3.1 models is evidenced by its strong performance on the MGSM and multilingual MMLU benchmarks (Dubey et al., 2024).\n\n\nFurthermore, the ability of smaller and specialized multilingual models (Aya) to generate coherent analogical exemplars, which improve frontier models over their own self-generated exemplars, is promising towards developing widely-available multilingual reasoners. We find that the improvements observed can be attributed to the auxiliary exemplars generated, which are in turn due to the model's understanding of language families and grammar rules from the pre-training data or its multilingual adaptation. The errors made by current models due to an inability to apply diverse and complex exemplars suggest that the linguistic reasoning task is an exciting and challenging evaluation setting for LM reasoning at large. That is, seeking to emulate human reasoning, where deduction involves a clear application of recognized patterns, provides a ripe space for future work.\n\n\nThe interesting phenomenon examined with language isolates also provides a glimpse of model capabilities to follow grammatical similarities, rather than relying on knowledge retrieval of language families. That is, the multilingual language understanding abilities of frontier models expand beyond typological knowledge, going so far as to create proxy fictitious languages which enable it to solve the problem correctly. We suggest that future efforts in multilingual adaptation be placed in identifying techniques to guide languages models to support typologically unique languages.\n\n\nResearch at the intersection of machine translation and reasoning in the development of the latest foundation models is important from a societal perspective. With large language models being adopted widely, the need for multilingual capabilities and rapid adaptation grows, and our work proposes an effective method by which this can be performed at test-time. Notably, we have demonstrated evidence that models follow language similarities \u2013 given the massive number of languages and dialects present worldwide, this could help guide humans to learn dying languages, thus keeping their tradition alive, while doing so in a scalable manner. We hope that these findings can inspire future models releases to include evaluation on challenging multilingual tasks such as these puzzles, and research on reasoning can explore the multilingual setting further in depth.\n\n\n**Limitations** We note that the reliance on exact match scoring as our primary signal of performance is not ideal, as it is a binary indicator. We have sought to examine other metrics which correspond to \"partial credit\" such as ChrF2 and BLEU; however, there are flaws in these methods as well. A stronger human understanding of the rules which these extremely low-resource languages follow could guide us to better metrics, especially capturing semantic meaning and word ordering inversions, where appropriate. For instance, some languages might retain the same meaning while inverting the word order \u2013 exact match is sensitive to this, and while ChrF2 and BLEU are not, we should only be insensitive to ordering for languages which follow this property. We also recognized that the IOL 2024 problems could not be used as a benchmark with our method, as they require multimodality \u2013 our method only analyzes unimodal text problems. Another limitation of our work is that we do not have a reliable means of verifying the correctness of analogical exemplars, nor contrasting the quality of exemplars generated across models to determine the best analogical generator model. An expert annotator who could identify where a mistake was made in the model's reasoning process also would have been helpful to yield further insights into the fallacies of current models' linguistic reasoning. Nonetheless, our most effective deducer models are able to leverage exemplars generated by models of various sizes for improved linguistic reasoning.\n\n\nReproducibility Statement. We include all prompts used for generating our baseline experimental results, and all analogical prompting methods, in Appendix F. We have also broken down the two stages of our analogical reasoning method for clarity on how the method should be applied with two separator models (e.g. weak-to-strong prompting, inference-time exemplar distillation). We evaluate our work on the modeLing dataset, which is publicly available. We have included details of the platforms through which the models we evaluate have been queried (OpenAI API, TogetherAI API, Apple MLX), along with the list of models studied. Lastly, we will release our code publicly.\n\n\n# REFERENCES\n\n- Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet \u00dcst\u00fcn, and Sara Hooker. Aya 23: Open weight releases to further multilingual progress, 2024. URL https://arxiv.org/abs/2405.15032.\n\n- Andrew M. Bean, Simi Hellsten, Harry Mayne, Jabez Magomere, Ethan A. Chi, Ryan Chi, Scott A. Hale, and Hannah Rose Kirk. Lingoly: A benchmark of olympiad-level linguistic reasoning puzzles in low-resource and extinct languages, 2024. URL https://arxiv.org/abs/2406.06196.\n\n- Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefer. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17682-17690, March 2024. ISSN 2159-5399. doi: 10.1609/aaai.v38i16.29720. URL http://dx.doi.org/10.1609/aaaai.v38i16.29720.\n\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.\n\n- Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Diana McCarthy and Shuly Wintner (eds.), 11th Conference of the European Chapter of the Association for Computational Linguistics, pp. 249-256, Trento, Italy, April 2006. Association for Computational Linguistics. URL https://aclanthology.org/E06-1032.\n\n- Nathan Chi, Teodor Malchev, Riley Kong, Ryan Chi, Lucas Huang, Ethan Chi, R. McCoy, and Dragomir Radev. ModeLing: A novel dataset for testing linguistic reasoning in language models. In Michael Hahn, Alexey Sorokin, Ritesh Kumar, Andreas Shcherbakov, Yulia Otmakhova, Jinrui Yang, Oleg Serikov, Priya Rani, Edoardo M. Ponti, Saliha Murado\u011flu, Rena Gao, Ryan Coterell, and Ekaterina Vylomova (eds.), Proceedings of the 6th Workshop on Research in Computational Linguistic Typology and Multilingual NLP, pp. 113-119, St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.sigtyp-1.14.\n\n- Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\n- Gael Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. Large language models are not strong abstract reasoners, 2024. URL https://arxiv.org/abs/2305.19555.\n\n- Haoyang Huang, Tianyi Tang, Dongdong Zhang, Xin Zhao, Ting Song, Yan Xia, and Furu Wei. Not all languages are created equal in LLMs: Improving multilingual capability by cross-lingual-thought prompting. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 12365\u201312394, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023-findings-emnlp.826. URL https://aclanthology.org/2023-findings-emnlp.826.\n\n- Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of experts, 2024. URL https://arxiv.org/abs/2401.04088.\n\n- Daniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, New York, 2011. ISBN 9780374275631 0374275637. URL https://www.amazon.de/ Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=wl_it_dp_o_pdT1_nS_nC?ie  $\\equiv$  UTF8&colid  $=$  151193SNGKJT9&coliid  $=$  I3OCESLZCVDFL7.\n\n- Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang goo Lee. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator, 2022. URL https://arxiv.org/abs/2206.08082.\n\n- Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 22199-22213. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf.\n\n- Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from explanations in context? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 537-563, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022-findings-emnlp.38. URL https://aclanthology.org/2022-findings-emnlp.38.\n\n- Bryan Li, Tamer Alkhouli, Daniele Bonadiman, Nikolaos Pappas, and Saab Mansour. Eliciting better multilingual structured reasoning from LLMs through code. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5154\u20135169, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.281. URL https://aclanthology.org/2024.acl-long.281.\n\n- Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios Anastasopoulos, Patrick Littell, and Graham Neubig. Choosing transfer languages for cross-lingual learning. In Anna Korhonen, David Traum, and Lluis M\u00e1rquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3125\u20133135, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1301. URL https://aclanthology.org/P19-1301.\n\n- Toan Q. Nguyen and David Chiang. Transfer learning across low-resource, related languages for neural machine translation. In Greg Kondrak and Taro Watanabe (eds.), Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 296-301, Taipei, Taiwan, November 2017. Asian Federation of Natural Language Processing. URL https://aclanthology.org/I17-2050.\n\n- Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114.\n\n- Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin (eds.), Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.\n\n- Maja Popovic. chrF: character n-gram F-score for automatic MT evaluation. In Ondrej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina (eds.), Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 392\u2013395, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https://aclanthology.org/W15-3049.\n\n- Matt Post. A call for clarity in reporting BLEU scores. In Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9veol, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186-191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https://aclanthology.org/W18-6319.\n\n- Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 2695-2709, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.163. URL https://aclanthology.org/2023.emnlp-main.163.\n\n- Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=bNt7oajl2a.\n\n- G\u00f6zde G\u00fcl \u015eahin, Yova Kementchedjhieva, Phillip Rust, and Iryna Gurevych. PuzzLing Machines: A Challenge on Learning From Small Data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 1241-1254, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.115. URL https://aclanthology.org/2020.acl-main.115.\n\n- Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=-cqvvvb-NkI.\n\n- Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. Large language models are in-context semantic reasoners rather than symbolic reasoners, 2023. URL https://arxiv.org/abs/2305.14825.\n\n- Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman. Hypothesis search: Inductive reasoning with language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=G7UtIGQmjm.\n\n- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. URL https://arxiv.org/abs/2203.11171.\n\n- Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. URL https://arxiv.org/abs/2206.07682.\n\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088.\n\n- Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. Are large language models really good logical reasoners? a comprehensive evaluation and beyond, 2024. URL https://arxiv.org/abs/2306.09841.\n\n- Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers, 2024a. URL https://arxiv.org/abs/2309.03409.\n\n- Jinghan Yang, Shuming Ma, and Furu Wei. Auto-icl: In-context learning without human supervision, 2024b. URL https://arxiv.org/abs/2311.09263.\n\n- Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. Language models as inductive reasoners. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 209-225, St. Julian's, Malta, March 2024c. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-long.13.\n\n- Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 11809-11822. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf.\n\n- Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. Large language models as analogical reasoners. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=AgDICX1h50.\n\n- Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. Large language models can learn rules, 2024. URL https://arxiv.org/abs/2310.07064.\n\n- Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning for low-resource neural machine translation, 2016. URL https://arxiv.org/abs/1604.02201.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634686.028133", "msecs": "28.0", "relativeCreated": "30851.607084274292", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability\n\n# Abstract\nPrecipitation nowcasting, a short-term (up to six hours) rain prediction, is arguably one of the most demanding weather forecasting tasks. To achieve accurate predictions, a forecasting model should consider miscellaneous meteorological and geographical data sources. Currently available datasets provide information only about precipitation intensity, vertically integrated liquid (VIL), or maximum reflectivity on the vertical section. Such single-level or aggregated data lacks description of the reflectivity change in vertical dimension, simplifying or distorting the corresponding models. To fill this gap, we introduce an additional dimension of the precipitation measurements in the RuDar dataset that incorporates 3D radar echo observations. Measurements are collected from 30 weather radars located mostly in the European part of Russia, covering multiple climate zones. Radar product updates every 10 minutes with a 2 km spatial resolution. The measurements include precipitation intensity (mm/h) at an altitude of 600 m, reflectivity (dBZ) and radial velocity (m/s) at 10 altitude levels from 1 km to 10 km with 1 km step. We also add the orography information as it affects the intensity and distribution of precipitation. The dataset includes over 50 000 timestamps over a two-year period from 2019 to 2021, totalling in roughly 100 GB of data. We evaluate several baselines, including optical flow and neural network models, for precipitation nowcasting on the proposed data. We also evaluate the uncertainty quantification for the ensemble scenario and show that the corresponding estimates do correlate with the ensemble errors on different sections of data. We believe that RuDar dataset will become a reliable benchmark for precipitation nowcasting models and also will be used in other machine learning tasks, e.g., in data shift studying, anomaly detection, or uncertainty estimation. Both dataset and code for data processing and model preparation are publicly available.\n\n\n# 1 INTRODUCTION\n\n\nPrecipitation nowcasting is the task of forecasting a rainfall situation (precipitation location and strength) for a short period of time, usually up to six hours. Due to climate change the frequency and magnitude of extreme weather events, e.g. sudden downpours, increase, and the techniques for forecasting such events are needed. Precipitation nowcasting can provide information about such events with a high spatiotemporal resolution. Such kind of weather forecasting plays an essential role in resource planning in the agricultural industry, aviation, sailing, etc. as well as in daily life.\n\n\nIncorrect precipitation forecasting could have a negative impact on human life activity, and data with diverse meteorological and geographical characteristics are needed for improving precipitation nowcasting models. The different benchmark dataset usage could improve the quality of precipitation nowcasting models to minimize the risk of forecasting error.\n\n\nPreviously published benchmarks Holleman (2007); Shi et al. (2017); Ansari et al. (2018); Ramsauer et al. (2018); Franch et al. (2020); Veillette et al. (2020) provide data collected with one or several\n\n\n![Figure 1: The geographical area covered by the proposed weather radar dataset (light areas). The covered area has a variety of geographic and climatic characteristics. The color indicates the height above sea level.](images/6b7a11ab3bcee6fb552fe032af459bc0a020f72b692afc4bead480af66c7a554.jpg)\n\n\n![](images/d32d32fc7354fd0c0181ce79e635e681ff299e6043a1604598286919cdb9bef2.jpg)\n\n\nweather radars. Some of those datasets only contain information about precipitation intensity, others provide vertically integrated liquid value (VIL) or maximum reflectivity on the vertical section.\n\n\nHowever, a single measurement type is often not enough for extreme weather events forecasting. Thereby, we propose a RuDar dataset that contains several measurement products: reflectivity (dBZ) and radial velocity  $(\\mathrm{m / s})$  on ten altitude levels from  $1\\mathrm{km}$  to  $10\\mathrm{km}$  with  $1\\mathrm{km}$  step and intensity  $(\\mathrm{mm / h})$  on a  $600\\mathrm{m}$  altitude level. Each measurement was carried out with a  $2\\mathrm{km}$  spatial resolution and a 10 minute temporal resolution. The dataset from 30 dual-pol Doppler weather radars were collected and processed at the Radar Center of the Central Aerological Observatory (CAO) of the Russian Federal Service for Hydrometeorology and Environmental Monitoring (ROSGIDROMET) and is used by our team within the conditions of commercial contract. For each radar, we additionally provide information about the surrounding orography Becker et al. (2009). The radars are located mostly in the European part of Russia as shown in Figure 1, therefore, a wide range of geographical and climatic conditions is considered. The proposed dataset includes more than 50 000 timestamps over a two year period from 2019 to 2021, allowing to investigate the effect of seasonality on rainfall forecast.\n\n\nWe illustrate the applicability of our dataset to the nowcasting task by benchmarking the current state-of-the-art optical flow approach Ayzel et al. (2019) and neural network models Shi et al. (2015); Veillette et al. (2020); Ravuri et al. (2021) on it. Experiments show a seasonal dependence having effect on the algorithm performance due to different precipitation intensity rates and differences between adjacent timestamps in different months.\n\n\nThe main paper contributions are (i) published weather radar dataset with different geographical and climatic conditions (provided under the CC BY NC SA 4.0 license) together with the accompanying exploratory data analysis, (ii) evaluations of common simple precipitation nowcasting models and its extension to support additional data, (iii) uncertainty estimation and its connection to the error for the nowcasting ensemble case, and (iv) accompanying source code for data processing and experiments.\n\n\nThe structure of the paper is as follows: Section 2 covers previously published datasets for the precipitation nowcasting task, Section 3 describes the proposed dataset, Section 4 introduces evaluated nowcasting benchmarks, Section 5 explores the uncertainty estimation scenario for the ensemble of models, and Section 6 concludes the paper.\n\n\n# 2 RELATED WORK\n\n\nDoppler weather radar is the most effective tool for detecting precipitation. The radar measures reflectivity of radio waves from precipitation drops, which can then be converted into precipitation intensity using Z-R relation Marshall & Palmer (1948). Standard ways of obtaining a single reflectivity measure from the different heights is either taking measurements from only the lower level (base reflectivity), or aggregating these measurements by the maximum value (composite reflectivity). In addition, a Doppler radar can detect movement towards or away from itself, which allows measuring the speed of precipitation movement along or against the direction of the radar. The latter type of measurement is called radial velocity.\n\n\nIn the public domain, one can find quite a variety of weather radar datasets collected and maintained by international agencies. We summarized the information about some of them in Table 1, where we\n\n\nprovided a comparison by spatial, temporal and pixel resolution, time periods, geographic coverage and number of radars from which measurements were taken.\n\n\nThe first three data sources in Table 1 are more large databases than ready-made ML benchmarks:\n\n- - NEXRAD Ansari et al. (2018) by US National Oceanic and Atmospheric Service (NOAA), which has been collected since 1994 and contains reflectivity data, radial velocity, and derivative products.\n\n- - MRMS Zhang et al. (2016) by NOAA National Severe Storms Laboratory combines the previous source together with various sources to provide severe weather, transportation, and precipitation products with 2-minute updates. This dataset is used in a number of nowcasting studies S\u00f8nderby et al. (2020), Klocek et al. (2021).\n\n- - Radarnet Fairman et al. (2017) by UK Met Office, which has been collected since 1970 and contains composite precipitation data derived from reflectivity data.\n\n\nThe ready-to-go datasets, suitable for exploration by ML practitioners and researchers include:\n\n- - HKO-7 Shi et al. (2017) by Hong Kong Observatory (HKO) contains a single-height reflectivity level from a single radar at the center of Hong Kong for a six years period.\n\n- - KNMI Overeem & Imhoff (2020) by the Royal Netherlands Meteorological Institute (KNMI) contains ten-year single-height composite reflectivity data from two radars.\n\n- - TAASRAD19 Franch et al. (2020) by Meteotrentino contains nine-year aggregated reflectivity data from a single radar located in the Italian Alps. It is interesting due to geographical specifics and a large amount of extreme phenomena, such as snowstorms, hails, downpours, etc.\n\n- - SEVIR Veillette et al. (2020) by NOAA and the Geostationary Environmental Satellite System (GOES) contains US radar and satellite data for a two year-period, sampled either randomly or on an event basis. It is focused on the detection of storm events.\n\n- - RADOLAN Ramsauer et al. (2018) by the German Weather Service contains three-year reflectivity and precipitation data collected by 18 radars in Germany.\n\n\nOur dataset was collected by 30 radars of the Central Aerological Observatory (CAO) and contains two years of observations (spring 2019 - spring 2021). Measurements were carried out mainly in the European part of Russia, but some areas of the Siberian and Far Eastern regions were also captured.\n\n\nThe main advantages of our dataset relative to the abovementioned are as follows:\n\n- 1. A large area is covered with different climatic conditions - from the extreme north (Arkhangelsk,  $64.62^{\\circ}N$ $40.51^{\\circ}E$ ) to the southern regions (Krasnodar,  $45.04^{\\circ}N$ $39.15^{\\circ}E$ ).\n\n- 2. Dataset introduces continuous measurements over a year period, which allows taking into account the influence of seasonal dependence on strength and distribution of precipitation.\n\n- 3. The data presents several radar products at once: reflectivity and radial velocity at 10 altitude levels  $(1 - 10\\mathrm{km})$  and precipitation  $(600\\mathrm{m})$ . We do not specifically aggregate the reflectivity and radial velocity data, as we believe that it can be useful for forecasting to know how moisture and its velocity are distributed depending on the height. There are situations when precipitation has not yet appeared at low levels, but at the same time, at high levels, there is information about future precipitation (see an example in Figure 6 in Supp.Mat.).\n\n- 4. In addition to the radar data itself, we provide information about the orography in the area around each radar. These data allow one to investigate the influence of the surrounding orography in predicting precipitation.\n\n\n# 3 DATASET DESCRIPTION AND PROCESSING\n\n\nThe proposed RuDar dataset contains measurements from 30 weather radars mainly located in the European part of Russia (Figure 1). Radars scan the area around with a radius of 250 kilometers.\n\n\nTable 1: Comparative table of various international open sources of radar data. The data were compared by spatio-temporal (km per pixel / minutes) and pixel resolution, as well as by time periods, geographical coverage and the number of radars involved. The main products contained in the datasets are base and composite reflectivity, as well as derived products: precipitation, maximum reflectivity and vertically integrated liquid (VIL). If there is more than one radar, all except NEXRAD and RuDar combine measurements from different radars into one frame.\n\n\n(*) For the 2020.03\u20132021.02 period, we publish every fifth day to be used for validation and testing.  \n(**) Feature codes: (1) precipitation rate, (2) radial velocity, (3) reflectivity by altitude layers, (4) integrated or one layer reflectivity, (5) satellites\n\n\n<table><tr><td>Dataset</td><td>Time periods</td><td>Spatio-temporal resolution</td><td>Pixel resolution per frame</td><td>Geography</td><td>No. of radars</td><td>Features</td><td>Ready to use</td></tr><tr><td>NEXRAD Ansari et al. (2018)</td><td>1994\u2013</td><td>1 km / 5 min</td><td>460\u00d7460</td><td>United States</td><td>160</td><td>1, 2, 3, 4</td><td>-</td></tr><tr><td>MRMS Zhang et al. (2016)</td><td>2014\u2013</td><td>1 km / 2 min</td><td>3500\u00d77000</td><td>United States</td><td>160</td><td>1, 3, 4</td><td>-</td></tr><tr><td>Radarnet Fairman et al. (2017)</td><td>1970\u2013</td><td>1 km / 5 min</td><td>1536\u00d71280</td><td>United Kingdom</td><td>15</td><td>1, 3</td><td>-</td></tr><tr><td>HKO-7 Shi et al. (2017)</td><td>2009\u20132015</td><td>1.06 km / 6 min</td><td>480\u00d7480</td><td>Hong-Kong</td><td>1</td><td>4</td><td>+</td></tr><tr><td>KNMI Overeem &amp; Imhoff (2020)</td><td>2008\u20132018</td><td>1 km / 5 min</td><td>400\u00d7400</td><td>Netherlands</td><td>2</td><td>4</td><td>+</td></tr><tr><td>TAASRAD19 Franch et al. (2020)</td><td>2010\u20132019</td><td>0.5 km / 5 min</td><td>480\u00d7480</td><td>Italian Alps</td><td>1</td><td>4</td><td>+</td></tr><tr><td>SEVIR Veillette et al. (2020)</td><td>2017\u20132019</td><td>1 km / 5 min</td><td>384\u00d7384</td><td>United States</td><td>160</td><td>4, 5</td><td>+</td></tr><tr><td>RADOLAN Ramsauer et al. (2018)</td><td>2014.12\u20132017.11</td><td>1 km / 5 min</td><td>900\u00d7900</td><td>Germany</td><td>18</td><td>1, 4</td><td>+</td></tr><tr><td>RuDar (Ours)</td><td>2019.03\u20132021.02*</td><td>2 km / 10 min</td><td>252\u00d7252</td><td>Russia</td><td>30</td><td>1, 2, 3</td><td>+</td></tr></table>\n\n\nMeasurements come from radars every 10 minutes and have a spatial resolution of  $2 \\times 2$  kilometers. The dataset covers two years from 2019 to 2021 and has over 50 000 unique timestamps.\n\n\nEach data sample is a three-dimensional tensor that contains the result of a 10 minute scan of the atmosphere with a single radar. The center of the frame corresponds to the location of the radar in the scanned area. Tensors have 21 channels with a spatial resolution of  $252 \\times 252$  pixels. The first channel contains information about precipitation intensity (mm/h) on a  $600\\mathrm{m}$  altitude level. The channels 2-11 contain reflectivity measurements (dBZ) performed at 10 altitude levels from  $1\\mathrm{km}$  to  $10\\mathrm{km}$  with  $1\\mathrm{km}$  step, and the channels 12-21 contain 10 radial velocity (m/s) measurements from the same  $1 - 10\\mathrm{km}$  altitudes. We also provide data on orography Becker et al. (2009) and latitude-longitude coordinates of the territory surrounding the area of the radar measurements. Coordinates were initially set on a kilometer grid with a step of 2 kilometers with zero at the radar location.\n\n\nThe data example is partially shown in Figure 2a. Please refer to the Section D in Supplementary for the comprehensive documentation<sup>2</sup>.\n\n\n# 3.1 PRECIPIATION INTENSITY\n\n\nPrecipitation intensity is measured at an altitude of 600 meters in mm/h and is represented by a single channel in the data. It is not a direct measurement: the precipitation rate is calculated from reflectivity values with Marshal-Palmer type of Z-R relationship Marshall & Palmer (1948). In addition to mm/h values, two special values are presented in data:  $-2\\in 6$  value marks areas where measurements are not available,  $-1\\in 6$  value marks areas where no precipitation events were detected. Both values can be reduced to  $0\\mathrm{mm / h}$ , however, when feeding a radar frame to the model, it is better to mask  $-2\\in 6$  values so that the model can distinguish the absence of precipitation from the blind zone of the radar.\n\n\nFigure 2d shows the distribution of precipitation by seasons. The data for each season is taken from the entire period 2019-2021. As can be seen in Figure 2d, the intensity has a seasonal dependence: the highest values of precipitation amount are reached in summer and the lowest in winter.\n\n\nWe investigated the distribution of precipitation intensity difference between two adjacent radar images. As Figure 2e shows, in the winter season the precipitation rate difference between two adjacent radar images is much lower than in the summer season. This means that in winter precipitation changes much less in the short-term interval than in summer, and therefore it is more difficult to solve the problem of nowcasting in warmer seasons.\n\n\nPeaks which are visible in Figure 2e are the result of specificity of the combination of radar-side filtration algorithms and Marshall-Palmer type of Z-R relationship.\n\n\n![(a)](images/9b019653c88aedfdbe7bacb10fa13d7823ae78b484546b301878fbd9ecf78d23.jpg)\n\n\n![](images/3486f0e198b84c80380ff99289f74bd14821c375782aafb4f6b5b213b73b4dfa.jpg)\n\n\n![(c)](images/5ebc53b64e0c891478909b3555467b551a789aef25589281138221548d2c26c9.jpg)\n\n\n![Figure 2: Featuring RuDar dataset. (a) The sky above the Moscow Region, Vnukovo Radar; axes ticks are given in kilometers. Ten levels of the precipitation intensity are shown (for each height level from  $1\\mathrm{km}$  to  $10\\mathrm{km}$ ), together with the orography (magnified for visualization purposes) on the bottom. The lined cuts on the left part of the plot demonstrate a specific case of urban obstruction in the radar vision: skyscrapers are not shown in the orography map yet they affect the scan result. (b) Tensor representation for a single radar at the given timestamp: each tensor has a shape of (252, 252, 21). Note that orography information is stored separately to avoid duplication. (c) Natural obstructions near Mineralnye Vody city: the mountain chain on the south blocks the radar, reducing the receptive field down to the area denoted by the white dotted line. (d) Seasonal dependence of the precipitation intensity distribution (mm/h per pixel). During the winter season events with high precipitation rates are much less represented than in the summer season. We clipped high precipitation rates ( $>140\\mathrm{mm/h}$ ). (e) The distribution of precipitation intensity difference between two adjacent radar images (mm/h per pixel) in RuDar dataset. In the winter season, precipitation events change slower than in the summer season. The periodical peaks are the result of a peculiarity of the combination of radar-side filtration algorithms and Marshall-Palmer type of Z-R relationship. Best viewed in color.](images/d2934a1d463d295dfe3e2859113d7f160bd1f4ab76986fddf40e17961fd467e8.jpg)\n\n\n![](images/9ea25e5b2e4e517987a3062f06cf5818c6a11e0965866b90b6b3925511bb3703.jpg)\n\n\n# 3.2 RADAR REFLECTIVITY\n\n\nReflectivity values represent direct radar measurements in dBZ units. The radar measures the amount of energy reflected from droplets distributed in the atmosphere, and this amount of energy turns out to be proportional to the amount of moisture in the air. All reflectivity measurements were carried out at heights of  $1 - 10\\mathrm{km}$  with  $1\\mathrm{km}$  step. We provide reflectivity data at several altitude levels, since we believe that not only information about precipitation changes in time, but also in space, is important for nowcasting, see Fig. 6 in Supp. Mat. for an example.\n\n\nTo demonstrate the geospatial diversity, we compare the mean reflectivity for two radars: Arkhangelsk (in the north) and Krasnodar (in the south) in Figure 3. It can be seen that both seasonal and altitudinal dependencies are different for these radars, which shows the wide range of weather conditions and data variability.\n\n\n# 3.3 RADIAL VELOCITY\n\n\nThe radial velocity is the wind velocity projected onto the ray starting in the radar. Negative values correspond to the movement towards the radar, and positive values correspond to movement away from the radar. Due to the peculiarities of this type of measurement, the values are available only at points where the radar has registered moisture droplets. The data is measured in m/s and provided\n\n\n![Figure 3: Comparison of two different regions: Krasnodar (on the south) and Arkhangelsk (on the north) in terms of mean reflectivity (left) and mean radial velocity (right) on different height levels. Only non-zero values are taken into account. Regions demonstrate different patterns in the altitude intensity and radial velocity distribution as well as the seasonal changes.](images/c2b85ab3f45c626550491af9d844518a1151ba94bb544d6d2517cd9a5379902d.jpg)\n\n\n![](images/6d8daf18a6f880a9e8c0b2e308697602233e380d75eca29dc8e115ce07296cb9.jpg)\n\n\nfor ten altitude levels from  $1\\mathrm{km}$  to  $10\\mathrm{km}$  with  $1\\mathrm{km}$  step similar to reflectivity data. According to Figure 3, radial velocity values are seasonally dependent.\n\n\n# 3.4 GEOGRAPHICAL INFORMATION\n\n\nIn addition to atmosphere scans, the RuDar dataset contains orography and geographical coordinates associated with each radar. Geographical coordinates (latitudes and longitudes) are provided for each point where measurements are presented. Orography is a two-dimensional tensor in which positive values correspond to elevations above sea level and negative values correspond to elevations below sea level. We resampled an available information Becker et al. (2009) to the used coordinate grid. An example of orography is shown in Figure 2c where the case of natural radar vision obstruction is demonstrated. The radar's visibility area in the figure is greatly reduced by high mountains located in the immediate vicinity of Mineralnye Vody. Note that the orography measurements do not include the urban areas, and Figure 2a shows that skyscrapers in Moscow may also pose a problem.\n\n\n# 4 NOWCASTING BASELINES\n\n\n# 4.1 PROBLEM STATEMENT\n\n\nThe nowcasting problem can be formulated in terms of sequence prediction task (precisely video prediction task) - the goal is to predict a sequence of future observations from an input sequence of historical measurements. More formally, let us have an input sequence of measurements  $X = (X_{t-M+1}, \\ldots, X_t)$  of length  $M$  for time step  $t$ , and a sequence of future measurements  $Y = (Y_{t+1}, \\ldots, Y_{t+K})$  of length  $K$ . Then our task is to construct a model  $f$  that predicts  $Y$  by  $X$ :\n\n\n$$\n\n$$\nY _ {t + 1}, Y _ {t + 2}, \\dots , Y _ {t + K} = f \\left(X _ {t - M + 1}, \\dots , X _ {t - 1}, X _ {t}\\right). \\tag {1}\n$$\n\n$$\n\n\nEach element  $X_{i}$  is a tensor of shape  $H \\times W \\times C_{input}$ , where  $H$  and  $W$  are the height and width of one frame, and  $C_{input}$  is the number of measurements used for prediction. An element  $Y_{i}$  of the output sequence is a tensor of shape  $H \\times W \\times C_{output}$ , where  $C_{output}$  is the number of predicted measurements. The numbers  $C_{input}$  and  $C_{output}$  may differ, since in the input sequence one can possibly use all the available measurements (intensity, reflectivity, radial velocity, orography), and the output sequence will contain only the target values. In our case, the target is future precipitation intensity, which implies  $C_{output} = 1$ .\n\n\nThe quality of the model can be measured by how well it predicts the strength of precipitation directly, as well as how well it copes with detecting precipitation. In the first case, we use pixelwise mean squared error (MSE) to measure quality, in the second we use  $F_{1}$ -score, see details of calculation in\n\n\nSection I of Supplementary Material. In meteorological articles Espeholt et al. (2021); Ravuri et al. (2021); Bouget et al. (2021), metrics such as Probability of Detection (POD), Success Ratio (SUCR), Critical Success Index (CSI) are sometimes used, but they are actually the equivalents of recall, precision and intersection over union (IOU), - standard metrics in machine learning. We chose  $F_{1}$  because it correlates with IOU and depends on both precision and recall.\n\n\n# 4.2 EXPERIMENTAL SETUP\n\n\n# 4.2.1 DATA SETUP\n\n\nWe are focusing on a year scale setup, which uses 2019 data for training, and every fifth day of 2020-2021 data for validation and testing.\n\n\nIn our experiments, we use 4 input tensors with historical observations to predict 12 future intensity frames ( $M = 4$ ,  $K = 12$ ,  $C_{output} = 1$ ). The spatial size of the input and output tensors is  $H = W = 252$ . We clip precipitation intensity values to  $50~\\mathrm{mm / h}$ , since high intensity rates are rare in the given geographical area. As we use an MSE loss to train our models, we use a binary mask for the special value  $-2 \\in 6$  to prevent penalizing our models for errors outside the radar visibility range.\n\n\nWhen only intensity is used as an input observation,  $C_{input} = 1$ . Using reflectivity levels increases  $C_{input}$  to 11. All values are clipped in the interval between  $-32$  dBZ and  $65$  dBZ, and then reduced to the interval from 0 to 1 by subtracting  $-32$  dBZ and dividing by  $65 - (-32) = 87$  dBZ. The intensity values for this setup are divided by  $50\\mathrm{mm / h}$  (for both input and target frames) to be also in the unit range (0, 1). Radial velocity also adds 10 input channels ( $C_{input} = 11$ ). All radial velocity values are clipped between  $-63\\mathrm{m / s}$  and  $63\\mathrm{m / s}$  and then divided by  $63\\mathrm{m / s}$  to be in the interval  $(-1, 1)$ . Orography adds one additional channel ( $C_{input} = 2$ ). Negative values of heights are reduced to zero, and positive values are divided by the maximum height equal to  $5336\\mathrm{m}$ .\n\n\nFor both radial velocity and orography, the intensity preprocessing is the same as in the absence of additional features.\n\n\n# 4.2.2 MODELS AND SETUPS\n\n\nWe represented  $f(\\cdot)$  with a persistent model as a weak baseline and with a state-of-the-art optical flow approach Ayzel et al. (2019) as a strong baseline. Also we trained UNet-like Veillette et al. (2020) and ConvLSTM-like Shi et al. (2015) neural network models (including Extended ConvLSTM architecture shown in Fig.4b) and tested a pretrained GAN-like model Ravuri et al. (2021). Each of the above models (excepting the pretrained GAN model) was trained on a single Tesla A100 80GB for approximately 70 hours. Details about model architectures are provided in Section E of Supplementary Materials.\n\n\nAlso we evaluated Earthformer Gao et al. (2022) on the proposed dataset using official implementation. Other strong neural network baselines S\u00f8nderby et al. (2020); Klocek et al. (2021); Espeholt et al. (2021) will be considered in future research as they require additional input data that is not presented in the proposed dataset. Satellite information is used in S\u00f8nderby et al. (2020), numerical weather forecast is used in Klocek et al. (2021), and Espeholt et al. (2021) requires both.\n\n\n# 4.3 RESULTS\n\n\nWe provide the results on  $F_{1}$  and  $MSE$  for each of the above models, as well as the results of Extended ConvLSTM using each of the additional features. We evaluate the baselines on seasonal test splits of the RuDar dataset. The results are shown in Table 2.\n\n\nAll baselines work better than Persistent, and among baselines without additional features, Extended ConvLSTM shows the best result in all seasons except summer: there it is inferior to Optical Flow in  $F_{1}$ .\n\n\nAdding reflectivity shows the best result for all metrics and seasons, except for  $F_{1}$  in summer (where it is inferior to Optical Flow) and in autumn (where it is inferior to Extended ConvLSTM with orography). Adding radial speed improves both  $MSE$  and  $F_{1}$  and adding orography improves  $F_{1}$  relative to Extended ConvLSTM without additional features.\n\n\nTable 2: Metrics of baseline models calculated separately for each season. The first value in the cell is  $MSE$ , the second value is  $F_{1}$ . Metrics are average across prediction horizons. Red highlights the maximum in the entire column, blue highlights the maximum among models without additional features.\n\n\n<table><tr><td></td><td>Spring 2020</td><td>Summer 2020</td><td>Autumn 2020</td><td>Winter 2020-2021</td></tr><tr><td>Persistent</td><td>0.2691 / 0.5177</td><td>0.6281 / 0.4975</td><td>0.0579 / 0.5233</td><td>0.0260 / 0.6425</td></tr><tr><td>Optical Flow</td><td>0.1936 / 0.6297</td><td>0.4768 / 0.6435</td><td>0.0384 / 0.6351</td><td>0.0190 / 0.6884</td></tr><tr><td>GAN</td><td>0.1885 / 0.6368</td><td>0.5820 / 0.6140</td><td>0.0352 / 0.6507</td><td>0.0218 / 0.6580</td></tr><tr><td>U-Net</td><td>0.1420 / 0.5649</td><td>0.3215 / 0.5406</td><td>0.0312 / 0.5730</td><td>0.0160 / 0.6232</td></tr><tr><td>ConvLSTM</td><td>0.1217 / 0.6222</td><td>0.3244 / 0.5580</td><td>0.0242 / 0.6650</td><td>0.0156 / 0.6799</td></tr><tr><td>Extended ConvLSTM</td><td>0.1192 / 0.6436</td><td>0.3193 / 0.5877</td><td>0.0233 / 0.6814</td><td>0.0150 / 0.6947</td></tr><tr><td>Extended ConvLSTM + reflectivity</td><td>0.1133 / 0.6597</td><td>0.3080 / 0.6170</td><td>0.0219 / 0.6936</td><td>0.0140 / 0.7014</td></tr><tr><td>Extended ConvLSTM + radial velocity</td><td>0.1170 / 0.6516</td><td>0.3153 / 0.5970</td><td>0.0228 / 0.6885</td><td>0.0144 / 0.7038</td></tr><tr><td>Extended ConvLSTM + orography</td><td>0.1193 / 0.6526</td><td>0.3197 / 0.5968</td><td>0.0234 / 0.6876</td><td>0.0148 / 0.7016</td></tr><tr><td>Earthformer</td><td>0.0124 / NA</td><td>0.0153 / NA</td><td>0.0047 / NA</td><td>0.0048 / NA</td></tr></table>\n\n\n![](images/9b975d14807bf789ca0c22282d655937c76750cb1835c552c8b7fba8f622c081.jpg)\n\n\n![](images/798e72c213a50109713fae9f8d01e9cbea2fe4c074f2769ad13e50e7f2280480.jpg)\n\n\n![Figure 4: (Left) Algorithm performance (left column:  $F_{1}$ , right column: MSE) as a function of a prediction horizon on the spring season of the test set. Top row: baseline algorithms, bottom row: Extended ConvLSTM with the various additional data. While optical flow may outperform other models on the initial time steps, NN-based and trained baselines easily outperform baselines. Usage of the radar reflectivity improves the performance of the ConvLSTM model. (Right) An extension of the architecture from Shi et al. (2015). The first ConvLSTM layer receives the output of a convolutional encoder as input, and the output of the last ConvLSTM layer is upsampled to the final prediction with a convolutional decoder. To output the next frame, the predictor uses the outputs of the last ConvLSTM layer from the previous step.](images/5a26ecde1fcf3943043f623642b88b80559d4db4f25fb822c4dfca64ef21b3a1.jpg)\n\n\n![](images/91639229c0a577590ed803505e8ed6a79303da312ba46cae8f54c98f30fc8f0c.jpg)\n\n\n![](images/5a55689205526b516be921d61af12155f78d142e91285d65c312219527c895e4.jpg)\n\n\nWe also show the relationship between algorithm performance and forecast horizon for the spring season in Figure 4a.\n\n\n# 5 UNCERTAINTY ESTIMATION FOR THE ENSEMBLE CASE\n\n\nWeather forecasting task, given its properties such as noisy data, probabilistic predictions, and general weather variability, may organically benefit from the uncertainty quantification. Uncertainty estimates (UEs) are helpful in a number of cases, including \"failing to predict\" (when the model understands that its prediction is not reliable enough, and gets the results from the basic robust model), active learning, data shift reaction, as well as the error analysis and model selection.\n\n\nWe focus on the last scenario and analyze the relationship between the error and UE for the ensemble case. Given an ensemble of five ConvLSTM models, trained on the same data but different seeds, we treat the mean and standard deviation of predictions as the output and UE, correspondingly. An ensemble error and UE on the test set are correlated with a large Spearman coefficient  $(>0.8)$ ; this relationship holds even when both metrics are averaged over the various dimensions: time, location, and forecast horizon, see Fig. 7, 8 in Section H of Supp.Mat. We also report that we did not find a relationship between the UE and orography (Pearson  $|r| < 0.1$ ).\n\n\nThis analysis suggests that UE metrics can be used to indicate large errors, which, in turn, can be used to deliver better predictions or uncertainty of the forecast, as well as assist the model selection and improvement.\n\n\n# 6 DISCUSSION\n\n\n# 6.1 SUMMARY\n\n\nIn this paper, we propose a weather radar dataset with a wide variety of geographical and climatic conditions and show in contrast to previously published works that a precipitation nowcasting task can be a seasonal dependent problem. This encourages the usage of the seasonal models and separate models for precipitation rate forecasting and precipitation events existence.\n\n\nWe also evaluated an ensemble of nowcasting models, analyzing the uncertainty estimates and showing its deep relation to the ensemble error on various scales.\n\n\nWe believe that our proposed dataset will become a reliable benchmark for precipitation nowcasting models. The code for data preprocessing and model preparation is publicly available  $^{3}$ . This dataset is provided under the CC BY NC SA 4.0 license. The data is maintained on the cloud service, and is maintained by authors team $^{4}$ .\n\n\n# 6.2 LIMITATIONS\n\n\nWe would like to point out a few limitations of our work:\n\n- - Dataset size. We included the two-year period only in order for the dataset to be comprehensible and processible by the community, as its unpacked size already exceeds 100 GB. This dataset is not intended for a study of long-term climatic changes.\n\n- - Data shift. Like any other piece of real-world data, our dataset may contain several peculiarities. In the text, we discuss the seasonal dependence and geographical shifts, however, it should be noted that the measurements within the sample are also not simultaneous as they were obtained by combining the measurements from the narrow beam that scans the atmosphere.\n\n- - Baselines. We provide the readers with simple baselines supporting the dataset. However, we plan to benchmark a number of state-of-the-art neural network-based baselines S\u00f8nderby et al. (2020); Espeholt et al. (2021); Klocek et al. (2021), in future studies.\n\n\n# 6.3 POSSIBLE PROSPECTS\n\n\nA two-year dataset may be applicable not for the nowcasting task only but in a number of contemporary ML problems, including:\n\n\nRare event detection. Various storms and rare weather conditions are of special interest both to the researchers and end-users. Some works even emphasize this area of research Veillette et al. (2020). We expect various anomaly detection algorithms to be of great use here.\n\n\nData shift and uncertainty estimation. The variety of both geographical and temporal conditions makes this dataset a good candidate for the modeling of distribution shift scenarios (see, e.g. Malinin et al. (2021)), where test data may naturally vary from the training data dramatically. In this case, various uncertainty estimation approaches, like in Gronquist et al. (2019) may be helpful.\n\n\nActive learning. One of the most important scenarios for the day-to-day forecasting systems is active learning since continuous data flow allows for the production model to learn from its own mistakes, correcting previous predictions. However, processing and retraining on huge amounts of data poses a challenge, and one may use smarter ways of data sampling (of both past archives and daily chunks of data) in order to reduce the data processing and model training times.\n\n\n# REFERENCES\n\n- Steve Ansari, Stephen Del Greco, Edward Kearns, Otis Brown, Scott Wilkins, Mohan Ramamurthy, Jeff Weber, Ryan May, Jed Sundwall, Jeff Layton, Ariel Gold, Adam Pasch, and Valliappa Lakshmanan. Unlocking the Potential of NEXRAD Data through NOAA's Big Data Partnership. Bulletin of the American Meteorological Society, 99(1):189 - 204, 2018. doi: 10.1175/BAMS-D-16-0021.1. URL https://journals.ametsoc.org/view/journals/bams/99/1/bams-d-16-0021.1.xml.\n\n- G. Ayzel, M. Heistermann, and T. Wintersmith. Optical flow models as an open benchmark for radar-based precipitation nowcasting (rainymotion v0.1). Geoscientific Model Development, 12 (4):1387-1402, 2019. doi: 10.5194/gmd-12-1387-2019. URL https://gmd.copernicus.org/articles/12/1387/2019/.\n\n- J. J. Becker, D. T. Sandwell, W. H. F. Smith, J. Braud, B. Binder, J. Depner, D. Fabre, J. Factor, S. Ingalls, S-H. Kim, R. Ladner, K. Marks, S. Nelson, A. Pharaoh, R. Trimmer, J. Von Rosenberg, G. Wallace, and P. Weatherall. Global bathymetry and elevation data at 30 arc seconds resolution: SRTM30_plus. Marine Geodesy, 32(4):355-371, 2009. doi: 10.1080/01490410903297766. URL https://doi.org/10.1080/01490410903297766.\n\n- Vincent Bouget, Dominique B\u00e9r\u00e9ziat, Julien Brajard, Anastase Charantonis, and Arthur Filoche. Fusion of rain radar images and wind forecasts in a deep learning model applied to rain nowcasting. Remote Sensing, 13(2), 2021. ISSN 2072-4292. doi: 10.3390/rs13020246. URL https://www.mdpi.com/2072-4292/13/2/246.\n\n- Lasse Espeholt, Shreya Agrawal, Casper S\u00f8nderby, Manoj Kumar, Jonathan Heek, Carla Bromberg, Cenk Gazen, Jason Hickey, Aaron Bell, and Nal Kalchbrenner. Skillful Twelve Hour Precipitation Forecasts using Large Context Neural Networks. arXiv preprint arXiv:2111.07470, 2021.\n\n- Jonathan G Fairman, David M Schultz, Daniel J Kirshbaum, Suzanne L Gray, and Andrew I Barrett. Climatology of Size, Shape, and Intensity of Precipitation Features over Great Britain and Ireland. Journal of Hydrometeorology, 18(6):1595-1615, 2017. doi: 10.1175/JHM-D-16-0222.1. URL https://journals.ametsoc.org/view/journals/hydr/18/6/jhm-d-16-0222{\\_}1.xml.\n\n- Gabriele Franch, Valerio Maggio, Luca Coviello, Marta Pendesini, Giuseppe Jurman, and Cesare Furlanello. TAASRAD19, a high-resolution weather radar reflectivity dataset for precipitation nowcasting. Scientific Data, 7(1):1-13, 2020.\n\n- Zhihan Gao, Xingjian Shi, Hao Wang, Yi Zhu, Bernie Wang, Mu Li, and Dit-Yan Yeung. Earthformer: Exploring space-time transformers for earth system forecasting. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=1zZstLVGVGW.\n\n- Peter Gronquist, Tal Ben-Nun, Nikoli Dryden, Peter Dueben, Luca Lavarini, Shigang Li, and Torsten Hoefler. Predicting weather uncertainty with deep convnets. arXiv preprint arXiv:1911.00630, 2019.\n\n- Iwan Holleman. Bias adjustment and long-term verification of radar-based precipitation estimates. Meteorological Applications, 14(2):195-203, 2007. doi: https://doi.org/10.1002/met.22. URL https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/met.22.\n\n- Sylwester Klocek, Haiyu Dong, Matthew Dixon, Panashe Kanengoni, Najeeb Kazmi, Pete Luferenko, Zhongjian Lv, Shikhar Sharma, Jonathan Weyn, and Siqi Xiang. MS-nowcasting: Operational Precipitation Nowcasting with Convolutional LSTMs at Microsoft Weather. arXiv preprint arXiv:2111.09954, 2021.\n\n- Andrey Malinin, Neil Band, German Chesnokov, Yarin Gal, Mark JF Gales, Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal Raina, et al. Shifts: A dataset of real distributional shift across multiple large-scale tasks. arXiv preprint arXiv:2107.07455, 2021.\n\n- J. S. Marshall and W. Mc K. Palmer. The distribution of raindrops with size. Journal of Atmospheric Sciences, 5(4):165 - 166, 1948. doi: 10.1175/1520-0469(1948)005<0165:TDORWS>2.0.CO;2. URL https://journals.ametsoc.org/view/journals/atsc/5/4/1520-0469_1948_005_0165_tdorws_2_0_co_2.xml.\n\n- Aart Overeem and Ruben Imhoff. *Archived 5-min rainfall accumulations from a radar dataset for the Netherlands*, Mar 2020. URL https://data.4tu.nl/articles/dataset/Archived_5-min_rainfall_accumulations_from_a_radar_dataset_for_the_Netherlands/12675278/1.\n\n- Thomas Ramsauer, Thomas Wei\u00df, and Philip Marzahn. Comparison of the GPM IMERG final precipitation product to RADOLAN weather radar data over the topographically and climatically diverse germany. Remote Sensing, 10(12), 2018. ISSN 2072-4292. doi: 10.3390/rs10122029. URL https://www.mdpi.com/2072-4292/10/12/2029.\n\n- Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Piotr Mirowski, Megan Fitzsimons, Maria Athanassiadou, Sheleem Kashem, Sam Madge, et al. Skilful precipitation nowcasting using deep generative models of radar. Nature, 597(7878):672-677, 2021.\n\n- Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. Springer, 2015.\n\n- Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun Woo. Convolutional LSTM network: a machine learning approach for precipitation nowcasting. In Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1, pp. 802-810, 2015.\n\n- Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun WOO. Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/a6db4ed04f1621a119799fd3d7545d3d-Paper.pdf.\n\n- Mennatullah Siam, Sepehr Valipour, Martin Jagersand, and Nilanjan Ray. Convolutional gated recurrent networks for video segmentation. In 2017 IEEE International Conference on Image Processing (ICIP), pp. 3090-3094, 2017. doi: 10.1109/ICIP.2017.8296851.\n\n- Casper Kaae S\u00f8nderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, and Nal Kalchbrenner. Metnet: A neural weather model for precipitation forecasting. arXiv preprint arXiv:2003.12140, 2020.\n\n- Mark Veillette, Siddharth Samsi, and Chris Mattioli. SEVIR: A storm event imagery dataset for deep learning applications in radar and satellite meteorology. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 22009-22019. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/fa78a16157fed00d7a80515818432169-Paper.pdf.\n\n- Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and Philip S Yu. PredRNN++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5123-5132. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/wang18b.html.\n\n- Jian Zhang, Kenneth Howard, Carrie Langston, Brian Kaney, Youcun Qi, Lin Tang, Heather Grams, Yadong Wang, Stephen Cocks, Steven Martinaitis, et al. Multi-Radar Multi-Sensor (MRMS) quantitative precipitation estimation: Initial operating capabilities. Bulletin of the American Meteorological Society, 97(4):621-638, 2016.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634691.7282052", "msecs": "728.0", "relativeCreated": "36551.679372787476", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# How Low Can We Go: Trading Memory for Error in Low-Precision Training\n\n# Abstract\nLow-precision arithmetic trains deep learning models using less energy, less memory and less time. However, we pay a price for the savings: lower precision may yield larger round-off error and hence larger prediction error. As applications proliferate, users must choose which precision to use to train a new model, and chip manufacturers must decide which precisions to manufacture. We view these precision choices as a hyperparameter tuning problem, and borrow ideas from meta-learning to learn the tradeoff between memory and error. In this paper, we introduce Pareto Estimation to Pick the Perfect Precision (PEPPP). We use matrix factorization to find non-dominated configurations (the Pareto frontier) with a limited number of network evaluations. For any given memory budget, the precision that minimizes error is a point on this frontier. Practitioners can use the frontier to trade memory for error and choose the best precision for their goals.\n\n\n# 1 INTRODUCTION\n\n\nTraining modern-day neural networks is becoming increasingly expensive as task and model sizes increase. The energy consumption of the corresponding computation has increased alarmingly, and raises doubts about the sustainability of modern training practices (Schwartz et al., 2019). Low-precision training can reduce consumption of both computation (De Sa et al., 2018) and memory (Sohoni et al., 2019a), thus minimizing the cost and energy to train larger models and making deep learning more accessible to resource-limited users.\n\n\nLow-precision training replaces 32-bit or 64-bit floating point numbers with fixed or floating point numbers that allocate fewer bits for the activations, optimizers, and weights. A rich variety of methods appear in recent literature, including bit-centering (De Sa et al., 2018), loss scaling (Micikevicius et al., 2017) and mixed-precision training (Zhou et al., 2016). There is however a fundamental tradeoff: lowering the number of bits of precision increases the quantization error, which may disrupt convergence and increase downstream error (Zhang et al., 2016; Courbariaux et al., 2014; Gupta et al., 2015). The low-precision configuration is thus a hyperparameter to be chosen according to the resource constraints of the practitioner.\n\n\n![(a) error vs memory on CIFAR-10 (Krizhevsky et al., 2009)](images/bf3eed83e98a16a1d63f290ba48a56b1ef09695237b8ca9d02b4c90af7095ce5.jpg)\n\n\n![(b) # activation bits in non-dominated configurations Figure 1: Test error vs memory for ResNet-18 across 99 low-precision floating point configurations. Figure (a) shows the tradeoff on CIFAR-10. (Non-dominated points are blue circles.) Figure (b) shows that the best precision to use varies depending on the memory budget, on 87 image datasets. See Section 4 for experimental details.](images/ca82afa05cb37f487616729bcd307fef9b46547a927f1c9a1d36718af2b3418e.jpg)\n\n\nHow should we choose this hyperparameter? Many believe the highest allowable precision (given a memory budget) generally produces the lowest error model. However, there are typically many ways to use the same amount of memory. As shown in Figure 1(a), some of these configurations produce much lower error than others! Figure 1(b) shows that no one configuration dominates all others. We might consult previous literature to choose a precision; but this approach fails for new applications.\n\n\nOur goal is to efficiently pick the best low-precision configuration under a memory budget. Efficiency is especially important for resource-constrained practitioners, such as individual users or early-stage startups. To promote efficiency, we use a meta-learning (Lemke et al., 2015; Vanschooren, 2018; Hospedales et al., 2020) approach: we train a small number of cheap very-low-precision models on the dataset to choose the perfect precision. The gains from choosing the right low-precision format can offset the cost of this extra training \u2014 but each precision must be chosen carefully to realize the benefits of low-precision training.\n\n\nWe use ideas from multi-objective optimization to characterize the tradeoff between memory and error and identify the Pareto frontier: the set of non-dominated solutions. Users will want to understand the tradeoff between error and memory so they can determine the resources needed to adequately train a model for a given machine learning task. This tradeoff may also influence the design of application-specific low-precision hardware, with profound implications for the future (Hooker, 2020). For example, among all 99 low-precision configurations we tried, we identified some configurations that are Pareto optimal across many different tasks (listed in Appendix D). These results could help hardware manufacturers decide which precision settings are worth manufacturing.\n\n\nComputing the Pareto frontier by training models for all low-precision configurations is expensive and unnecessary. Cloud computing platforms like Google Cloud and Amazon EC2 charge more for machines with the same CPU and double memory:  $25\\%$  more on Google  $\\mathrm{Cloud}^1$  and  $100\\%$  more on Amazon  $\\mathrm{EC2}^2$  as of mid-September 2021. We use techniques from meta-learning to leverage the information from other low-precision training runs on related datasets. This approach allows us to estimate the Pareto frontier without evaluating all of the low-precision configurations.\n\n\nOur system, Pareto Estimation to Pick the Perfect Precision (PEPPP), has two goals. The first goal is to find the Pareto frontiers of a collection of related tasks, and is called meta-training in the meta-learning literature. Meta-training requires a set of measurements, each collected by training and testing a neural network with a given precision on a dataset. This information can be gathered offline with a relatively large resource budget, or by crowdsourcing amongst the academic or open-source community. Still, it is absurdly expensive to exhaustively evaluate all measurements: that is, every possible low-precision configuration on every task. Instead, we study how to choose a subset of the possible measurements to achieve the best estimate. The second goal we call meta-test: using the information learned on previous tasks, how can we transfer that information to a new task to efficiently estimate its Pareto frontier? This goal corresponds to a resource-constrained individual or startup who wants to determine the best low-precision configuration for a new dataset.\n\n\nBoth meta-training and meta-test rely on matrix completion and active learning techniques to avoid exhaustive search: we make a subset of all possible measurements and predict the rest. We then estimate the Pareto frontier to help the user make an informed choice of the best configuration. We consider two sampling schemes: uniform and non-uniform sampling. Uniform sampling is straightforward. Non-uniform sampling estimates the Pareto frontier more efficiently by making fewer or even no high-memory measurements.\n\n\nTo the best of our knowledge, PEPPP is the first to study the error-memory tradeoff in low-precision training and inference without exhaustive search. Some previous works show the benefit of low-precision arithmetic in significant energy reduction at the cost of a small accuracy decrease (Hashemi et al., 2017), and propose hardware-software codesign frameworks to select the desired model (Langroudi et al., 2019). Many other papers focus on low-precision inference only and use it for model compression (Wu et al., 2018; Cai & Vasconcelos, 2020). Another line of work seeks state-of-the-art (SOTA) performance with low-precision training, using carefully designed learning schedules (Sun et al., 2019; 2020). Our work imagines low-precision as a way to reduce training or deployment costs of customized models trained on proprietary datasets (not SOTA models on ML benchmarks). Even so, we show in Section 4 that our work picks promising models for every memory budget, which enables near-SOTA results on CIFAR-10.\n\n\nFigure 2 shows a flowchart of PEPPP. The rest of this paper is organized as follows. Section 2 introduces notation and terminology. Section 3 describes the main ideas we use to actively sample configurations and approximate Pareto frontiers. Section 4 shows experimental results.\n\n\n![Figure 2: The PEPPP workflow. We begin with a collection of (meta-) training datasets and low precision configurations. In the meta-training phase, we sample dataset-configuration pairs to train, and compute the misclassification error. We use matrix factorization to compute a low dimensional embedding of every configuration. In the meta-test phase, our goal is to pick the perfect precision (within our memory budget) for the meta-test dataset. We compute the memory required for each configuration, and we select a subset of fast, informative configurations to evaluate. By regressing the errors of these configurations on the configuration embeddings, we find an embedding for the meta-test dataset, which we use to predict the error of every other configuration (including more expensive ones) and select the best subject to our memory budget.](images/583df5bab9b72a01338cc0e6beef3e4a0014806c0431755d35b706c81e7b074b.jpg)\n\n\n# 2 NOTATION AND TERMINOLOGY\n\n\nLow-precision formats. We use floating point numbers for low-precision training in this paper.\n\n\nAs an example, Figure 3 shows an 8-bit floating point number with 3 exponent bits and 4 mantissa bits. A specific low-precision representation with a certain number of bits for each part is called a low-precision format. We may use different low-precision formats for the weights, activations, and optimizer. In the case of using two formats to train and represent a\n\n\n![Figure 3: An 8-bit floating point number representing  $(-1)^{\\mathrm{sign}}\\cdot 2^{\\mathrm{exponent} - 7}\\cdot 1.b_{3}b_{2}b_{1}b_{0}$](images/33960eaeb3ba45416a9deefa3a5df94f3025b3da72bbfa261173c0dd729733c0.jpg)\n\n\nneural network (as discussed in Section 3.1), these two formats are called Format  $A$  and  $B$ . A specific combination of these two formats is a hyperparameter setting of the neural network, and we call it a low-precision configuration.\n\n\nMath basics. We define  $[n] = \\{1, \\dots, n\\}$  for a positive integer  $n$ . With a Boolean variable  $\\mathcal{X}$ , the indicator function  $\\mathbb{1}(\\mathcal{X})$  equals 1 if  $\\mathcal{X}$  is true, and 0 otherwise. With a scalar variable  $x$ , we use  $x_{+}$  to denote  $\\max \\{x, 0\\} \\subseteq$  and  $\\subset$  denote subset and strict subset, respectively.\n\n\nLinear algebra. We denote vector and matrix variables respectively by lowercase letters  $(a)$  and capital letters  $(A)$ . All vectors are column vectors. The Euclidean norm of a vector  $a \\in \\mathbb{R}^n$  is  $\\|a\\| := \\sqrt{\\sum_{i=1}^{n} a_i^2}$ . To denote a part of a matrix  $A \\in \\mathbb{R}^{n \\times d}$ , we use a colon to denote the varying dimension:  $A_{i,:}$  and  $A_{:,j}$  (or  $a_j$ ) denote the  $i$ th row and  $j$ th column of  $A$ , respectively, and  $A_{ij}$  denotes the  $(i,j)$ -th entry. With an ordered index set  $\\mathcal{S} = \\{s_1, \\ldots, s_k\\}$  where  $s_1 < \\ldots < s_k \\in [d]$ , we denote  $A_{:\\mathcal{S}} := [A_{:,s_1} \\cdots A_{:,s_k}] \\in \\mathbb{R}^{n \\times k}$ . Given two vectors  $x, y \\in \\mathbb{R}^n$ ,  $x \\preceq y$  means  $x_i \\leq y_i$  for each  $i \\in [n]$ . Given a matrix  $A \\in \\mathbb{R}^{n \\times d}$  and a set of observed indices as  $\\Omega \\subseteq [n] \\times [d]$ , the partially observed matrix  $P_{\\Omega}(A) \\in \\mathbb{R}^{n \\times d}$  has entries  $(P_{\\Omega}(A))_{ij} = A_{ij}$  if  $(i,j) \\in \\Omega$ , and 0 otherwise.\n\n\nPareto frontier. Multi-objective optimization simultaneously minimizes  $n$  costs  $\\{c_i\\}_{i \\in [n]}$ . A feasible point  $c^{(1)} = (c_1^{(1)}, \\ldots, c_n^{(1)})$  is Pareto optimal if for any other feasible point  $c^{(2)} = (c_1^{(2)}, \\ldots, c_n^{(2)})$ ,\n\n\n$c^{(2)} \\preceq c^{(1)}$  implies  $c^{(2)} = c^{(1)}$  (Boyd & Vandenberghe, 2004). The set of Pareto optimal points is the Pareto frontier.\n\n\nTasks, datasets, measurements and evaluations. A task carries out a process (classification, regression, image segmentation, etc.) on a dataset. Given a deep learning model and a dataset, the training and testing of the model on the dataset is called a measurement. In our low-precision context, we evaluate a configuration on a dataset to make a measurement.\n\n\nMeta-learning. Meta-learning transfers knowledge from past (meta-training) tasks to better understand the new (meta-test) task. To evaluate the error of meta-training, we use meta-leave-one-out cross-validation (meta-LOOCV): on a collection of tasks, each time we treat one task as meta-test and the others as meta-training, and average the results over all splits.\n\n\nError matrix and memory matrix. Given a neural network, the errors of different low-precision configurations on meta-training datasets form an error matrix, whose  $(i,j)$ -th entry  $E_{ij}$  is the test error of the  $j$ th configuration on the  $i$ th dataset. To compute the error matrix, we split the  $i$ -th dataset into training and test subsets (or use a given split), train the neural network at the  $j$ -th configuration on the training subset, and evaluate the test error on the test subset. The memory required for each measurement forms a memory matrix  $M$ , which has the same shape as the corresponding error matrix. Example error and memory matrices are shown in Figure 4.\n\n\n<table><tr><td colspan=\"5\">low-precision configurations</td></tr><tr><td></td><td>a</td><td>b</td><td>c</td><td>d</td></tr><tr><td>1</td><td>0.85</td><td>0.71</td><td>0.32</td><td>0.28</td></tr><tr><td>2</td><td>0.80</td><td>0.80</td><td>0.80</td><td>0.69</td></tr><tr><td>3</td><td>0.80</td><td>0.68</td><td>0.46</td><td>0.45</td></tr></table>\n\n\n<table><tr><td colspan=\"5\">low-precision configurations</td></tr><tr><td></td><td>a</td><td>b</td><td>c</td><td>d</td></tr><tr><td rowspan=\"3\">1 datasets</td><td>181</td><td>269</td><td>272</td><td>295</td></tr><tr><td>180</td><td>269</td><td>272</td><td>295</td></tr><tr><td>590</td><td>924</td><td>927</td><td>1032</td></tr><tr><td colspan=\"5\">(b) memory matrix M (MB)</td></tr></table>\n\n\nFigure 4: Example error and memory matrices for some datasets and low-precision configurations. Dataset 1: CIFAR-10, 2: CIFAR-100 (fruit and vegetables), 3: ImageNet-stick (Deng et al., 2009). Configuration Format A (exponent bits, mantissa bits), Format B (exponent bits, mantissa bits). a: (3, 1), (6, 7); b: (3, 4), (7, 7); c: (4, 3), (8, 7); d: (5, 3), (6, 7).\n\n\n# 3 METHODOLOGY\n\n\nPEPPP operates in two phases: meta-training and meta-test. First in meta-training, we learn configuration embeddings by training a neural network of a specific architecture at different low-precision configurations on different datasets. As listed in Appendix A, we study image classification tasks and a range of low-precision formats that vary in the number of bits for the exponent and mantissa for the activations, optimizer, and weights. Our hope is to avoid enumerating every possible configuration since exhaustive search is prohibitive in practice. To this end, we make a few measurements and then predict the other test errors by active learning techniques. We also compute the memory matrix to understand the memory consumption of each measurement.\n\n\nThen in meta-test, we assume that we have completed meta-training, and hence know the configuration embeddings. The meta-test goal is to predict the tradeoff between memory usage and test error on a new (meta-test) dataset. This step corresponds to inference in traditional machine learning; it must be quick and cheap to satisfy the needs of resource-constrained practitioners. To select the configuration that has smallest test error and takes less than the memory limit, we measure a few selected (informative and cheap) configurations on the meta-test dataset, and use the information of their test errors to predict the rest. The problem of choosing measurements is known as the \"cold-start\" problem in the literature on recommender systems. Then we use the model built on the meta-training datasets to estimate values for the other measurements on the meta-test dataset.\n\n\n# 3.1 META-TRAINING\n\n\nOn the  $n$  meta-training datasets, we first theoretically compute the memory needed to evaluate each of the  $d$  low-precision configurations to obtain the full memory matrix  $M \\in \\mathbb{R}^{n \\times d}$ . The total memory consists of memory needed for model weights, activations, and the optimizer (gradients, gradient accumulators, etc.) (Sohoni et al., 2019b). Among these three types, activations typically dominate, as shown in Figure 5. Thus using lower precision for activations drastically reduces memory usage. Additionally, empirical studies (Zhou et al., 2016) have shown that adopting higher precision formats for the optimizer can substantially improve the accuracy of the trained model. Since the optimizer usually requires a relatively small memory, it is often the best to use a higher precision format for this component.\n\n\nThus for each low-precision configuration, it is typical to use a lower precision for network weights and activations, and a higher precision for the optimizer, resulting in combinatorially many choices in total. An example of the memory usage in this low-precision scenario is also shown in Figure 5.\n\n\nIn practice, ML tasks are often correlated: for example, meta-training datasets might be subsets of a large dataset like CIFAR-100 or ImageNet. The ranking of different configurations tends to be similar on similar tasks. For example, we computed the test errors of 99 low-precision configurations on 87 datasets (both listed in Appendix A) to form a performance vector in  $\\mathbb{R}^{99}$  for each dataset. We use the Kendall tau correlation to characterize the alignment between the ranking of errors incurred by different configurations on two datasets: the Kendall tau correlation is 1 if the order is the same and -1 if the order is reversed. As shown in Figure 6(a), similar datasets have larger correlations: for example, datasets with indices 38-87 correspond to ImageNet subproblems such as distinguishing types of fruit or boat. Notice also that some dataset pairs have configuration performance rankings that are negatively correlated. The corresponding error matrix\n\n\n![Figure 5: Memory usage under two training paradigms. Both train a ResNet-18 on CIFAR-10 with batch size 32.](images/2a70afb2bf381683e5dc10e6cdfbcc2a69950b0735c8b1bf9ad622b0a2e2e63c.jpg)\n\n\n$E$  concatenates the performance vector for each dataset. It is not low rank, but its singular values decay rapidly, as shown in Figure 6(b). Hence we expect low rank approximation of this matrix from a few measurements to work well: it is not necessary to measure every configuration.\n\n\nIn the uniform sampling scheme, we sample measurements uniformly at random to obtain a partially observed error matrix  $P_{\\Omega}(E)$ . We then estimate the full error matrix  $E$  using a low rank matrix completion method to form estimate  $\\widehat{E}$ . In this paper, we use SOFTIMPUTE (Mazumder et al., 2010; Hastie et al., 2015) (Algorithm 1, Appendix B). Using the estimated error matrix  $\\widehat{E}$  and computed memory matrix  $M$ , we compute the Pareto frontier for each dataset to understand the (estimated) error-memory tradeoff. Section 4 Figure 10 shows an example.\n\n\nIn the non-uniform sampling scheme, we sample measurements with non-uniform probabilities, and use a weighted variant of SOFTIMPUTE, weighting by the inverse sampling probability, to complete the error matrix (Ma & Chen, 2019). Then we estimate\n\n\n![(a) dataset correlation](images/c05558055a70d7882859e1f1181e4f2652c2ddd1a7a9c80a98fee186323204a7.jpg)\n\n\n![(b) singular value decay Figure 6: Kendall tau correlation of test error of all configurations between all pairs of datasets, and singular value decay of corresponding error matrix. Strong correlations allow PEPP to succeed with a few measurements. Details in Appendix A.](images/c53c7c4f029ae09d435248a221efdf675b8eefc9e6f1432e9ede309d3db7c532.jpg)\n\n\nthe Pareto frontier in the same way as above. Non-uniform sampling is useful to reduce the memory needed for measurements. To this end, we construct a probability matrix  $P$  by entry-wise transformation  $P_{ij} = \\sigma (1 / M_{ij})$ , in which the monotonically increasing  $\\sigma : \\mathbb{R} \\rightarrow [0,1]$  maps inverse memory usage to a sampling probability. In this way, we make more low-memory measurements, and thus reduce the total memory usage.\n\n\n# 3.2 META-TEST\n\n\nHaving estimated the embedding of each configuration, our goal is to quickly compute the error-memory tradeoff on the meta-test dataset, avoiding the exhaustive search of all possible measurements.\n\n\nWe first compute the memory usage of each low-precision configuration on the meta-test dataset. With this information and guided by meta-training, we evaluate only a few (cheap but informative) configurations and predict the rest. Users then finally select the non-dominated configuration with highest allowable memory. An example of the process is shown in Figure 7.\n\n\nPEPPP uses Experiment Design with Matrix Factorization (ED-MF) described below to choose informative configurations. With more time, ED-MF evaluates more configurations, improving our estimates. We may also set a hard cutoff on memory: we do not evaluate low-precision configurations exceeding the memory limit. This setting has been studied in different contexts, and is called active learning or sequential decision making.\n\n\nED-MF picks measurements to minimize the variance of the resulting estimate. Specifically, we factorize the true (or estimated) error matrix  $E \\in \\mathbb{R}^{n \\times d}$  (or  $\\widehat{E}$ ) into its best rank- $k$  approximation, and get dataset embeddings  $X \\in \\mathbb{R}^{n \\times k}$  and configuration embeddings  $Y \\in \\mathbb{R}^{d \\times k}$  as  $E \\approx X^{\\top}Y$  (Fusi et al., 2018; Yang et al., 2019). On a meta-test dataset, we denote the error and memory vectors of the configurations as  $e^{\\mathrm{new}} \\in \\mathbb{R}^d$  and  $m^{\\mathrm{new}} \\in \\mathbb{R}^d$ , respectively. We model the error on the new dataset as  $e^{\\mathrm{new}} = Y^{\\top}x^{\\mathrm{new}} + \\epsilon \\in \\mathbb{R}^d$ , where  $x^{\\mathrm{new}} \\in \\mathbb{R}^k$  is the embedding of the new dataset and  $\\epsilon \\in \\mathbb{R}^d$  accounts for the errors from both measurement and low rank decomposition. We estimate the embedding  $x^{\\mathrm{new}}$  from a few measurements (entries) of  $e^{\\mathrm{new}}$  by least squares. Note that this requires at least  $k$  measurements on the meta-test dataset to make meaningful estimations, in which  $k$  is the rank for matrix factorization.\n\n\nIf  $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$ , the variance of the estimator is  $\\left(\\sum_{j\\in S}y_jy_j^\\top\\right)^{-1}$ , in which  $y_{j}$  is the  $j$ th column of  $Y$ . D-optimal experiment design selects measurements on the new dataset by minimizing (a scalarization of) the variance,\n\n\n![](images/8faf4c5a78abbc7492cb768b16d122c0456f5052d78ef06aa1e94989c80cd019.jpg)\n\n\nminimize  $\\log \\operatorname*{det}\\left(\\sum_{j\\in S}y_jy_j^\\top\\right)^{-1}$  subject to  $|S|\\leq l$ $S\\subseteq [d],$  (1)\n\n\nto find  $S$ , the set of indices of configurations to evaluate. The positive integer  $l$  bounds the number of measurements. Given a memory cap  $m_{\\mathrm{max}}$ , we replace the second constraint above by  $S \\subseteq T$ , in which  $T = \\{j \\in [d] \\mid m_j^{\\mathrm{new}} \\leq m_{\\mathrm{max}}\\}$  is the set of feasible configurations. Since Problem 1 is combinatorially hard, we may either relax it to a convex optimization\n\n\nproblem by allowing decision variables to have non-integer values between 0 and 1, or use a greedy method to incrementally choose measurements (initialized by measurements chosen by the column-pivoted QR decomposition (Gu & Eisenstat, 1996; Golub & Van Loan, 2012)). We compare these two approaches in Appendix C.\n\n\nFigure 7: Meta-test on CIFAR-10. After meta-training on all other datasets in Appendix A Table 2, we use ED-MF to choose six informative measurements (orange squares) with a 275MB memory limit for each measurement on CIFAR-10. Then we estimate test errors of other configurations by ED-MF, and restrict our attention to configurations that we estimate to be non-dominated (red x's). Note some of these are in fact dominated, since we plot true (not estimated) test error! Finally we select the estimated non-dominated configuration with highest allowable memory (blue square).\n\n\nIn Section 4.2, we compare ED-MF with competing techniques and show it works the best to estimate the Pareto frontier and select the final configuration.\n\n\n# 4 EXPERIMENTS AND DISCUSSIONS\n\n\nThe code for PEPPP and experiments is in the anonymous GitHub repository at https://github.com/barterer/lp. We use QPyTorch (Zhang et al., 2019) to simulate low-precision formats on standard hardware. The 87 datasets and 99 low-precision configurations in experiments are listed in Appendix A. The datasets consist of natural and medical images from various domains. Apart from CIFAR-10, the datasets include 20 CIFAR-100 partitions from mutually exclusive subsets, based on the superclass labels. They also include 50 subsets of ImageNet (Deng et al., 2009), which contains over 20,000 classes grouped into multiple major hierarchical categories like fungus and amphibian; each of the 50 datasets come from different hierarchies. Finally, we use 16 datasets from the visual domain benchmark (Wallace & Hariharan, 2020) to increase the diversity of domains.\n\n\nThe 99 low-precision configurations use mixed-precision as described in Section 3.1. Format A (for the activations and weights) uses 5 to 9 bits; Format B (for the optimizer) ranges from 14 to 20 bits. For each, these bits may be split arbitrarily between the exponent and mantissa. Across all configurations, we use ResNet-18, ResNet-34 and VGG (Simonyan & Zisserman, 2014) variants (11, 13, 16, 19) with learning rate 0.001, momentum 0.9, weight decay 0.0005 and batch size 32. Each training uses only 10 epochs as an early stopping strategy (Yao et al., 2007) to prevent overfitting. All the measurements take 35 GPU days on NVIDIA\u00ae GeForce\u00ae RTX 3090.\n\n\nDeveloping a method to select the above hyperparameters at the same time as the low-precision format is an important topic for future research, but not our focus here. In Section 4.3 we demonstrate how PEPP can naturally extend to efficiently select both optimization and low-precision hyperparameters.\n\n\nA number of metrics may be used to evaluate the quality of the obtained Pareto frontier in a multi-objective optimization problem (Wu & Azarm, 2001; Li et al., 2014; Audet et al., 2020): the distance between approximated and true frontiers (convergence), the uniformity of the distances between neighboring Pareto optimal points (uniformity), how well the frontier is covered by the approximated points (spread), etc. In our setting, the memory is accurate and the test error is estimated, so we do not have full control of the uniformity and spread of the 2-dimensional frontiers between test error and memory. As illustrated in Figure 8, we evaluate the quality of our estimated Pareto frontiers by the following two metrics:\n\n\nConvergence (Deb et al., 2002) between the sets of true and estimated Pareto optimal points  $\\mathcal{P}$ ,  $\\widehat{\\mathcal{P}}$  is  $\\frac{1}{|\\widehat{\\mathcal{P}}|} \\sum_{v \\in \\widehat{\\mathcal{P}}} \\mathrm{dist}(v, \\mathcal{P})$ , where the distance between point  $v$  and set  $\\mathcal{P}$  is  $\\mathrm{dist}(v, \\mathcal{P}) = \\min \\{\\|v - w\\| : w \\in \\mathcal{P}\\}$ . This is a surrogate for the distance between Pareto frontiers.\n\n\n![(a) convergence](images/c67b113bc487bd92647d5645c647b4cd6296ee97ad11f3dc882fa67db7fab9ae.jpg)\n\n\n![(b) HyperDiff](images/5e94b37ac20e45690800757e6b30ccee3e03578f362cc5e461902584a1c59ccc.jpg)\n\n\n![Figure 8: Illustration of Pareto frontier metrics. (a) Convergence is the average distance from each estimated Pareto optimal point to its closest true point: average  $(d_{1}, d_{2}, d_{3})$ . (b) HyperDiff is the absolute difference in area of feasible regions given by the true and estimated Pareto optimal points: the shaded area between Pareto frontiers.](images/3ebfaa8e4306bb7a51bb5d7bf7dbcc119f354612bb42fc125bdde32719147b2e.jpg)\n\n\nHypervolume difference (HyperDiff) (Zitzler & Thiele, 1998; Coello & Sierra, 1999; Wu & Azarm, 2001) is the absolute difference between the volumes of solution spaces dominated by the true and estimated Pareto frontiers. This metric improves with better convergence, uniformity, and spread. Its computation requires an upper boundary for each resource.\n\n\nWhen computing these metrics, we normalize the memories by proportionally scaling them to between 0 and 1. To evaluate the matrix completion performance, we use the relative error defined as  $\\|\\widehat{v} - v\\| / \\|v\\|$ , in which  $\\widehat{v}$  is the predicted vector and  $v$  is the true vector.\n\n\n# 4.1 META-TRAINING\n\n\nWe study the effectiveness of uniform and non-uniform sampling schemes. For simplicity, we regard all 87 available datasets as meta-training in this section. In SOFTIMPUte, we use  $\\lambda = 0.1$  (chosen by cross-validation from a logarithmic scale); and we choose a rank 5 approximation, which accounts for  $78\\%$  of the variance in the error matrix (as shown in Figure 6(b)).\n\n\nIn the uniform sampling scheme, we investigate how many samples we need for accurate estimates: we sample the error matrix at a number of different ratios, ranging from  $5\\%$  to  $50\\%$ , and complete the error matrix from each set of sampled measurements. In uniform sampling, the sampling ratio is also the percentage of memory we need to make the measurements in parallel, compared to exhaustive search. Hence we show the relationship between Pareto frontier estimation performance and sampling ratio in Figure 9. We can see the estimates are more accurate at larger sampling ratios, but sampling  $20\\%$  of the entries already suffices for good performance.\n\n\nAs a more intuitive example, Figure 10 shows the error-memory tradeoff on CIFAR-10. Compared to the estimated Pareto optimal points at sampling ratio  $5\\%$ , the ones at  $20\\%$  are closer to the true Pareto frontier (better convergence), lie more evenly (better uniformity) and cover the true Pareto frontier better (better spread), as shown by the respective convergence and HyperDiff values.\n\n\nThe non-uniform sampling scheme has a similar trend and is shown in Appendix E.2.\n\n\n# 4.2 META-LEAVE-ONE-OUT CROSS-VALIDATION (META-LOOCV)\n\n\nNow suppose that we have already collected measurements on the meta-training datasets to form a meta-training error matrix  $E$  (or its low rank approximation  $\\widehat{E}$ ). On the meta-test dataset, PEPP estimates the Pareto frontier by the active learning technique ED-MF. We compare ED-MF with a few other active learning techniques: Random selection with matrix factorization (RANDOM-MF),\n\n\n![(a) relative error](images/bf5ce4196bddea2bcb33a6aef3d9f9551f850f0657c707ae92eb767c1ecf54d8.jpg)\n\n\n![(b) convergence](images/c392a2d0c157d4d7eee8e70fbcfecc08bbc7a1477cf044f598e944a351b05a93.jpg)\n\n\n![(c) HyperDiff](images/07c86db75cf0491e2fbc5b4ca2da6277433f9aba22cd15ee15758bb487dcda77.jpg)\n\n\n![(a) sampling ratio  $5\\%$](images/6149c2b954562b7d9e16df93bb6cee488f27471b7cce98997eb272e9464f9adb.jpg)\n\n\n![(b) sampling ratio  $20\\%$](images/935cb5e34aba90c2fa5e3318d947b2f41232c361c7b16501bad4f88dbd8bc4da.jpg)\n\n\n![Figure 9: Pareto frontier estimation in PEPPP metatraining, with uniform sampling of configurations. The violins show the distribution of the performance on individual datasets, and the error bars (blue) show the range. The red error bars show the standard deviation of the error on CIFAR-10 across 100 random samples of the error matrix. Figure (a) shows the matrix completion error for each dataset; Figure (b) and (c) show the performance of the Pareto frontier estimates. Modest sampling ratios (around 0.1) already yield good performance. Figure 10: Error vs memory on CIFAR-10 with true and estimated Pareto frontiers from uniform sampling in PEPPP meta-training. A  $20\\%$  uniform sample of entries yields a better estimate of the Pareto frontier (convergence 0.03 and HyperDiff 0.02) compared to a  $5\\%$  sample (convergence 0.09 and HyperDiff 0.16).](images/51d898005fece4b8e3c2c6f60d56fa0881786c6819c847c441c2e8e5c1153887.jpg)\n\n\nQR decomposition with column pivoting and matrix factorization (QR-MF) and two Bayesian optimization techniques (BO-MF and BO-FULL), to understand whether the strong assumptions in ED-MF (low rank and Gaussian errors) are a hindrance or a help. An introduction to these techniques can be found in Appendix E.1.\n\n\nWe use rank 3 for matrix factorization:  $Y_{\\cdot ,j} \\in \\mathbb{R}^3$  for each  $j \\in [d]$ . In BO, we tune hyperparameters on a logarithmic scale and choose the RBF kernel with length scale 20, white noise with variance 1, and  $\\xi = 0.01$ . Table 1 shows the meta-LOOCV settings for each acquisition technique. We compare the techniques at a range of number of configurations to measure in each meta-LOOCV split,\n\n\nresembling what practitioners do in hyperparameter tuning: evaluate an informative subset and infer the rest. Setting I is the most basic, Setting IV and VI are the most practical. We only show results of Setting I and IV in the main paper, and defer the rest to Appendix E.\n\n\nIn Setting I, we do meta-LOOCV with the full meta-training error matrix in each split and do not cap the memory for meta-test. This means we evaluate every configuration on the meta-training datasets. We can see from Figure 11(a) and 11(b) that:\n\n- - ED-MF stably outperforms under both metrics, especially with fewer measurements.\n\n- - QR-MF overall matches the performance of ED-MF.\n\n- - BO-MF, BO-FULL and RANDOM-MF underperform ED-MF and QR-MF at lower memory usage, but often match their performance with higher memory.\n\n\nPractitioners may have only collected part of the meta-training performance, and desire or are limited by a memory cap when they do meta-test on the new dataset. In Setting IV, we cap the single-configuration memory usage for meta-test at 816MB, the median memory of all possible measurements across configurations and datasets. Additionally, we uniformly sample  $20\\%$  configurations from the meta-training error matrix in each split. In Figure 11(c) and 11(d), we can see similar trends as Setting I, except that QR-MF is slightly worse.\n\n\nUltimately, users would want to select a configuration that both achieves a small error and takes lower memory than the limit. As shown in Figure 2, PEPPP offers users the predicted Pareto frontier and chooses the non-dominated configuration with the highest allowable memory and hence the lowest error. We compare the acquisition techniques with the \"random high-memory\" baseline that randomly chooses a configuration that takes the highest allowable memory: an approach that follows the \"higher memory, lower error\" intuition. Figure 12 shows an example.\n\n\nTable 1: Meta-LOOCV experiment settings\n\n\n<table><tr><td>meta-training error matrix</td><td colspan=\"2\">memory cap on meta-test?</td></tr><tr><td></td><td>no</td><td>yes</td></tr><tr><td>full</td><td>I</td><td>II</td></tr><tr><td>uniformly sampled</td><td>III</td><td>IV</td></tr><tr><td>non-uniformly sampled</td><td>V</td><td>VI</td></tr></table>\n\n\n![(a) Setting I convergence](images/3ee79fd8a74f8ffaaab0159e33cef2fa4eb0643eb7cf6e3f1278f16e01acb89f.jpg)\n\n\n![(c) Setting IV convergence](images/42693977e879fa3cfff4b6b6cc18e8c5c90ddcf236516152caf01451925860f1.jpg)\n\n\n![(d) Setting IV HyperDiff](images/57fddde2422db373e4984d676196eaf74f026d5c6054a2a6afb4a7bf45651ded.jpg)\n\n\n![](images/ffccd97c3a5b3c40a464fa0dc3c3b36cf02b1c9865136fea6faaca803b37bff8.jpg)\n\n\n![(b) Setting I HyperDiff](images/a7566352a50ab9c0d4de4437675fccb810dd0adc609fdf35607a56002c813165.jpg)\n\n\nOverall, among all approaches and across all memory usage, ED-MF outperforms, especially at a smaller number of measurements. Compared to BO techniques, ED-MF also enjoys the benefit of having less hyperparameters to tune. Although techniques like QR-MF and RANDOM-MF are easier to implement, the additional cost of ED-MF is much smaller than the cost of making measurements: neural network training and testing.\n\n\n# 4.3 TUNING OPTIMIZATION HYPERPARAMETERS\n\n\nThe low-precision networks still underfit after 10 epochs of training. This situation is typical: underfitting due to budget constraints is unfortunately common in deep learning. Luckily, meta-learning the best precision will succeed so long as the validation errors are correctly ordered, even if they all overestimate the error of the corresponding fully trained model. Indeed, our validation errors correlate well with the errors achieved after further training: on CIFAR-\n\n\n10, the Kendall tau correlation between ResNet-18 errors at 10 epochs and 100 epochs is 0.73, shown in Figure 19. The lowest error at 100 epochs is  $9.7\\%$ , only approximately  $2\\%$  higher than SOTA (Bungert et al., 2021).\n\n\n![Figure 11: Pareto frontier estimates in meta-LOOCV Setting I and IV (with a  $20\\%$  meta-training sampling ratio and an 816MB meta-test memory cap). Each error bar is the standard error across datasets. The x axis measures the memory usage relative to exhaustively searching the permissible configurations. ED-MF consistently picks the configurations that give the best PF estimates. RANDOM-MF random highest-memory](images/99d0cc17daed4e499011157ccfefefaa99c0dc2adb0a34cf08ce60d374631ec5.jpg)\n\n\n![BO-MF BO-FULL](images/5f5882e4587b8709b044284e0b637e89cd70dabb2b0fbe108dfff43dcde0008a.jpg)\n\n\n![Figure 12: Relative performance with respect to ED-MF in meta-test Setting IV when making 3 measurements (memory usage  $\\sim 10\\%$ ) on 10 ImageNet partitions. ED-MF outperforms in most cases.](images/3175af61d969fa694d59d06b5d5f9aaa5b3ee2397158d982114d93f3b5a58cc1.jpg)\n\n\nPreviously, we have shown that PEPPP can estimate the error-memory tradeoff and select a promising configuration with other hyperparameters fixed. In practice, users may also want to tune hyperparameters like learning rate to achieve the lowest error. In Appendix E.4, we tune learning rate in addition to precision, and show that the methodology can be used in broader settings of hyperparameter tuning.\n\n\n# 4.4 META-LEARNING ACROSS ARCHITECTURES\n\n\nThe meta-learning in previous sections were conducted on ResNet-18. In Appendix E.5, we show that on 10 ImageNet partitions, PEPP is also capable of estimating the error-memory tradeoff of ResNet-34, VGG-11, VGG-13, VGG-16 and VGG-19 competitively. Moreover, the meta-learning across architectures works better than considering each architecture separately.\n\n\n# 5 CONCLUSION\n\n\nThis paper proposes PEPPP, a meta-learning system to select low-precision configurations that leverages training information from related tasks to efficiently pick the perfect precision given a memory budget. Built on low rank matrix completion with active learning, PEPPP estimates the Pareto frontier between memory usage and model performance to find the best low-precision configuration at each memory level. By reducing the cost of hyperparameter tuning in low-precision training, PEPPP allows practitioners to efficiently train accurate models.\n\n\n# REFERENCES\n\n- Charles Audet, Jean Bigeon, Dominique Cartier, S\u00e9bastien Le Digabel, and Ludovic Salomon. Performance indicators in multiobjective optimization. European journal of operational research, 2020.\n\n- Stephen R Becker, Emmanuel J Cand\u00e8s, and Michael C Grant. Templates for convex cone problems with applications to sparse signal recovery. Mathematical programming computation, 3(3):165, 2011.\n\n- C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer, 2006. ISBN 9780387310732.\n\n- Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n\n- Leon Bungert, Tim Roith, Daniel Tenbrinck, and Martin Burger. A bregman learning framework for sparse neural networks. arXiv preprint arXiv:2105.04319, 2021.\n\n- Zhaowei Cai and Nuno Vasconcelos. Rethinking differentiable search for mixed-precision neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2349-2358, 2020.\n\n- Carlos A Coello Coello and Margarita Reyes Sierra. Multiobjective evolutionary algorithms: classifications, analyses, and new innovations. In Evolutionary Computation. CiteSeer, 1999.\n\n- Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks with low precision multiplications. arXiv preprint arXiv:1412.7024, 2014.\n\n- Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher R Aberger, Kunle Olokotun, and Christopher R\u00e9. High-accuracy low-precision training. arXiv preprint arXiv:1803.03383, 2018.\n\n- Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation, 6(2):182-197, 2002.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255. IEEE, 2009.\n\n- Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.\n\n- Nicolo Fusi, Rishit Sheth, and Melih Elibol. Probabilistic matrix factorization for automated machine learning. In Advances in neural information processing systems, pp. 3348-3357, 2018.\n\n- Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU press, 2012.\n\n- Ming Gu and Stanley C Eisenstat. Efficient algorithms for computing a strong rank-revealing qr factorization. SIAM Journal on Scientific Computing, 17(4):848-869, 1996.\n\n- Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International conference on machine learning, pp. 1737-1746. PMLR, 2015.\n\n- David A Harville. Matrix algebra from a statistician's perspective, 1998.\n\n- Soheil Hashemi, Nicholas Anthony, Hokchhay Tann, R Iris Bahar, and Sherief Reda. Understanding the impact of precision quantization on the accuracy and energy of neural networks. In Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017, pp. 1474-1479. IEEE, 2017.\n\n- Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix completion and low-rank svd via fast alternating least squares. The Journal of Machine Learning Research, 16(1):3367-3402, 2015.\n\n- Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020.\n\n- Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439, 2020.\n\n- Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455-492, 1998.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\n- Hamed F Langroudi, Zachariah Carmichael, David Pastuch, and Dhireesha Kudithipudi. Cheetah: Mixed low-precision hardware & software co-design framework for dnns on the edge. arXiv preprint arXiv:1908.02386, 2019.\n\n- Christiane Lemke, Marcin Budka, and Bogdan Gabrys. Metalearning: a survey of trends and technologies. Artificial intelligence review, 44(1):117-130, 2015.\n\n- Miqing Li, Shengxiang Yang, and Xiaohui Liu. Diversity comparison of Pareto front approximations in many-objective optimization. IEEE Transactions on Cybernetics, 44(12):2568-2584, 2014.\n\n- Wei Ma and George H Chen. Missing not at random in matrix completion: The effectiveness of estimating missingness probabilities under a low nuclear norm assumption. In Advances in Neural Information Processing Systems, volume 32, pp. 14900-14909, 2019.\n\n- Vivek Madan, Mohit Singh, Uthaipon Tantipongpipat, and Weijun Xie. Combinatorial algorithms for optimal design. In Conference on Learning Theory, pp. 2210-2258. PMLR, 2019.\n\n- Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. The Journal of Machine Learning Research, 11:2287-2322, 2010.\n\n- Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.\n\n- Jonas Mo\u010dkus. On Bayesian methods for seeking the extremum. In *Optimization techniques IFIP technical conference*, pp. 400-404. Springer, 1975.\n\n- Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. arXiv preprint arXiv:1907.10597, 2019.\n\n- Jack Sherman and Winifred J Morrison. Adjustment of an inverse matrix corresponding to a change in one element of a given matrix. The Annals of Mathematical Statistics, 21(1):124-127, 1950.\n\n- Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\n- Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczy\u0144ski, Jian Zhang, and Christopher R\u00e9. Low-memory neural network training: A technical report. arXiv preprint arXiv:1904.10631, 2019a.\n\n- Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczy\u0144ski, Jian Zhang, and Christopher R\u00e9. Low-memory neural network training: A technical report, 2019b.\n\n- Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi (Viji) Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. Advances in Neural Information Processing Systems, 33, 2020.\n\n- Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018.\n\n- Bram Wallace and Bharath Hariharan. Extending and analyzing self-supervised learning across domains. In European Conference on Computer Vision, pp. 717-734. Springer, 2020.\n\n- Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt Keutzer. Mixed precision quantization of convnets via differentiable neural architecture search. arXiv preprint arXiv:1812.00090, 2018.\n\n- Jin Wu and Shapour Azarm. Metrics for quality assessment of a multiobjective design optimization solution set. J. Mech. Des., 123(1):18-25, 2001.\n\n- Chengrun Yang, Yuji Akimoto, Dae Won Kim, and Madeleine Udell. Oboe: Collaborative filtering for automl model selection. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1173-1183, 2019.\n\n- Chengrun Yang, Jicong Fan, Ziyang Wu, and Madeleine Udell. Automl pipeline selection: Efficiently navigating the combinatorial space. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1446-1456, 2020.\n\n- Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289-315, 2007.\n\n- Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. The zipml framework for training models with end-to-end low precision: The cans, the cannons, and a little bit of deep learning. arXiv preprint arXiv:1611.05402, 2016.\n\n- Tianyi Zhang, Zhiqiu Lin, Guandao Yang, and Christopher De Sa. QPyTorch: A low-precision arithmetic simulation framework, 2019.\n\n- Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.\n\n- Eckart Zitzler and Lothar Thiele. Multiobjective optimization using evolutionary algorithms\u2014a comparative case study. In International conference on parallel problem solving from nature, pp. 292-301. Springer, 1998.\n\n\nAll the tables in this appendix are shown in the last page.\n\nResponse: Given the comprehensive nature of the paper and the thorough experimental setup, here are my considerations leading to an evaluation:\n\n1. **Methodological Rigor**: The authors present a novel approach to optimizing low-precision training by leveraging meta-learning techniques. They rigorously describe the underlying mathematical foundations, including multi-objective optimization, matrix factorization, and active learning. The theoretical grounding seems sound.\n\n2. **Experimental Validity**: The paper includes a wide range of datasets and configurations, providing ample data to validate the proposed method. The use of various sampling schemes (uniform and non-uniform) helps to ensure robustness and generalizability.\n\n3. **Performance Metrics**: The authors employ multiple metrics to assess the quality of the Pareto frontiers, ensuring a holistic evaluation. The inclusion of both convergence and uniformity metrics provides a balanced view of the method\u2019s performance.\n\n4. **Comparison with Baselines**: The paper compares PEPPP against several baselines, including random selection and Bayesian optimization methods, demonstrating that PEPPP performs favorably across different scenarios.\n\n5. **Scalability and Efficiency**: The method is designed to be scalable and efficient, particularly when dealing with large datasets and configurations. The use of matrix completion techniques allows for the estimation of the Pareto frontier without exhaustively evaluating all configurations.\n\n6. **Practicality and Generalizability**: The approach is presented as practical for resource-constrained practitioners and is demonstrated to work across different architectures (ResNet-18, 34, VGG-11, 13, 16, 19) and datasets (CIFAR-10, CIFAR-100, ImageNet).\n\nConsidering these factors, the paper appears to meet the criteria for a high-quality research contribution. The novelty, depth of analysis, and practical applicability of the proposed method make it a valuable addition to the field of low-precision training.\n\n### Final Decision:\n\\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634692.6922693", "msecs": "692.0", "relativeCreated": "37515.74349403381", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Calibration-then-Calculation: A Variance Reduced Metric Framework\n\n# Abstract\nDeep learning has been widely adopted across various fields, but there has been little focus on evaluating the performance of deep learning pipelines. With the increased use of large datasets and complex models, it has become common to run the training process only once and compare the result to previous benchmarks. However, this procedure can lead to imprecise comparisons due to the variance in neural network evaluation metrics. The metric variance comes from the randomness inherent in the training process of deep learning pipelines. Traditional solutions such as running the training process multiple times are usually not feasible in deep learning due to computational limitations. In this paper, we propose a new metric framework, Calibrated Loss, that addresses this issue by reducing the variance in its vanilla counterpart. As a result, the new metric has a higher accuracy to detect effective modeling improvement. Our approach is supported by theoretical justifications and extensive experimental validations in the context of Deep Click-Through Rate Prediction Models and Image Classification Models.\n\n\n# 1 INTRODUCTION\n\n\nThe progress in machine learning is largely influenced by experimental outcomes, particularly in the era of deep learning. Researchers often evaluate the performance of new methods by comparing them to previous benchmark results, in order to demonstrate their superiority. However, it is well known that the performance of deep learning models can vary greatly, even when using the same pipeline (Picard, 2021), where, in this work, we define the pipeline broadly, which includes but is not limited to the selection of feature sets, model architectures, optimization algorithms, initialization schemes, and hyper-parameters. This variability poses a significant challenge to evaluating deep learning methods because it is non-trivial to determine whether the performance gain is a result of the proposed method or simply due to randomness.\n\n\nIn fact, it has been shown that by selecting a fortunate random initialization seed, one can achieve a model performance that is significantly better than average (Picard, 2021). This difference can be substantial enough to be used as a strong argument for publications in selective venues (Picard, 2021).\n\n\nThis issue is exacerbated in industry where the production model performance is hard to get improvement, while there are hundreds of machine learning engineers working on the same model at the same time. The performance gain of a modeling proposal is usually small and within the metric variance, making it difficult to judge the effectiveness of the modeling proposal.\n\n\nTo address this issue, a common approach is to run the training pipeline multiple times and report the average, standard deviation, minimum, and maximum performance scores (Picard, 2021). However, with the rise of large training data and big models, this approach is not always practical due to limited computational resources (Bouthillier & Varouquaux, 2020).\n\n\nIn this work, we take a different approach to solving this problem by designing a new metric that can evaluate proposed deep learning methods more reliably. This new metric has a smaller variance and achieves more accurate comparisons between different deep learning pipelines. We validate our approach in the context of Deep Click-Through Rate Prediction Models and Image Classification Models.\n\n\nOur main contributions are:\n\n- 1) Formulating the deep learning pipeline evaluation problem and proposing to tackle it by designing new metrics.\n\n- 2) Proposing a new metric framework, Calibrated Loss, which can mitigate the above deep learning pipeline evaluation issue.\n\n- 3) Conducting extensive experiments to demonstrate the effectiveness of the proposed metric, using synthetic dataset, ads click dataset and image classification dataset.\n\n- 4) Providing theoretical guarantees under a particular setting of linear regression that the proposed metric has a smaller variance than its vanilla counterpart.\n\n\n# 2 PRELIMINARIES AND PROBLEM SETTING\n\n\nIn this work, we are examining the standard supervised learning setting, where we assume that the training data and test data are randomly drawn from an unknown distribution in an i.i.d. manner, denoted as  $\\mathcal{D}$ .\n\n\nOur goal is to develop a good pipeline that maps from a training distribution to a possibly random model,  $h \\in \\mathcal{H}$ , that generalizes well during the test time. As we mentioned in Introduction, the pipeline includes the whole procedures of training a model, including the selection of model architectures, optimization algorithms, initialization schemes, and hyper-parameters. Model performance is evaluated by a metric,  $e$ , and thus the expected performance of a model  $h$  during the test time is\n\n\n$$\n\n$$\nR _ {e} (h) = \\mathbb {E} _ {\\mathcal {D}} [ e (h (X), Y) | h ]. \\tag {1}\n$$\n\n$$\n\n\nIn practice,  $R_{e}(h)$  is estimated by finite-sample average on the test dataset  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$ . That is,\n\n\n$$\n\n$$\n\\hat {R} _ {e} (h, \\hat {\\mathcal {D}} _ {\\text {t e s t}}) = \\frac {1}{| \\hat {\\mathcal {D}} _ {\\text {t e s t}} |} \\sum_ {(x, y) \\in \\hat {\\mathcal {D}} _ {\\text {t e s t}}} e (h (x), y). \\tag {2}\n$$\n\n$$\n\n\nIt's worth noting that the expected risk,  $R_{e}(h)$ , is a random variable, as  $h$  is random and it depends on a specific model that is produced by the underlying deep learning pipeline. The output model is random due to data randomness from the sample collection and intrinsic randomness during the training process in the deep learning pipeline, such as data order and randomness from stochastic gradient descent. Therefore, a proper evaluation and comparison of different deep learning pipelines should take into account the distribution of  $R_{e}(h)$  (Bouthillier et al., 2021). It's also important to note that the term \"deep learning pipeline\" in this context is general, as we consider different model configurations (e.g. different model hyperparameters) as different \"deep learning pipelines\", even though they may belong to the same model class.\n\n\nTo compare the performance of different deep learning pipelines, we should compare the distribution of  $R_{e}(h,\\mathcal{D}_{\\mathrm{test}})$ . When comparing two pipelines,  $A$  and  $B$ , we say that pipeline  $A$  is better than pipeline  $B$  with respect to metric  $e$  if and only if the probability that pipeline  $A$  produces a better model (i.e. smaller risk), measured by the metric  $e$ , is greater than 0.5. This is represented by the inequality:\n\n\n$$\n\n$$\nP \\left(R _ {e} \\left(h _ {A}\\right) <   R _ {e} \\left(h _ {B}\\right)\\right) > 0. 5, \\tag {3}\n$$\n\n$$\n\n\nwhere  $h_A$  and  $h_B$  are random variables representing the output models produced by pipeline  $A$  and  $B$  respectively.\n\n\nOur objective is to compare the performance of two pipelines,  $A$  and  $B$ , with respect to the metric  $e$  by running the training pipeline only once. Ideally, to estimate  $P(R_{e}(h_{A}) < R_{e}(h_{B}))$ , we could use the Monte Carlo method, but this requires a huge amount of computational resources which is not realistic. In this work, we propose a different approach: we aim to come up with an alternative metric that has roughly the same mean but a smaller variance, i.e., for a new metric  $e_1$ , we would like to have\n\n\n$$\n\n$$\n\\mathbb {E} \\left[ R _ {e _ {1}} (h) \\right] \\approx \\mathbb {E} \\left[ R _ {e} (h) \\right] \\text {a n d} v a r \\left(R _ {e _ {1}} (h)\\right) <   v a r \\left(R _ {e} (h)\\right),\n$$\n\n$$\n\n\nwhere the randomness is from the pipeline producing  $h$ . As a result, the new metric is able to compare the performance of pipelines  $A$  and  $B$  with limited computational resources more accurately.\n\n\nAssuming that pipeline  $A$  is better than  $B$  with respect to the metric  $e$  (i.e. pipeline  $A$  is more likely to produce a better model than pipeline  $B$  in the ground truth if measured by metric  $e$ ), we say that a metric  $e_1$  is better than  $e_2$  with respect to metric  $e$  if and only if the probability that pipeline  $A$  produces a better model than pipeline  $B$  measured by metric  $e_1$  is greater than the probability measured by metric  $e_2$ . This is represented by the inequality:\n\n\n$$\n\n$$\nP \\left(R _ {e _ {1}} \\left(h _ {A}\\right) <   R _ {e _ {1}} \\left(h _ {B}\\right)\\right) > P \\left(R _ {e _ {2}} \\left(h _ {A}\\right) <   R _ {e _ {2}} \\left(h _ {B}\\right)\\right) \\tag {4}\n$$\n\n$$\n\n\nIn other words, using metric  $e_1$  is more likely to accurately detect that pipeline  $A$  is better than pipeline  $B$ , which aligns with the ground truth. Here, we allow for a general form of the risk function, which may not admit the expectation form; i.e.,  $R_{e_1}(h)$  may not necessarily have the form  $\\mathbb{E}_{\\mathcal{D}}[e_1(h(X),Y)]$ .\n\n\nHere, we assume without loss of generality that pipeline  $A$  is better than  $B$  with respect to the metric  $e$ . We define the accuracy of a metric  $\\bar{e}$  with respect to metric  $e$  and pipeline  $A$  and  $B$  as:\n\n\n$$\n\n$$\n\\operatorname {A c c} (\\bar {e}) \\triangleq P \\left(R _ {\\bar {e}} \\left(h _ {A}\\right) <   R _ {\\bar {e}} \\left(h _ {B}\\right)\\right). \\tag {5}\n$$\n\n$$\n\n\nOur goal is to find a metric  $\\bar{e}$  associated with the risk function  $R_{\\bar{e}}$  that has higher accuracy than the original metric  $e$  for a wide range of pipelines  $A$  and  $B$ . In the next section, we will present a new metric framework, Calibrated Loss. In the context of Deep Click-Through Rate Prediction Models, a special instance of Calibrated Loss, Calibrated Log Loss achieves higher accuracy than the vanilla Log Loss. The intuition is that the bias in the function  $h$  is always volatile and carries on a large amount of randomness. Calibrating the bias will usually not change the comparison between two pipelines but can reduce the randomness. In Section 4, we will present a theoretical analysis that justifies this intuition by showing that our new metric has a smaller variance under the linear regression setting. Through extensive experiments in Section 5, we will show that Calibrated Log Loss achieves higher accuracy than Log Loss for a wide range of pipelines, including those with different batch sizes, number of features, model architectures, regularization weights, model sizes, etc.\n\n\n# 3 CALIBRATED LOSS FRAMEWORK\n\n\nCalibrated Log Loss (Binary Classification) In the field of Deep Click-Through Rate Prediction Models, it is common for models to overfit when trained for more than one epoch (Zhou et al., 2018; Zhang et al., 2022). As a result, models are often only trained for a single epoch in practice (Zhang et al., 2022), leaving it uncertain if the model has been fully optimized. This leads to the volatility of the bias term in the final layer of neural networks, creating additional randomness. To address this issue, we propose the following risk function:\n\n\n$$\n\n$$\nR _ {e _ {1}} (h) = \\min _ {c} \\mathbb {E} _ {\\mathcal {D}} [ Y \\log (h ^ {c} (X)) + (1 - Y) \\log (1 - h ^ {c} (X)) | h ],\n$$\n\n$$\n\n\nwhere  $h^c (X) = (1 + e^{-\\log \\mathrm{it}(h(X)) + c})^{-1}$  and  $\\mathrm{logit}(p) = \\log (p / (1 - p))$ .\n\n\nTo execute the aforementioned procedure on a finite-sample test set, we split the test data  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$  into two parts: a test-validation dataset  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and a remaining test dataset  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ . By using the test-validation dataset  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ , we are able to correct the bias term, and then calculate the Log Loss using  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$  with the bias-adjusted predictions. This is referred to as Calibrated Log Loss. The calculation procedure is outlined in Algorithm 1.\n\n\nMathematically speaking, we define bias-adjusted predictions as:  $q_{i} = h^{c}(x_{i})$  for  $x_{i}$  in the test set where  $c$  is the bias-adjusted term we are optimizing.\n\n\nTo optimize  $c$ , the following optimization program is solved, which is the log loss between bias-adjusted predictions  $q_{i}$  and labels  $y_{i}$ :\n\n\n$$\n\n$$\n\\min  _ {c} \\left\\{- \\sum_ {\\substack {(x, y) \\in \\\\ \\hat {\\mathcal {D}} _ {\\text {val - test}}}} (y \\log \\left(h ^ {c} (x)\\right) + (1 - y) \\log \\left(1 - h ^ {c} (x)\\right)) \\right\\}. \\tag{6}\n$$\n\n$$\n\n\nIt can be easily shown that, after optimization, the bias-adjusted predictions  $q_{i}$  are well-calibrated in  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ , meaning that  $\\sum_{i\\in \\hat{\\mathcal{D}}_{\\mathrm{val - test}}}q_i = \\sum_{i\\in \\hat{\\mathcal{D}}_{\\mathrm{val - test}}}y_i$ .\n\n\nLet  $c^*$  be the minimizer of the optimization problem (6). The final risk and metrics are\n\n\n$$\n\n$$\n\\hat{R}_{e_{1}}(h,\\hat{\\mathcal{D}}_{\\text{test}}) = \\frac{1}{|\\hat{\\mathcal{D}}_{\\text{remaining - test}}|}\\sum_{\\substack{(x,y)\\in \\\\ \\hat{\\mathcal{D}}_{\\text{remaining - test}}}}e_{1}(h(x),y),\n$$\n\n$$\n\n\nand\n\n\n$$\n\n$$\ne _ {1} (h (x), y) = y \\log (h ^ {c ^ {*}} (x)) + (1 - y) \\log (1 - h ^ {c ^ {*}} (x)).\n$$\n\n$$\n\n\nExplanations The optimization problem (6) corrects the bias term of original predictions  $h(x)$  using test-validation dataset  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ . The bias-adjusted predictions  $h^{c^*}(x)$  is guaranteed to be well-calibrated in  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ , hence the name Calibrated Log Loss.\n\n\n# ALGORITHM 1 CALCULATE CALIBRATED LOG LOSS\n\n- 1: Input: Model  $M$ , labeled test data  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$\n\n- 2: Output: Calibrated Log Loss:  $\\dot{R}_{e_1}(h, \\mathcal{D}_{\\mathrm{test}})$ .\n\n- 3: Partition  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$  into  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ .\n\n- 4: Compute model predictions on  $\\hat{D}_{\\mathrm{val - test}}$  and  $\\hat{D}_{\\mathrm{remaining - test}}$ , and obtain the model predictions  $p_i^{\\mathrm{val - test}}$  and  $p_i^{\\mathrm{remaining - test}}$ .\n\n- 5: Solve the optimization problem (6) using  $p_i^{\\mathrm{val - test}}$  and  $y_{i}^{\\mathrm{val - test}}$  and obtain the learned bias term  $c^{*}$ .\n\n- 6: Calculate bias-adjusted predictions  $q_{i}^{\\mathrm{remaining - test}}$  using formula  $q_{i} = h^{c^{*}}(x_{i})$ .\n\n- 7: Calculate the Calibrated Log Loss  $\\hat{R}_{e_1}(h,\\hat{\\mathcal{D}}_{\\mathrm{test}})$  as the Log Loss of  $q_{i}^{\\mathrm{remaining - test}}$  and  $y_{i}^{\\mathrm{remaining - test}}$ .\n\n\nGeneralization to Multiclass Classification Instead of optimizing problem (6) to calculate calibrated predictions, we use \"Temperature Scaling\" method proposed in (Guo et al., 2017a). The rest are exactly the same.\n\n\nGeneralization to Quadratic Loss Calibrated Quadratic Loss is calculated in a similar manner as Calibrated Log Loss, i.e. first perform calibration on  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and calculate bias-adjusted predictions on  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ . Here, we define the Quadratic Loss and Calibrated Quadratic Loss:\n\n\n$$\n\n$$\ne (h (x), y) = (y - h (x)) ^ {2}, \\text {a n d}\n$$\n\n$$\n\n\n$$\n\n$$\ne _ {1} (h (x), y) = (y - h (x) - (\\mathbb {E} _ {\\mathcal {D}} [ Y ] - \\mathbb {E} _ {\\mathcal {D}} [ h (X) | h ]) ^ {2}).\n$$\n\n$$\n\n\n# 4 THEORY ON LINEAR REGRESSION\n\n\nIn this section, we provide theoretical justification that our new metric has a smaller variance than its vanilla counterpart under Linear Regression setting, where the randomness only comes from the data randomness. We choose to provide a theoretical guarantee under Linear Regression due to its simplicity. We empirically verify our method's performance under Logistic Regression and Neural Networks in the next section. Note that in Linear Regression, Quadratic Loss is used instead of Log Loss. As a result, in our theory, we compare the variance of Calibrated Quadratic Loss with vanilla Quadratic Loss.\n\n\nTheorem 4.1. Suppose that the features  $X \\in \\mathbb{R}^d$  and the label  $Y$  are distributed jointly Gaussian. We consider linear regression  $h(x) = \\beta^\\top x + \\alpha$ . Let  $\\hat{\\beta}_n$  be the coefficient learned from the training data with sample size  $n$ . Then, we have\n\n\n$$\n\n$$\n\\left(1 + \\frac {1}{n}\\right) \\mathbb {E} [ e _ {1} (h (X), Y) | \\hat {\\beta} _ {n} ] = \\mathbb {E} [ e (h (X), Y) | \\hat {\\beta} _ {n} ],\n$$\n\n$$\n\n\nwhere the expectation is taken over the randomness over both the training and test samples.\n\n\nLet  $\\hat{\\alpha}_n$  be the learned intercept. Note that the original risk and the calibrated risk are\n\n\n$$\n\n$$\n\\begin{array}{l} R _ {e} (h) = \\mathbb {E} \\left[ e (h (X), Y) | \\hat {\\beta} _ {n}, \\hat {\\alpha} _ {n} \\right], \\text {a n d} \\\\ R _ {e _ {1}} (h) = \\mathbb {E} [ e _ {1} (h (X), Y) | \\hat {\\beta} _ {n}, \\hat {\\alpha} _ {n} ] = \\mathbb {E} [ e _ {1} (h (X), Y) | \\hat {\\beta} _ {n} ]. \\\\ \\end{array}\n$$\n\n$$\n\n\nTherefore, Theorem 4.1 implies that\n\n\n$$\n\n$$\n(1 + 1 / n) \\mathbb {E} \\left[ R _ {e _ {1}} (h) \\right] = \\mathbb {E} \\left[ R _ {e} (h) \\right].\n$$\n\n$$\n\n\nFurthermore, to make  $e$  and  $e_1$  comparable, we should scale  $e_1$  to  $(1 + 1/n)e_1$ . We demonstrate that after scaling,  $(1 + 1/n)R_{e_1}(h)$  has a smaller variance than  $R_e(h)$  in the next corollary. In practice, as  $(1 + 1/n)$  is a constant as long as the training sample size is fixed, we can directly compare two pipelines using  $R_{e_1}(h)$ .\n\n\nCorollary 4.2. Suppose that  $h_1(x)$  and  $h_2(x)$  are two different learned linear functions in different feature sets. Then, we have\n\n\n$$\n\n$$\n\\mathbb {E} \\left[ R _ {e} \\left(h _ {1}\\right) \\right] = \\mathbb {E} \\left[ R _ {e} \\left(h _ {2}\\right) \\right] \\Leftrightarrow \\mathbb {E} \\left[ R _ {e _ {1}} \\left(h _ {1}\\right) \\right] = \\mathbb {E} \\left[ R _ {e _ {1}} \\left(h _ {2}\\right) \\right] \\tag {7}\n$$\n\n$$\n\n\nand\n\n\n$$\n\n$$\n\\operatorname {v a r} \\left(\\left(1 + 1 / n\\right) R _ {e _ {1}} (h)\\right) <   \\operatorname {v a r} \\left(R _ {e} (h)\\right)\n$$\n\n$$\n\n\nfor any  $h$  learned from linear regression.\n\n\nCorollary 4.2 indicates that Calibrated Quadratic Loss has a smaller variance than vanilla Quadratic Loss without changing the mean after appropriate scaling. Note that smaller variance and higher accuracy (Inequality 4) are highly correlated under mild conditions, but smaller variance alone does not guarantee higher accuracy. In the next section, we will empirically demonstrate that the new metric has a smaller variance and achieves higher accuracy. All proofs can be found in Appendix A.\n\n\n# 5 EXPERIMENT RESULTS\n\n\n# 5.1 ESTIMATION OF ACCURACY\n\n\nRecall that accuracy of a metric  $\\bar{e}$  is defined as:\n\n\n$$\n\n$$\n\\operatorname {A c c} (\\bar {e}) \\triangleq P \\left(R _ {\\bar {e}} \\left(h _ {A}\\right) <   R _ {\\bar {e}} \\left(h _ {B}\\right)\\right). \\tag {8}\n$$\n\n$$\n\n\nTo get an estimation of  $\\mathrm{Acc}(\\bar{e})$ , we run pipelines  $A$  and  $B$  for  $m$  times, obtaining models  $h_{A_i}$  and  $h_{B_i}$  for  $i \\in [m]$ .  $\\mathrm{Acc}(\\bar{e})$  can be estimated as:\n\n\n$$\n\n$$\n\\widehat {\\operatorname {A c c}} (\\bar {e}) = \\frac {1}{m ^ {2}} \\sum_ {(i, j)} \\mathbb {1} \\left(\\hat {R} _ {\\bar {e}} \\left(h _ {A _ {i}}, \\hat {\\mathcal {D}} _ {\\text {t e s t}}\\right) <   \\hat {R} _ {\\bar {e}} \\left(h _ {B _ {j}}, \\hat {\\mathcal {D}} _ {\\text {t e s t}}\\right)\\right) \\tag {9}\n$$\n\n$$\n\n\n$\\widehat{\\mathrm{Acc}} (\\bar{e})$  is an unbiased estimator of  $\\mathrm{Acc}(\\bar{e})$ , and in the experiments below, we report  $\\widehat{\\mathrm{Acc}} (\\bar{e})$  as our accuracy metric. In all the tables in this section, without loss of generality, we write the tables as pipeline A is better than pipeline B in the sense of  $P(R_{e}(h_{A}) < R_{e}(h_{B})) > 0.5$ .\n\n\n# 5.2 SYNTHETIC DATA\n\n\nIn Appendix B, we consider a linear regression model to give empirical evidence to support our theory. We further consider logistic regression model to demonstrate the effectiveness of Calibrated Log Loss in synthetic data setting. All the details and results can be found in Appendix B.\n\n\n# 5.3 AVAZU CTR PREDICTION DATASET\n\n\nDataset The Avazu CTR Prediction dataset  ${}^{1}$  is a common benchmark dataset for CTR predictions. Due to computational constraints in our experiments, we use the first 10 million samples, shuffle the dataset randomly, and split the whole dataset into  ${80}\\% {\\widehat{D}}_{\\text{train }},2\\% {\\widehat{D}}_{\\text{val - test }}$  and  ${18}\\% {\\widehat{D}}_{\\text{remaining - test }}$  .\n\n\nBase Model We use the xDeepFM model (Lian et al., 2018) open sourced in Shen (2017) as our base model. We primarily conduct experiments using xDeepFM models, including hyperparameter related experiments and feature related experiments. To demonstrate our new metric can also handle comparisons between different model architectures, we also conduct experiments using DCN (Wang et al., 2017), DeepFM (Guo et al., 2017b), FNN (Zhang et al., 2016), and DCNMix (Wang et al., 2021).\n\n\nExperiment Details We consider neural networks with different architectures, different training methods, different hyper-parameters, and different levels of regularization as different pipelines. Such comparisons represent common practices for research and development in both industry and academia. For each pipeline, we train the model 60 times with different initialization seeds and data orders to calculate  $\\widehat{\\mathrm{Acc}} (\\bar{e})$ . Note that we use \"Log Loss\" as our ground truth metric to determine the performance rank of different pipelines. Due to computational constraints, we cannot afford to run the experiments for multiple rounds. Instead, we run the experiments for one round and report accuracy. Note that in the neural network experiments, we do not re-sample the training data each time, as there is intrinsic randomness in the neural network training process. This is the main difference from the Linear Regression and Logistic Regression experiments.\n\n\nPipelines with Different Number of Features In this set of experiments, for pipeline  $A$ , we use all the features available. For pipeline  $B$ , we remove some informative features. We tested the removal of 6 dense features and 1 sparse features respectively.\n\n\nTable 1: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (features)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>Baseline</td><td>remove dense</td><td>81.8%</td><td>88.8%</td></tr><tr><td>Baseline</td><td>remove sparse</td><td>78.6%</td><td>85.9%</td></tr></table>\n\n\nTable 2: Mean and Standard Deviation of Log Loss and Calibrated Log Loss under Neural Networks (features)\n\n\n<table><tr><td>Pipeline</td><td>Log Loss Mean</td><td>Calibrated Log Loss Mean</td><td>Log Loss Std</td><td>Calibrated Log Loss Std</td></tr><tr><td>dense</td><td>0.37408</td><td>0.37403</td><td>0.00047</td><td>0.00038</td></tr><tr><td>sparse</td><td>0.37404</td><td>0.37398</td><td>0.0005</td><td>0.00042</td></tr></table>\n\n\nFrom the result in Table 1, we can clearly see that Calibrated Log Loss has a higher accuracy, indicating its effectiveness when comparing the performance of pipelines with different features.\n\n\nFrom the result in Table 2, we can see that Calibrated Log Loss has a smaller standard deviation (16% - 19% smaller) while the mean of Log Loss and Calibrated Log Loss is almost on par (within 0.02% difference).\n\n\nPipelines with Different Model Architectures In this set of experiments, for pipeline  $A$ , we use one model architecture. For pipeline  $B$ , we use another model architecture. We tested a variety of different model architectures, including DCN (Wang et al., 2017), DeepFM (Guo et al., 2017b), FNN (Zhang et al., 2016), and DCNMix (Wang et al., 2021).\n\n\nTable 3: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (model architectures)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>DCN</td><td>DCNMix</td><td>64.4%</td><td>71.5%</td></tr><tr><td>DeepFM</td><td>DCN</td><td>77.2%</td><td>83.9%</td></tr><tr><td>DeepFM</td><td>FNN</td><td>76.9%</td><td>79.9%</td></tr><tr><td>FNN</td><td>DCNMix</td><td>61.5%</td><td>72.0%</td></tr><tr><td>DeepFM</td><td>DCNMix</td><td>84.8%</td><td>93.4%</td></tr></table>\n\n\nFrom the result in Table 3, we can clearly see that Calibrated Log Loss has higher accuracy, again indicating its effectiveness when comparing the performance of pipelines with different model architectures. In Table 4, we report the mean and standard deviation of Log Loss and Calibrated Log Loss, consistent with previous results.\n\n\nTable 4: Mean and Standard Deviation of Log Loss and Calibrated Log Loss under Neural Networks (model architectures)\n\n\n<table><tr><td>Pipeline</td><td>Log Loss Mean</td><td>Calibrated Log Loss Mean</td><td>Log Loss Std</td><td>Calibrated Log Loss Std</td></tr><tr><td>DCN</td><td>0.38021</td><td>0.38011</td><td>0.00044</td><td>0.00033</td></tr><tr><td>DeepFM</td><td>0.37971</td><td>0.3796</td><td>0.00059</td><td>0.00037</td></tr><tr><td>FNN</td><td>0.38029</td><td>0.38006</td><td>0.00064</td><td>0.0004</td></tr><tr><td>DCNMix</td><td>0.38046</td><td>0.38037</td><td>0.00048</td><td>0.00034</td></tr></table>\n\n\nPipelines with Different Model Hyperparameters In this set of experiments, we compare pipelines with different model hyperparameters, including neural network layer size, Batch Normalization (BN) (Ioffe & Szegedy, 2015), Dropout (Srivastava et al., 2014), and regularization weight.\n\n\nIn the first experiment, we compare a pipeline using a baseline model size with a pipeline using a smaller model size. In the second experiment, we compare a pipeline using Batch Normalization with a pipeline not using Batch Normalization. In the third experiment, we compare a pipeline not using Dropout with a pipeline using Dropout with dropout probability 0.7. In the fourth experiment, we compare a pipeline not using regularization with a pipeline using L2 regularization with regularization weight  $10^{-6}$ .\n\n\nTable 5: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (hyperparameters)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>Baseline Size</td><td>Smaller Size</td><td>69.6%</td><td>73.6%</td></tr><tr><td>BN</td><td>no BN</td><td>80.2%</td><td>89.7%</td></tr><tr><td>no Dropout</td><td>p = 0.7</td><td>95.0%</td><td>99.3%</td></tr><tr><td>no regularization</td><td>10-6</td><td>95.2%</td><td>98.8%</td></tr></table>\n\n\n![(a) Log Loss Plot Figure 1: Batch Normalization Experiment](images/10c97d6bc5fd44ae18ddeda3b3cc0c850b6042abd2d92567fba791b85eab631f.jpg)\n\n\n![(b) Calibrated Log Loss Plot](images/1419d9a784c7f0b9c8b5b475f4d8c9de29ac93f7238bb94c371aff35acb4c744.jpg)\n\n\nFrom figure 1, we can see that by using Calibrated Log Loss, it becomes easier to separate pipeline using Batch Normalization from pipeline without Batch Normalization.\n\n\nFrom the result in Table 5, we can see that Calibrated Log Loss has a higher accuracy regardless of the hyperparameters we are tuning, indicating its effectiveness when comparing the performance of pipelines with different hyperparameters, which is a very common task in Deep Learning. In Appendix B.3 Table 12, we report the mean and standard deviation of Log Loss and Calibrated Log Loss, again consistent with previous results.\n\n\nPipelines with Different Levels of Regularization In this set of experiments, we take a closer look at one hyperparameter we conduct in the previous section: regularization weight. For pipeline  $A$ , we use the baseline model. For pipeline  $B$ , we use different L2 regularization weights.\n\n\nFrom the result in Table 6, we can see that Calibrated Log Loss has a higher accuracy across all different regularization weights, indicating its robustness to different values of regularization weight. As we increase the regularization weight in pipeline  $B$ , the accuracies of both metrics increase. This is because pipeline  $A$  and  $B$  differ more with larger regularization weight, making performance comparison easier.\n\n\nTable 6: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (regularization weight)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>0</td><td>3*10-7</td><td>63.2%</td><td>69.3%</td></tr><tr><td>0</td><td>5*10-7</td><td>82.2%</td><td>88.2%</td></tr><tr><td>0</td><td>7*10-7</td><td>86.6%</td><td>92.4%</td></tr><tr><td>0</td><td>10-6</td><td>95.2%</td><td>98.8%</td></tr><tr><td>0</td><td>2*10-6</td><td>98.8%</td><td>100.0%</td></tr></table>\n\n\nFrom the result in Appendix B.3 Table 13, we can see that Calibrated Log Loss has a much smaller standard deviation (15% - 40% smaller) than Log Loss while the mean of Log Loss and Calibrated Log Loss is almost on par (within 0.05% difference), again consistent with previous results.\n\n\n# 5.4 CIFAR10\n\n\nDataset We use the default CIFAR10 train dataset as  $\\hat{\\mathcal{D}}_{\\mathrm{train}}$ . We split CIFAR10 test dataset into  $20\\%$ $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and  $80\\%$ $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ .\n\n\nBase Model We use the ResNet18 model (He et al., 2016) as our base model. We compare ResNet18 with other configurations of ResNet and DenseNet (Huang et al., 2017). We used the open source implementations  ${}^{2}$  to conduct all the experiments.\n\n\nExperiment Details Same as Avazu CTR experiments, for each pipeline, we train the model 60 times with different initialization seeds and data orders to calculate  $\\widehat{\\mathrm{Acc}} (\\bar{e})$ . Note that we use \"Classification Accuracy\" as our ground truth metric to determine the performance rank of different pipelines. We train the model for 200 epochs. We run the experiments for one round and report accuracy. Note that in the neural network experiments, we do not re-sample the training data each time, as there is intrinsic randomness in the neural network training process.\n\n\nCalibration Stage Predicting the image class in CIFAR10 is a Multiclass classification task, which is different from the binary classification task in the CTR prediction context. We use the \"Temperature Scaling\" (Guo et al., 2017a) as our calibration method.\n\n\nExperiment Results From the result in Table 7, we can see that Calibrated Log Loss has a higher\n\n\nTable 7: Accuracy of Log Loss and Calibrated Log Loss (CIFAR10)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>resnet18</td><td>resnet101</td><td>5.6%</td><td>74.7%</td></tr><tr><td>resnet18</td><td>resnet152</td><td>8.1%</td><td>87.4%</td></tr><tr><td>resnet18</td><td>DenseNet121</td><td>51.5%</td><td>85.2%</td></tr><tr><td>resnet18</td><td>resnet34</td><td>13.5%</td><td>56.7%</td></tr></table>\n\n\naccuracy, indicating its effectiveness when comparing the performance of pipelines using different image classification models.\n\n\nWe can also see that sometimes the accuracy of Log Loss is below  $50\\%$  (recall that Pipeline A is better than Pipeline B), which indicates that Log Loss and Classification Accuracy will give different results when comparing the performance of pipeline  $A$  and pipeline  $B$  (e.g. pipeline A performs better if using metric Log Loss while pipeline B performs better if using metric Classification Accuracy). This metric inconsistency can be mitigated by Calibrated Log Loss. This phenomenon indicates that some models may have bad calibration (and hence bad Log Loss) while their performances are actually good if measured by accuracy.\n\n\n# 6 RELATED WORK\n\n\nIn recommendation systems, recommending proper items to users is a fundamental task. In order to do accurate recommendations, it is essential to build Click-Through Rate (CTR) prediction models to rank the items and achieve business goals. Prior to Deep Learning Era, traditional machine learning models (Friedman, 2001; Koren et al., 2009; Rendle, 2010; Desrosiers & Karypis, 2010; Canini et al., 2012) like logistic regression, boosted decision trees and factorization machines are commonly used to build CTR models. In the Deep Learning Era, not surprisingly, CTR prediction models have been transitioned to deep models as well (Cheng et al., 2016; Guo et al., 2017b; Covington et al., 2016; Wang et al., 2017; Zhou et al., 2018; Naumov et al., 2019).\n\n\nA typical Deep Click-Through Rate (CTR) prediction model consists of embedding layers and multilayer perceptron (MLP) layers on top of embedding layers. Embedding layers transform raw discrete IDs to low dimensional vectors. Following embedding layers, MLP layers then learn the interactions of different features represented the low dimensional vectors, and finally output the final prediction (i.e. CTR). Currently, Deep Click-Through Rate (CTR) prediction models achieve state-of-the-art performance for CTR tasks, and are commonly used in industries to power various recommendation tasks, including personalized ads recommendations, content recommendations, etc.\n\n\nThere are a number of commonly used metrics (Yi et al., 2013) to evaluate the performance of CTR prediction models. Area Under the ROC Curve (AUC) (Fawcett, 2006; 2004) along with its variants Zhu et al. (2017) and Log Loss are the most common metrics. For example, He et al. (2014); Wang et al. (2017); McMahan et al. (2013) use Log Loss as their core metric, while Zhou et al. (2018); McMahan et al. (2013) use AUC as their core metric. However, AUC has been criticized for not taking into account the predictive probability (Yi et al., 2013). Log Loss, on the other hand, takes predictive probability into account. As a result, in applications where predictions are expected to be calibrated (e.g. Ads Recommendation Systems), Log Loss is preferred.\n\n\n# 7 CONCLUSION AND DISCUSSION\n\n\n# CONCLUSION\n\n\nIn this paper, we have presented a new approach to comparing the performance of different deep learning pipelines. We proposed a new metric framework, Calibrated Loss, which has a higher accuracy and smaller variance than its vanilla counterpart for a wide range of pipelines. Our experiments in section 5 demonstrated the superiority of Calibrated Loss, and we believe this new metric can be used to more effectively and efficiently compare the performance of different pipelines in similar settings. Future work includes expanding this idea to evaluate NLP pipelines, and establish theoretical guarantees under more general settings.\n\n\n# LIMITATIONS\n\n\nOur method sacrifices accuracy when comparing some specific pipelines. For example, if pipeline  $B$  can reliably improve the model calibration in test distribution over pipeline  $A$ , Calibrated Log Loss will not be able to correctly detect the benefits of pipeline  $B$ , while Log Loss is able to. However, for most pipeline comparisons conducted in industry and academia like feature engineering, tuning parameters, etc., Calibrated Log Loss has a huge accuracy boost over Log Loss as we demonstrated in Section 5.\n\n\n# POTENTIAL APPLICATIONS\n\n\nOur method may have applications in AutoML domain. AutoML (Automated Machine Learning) systems are designed to automate the process of selecting, designing, and tuning machine learning models, and a key component of these systems is the selection of the best-performing pipeline (e.g. hyperparameters, model architectures etc.). The new metric can be used as a more accurate way of comparing the performance and selecting the best one. The new metric is in particular useful when performing hyperparameter tuning.\n\n\n# REFERENCES\n\n- Xavier Bouthillier and Gael Varoquaux. Survey of machine-learning experimental methods at NeurIPS2019 and ICLR2020. PhD thesis, Inria Saclay Ile de France, 2020.\n\n- Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, et al. Accounting for variance in machine learning benchmarks. Proceedings of Machine Learning and Systems, 3:747-769, 2021.\n\n- Kevin Canini, Tushar Chandra, Eugene Ie, Jim McFadden, Ken Goldman, Mike Gunter, Jeremiah Harmsen, Kristen LeFevre, Dmitry Lepikhin, Tomas Lloret Llinares, et al. Sibyl: A system for large scale supervised machine learning. Technical Talk, 1:113, 2012.\n\n- Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems, pp. 7-10, 2016.\n\n- Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems, pp. 191-198, 2016.\n\n- Christian Desrosiers and George Karypis. A comprehensive survey of neighborhood-based recommendation methods. Recommender systems handbook, pp. 107-144, 2010.\n\n- Tom Fawcett. Roc graphs: Notes and practical considerations for researchers. Machine learning, 31 (1):1-38, 2004.\n\n- Tom Fawcett. An introduction to roc analysis. Pattern recognition letters, 27(8):861-874, 2006.\n\n- Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pp. 1189-1232, 2001.\n\n- Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pp. 1321-1330. PMLR, 2017a.\n\n- Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-machine based neural network for ctr prediction. arXiv preprint arXiv:1703.04247, 2017b.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising, pp. 1-9, 2014.\n\n- Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708, 2017.\n\n- Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448-456. pmlr, 2015.\n\n- Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30-37, 2009.\n\n- Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 1754-1763, 2018.\n\n- H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1222-1230, 2013.\n\n- Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091, 2019.\n\n- David Picard. Torchmanual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision. CoRR, abs/2109.08203, 2021. URL https://arxiv.org/abs/2109.08203.\n\n- Steffen Rendle. Factorization machines. In 2010 IEEE International conference on data mining, pp. 995-1000. IEEE, 2010.\n\n- Weichen Shen. Deepctr: Easy-to-use, modular and extendible package of deep-learning based ctr models. https://github.com/shenweichen/deepctr, 2017.\n\n- Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.\n\n- Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17, pp. 1-7. 2017.\n\n- Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the Web Conference 2021, pp. 1785-1797, 2021.\n\n- Jeonghee Yi, Ye Chen, Jie Li, Swaraj Sett, and Tak W Yan. Predictive model performance: Offline and online evaluations. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1294-1302, 2013.\n\n- Weinan Zhang, Tianming Du, and Jun Wang. Deep learning over multi-field categorical data. In European conference on information retrieval, pp. 45-57. Springer, 2016.\n\n- Zhao-Yu Zhang, Xiang-Rong Sheng, Yujing Zhang, Biye Jiang, Shuguang Han, Hongbo Deng, and Bo Zheng. Towards understanding the overfitting phenomenon of deep click-through rate prediction models. arXiv preprint arXiv:2209.06053, 2022.\n\n- Guorui Zhou, Xiaogiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1059-1068, 2018.\n\n- Han Zhu, Junqi Jin, Chang Tan, Fei Pan, Yifan Zeng, Han Li, and Kun Gai. Optimized cost per click in taobao display advertising. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 2191-2200, 2017.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634694.0340824", "msecs": "34.0", "relativeCreated": "38857.55658149719", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# An Agnostic Approach to Federated Learning with Class Imbalance\n\n# Abstract\nFederated Learning (FL) has emerged as the tool of choice for training deep models over heterogeneous and decentralized datasets. As a reflection of the experiences from different clients, severe class imbalance issues are observed in real-world FL problems. Moreover, there exists a drastic mismatch between the imbalances from the local and global perspectives, i.e. a local majority class can be the minority of the population. Additionally, the privacy requirement of FL poses an extra challenge, as one should handle class imbalance without identifying the minority class. In this paper we propose a novel agnostic constrained learning formulation to tackle the class imbalance problem in FL, without requiring further information beyond the standard FL objective. A meta algorithm, CLIMB, is designed to solve the target optimization problem, with its convergence property analyzed under certain oracle assumptions. Through an extensive empirical study over various data heterogeneity and class imbalance configurations, we showcase that CLIMB considerably improves the performance in the minority class without compromising the overall accuracy of the classifier, which significantly outperforms previous arts. In fact, we observe the greatest performance boost in the most difficult scenario where every client only holds data from one class.\n\n\n# 1 INTRODUCTION\n\n\nClass imbalance is ubiquitous in real world supervised learning problems. Examples span from medical applications (Lee & Shin, 2020; Roy et al., 2019; Choudhury et al., 2019), fraud detection (Yang et al., 2019; Chan et al., 1999), to consumer based applications (Wang et al., 2021b; Wu et al., 2020; Long et al., 2020). In these scenarios, data belonging to a subset of classes constitute a great proportion of the population while data from the minority classes, generated by uncommon events, are scarce (He & Garcia, 2009). Having a non-uniform number of samples per class deteriorates the performance of the classifier in the minority class (Huang et al., 2016), resulting in low training and testing accuracy. More importantly, unintended consequences of mistreating the minority class can be catastrophic (Van Hulse et al., 2007) if the problem is not handled appropriately.\n\n\nFrom the perspectives of data heterogeneity and privacy, the issue of class imbalance is even more significant in the setting of Federated Learning (FL). Due to the heterogeneity of the local data distributions, there can be a significant mismatch between the local and global imbalance, i.e. the class that is a minority locally can actually be a majority class globally. Moreover, for the purpose of privacy protection in FL, one should tackle the class-imbalance problem in an agnostic manner, i.e. the proposed algorithms should not require the minority class to be identified.\n\n\nIn the centralized training setting, techniques like balanced sampling, loss re-weighting, and gradient tuning have achieved many successes (Johnson & Khoshgoftaar, 2019). However, due to the extra difficulty of handling the class imbalance problem in FL, previous arts that rely on the explicit identification of the minority class do not directly apply. While research along this line is quite limited, a commonly used heuristic is to estimate the portion of a class using the norm of the gradient per class. When used as proxies, these quantities are then utilized to reweight the losses corresponding to different classes (Wang et al., 2021a; Yang et al., 2020). A key drawback of (Wang et al., 2021a) is that a subset of the client's local dataset needs to be shared, which is not ideal for privacy preserving purposes. Moreover, the effectiveness of such approaches degrades when there is a notable mismatch between the local imbalance and the global imbalance (Wang et al., 2021a).\n\n\nIn this work, we target the most difficult yet possibly most interesting setting in FL, where there is a significant mismatch between the local and global imbalance. Concretely, on certain clients, a minority class from a global perspective is in the majority locally. Practitioners often encounters such a mismatch due to the highly heterogeneous data configuration in FL and the principle of locality (Wang et al., 2021a). For example, when the local data are produced by a minority user, the corresponding minority data could occupy a large portion of the local distribution. In this scenario, due to the scarceeness of the minority data from a global perspective, the corresponding underrepresented client will usually experience poor service quality from the trained model, giving rise to potentially severe fairness issue.\n\n\nTo overcome the aforementioned challenges, we propose a constrained learning formulation to handle the class imbalance issue in FL while accounting for both heterogeneity and privacy. In brief, we impose constraints on the standard FL formulation so that the empirical loss on every client should not overly exceed the average empirical loss. Such constraints are shown to force the classifier to account for all classes equally and hence mitigate the detrimental effect of class imbalance, under a type of heterogeneous data configuration that captures the mismatch between the local and global imbalance. The advantages of our formulation are threefold: First, in contrast to previous arts which usually rely on some heuristics to reweight the loss functions for different classes, our approach is principled, yielding a simple optimization interpretation. Second, unlike existing methods, our formulation requires no additional information compared to the original FL formulation and it treats data from different classes agnostically, and hence is less likely to leak the private information of the clients. Third, from our extensive empirical study, our formulation can significantly improve the testing accuracy on the minority class, without compromising the overall performance.\n\n\nContribution. We summarize our contributions as follows.\n\n- - Problem Formulation: To address the challenge of class imbalance in FL with data heterogeneity, we propose a novel constrained FL formulation with explicit enforcement on the similarity between the local empirical losses. Using only information from the standard FL objective, our approach is completely agnostic to class distribution of the client data and its proxies, as opposed to the existing literature (which mostly violate privacy requirements of FL).\n\n- - Meta-Algorithm: We solve our constrained optimization problem via a primal-dual approach. For a fixed dual variable, the corresponding Lagrangian function enjoys a similar structure as the standard FL loss, but with non-uniform weights on the local objectives. Accordingly, we propose an efficient (meta-) algorithm called CLIMB that can use any FL optimization method as a subroutine, with nearly negligible communication overhead. Furthermore, we analyze the convergence properties of CLIMB under certain oracle assumptions.\n\n- - Accuracy Improvement in the minority class: On benchmark datasets, CLIMB with Fed-Avg as base solver achieves significant enhancement of the accuracy in the minority class without compromising the overall accuracy under various data heterogeneity and class imbalance scenarios.\n\n\n# 2 RELATED WORKS\n\n\nClass Imbalance in Centralized Setting. In the centralized learning setting, the number of samples per class is known. We categorize existing solutions that exploit such information as follows.\n\n\nBalanced sampling. In order to balance the data used for gradient calculation, the most common approaches are either to under sample from the majority classes (Liu et al., 2008), or to augment the minority class (Chawla et al., 2002; Guo & Viktor, 2004). Both methods seek to generate an artificial uniform distribution of data (Buda et al., 2018; Pouyanfar et al., 2018).\n\n\nLoss reweighting. Methods in this category focus on re-weighting the loss functions for different classes, with extra emphases on the mistakes made in the minority class (Cui et al., 2019; Ling & Sheng, 2008; Sun et al., 2007; Wang et al., 2016). There also exist works like (Lin et al., 2017) that adjust the scale of the loss sample-wise, without exploiting the global data distribution.\n\n\nGradient tuning. In the context of neural networks, some other works focus on tuning the gradient per class (Anand et al., 1993) showing faster rates of convergence in the class imbalance setting.\n\n\nTo do this, the gradient is reshaped in order to have an equal magnitude when projected to all the gradients per class, while keeping the same norm as the original gradient.\n\n\nClass Imbalance in Federated Learning. Combating with the issue of class imbalance is more challenging in FL since the data composition in such a setting is generally unknown and the minority classes are often difficult to identify Li et al. (2020); Wang et al. (2020b). While research in this direction are quite limited, to the best of our knowledge, we classify them into the following two classes based on whether a proxy of the data composition is explicitly built.\n\n\n(i) A common strategy in existing methods is to explicitly build proxies of the data composition based on some heuristics. These proxies are dynamically adjusted during the learning procedure and allow the aforementioned three techniques, balanced sampling, loss re-weighting and gradient tuning, to be utilized in the FL setting: Wang et al. (2021a) suggests that there exists a proportional relation between the magnitude of the gradients the corresponds to the last layer of the neural network and the sample quantity. Based on such observation, the proposed Ratio-Loss, a class-wise re-weighted version of the standard cross entropy loss. We note that to define the Ratio-Loss, one needs to collect a subset of client data as \"auxiliary data\", which may not be ideal in the setting of FL. On the same vein, Yang et al. (2020) argues that the gradient per class can be used as a proxy to infer the imbalances in the distributions of the clients, and thus clients should be selected according to how uniform the magnitude of gradient per class is. We also notice the work Astraea which introduces extra virtual components called mediators between the FL server and clients (Duan et al., 2019). The mediator is assumed to have access to the local data distributions of the clients so that client rescheduling and data augmentation can be carried out accordingly. Through the prepossessing on the mediator, the gradients communicated to the server are made balanced class-wise. However, when privacy is taken into consideration, methods like Astraea may not be appropriate as in essence it is directly built on the global data distribution.\n\n\n(ii) Without explicitly constructing an estimation of the data composition, there are works that resort to techniques like active learning and reinforcement learning to implicitly learn the data composition as the optimization progresses. Works along this research line usually rely on client selection to mitigate the effects of class imbalances. In (Goetz et al., 2019), to address the class imbalance problem, there will a higher probability to sampling clients that posses the global minority class. Other works, leverage client selection as a multi-armed bandit problem, and design a client selection policy to balance the gradients (Xia et al., 2020). Other arts leverage Q-learning techniques to select clients in order to minimize the overall loss Wang et al. (2020a). However, we believe in the most practical setting of FL, clients that are available per communication round is incoercible and hence these strategies have limited applicability.\n\n\nComparison with Previous Works. Our method significantly differs from the above strategies in the following ways: (1) Our method requires no knowledge of the data composition, nor any proxy of such information. Consequently, our approach is agnostic to the minority class and better preserves clients' privacy. Such a property draws clear distinction between our work and the works listed in class (i) above. (2) As will be more clear in the following section, we only perform clientwise re-weighting as opposed to the class-wise re-weighting schemes used in previous works, which further emphasize the agnostic nature of our approach. (3) In contrast to the methods in class (ii) above, our approach does not require active client selection, enjoying a broader applicability in the most practical and interesting settings.\n\n\n# 3 FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS\n\n\nWe consider the problem of multi-class classification. Let  $x \\in \\mathcal{X} \\subseteq \\mathbb{R}^d$  be the input and use  $y \\in \\mathcal{Y} = \\{1, \\dots, C\\}$  to denote the target label, where  $C$  is the total number of classes. In the context of FL, we assume to have  $N$  clients, each of which possesses its own private data distribution  $p_i(x, y)$ . Here,  $p_i(x, y)$  is a joint probability distribution of the input  $x$  and the output label  $y$ . One can decompose  $p_i(x, y)$  as  $p_i(x, y) = p(x|y)p_i(y)$ , where  $p(x|y)$  is the conditional distribution of the input  $x$  given class  $y$  and  $p_i(y)$  is the marginal distribution of class  $y$  on client  $i$ . We assume that the conditional distribution  $p(x|y)$  is identical on all devices, but the marginal distribution  $p_i(y)$  can vary significantly due to the heterogeneity of the data configuration.\n\n\nLet  $\\mathcal{H} = \\{\\phi : \\mathcal{X} \\times \\Theta \\to \\mathbb{R}^C\\}$  be a family of parameterized predictors with parameter  $\\theta \\in \\Theta \\subseteq \\mathbb{R}^Q$ . Let  $\\ell : \\mathbb{R}^C \\times \\mathcal{Y} \\to \\mathbb{R}_+$  be the loss function, then the local objective function of client  $i$  is defined as\n\n\n$$\n\n$$\nf _ {i} (\\theta) = \\mathbb {E} _ {(x, y) \\sim p _ {i}} [ \\ell (\\phi (x, \\theta), y) ]. \\tag {1}\n$$\n\n$$\n\n\nFor multi-class classification,  $\\ell$  is often chosen to be the cross entropy loss. In the standard formulation of FL, the goal is to minimize the global average of local objectives\n\n\n$$\n\n$$\n\\min  _ {\\theta \\in \\Theta} \\bar {f} (\\theta) := \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (\\theta). \\tag {2}\n$$\n\n$$\n\n\nWhile it is well known that, in the presence of class imbalance, the above vanilla formulation will produce models that perform poorly on the minority data, we consider the following configuration of heterogeneous local class distributions in order to make a quantitative analysis of such a phenomenon for a concrete class imbalance setting. More importantly, such a setting will also motivate our constrained FL formulation. We emphasize that the following setting is just used as a motivating example to showcase our claims, and our results apply generally to any FL setting.\n\n\nMotivating Example. Let  $u$  be the uniform distribution over the classes, i.e. for  $y \\sim u$ ,  $\\operatorname{Pr}(y = c) = \\frac{1}{C}, \\forall c \\in \\mathcal{Y}$  and let  $\\delta_c$  be the Dirac distribution of class  $c$ . We assume for some small but fixed  $\\alpha \\in [0,1]$ , the local class distribution of the client  $i$  is a mixture of the uniform distribution over all classes and the Dirac distribution of a fixed class  $c_i$ , i.e.  $p_i = \\alpha u + (1 - \\alpha)\\delta_{c_i}$ . We use  $N_c$  to denote the number of clients with  $c_i = c$ . Note that in the limit setting when  $\\alpha = 0$ , the aforementioned configuration captures the most heterogeneous setting: clients only have data from a single class.\n\n\nWe consider an extreme case of class imbalance under the above configuration. Without loss of generality, we consider the binary classification problem, i.e.  $C = 2$ , and we assume class 1 to be the minority class with  $N_{1} = 1$ . We define  $g_{i}(\\theta) \\coloneqq \\mathbb{E}_{x \\sim p(x|y = i)}[\\ell(\\phi(x,\\theta),i)]$  as the loss of the predictor  $\\phi(\\cdot,\\theta)$  on the data with  $y = i$ . We can calculate that\n\n\n$$\n\n$$\n\\bar {f} (\\theta) = \\left(\\frac {\\alpha}{2} + \\frac {1 - \\alpha}{N}\\right) g _ {1} (\\theta) + \\left(\\frac {\\alpha}{2} + \\frac {(1 - \\alpha) (N - 1)}{N}\\right) g _ {2} (\\theta). \\tag {3}\n$$\n\n$$\n\n\nClearly, when  $\\alpha$ , the portion of data with uniform label distribution, is small, e.g.  $\\alpha = 0$ , and  $N$ , the total number of clients, is large, the loss of the predictor on class 1 has negligible weight, which often leads to the poor performance on the minority class in the trained classifier as the majority classes dominate the gradient. Moreover, observe that when  $\\alpha$  is small, there is a significant mismatch between the local and global class imbalance: the global minority class 1 is in the majority on the corresponding client locally. Such phenomenon is pertinent to the FL setting due to the data heterogeneity and poses a great challenge for tackling the issue of class imbalance in FL.\n\n\n# 3.1 CONSTRAINED FL FORMULATION\n\n\nIn our work, to address the class imbalance challenge, we propose to minimize the following constrained FL (CFL) formulation\n\n\n$$\n\n$$\nP _ {\\epsilon} ^ {*} = \\min  _ {\\theta \\in \\Theta} \\bar {f} (\\theta) := \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (\\theta) \\tag {CFL}\n$$\n\n$$\n\n\n$$\n\n$$\n\\begin{array}{l} \\text {s . t .} f _ {i} (\\theta) - \\bar {f} (\\theta) \\leq \\epsilon , \\forall i \\in [ 1, \\dots , N ]. \\end{array}\n$$\n\n$$\n\n\nHere,  $\\epsilon$  is a tolerance constant that controls the enforced closeness in the training loss among clients, and we emphasize the dependence of the optimal value  $P_{\\epsilon}^{*}$  on  $\\epsilon$  by encoding it in the subscript. As a motivation, we show that the constraint in our formulation (CFL) can be translated to a constraint on the performance of the minority class, under the above class imbalance setting: Consider the setting where the tolerance constant is very small, i.e.  $\\epsilon$  is close to zero. WLOG, we assume that the first device is the one that has  $c_{i} = 1$ . Recall the definition of  $g_{i}$  above Eq.(3). One can compute that\n\n\n$$\n\n$$\nf _ {1} (\\theta) - \\bar {f} (\\theta) = \\frac {(1 - \\alpha) (N - 1)}{N} \\left(g _ {1} (\\theta) - g _ {2} (\\theta)\\right) \\leq \\epsilon \\Longleftrightarrow g _ {1} (\\theta) - g _ {2} (\\theta) \\leq \\frac {N \\epsilon}{(1 - \\alpha) (N - 1)}.\n$$\n\n$$\n\n\nTherefore, our formulation (CFL) enjoys a clear class-balancing interpretation in the highly heterogeneous setting when  $\\alpha$  is small and  $N$  is large. Note that this is achieved without the identification of the minority class and server collects no additional information compared to the vanilla FL.\n\n\n```\n1: Input: initial model  $\\theta^0$ , a subroutine ClientUpdate, dual step size  $\\eta_{D}$ , maximum round  $T$ ;  \n2: Initialize the dual variables  $\\lambda = [0,\\dots ,0]$ ;  \n3: for  $t = 1,2,\\ldots ,T$  do  \n4: Compute weights:  $\\forall i\\in [N],w_i = 1 + \\lambda_i - \\bar{\\lambda}$  with  $\\bar{\\lambda} = \\frac{1}{N}\\sum_{i = 1}^{N}\\lambda_{i}$ ;  \n5: Primal Update:  $\\theta^{t + 1}\\gets \\text{ClientUpdate} (\\{w_i\\}_{i = 1}^N,\\theta^t)$ ;  \n6: Dual Update:  $\\forall i\\in [N],\\lambda_i\\gets [\\lambda_i + \\eta_D(f_i(\\theta^{t + 1}) - \\bar{f} (\\theta^{t + 1}) - \\epsilon)]_+$ , with  $\\bar{f} = \\frac{1}{N}\\sum_{i = 1}^{N}f_{i}$ ;  \n7: end for  \n8: Output: model  $\\theta^{T + 1}$\n```\n\n\nWhile the above nice interpretation of the proposed constraint may not hold exactly for general class imbalance settings, in spirit, we want the resulting classifier from our algorithm to perform similarly on every class, or in other words to account for the minority class and majority class equally. In the following section, we discuss how to solve the proposed formulation by alternating the primal and dual updates of an equivalent Lagrangian formulation.\n\n\n# 3.2 ALGORITHM CONSTRUCTION\n\n\nIn order to solve problem (CFL), we resort to the method of Lagrange multipliers. By introducing the dual variables  $\\lambda = [\\lambda_1, \\dots, \\lambda_N] \\in \\mathbb{R}_+^N$ , we define the Lagrangian function as,\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) = \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (\\theta) + \\lambda_ {i} \\left(f _ {i} (\\theta) - \\frac {1}{N} \\sum_ {j = 1} ^ {N} f _ {j} (\\theta) - \\epsilon\\right) (4) \\\\ = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\left(1 + \\lambda_ {i} - \\bar {\\lambda}\\right) f _ {i} (\\theta) - \\lambda_ {i} \\epsilon , \\quad \\text {w i t h} \\bar {\\lambda} = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\lambda_ {i}. (5) \\\\ \\end{array}\n$$\n\n$$\n\n\nWith this in hand, we can construct a lower bound of the above constrained optimization problem using the Lagrangian  $\\mathcal{L}(\\theta ,\\lambda)$  as follows:\n\n\n$$\n\n$$\nD _ {\\epsilon} ^ {*} = \\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\min  _ {\\theta \\in \\Theta} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) \\leq \\min  _ {\\theta \\in \\Theta} \\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) = P _ {\\epsilon} ^ {*},\n$$\n\n$$\n\n\nwhere we often refer to  $D_{\\epsilon}^{*}$  as the dual problem. To solve the dual problem, we propose CLIMB, a method described in Algorithm 1, which is discussed in detail as follows.\n\n\nCLIMB proceeds by alternatingly optimizing over the primal variable  $\\theta$  and the dual variable  $\\lambda$ .\n\n\nPrimal Update. For a fixed  $\\lambda$ , the minimization of  $\\mathcal{L}$  with respect to  $\\theta$  is equivalent to a re-weighted version of the standard unconstrained FL objective (2):\n\n\n$$\n\n$$\n\\min  _ {\\theta \\in \\Theta} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) \\Longleftrightarrow \\min  _ {\\theta \\in \\Theta} \\frac {1}{N} \\sum_ {i = 1} ^ {N} (1 + \\lambda_ {i} - \\bar {\\lambda}) f _ {i} (\\theta). \\tag {6}\n$$\n\n$$\n\n\nImportantly, this simple and canonical form allows us to perform the update on  $\\theta$  using any FL solver as the base optimizer. For flexibility, we do not explicitly choose the base optimizer in our algorithm description, but refer to the update on  $\\theta$  as the subroutine ClientUpdate( $\\{w_i\\}_{i=1}^N, \\theta^t) \\to \\theta^{t+1}$ . Such a subroutine takes the non-uniform weights  $\\{w_i\\}_{i=1}^N$  on the local objectives and the current consensus model  $\\theta^t$  as inputs, and returns an updated model  $\\theta^{t+1}$ . Note that every call to ClientUpdate may consists of multiple communication rounds.\n\n\nDual Update. Once we have obtained the new consensus model  $\\theta^{t + 1}$ , we perform dual update on  $\\lambda$  by taking a single dual ascent step of the following equivalent objective (given some fixed  $\\theta$ ),\n\n\n$$\n\n$$\n\\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) \\Longleftrightarrow \\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\lambda_ {i} \\left(f _ {i} (\\theta) - \\frac {1}{N} \\sum_ {j = 1} ^ {N} f _ {j} (\\theta) - \\epsilon\\right). \\tag {7}\n$$\n\n$$\n\n\nTo evaluate the gradient of  $\\mathcal{L}$  with respect to  $\\lambda$ , we need to first broadcast the consensus model  $\\theta^{t + 1}$  and then aggregate the function values  $f(\\theta^{t + 1})$ . Since the broadcast model can be used for the next round of primal update, the only overhead of CLIMB compared to the standard FL solver ClientUpdate, is to transmit the functional value, which is negligible.\n\n\n# 3.3 THEORETICAL GUARANTEES\n\n\nBased on the recent progress in constrained learning (Chamon et al., 2021), we show that under the following mild regularity conditions, the duality gap between the dual problem  $D_{\\epsilon}^{*}$  and the primal problem  $P_{\\epsilon}^{*}$  can be controlled by some quantity that describes the capability of the parametric function class  $\\mathcal{H}$ .\n\n\nAssumption 3.1 The loss function  $\\ell$  in the definition of the local objective (1) is  $L$ -Lipschitz, i.e.  $\\| \\ell (x,\\cdot) - \\ell (z,\\cdot)\\| \\leq L\\| x - z\\|$ , and bounded by  $B$ .\n\n\nAssumption 3.2 The conditional distribution  $p(x|y)$  is non-atomic for all  $y \\in \\mathbb{R}^C$ .\n\n\nWhile usually the local data distribution is discrete, we can always augment it by randomly perturbing the data points with white noise, this is often used as data augmentation in vision tasks.\n\n\nAssumption 3.3 There exists a convex hypothesis class  $\\hat{\\mathcal{H}}$  such that  $\\mathcal{H} \\subseteq \\hat{\\mathcal{H}}$ , and there exists a constant  $\\xi > 0$  such that  $\\forall \\hat{\\phi} \\in \\hat{\\mathcal{H}}$ , there exists  $\\theta \\in \\Theta$  such that  $\\sup_{x \\in \\mathcal{X}} \\| \\hat{\\phi}(x) - \\phi(x, \\theta) \\| \\leq \\xi$ .\n\n\nA simple strategy to construct  $\\hat{\\mathcal{H}}$  is to take the convex hull of  $\\mathcal{H}$ . When  $\\mathcal{H}$  is sufficiently rich,  $\\xi$  can be expected to be small. Notice that this bound can be decreased by increasing the richness of the function class  $\\mathcal{H}$ . All the proofs can be found in the Appendix.\n\n\nTheorem 3.1 (Near Zero Duality Gap) Under Assumptions 3.1, 3.2, 3.3, and the Constrained Federated Learning problem is feasible in  $\\hat{\\mathcal{H}}$  with constraint  $\\epsilon - 2L\\xi$ , the Constrained Federated Learning problem has near zero-duality gap,\n\n\n$$\n\n$$\nP _ {\\epsilon} ^ {*} - D _ {\\epsilon} ^ {*} \\leq (2 | \\boldsymbol {\\lambda} _ {\\epsilon - 2 L \\xi} ^ {*} | _ {1} + 1) L \\xi , \\tag {8}\n$$\n\n$$\n\n\nwhere  $\\lambda_{\\epsilon -2L\\xi}^{*}$  is the optimal dual variable associated with the Constrained Federated Learning problem equation CFL with constraints  $\\epsilon -2L\\xi$  over the space of functions  $\\hat{\\mathcal{H}}$ .\n\n\nTheorem 3.1 establishes an upper bound on the duality gap of the Constrained Federated Learning Problem CFL. The gap depends on the Lipschitz constant of the loss function  $L$ , the richness of the function class  $\\xi$ , and the optimal dual variable of a more restrictive problem. Note that we required the Constrained Federated Learning problem to be feasible for constraint  $\\epsilon - 2L\\xi$ . In the case of the cross-entropy loss, as long as  $\\epsilon - 2L\\xi > 0$ , this can be satisfied by a classifier that assigns a uniform label for every sample, as each individual loss will be equal to each other.\n\n\nSince the minimization of the Lagrangian function  $\\mathcal{L}$  is non-convex, to show the convergence of CLIMB, we need an additional oracle assumption as follows.\n\n\nAssumption 3.4 (Approximate Solution) For every dual variable  $\\pmb{\\lambda} \\in \\mathbb{R}_{+}^{N}$ , and precision  $\\delta > 0$  there exists an oracle approximate solution  $\\theta_{\\lambda}$  such that  $\\mathcal{L}(\\theta_{\\lambda}, \\pmb{\\lambda}) \\leq \\min_{\\theta \\in \\mathbb{R}^Q} \\mathcal{L}(\\theta, \\pmb{\\lambda}) + \\delta$ .\n\n\nTheorem 3.2 (Convergence) Define the dual function  $d(\\lambda) = \\min_{\\theta \\in \\Theta} \\mathcal{L}(\\theta, \\lambda)$ . Under Assumptions 3.1 to 3.4, for a fixed tolerance  $r > 0$ , the iterates generated by Algorithm 1, achieve a neighborhood of the dual problem  $D_{\\epsilon}^{*}$  in at most  $T_r = \\mathcal{O}(1 / r)$  steps, i.e.,\n\n\n$$\n\n$$\nd \\left(\\boldsymbol {\\lambda} ^ {T _ {r}}\\right) \\geq D _ {\\epsilon} ^ {*} - \\delta - \\frac {\\eta_ {D}}{2} B ^ {2} - r, \\tag {9}\n$$\n\n$$\n\n\nDue to space limitation, a detailed discussion of the convergence theorem is deferred to the appendix.\n\n\n# 4 EXPERIMENTS\n\n\nIn this section, we evaluate our formulation (CFL) against the competitors on various FL scenarios at the presence of class imbalance. Our results highlight the benefits of our approach especially when the local data distributions are severely heterogeneous and there is a significant mismatch between local and global imbalance. To ensure a fair comparison, we adopt the same base optimizer, Fed-Avg\n\n\n<table><tr><td>Imbalance ratio</td><td>Dataset</td><td>Level of heterogeneity</td><td>Baseline (Eq.(2))</td><td>CLIMB (this work)</td><td>Ratio-Loss</td><td>Focal-Loss</td></tr><tr><td rowspan=\"12\">\u03c1 = 20</td><td rowspan=\"6\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.0532 (0.6754)</td><td>0.2080 (0.6829)</td><td>0.1140 (0.6727)</td><td>0.0450 (0.6445)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.137 (0.7121)</td><td>0.3230 (0.7121)</td><td>0.1790 (0.7037)</td><td>0.0430 (0.6914)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0 (0.5669)</td><td>0.2810 (0.6031)</td><td>0 (0.5746)</td><td>0 (0.6565)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.1279 (0.7098)</td><td>0.3240 (0.7167)</td><td>0.1790 (0.7054)</td><td>0.0552 (0.6905)</td></tr><tr><td rowspan=\"6\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.6889 (0.9375)</td><td>0.8552 (0.9556)</td><td>0.8472 (0.9544)</td><td>0.6186 (0.9278)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.7925 (0.9540)</td><td>0.8748 (0.9616)</td><td>0.8052 (0.9555)</td><td>0.7784 (0.9479)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.3425 (0.8260)</td><td>0.6987 (0.9158)</td><td>0.4134 (0.8484)</td><td>0.1944 (0.7938)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.4720 (0.8596)</td><td>0.7290 (0.9153)</td><td>0.6717 (0.9063)</td><td>0.4602 (0.8654)</td></tr><tr><td rowspan=\"12\">\u03c1 = 10</td><td rowspan=\"6\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.2058 (0.6841)</td><td>0.3629 (0.7041)</td><td>0.2164 (0.6839)</td><td>0.0414 (0.6543)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.1813 (0.7113)</td><td>0.3743 (0.7312)</td><td>0.2657 (0.7083)</td><td>0.1347 (0.6911)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.0492 (0.5933)</td><td>0.2280 (0.6358)</td><td>0.0315 (0.5825)</td><td>0 (0.5548)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.1064 (0.6380)</td><td>0.2734 (0.6696)</td><td>0.0993 (0.6211)</td><td>0.0298 (0.5982)</td></tr><tr><td rowspan=\"6\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.8473 (0.9534)</td><td>0.9305 (0.9584)</td><td>0.8851 (0.9558)</td><td>0.8469 (0.9470)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.8962 (0.9634)</td><td>0.9239 (0.9657)</td><td>0.8798 (0.9615)</td><td>0.8953 (0.9586)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.6045 (0.8906)</td><td>0.8084 (0.9356)</td><td>0.6981 (0.9119)</td><td>0.4690 (0.8670)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.7272 (0.9190)</td><td>0.8195 (0.9392)</td><td>0.7663 (0.9320)</td><td>0.6961 (0.9099)</td></tr></table>\n\n\nTable 1: The minority class testing accuracy and the overall testing accuracy (the quantity in the parentheses) after sufficiently many communication rounds. If there are multiple minority classes, we report the worst of them. Here  $N$ , the number of devices, is 500. The base FL solver is Fed-Avg with partial-participation: 100 devices participate in every communication round.\n\n\nin the current experiment<sup>1</sup>, for all formulations: the standard FL objective in Eq.(2), the proposed formulation in Eq.(CFL), Ratio-Loss (Wang et al., 2021a), and Focal-Loss (Lin et al., 2017). We emphasize that the novelty of our work lies in the new constrained FL formulation (CFL) and is orthogonal to the research of how the formulation is solved. In the following, we describe the datasets and models used in our experiments where more details are provided in the Appendix A.\n\n\nDatasets Two benchmark datasets are used in our experiments with the default train/test splits, which are MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al., 2009).\n\n\nHeterogeneity. We generate heterogeneity in the local data distributions according to the strategy from (Karimireddy et al., 2020; Hsu et al., 2019): Let  $\\alpha \\in [0,1]$  be some constant that determines\n\n\nthe level of heterogeneity. For a fixed  $\\alpha$ , we divide the dataset among  $N = 100$  (moderate) or  $N = 500$  (massive) clients as follows: for we allocate to each client a portion of  $\\alpha$  i.i.d. data and the remaining portion of  $(1 - \\alpha)$  by sorting according to label.\n\n\nData Imbalance. We simulate the phenomenon of class imbalance by removing data belong to the minority classes: Observe that the datasets included in our experiments both have 10 perfectly balanced classes. For the minority class(es), we retain only  $1 / \\rho$  portion of the corresponding data. Here,  $\\rho \\geq 1$  the ratio between the numbers of data in the majority class and in the minority class and is termed the imbalance ratio. For example, when there are 3 minority classes with  $\\rho = 10$ ,  $90\\%$  of the data belong to classes 0, 1, 2 (without loss of generality) are manually removed. In our experiments, we consider the setting of 1 or 3 minority classes and we take  $\\rho = 5, 10, 20$ .\n\n\nModels We follow the choice of model architectures in (Acar et al., 2020; McMahan et al., 2017). Specifically, we use a 2 hidden layer fully-connected neural network for MNIST, where the numbers of neurons are (128, 128). For CIFAR10, we use a CNN model consisting of 2 convolutional layers with  $645 \\times 5$  filters followed by 2 fully connected layers with 394 and 192 neurons. We note that higher testing accuracy on the included datasets can be obtained by using models with high capacity, but is orthogonal to our research.\n\n\n# 4.1 RESULTS SUMMARY\n\n\nTo evaluate the effectiveness of an approach against the challenge of class imbalance, one needs to take into consideration both the performance on the minority class(es) and the average performance on all the classes. We report both quantities after sufficient communication rounds (5000 rounds for CIFAR10 and 1000 rounds for MNIST) under various experiment settings in Tables 1 and 2. In every cell of these tables, the quantity above denote the minority class testing accuracy and the quantity in the parentheses is the average performance on all the classes. We also considered the case where there are multiple minority classes and we report the worst accuracy among the minority classes. Our approach outperforms previous arts in all cases, often by a large margin.\n\n\nImbalance Ratio and Number of Minority Classes. The imbalance ratio  $\\rho$  and the number of minority classes are two important quantities to measure the difficulty of a class imbalance problem. Under all the included choices, CLIMB consistently beats previous arts in both minority class testing accuracy and average performance on all the classes. Therefore, we conclude that CLIMB is able to boost the performance on the minority class without compromising the performance on the other classes. This is a rare merit when addressing the class imbalance problem. In fact, methods like Ratio-Lossis able to improve the testing accuracy on the minority class, but it also sacrifices the performance on the rest classes, leading to an inferior average testing accuracy.\n\n\nLevel of Heterogeneity. We test the performance of CLIMB under different levels of heterogeneity and observe that CLIMB outperforms the included methods considerably in all settings. It has the biggest advantage over the competitors in the most heterogeneous setting,  $\\alpha = 0$ . There are situations that existing methods completely fail in the minority class with less than  $10\\%$  accuracy, but CLIMB is still able to correctly classify most of the minority data, e.g. see Table  $2 \\rightarrow \\rho = 5 \\rightarrow \\mathrm{MNIST} \\rightarrow 3$  minority classes  $\\rightarrow \\alpha = 0$ .\n\n\nModerate vs. Large Number of Devices An important goal of FL is to exploit the computational resources of the IoT devices. Hence, the scalability to a large number of devices is a critical property of an FL method. We hence test CLIMB on both moderate ( $N = 100$ , see Table 2) and massive devices ( $N = 500$ , see Table 1) settings. To make things more practical, we instantiate the subroutine ClientUpdate using Fed-Avg with the partial-participation scheme in the massive device setting. Specifically, 100 devices participate model training after every Fed-Avg global communication round. We clearly observe the advantage of CLIMB in all of these setups.\n\n\n# CONCLUSION\n\n\nIn this paper we proposed a novel agnostic constrained learning formulation to tackle the problem of class imbalance in the Federated Learning setting. By introducing constraints in the learning procedure we enforced the performance to be similar in all clients, thus accounting for the class imbalances. In terms of privacy protection, our formulation requires no further information than the standard FL objective to be collected in the server and it never estimates the data composition as opposed to all previous approaches. Moreover, compared with previous arts which are usually heuristic based, our approach is principled as it is purely optimization based and can be efficiently solved via the proposed meta-algorithm CLIMB, yielding major practical benefits. Our extensive empirical study showcases the superiority of proposed constrained formulation over previous arts.\n\n\n<table><tr><td>Imbalance ratio</td><td>Dataset</td><td>Level of heterogeneity</td><td>Baseline (Eq.(2))</td><td>CLIMB (this work)</td><td>Ratio-Loss</td><td>Focal-Loss</td></tr><tr><td rowspan=\"16\">\u03c1 = 10</td><td rowspan=\"8\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0229(0.5734)</td><td>0.5575(0.6076)</td><td>0(0.4836)</td><td>0(0.4205)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.2753(0.7143)</td><td>0.5054(0.7246)</td><td>0.2929(0.6951)</td><td>0.2284(0.6860)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.29880.7348</td><td>0.4689(0.7511)</td><td>0.3825(0.7329)</td><td>0.26180.7249</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0402(0.5534)</td><td>0.2756(0.5598)</td><td>0(0.4678)</td><td>0(0.4527)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.1316(0.6189)</td><td>0.3399(0.6637)</td><td>0.0690(0.615)</td><td>0.0408(0.5976)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.2566(0.6659)</td><td>0.3292(0.6983)</td><td>0.1916(0.6504)</td><td>0.1213(0.6346)</td></tr><tr><td rowspan=\"8\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.3092(0.8630)</td><td>0.9175(0.9341)</td><td>0.4650(0.8630)</td><td>0.3078(0.8556)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.8597(0.9586)</td><td>0.9428(0.9675)</td><td>0.9189(0.9648)</td><td>0.8348(0.9529)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.8750(0.9640)</td><td>0.9377(0.9715)</td><td>0.9283(0.9706)</td><td>0.8882(0.9631)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0153(0.7133)</td><td>0.8029(0.9115)</td><td>0.06310.7222</td><td>0.07170.7401</td></tr><tr><td>\u03b1 = 0.1</td><td>0.7263(0.9189)</td><td>0.8828(0.9522)</td><td>0.7870(0.9341)</td><td>0.6674(0.9069)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.7950(0.9352)</td><td>0.8004(0.9416)</td><td>0.7801(0.9364)</td><td>0.7785(0.9274)</td></tr><tr><td rowspan=\"16\">\u03c1 = 5</td><td rowspan=\"8\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.2892(0.6382)</td><td>0.5987(0.6468)</td><td>0.0942(0.5506)</td><td>0.1491(0.5631)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.4101(0.7186)</td><td>0.6075(0.7351)</td><td>0.4011(0.7008)</td><td>0.3558(0.6994)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.5335(0.7536)</td><td>0.6063(0.7556)</td><td>0.5054(0.7427)</td><td>0.4359(0.7380)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0313(0.4742)</td><td>0.3813(0.5639)</td><td>0.0512(0.5108)</td><td>0(0.4514)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.5135(0.6636)</td><td>0.6420(0.6977)</td><td>0.4600(0.6632)</td><td>0.4213(0.6384)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.5251(0.6960)</td><td>0.6328(0.7202)</td><td>0.5751(0.6883)</td><td>0.5311(0.6749)</td></tr><tr><td rowspan=\"8\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.7245(0.8953)</td><td>0.9154(0.9224)</td><td>0.8020(0.9016)</td><td>0.7429(0.8992)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.9378(0.9666)</td><td>0.9582(0.9693)</td><td>0.9286(0.9651)</td><td>0.9408(0.9624)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.9448(0.9711)</td><td>0.9670(0.9734)</td><td>0.9561(0.9733)</td><td>0.9481(0.9686)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0160(0.7722)</td><td>0.8744(0.9137)</td><td>0(0.7370)</td><td>0.0544(0.7826)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.8294(0.9422)</td><td>0.9217(0.9606)</td><td>0.8520(0.9501)</td><td>0.7987(0.9361)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.8557(0.9536)</td><td>0.8818(0.9595)</td><td>0.8730(0.9536)</td><td>0.8321(0.9442)</td></tr></table>\n\n\nTable 2: The minority class testing accuracy and the overall testing accuracy (the quantity in the parentheses) after sufficiently many communication rounds. If there are multiple minority classes, we report the worst of them. Here  $N$ , the number of devices, is 100. The base FL solver is Fed-Avg with full-participation: all devices participate in every communication round.\n\n\n# REFERENCES\n\n- Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Conference on Learning Representations, 2020.\n\n- Rangachari Anand, Kishan G Mehrotra, Chilukuri K Mohan, and Sanjay Ranka. An improved algorithm for neural network classification of imbalanced training sets. IEEE Transactions on Neural Networks, 4(6):962-969, 1993.\n\n- Dimitri Bertsekas. Convex optimization theory. Athena Scientific, 2009.\n\n- Dimitri Bertsekas. Convex optimization algorithms. Athena Scientific, 2015.\n\n- Stephen Boyd and Almir Mutapcic. Subgradient methods.\n\n- Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n\n- Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. Neural Networks, 106:249-259, 2018.\n\n- Luiz Chamon and Alejandro Ribeiro. Probably approximately correct constrained learning. Advances in Neural Information Processing Systems, 33, 2020.\n\n- Luiz FO Chamon, Santiago Paternain, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained learning with non-convex losses. arXiv preprint arXiv:2103.05134, 2021.\n\n- Philip K Chan, Wei Fan, Andreas L Prodromidis, and Salvatore J Stolfo. Distributed data mining in credit card fraud detection. IEEE Intelligent Systems and Their Applications, 14(6):67-74, 1999.\n\n- Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.\n\n- Olivia Choudhury, Yoonyoung Park, Theodoros Salonidis, Aris Gkoulalas-Divanis, Issa Sylla, et al. Predicting adverse drug reactions on distributed health data using federated learning. In AMIA Annual symposium proceedings, volume 2019, pp. 313. American Medical Informatics Association, 2019.\n\n- Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9268-9277, 2019.\n\n- Moming Duan, Duo Liu, Xianzhang Chen, Yujuan Tan, Jinting Ren, Lei Qiao, and Liang Liang. Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications. In 2019 IEEE 37th international conference on computer design (ICCD), pp. 246-254. IEEE, 2019.\n\n- Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, and Anuj Kumar. Active federated learning. arXiv preprint arXiv:1909.12641, 2019.\n\n- Hongyu Guo and Herna L Viktor. Learning from imbalanced data sets with boosting and data generation: the databost-im approach. ACM Sigkdd Explorations Newsletter, 6(1):30-39, 2004.\n\n- Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):1263-1284, 2009.\n\n- Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.\n\n- Chen Huang, Yining Li, Chen Change Loy, and Xiaou Tang. Learning deep representation for imbalanced classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5375-5384, 2016.\n\n- Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal of Big Data, 6(1):1-54, 2019.\n\n- Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.\n\n- Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.\n\n- Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\n- Geun Hyeong Lee and Soo-Yong Shin. Federated learning on clinical benchmark data: Performance assessment. J Med Internet Res, 22(10):e20891, Oct 2020. ISSN 1438-8871. doi: 10.2196/20891. URL http://www.jmir.org/2020/10/e20891/.\n\n- Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data, 2020.\n\n- Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr D\u00f3lar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980-2988, 2017.\n\n- Charles X Ling and Victor S Sheng. Cost-sensitive learning and the class imbalance problem. Encyclopedia of machine learning, 2011:231-235, 2008.\n\n- Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. Exploratory undersampling for class-imbalance learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(2):539-550, 2008.\n\n- Guodong Long, Yue Tan, Jing Jiang, and Chengqi Zhang. Federated Learning for Open Banking, pp. 240-254. Springer International Publishing, Cham, 2020. ISBN 978-3-030-63076-8. doi: 10.1007/978-3-030-63076-8_17. URL https://doi.org/10.1007/978-3-030-63076-8_17.\n\n- Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pp. 1273-1282. PMLR, 20-22 Apr 2017. URL https://proceedings.mlr.press/v54/mcmahan17a.html.\n\n- Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S Kaseb, Kent Gauen, Ryan Dailey, Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-Ching Chen, et al. Dynamic sampling in convolutional neural networks for imbalanced data classification. In 2018 IEEE conference on multimedia information processing and retrieval (MIPR), pp. 112-117. IEEE, 2018.\n\n- Alejandro Ribeiro. Optimal resource allocation in wireless communication and networking. EURASIP Journal on Wireless Communications and Networking, 2012(1):1-19, 2012.\n\n- Abhijit Guha Roy, Shayan Siddiqui, Sebastian P\u00f6lsterl, Nassir Navab, and Christian Wachinger. Brantorrent: A peer-to-peer environment for decentralized federated learning. arXiv preprint arXiv:1905.06731, 2019.\n\n- Yanmin Sun, Mohamed S Kamel, Andrew KC Wong, and Yang Wang. Cost-sensitive boosting for classification of imbalanced data. Pattern recognition, 40(12):3358-3378, 2007.\n\n- Fabio Tardella. A new proof of the lyapunov convexity theorem. SIAM journal on control and optimization, 28(2):478-481, 1990.\n\n- Jason Van Hulse, Taghi M Khoshgoftaar, and Amri Napolitano. Experimental perspectives on learning from imbalanced data. In Proceedings of the 24th international conference on Machine learning, pp. 935-942, 2007.\n\n- Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li. Optimizing federated learning on non-iid data with reinforcement learning. In IEEE INFOCOM 2020-IEEE Conference on Computer Communications, pp. 1698-1707. IEEE, 2020a.\n\n- Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization, 2020b.\n\n- Lixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. Addressing class imbalance in federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 10165-10173, 2021a.\n\n- Shoujin Wang, Wei Liu, Jia Wu, Longbing Cao, Qinxue Meng, and Paul J Kennedy. Training deep neural networks on imbalanced data sets. In 2016 international joint conference on neural networks (IJCNN), pp. 4368-4374. IEEE, 2016.\n\n- Yi Wang, Imane Lahmam Bennani, Xiufeng Liu, Mingyang Sun, and Yao Zhou. Electricity consumer characteristics identification: A federated learning approach. IEEE Transactions on Smart Grid, 12(4):3637-3647, 2021b. doi: 10.1109/TSG.2021.3066577.\n\n- Qiong Wu, Xu Chen, Zhi Zhou, and Junshan Zhang. Fedhome: Cloud-edge based personalized federated learning for in-home health monitoring. IEEE Transactions on Mobile Computing, pp. 1-1, 2020. doi: 10.1109/TMC.2020.3045266.\n\n- Wenchao Xia, Tony QS Quek, Kun Guo, Wanli Wen, Howard H Yang, and Hongbo Zhu. Multiarmed bandit-based client scheduling for federated learning. IEEE Transactions on Wireless Communications, 19(11):7108-7123, 2020.\n\n- Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.\n\n- Miao Yang, Akitanoshou Wong, Hongbin Zhu, Haifeng Wang, and Hua Qian. Federated learning with class imbalance reduction, 2020.\n\n- Wensi Yang, Yuhang Zhang, Kejiang Ye, Li Li, and Cheng-Zhong Xu. Ffd: A federated learning based method for credit card fraud detection. In Keke Chen, Sangeetha Seshadri, and Liang-Jie Zhang (eds.), Big Data - BigData 2019, pp. 18-32, Cham, 2019. Springer International Publishing. ISBN 978-3-030-23551-2.\n\n- Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference on Machine Learning, pp. 7252-7261. PMLR, 2019.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "TextualGradientDescent prompt for update", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "165", "funcName": "_update_prompt", "created": "1768634694.0437963", "msecs": "43.0", "relativeCreated": "38867.27046966553", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "prompt": "Here is the role of the variable you will improve: <ROLE>System prompt for paper acceptance prediction. This prompt sets the context and approach for the AI reviewer.</ROLE>.\n\nThe variable is the text within the following span: <VARIABLE> You are an expert academic reviewer tasked with evaluating research papers. </VARIABLE>\n\nHere is the context and feedback we got for the variable:\n\n<CONTEXT></CONTEXT>\n\nImprove the variable (System prompt for paper acceptance prediction. This prompt sets the context and approach for the AI reviewer.) using the feedback provided in <FEEDBACK> tags.\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.", "message": "TextualGradientDescent prompt for update"}
{"name": "textgrad", "msg": "TextualGradientDescent optimizer response", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "179", "funcName": "step", "created": "1768634706.5006337", "msecs": "500.0", "relativeCreated": "51324.10788536072", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "optimizer.response": "<think>\nOkay, let's tackle this. The user wants me to improve the system prompt for a paper acceptance prediction model. The original variable is \"You are an expert academic reviewer tasked with evaluating research papers.\" \n\nFirst, I need to understand the role of this prompt. It's meant to set the context and approach for the AI reviewer. The current prompt is pretty basic. It tells the AI it's an expert reviewer, but maybe it's too vague. \n\nThe feedback isn't provided here, but the context is empty. Hmm, maybe the user expects me to make improvements based on common issues with such prompts. Typically, system prompts should be more detailed. They should specify the criteria for evaluation, like originality, methodology, significance, clarity, and relevance to the field. \n\nAlso, the original prompt doesn't mention the goal of predicting acceptance. The AI needs to know it's not just evaluating but predicting whether the paper would be accepted. So adding that aspect is important. \n\nMaybe include instructions on how to balance different factors. For example, weighing innovation against feasibility. Also, mention considering the journal's scope and audience. \n\nI should structure the prompt to guide the AI through a structured evaluation process. Perhaps start by defining the role, then outline the criteria, and then the goal. \n\nLet me draft something like: \"You are an expert academic reviewer tasked with evaluating research papers to predict their acceptance by a peer-reviewed journal. Assess the paper based on originality, methodology, significance, clarity, and relevance to the field. Prioritize innovation while ensuring feasibility and alignment with the journal's scope. Provide a clear rationale for your prediction, balancing strengths and weaknesses.\" \n\nWait, does that cover all aspects? Maybe include the importance of adhering to submission guidelines and the target audience. Also, ensure the AI considers the paper's contribution to the field and its potential impact. \n\nI think that's a good start. Let me check if the original prompt was too brief and needed more specifics. Yes, adding the prediction aspect and detailed criteria should make the prompt more effective for the AI to follow.\n</think>\n\n<IMPROVED_VARIABLE>You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.</IMPROVED_VARIABLE>", "message": "TextualGradientDescent optimizer response"}
{"name": "textgrad", "msg": "TextualGradientDescent updated text", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "187", "funcName": "step", "created": "1768634706.5008962", "msecs": "500.0", "relativeCreated": "51324.37038421631", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "parameter.value": "You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.", "message": "TextualGradientDescent updated text"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634710.0830686", "msecs": "83.0", "relativeCreated": "54906.54277801514", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Enforcing fairness in private federated learning via the modified method of differential multipliers\n\n# Abstract\nFederated learning with differential privacy, or private federated learning, provides a strategy to train machine learning models while respecting users' privacy. However, differential privacy can disproportionately degrade the performance of the models on under-represented groups, as these parts of the distribution are difficult to learn in the presence of noise. Existing approaches for enforcing fairness in machine learning models have considered the centralized setting, in which the algorithm has access to the users' data. This paper introduces an algorithm to enforce group fairness in private federated learning, where users' data does not leave their devices. First, the paper extends the modified method of differential multipliers to empirical risk minimization with fairness constraints, thus providing an algorithm to enforce fairness in the central setting. Then, this algorithm is extended to the private federated learning setting. The proposed algorithm, FPFL, is tested on a federated version of the Adult dataset and an \"unfair\" version of the FEMNIST dataset. The experiments on these datasets show how private federated learning accentuates unfairness in the trained models, and how FPFL is able to mitigate such unfairness.\n\n\n# 1 INTRODUCTION\n\n\nMachine learning requires data to build models. This data is often private and resides on users' devices. Federated learning (FL) (McMahan et al., 2017) is a strategy where multiple users (or other entities) collaborate in training a model under the coordination of a central server or service provider. In FL, devices share only data statistics (e.g. gradients of the model computed from their data) with the server, and therefore the users' data never leaves their devices.\n\n\nHowever, these statistics can still leak information about the users (see Shokri et al. (2017); Fredrikson et al. (2015) for practical attacks recovering the users' identity and reconstructing the users' face images). To protect against this leakage, differential privacy (DP) can be provided. Differential privacy offers a mathematical guarantee on the maximal amount of information any attacker can obtain about a user after observing the released model. In practice, often (Bhowmick et al., 2018; McMahan et al., 2018a;b; Truex et al., 2019; Granqvist et al., 2020) this guarantee is achieved by adding a certain amount of noise to the individuals' statistics. In this paper, federated learning with differential privacy will be referred to as private federated learning (PFL).\n\n\nThe models trained with PFL are often neural networks, since they work well in many use cases, and turn out to be resistant to the noise added for DP. These models have been successfully trained with PFL for tasks like next word prediction or speaker verification (McMahan et al., 2017; Granqvist et al., 2020). However, they are prone to perpetuating societal biases existing in the data (Caliskan et al., 2017) or to discriminate against certain groups even when the data is balanced (Buolamwini & Gebru, 2018). Moreover, when the training is differentially private, degradation in the performance of these models disproportionately impacts under-represented groups (Bagdasaryan et al., 2019). More specifically, the accuracy of the model on minority groups is deteriorated to a larger extent than the accuracy for the majority groups.\n\n\nIn the realm of FL, there has been some research studying how to achieve that the performance of the model is similar among devices (Hu et al., 2020; Huang et al., 2020; Li et al., 2019; 2021). However,\n\n\nthis notion of fairness falls short in terms of protecting users from under-represented groups falling foul of disproportionate treatment. For example, Castelnovo et al. (2021, Section 3.6) show that a model that performs well on the majority of the users will have a good score on individual fairness metrics, even if all the users suffering from bad performance belong to the same group. Conversely, there is little work proposing solutions to enforce group fairness, i.e. that the performance is similar among users from different groups. Current work in this area is either limited to reducing demographic parity in logistic regression (Du et al., 2021) or to minimizing the largest loss across the different groups with a minimax optimization (Mohri et al., 2019). Concurrently to this paper, Cui et al. (2021) proposed an approach to enforce group fairness in FL for general models and metrics. However, this approach does not consider DP. Moreover, most prior work does not consider the adverse effects that DP has on the models trained with private federated learning, or is restricted to logistic regression models (Abay et al., 2020).\n\n\nOn the other hand, work studying the trade-offs between privacy and group fairness proposes solutions that are either limited to simple models such as linear logistic regression (Ding et al., 2020), require an oracle (Cummings et al., 2019), scale poorly for large hypothesis classes (Jagielski et al., 2019), or only offer privacy protection for the variable determining the group (Tran et al., 2021; Rodr\u00edguez-G\u00e1lvez et al., 2021). Furthermore, the aforementioned work only considers the central learning paradigm, and the techniques are not directly adaptable to FL.\n\n\nFor all the above reasons, in this paper, we propose an algorithm to train neural networks with PFL while enforcing group fairness. We pose the problem as an optimization problem with fairness constraints and extend the modified method of differential multipliers (MMDM) (Platt & Barr, 1987) to solve such a problem with FL and DP. Hence, the resulting algorithm (i) is applicable to any model that can be learned using stochastic gradient descent (SGD) or any of its variants, (ii) can be tailored to enforce the majority of the group fairness metrics, (iii) can consider any number of attributes determining the groups, and (iv) can consider both classification and regression tasks.\n\n\nThe paper is structured as follows: in Section 2, we review the background on differential privacy, private federated learning, the MMDM algorithm, and group fairness; in Section 3, we present our approach for fair private federated learning; in Section 4, we describe our experimental results; and in Section 5, we conclude with a summary and interpretation of our findings.\n\n\n# 2 BACKGROUND\n\n\nIn this section, we review several aspects on the theory of private federated learning, the MMDM algorithm, and group fairness that are necessary to develop and understand the proposed algorithm.\n\n\n# 2.1 FEDERATED LEARNING\n\n\nThe federated learning setting focuses on learning a model that minimizes the expected value of a loss function  $\\ell$  using a dataset  $d = (z_{1},z_{2},\\ldots ,z_{n})\\in \\mathcal{D}$  of  $n$  samples distributed across  $K$  users. This paper will consider models parametrized by a fixed number of parameters  $\\boldsymbol {w}\\in \\mathbb{R}^{p}$  and differentiable loss functions  $\\ell$ , which includes neural networks.\n\n\nSince the distribution of the data samples  $Z$  is unknown and each user  $k$  is the only owner of their private dataset  $d^{k}$ , the goal of FL is to find the parameters  $\\boldsymbol{w}^{\\star}$  that minimize the loss function across the samples of the users. That is, defining  $L(d^{k},\\boldsymbol {w})\\coloneqq \\sum_{\\boldsymbol {z}\\in d^{k}}\\ell (\\boldsymbol {z},\\boldsymbol {w})$ , to solve\n\n\n$$\n\n$$\n\\boldsymbol {w} ^ {\\star} = \\underset {\\boldsymbol {w} \\in \\mathbb {R} ^ {p}} {\\arg \\min } \\frac {1}{n} \\sum_ {k = 1} ^ {K} L \\left(d ^ {k}, \\boldsymbol {w}\\right). \\tag {1}\n$$\n\n$$\n\n\nThis way, the parameters  $\\boldsymbol{w}$  can be learned with an approximation of SGD. McMahan et al. (2017) suggest that the central server iteratively samples  $m$  users; they compute and send back to the server an approximation of the model's gradient  $\\nabla_{\\boldsymbol{w}}L(d^{k},\\boldsymbol{w})$ ; and finally the server updates the parameters as dictated by gradient descent  $\\boldsymbol{w} \\gets \\boldsymbol{w} - \\frac{1}{n}\\sum_{k}\\eta \\nabla_{\\boldsymbol{w}}L(d^{k},\\boldsymbol{w})$ , where  $\\eta$  is the learning rate. If the users send back the exact gradient the algorithm is known as FederatedSGD. A generalisation of this, FederatedAveraging, involves users running several local epochs of mini-batch SGD and sending up the difference. However, the method in this paper considers an optimization with fairness constraints, requiring the communication of extra information in each iteration, and therefore will extend FederatedSGD.\n\n\n# 2.2 PRIVATE FEDERATED LEARNING\n\n\nPrivate federated learning combines federated learning with differential privacy. Differential privacy (Dwork et al., 2006; 2014) provides a mathematical guarantee by restricting the amount of information that a randomized function  $m$  of a dataset  $d$  leaks about any individual. More precisely, it ensures that the probability that the output of the function  $m$  on the dataset  $d$  belongs to any set of outcomes  $\\mathcal{O}$  is close to the probability that the output of the function  $m$  applied to a neighbouring dataset  $d'$  (i.e. equal to  $d$  but where all samples from any individual are removed or replaced) belongs to said set of outcomes. Formally, a randomized function  $m$  is  $(\\epsilon, \\delta)$ -differentially private if\n\n\n$$\n\n$$\n\\mathbb {P} [ \\boldsymbol {m} (d) \\in \\mathcal {O} ] \\leq e ^ {\\epsilon} \\mathbb {P} [ \\boldsymbol {m} (d ^ {\\prime}) \\in \\mathcal {O} ] + \\delta , \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\epsilon$  and  $\\delta$  are parameters that determine the strength of the guarantee. Two useful properties of DP are that it composes (i.e. the combination of DP functions is DP) and that post-processing does not undo its privacy guarantees (Dwork et al., 2014). These properties ensure that the modifications that we will make to FederatedSGD will keep the algorithm differentially private.\n\n\nTypically, the functions of interest are not differentially private. For this reason, they are privatized with \"privacy mechanisms\", which apply a random transformation to the output of the function on the dataset. A standard mechanism is the Gaussian mechanism, which adds Gaussian noise to the function's output, calibrating the noise variance with the privacy parameters  $(\\epsilon, \\delta)$  and the maximum individual's contribution to the function's output measured by the  $\\ell_2$  norm.\n\n\nIn the context of FederatedSGD, the functions to privatize are the gradient estimates sent to the server. The individual's contribution is the gradient computed on their data. This gradient is \"norm-clipped\" (Abadi et al., 2016), i.e. it is scaled down so its  $\\ell_2$  norm is at most a predetermined value  $C$ . Then, Gaussian noise is added to the sum of contributions, which is sent to the server.\n\n\nApplying the privacy mechanism to a sum of contributions is a form of central DP. An alternative is local DP (Dwork et al., 2014), where the privacy mechanism is applied to the statistics before leaving the device. However, models trained with local DP often suffer from low utility (Granqvist et al., 2020), and are thus not considered in this paper. Central DP can involve trust in the server that updates the model, or a trusted third party separated from the model, which is how DP was originally formulated. This paper instead assumes the noisy sum is computed through secure multi-party computation (Goryczka & Xiong, 2015), such as secure aggregation (Bonawitz et al., 2017). This is a cryptographic method that ensures the server only sees the sum of contributions. A crucial limitation of secure aggregation is that it can only compute sums, so the novel method introduced in Section 3 is restricted to working only on sums of individuals' statistics.\n\n\n# 2.3 THE MODIFIED METHOD OF DIFFERENTIAL MULTIPLIERS\n\n\nUltimately, our objective is to find a parametric model that minimizes a differentiable loss function and respects some fairness constraints. Thus, this subsection reviews a constrained differential optimization algorithm, the modified method of differential multipliers (MMDM) (Platt & Barr, 1987).\n\n\nThis algorithm tries to find a solution to the following constrained optimization problem:\n\n\n$$\n\n$$\n\\boldsymbol {w} ^ {\\star} = \\underset {\\boldsymbol {w} \\in \\mathbb {R} ^ {p}} {\\arg \\min } \\phi (\\boldsymbol {w}) \\quad \\text {s . t .} \\boldsymbol {g} (\\boldsymbol {w}) = \\boldsymbol {0}, \\tag {P1}\n$$\n\n$$\n\n\nwhere  $\\phi : \\mathbb{R}^p \\to \\mathbb{R}$  is the function to minimize,  $\\pmb{g}(\\pmb{w}) = (g_1(\\pmb{w}), g_2(\\pmb{w}), \\dots, g_r(\\pmb{w}))$  is the concatenation of  $r$  constraint functions, and  $\\{\\pmb{w} \\in \\mathbb{R}^p : \\pmb{g}(\\pmb{w}) = 0\\}$  is the solution subspace.\n\n\nThe algorithm consists of solving the following set of differential equations resulting from the Lagrangian of (P1) with an additional quadratic penalty  $cg(w)^2$  using gradient descent/ascent. This results in an algorithm that applies these updates iteratively:\n\n\n$$\n\n$$\n\\left\\{ \\begin{array}{l} \\boldsymbol {\\lambda} \\leftarrow \\boldsymbol {\\lambda} + \\gamma \\boldsymbol {g} (\\boldsymbol {w}) \\\\ \\boldsymbol {w} \\leftarrow \\boldsymbol {w} - \\eta \\left(\\nabla_ {\\boldsymbol {w}} \\phi (\\boldsymbol {w}) + \\boldsymbol {\\lambda} ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w}) + c \\boldsymbol {g} (\\boldsymbol {w}) ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w})\\right) \\end{array} , \\right. \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\lambda \\in \\mathbb{R}^r$  is a Lagrange (or dual) multiplier,  $c\\in \\mathbb{R}_+$  is a damping parameter,  $\\eta$  is the learning rate of the model parameters, and  $\\gamma$  is the learning rate of the Lagrange multiplier.\n\n\nIntuitively, these updates gradually fulfill (P1). Updating the parameters  $\\nabla_{\\boldsymbol{w}}\\phi(\\boldsymbol{w})$  and  $\\nabla_{\\boldsymbol{w}}g(\\boldsymbol{w})$  respectively enforce function minimization and constraint's satisfaction. Then, the multiplier  $\\lambda$  and the multiplicative factor  $cg(\\boldsymbol{w})$  control how strongly the constraints' violations are penalized.\n\n\nA desirable property of MMDM is that for small enough learning rates  $\\eta, \\gamma$  and large enough damping parameter  $c$ , there is a region comprising the vicinity of each constrained minimum such that if the parameters' initialization is in that region and the parameters remain bounded, then the algorithm converges to a constrained minimum (Platt & Barr, 1987). Intuitively, the term  $cg(\\boldsymbol{w}) \\nabla_{\\boldsymbol{w}} g(\\boldsymbol{w})$  enforces a quadratic shape on the optimization search space to the neighbourhood of the solution subspace, and this local behavior is stronger the larger the damping parameter  $c$ . Thus, the algorithm converges to a minimum if the parameters' initialization is in this locally quadratic region.\n\n\n# 2.4 GROUP FAIRNESS\n\n\nIn this subsection, we mathematically formalize what group fairness means. To ease the exposition, we describe this notion in the central setting, i.e. where the data is not distributed across users.\n\n\nConsider a dataset  $d = (z_{1}, z_{2}, \\ldots, z_{n})$  of  $n$  samples. The dataset is partitioned into groups from  $\\mathcal{A}$  such that each sample  $z_{i}$  belongs to group  $a_{i}$ . Group fairness considers how differently a model treats the samples belonging to each group. Many fairness metrics can be written in terms of the similarity of the expected value of a function of interest  $f$  of the model evaluated on the general population  $d$  with that on the population of each group  $d_{a} = \\{z_{i} \\in d : a_{i} = a\\}$  (Agarwal et al., 2018; Fioretto et al., 2020). That is, if we consider a supervised learning problem, where  $z_{i} = (x_{i}, y_{i}, a_{i})$  and where the output of the model is an approximation  $\\hat{\\pmb{y}}_{i}$  of  $\\pmb{y}_{i}$ , we say a model is fair if\n\n\n$$\n\n$$\n\\mathbb {E} [ f (\\boldsymbol {X}, \\boldsymbol {Y}, \\hat {\\boldsymbol {Y}}) \\mid A = a ] = \\mathbb {E} [ f (\\boldsymbol {X}, \\boldsymbol {Y}, \\hat {\\boldsymbol {Y}}) ] \\text {f o r a l l} a \\in \\mathcal {A}. \\tag {4}\n$$\n\n$$\n\n\nMost of the group fairness literature focuses on the case of binary classifiers, i.e.  $\\hat{y}_i \\in \\{0,1\\}$ , and on the binary group case, i.e.  $\\mathcal{A} = \\{0,1\\}$ . However, many of the fairness metrics can be extended to general output spaces  $\\mathcal{V}$  and categorical groups  $\\mathcal{A}$ . It is common that the function  $f$  is the indicator function  $\\mathbb{1}$  of some logical relationship between the random variables, thus turning (4) to an equality between probabilities. As an example, we describe two common fairness metrics that will be used later in the paper. For a comprehensive survey of different fairness metrics and their inter-relationships, please refer to Verma & Rubin (2018); Castelnovo et al. (2021).\n\n\nFalse negative rate (FNR) parity (or equal opportunity) (Hardt et al., 2016) This fairness metric is designed for binary classification and binary groups. It was originally defined as equal true positive rate between the groups, which is equivalent to an equal FNR between each group and the overall population. That is, if we let  $f(\\mathbf{X}, Y, \\hat{Y}) = \\mathbb{E}[\\mathbb{1}(\\hat{Y} = 0) \\mid Y = 1]$  then (4) reduces to\n\n\n$$\n\n$$\n\\mathbb {P} [ \\hat {Y} = 0 \\mid Y = 1, A = a ] = \\mathbb {P} [ \\hat {Y} = 0 \\mid Y = 1 ] \\text {f o r a l l} a \\in \\mathcal {A}. \\tag {5}\n$$\n\n$$\n\n\nThis is usually a good metric when the target variable  $Y$  is something positive such as being granted a loan or hired for a job, since we want to minimize the group disparity of misclassification among the individuals deserving such a loan or such a job (Hardt et al., 2016; Castelnovo et al., 2021).\n\n\nAccuracy parity (or overall misclassification rate) (Zafar et al., 2017) This fairness metric is also designed for binary classification and binary groups. Nonetheless, it applies well to general tasks and categorical groups. In this case, if we let  $f(\\boldsymbol{X}, Y, \\hat{Y}) = \\mathbb{1}(\\hat{Y} = Y)$  then (4) reduces to\n\n\n$$\n\n$$\n\\mathbb {P} [ \\hat {Y} = Y \\mid A = a ] = \\mathbb {P} [ \\hat {Y} = Y ] \\text {f o r a l l} a \\in \\mathcal {A}. \\tag {6}\n$$\n\n$$\n\n\nThis is usually a good metric when there is not a clear positive or negative semantic meaning to the target variable, and also when this variable is not binary.\n\n\n# 3 AN ALGORITHM FOR FAIR AND PRIVATE FEDERATED LEARNING (FPFL)\n\n\nIn this section, we describe the proposed algorithm to enforce fairness in PFL of parametric models learned with SGD. First, we describe an adaptation of the MMDM algorithm to enforce fairness in standard central learning. Then, we extend that algorithm to PFL.\n\n\n# 3.1 ADAPTING THE MMDM ALGORITHM TO ENFORCE FAIRNESS\n\n\nConsider a dataset  $d = (z_{1}, z_{2}, \\ldots, z_{n})$  of  $n$  samples. The dataset is partitioned into groups from  $\\mathcal{A}$  such that each sample  $z_{i}$  belongs to group  $a_{i}$ . Then, let us consider also a supervised learning setting where  $z_{i} = (x_{i}, y_{i}, a_{i})$ , the model's output  $\\hat{y}_{i}$  approximates  $y_{i}$ , and the model is parametrized by the parameters  $\\boldsymbol{w} \\in \\mathbb{R}^{p}$ . Finally, assume we have no information about the variables'  $(X, Y, A)$  distribution apart from the available samples.\n\n\nWe concern ourselves with the task of finding the model's parameters  $\\boldsymbol{w}^{\\star}$  that minimize a loss function  $\\ell$  across the data samples and enforce a measure of fairness on the model. That is, we substitute the expected values of the fairness constraints in (4) with empirical averages:\n\n\n$$\n\n$$\n\\boldsymbol {w} ^ {\\star} = \\underset {\\boldsymbol {w} \\in \\mathbb {R} ^ {p}} {\\arg \\min } \\frac {L (d , \\boldsymbol {w})}{n} \\quad \\text {s . t .} \\left| \\frac {F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}} \\right| \\leq \\alpha , \\text {f o r a l l} a \\in \\mathcal {A}, \\tag {P2}\n$$\n\n$$\n\n\nwhere  $L(d, \\boldsymbol{w}) \\coloneqq \\sum_{\\boldsymbol{z} \\in d} \\ell(\\boldsymbol{z}, \\boldsymbol{w}), F(d', \\boldsymbol{w}) \\coloneqq \\sum_{\\boldsymbol{z} \\in d'} f(\\boldsymbol{z}, \\boldsymbol{w}), d'$  is a subset of  $d$  that varies among different fairness metrics,  $n'$  is the number of samples in  $d'$ ,  $d'_a$  is the set of samples in  $d'$  such that  $a_i = a$ ,  $n'_a$  is the number of samples in  $d'_a$ ,  $f$  is the function employed for the fairness metric definition, and  $\\alpha$  is a tolerance threshold. The subset  $d'$  is chosen based on the function  $f$  used for the fairness metric: if the fairness function does not involve any conditional expectation (e.g. accuracy parity), then  $d' = d$ ; if, on the other hand, the subset involves a conditional expectation, then the  $d'$  is the subset of  $d$  where that condition holds, e.g. when the fairness metric is FNR parity  $d' = \\{(x, y, a) \\in d : y = 1\\}$ . We exclude the case where the conditioning in the expectation is  $\\hat{Y}$ , thus excluding predictive parity as a target fairness metric.\n\n\nNote that the constraints are not strict, meaning that there is a tolerance  $\\alpha$  for how much the function  $f$  can vary between certain groups and the overall population. The reason for this choice is twofold. First, it facilitates the training since the solution subspace is larger. Second, it is known that some fairness metrics, such as FNR parity, are incompatible with DP and non-trivial accuracy. However, if the fairness metric is relaxed, fairness, accuracy, and privacy can coexist (Jagielski et al., 2019; Cummings et al., 2019).\n\n\nThis way, we may re-write (P2) in the form of (P1) to solve the problem with MMDM. To do so we let  $\\phi(\\boldsymbol{w}) = L(d, \\boldsymbol{w}) / n$  and  $\\boldsymbol{g}(\\boldsymbol{w}) = (g_0(\\boldsymbol{w}), g_1(\\boldsymbol{w}), \\ldots, g_{|\\mathcal{A}| - 1}(\\boldsymbol{w}))$ , where\n\n\n$$\n\n$$\ng _ {a} (\\boldsymbol {w}) = \\left\\{ \\begin{array}{l l} h _ {a} (\\boldsymbol {w}) & \\text {i f} h _ {a} (\\boldsymbol {w}) \\geq 0 \\\\ 0 & \\text {o t h e r w i s e} \\end{array} \\right. \\quad \\text {a n d} \\quad h _ {a} (\\boldsymbol {w}) := \\left| \\frac {F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}} \\right| - \\alpha . \\tag {7}\n$$\n\n$$\n\n\nTherefore, the parameters are updated according to\n\n\n$$\n\n$$\n\\left\\{ \\begin{array}{l} \\boldsymbol {\\lambda} \\leftarrow \\boldsymbol {\\lambda} + \\gamma \\boldsymbol {g} (\\boldsymbol {w}) \\\\ \\boldsymbol {w} \\leftarrow \\boldsymbol {w} - \\eta \\left(1 / _ {n} \\nabla_ {\\boldsymbol {w}} L (d, \\boldsymbol {w}) + \\boldsymbol {\\lambda} ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w}) + c \\boldsymbol {g} (\\boldsymbol {w}) ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w})\\right) \\end{array} , \\right. \\tag {8}\n$$\n\n$$\n\n\nwhere we note that  $\\nabla_{\\pmb{w}}\\pmb{g}(\\pmb{w}) = (\\nabla_{\\pmb{w}}g_0(\\pmb{w}),\\nabla_{\\pmb{w}}g_1(\\pmb{w}),\\dots,\\nabla_{\\pmb{w}}g_{|\\mathcal{A}| - 1}(\\pmb{w}))$  and\n\n\n$$\n\n$$\n\\nabla_ {\\boldsymbol {w}} g _ {a} (\\boldsymbol {w}) = \\left\\{ \\begin{array}{l l} \\operatorname {s i g n} \\left(\\frac {F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}}\\right) \\left(\\frac {\\nabla_ {\\boldsymbol {w}} F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {\\nabla_ {\\boldsymbol {w}} F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}}\\right) & \\text {i f} h _ {a} (\\boldsymbol {w}) \\geq 0 \\\\ 0 & \\text {o t h e r w i s e} \\end{array} . \\right. \\tag {9}\n$$\n\n$$\n\n\nNow, the fairness-enforcing problem (P2) can be solved with gradient descent/ ascent or mini-batch stochastic gradient descent/ ascent, where instead of the full dataset  $d$ ,  $d'$ , and  $d_{a}^{\\prime}$ , one considers batches  $b$ ,  $b^{\\prime}$ , and  $b_{a}^{\\prime}$  (or subsets) of that dataset. Moreover, it can be learned with DP adapting the DP-SGD algorithm from (Abadi et al., 2016), where a clipping bound and Gaussian noise is included in both the network parameters' and multipliers' individual updates. Nonetheless, there are a series of caveats of doing so. First, the batch size  $|b^{\\prime}|$  should be large enough to, on average, have enough samples of each group  $a \\in \\mathcal{A}$  so that the difference  $\\left(\\frac{F(b', w)}{|b'|} - \\frac{F(b_{a}', w)}{|b_{a}'|}\\right)$  is well estimated. Second, in many situations of interest such as when we want to enforce FNR parity or accuracy parity, the function  $f$  employed for the fairness metric is not differentiable and thus  $\\nabla_{w} F(d', w)$  does not exist. To solve this issue, we resort to estimate the gradient  $\\nabla_{w} F(d', w)$  using a differentiable estimation of the function aggregate  $F(d', w)$ ; see Appendix A.1 for the details on these estimates in the above situations of interest.\n\n\nWe conclude this section noting the similarities and differences of this work and (Tran et al., 2021). Even though their algorithm is derived from first principles on Lagrangian duality, it is ultimately equivalent to an application of the basic method of differential multipliers (BMDM) to solve a problem equivalent to (P2) when  $\\alpha = 0$ . Nonetheless, the two algorithms differ in three main aspects:\n\n- 1. BMDM vs MMDM: BMDM is equivalent to MMDM when  $c = 0$ , that is, when the effect of making the neighbourhood of the solution subspace quadratic is not present. Moreover, the guarantee of achieving a local minimum that respects the constraints does not hold for  $c = 0$  unless the problem is simple (e.g. quadratic programming).\n\n- 2. How they deal with impossibilities in the goal of achieving perfect fairness together with privacy and accuracy. Tran et al. (2021) include a limit  $\\lambda_{\\mathrm{max}}$  to the Lagrange multiplier to avoid floating point errors and reaching trivial solutions, which in our case is taken care by the tolerance  $\\alpha$ , which, in contrast to  $\\lambda_{\\mathrm{max}}$ , is an interpretable parameter.\n\n- 3. This paper uses the exact expression in (9) for the gradient  $\\nabla_{\\boldsymbol{w}}\\left|\\frac{F(d', \\boldsymbol{w})}{n'} - \\frac{F(d_{a}', \\boldsymbol{w})}{n_{a}'}\\right|$ , while Tran et al. (2021) ignore the sign of the difference and use  $\\left|\\frac{\\nabla_{\\boldsymbol{w}} F(d', \\boldsymbol{w})}{n'} - \\frac{\\nabla_{\\boldsymbol{w}} F(d_{a}', \\boldsymbol{w})}{n_{a}'}\\right|$ .\n\n\nIn the next subsection, we extend the algorithm to PFL, which introduces two new differences with (Tran et al., 2021). Firstly, the privacy guarantees will be provided for the individuals, and not only to the group to which they belong; and secondly, the algorithm will be tailored to FL.\n\n\n# 3.2 EXTENDING THE ALGORITHM TO PRIVATE FEDERATED LEARNING\n\n\nIn the federated learning setting we now consider that the dataset  $d$  is distributed across  $K$  users such that each user maintains a local dataset  $d^{k}$  with  $n^k$  samples. This setting applies to both crossdevice and cross-silo FL, since each sample contains the group information  $a_{i}$ . Nonetheless, as in the central setting, the task is to find the model's parameters  $\\boldsymbol{w}^{\\star}$  that minimize the loss function across the data samples of the users while enforcing a measure of fairness to the model. That is, to solve (P2).\n\n\nTo achieve this goal, we might first combine the ideas from FederatedSGD (McMahan et al., 2017) and the previous section to extend the developed adaptation of MMDM to FL. Our adaptation of MMDM cannot be combined with FederatedAveraging; for more details see Appendix A.2. In order to perform the model updates from (8), the central server requires the following statistics:\n\n\n$$\n\n$$\n\\nabla_ {\\boldsymbol {w}} L (d, \\boldsymbol {w}); F \\left(d ^ {\\prime}, \\boldsymbol {w}\\right); \\left[ F \\left(d _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}; \\nabla_ {\\boldsymbol {w}} F \\left(d ^ {\\prime}, \\boldsymbol {w}\\right); \\left[ \\nabla_ {\\boldsymbol {w}} F \\left(d _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}; n ^ {\\prime}; \\left[ n _ {a} ^ {\\prime} \\right] _ {a \\in \\mathcal {A}}. \\tag {10}\n$$\n\n$$\n\n\nHowever, some of these statistics can be obtained from the others:  $F(d', \\boldsymbol{w}) = \\sum_{a \\in \\mathcal{A}} F(d'_a, \\boldsymbol{w})$ ,  $\\nabla_{\\boldsymbol{w}} F(d', \\boldsymbol{w}) = \\sum_{a \\in \\mathcal{A}} \\nabla_{\\boldsymbol{w}} F(d'_a, \\boldsymbol{w})$ , and  $n' = \\sum_{a \\in \\mathcal{A}} n'_a$ . Moreover, as mentioned in the previous section, one might use a sufficiently large batch  $b$  of the data instead of the full dataset for each update. Therefore, we consider an iterative algorithm where, at each iteration, the central server samples a cohort of  $m$  users  $S$  that report a vector with the sufficient statistics for the update, that is\n\n\n$$\n\n$$\n\\boldsymbol {v} ^ {k} = \\left[ \\nabla_ {\\boldsymbol {w}} L \\left(d ^ {k}, \\boldsymbol {w}\\right), \\left[ F \\left(\\left(d ^ {k}\\right) _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\nabla_ {\\boldsymbol {w}} F \\left(\\left(d ^ {k}\\right) _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\left(n ^ {k}\\right) _ {a} ^ {\\prime} \\right] _ {a \\in \\mathcal {A}} \\right]. \\tag {11}\n$$\n\n$$\n\n\nThis way, if we define the batch  $b$  as the sum of the  $m$  users' local datasets  $b \\coloneqq \\sum_{k \\in S} d^k$  and the batch  $b'$  analogously, then the aggregation of each user's vectors results in\n\n\n$$\n\n$$\n\\boldsymbol {v} = \\sum_ {k \\in S} \\boldsymbol {v} ^ {k} = \\left[ \\nabla_ {\\boldsymbol {w}} L (b, \\boldsymbol {w}), \\left[ F \\left(b _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\nabla_ {\\boldsymbol {w}} F \\left(b _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\left| b _ {a} ^ {\\prime} \\right| \\right] _ {a \\in \\mathcal {A}} \\right], \\tag {12}\n$$\n\n$$\n\n\nwhich contains all the sufficient statistics for the parameters' update.\n\n\nFinally, the resulting algorithm, termed Fair PFL or FPFL and described in Algorithm 1, inspired by the ideas from (McMahan et al., 2018b; Truex et al., 2019; Granqvist et al., 2020; Bonawitz et al., 2017) guarantees the users' privacy by (i) making sure the aggregation of the users' sent vectors is done securely with secure aggregation (Bonawitz et al., 2017), and (ii) clipping each users' vectors with a clipping bound  $C$  and employing the Gaussian mechanism to the sum of the clipped vectors (lines 13 and 15). The variance of the Gaussian noise is calculated according to the refined moments accountant privacy analysis from (Wang et al., 2019), taking into account the number of iterations  $T$ , the cohort size  $m$ , the total number of users (or population size)  $K$ , and the privacy parameters  $\\epsilon$  and  $\\delta$ . Note that the post-processing property of DP keeps the algorithm private even if the received noisy vector  $v$  is processed to extract the relevant information for the model's update.\n\n\nBatch size. This algorithm can also be employed using only a fraction of the user's data in each update, i.e. using a batch  $b^{k}$  of their local dataset  $d^{k}$ . Nonetheless, it is desirable (i) to delegate as much computation to the user as possible and (ii) to use as much users' data as possible to have a good approximation of the performance metrics  $F(d_{a}^{\\prime},w)$ , which are needed to enforce fairness.\n\n\n# 4 EXPERIMENTAL RESULTS\n\n\nWe study the performance of the algorithm in two classification tasks. The first task is a binary classification based on some demographic data from the publicly available Adult dataset (Dua & Graff, 2017). The fairness metric considered for this task is FNR parity. The second task is a multi-class classification where there are three different attributes. This task considers accuracy parity as the fairness metric and uses a modification of the publicly available FEMNIST dataset (Caldas et al., 2018), where only digits are considered and the classes are digits written with a black pen on a white sheet, digits written with a blue pen on a white sheet, and digits written with white chalk on a blackboard (see Figure 1).\n\n\n![Figure 1: Samples from the Unfair FEMNIST dataset. Digits written on a white sheet with black pen (top), on a white sheet with blue pen (middle), and on a blackboard with white chalk (bottom).](images/85dfd2b5c6ccd6c20a5a5b9aa63971fd2f1060733ee6962cf97bee00b226c963.jpg)\n\n\nFor the first task, we first compare the performance of\n\n\nthe MMDM algorithm with BMDM, Tran et al. (2021), and vanilla SGD centrally and without privacy. After that, for both tasks, we confirm how FederatedSGD deteriorates the performance of the model for the under-represented classes when clipping and noise (DP) are introduced. Finally, we demonstrate how FPFL can, under the appropriate circumstances, level the performance of the model across the groups without largely decreasing the overall performance of the model.\n\n\nIn all our experiments, the fairness metrics are defined as the maximum difference of the value of a performance measure between the general testing data and each of the groups described by that sensitive attribute. Moreover, the privacy parameters are  $(\\epsilon, \\delta) = (2, 1 / K)$ , where  $K$  is the population size. All experimental details and additional experiments are in Appendix B.\n\n\n# 4.1 RESULTS ON THE ADULT DATASET\n\n\nWe start our experiments comparing the performance and fairness in the central, non-federated, non-private setting. We trained a shallow network (consisting of one hidden layer with 10 hidden units and a ReLU activation function) with these algorithms, where we tried to enforce FNR parity with a tolerance  $\\alpha = 0.02$ . For MMDM, the damping parameter is  $c = 2$ . For Tran et al. (2021), the multiplier threshold is  $\\lambda_{\\mathrm{max}} = 0.05$ . The results after 1,000 iterations are displayed in Table 1, which also shows the gap in other common measures of fairness such as the equalized odds, the demographic parity, or the predictive parity, see e.g. Castelnovo et al. (2021).\n\n\nCompared to Federated SGD, the MMDM algorithm reduces the FNR gap from  $7\\%$  to  $0.5\\%$ , while hardly reducing the accuracy of the model. Similarly, the gap in the equalized odds (EO), which is a stronger fairness notion than the FNR parity, also decreases from around  $7\\%$  to  $3\\%$ . Moreover, the demographic parity (DemP) gap, which considers the probability of predicting one or the other target class, also improves. In terms of predictive parity (PP), which uses the precision as the performance function, the MMDM algorithm did not improve the parity among groups. The BMDM algorithm, which lacks a quadratic term of the fairness function, performs even slightly better in the central scenario. However, the algorithm from Tran et al. (2021), an approximation to BMDM as discussed in Section 3.1, gives only a small improvement. Therefore, we continue our analysis to the federated and private settings only with the BMDM and MMDM algorithms.\n\n\nThe second experiment is to study how federated learning, clipping, and DP decrease the performance for the under-represented groups, thus increasing the fairness gap on the different fairness metrics. For that, we trained the same network with FederatedSGD and versions of this algorithm where only clipping was performed and where both clipping and DP were included. They were trained with a cohort size of  $m = 200$  for  $T = 1,000$  iterations and the model with best training cohort accuracy was selected. The privacy parameters are  $(\\epsilon, \\delta) = (2,5 \\cdot 10^{-5})$ . The results\n\n\nTable 1: Performance in the central (i.e. non-federated) setting on the Adult dataset. The fairness metric optimized is FNR parity. The tolerance for BMDM and MMDM is  $\\alpha = 0.02$ .\n\n\n<table><tr><td>Algorithm</td><td>Accuracy</td><td>FNR gap</td><td>EO gap</td><td>DemP gap</td><td>PP gap</td></tr><tr><td>SGD</td><td>0.857</td><td>0.071</td><td>0.071</td><td>0.113</td><td>0.016</td></tr><tr><td>Tran et al. (2021) without privacy</td><td>0.856</td><td>0.048</td><td>0.048</td><td>0.108</td><td>0.036</td></tr><tr><td>BMDM</td><td>0.857</td><td>0.001</td><td>0.030</td><td>0.094</td><td>0.046</td></tr><tr><td>MMDM (this paper)</td><td>0.856</td><td>0.005</td><td>0.028</td><td>0.091</td><td>0.063</td></tr></table>\n\n\nTable 2: Performance of a neural network on the Adult dataset when trained with FL. The fairness metric considered for BMDM-FL and FPFL is FNR parity and the tolerance is  $\\alpha = 0.02$ .\n\n\n<table><tr><td>Algorithm</td><td>Accuracy</td><td>FNR gap</td><td>EO gap</td><td>DemP gap</td><td>PP gap</td></tr><tr><td>FL</td><td>0.851</td><td>0.121</td><td>0.121</td><td>0.122</td><td>0.037</td></tr><tr><td>BMDM-FL</td><td>0.854</td><td>0.043</td><td>0.043</td><td>0.101</td><td>0.016</td></tr><tr><td>Fair FL</td><td>0.855</td><td>0.036</td><td>0.039</td><td>0.108</td><td>0.033</td></tr><tr><td>FL + Clip</td><td>0.844</td><td>0.169</td><td>0.169</td><td>0.129</td><td>0.051</td></tr><tr><td>BMDM-FL + Clip</td><td>0.851</td><td>0.052</td><td>0.052</td><td>0.105</td><td>0.008</td></tr><tr><td>Fair FL + Clip</td><td>0.853</td><td>0.018</td><td>0.029</td><td>0.090</td><td>0.016</td></tr><tr><td>Private FL</td><td>0.847</td><td>0.148</td><td>0.148</td><td>0.126</td><td>0.041</td></tr><tr><td>BMDM-PFL</td><td>0.850</td><td>0.023</td><td>0.035</td><td>0.097</td><td>0.009</td></tr><tr><td>Fair Private FL</td><td>0.851</td><td>0.001</td><td>0.027</td><td>0.087</td><td>0.042</td></tr></table>\n\n\nare displayed in Table 2. The network becomes less fair under all the metrics considered, compared with the central setting. Moreover, the introduction of clipping largely increases the unfairness of the models reaching more than a  $16\\%$  gap in FNR. The addition of DP requires an increase in cohort size from 200 to 1,000, but does not have a larger effect on the unfairness of the models (see Table 5 in Appendix B.1 for the details). These observations are in line with (Bagdasaryan et al., 2019), where they note that the under-represented groups usually have the higher loss gradients and thus clipping affects them more than the majority groups.\n\n\nAfter that, we repeated the above experiment with FPFL and BMDM with DP. FPFL and BMDM with DP converged to a solution faster than FederatedSGD, and thus the models were trained for only  $T = 250$  iterations. Here the model with the best training cohort accuracy while respecting the fairness condition on the training cohort data was selected. Note that the fairness condition is evaluated with noisy statistics, so a model may be deemed as fair while slightly violating the desired constraints. The results are also included in Table 2 to aid the comparison. We note how, similarly to the central case, the model trained with FPFL manages to enforce the fairness constraints while keeping a similar accuracy. In contrast, BMDM seems to not be able to find network weights that respect the fairness conditions, as expected by the less favorable (\"less convex\") loss surface around these solutions. Moreover, clipping does not seem to affect largely the performance of FPFL since it compensates the gradient loss clipping with the fairness enforcement.\n\n\n# 4.2 RESULTS ON THE UNFAIR FEMNIST DATASET\n\n\nWe start our experiments confirming again the hypothesis and findings from Bagdasaryan et al. (2019) that clipping and DP disproportionately affect under-represented groups. For that, we trained a convolutional network (consisting of 2 convolution layers with kernel of size  $5 \\times 5$ , stride of 2, ReLU activation function, and 32 and 64 filters respectively) with FederatedSGD and versions of this algorithm where only clipping was performed and where both clipping and DP were included. For the FEMNIST experiments, the privacy parameters are  $(\\epsilon, \\delta) = (2, 2.5 \\cdot 10^{-4})$  and the damping parameter is  $c = 20$ . They were trained with a cohort size of  $m = 100$  for  $T = 2,000$  iterations and the model with best training cohort accuracy was selected. The results are displayed in Table 3. Similarly to before, clipping increases the accuracy gap from  $13\\%$  to almost  $17\\%$ . In this case, since the number of users  $K$  is small, the DP noise is large compared to the users' statistics. Therefore, the\n\n\naccuracy drops from more than  $94\\%$  with clipping to  $80.7\\%$  when DP is also used, and the accuracy gap increases to more than  $40\\%$ .\n\n\nThe second experiment tests whether FPFL can remedy the unfairness without decreasing accuracy too much. We trained the same convolutional network, again for  $T = 2,000$  iterations, and selected the model with the best training cohort accuracy that respected the fairness condition on the training cohort. When DP noise is not included, FPFL reduces the accuracy gap with FederatedSGD by around  $9\\%$  while keeping the accuracy within  $1\\%$ . We note how, as before, clipping largely does not affect the ability of FPFL to enforce fairness. However, note that since the data is more non-i.i.d. than before (i.e. there are more differences between the distribution of each user) the models that are deemed fair in the training cohort may not be as fair in the general population, and now we see a larger gap between the desired tolerance  $\\alpha = 0.04$  and the obtained accuracy gap from FPFL without noise (0.047 and 0.053 without and with clipping).\n\n\nTable 3: Performance of a convolutional network on the Unfair FEMNIST when trained with different algorithms: like Table 2, starting with FederatedSGD and ending with FPFL. The fairness metric considered for FPFL is accuracy parity and the tolerance is  $\\alpha = 0.04$ .\n\n\n<table><tr><td>Algo.</td><td>Population</td><td>Accuracy</td><td>Acc. gap</td></tr><tr><td colspan=\"4\">m = 100</td></tr><tr><td>FL</td><td>K</td><td>0.960</td><td>0.134</td></tr><tr><td>FFL</td><td>K</td><td>0.950</td><td>0.047</td></tr><tr><td>FL + Clip</td><td>K</td><td>0.946</td><td>0.166</td></tr><tr><td>FFL + Clip</td><td>K</td><td>0.954</td><td>0.053</td></tr><tr><td>PFL</td><td>K</td><td>0.807</td><td>0.409</td></tr><tr><td>FPFL</td><td>K</td><td>0.093</td><td>0.015</td></tr><tr><td colspan=\"4\">m = 2,000</td></tr><tr><td>PFL</td><td>100K</td><td>0.951</td><td>0.157</td></tr><tr><td>FPFL</td><td>100K</td><td>0.903</td><td>0.074</td></tr><tr><td>PFL</td><td>1,000K</td><td>0.951</td><td>0.153</td></tr><tr><td>FPFL</td><td>1,000K</td><td>0.927</td><td>0.073</td></tr></table>\n\n\nWhen DP is used, the noise is too large for FPFL to function properly and often the sign of the constraints' gradient, see (9), flips. Note that in the estimation of the performance function, i.e.  $F(d_{a}, \\boldsymbol{w}) / n_{a}$ , both the numerator and denominator are obtained from a noisy vector, thus making the estimators more sensitive to noise than the estimators for plain FederatedSGD. This is due to two main factors: (i) the larger the model, the larger the aggregation of the gradients for each weight, and thus more noise needs to be added; and (ii) the smaller the amount of users, the larger the noise that needs to be added.\n\n\nFor this reason, we considered the hypothetical scenario where the population, used for calculating the DP noise, is 100 and 1,000 times larger, which is a conservative assumption for federated learning deployments (Apple, 2017). Then, we repeated the experiment with DP FederatedSGD and FPFL and increased the cohort size to  $m = 2,000$ . In this scenario, DP FederatedSGD maintained an accuracy gap of more than  $15\\%$  while FPFL reduced this gap to less than a half in both cases. Nonetheless, the accuracy still decreases slightly, with a reduction of around  $5\\%$  and  $2\\%$  respectively compared with DP FederatedSGD.\n\n\n# 5 CONCLUSION\n\n\nIn this paper, we studied and proposed a solution to the problem of group fairness in private federated learning. For this purpose, we adapt the modified method of multipliers (MMDM) (Platt & Barr, 1987) to empirical loss minimization with fairness constraints, producing an algorithm for enforcing fairness in central learning. Then, we extend this algorithm to private federated learning.\n\n\nThrough experiments in the Adult (Dua & Graff, 2017) and a modified version of the FEM-NIST (Caldas et al., 2018) datasets, we first confirm previous knowledge that DP disproportionately affects performance on under-represented groups (Bagdasaryan et al., 2019), with the further observation that this is true for many different fairness metrics, and not just for accuracy parity. The proposed FPFL algorithm is able to remedy this unfairness even in the presence of DP.\n\n\nLimitations and future work. The FPFL algorithm is more sensitive to DP noise than other algorithms for PFL. This requires increasing the cohort size or ensuring that enough users take part in training. Still, for the experiments, the number of users required is still lower (more than an order of magnitude) than the usual amount of users available in deployments of federated learning settings (Apple, 2017). In the future, we intend to scale the algorithm to larger models and empirically integrate it with faster training methods like Federated Averaging (McMahan et al., 2017).\n\n\n# REFERENCES\n\n- Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security (CCS), pp. 308-318, 2016.\n\n- Annie Abay, Yi Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube Chuba, and Heiko Ludwig. Mitigating bias in federated learning. arXiv preprint arXiv:2012.02447, 2020.\n\n- Alekh Agarwal, Alina Beygelzimer, Miroslav Dud\u00edk, John Langford, and Hanna Wallach. A reductions approach to fair classification. In International Conference on Machine Learning, pp. 60-69. PMLR, 2018.\n\n- Differential Privacy Team Apple. Learning with privacy at scale. Technical report, Apple, 2017.\n\n- Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate impact on model accuracy. Advances in Neural Information Processing Systems (NeurIPS), 32: 15479-15488, 2019.\n\n- Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan Rogers. Protection against reconstruction and its applications in private federated learning. arXiv preprint arXiv:1812.00984, 2018.\n\n- Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 1175-1191, 2017.\n\n- Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pp. 77-91. PMLR, 2018.\n\n- Sebastian Caldas, Peter Wu, Tian Li, Jakub Kone\u010dny, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018. URL https://arxiv.org/abs/1812.01097.\n\n- Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183-186, 2017.\n\n- Alessandro Castelnovo, Riccardo Crupi, Greta Greco, and Daniele Regoli. The zoo of fairness metrics in machine learning. arXiv preprint arXiv:2106.00467, 2021.\n\n- Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 2921-2926. IEEE, 2017.\n\n- Sen Cui, Weishen Pan, Jian Liang, Changshui Zhang, and Fei Wang. Addressing algorithmic disparity and performance inconsistency in federated learning, 2021.\n\n- Rachel Cummings, Varun Gupta, Dhamma Kimpara, and Jamie Morgenstern. On the compatibility of privacy and fairness. In Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization, pp. 309-315, 2019.\n\n- Jiahao Ding, Xinyue Zhang, Xiaohuan Li, Junyi Wang, Rong Yu, and Miao Pan. Differentially private and fair classification via calibrated functional mechanism. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 622-629, 2020.\n\n- Wei Du, Depeng Xu, Xintao Wu, and Hanghang Tong. Fairness-aware agnostic federated learning. In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM), pp. 181-189. SIAM, 2021.\n\n- Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.\n\n- Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pp. 265-284. Springer, 2006.\n\n- Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9(3-4):211-407, 2014.\n\n- Ferdinando Fioretto, Pascal Van Hentenryck, Terrence WK Mak, Cuong Tran, Federico Baldo, and Michele Lombardi. Lagrangian duality for constrained deep learning. arXiv preprint arXiv:2001.09394, 2020.\n\n- Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pp. 1322-1333, 2015.\n\n- Slawomir Goryczka and Li Xiong. A comprehensive comparison of multiparty secure additions with differential privacy. IEEE Transactions on Dependable and Secure Computing, 2015. doi: 10.1109/TDSC.2015.2484326.\n\n- Filip Granqvist, Matt Seigel, Rogier van Dalen, \u00c1ine Cahill, Stephen Shum, and Matthias Paulik. Improving on-device speaker verification using federated learning with privacy. Interspeech, pp. 4328-4332, 2020.\n\n- Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in neural information processing systems, 29:3315-3323, 2016.\n\n- Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, and Yaoliang Yu. FedMGDA+: Federated learning meets multi-objective optimization. arXiv preprint arXiv:2006.11489, 2020.\n\n- Wei Huang, Tianrui Li, Dexian Wang, Shengdong Du, and Junbo Zhang. Fairness and accuracy in federated learning. arXiv preprint arXiv:2012.10069, 2020.\n\n- Matthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman. Differentially private fair learning. In International Conference on Machine Learning (ICML), pp. 3000-3008. PMLR, 2019.\n\n- Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning. arXiv preprint arXiv:1905.10497, 2019.\n\n- Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In Proceedings of the 38th International Conference on Machine Learning (ICML), pp. 6357-6368. PMLR, 2021.\n\n- Jingcheng Liu and Kunal Talwar. Private selection from private candidates. In Proceedings of the ACM SIGACT Symposium on Theory of Computing, 2019.\n\n- Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag\u00fcera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1273-1282. PMLR, 2017.\n\n- H Brendan McMahan, Galen Andrew, Ulfar Erlingsson, Steve Chien, Ilya Mironov, Nicolas Papernot, and Peter Kairouz. A general approach to adding differential privacy to iterative training procedures. arXiv preprint arXiv:1812.06210, 2018a.\n\n- H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. In International Conference on Learning Representations (ICLR), 2018b.\n\n- Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In International Conference on Machine Learning, pp. 4615-4625. PMLR, 2019.\n\n- John C Platt and Alan H Barr. Constrained differential optimization. In Proceedings of the 1987 International Conference on Neural Information Processing Systems, pp. 612-621, 1987.\n\n- Borja Rodr\u00edguez-G\u00e1lvez, Ragnar Thobaben, and Mikael Skoglund. A variational approach to privacy and fairness. In 2nd Privacy Preserving Artificial Intelligence Workshop (PPAI) of the AAAI Conference on Artificial Intelligence, 2021. URL https://arxiv.org/abs/2006.06332.\n\n- Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 3-18. IEEE, 2017.\n\n- Cuong Tran, Ferdinando Fioretto, and Pascal Van Hentenryck. Differentially private and fair deep learning: A Lagrangian dual approach. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 9932-9939, 2021.\n\n- Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and Yi Zhou. A hybrid approach to privacy-preserving federated learning. In Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security, pp. 1-11, 2019.\n\n- Sahil Verma and Julia Rubin. Fairness definitions explained. In 2018 IEEE/ACM International Workshop on Software Fairness (FairWare), pp. 1-7. IEEE, 2018.\n\n- Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, PEARU Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cirmrnan, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2.\n\n- Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled R\u00e9nyi differential privacy and analytical moments accountant. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1226-1235. PMLR, 2019.\n\n- Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th international conference on world wide web, pp. 1171-1180, 2017.\n\nResponse: \\boxed{Accept}\n\n### Rationale:\n\n#### Originality:\nThe paper introduces a novel approach to enforce group fairness in private federated learning, which addresses a significant gap in the current literature. Most existing work either focuses on the centralized setting or does not consider the adverse effects of differential privacy on the models trained with PFL.\n\n#### Methodological Rigor:\n- **Adaptation of MMDM**: The authors successfully extend the modified method of differential multipliers (MMDM) to enforce fairness in PFL, demonstrating rigorous theoretical grounding.\n- **Empirical Validation**: The paper presents extensive experiments on two datasets, providing strong empirical validation of the proposed algorithm\u2019s effectiveness in mitigating unfairness while maintaining acceptable performance.\n\n#### Significance of Contribution:\n- **Impact on Underrepresented Groups**: The study confirms that differential privacy disproportionately affects under-represented groups, leading to significant performance gaps. The proposed algorithm, FPFL, shows promise in addressing this issue.\n- **Vers", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634717.3328204", "msecs": "332.0", "relativeCreated": "62156.29458427429", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning\n\n# Abstract\nModel-agnostic meta-learning (MAML) is a popular method for few-shot learning but assumes that we have access to the meta-training set. In practice, training on the meta-training set may not always be an option due to data privacy concerns, intellectual property issues, or merely lack of computing resources. In this paper, we consider the novel problem of repurposing pretrained MAML checkpoints to solve new few-shot classification tasks. Because of the potential distribution mismatch, the original MAML steps may no longer be optimal. Therefore we propose an alternative meta-testing procedure and combine MAML gradient steps with adversarial training and uncertainty-based stepsize adaptation. Our method outperforms \"vanilla\" MAML on same-domain and cross-domains benchmarks using both SGD and Adam optimizers and shows improved robustness to the choice of base stepsize.\n\n\n# 1 INTRODUCTION\n\n\nDeep learning approaches have shown improvements based on massive datasets and enormous computing resources. Despite their success, it is still challenging to apply state-of-the-art methods in the real world. For example, in semiconductor manufacturing (Nishi & Doering, 2000), collecting each new data point is costly and time consuming because it requires setting up a new manufacturing process accordingly. Moreover, in the case of a \"destructive inspection\", the cost is very high because the wafer must be destroyed for measurement. Therefore, learning from small amounts of data is important for practical purposes.\n\n\nMeta-learning (learning-to-learn) approaches have been proposed for learning under limited data constraints. A meta-learning model optimizes its parameters for the best performance on the distribution of tasks. In particular, few-shot learning (FSL) formulates \"learning from limited data\" as an  $n$ -way  $k$ -shot problem, where  $n$  is the number of classes and  $k$  is the number of labeled samples per class. For each task in FSL, a support set is provided for training, while a query set is provided for evaluation. Ideally, a meta-learning model trained over a set of tasks (meta-training) will generalize well to new tasks (meta-testing).\n\n\nModel-agnostic meta-learning (MAML) (Finn et al., 2017) is a general end-to-end approach for solving few-shot learning tasks. MAML is trained on the meta-training tasks to learn a model initialization (also known as checkpoint) such that a few gradient steps on the support set will yield the best predictions on the query set.\n\n\nHowever, in practice it may not always be possible to retrain or finetune on the meta-training set. This situation may arise when the meta-training data is confidential, subject to restrictive licences, contains private user information, or protected intellectual property such as semiconductor manufacturing know-how. Another reason is that one may not have the computing resources necessary for running large-scale meta-training.\n\n\nIn this paper, we consider the novel problem of repurposing MAML checkpoints to solve new few-shot classification tasks, without the option of (re)training on the meta-training set. Since the meta-testing set (new tasks) may differ in distribution from the meta-training set, the implicit assumption \u2013in end-to-end learning\u2013 of identically distributed tasks may not hold, so there is no reason why the meta-testing gradient steps should match the meta-training. Therefore, we investigate various improvements over the default MAML gradient steps for test time adaptation.\n\n\nConceptually, our approach consists of collecting information while training the model on a new support set and then proposing ways to use this information to improve the adaptation. In this paper, we consider the variance of model parameters during ensemble training as a source of information to use. We propose algorithms that uses this information both to adapt the stepsizes for MAML as well as to generate \"task-specific\" adversarial examples to help robust adaptation to the new task.\n\n\nOur main contributions are the following:\n\n- - We motivate the novel problem of repurposing MAML checkpoints to solve cross-domain few-shot classification tasks, in the case where the meta-training set is inaccessible, and propose a method based on uncertainty-based stepsize adaptation and adversarial data augmentation, which has the particularity that meta-testing differs from meta-training steps.\n\n- - Compared to \"vanilla\" MAML, our method shows improved accuracy and robustness to the choice of base stepsizes on popular cross-domain and same-domain benchmarks, using both the SGD and Adam (Kingma & Ba, 2014) optimizers. Our results also indicate that adversarial training (AT) is helpful in improving the model performance during meta-testing.\n\n\nTo the best of our knowledge, our work is the first few-shot learning method to combine the use of ensemble methods for stepsize computation and generating adversarial examples from the meta-testing support set for improved robustness. Moreover, our empirical observation of improving over the default meta-testing procedure of MAML motivates further research on alternative ways to leverage published model checkpoints.\n\n\n# 2 RELATED WORK\n\n\n# 2.1 META-LEARNING AND MODEL-AGNOSTIC META-LEARNING\n\n\nFSL approaches deal with an extremely small amount of training data and can be classified into three categories. First, metric-based approaches solve few-shot tasks by training a feature extractor that maximizes inter-class similarity and intra-class dissimilarity (Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018). Second, memory-based approaches utilize previous tasks for new tasks with external memories (Santoro et al., 2016; Mishra et al., 2018). Third, optimization-based approaches search for good initial parameters during training and adapt the pretrained model for new tasks (Finn et al., 2017; Lee & Choi, 2018; Grant et al., 2018). We focus on the optimization-based approaches and suggest better training especially for the general family of MAML methods.\n\n\n# 2.2 UNCERTAINTY FOR MODEL TRAINING\n\n\nUncertainty is an important criterion for measuring the robustness of a neural network (NN). Bayesian neural networks (BNN) (Blundell et al., 2015) obtain model uncertainty by placing prior distributions over the weights  $p(\\omega)$ . This uncertainty has been used to adapt the stepsizes during continual learning in Uncertainty-guided Continual BNNs (UCB) (Ebrahimi et al., 2020). For each parameter, UCB scales its stepsize inversely proportional to the uncertainty of the parameter in the BNN to reduce changes in important parameters while allowing less important parameters to be modified faster in favor of learning new tasks. Our approach also decreases the step sizes for \"uncertain\" parameters, but using a different notion of uncertainty, and instead in the context of FSL with a pretrained MAML checkpoint.\n\n\nUnlike BNNs, deep ensembles (Lakshminarayanan et al., 2017) estimate uncertainty of NNs using ensembles over randomly initialized parameters combined with adversarial training (AT). Deep ensembles have been successfully used to boost predictive performance and AT has been used to improve robustness to adversarial examples. Lakshminarayanan et al. (2017) showed that they produce predictive uncertainty estimates comparable in quality to BNNs. Deep ensembles, however, are not directly applicable for FSL tasks, as training randomly initialized parameters from scratch with a limited amount of training data yields poor performance. Our approach is partly inspired from deep ensembles, adapting it to the FSL setting by using instead a parameter perturbations of the MAML checkpoint model rather than a random initialization. We use in particular a multiplicative Gaussian perturbation that rescales the parameters, as the information content of the weights is said to be invariant to their scale (Wen et al., 2018).\n\n\n# 2.3 ADVERSARIAL TRAINING FOR ATTACK AND DEFENSE\n\n\nDeep NNs are sensitive to adversarial attacks (Goodfellow et al., 2015; Madry et al., 2018; Moosavi-Dezfooli et al., 2016). These methods generate an adversarial sample to fool a trained model, where the generated image looks identical to the original one for humans. Goodfellow et al. (2015) proposed the fast gradient sign method (FGSM) which generates adversarial example using sign of input gradient. While stronger attacks have been proposed (such as using projected gradient descent (Madry et al., 2018)), we will focus on the FGSM approach in this paper for simplicity. In FSL, adversarial attack and defense have also been studied. ADversarialMeta-Learner (Yin et al., 2018) utilized one-step adversarial attack for generating adversarial samples during meta-training. There is little consideration, however, for the degradation of accuracy in the original sample in adversarial defense approaches. Xie et al. (2020) reported improvement using generated examples for adversarial attack on the large scale training. Motiian et al. (2017) propose adversarial domain adaptation for FSL. Despite similar keywords, their work is distinct from ours and relies on using generative adversarial networks discriminators to confuse domains, whereas we rely on adversarial examples for improved robustness. To our best knowledge, there is no prior FSL work which uses adversarial examples to enhance the model at test-time.\n\n\n# 2.4 DOMAIN ADAPTATION\n\n\nDomain adaptation methods (DA) (Ben-David et al., 2010) attempt to alleviate the distribution mismatch between source and target domains. Most of the recently proposed DA approaches are based on generative adversarial networks (GANs) (Goodfellow et al., 2014). GAN-based DA approaches require having access to the large amount of unlabeled data from both the source and target datasets (Tzeng et al., 2017; Zhang et al., 2019; Wilson & Cook, 2020). DA methods cannot be directly applied in the FSL scenario due to limited number of target domain samples (shots). Some domain-adaptive FSL (DA-FSL) methods have been proposed in the case where only a very few samples from the target domain are available (Motiian et al., 2017; Zhao et al., 2020).\n\n\nDA and DA-FSL methods cannot be directly applied to our setting because we assume that the meta-training dataset is inaccessible \u2014 mirroring real world situations in which access to the meta-training set is restricted by privacy and confidentiality concerns. Our approach differs from DA and DA-FSL by not requiring access to the meta-training dataset (source domain).\n\n\n# 3 PROPOSED METHOD\n\n\nAt meta-testing time, MAML normally uses the support set to compute fixed gradient steps, which were \"calibrated\" using end-to-end learning during meta-training. That is, the learned initialization is such that a fixed combination of stepsize and loss result in the desired result. However, those stepsizes and losses may be suboptimal on the new task, especially if the new task is out-of-domain with respect to the meta-training tasks.\n\n\nOur method is based on the assumption that the support set can be used to improve the meta-testing procedure itself, beyond merely serving as training examples. We start by leveraging the support set to estimate task-specific uncertainties over the model parameters. Then, we propose two improvements over the \"vanilla\" MAML gradient steps: we scale the gradient steps using layer-wise stepsizes computed from the support set and we train using task-specific adversarial examples.\n\n\n# 3.1 MOTIVATING HYPOTHESES\n\n\nAt meta-testing time, we start by assuming that we can estimate task-specific uncertainties over the model parameters. One possibility, which we adopt in Section 3.2, is to train deep ensembles (Lakshminarayanan et al., 2017) on the support set and use the ensemble to estimate variances over model parameters. Each model learns slightly different parameters, and yields slightly different gradients. We regard the variance over the parameters and input gradients as task-specific uncertainties. Given the uncertainty estimates, we propose two modifications to the original MAML gradient steps.\n\n\nProposal 1 (Task-specific step sizes) Use lower step sizes for model parameters with higher variance.\n\n\nIn our case, the variance could be further amplified if high-variance components were to be moved with large stepsizes. Therefore, we carefully move high-variance components with a lower stepsize with the hope is that we can limit the variance over the model parameters. This approach can be related with the fact that lower step sizes should be taken for SGD when the gradients are very noisy (Dieuleveut et al., 2020).\n\n\nProposal 2 (Task-specific adversarial examples) Use adversarial examples with higher adversarial perturbation on input components with higher variance over the input gradient.\n\n\nThe intuition is that if only slightly perturbed models (from the ensemble) disagree on parts of the input gradient, then it means that they disagree on what to learn and therefore those parts of the input are more vulnerable to attack. Therefore, we propose to use AT with stronger adversarial perturbation in the weak parts of the input, with the hope to incur improved robustness on those parts of the input.\n\n\nWe regard adversarial training as a form of data augmentation or regularization at meta-testing time, which we hope allows the model to overcome the limited size of the support set.\n\n\n# 3.2 UNCERTAINTY-BASED GRADIENT STEPS AT TEST-TIME\n\n\nWe improve over the default MAML gradient steps by implementing the ideas presented in the previous section. The resulting approach is detailed in Algorithm 1 and is a combination of uncertainty-based stepsize adaption (USA, based on Proposal 1) and uncertainty-based fast gradient sign method (UFGSM), adversarial training (AT), and generating additional adversarial examples from deep ensembles (EnAug) which are based on Proposal 2.\n\n\n```\nData: New task support set  $D^{\\mathrm{Spt}} = \\{(x_1,y_1),\\ldots ,(x_{n\\times k},y_{n\\times k})\\}$  with  $n$  ways and  $k$  shots.   \nRequire: Base stepsize  $\\alpha$  , pretrained weights  $\\theta_0$  , Gaussian variance  $\\sigma$  , AT coefficient  $\\epsilon$  , size of ensemble  $M$  , number of gradient steps  $T$  , selection coefficients in  $\\{0,1\\}$  for: base stepsize  $\\lambda_{\\alpha}$  , adversarial loss  $\\lambda_{\\mathrm{AT}}$  , and augmented cross-entropy  $\\lambda_{\\mathrm{Aug}}$  .   \n1  $D^{\\mathrm{Aug}}\\gets \\emptyset ,\\alpha^{\\mathrm{adap}}\\gets \\alpha$    \n2 for  $m = 1$  to  $M$  do   \n3  $\\theta_0^m\\gets \\theta_0(1 + \\mathcal{N}(0,\\sigma))$  \u25b7Initialize deep ensemble   \n4 end   \n5 for  $t = 1$  to  $T$  do   \n6 for  $m = 1$  to  $M$  do   \n7  $\\theta_t^m\\gets \\theta_{t - 1}^m -\\alpha^{\\mathrm{adap}}\\odot \\nabla_{\\theta_{t - 1}^m}[\\mathcal{L}(D^{\\mathrm{Spt}},\\theta_{t - 1}^m) + \\mathcal{L}_{\\mathrm{AT}}(D^{\\mathrm{Spt}},\\theta_{t - 1}^m)]$    \n8  $D^{\\mathrm{Aug}}\\gets D^{\\mathrm{Aug}}\\cup \\{(\\mathcal{A}_{\\theta_{t - 1}^m}(x,y),y)\\mid (x,y)\\in D^{\\mathrm{Spt}}\\}$  \u25b7EnAug   \n9 end   \n10  $\\alpha^{\\mathrm{adap}}\\gets \\mathrm{USA}(\\alpha ,\\theta_t^{1:M})$  \u25b7Algorithm 2   \n11  $D^{\\mathrm{Aug}}\\gets D^{\\mathrm{Aug}}\\cup \\{\\mathrm{UFGSM}(\\theta_0,\\theta_t^{1:M},x,y)\\mid (x,y)\\in D^{\\mathrm{Spt}}\\}$  \u25b7Algorithm 3   \n12 end   \n13  $\\alpha \\gets \\lambda_{\\alpha}\\alpha +(1 - \\lambda_{\\alpha})\\alpha^{\\mathrm{adap}}$    \n14 for  $t = 1$  to  $T$  do   \n15  $\\theta_t\\gets \\theta_{t - 1} - \\alpha \\odot \\nabla_{\\theta_{t - 1}}[\\mathcal{L}(D^{\\mathrm{Spt}},\\theta_{t - 1}) + \\lambda_{\\mathrm{AT}}\\mathcal{L}_{\\mathrm{AT}}(D^{\\mathrm{Spt}},\\theta_{t - 1}) + \\lambda_{\\mathrm{Aug}}\\mathcal{L}(D^{\\mathrm{Aug}},\\theta_{t - 1})]$    \n16 end\n```\n\n\nDenote  $\\mathcal{L}(D,\\theta) = 1 / |D|\\sum_{(x,y)\\in D}l_{\\theta}(x,y)$  the cross-entropy loss for model  $\\theta$  over the labeled dataset  $D = \\{(x,y)\\ldots \\}$ ,  $\\mathcal{A}_{\\theta}(x,y) = x + \\epsilon \\mathrm{sign}(\\nabla_{x}l_{\\theta}(x,y))$  the FGSM adversarial example computed from  $(x,y)$  and  $\\mathcal{L}_{\\mathrm{AT}}(D,\\theta) = 1 / |D|\\sum_{(x,y)\\in D}l_{\\theta}(\\mathcal{A}_{\\theta}(x,y),y)$  the resulting adversarial\n\n\n![(a) Uncertainty of layers](images/96283a6fe48814d04c04895478c16e85cf7aafdf9b4876c98ad155eb2c850080.jpg)\n\n\n![(b) Uncertainty-based stepsize adaption Figure 1: USA converts uncertainty into layer-wise adapted stepsize. Here  $\\alpha = 0.01$ . (a) is standard deviation of trained ensemble models' weights. Each layer has a difference uncertainty. (b) is adapted stepsize by USA. Each layer has a different stepsize based on the uncertainty.](images/0fa625b9ac33a4a7cd8c22b8b9c67cee0ad7150187c0b4c0263b2d8f9264dbcc.jpg)\n\n\ncross-entropy, which we refer to as AT. For easy reference, all notations are summarized in Appendix A.1.\n\n\nStarting from the pretrained MAML checkpoint  $\\theta_0$ , we perturb the model parameters with multiplicative Gaussian noise to create a deep ensemble  $(\\theta_0^m)_{1\\leq m\\leq M}$  (lines 2-4). Then, we repeat the following for  $T$  steps. At time  $t$ , run gradient descent on each model  $\\theta_t^m$  of the ensemble (line 7), where the loss is a combination of the usual cross-entropy  $\\mathcal{L}$  and AT loss  $\\mathcal{L}_{\\mathrm{AT}}$  as in (Lakshminarayanan et al., 2017) but on the support set, and the stepsizes  $\\alpha^{\\mathrm{adap}}$  are updated using USA (details below). Also, we generate adversarial examples using FGSM and UFGSM (details below) and store them into  $D^{\\mathrm{Aug}}$  (lines 8 and 11). Finally, we run  $T$  gradient steps on the original checkpoint, where the loss is a combination of cross-entropy on the support set  $D^{\\mathrm{Spt}}$ , AT loss on the support set, and cross-entropy on the ensemble-augmented support set  $D^{\\mathrm{Aug}}$  (lines 13-16). Note that we recover the original MAML steps for  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 0, \\lambda_{\\alpha} = 1)$ , while we refer to the case  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 1, \\lambda_{\\alpha} = 0)$  as our full method.\n\n\nUSA. We propose uncertainty-based stepsize adaptation (USA), which assigns lower step sizes to layers<sup>1</sup> with higher uncertainty (Algorithm 2) - we call this loosely an \"inverse-relationship\" below. Let  $\\alpha$  denote the default (scalar) stepsize and  $\\theta^{1:M}$  the parameters of the ensemble models, where  $M$  is the size of the ensemble and the number of layers in model is  $L$ . To compute the adapted layer-wise stepsizes  $\\alpha^{\\mathrm{adap}}$ , we compute  $u$ , the parameter-wise standard deviation of the model parameters over the ensemble (line 2), apply an inverse-relationship transformation which flips the max with the min (line 3), average over each layer (line 4) and  $L_{1}$ -normalize the result (line 5). The design choices for USA are explained in Appendix A.2. The resulting step sizes  $\\alpha^{\\mathrm{adap}}$  have an inverse-relationship with the variance of each layer. We give an example of applying USA to the 4-ConvNet architecture on miniImageNet (Figure 1). On the left, we plot the layer-wise standard deviations of the parameters and on the right the corresponding USA step sizes, which follow an inverse relationship with the standard deviations.\n\n\n![Figure 2: Applying UFGSM to 4-ConvNet on miniImageNet with  $\\epsilon = 0.05$ . Starting from the clean image  $x$ , we add the signed gradient  $\\mathrm{sign}(\\nabla_x l_\\theta(x, y))$  after rescaling it by the uncertainty over the input gradient  $u$ , to generate the adversarial example  $x'$  (UFGSM). Note how UFGSM generated a more natural image than FGSM (rightmost,  $u = 1$ ). UFGSM. We also propose uncertainty-based FGSM (UFGSM) to generate adversarial examples, with higher adversarial perturbation on input components with higher variance over the input gradient (Algorithm 3). Starting from an image  $x$  and label  $y$ , we compute the input gradient for each model from the ensemble and calculate the standard deviation  $u$  over the ensemble (line 2). Then, we linearly map  $u$  between 0 and 1 (line 3), compute the FGSM adversarial example for the pretrained model  $\\theta_0$  and rescale it using  $u$ , so that areas of higher variance get more perturbation. We give an example of applying UFGSM to miniImageNet in Figure 2.](images/c9456698fd027d2ea74f43c14b939f7d1ff4e0dcc11a5f0423b44dd9d98016db.jpg)\n\n\n```\n1 Function USA  $(\\alpha, \\theta_t^{1:M})$   \n2  $u = \\operatorname{Std}(\\theta_t^{1:M})$   \n3  $c = \\operatorname{Max}(u) - u + \\operatorname{Min}(u)$   \n4  $\\mu^l \\gets \\text{average of } c \\text{ over each layer } l$   \n5  $\\alpha^{\\text{adap}} = \\alpha \\mu^l / (1/L \\sum_{l=1}^L \\mu^l)$   \n6 return  $\\alpha^{\\text{adap}}$   \n7 end\n```\n\n\n```\nDefine:MinMaxNorm  $(x) = \\frac{x - \\operatorname{Min}(x)}{\\operatorname{Max}(x) - \\operatorname{Min}(x)}$    \n1 Function UFGSM  $(\\theta ,\\theta_t^{1:M},x,y)$    \n2  $u = \\mathrm{Std}(\\{\\nabla_xl_{\\theta '}(x,y)|\\theta '\\in \\theta_t^{1:M}\\})$    \n3  $u = \\mathrm{MinMaxNorm}(u)$    \n4  $x^{\\prime} = x + \\epsilon u\\odot \\mathrm{sign}(\\nabla_{x}l_{\\theta}(x,y))$    \n5 return  $x^{\\prime}$    \n6 end\n```\n\n\n# 4 EXPERIMENTAL EVALUATION\n\n\nWe train MAML on miniImageNet (Vinyals et al., 2016) training split; we then apply our method on the resulting checkpoint. We evaluate our model on the test split of miniImageNet \u2013 for the same-domain setting \u2013 as well as CUB-200-2011 (Welinder et al., 2010), Traffic Sign (Houben et al., 2013) and VGG Flower (Nilsback & Zisserman, 2008) \u2013 for the cross-domain setting. These datasets are denoted as Mini, Birds, Signs and Flowers respectively. A desirable feature for an optimizer is to maintain good performance in a broad range of step sizes (Asi & Duchi, 2019). Therefore, we evaluate our approach not only at the optimal stepsize, but also over a broad range of base step sizes $^{2}$  from  $10^{-4}$  to 1. We evaluate the performance with three metrics: All, Top-1 and Top-40%. If two methods have comparable Top-1 performance, but one has better Top-40% performance, then that method is more robust to the choice of base stepsize. Detailed experimental setup and pretrained model selection are included in Appendix A.3. Our code is available at https://anonymous.4open.science/r/08ef52cf-456a-4e36-a408-04e1ad0bc5a9/ (anonymous project).\n\n\n# 4.1 MAIN RESULTS\n\n\nOur main results are in Table 1. We compare the default MAML steps, which we take as our baseline (denoted as SGD), with our full method (denoted as SGD+All), which consists of combining USA, UFSGM, EnAug, and AT ( $\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 1$ ,  $\\lambda_{\\alpha} = 0$  in Algorithm 1). In terms of absolute performance (Top-1 row), our method outperforms the baseline on cross-domain tasks (Birds,\n\n\nTable 1: Main results. We compare the default MAML steps (SGD) with our method (SGD+All) on same-domain and cross-domain benchmarks.\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"2\">5-way 1-shot</td><td colspan=\"2\">5-way 5-shot</td><td colspan=\"2\">10-way 1-shot</td></tr><tr><td>SGD</td><td>SGD+All</td><td>SGD</td><td>SGD+All</td><td>SGD</td><td>SGD+All</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>37.21\u00b10.22</td><td>36.24\u00b10.22</td><td>38.14\u00b10.39</td><td>43.48\u00b10.38</td><td>21.31\u00b10.37</td><td>22.07\u00b10.4</td></tr><tr><td>Birds</td><td>30.07\u00b10.52</td><td>29.83\u00b10.46</td><td>33.29\u00b10.29</td><td>38.07\u00b10.33</td><td>17.54\u00b10.06</td><td>17.81\u00b10.12</td></tr><tr><td>Flowers</td><td>40.54\u00b10.48</td><td>40.86\u00b10.45</td><td>44.55\u00b10.43</td><td>53.10\u00b10.55</td><td>26.79\u00b10.41</td><td>28.07\u00b10.41</td></tr><tr><td>Signs</td><td>34.19\u00b10.76</td><td>34.76\u00b10.61</td><td>45.39\u00b10.45</td><td>51.02\u00b10.5</td><td>23.77\u00b10.13</td><td>24.88\u00b10.13</td></tr><tr><td>Avg.</td><td>35.50\u00b10.49</td><td>35.42\u00b10.44</td><td>40.34\u00b10.39</td><td>46.42\u00b10.44</td><td>22.35\u00b10.24</td><td>23.21\u00b10.26</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.71\u00b10.81</td><td>46.62\u00b10.58</td><td>62.46\u00b10.68</td><td>62.16\u00b10.66</td><td>31.10\u00b10.66</td><td>31.42\u00b10.82</td></tr><tr><td>Birds</td><td>36.03\u00b10.48</td><td>36.43\u00b10.58</td><td>50.71\u00b10.54</td><td>51.82\u00b10.64</td><td>23.16\u00b10.21</td><td>23.61\u00b10.39</td></tr><tr><td>Flowers</td><td>51.97\u00b10.79</td><td>54.36\u00b11.04</td><td>70.63\u00b10.76</td><td>74.52\u00b10.69</td><td>38.61\u00b10.49</td><td>40.68\u00b10.58</td></tr><tr><td>Signs</td><td>43.44\u00b10.99</td><td>44.10\u00b11.03</td><td>72.52\u00b10.84</td><td>74.13\u00b10.90</td><td>35.10\u00b10.21</td><td>35.11\u00b10.26</td></tr><tr><td>Avg.</td><td>44.54\u00b10.77</td><td>45.37\u00b10.81</td><td>64.08\u00b10.71</td><td>65.66\u00b10.72</td><td>31.99\u00b10.39</td><td>32.70\u00b10.51</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.07\u00b10.52</td><td>45.79\u00b10.44</td><td>55.38\u00b10.46</td><td>60.70\u00b10.91</td><td>28.65\u00b10.64</td><td>30.56\u00b10.77</td></tr><tr><td>Birds</td><td>34.88\u00b10.55</td><td>35.77\u00b10.55</td><td>45.50\u00b10.49</td><td>51.03\u00b10.63</td><td>21.78\u00b10.07</td><td>22.87\u00b10.30</td></tr><tr><td>Flowers</td><td>49.98\u00b10.69</td><td>53.16\u00b10.73</td><td>65.16\u00b10.73</td><td>73.55\u00b10.86</td><td>36.14\u00b10.53</td><td>39.75\u00b10.61</td></tr><tr><td>Traffic</td><td>41.81\u00b11.02</td><td>43.39\u00b11.01</td><td>67.34\u00b10.43</td><td>71.45\u00b10.90</td><td>33.56\u00b10.11</td><td>34.27\u00b10.17</td></tr><tr><td>Avg.</td><td>42.94\u00b10.70</td><td>44.52\u00b10.68</td><td>58.34\u00b10.53</td><td>64.18\u00b10.82</td><td>30.03\u00b10.34</td><td>31.86\u00b10.46</td></tr></table>\n\n\nFlowers, Signs), while the performance is comparable on same-domain tasks (Mini). In terms of robustness to the base stepsize (All and Top-40% rows), our method outperforms the baseline over a large range of stepsizes for 5-way 5-shot and 10-way 1-shot tasks, while for 5-way 1-shot the results are either comparable (All) or better (Top-40%) depending on the metric considered. More results can be found in Appendix A.6.\n\n\n# 4.2 DISCUSSION\n\n\nAblation Study. We perform an ablation study for the 5-way 1-shot case in Table 2 and plot the accuracy at different base step sizes for the Flowers dataset in Figure 3. Comparing SGD to SGD+AT in Table 2, we observe that adversarial training is beneficial over the baseline (Top1 and Top-40%), except for large step sizes with SGD, which is reflected in the All metric and in the dip in performance in Figure 3a. Notice also how the use of AT flattens the accuracy curve near the optimal stepsize. Comparing SGD to SGD+USA, we observe a very small but consistent improvement over the baseline in the vicinity of the optimal stepsize (Top-1 and Top-40%). Comparing SGD+USA to SGD+USA+UFGSM, we observe improvement over a wide range of step sizes, which is reflected in the curves and in All and Top-40% metrics. Comparing SGD+USA+UFGSM+EnAug+AT to SGD+USA+UFGSM+EnAug shows that AT+EnAug is beneficial in terms of absolute performance (Top-1) as well as creating robustness to the choice of stepsize (Top-40%). The drop for the All metric is explained by the dip in the curve for the largest step sizes. Overall, the best absolute performance (Top-1) is always obtained through some use of adversarial training, while using the full method consistently results in increased robustness to the choice of stepsize (best Top-40% performance). Appendix A.5 shows various AT results.\n\n\nUFGSM vs. FGSM. We compare UFGSM to FGSM (Goodfellow et al., 2015) in Table 3. The results show that our uncertainty-based approach consistently outperforms FGSM, which suggests that the uncertainty information extracted from the support set was useful.\n\n\nSGD vs. Adam. Our method can be used with SGD and Adam, however, we have mainly focused on SGD throughout the paper. SGD tends to yield the best results (Top-1 and Top-40%), as shown in Table 4 for the baseline and full method. More results for Adam can be found in Sections A.6 and A.8 of the appendix.\n\n\nBest checkpoint vs. Overfitted checkpoint. We also consider the problem of repurposing an overfitted checkpoint. We find that our method improves both absolute performance and robustness to stepsize. In fact, the gains are more substantial on the overfitted (worse) checkpoint than the best checkpoint (Appendix A.8).\n\n\nTable 2: Ablation Study for 5-way 1-shot classification with SGD optimizer. SGD+AT corresponds to using  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\alpha} = 1, \\lambda_{\\mathrm{Aug}} = 0)$  in Algorithm 1, SGD+USA to  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = \\lambda_{\\alpha} = 0)$ , SGD+USA+UFGSM to  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\alpha} = 0, \\lambda_{\\mathrm{Aug}} = 1)$  and SGD+USA+UFGSM+EnAug+AT to  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 1, \\lambda_{\\alpha} = 0)$ .\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"5\">5-way 1-shot</td></tr><tr><td>SGD</td><td>SGD+AT</td><td>SGD+USA</td><td>SGD+USA +UFGSM</td><td>SGD+USA +UFGSM +EnAug+AT</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>37.21\u00b10.22</td><td>34.60\u00b10.18</td><td>37.53\u00b10.27</td><td>38.71\u00b10.28</td><td>36.24\u00b10.22</td></tr><tr><td>Birds</td><td>30.07\u00b10.52</td><td>28.81\u00b10.45</td><td>30.39\u00b10.53</td><td>31.07\u00b10.50</td><td>29.83\u00b10.46</td></tr><tr><td>Flowers</td><td>40.54\u00b10.48</td><td>38.80\u00b10.44</td><td>41.09\u00b10.49</td><td>42.33\u00b10.60</td><td>40.86\u00b10.45</td></tr><tr><td>Signs</td><td>34.19\u00b10.76</td><td>33.42\u00b10.59</td><td>35.02\u00b10.82</td><td>36.10\u00b10.48</td><td>34.76\u00b10.61</td></tr><tr><td>Avg.</td><td>35.50\u00b10.49</td><td>33.91\u00b10.41</td><td>36.01\u00b10.36</td><td>37.05\u00b10.27</td><td>35.42\u00b10.44</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.71\u00b10.81</td><td>46.58\u00b10.65</td><td>46.72\u00b10.71</td><td>46.66\u00b10.77</td><td>46.62\u00b10.58</td></tr><tr><td>Birds</td><td>36.03\u00b10.48</td><td>36.40\u00b10.48</td><td>36.08\u00b10.49</td><td>36.02\u00b10.48</td><td>36.43\u00b10.58</td></tr><tr><td>Flowers</td><td>51.97\u00b10.79</td><td>54.55\u00b10.84</td><td>52.01\u00b10.77</td><td>52.30\u00b10.77</td><td>54.36\u00b11.04</td></tr><tr><td>Signs</td><td>43.43\u00b10.99</td><td>43.98\u00b10.98</td><td>43.42\u00b11.06</td><td>43.96\u00b10.64</td><td>44.10\u00b11.03</td></tr><tr><td>Avg.</td><td>44.54\u00b10.77</td><td>45.38\u00b10.74</td><td>44.56\u00b10.44</td><td>44.73\u00b10.41</td><td>45.37\u00b10.81</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.07\u00b10.52</td><td>45.53\u00b10.50</td><td>45.23\u00b10.59</td><td>45.76\u00b10.62</td><td>45.79\u00b10.44</td></tr><tr><td>Birds</td><td>34.88\u00b10.55</td><td>35.57\u00b10.60</td><td>35.17\u00b10.55</td><td>35.30\u00b10.48</td><td>35.77\u00b10.55</td></tr><tr><td>Flowers</td><td>49.98\u00b10.69</td><td>52.92\u00b10.68</td><td>50.39\u00b10.71</td><td>51.07\u00b10.67</td><td>53.16\u00b10.73</td></tr><tr><td>Signs</td><td>41.83\u00b11.02</td><td>43.19\u00b10.96</td><td>42.08\u00b11.06</td><td>43.18\u00b10.73</td><td>43.39\u00b11.01</td></tr><tr><td>Avg.</td><td>42.94\u00b10.70</td><td>44.30\u00b10.68</td><td>43.22\u00b10.43</td><td>43.83\u00b10.39</td><td>44.52\u00b10.68</td></tr></table>\n\n\n![(a) With SGD optimizer](images/3df392ad3021c71ce0e7946c9228b5982feb1bef6db2f2f73375c4d33d688279.jpg)\n\n\n![(b) With Adam optimizer Figure 3: Ablation study for 5-way 1-shot classification on Flowers dataset.](images/891c2152ff15791a2798f7419bd448857b22aa1092a3e328b022664f1e282708.jpg)\n\n\nTable 3: Comparing UFGSM against FGSM (Goodfellow et al., 2015) on 5-way 1-shot tasks. For all metrics and datasets, the proposed uncertainty-based method is better.\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"2\">SGD+USA</td></tr><tr><td>+UFGSM</td><td>+FGSM</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>38.71\u00b10.28</td><td>36.97\u00b10.54</td></tr><tr><td>Birds</td><td>31.07\u00b10.50</td><td>30.05\u00b10.55</td></tr><tr><td>Flowers</td><td>42.33\u00b10.60</td><td>41.17\u00b10.55</td></tr><tr><td>Signs</td><td>36.10\u00b10.48</td><td>34.78\u00b10.50</td></tr><tr><td>Avg.</td><td>37.05\u00b10.27</td><td>35.74\u00b10.27</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.66\u00b10.77</td><td>43.93\u00b10.85</td></tr><tr><td>Birds</td><td>36.02\u00b10.48</td><td>34.52\u00b10.74</td></tr><tr><td>Flowers</td><td>52.30\u00b10.77</td><td>50.63\u00b10.84</td></tr><tr><td>Signs</td><td>43.96\u00b10.64</td><td>42.62\u00b11.00</td></tr><tr><td>Avg.</td><td>44.73\u00b10.41</td><td>42.93\u00b10.47</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.76\u00b10.62</td><td>43.18\u00b10.87</td></tr><tr><td>Birds</td><td>35.30\u00b10.48</td><td>33.88\u00b10.75</td></tr><tr><td>Flowers</td><td>51.07\u00b10.67</td><td>49.47\u00b10.69</td></tr><tr><td>Signs</td><td>43.18\u00b10.73</td><td>41.92\u00b10.90</td></tr><tr><td>Avg.</td><td>43.83\u00b10.39</td><td>42.11\u00b10.46</td></tr></table>\n\n\nTable 4: Comparing SGD vs. Adam for default MAML steps (baseline) and our method (baseline+all) on 5-way 1-shot classification.\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"2\">Baseline</td><td colspan=\"2\">Baseline+All</td></tr><tr><td>SGD</td><td>Adam</td><td>SGD</td><td>Adam</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>37.21\u00b10.22</td><td>31.87\u00b10.20</td><td>36.24\u00b10.22</td><td>33.57\u00b10.29</td></tr><tr><td>Birds</td><td>30.07\u00b10.52</td><td>28.02\u00b10.39</td><td>29.83\u00b10.46</td><td>29.51\u00b10.46</td></tr><tr><td>Flowers</td><td>40.54\u00b10.48</td><td>40.33\u00b10.57</td><td>40.86\u00b10.45</td><td>44.58\u00b10.70</td></tr><tr><td>Signs</td><td>34.19\u00b10.76</td><td>33.98\u00b10.43</td><td>34.76\u00b10.61</td><td>35.10\u00b10.43</td></tr><tr><td>Avg.</td><td>35.50\u00b10.49</td><td>33.55\u00b10.22</td><td>35.42\u00b10.44</td><td>35.69\u00b10.29</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.71\u00b10.81</td><td>43.08\u00b10.10</td><td>46.62\u00b10.58</td><td>44.72\u00b10.35</td></tr><tr><td>Birds</td><td>36.03\u00b10.48</td><td>34.00\u00b10.63</td><td>36.43\u00b10.58</td><td>35.45\u00b10.67</td></tr><tr><td>Flowers</td><td>51.97\u00b10.79</td><td>51.28\u00b10.47</td><td>54.36\u00b11.04</td><td>56.04\u00b11.14</td></tr><tr><td>Signs</td><td>43.43\u00b10.99</td><td>42.53\u00b10.79</td><td>44.10\u00b11.03</td><td>43.01\u00b10.85</td></tr><tr><td>Avg.</td><td>44.54\u00b10.77</td><td>42.72\u00b10.32</td><td>45.37\u00b10.81</td><td>44.80\u00b10.54</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.07\u00b10.52</td><td>40.18\u00b10.14</td><td>45.79\u00b10.44</td><td>42.92\u00b10.35</td></tr><tr><td>Birds</td><td>34.88\u00b10.55</td><td>32.38\u00b10.61</td><td>35.77\u00b10.55</td><td>34.54\u00b10.66</td></tr><tr><td>Flowers</td><td>49.98\u00b10.69</td><td>48.18\u00b10.44</td><td>53.16\u00b10.73</td><td>54.08\u00b10.76</td></tr><tr><td>Signs</td><td>41.83\u00b11.02</td><td>40.90\u00b10.70</td><td>43.39\u00b11.01</td><td>42.26\u00b10.77</td></tr><tr><td>Avg.</td><td>42.94\u00b10.70</td><td>40.41\u00b10.29</td><td>44.52\u00b10.68</td><td>43.45\u00b10.40</td></tr></table>\n\n\n**Validating Proposal 1 and 2.** Our method is built on the hypotheses that we should take small step sizes on high uncertainty parameters and add more adversarial perturbation on high uncertainty input gradients. To validate those choices, we have tried taking large step sizes on high uncertainty parameters, and using less adversarial perturbation on high uncertainty input gradients, which resulted in lower accuracy and robustness to the stepsize (Appendix A.7).\n\n\nFreezing most uncertain layers. We observed that the batch normalization (BN) layers have the most uncertainty (Figure 1). In Proposal 1, we argued that the higher uncertainty component amplifies the error when we use bigger stepsize. As suggested by an anonymous reviewer, this motivates an additional experiment that we perform where we \"freeze\" the BN layer during meta-testing (i.e. we manually set the stepsize for the BN scale and shift parameters to zero). We performed the BN layer freezing experiment on  $\\mathrm{SGD + All}$  (see detailed results in Table 10 and Figure 12 in Appendix A.9). It turns out that  $\\mathrm{SGD + All}$  (w/ freezing BN) outperforms  $\\mathrm{SGD + All}$  (w/o freezing BN) on the All metric, with most improvement in the higher stepsize range ( $>0.1$ ). This is consistent with our intuition that updating high-uncertainty layers (such as BN) with large step sizes can be harmful.\n\n\n# 5 CONCLUSION\n\n\nIn this paper we considered the novel problem of repurposing pretrained MAML checkpoints for out-of-domain few-shot learning. Our method uses deep ensembles to estimate model parameter and input gradient uncertainties over the support set, and builds upon the default MAML gradient steps through the addition of uncertainty-based adversarial training and adaptive step sizes. Our experiments over popular few-shot benchmarks show that our method yields increased accuracy and robustness to the choice of base stepsize. More generally, our results motivate the use of adversarial learning as a data augmentation scheme for improving few-shot generalization. In the future, it would be interesting to apply our method to related settings such as domain adaption and transfer learning.\n\n\n# REFERENCES\n\n- Hilal Asi and John C. Duchi. The importance of better models in stochastic optimization. In National Academy of Sciences, 2019.\n\n- Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. In Machine Learning, 2010.\n\n- Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, 2015.\n\n- Aymeric Dieuleveut, Alain Durmus, Francis Bach, et al. Bridging the gap between constant step size stochastic gradient descent and markov chains. In Annals of Statistics. Institute of Mathematical Statistics, 2020.\n\n- Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided continual learning with bayesian neural networks. In International Conference on Learning Representations, 2020.\n\n- Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, 2017.\n\n- Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014.\n\n- Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.\n\n- Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-based meta-learning as hierarchical bayes. In International Conference on Learning Representations, 2018.\n\n- Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of traffic signs in real-world images: The german traffic sign detection benchmark. In International Joint Conference on Neural Networks, 2013.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In arXiv preprint arXiv:1412.6980, 2014.\n\n- Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, 2017.\n\n- Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In International Conference on Machine Learning, 2018.\n\n- Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\n\n- Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive metalearner. In International Conference on Learning Representations, 2018.\n\n- Seyed Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and accurate method to fool deep neural networks. In Computer Vision and Pattern Recognition, 2016.\n\n- Saeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gianfranco Doretto. Few-shot adversarial domain adaptation. In Advances in Neural Information Processing Systems, 2017.\n\n- Maria Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008.\n\n- Yoshio Nishi and Robert Doering. Handbook of semiconductor manufacturing technology. CRC press, 2000.\n\n- Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations, 2017.\n\n- Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International Conference on Machine Learning, 2016.\n\n- Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, 2017.\n\n- Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales. Learning to compare: Relation network for few-shot learning. In Computer Vision and Pattern Recognition, 2018.\n\n- Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Metadata: A dataset of datasets for learning to learn from few examples. In International Conference on Learning Representations, 2020.\n\n- Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and Ming-Hsuan Yang. Cross-domain few-shot classification via learned feature-wise transformation. In International Conference on Learning Representations, 2020.\n\n- Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Conference on Computer Vision and Pattern Recognition, 2017.\n\n- Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, 2016.\n\n- Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010.\n\n- Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudo-independent weight perturbations on mini-batches. In International Conference on Learning Representations, 2018.\n\n- Garrett Wilson and Diane J. Cook. A survey of unsupervised deep domain adaptation. In ACM Transactions on Intelligent Systems and Technology, 2020.\n\n- Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and Quoc V. Le. Adversarial examples improve image recognition. In Computer Vision and Pattern Recognition, 2020.\n\n- Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. Adversarial meta-learning. In arXiv preprint arXiv:1806.03316, 2018.\n\n- Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, 2014.\n\n- Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan. Domain-symmetric networks for adversarial domain adaptation. In Computer Vision and Pattern Recognition, 2019.\n\n- An Zhao, Mingyu Ding, Zhiwu Lu, Tao Xiang, Yulei Niu, Jiechao Guan, Ji Rong Wen, and Ping Luo. Domain-adaptive few-shot learning. In arXiv, 2020.\n\nResponse: \\boxed{Accept}\n\n### Rationale for Prediction:\n\n#### Originality:\n- The paper presents a novel approach to leveraging pretrained MAML checkpoints for out-of-domain few-shot learning. This is an innovative extension of MAML, addressing the limitation of needing access to the meta-training set.\n- The authors propose several novel techniques including uncertainty-based stepsize adaptation and adversarial training at test time, which are not widely discussed in existing literature on few-shot learning.\n\n#### Methodological Rigor:\n- The methodology is clearly described, with pseudocode provided for key steps. The experimental setup is detailed and reproducible.\n- The authors conduct thorough ablation studies to validate the effectiveness of each component of their method, providing strong evidence for its robustness and effectiveness.\n- The evaluation is comprehensive, covering multiple datasets (miniImageNet, CUB-200-2011, Traffic Sign, VGG Flower) and different base step sizes, demonstrating the versatility and generalizability of their approach.\n\n#### Significance of Contribution:\n- The proposed method significantly improves performance over the default MAML approach across various datasets and base step sizes.\n- By leveraging ensemble methods and adversarial training, the authors achieve better robustness to the choice of base step size, which is crucial for practical applications.\n- The method addresses a common issue in few-shot learning\u2014limited meta-training data\u2014and thus has broader implications for the field.\n\n#### Clarity of Presentation:\n- The paper is well-written, with a clear introduction, motivation, and detailed description of the proposed method.\n- The results are presented in a structured manner, making it easy to compare the performance of different components and configurations.\n- The figures and tables effectively support the text, providing visual insights into the behavior of the method.\n\n#### Alignment with Journal Scope:\n- The paper aligns well with the scope of ICLR, which focuses on the intersection of machine learning and artificial intelligence, particularly in the areas of representation learning and meta-learning.\n- The contributions made in this paper are relevant to the broader goals of improving the robustness and generalization capabilities of deep learning models.\n\n### Weaker Points:\n- The paper does not delve deeply into theoretical justifications for the proposed methods, such as the mathematical derivation behind uncertainty-based stepsize adaptation or the effectiveness of adversarial training in this context.\n- The discussion around the limitations and potential future work could be expanded to encourage further research directions.\n\nOverall, the paper provides significant advances in the field of few-shot learning and demonstrates strong methodological rigor and practical utility. Given these factors, it is highly likely to be accepted by ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634721.9278932", "msecs": "927.0", "relativeCreated": "66751.36733055115", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Unified Discrete Diffusion for Simultaneous Vision-Language Generation\n\n# Abstract\nThe recently developed discrete diffusion model performs extraordinarily well in generation tasks, especially in the text-to-image task, showing great potential for modeling multimodal signals. In this paper, we leverage these properties and present a unified multimodal generation model, which can perform text-based, image-based, and even vision-language simultaneous generation using a single model. Specifically, we unify the discrete diffusion process for multimodal signals by proposing a unified Markov transition matrix and a unified objective. Moreover, we design a multimodal mutual attention module to highlight the inter-modal linkages, which is vital for multimodal generation. Extensive experiments indicate that our proposed method can perform comparably to the state-of-the-art solutions in various generation tasks.\n\n\n# 1 INTRODUCTION\n\n\n![Figure 1: Examples of various tasks supported by UniD3. The dark brown portions of the image and description represent the [MASK].](images/cc2b5f09ac9982cc9c03416dfc4b49f4a6f6d2c018ec7e9c0050376f60b90413.jpg)\n\n\nDiffusion models (Ho et al., 2020; Song et al., 2021b) have garnered significant interest on various high quality conditional image generation tasks, such as image super-resolution (Rombach et al., 2022), image inpainting (Lugmayr et al., 2022), image editing (Avrahami et al., 2022), image translation (Saharia et al., 2022a), among others. Concurrently, the Vector Quantized (VQ) models have also achieved rapid advances in image generations, especially on cross-modal tasks, examples include text-to-image (Kim et al., 2022), sketch-to-image (Esser et al., 2021b), image-to-video (Wu et al., 2021).\n\n\nDespite their success, all of these generation tasks are designed for only a single special modality, i.e. either modality translation or modality generation, with the help of a powerful diffusion model. In particular, the former modality translation translates the given conditional signals into the traget domain, while the latter modality generation only works for the unconditional image generation, e.g. image generation on CelebA Karras et al. (2017) or LSUN Yu et al. (2015) datasets. However, none of them consider to learn a join distribtuion for the mixture modality.\n\n\n![Figure 2: The pipeline of UniD3. With an offline model (red background part), the given inputs are represented by discrete token sequence in separate domain. The fusion embedding concatenates the tokens in different modal and embed them to the same space. The unified diffusion (in blue background) will construct the joint distribution of all modalities based on the fused embedding with a fixed unified Markov transition matrix.](images/897c71805f291805d2fee983dc2442c5735f174a67302da7f5091938a67de31c.jpg)\n\n\nHere, we take the use of discrete diffusion model into a new realm - multi-modality generation using a unified vision-language model. In contrast to the modality translation tasks outlined above, our multi-modality generative model does not require any conditional signals given in prior and is capable of simultaneously generating content pairs with the associated multi-modalities. UniD3, our new Unified Discrete Denoising Diffusion model, allows us to construct a joint vision-language probability distribution by mixing discrete image tokens with text tokens, leading to a capability of simultaneously generating cross-domain results. This is achieved by the two-stages framework, illustrated in Fig. 2, (1) An offline model to generate a compact yet expressive discrete representation for both images and texts via discrete VAE (dVAE) and Byte-Pair Encoding (BPE), respectively (the pink part in Fig. 2). (2) A novel unified discrete diffusion model to estimate the joint distribution of such latent visual and language codes (the cyan part in Fig. 2). Once trained, UniD3 can not only inherit the ability to manipulate the provided text or image, but is also able to unify the text and image generation, e.g., unconditional vision-language pairings generation, cross modal manipulation, text guided image completion, and image conditional text caption (Fig. 1 depicts the tasks that our model is capable of handling).\n\n\nBased on the empirical exploration, our model can achieve comparable image quality to the text condition in the unconditional case. In terms of image caption, our model is also comparable to some states of the art.\n\n\nIn summary, our key contributions include the following:\n\n- - We design a specific Markov transition matrix for our unified discrete denoising diffusion model, which leads to a sophisticated control of diffusion process, to estimate the joint distribution of language and image. The purposive design of transfer matrices based on task objectives and data properties is also pioneering for discrete diffusion models.\n\n- - We additionally propose mutual mechanism with fuse embedding to fulfil the objective of multi-modal integration. And we alter a unified objective function to offer the optimization with more concise restrictions.\n\n- - To the best of our knowledge, the UniD3 is the first work to solve both multi-modality generation and modality translation problems that is capable of handling the simultane\n\n\n![Figure 3: Illustration of transformer blocks with mutual attention. A unified transformer is composed of several blocks stacked on top of one another.](images/302a776957349843c03749a62a54d31e52492623a0fbf66b44d4a0ba153f0445.jpg)\n\n\nous unconditional vision-language generation and bi-directional vision-language synthesis tasks with a single model.\n\n\n# 2 PRELIMINARIES\n\n\nVector Quantised Model The Vector-Quantised Variational Auto Encoder (VQ-VAE) (Van Den Oord et al., 2017) learns to embed the high-dimensional data, e.g., image or audio, into a discrete representation. In particular, given a high dimensional data  $\\pmb{x} \\in \\mathbb{R}^{C \\times H \\times W}$ , the encoder  $E$  first converts it to the spatial latent features  $\\pmb{z} = \\{z_{i,j}\\} \\in \\mathbb{R}^{d \\times h \\times w}$ , and then transfer this continuous features into discrete space by looking up the closest features in the codebook  $\\mathcal{Z} = \\{z_k\\} \\in \\mathbb{R}^{K \\times d}$  to obtain the tokens  $\\pmb{z}_q$ :\n\n\n$$\n\n$$\n\\boldsymbol {z} _ {q} = \\operatorname {Q u a n t i s e} (\\boldsymbol {z}) = \\operatorname {Q u a n t i s e} (E (\\boldsymbol {x})) := \\arg \\min  _ {k} \\| z _ {i, j} - z _ {k} \\|, \\tag {1}\n$$\n\n$$\n\n\nwhere the dimensions  $h, w$  of latent feature  $\\mathbf{z}$  are substantially smaller than the original dimensions  $H, W$ . The reconstructions can be obtained through a decoder  $G$ :\n\n\n$$\n\n$$\n\\hat {\\boldsymbol {x}} = G \\left(\\boldsymbol {z} _ {q}\\right) = G (\\text {Q u a n t i s e} (E (\\boldsymbol {x}))). \\tag {2}\n$$\n\n$$\n\n\nRecently, there have been significant advances in learning more compact representation, yet higher-quality reconstruction, such as introducing new losses (Esser et al., 2021a), applying powerful backbone (Yu et al., 2021), using multiple channels representation (Lee et al., 2022) and spatially normalization (Zheng et al., 2022). However, in this paper, we focus on applying the technology in our novel UniD3, rather than exploring a new codebook learning approach.\n\n\nDiscrete Diffusion The discrete diffusion model was originally mentioned in (Sohl-Dickstein et al., 2015), with transitions converging to a binomial distribution. Subsequent work extended this to multinomial diffusion (Hoogeboom et al., 2021; Song et al., 2021a), while Austin et al. (2021) provided more options for transition matrices. Recent works integrated discrete diffusion models with VQ-VAE, allowing for high-quality image synthesis (Gu et al., 2022; Hu et al., 2022).\n\n\nHere we briefly describe the multinomial diffusion with the absorbing state, as employed in VQ-Diffusion (Gu et al., 2022). Besides  $K$  tokens from a discrete VAE, an additional [MASK] token is introduced. The forward process is defined as:\n\n\n$$\n\n$$\nq \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {t - 1}\\right) = \\operatorname {C a t} \\left(\\boldsymbol {x} _ {t}; \\boldsymbol {p} = \\boldsymbol {Q} _ {t} \\boldsymbol {x} _ {t - 1}\\right) = \\boldsymbol {x} _ {t} ^ {T} \\boldsymbol {Q} _ {t} \\boldsymbol {x} _ {t - 1}, \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\pmb{x}$  is a one-hot vector identifying the token index. Here  $[Q_t]_{i,j} = q(x_t = i|x_{t - 1} = j)\\in$ $\\mathbb{R}^{(K + 1)\\times (K + 1)}$  is the Markov transition matrix from  $t - 1$  to  $t$ , which can be expressed as:\n\n\n$$\n\n$$\n\\boldsymbol {Q} _ {[ t - 1 \\rightarrow t ]} = \\left[\\begin{array}{c c c c c}\\alpha_ {t} + \\beta_ {t}&\\beta_ {t}&\\beta_ {t}&\\dots&0\\\\\\beta_ {t}&\\alpha_ {t} + \\beta_ {t}&\\beta_ {t}&\\dots&0\\\\\\beta_ {t}&\\beta_ {t}&\\alpha_ {t} + \\beta_ {t}&\\dots&0\\\\\\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\\\gamma_ {t}&\\gamma_ {t}&\\gamma_ {t}&\\dots&1\\end{array}\\right], \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\alpha_{t} \\in [0,1]$  is the probability of retaining the token, and has a probability of  $\\gamma_{t}$  to be replaced by the [MASK] token, leaving a chance of  $\\beta_{t} = (1 - \\alpha_{t} - \\gamma_{t}) / K$  to be diffused.\n\n\nThe posterior of the diffusion process can be formulated as:\n\n\n$$\n\n$$\nq \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}, \\boldsymbol {x} _ {0}\\right) = \\frac {q \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {t - 1} , \\boldsymbol {x} _ {0}\\right) q \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {0}\\right)}{q \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {0}\\right)} = \\frac {\\left(\\boldsymbol {x} _ {t} ^ {T} \\boldsymbol {Q} _ {t} \\boldsymbol {x} _ {t - 1}\\right) \\left(\\boldsymbol {x} _ {t - 1} ^ {T} \\overline {{\\boldsymbol {Q}}} _ {t - 1} \\boldsymbol {x} _ {0}\\right)}{\\boldsymbol {x} _ {t} ^ {T} \\overline {{\\boldsymbol {Q}}} _ {t} \\boldsymbol {x} _ {0}}, \\tag {5}\n$$\n\n$$\n\n\ngiven  $\\overline{\\mathbf{Q}}_t = \\mathbf{Q}_t\\cdot \\cdot \\cdot \\mathbf{Q}_1$  , which can be calculated in closed form, and\n\n\n$$\n\n$$\nq \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {0}\\right) = \\operatorname {C a t} \\left(\\boldsymbol {x} _ {t}; \\boldsymbol {p} = \\overline {{\\boldsymbol {Q}}} _ {t} \\boldsymbol {x} _ {0}\\right) = \\boldsymbol {x} _ {t} ^ {T} \\overline {{\\boldsymbol {Q}}} _ {t} \\boldsymbol {x} _ {0}. \\tag {6}\n$$\n\n$$\n\n\nIn the reverse process, instead of explicitly predicting the posterior using a denoising neural network, the  $x_0$ -parameterisation might increase the stability and permit fast inference (skipping  $\\Delta t$  steps per iteration). The reverse transition with reparameterisation is given as:\n\n\n$$\n\n$$\np _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}\\right) \\propto \\sum_ {\\tilde {\\boldsymbol {x}} _ {0}} q \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}, \\tilde {\\boldsymbol {x}} _ {0}\\right) \\tilde {p _ {\\theta}} \\left(\\tilde {\\boldsymbol {x}} _ {0} \\mid \\boldsymbol {x} _ {t}\\right), \\tag {7}\n$$\n\n$$\n\n\nin which the neural network predicts the logits of the target data  $q(\\pmb{x}_0)$ .\n\n\n# 3 METHOD\n\n\nOur goal is to adapt the discrete diffusion model to learn the joint distribution of linguistic and visual features concurrently. First, we propose a transition matrix that allows the diffusion model to capture the implicit association between text and images. Second, we present a mutual attention transformer architecture with fuse embedding layer as the denoising function and a unified objective function, which fits our unified diffusion process objective and permits for more precise predictions.\n\n\nOur overall pipeline is illustrated in Fig. 2. Specifically, our solution begins by compressing the figures and texts into discrete token sequences using dVAE and BPE, respectively. A robust diffusion model with unified transition matrix is then constructed to fit the joint distribution across different modalities, which is further empowered by a transformer with a mutual attention mechanism.\n\n\n# 3.1 UNIFIED DIFFUSION PROCESS\n\n\nUnified Transition Matrix Discrete diffusion models with transition matrices can capture global links. The presence of a transition matrix determines the nature of the discrete diffusion model, which also provides us with more choices for token evolution. Thus we may wonder if it is feasible to design transition matrices that capture the global connections between various modalities.\n\n\nThe Markov transition matrix of the discrete diffusion model should satisfy the following requirements: 1. each column in  $Q_{t}$  should sum to one to conserve probability mass; 2. each column in the cumulative-product  $\\overline{Q}_{t}$  should converge to either a known stationary distribution or a learnt prior when  $t$  becomes large.\n\n\nOn the basis of these criteria, we construct a unified transition matrix  $[Q_t]_{i,j} = q(x_t = i|x_{t - 1} = j)$  capable of encapsulating discrete representations among various modalities. The following matrix  $Q_{t}\\in \\mathbb{R}^{(K + K^{*} + 1)\\times (K + K^{*} + 1)}$  illustrates a unified transition process with only text and image modalities:\n\n\n$$\n\n$$\n\\boldsymbol {Q} _ {t} = \\left[ \\begin{array}{c c c c c c c c c c c} \\alpha_ {t} + \\beta_ {t} & \\beta_ {t} & \\beta_ {t} & \\dots & \\beta_ {t} & 0 & 0 & 0 & \\dots & 0 & 0 \\\\ \\beta_ {t} & \\alpha_ {t} + \\beta_ {t} & \\beta_ {t} & \\dots & \\beta_ {t} & 0 & 0 & 0 & \\dots & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \\beta_ {t} & \\beta_ {t} & \\beta_ {t} & \\dots & \\alpha_ {t} + \\beta_ {t} & 0 & 0 & 0 & \\dots & 0 & 0 \\\\ \\hline 0 & 0 & 0 & \\dots & 0 & \\alpha_ {t} + \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\dots & \\beta_ {t} ^ {*} \\\\ 0 & 0 & 0 & \\dots & 0 & \\beta_ {t} ^ {*} & \\alpha_ {t} + \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\dots & \\beta_ {t} ^ {*} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\dots & 0 & \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\dots & \\alpha_ {t} + \\beta_ {t} ^ {*} \\\\ \\hline \\gamma_ {t} & \\gamma_ {t} & \\gamma_ {t} & \\dots & \\gamma_ {t} & \\gamma_ {t} & \\gamma_ {t} & \\gamma_ {t} & \\dots & \\gamma_ {t} \\\\ \\end{array} \\right],\n$$\n\n$$\n\n\nwhere  $\\alpha_{t} \\in [0,1]$  is the probability to keep this token,  $\\beta_{t}$  and  $\\beta_{t}^{*}$  are the probabilities of a token to be replaced by any other accessible tokens in different modality, and  $\\gamma_{t}$  is the absorbing probability.\n\n\nThe dimension of the matrix  $Q_{t}$  is  $(K + K^{*} + 1) \\times (K + K^{*} + 1)$ , where  $K$  and  $K^{*}$  are the number of states in different modals respectively, e.g.,  $K$  is the size from the codebook in discrete VAE and  $K^{*}$  is the dictionary size of BPE.\n\n\nThe matrix comprises five sections:\n\n- - The final row and column form a section associated with the transition of the absorbing state. Intuitively, if the token is [MASK] at  $t - 1$  step, then the token must be [MASK] at time  $t$ . Conversely, any other modal token has the equal possibility  $\\gamma_{t}$  of being diffused to [MASK].\n\n- - The remainder of the matrix is composed of four quadrants, the first and third of which are fully zeros. Specifically, these two sub-matrices prevent tokens transitioning from one modality to another. The form of the second and fourth quadrants closely resembles that for multinomial diffusion. Here, in addition to some probability of being converted into a [MASK] token, each token also has some chance of transiting to a different state within the same modality, or remaining unaltered.\n\n- - The dimensions of these four quadrants are not identical, which are  $K^{*} \\times K$ ,  $K \\times K$ ,  $K \\times K^{*}$  and  $K^{*} \\times K^{*}$ , respectively.\n\n\nIt is worth noting that  $\\alpha_{t}$  and  $\\gamma_{t}$  are the same in all modalities, whereas  $\\beta_{t}$  varies based on the number of states in different modalities. Mathematically,  $\\beta_{t} = (1 - \\alpha_{t} - \\gamma_{t}) / K$  and  $\\beta_{t}^{*} = (1 - \\alpha_{t} - \\gamma_{t}) / K^{*}$ .\n\n\nThe sum of each column in this transition matrix is one to preserve probability mass, and also all the mass of the stationary distribution falls on the [MASK] token, which satisfies the prerequisite for a discrete diffusion model transition matrix.\n\n\nThe computation of  $\\overline{\\mathbf{Q}}_t\\mathbf{x}_0$ , needed for  $q(\\mathbf{x}_t|\\mathbf{x}_0)$  in Eq. 6, can be efficiently obtained in closed form:\n\n\n$$\n\n$$\n\\overline {{Q}} _ {t} \\boldsymbol {x} _ {0} = \\overline {{\\alpha}} _ {t} \\boldsymbol {x} _ {0} + \\left[ \\overline {{\\gamma}} _ {t} - \\mathbb {1} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} - \\mathbb {1} ^ {*} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} ^ {*} \\right] \\boldsymbol {x} _ {[ \\mathbb {M} ]} + \\mathbb {1} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} + \\mathbb {1} ^ {*} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} ^ {*},\n$$\n\n$$\n\n\nwhere  $\\mathbb{1}(\\pmb{x}_0) = \\left\\{ \\begin{array}{ll} 1 & \\text{if argmax } \\pmb{x}_0 \\in [0, K), \\\\ 0 & \\text{otherwise.} \\end{array} \\right.$ ,  $\\mathbb{1}^*(\\pmb{x}_0) = \\left\\{ \\begin{array}{ll} 1 & \\text{if argmax } \\pmb{x}_0 \\in [K, K + K^*), \\\\ 0 & \\text{otherwise.} \\end{array} \\right.$ ,\n\n\n$$\n\n$$\n\\boldsymbol {x} _ {[ \\mathbb {M} ]} = \\boldsymbol {x} \\leftarrow \\operatorname {a r g m a x} \\boldsymbol {x} = K + K ^ {*} \\text {a n d} \\bar {\\alpha} _ {t}, \\bar {\\beta} _ {t}, \\bar {\\beta} _ {t} ^ {*}, \\bar {\\gamma} _ {t} \\text {a r e t h e c o r r e s p o n d i n g c u m u l a t i v e p r o d u c t .} \\tag {9}\n$$\n\n$$\n\n\nThe detailed proof is provided in Appendix E.\n\n\nUnified Objective For conditional generation tasks such as text-to-image generation, the goal is to maximize  $p(\\pmb{x}|\\pmb{y})$  by finding  $\\pmb{x}$ , where  $\\pmb{y}$  is the given text condition. In our task, it can be approximated that the model try to maximize the joint distribution  $p(\\pmb{x},\\pmb{y})$  simultaneously. In practice, we minimize the Kullback-Leibler divergence between  $q(\\pmb{x}_{t - 1}|\\pmb{x}_t,\\pmb{x}_0)$  and  $p_{\\theta}(\\pmb{x}_{t - 1}|\\pmb{x}_t)$  in both image and text directions, as shown in the following:\n\n\n$$\n\n$$\n\\mathcal {L} _ {0} = - \\mathbb {E} _ {q \\left(\\boldsymbol {x} _ {1} \\mid \\boldsymbol {x} _ {0}\\right)} \\left[ \\log p _ {\\theta} \\left(\\boldsymbol {x} _ {0} ^ {i m g} \\mid \\boldsymbol {x} _ {1}, \\boldsymbol {x} _ {0} ^ {t x t}\\right) + \\log p _ {\\theta} \\left(\\boldsymbol {x} _ {0} ^ {t x t} \\mid \\boldsymbol {x} _ {1}, \\boldsymbol {x} _ {0} ^ {i m g}\\right) \\right], \\tag {10}\n$$\n\n$$\n\n\n$$\n\n$$\n\\mathcal {L} _ {t - 1} = \\mathbb {E} _ {q \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {0}\\right)} \\left[ D _ {\\mathrm {K L}} \\left(q \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}, \\boldsymbol {x} _ {0}\\right) \\| \\left[ p _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {i m g} \\mid \\boldsymbol {x} _ {t}\\right); p _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {t x t} \\mid \\boldsymbol {x} _ {t}\\right) \\right]\\right) \\right], \\tag {11}\n$$\n\n$$\n\n\nwhere  $p_{\\theta}(\\boldsymbol{x}_{t-1})$  is the integration of the logits  $p_{\\theta}(\\boldsymbol{x}_{t-1}^{img})$  and  $p_{\\theta}(\\boldsymbol{x}_{t-1}^{tax})$  from separate modal, and they can be obtained with  $\\boldsymbol{x}_0$ -parameterisation as following:\n\n\n$$\n\n$$\np _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {i m g} \\mid \\boldsymbol {x} _ {t}\\right) \\propto \\sum_ {\\tilde {\\boldsymbol {x}} _ {0} ^ {i m g}} q \\left(\\boldsymbol {x} _ {t - 1} ^ {i m g} \\mid \\boldsymbol {x} _ {t}, \\tilde {\\boldsymbol {x}} _ {0} ^ {i m g}, \\boldsymbol {x} _ {0} ^ {t x t}\\right) \\tilde {p _ {\\theta}} \\left(\\tilde {\\boldsymbol {x}} _ {0} ^ {i m g} \\mid \\boldsymbol {x} _ {t}\\right), \\tag {12}\n$$\n\n$$\n\n\n$$\n\n$$\np _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {t x t} \\mid \\boldsymbol {x} _ {t}\\right) \\propto \\sum_ {\\tilde {\\boldsymbol {x}} _ {0} ^ {t x t}} q \\left(\\boldsymbol {x} _ {t - 1} ^ {t x t} \\mid \\boldsymbol {x} _ {t}, \\tilde {\\boldsymbol {x}} _ {0} ^ {t x t}, \\boldsymbol {x} _ {0} ^ {i m g}\\right) \\tilde {p} _ {\\theta} \\left(\\tilde {\\boldsymbol {x}} _ {0} ^ {t x t} \\mid \\boldsymbol {x} _ {t}\\right). \\tag {13}\n$$\n\n$$\n\n\nDue to the  $x_0$ -parameterisation, our model is also capable of achieving fast sampling by increasing the step size. And the last term of the variational lower bound loss  $\\mathcal{L}_T$  is a constant and can be ignored during the training:\n\n\n$$\n\n$$\n\\mathcal {L} _ {T} = D _ {\\mathrm {K L}} \\left(q \\left(\\boldsymbol {x} _ {T} \\mid \\boldsymbol {x} _ {0}\\right) \\| p \\left(\\boldsymbol {x} _ {T}\\right)\\right), \\tag {14}\n$$\n\n$$\n\n\nas the prior  $p(\\boldsymbol{x}_T)$  is fixed with the unified transition matrix:\n\n\n$$\n\n$$\np \\left(\\boldsymbol {x} _ {T}\\right) = \\left[ \\bar {\\beta} _ {T}, \\bar {\\beta} _ {T}, \\dots , \\bar {\\beta} _ {T} ^ {*}, \\bar {\\beta} _ {T} ^ {*}, \\dots , \\bar {\\gamma} _ {T} \\right]. \\tag {15}\n$$\n\n$$\n\n\nThe full expression of the loss function can be found in Appendix C.\n\n\n# 3.2 DENOISING FUNCTION FOR MULTIMODAL\n\n\nMutual Attention As indicated by Eqs. 12 & 13, the neural network is responsible for prediction of the distribution  $\\tilde{p}_{\\theta}(\\tilde{\\boldsymbol{x}}_0|\\boldsymbol{x}_t)$ . However, the input to the neural network covers all modalities, and a simple self-attention mechanism can scarcely highlight the inter-modal linkages. In addition, the distributions predicted by the neural network need to be decoupled according to the various modalities throughout the reparameterisation. In other words, the network predictions should be expressed in terms of different modalities, e.g.,  $\\tilde{p}_{\\theta}(\\tilde{\\boldsymbol{x}}_0^{img}|\\boldsymbol{x}_t)$  and  $\\tilde{p}_{\\theta}(\\tilde{\\boldsymbol{x}}_0^{txt}|\\boldsymbol{x}_t)$ , with interconnections.\n\n\nTherefore, we propose the mutual attention mechanism and construct the unified transformer as shown in Fig. 3. The unified transformer contains several transformer blocks, each of which consists of one self-attention and two parallel mutual attention operations, and one feed-forward module. Each block receives a sequence of mixed-modal tokens as input that traverses a layer of self-attention to capture the inherent connection within the modalities. Afterwards, the hidden tokens are decoupled according to the location of the different modalities and fed to the corresponding mutual attention layers. Mutual attention is a modified version of the cross-attention layer, with the conditional inputs to cross-attention being maintained constant while the inputs to mutual attention are derived from hidden features. Next, the outputs from the various mutual attention are concatenated into one mixed-modal token sequence for transmission to the next block. At the end of the unified transformer, we layer-normalise the sequence of tokens from the blocks, which is then decoupled to different predictive distributions by fully-connected layers.\n\n\nIn the conventional multi-model generative model, conditional modality  $C$  is given and fixed. We have the Cross attention block to inject the conditional information  $C$  into the target modality  $X$ :\n\n\n$$\n\n$$\n\\operatorname {C A} (X, C) = \\operatorname {A t t n} (X, C; W) = \\operatorname {s o f t m a x} \\left(\\frac {Q K ^ {T}}{\\sqrt {2}}\\right) V, \\tag {16}\n$$\n\n$$\n\n\nwhere  $Q = W_{Q}X, K = W_{K}C \\& V = W_{V}C$ , and  $W$  is the learnable parameters. Noted that the cross attention  $CA(X,C)$  could convert to self-attention  $SA(X)$  when  $X = C$ .\n\n\nIn our model, each modality may become a component of what needs to be generated. Therefore, we propose mutual attention to enable tokens of different modalities in a sequence to be conditional on each other, allowing the capture of the relationships between the various modalities. Mathematically, our mutual attention can be expressed as\n\n\n$$\n\n$$\n\\mathrm {M A} (T _ {i}, T _ {j}) = \\operatorname {A t t n} (T _ {i}, T _ {j}; W) = \\operatorname {s o f t m a x} (\\frac {Q _ {i} K _ {j} ^ {T}}{\\sqrt {2}}) V _ {j},\n$$\n\n$$\n\n\nwhere  $Q_{i} = W_{Q}T_{i}, K = W_{K}T_{j} \\& V = W_{V}T_{j}$ , and  $T_{i} \\& T_{j}$  are the tokens in a different modality, e.g. image and text. The Unified block contains a self-attention and several mutual attention in parallel. Given a hidden token vector  $H' = [H_i', H_j']$ , the whole pipeline of Unified block is\n\n\n$$\n\n$$\n\\begin{array}{l} 1. T ^ {\\prime} = S A \\left(H ^ {\\prime}\\right) \\\\ 2. D e c o u p l e: T ^ {\\prime} \\rightarrow T _ {i} ^ {\\prime}, T _ {j} ^ {\\prime} \\\\ 3. \\left\\{ \\begin{array}{l} T _ {i} = \\operatorname {M A} \\left(T _ {i} ^ {\\prime}, T _ {j} ^ {\\prime}\\right) \\\\ T _ {j} = \\operatorname {M A} \\left(T _ {j} ^ {\\prime}, T _ {i} ^ {\\prime}\\right) \\end{array} \\right. \\\\ 4. C o u p l e: T \\leftarrow T _ {i}, T _ {j} \\\\ \\end{array}\n$$\n\n$$\n\n\n![this bird has a white belly, a spotted breast, a short tail, and pointed bill.](images/be3ba2e73ec33a7e8f0c7659132ab3d83c39a3321a6ccb02bc2238cac405112d.jpg)\n\n\n![a white bird with a black crown and a long beak. this bird is all black and has a small yellow eyes and white beak.](images/522ae7abc6e60b30bd016da5a1f6e902067ea2c124a985b3257b0920ddec7cae.jpg)\n\n\n![a clean and tide kitchen with wooden cabinets for use.](images/5ce07f9fef59c0e50194be76fbb3baed0fdcb7fb136572eb3c0ec9c755dedc11.jpg)\n\n\n![a tall giraffe is standing in a forest.](images/865e92291e7a191fb329a9789ce636bce0a2c0ff03f05db3160b52fd080415b3.jpg)\n\n\n![a tall building with a park area.](images/72be18aa80d83d42df4eccb6d7f61a4c8bcde972dadcf62f5180c0b95b49eca7.jpg)\n\n\n![](images/527131bb59ea1574d8f2ab9b790cbbb8f8b2fae64cdb22b79d2355378e8b0dc3.jpg)\n\n\n![this bird has wings that are grey and has a white belly. the small bird is mostly brown and has a light red head. Figure 4: Generated vision-language Pairs from CUB-200 and MSCOCO. Both the image and caption are generated simultaneously. The quality of the created photos and text is comprehensible, and there is a correlation between the descriptions and the visuals.](images/95923822c90433451db30ebe5a577a9108e8f4e50a38eff6f2da664498b46b10.jpg)\n\n\n![this bird has wings that are black and has a yellow and black belly.](images/e057321fedae1534a673629a4c1dc5ce5cc5cfa38484efbbcb7a1f74f551300f.jpg)\n\n\n![colorful flowers in an orange vase on the window.](images/b06ee4ec99a9307d24bc28278d4fe8f9ecf1809cf7ca185efbcc332cfadbb331.jpg)\n\n\n![there is a bench by the trees in the seaside park.](images/c7bbed106ab869a47e6ef9a61541aa80f7844489a65a671a9da0814a9351068f.jpg)\n\n\n![a window on the front of a brick building.](images/ff73cc3f4fd15a16f9cce2924de2ae597fcdb6ac1632b0ce25c4f879291152aa.jpg)\n\n\n![](images/b3096ae11cbdf82c0e3fc7712ceb28743f0a864e9e13248109d4e74336bad61d.jpg)\n\n\nFused Embedding In addition, discrete tokens of various modalities should be embedded in the same space. Considering that two modalities have  $K$  and  $K^{*}$  states respectively, we firstly create an embedding layer with size  $K + K^{*} + 1$ , with the additional state for the [MASK] token. We then create a learnable spatial position encoding for the image modality and a learnable sequence position encoding for the text modality. The final fused embedding is obtained by adding the embedded vectors to the positional encoding of their associated modalities.\n\n\n# 4 EXPERIMENTS\n\n\nThe description of the datasets and model experimental details can be found in Appendix B.\n\n\n# 4.1 QUANTITATIVE RESULTS\n\n\nIn the unconditional scenario, our model is able to sample from all modalities while maintaining the relationship between them. For conditional generation, we conducted experiments on text-based image generation and image-based caption generation, respectively.\n\n\nMetrics We evaluate the multimodal capacity of our model in three distinct areas: Image sampling quality, text sampling quality and the similarity between sampled image and text. Fr\u00e9chet inception distance (FID) (Heusel et al., 2017) and Inception Score (IS) (Salimans et al., 2016) are used to evaluate the realism and variety of the produced images. The BLEU-4, METEOR and SPICE are used to assess the quality of a picture caption. And CLIP (Radford et al., 2021) is used to identify the similarity between visuals and text.\n\n\nImage Quality We quantitatively compare our unconditional and text-conditional results with several state-of-the-art text-to-image and bidirectional conditional generation methods, including GAN-based methods, diffusion-based methods and autoregressive methods. The comparison results are given in Table 1, in which  $\\text{Pair}$  means the unconditional generated vision-language pairings and  $T2I$  represents the text-to-image generation.\n\n\nThe first section of the table solely covers modality translation methods. In the second part, these models are claimed to be endowed with cross modal capabilities. However, such methods are not able to achieve simultaneous vision-language pair generation and necessitate given signals. For a fair\n\n\n![Figure 5: Presentation of the results of cross modal vision and language manipulation and infilling. The dark brown portions of the image and description represent the [MASK], while the strike-through represent the caption manipulation. Image and text compliment each other simultaneously.](images/0a15e5ddbaeffe1921a7d22d22d77a10c46d0440e523142f380ea33d09fe0a93.jpg)\n\n\ncomparison, we exhibit the quantitative generative results based on various modalities individually. We provide more experimental results for text conditional generation in Appendix G.2\n\n\n<table><tr><td rowspan=\"2\"></td><td colspan=\"2\">CUB</td><td colspan=\"2\">MSCOCO</td></tr><tr><td>FID (\u2193)</td><td>IS (\u2191)</td><td>FID (\u2193)</td><td>IS (\u2191)</td></tr><tr><td>DM-GAN (Zhu et al., 2019)</td><td>16.09</td><td>4.75</td><td>32.64</td><td>27.3</td></tr><tr><td>DF-GAN (Tao et al., 2022)</td><td>14.81</td><td>5.1</td><td>21.42</td><td>-</td></tr><tr><td>VQ-Diffusion (Gu et al., 2022)</td><td>11.94</td><td>-</td><td>19.75</td><td>-</td></tr><tr><td>LDM w/o G (Rombach et al., 2022)</td><td>-</td><td>-</td><td>23.31</td><td>20.23</td></tr><tr><td>LAFITE(LF) (Zhou et al., 2022)</td><td>27.53</td><td>4.32</td><td>18.04</td><td>27.2</td></tr><tr><td>Unifying (Huang et al., 2021)</td><td>-</td><td>-</td><td>29.9</td><td>-</td></tr><tr><td>L-Verse (Kim et al., 2022)</td><td>-</td><td>-</td><td>45.8</td><td>-</td></tr><tr><td>OFA (Wang et al., 2022)</td><td>-</td><td>-</td><td>10.5</td><td>31.1</td></tr><tr><td>Ours(Pair)</td><td>17.38</td><td>6.11</td><td>28.63</td><td>24.44</td></tr><tr><td>Ours(T2I)</td><td>16.19</td><td>6.02</td><td>25.11</td><td>21.03</td></tr></table>\n\n\nTable 1: Evaluation of image synthesis on the CUB-200-2011 and MS-COCO dataset. The first section is the text-conditioned results while the model in second part includes multimodal solution.\n\n\n<table><tr><td></td><td>B-4 (\u2191)</td><td>M (\u2191)</td><td>S (\u2191)</td></tr><tr><td>BUTD (Anderson et al., 2018)</td><td>36.3</td><td>27.7</td><td>21.4</td></tr><tr><td>ORT (Herdade et al., 2019)</td><td>38.6</td><td>28.7</td><td>22.6</td></tr><tr><td>AoA (Huang et al., 2019)</td><td>38.9</td><td>29.3</td><td>-</td></tr><tr><td>X-LAN (Pan et al., 2020)</td><td>39.7</td><td>29.5</td><td>23.4</td></tr><tr><td>SimVLM (Wang et al., 2021)</td><td>40.6</td><td>33.7</td><td>25.4</td></tr><tr><td>OSCAR (Li et al., 2020)</td><td>40.5</td><td>29.7</td><td>22.8</td></tr><tr><td>Unifying (Huang et al., 2021)</td><td>37.3</td><td>28.2</td><td>21.9</td></tr><tr><td>L-verse (Kim et al., 2022)</td><td>39.9</td><td>31.4</td><td>23.3</td></tr><tr><td>OFA (Wang et al., 2022)</td><td>41.0</td><td>30.9</td><td>24.2</td></tr><tr><td>Ours(I2T)</td><td>39.6</td><td>29.3</td><td>23.4</td></tr></table>\n\n\nTable 2: Evaluation of image caption on the MSCOCO Karpathy split. The first section is the text-conditioned results while the model in second part includes multimodal solution.\n\n\nText Quality In the case of image captions, we compared several other text description methods, the details are shown in Table 2, where  $I2T$  is the image-based text-generation task. Similarly, the first part of the table has pure image caption solutions and the bi-directional methods are demonstrated in the second part. Some results for image caption tasks are provided in Appendix G.1\n\n\nSimilarity In the multimodal generation phase, all the other methods for comparison need the input of at least one specified modality; instead our method generates both visual and verbal features. The generated vision-language pairings are shown in Fig. 4 while more pair samples are provided in Appendix G.4. In this experiment, we use CLIP scores to evaluate the similarity between our generated visual and verbal modalities, while generated images with given texts were used for other methods. The comparison results are given in Table 3, where a higher correlation between the text/image samples of our model can be found.\n\n\n# 4.2 CROSS MODAL MANIPULATION\n\n\nTo demonstrate the modification potential of our vision-language federation, we present the outcomes of picture alteration with description modification in Fig. 5 and more samples can be found in Appendix G.3. In this experiment, we obscured a portion of the image and modified the text description as well, the tokens in the masked region are merely designated as [MASK] tokens. We anticipate that the model will complete the image based on the amended text, in conjunction with the description based on the visible image region. Depending on the unmasked images, our model may supplement the caption that is derived from a portion of the image. Moreover, based on the amended description, the model may enhance the appearance of the masked component. These findings indicate that our model is capable of modifying text and pictures in both ways, in addition to jointly generating text and images.\n\n\n# 4.3 ABLATION STUDY\n\n\nIn the ablation test, we utilized a rapid sampling scheme, with an interval between each step of 10. First, we evaluated the outcomes without mutual attention, where we replaced all mutual attention in the model structure with causal self-attention. In addition, we evaluated the performance without the unified transition matrix, we employed a matrix similar to Equation 4, with the out-of-bound outputs substituted by the minimal ordinal number of the corresponding modal.\n\n\n<table><tr><td>(\u2191)</td><td>CUB</td><td>COCO</td></tr><tr><td>Eval set</td><td>0.286</td><td>0.306</td></tr><tr><td>VQ-Diff (Gu et al., 2022)</td><td>-</td><td>0.257</td></tr><tr><td>iVQ-Diff (Tang et al., 2022)</td><td>-</td><td>0.304</td></tr><tr><td>Unifying (Huang et al., 2021)</td><td>-</td><td>0.309</td></tr><tr><td>OFA (Wang et al., 2022)</td><td>-</td><td>0.344</td></tr><tr><td>Ours(Pair)</td><td>0.280</td><td>0.301</td></tr><tr><td>Ours(T2I)</td><td>0.293</td><td>0.306</td></tr><tr><td>Ours(I2T)</td><td>0.302</td><td>0.312</td></tr></table>\n\n\nTable 3: The CLIP similarity between the generated captions and images.\n\n\n<table><tr><td>(\u2193)</td><td>FID</td></tr><tr><td>Ours</td><td>17.38</td></tr><tr><td>w/ Fast Sampling \u0394 t = 10</td><td>20.96</td></tr><tr><td>w/o Mutual Attention</td><td>23.19</td></tr><tr><td>w/o Unified Transition Matrix</td><td>32.63</td></tr><tr><td>Ours (T2I)</td><td>16.19</td></tr><tr><td>w/ Fast Sampling \u0394 t = 10</td><td>18.33</td></tr><tr><td>w/o Mutual Attention</td><td>21.65</td></tr><tr><td>w/o Unified Transition Matrix</td><td>28.93</td></tr></table>\n\n\nTable 4: Ablations studies. The experiments are conducted on the CUB-200.\n\n\n# 5 RELATED WORK\n\n\nDenoising Diffusion Model Sohl-Dickstein et al. (2015) initially presented the diffusion model, which consists of a base Gaussian diffusion and a basic binomial diffusion. Ho et al. (2020) recently reigned interest in the extraordinary generating capacity of diffusion models. This is extended in subsequent research to enhance quality with the classifier guidance (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021) and speed of sampling (Song et al., 2021a; Salimans & Ho, 2021). In addition to continuous diffusion models that have received the significant attention, there is also substantial research based on discrete diffusion models (Hoogeboom et al., 2021; Austin et al., 2021) for text generation or image segmentation. With the help of VQ-VAE, Esser et al. (2021a); Hu et al. (2022); Gu et al. (2022); Tang et al. (2022) discrete diffusion models have become capable of generating high quality images.\n\n\nModality Translation Text-to-image generation is one of the key aspects of modality translation tasks. On simpler datasets, traditional GAN-based models (Xu et al., 2018; Zhu et al., 2019) may create high-quality pictures based on text hints, but face difficulties on more complex datasets. As a result of the emergence of transformers employing the attention mechanism (Vaswani et al., 2017), conventional autoregressive models (Van den Oord et al., 2016) have gained more powerful generating capacities as ART (Auto-regressive Transformers) (Ramesh et al., 2021; Esser et al., 2021b; Yu et al., 2022). Other types of modality translation models involve the addition of prospective modalities as criteria for improved generation outcomes (Gafni et al., 2022). There are also some works that focus on addressing bidirectional generation between different modalities (Kim et al., 2022; Huang et al., 2021; Wang et al., 2022); however, these solutions are task-specific, necessitating different input formats and specific models with distinct modalities.\n\n\n# 6 CONCLUSION\n\n\nThis work presents UniD3, a novel unified framework for multi-modality generation tasks. We firstly introduced a unified transition matrix for the diffusion process which permits the states of fused tokens to be corrupted in a stable way. We demonstrated that designing transition matrices based on the objective and data characteristics is beneficial for discrete diffusion models. To capture the connection from the various modalities, we suggested a mutual attention with fuse embedding for noiseless state recovery. Our method is also generic and competent to perform modality translation tasks for the specified modality. The specific designs of transition matrix are intriguing for further exploring, as well as the performance of more modalities is also one of the future directions.\n\n\n# 7 ETHICS\n\n\nIn this work, we propose UniD3, a new method for multimodal multi-task generation based on the discrete denoising diffusion model. All datasets evaluated in our experiments are open source, publicly accessible, and used with permission.\n\n\nSimilar to other generation algorithms, our method has both beneficial and negative societal effects, depending on its application and usage.\n\n- - Positively, UniD3 may explore the limits of deep learning generation through a unique pattern of graphic pair generation, while letting the typical user's imagination run wild and decreasing the entrance barrier for visual content production.\n\n- - On the other hand, UniD3 may be used to make modified photographs of great quality that are difficult to discern from the originals. Ways to fool humans and propagate falsehoods that are malicious. UniD3 can be abused by nefarious users, which may have serious repercussions.\n\n- - In our code distribution. We shall outline the permitted uses of our system and give the corresponding licences.\n\n\nHowever, we observe that the present discriminators used to distinguish generated images are ineffective at identifying images generated by diffusion models. There is still the necessity for more exploration to discern between authentic and fake images.\n\n\n# REFERENCES\n\n- Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6077-6086, 2018.\n\n- Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981-17993, 2021.\n\n- Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18208-18218, 2022.\n\n- Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780-8794, 2021.\n\n- Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis. Advances in Neural Information Processing Systems, 34, 2021a.\n\n- Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873-12883, 2021b.\n\n- Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203.13131, 2022.\n\n- Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696-10706, 2022.\n\n- Simao Herdade, Armin Kappeler, Kofi Boakye, and Joao Soares. Image captioning: Transforming objects into words. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\n- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020.\n\n- Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454-12465, 2021.\n\n- Minghui Hu, Yujie Wang, Tat-Jen Cham, Jianfei Yang, and Ponnuthurai N Suganthan. Global context with discrete diffusion in vector quantised modelling for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11502-11511, 2022.\n\n- Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. Attention on attention for image captioning. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4634-4643, 2019.\n\n- Yupan Huang, Hongwei Xue, Bei Liu, and Yutong Lu. Unifying multimodal transformer for bidirectional image and text generation. In Proceedings of the 29th ACM International Conference on Multimedia, pp. 1138-1147, 2021.\n\n- Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.\n\n- Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, and Kyunghoon Bae. L-verse: Bidirectional generation between image and text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16526-16536, 2022.\n\n- Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, et al. Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages, 2(3):18, 2017.\n\n- Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11523-11532, 2022.\n\n- Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pp. 121-137. Springer, 2020.\n\n- Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr D\u00f3ll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740-755. Springer, 2014.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.\n\n- Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11461-11471, 2022.\n\n- Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pp. 8162-8171. PMLR, 2021.\n\n- Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks for image captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10971-10980, 2020.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. PMLR, 2021.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.\n\n- Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video transformer. In VISIGRAPP (5: VISAPP), 2021.\n\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821-8831. PMLR, 2021.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684-10695, 2022.\n\n- Chitwan Sahara, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pp. 1-10, 2022a.\n\n- Chitwan Sahara, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b.\n\n- Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2021.\n\n- Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.\n\n- Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256-2265. PMLR, 2015.\n\n- Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021a.\n\n- Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021b.\n\n- Zhicong Tang, Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen. Improved vector quantized diffusion models. arXiv preprint arXiv:2205.16007, 2022.\n\n- Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A simple and effective baseline for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16515-16525, 2022.\n\n- Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelCNN decoders. Advances in neural information processing systems, 29, 2016.\n\n- Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\n- Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. California Institute of Technology, 2011.\n\n- Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pp. 23318-23340. PMLR, 2022.\n\n- Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In International Conference on Learning Representations, 2021.\n\n- Wansen Wu, Tao Chang, and Xinmeng Li. Visual-and-language navigation: A survey and taxonomy. arXiv preprint arXiv:2108.11544, 2021.\n\n- Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attingan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1316-1324, 2018.\n\n- Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n\n- Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In International Conference on Learning Representations, 2021.\n\n- Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\n\n- Chuanxia Zheng, Long Tung Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35, 2022.\n\n- Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17907-17917, 2022.\n\n- Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5802-5810, 2019.\n\nResponse: Given the comprehensive evaluation criteria and the content of the paper, here's a systematic analysis leading to my prediction:\n\n### Originality\n- **Strength**: The paper introduces a novel unified multimodal generation framework called UniD3, which combines discrete diffusion models with BPE and dVAE to handle text and image generation simultaneously. This is indeed an innovative approach that bridges the gap between text and image generation.\n- **Weakness**: While the idea is novel, it doesn't introduce entirely new methodologies or technologies but rather combines existing techniques in a novel way.\n\n### Methodological Rigor\n- **Strength**: The authors provide a detailed explanation of their unified transition matrix, mutual attention mechanism, and the unified objective function. They justify why these components are necessary for their unified diffusion process.\n- **Weakness**: The technical details could be more rigorous, especially in explaining how the transition matrix captures the inter-modal linkages and how the mutual attention mechanism ensures the correct coupling of different modalities. More extensive experimental validation would strengthen the methodology.\n\n### Significance of Contribution\n- **Strength**: The model achieves state-of-the-art results in various tasks, including unconditional generation, text-based image generation, and image-based captioning. It also demonstrates the ability to perform cross-modal manipulations, which is crucial for real-world applications like photo editing and augmented reality.\n- **Weakness", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634728.0312471", "msecs": "31.0", "relativeCreated": "72854.72130775452", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Optimal Transport for Long-Tailed Recognition with Learnable Cost Matrix\n\n# Abstract\nIt is attracting attention to the long-tailed recognition problem, a burning issue that has become very popular recently. Distinctive from conventional recognition is that it posits that the allocation of the training set is supremely distorted. Predictably, it will pose challenges to the generalisation behaviour of the model. Approaches to these challenges revolve into two groups: firstly, training-aware methods, with the aim of enhancing the generalisability of the model by exploiting its potential in the training period; and secondly, post-hoc correction, liberally coupled with training-aware methods, which is intended to refine the predictions to the extent possible in the post-processing stage, offering the advantages of simplicity and effectiveness. This paper introduces an alternative direction to do the post-hoc correction, which goes beyond the statistical methods. Mathematically, we approach this issue from the perspective of optimal transport (OT), yet, choosing the exact cost matrix when applying OT is challenging and requires expert knowledge of various tasks. To overcome this limitation, we propose to employ linear mapping to learn the cost matrix without necessary configurations adaptively. Testing our methods in practice, along with high efficiency and excellent performance, our method surpasses all previous methods and has the best performance to date.\n\n\n# 1 INTRODUCTION\n\n\nClassification problems in the real world are generally challenged by the long-tailed label distribution, i.e., having a small number of samples for a majority of labels, and a dominant number of samples for a minority of labels (Van Horn & Perona, 2017; Buda et al., 2018; Liu et al., 2019) on the training data set. It is also known as imbalanced recognition, which has been widely studied in the past decades (Cardie & Howe, 1997; Chawla et al., 2002; Qiao & Liu, 2009; Cui et al., 2019). These distribution biases pose a significant challenge to predictive modeling; conceivably, models often suffer from poor generalisation and undesirable estimation bias (Cao et al., 2019; Kang et al., 2019; Zhou et al., 2020).\n\n\nRecently, a renewed interest in the problem of long-tail recognition has emerged following the context of neural networks, as numerous publications in the literature endeavour to resolve the problem albeit in different ways including decouple (Kang et al., 2019), meta-learning (Ren et al., 2020; Wang et al., 2020a; Li et al., 2021), post-hoc correction (Tang et al., 2020; Hong et al., 2020), etc (Liu et al., 2019; Cao et al., 2019; Tang et al., 2020). One of the representative methods of post-hoc correction, Logit Adjustment Menon et al. (2020), provides a statistical correction to the prediction, receiving widespread attention for its simplicity and validity, but the downside is that its optimisation is conducted on individual samples, without any indication that rectified marginal distribution satisfies the desired distribution we are striving for.\n\n\nFiguring out how to solve the above problem, our explicit modeling of the problem mathematically turns into an equational constraint, meanwhile to minimise the difference between refined distribution and the original one, this minimisation is motivated upon the inner-product similarity. A little further, the resulting problem can be linked to OT. Drawing on this linkage, we develop it further by proposing a linear mapping to automatically learn cost matrix, thereby circumventing the requirement for expert knowledge to configure this matrix. In short, we contribute as follows:\n\n\n- We propose an alternative direction based on convex optimisation to do post-hoc correction, which goes beyond previous direction from the statistical view.\n\n- - Imposing marginal distributions to align ideal marginal distributions, we derive an optimisation problem tied to the OT that is solved using Sinkhorn, a highly efficient algorithm. More further, for better learning of the cost matrix, we present a linear mapping enabling very elegant learning with one-layer neural network.\n\n- - The experimental evidence shows the high efficiency of the algorithm, and the best performance achieved on three benchmarks, also supports our claim that addressing the post-hoc problem via OT is helpful and effective.\n\n\n# 2 PRELIMINARIES\n\n\nIn this section, we begin by giving a notational definition, followed by an introduction to the long-tailed recognition problem, and finally, we briefly review the OT, and give a quick recap of Logit Adjustment Menon et al. (2020).\n\n\nNotations: In what follows, for two matrices  $\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{N \\times K}$ , we denote  $\\langle \\mathbf{X}, \\mathbf{Y} \\rangle = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\mathbf{X}_{nk} \\mathbf{Y}_{nk}$  as the Frobenius dot-product.  $\\delta(\\cdot)$  stands for the Dirac function,  $p(\\cdot)$  represents the probability distribution.  $U(\\boldsymbol{r}, \\boldsymbol{c}) = \\{\\boldsymbol{P} \\in \\mathbb{R}_+^{N \\times K} | \\boldsymbol{P} \\mathbf{1}_K = \\boldsymbol{r}, \\boldsymbol{P}^\\intercal \\mathbf{1}_N = \\boldsymbol{c}\\}$ , where  $\\mathbf{1}_N$  and  $\\mathbf{1}_K$  are  $N$ -dimension and  $K$ -dimension vector whose elements are all 1.  $\\boldsymbol{r}$  and  $\\boldsymbol{c}$  refer to the vectors of size  $N$  and  $K$  respectively,  $U(\\boldsymbol{r}, \\boldsymbol{c})$  include all matrices with row and column sums  $\\boldsymbol{r}$  and  $\\boldsymbol{c}$  respectively\n\n\n# 2.1 PROBLEM FORMULATION\n\n\nHaving a collection of training samples  $\\{(x_n^s, y_n^s)\\}_{n=1}^{N_s}$ , validation samples  $\\{(x_n^v, y_n^v)\\}_{n=1}^{N_v}$  and test samples  $\\{(x_n^t, y_n^t)\\}_{n=1}^{N_t}$  for classification with  $K$  labels and input  $\\mathbf{x} \\in \\mathbb{R}^d$ , long-tailed recognition assumes that the class-prior distribution for training data  $p(y^s)$  is different from that for validation data  $p(y^v)$  and test data  $p(y^t)$ . Specifically, long-tailed recognition means the distribution  $p(y^s)$  is highly skewed, some classes have the dominant number of samples, while tailed labels own a very small number of samples. We can use imbalance ratio to measure the skewness in training data set, which can be defined as  $R = \\frac{N_{max}^s}{N_{min}^s}$ , where  $N_{max}^s$  and  $N_{min}^s$  denote the largest and smallest number of samples in the training data set, respectively. In this paper, we assume that the marginal distribution of the test set is known, we consider it as an implicit prior knowledge to be applied. Stepping back, even if we do not know the marginal distribution of the test dataset in advance. There are still ways to estimate the marginal distribution of the test dataset relatively precisely, such as methods in Hendrycks et al. (2018); Azizzadenesheli et al. (2019).\n\n\nObviously, most models trained on imbalanced training data set would suffer from extremely limited generalisation ability. Hence the ultimate goal is to learn a model that minimises the empirical risk, which can be formulated as:\n\n\n$$\n\n$$\n\\mathcal {J} \\left(\\Phi \\left(\\boldsymbol {x} _ {n} ^ {s}\\right), y _ {n} ^ {s}\\right) = \\frac {1}{N _ {s}} \\sum_ {n = 1} ^ {N _ {s}} \\mathcal {L} \\left(\\Phi \\left(\\boldsymbol {x} _ {n} ^ {s}\\right), y _ {n} ^ {s}\\right), \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\Phi (\\pmb{x}_n^s)\\in \\mathbb{R}^K$  denotes logits with associated sample,  $\\Phi (\\cdot):\\mathbb{R}^d\\to \\mathbb{R}^K$  represents the mapping via neural networks,  $\\mathcal{L}$  stands for the loss function, typically cross entropy loss function for classification problem.\n\n\n# 2.2 REMIN DERS ON OPTIMAL TRANSPORT\n\n\nOT is used to calculate the cost of transporting one probability measure to another. We next present a brief introduction to OT with the aim of helping us better view the long-tailed problem from an OT perspective.\n\n\nConsider probability measures  $r$  and  $c$  of the two random variables  $X$  and  $Y$  respectively, when the cost function  $\\mathcal{C}(X,Y): X \\times Y \\to \\mathbb{R}_+$  is defined to denote the cost of transporting  $X$  to  $Y$ , the OT distance can be defined as:\n\n\n$$\n\n$$\nd (r, c) = \\min  _ {\\boldsymbol {\\pi} \\in \\Pi (r, c)} \\int_ {X \\times Y} \\mathcal {C} (\\boldsymbol {x}, \\boldsymbol {y}) \\pi (\\boldsymbol {x}, \\boldsymbol {y}) d \\boldsymbol {x} d \\boldsymbol {y}, \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\Pi(r, c) = \\left\\{\\int_{\\mathcal{Y}} \\pi(\\boldsymbol{x}, \\boldsymbol{y}) d\\boldsymbol{y} = r, \\int_{X} \\pi(\\boldsymbol{x}, \\boldsymbol{y}) d\\boldsymbol{x} = c\\right\\}$  is the joint probability measure with  $r$  and  $c$ . When we extend the above continuous case to the discrete situation, we consider following discrete distributions:\n\n\n$$\n\n$$\n\\boldsymbol {r} = \\sum_ {i = 1} ^ {N} p _ {i} (\\boldsymbol {x} _ {i}) \\delta (\\boldsymbol {x} _ {i}) \\quad \\boldsymbol {c} = \\sum_ {j = 1} ^ {K} p _ {i} (\\boldsymbol {y} _ {j}) \\delta (\\boldsymbol {y} _ {j}) \\tag {3}\n$$\n\n$$\n\n\nwhere  $p_i(\\boldsymbol{x}_i)$  and  $p_i(\\boldsymbol{y}_j)$  represent the probability mass to the sample  $\\boldsymbol{x}_i$  and  $\\boldsymbol{y}_j$  respectively. In this context, OT distance can be expressed as:\n\n\n$$\n\n$$\nd _ {M} (\\boldsymbol {r}, \\boldsymbol {c}) = \\min  _ {\\boldsymbol {P} \\in U (\\boldsymbol {r}, \\boldsymbol {c})} \\langle \\boldsymbol {P}, \\boldsymbol {M} \\rangle . \\tag {4}\n$$\n\n$$\n\n\nwhere  $M$  stands for the cost matrix constructed by  $M_{ij} = \\mathcal{C}(\\pmb{x}_i, \\pmb{y}_j)$ . The goal of OT is to find a transportation matrix  $\\pmb{P}$  that minimizes the distance  $d_M(\\pmb{r}, \\pmb{c})$\n\n\nAs we can see, OT is a distance measure between two probability distributions under some cost matrix (Villani, 2008). However, when we use network simplex or interior point methods to solve the above optimization problem, it often comes at the cost of heavy computational demands. To tackle this issue, OT with entropy constraint is proposed to allow the optimisation at small computational cost in sufficient smoothness (Cuturi, 2013). By adding a Lagrangian multiplier to the entropy constraint, the new formulation can be defined as follows:\n\n\n$$\n\n$$\nd _ {M} ^ {\\lambda} (\\boldsymbol {r}, \\boldsymbol {c}) = \\langle \\boldsymbol {P} ^ {\\lambda}, \\boldsymbol {M} \\rangle \\quad \\text {w h e r e} \\quad \\boldsymbol {P} ^ {\\lambda} = \\underset {\\boldsymbol {P} \\in U (\\boldsymbol {r}, \\boldsymbol {c})} {\\arg \\min } \\langle \\boldsymbol {P}, \\boldsymbol {M} \\rangle - \\lambda h (\\boldsymbol {P}), \\tag {5}\n$$\n\n$$\n\n\nwhere  $\\lambda \\in [0, +\\infty]$ ,  $h(P) = -\\sum_{n=1}^{N}\\sum_{k=1}^{K}P_{nk}\\log P_{nk}, d_M^\\lambda(r,c)$  is also known as dual-Sinkhorn divergence, besides, it can be calculated with matrix scaling algorithms for cheaper computational demand. The following lemma guarantees the convergence and uniqueness of the solution of the algorithm.\n\n\nLemma 1 For  $\\lambda >0$ , the solution  $P^{\\lambda}$  is unique and has the form  $P^{\\lambda} = \\mathbf{diag}(u)K\\mathbf{diag}(v)$ , where  $\\mathbf{u}$  and  $\\mathbf{v}$  are two non-negative vectors uniquely defined up to a multiplicative factor and  $\\mathbf{K} = e^{-M / \\lambda}$  is the element-wise exponential of  $-M / \\lambda$ .\n\n\nThe above lemma states the uniqueness of  $P^{\\lambda}$  (Sinkhorn, 1974), and  $P^{\\lambda}$  can be efficiently computed via Sinkhorn's fixed point iteration  $\\mathbf{u}, \\mathbf{v} \\gets r. / K\\mathbf{v}, c. / K^{\\intercal}\\mathbf{u}$ .\n\n\n# 2.3 A QUICK RECAP OF LOGIT ADJUSTMENT\n\n\nWe give a brief introduction to Logit Adjustment (Menon et al., 2020; Hong et al., 2020). For the model  $\\Phi(\\cdot)$ , it is trained by the standard cross-entropy loss function on imbalanced training data set, and evaluated on test data. In this algorithm, the test logit is adjusted as follows:\n\n\n$$\n\n$$\n\\Phi \\left(\\boldsymbol {x} _ {n} ^ {t}\\right) = \\Phi \\left(\\boldsymbol {x} _ {n} ^ {t}\\right) - \\log p \\left(y ^ {s}\\right) \\tag {6}\n$$\n\n$$\n\n\nThis simple procedure is derived from the Bayes optimal rule. It is apparent that Logit Adjustment involves a post hoc correction on an individual sample, which does not necessarily guarantee that the marginal distribution of the whole dataset matches the desired distribution.\n\n\n# 3 METHODOLOGY\n\n\nThe first part of this section explores post-hoc correction from an OT perspective, proceeds to the automatic learning of the cost matrix via linear mapping. Lastly, we demonstrate how it can be achieved simply with one-layer neural network.\n\n\n# 3.1 POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE\n\n\nSince Logit Adjustment applies adjustment at the individual sample level. It doesn't assure that the marginal distribution of the overall data set fulfils our desired distribution. In this respect, we clearly put the constraint into an equation:\n\n\n$$\n\n$$\n\\boldsymbol {Y} ^ {\\intercal} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\tag {7}\n$$\n\n$$\n\n\nwhere  $\\mathbf{Y} \\in \\mathbb{R}^{N \\times K}$  indicates the refined prediction value in matrix form,  $\\pmb{\\mu}$  represents the expected distribution on the test set. Alternatively, it is desirable to preserve another characteristic of  $\\mathbf{Y}$ , namely, remaining almost as similar to the original prediction as possible. We consider inner-product based similarity to measure the similarity, which is a straightforward yet useful similarity measure. In mathematical terms, this can be expressed as follows:\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y}} {\\operatorname {m a x i m i z e}} \\quad \\langle C (\\hat {\\boldsymbol {Z}}), \\boldsymbol {Y} \\rangle , \\tag {8}\n$$\n\n$$\n\n\nwhere  $\\hat{Z}$  represents the original prediction in matrix form,  $C(\\cdot)$  denotes to perform some transformation to  $\\hat{Z}$ , it can be some simple function, like Logarithmic function  $\\log(z)$ , exponential function  $z^{\\alpha}$ . Here we select  $-\\log(\\cdot)$  as the cost function. This choice was driven by the requirement that the cost matrix must be positive definite, whereas the transformation of the original prediction by  $-\\log(\\cdot)$  satisfies this condition, in addition, as log likelihood represents the local probability density of the associated samples, it can also be used to substitute  $\\hat{Z}$  for the similarity approximation. In brief, the resulting numerical form can be put in formal terms as follows:\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y}} {\\text {m i n i m i z e}} \\quad \\langle - \\log (\\hat {\\boldsymbol {Z}}), \\boldsymbol {Y} \\rangle \\tag {9}\n$$\n\n$$\n\n\n$$\n\n$$\n\\text {s u b j e c t} \\quad \\boldsymbol {Y} ^ {\\mathsf {T}} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\quad \\boldsymbol {Y} \\mathbf {1} _ {K} = \\mathbf {1} _ {N}. \\tag {10}\n$$\n\n$$\n\n\nExtra constraint on  $\\mathbf{Y}$  are imposed simply cos the tuned estimation has to fulfil the basic probabilistic requirement that its sum is one. Comparing Eq. (9-10) with Eq. (4), we can see that if we substitute  $\\mathbf{P}$  with  $\\mathbf{Y}$  and substitute  $\\mathbf{r}$  and  $\\mathbf{c}$  with  $\\mathbf{1}_N$  and  $\\boldsymbol{\\mu}$  respectively, we find that the above optimization problem is actually a special case of OT.\n\n\nIn preliminaries, the entropy regularised OT is introduced (EOT), by adding entropy regularisation to OT, the given equation can be solved efficiently by Sinkhorn algorithm. Specifically, the optimisation problem is:\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y}} {\\text {m i n i m i z e}} \\quad \\left\\langle - \\log (\\hat {\\boldsymbol {Z}}), \\boldsymbol {Y} \\right\\rangle + \\lambda \\boldsymbol {Y} \\log (\\boldsymbol {Y}) \\tag {11}\n$$\n\n$$\n\n\n$$\n\n$$\n\\text {s u b j e c t} \\quad \\boldsymbol {Y} ^ {\\intercal} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\quad \\boldsymbol {Y} \\mathbf {1} _ {K} = \\mathbf {1} _ {N}. \\tag {12}\n$$\n\n$$\n\n\nThe associated algorithmic flow is outlined in detail in Algorithm 1.\n\n\n```txt\nInput: Cost matrix  $M$ , trade-off parameter  $\\lambda$ , max number of iterations  $N_T$ , iteration number  $t$ , error threshold  $\\epsilon$ , current error  $\\delta$ , two probability vectors  $\\mathbf{r}$  and  $\\mathbf{c}$ ,  $|\\cdot|$  denotes the vector norm.\n```\n\n\n```txt\n1. Initialise  $\\pmb{K} = e^{-M / \\lambda},\\pmb{u}_{old} = \\mathbf{1}_N,\\pmb {v} = \\mathbf{1}_K,t = 0;$\n```\n\n\n```txt\n2 while  $t \\leq N_T$  and  $\\delta \\leq \\epsilon$  do\n```\n\n\n```txt\n3  $\\begin{array}{rl} & {\\pmb {u} = \\pmb {r}. / \\pmb {K}\\pmb {v};}\\\\ & {\\pmb {v} = \\pmb {c}. / \\pmb{K}^{\\intercal}\\pmb {u};}\\\\ & {\\sigma = |\\pmb{u}_{old} - \\pmb {u}|;}\\\\ & {\\pmb{u}_{old} = \\pmb {u};}\\\\ & {t = t + 1;} \\end{array}$    \n4   \n5   \n6   \n7\n```\n\n\n```txt\ns end\n```\n\n\n```latex\n9 Output  $\\pmb{Y} = \\mathbf{diag}(\\pmb{u})\\pmb{K}\\mathbf{diag}(\\pmb{v})$\n```\n\n\nAssigning  $\\lambda$  with 1, it is observed that we equate our objective function to the KL divergence, thus illustrating the extensive nature of our approach. DARP (Kim et al., 2020a) has previously applied it to long-tailed semi-supervised classification.\n\n\nRemark We would like to illustrate the non-applicable scenarios of our method. Firstly, our method requires a large number of samples for evaluation. This is because if the batch size is small, we can not guarantee that the desired marginal distribution can be satisfied within the batch. In some online scenarios, the sample-wise correction method is more suitable. In addition, our method assumes that the marginal distribution is already known, and in the paper we assume in particular that it is consistent with a uniform distribution.\n\n\n![Figure 1: Our proposed framework OTLM. The logit  $\\Phi(x)$  is fed into a single-layer feed-forward network to infer  $\\tilde{\\mathbf{Z}}$ . The cost matrix  $M$  is set to  $-\\log \\tilde{\\mathbf{Z}}$ . By solving the optimal transport problem via Sinkhorn algorithm, we can obtain the refined prediction  $\\mathbf{Y}$ .](images/9133a22319add0bd3e8fd0a9710c0179f099b050b88f2c773937afae5d50fe95.jpg)\n\n\n# 3.2 COST FUNCTION LEARNING VIA LINEAR MAPPING\n\n\nSimple functions is likely to be sub-optimal for the real data sets; this suggests the design of a better cost function to better fit the long-tailed recognition problem. However, manually designed cost functions require expert knowledge in different domains. Thus, we propose to use a linear mapping to automatically learn the cost function, which relieves the need to configuration.\n\n\nMore specifically, for predictions  $\\tilde{Z}$  generated by Softmax operation via leveraging linear transformation matrix  $\\mathbf{W}$ ,  $\\mathbf{W} \\in \\mathbb{R}^{K \\times K}$  is learned so that the following objective function is minimised.\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y} \\in \\mathbb {R}} {\\text {m i n i m i z e}} \\quad - \\left\\langle \\boldsymbol {Y}, \\log \\tilde {\\boldsymbol {Z}} \\right\\rangle + \\lambda \\left\\langle \\boldsymbol {Y}, \\log \\boldsymbol {Y} \\right\\rangle , \\tag {13}\n$$\n\n$$\n\n\n$$\n\n$$\n\\boldsymbol {Y} \\mathbf {1} _ {K} = \\mathbf {1} _ {N} \\quad \\boldsymbol {Y} ^ {\\intercal} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\quad \\tilde {\\boldsymbol {Z}} _ {n k} = \\frac {\\exp \\left(\\boldsymbol {W} ^ {\\intercal} \\boldsymbol {\\Phi} \\left(\\boldsymbol {x} _ {n}\\right)\\right) _ {k}}{\\sum_ {k} ^ {K} \\exp \\left(\\boldsymbol {W} ^ {\\intercal} \\boldsymbol {\\Phi} \\left(\\boldsymbol {x} _ {n}\\right)\\right) _ {k}}.\n$$\n\n$$\n\n\nThe resulting formula can be illustrated using a simple one-layer network of weight parameter  $W$ , together with an error function in Eq. (13). By normalising each row of  $W$ , each element can be understood as the probability value of confusing two classes, and somehow as a transition matrix between classes. We initialise  $W$  with an identity matrix and use small learning rate to learn  $W$ . One could also absorb the Logit Adjustment term  $-\\log p(y^{s})$  as the fixed bias parameter into the network. Motivated by this description, we can use a general gradient descent algorithm, such as SGD, to optimise the error function. Take into account the implementation in practice, the direct calculation of Eq. (13) can be done using the Sinkhorn iterations (Cuturi, 2013; Frogner et al., 2015; Peyr\u00e9 et al., 2019) in mini-batch training efficiently. Besides, We term the proposed method as OTLM (Optimal Transport via Linear Mapping), Figure 1 illustrates the overview of OTLM.\n\n\nCompute the gradients w.r.t  $W$  With the optimisation in Algorithm 1, it was conducted on the overall data set with a comparatively large number of data. Quite a different scenario now, as minibatch training is more favoured when it comes to neural networks. For this reason, the optimisation workflow in Algorithm 1, as pointed by Peyre et al. (2019); Viehmann (2019), will have a problem of batch stabilisation. To this end, we perform logarithmic transformation to step 3-4 in Algorithm 1.\n\n\n$$\n\n$$\n\\log \\boldsymbol {u} = \\log \\boldsymbol {r} - \\log (\\boldsymbol {M} \\boldsymbol {v}) = \\log \\boldsymbol {r} - \\operatorname {l o g s u m e x p} \\left(\\frac {1}{\\lambda} \\boldsymbol {M} - \\log \\boldsymbol {v}\\right) \\tag {14}\n$$\n\n$$\n\n\n$$\n\n$$\n\\log \\boldsymbol {v} = \\log \\boldsymbol {c} - \\log \\left(\\boldsymbol {M} ^ {\\intercal} \\boldsymbol {u}\\right) = \\log \\boldsymbol {c} - \\operatorname {l o g s u m e x p} \\left(\\frac {1}{\\lambda} \\boldsymbol {M} - \\log \\boldsymbol {u}\\right). \\tag {15}\n$$\n\n$$\n\n\nwith log-sum-exp operation  $\\mathrm{logsumexp}(\\boldsymbol{x}) = \\log (\\sum_{i}\\exp \\boldsymbol{x}_{i})$ . Assuming convergence is achieved in the Sinkhorn's loops, it is not necessary for us manually to compute the derivative of the loss function in Eq. (13) w.r.t  $W$ . As a matter of fact, as long as we make sure that the entire optimisation process is differentiable, modern deep learning libraries can then automatically implement end-to-end derivations, as it allows us to differentiate between the results of Sinkhorn's loops as a mere composition of elementary operations.\n\n\nPerformance degradation caused by imbalanced distributions can be addressed with meta-learning based methods (Ren et al., 2020; Wang et al., 2020a; Li et al., 2021). Recent works have illustrated neu\n\n\nral networks can learn more meaningful representations from the validation data set  $\\{(\\pmb{x}_n^v, y_n^v)\\}_{n=1}^{N_v}$ . Inspired by the methods based on meta learning, Here we also take a validation data set to optimise the parameter  $\\pmb{W}$ , but with the still significant difference in that we require no labelling information, thus avoiding a large expanse of labeling samples.\n\n\n# 4 EXPERIMENTS\n\n\nIn this section, we first conduct experiments comparing our approach versus extant post-hoc correction methods on three data sets, including CIFAR-100-LT (Cao et al., 2019), ImageNet-LT (Liu et al., 2019), and iNaturalist (Van Horn et al., 2018) with varying backbones. Finally, we empirically make a comparison of our algorithm with alternative cutting-edge long-tailed recognition methods. Observe that, for one thing, a plausible coupling of our algorithm to any training-aware long-tailed recognition method exists. As an illustration of the potency and strong generalisation of our approach, we conducted post-hoc correction of the pseudo-predictions of both methods, which are fairly typical for the training stage of all data sets: The first is based on the cross entropy (CE), and the other is the recently proposed  $RIDE$  (Wang et al., 2020b), which is based on multi experts.\n\n\n# 4.1 DATA SETS AND BASELINES\n\n\nThe baseline methods and data set are briefly described here, with implementation details placed in the appendix. In our experiments, we assume that the marginal distribution of the test set is uniform due to the characteristics of the three data sets.\n\n\nBaselines: We compare our methods with (i) post-hoc correction methods including Logit Adjustment (Menon et al., 2020), DARP (Kim et al., 2020a), (ii) state-of-the-art methods including Focal Loss (Lin et al., 2017), LDAM (Cao et al., 2019), BBN (Zhou et al., 2020), Balanced Softmax (Ren et al., 2020), Causal Norm (Tang et al., 2020), LADE (Hong et al., 2020), M2m (Kim et al., 2020b), Decouple (Kang et al., 2019), LFME (Xiang et al., 2020), RIDE (Wang et al., 2020b)\n\n\nLong-tailed data set: We take experiments on three data sets including CIFAR-100-LT, ImageNet-LT, and iNaturalist. We build the imbalanced version of CIFAR-100 by downsampling samples per class following the profile in Liu et al. (2019); Kang et al. (2019) with imbalanced ratio 10, 50, and 100. For all the benchmarks, we evaluate the performance on the test data set using a model trained on the training data set, and report the results using top-1 accuracy.\n\n\n# 4.2 MAIN RESULTS\n\n\nTable 1 illustrates the results on the CIFAR-100-LT data set. For CE-based training-aware methods, it first shows that Logit Adjustment is indeed a simple yet effective method for post-hoc correction. Remarkably, it outperforms the baseline methods by  $2.6\\%$ ,  $4.1\\%$ , and  $4.1\\%$  under the imbalance ratio of 10, 50, and 100 respectively. With a quick look at the results based on the OT approach, the superior results stress the advantages of our approach over the DARP and Logit adjustments. OT algorithm can further improve the performance by  $0.62\\%$ ,  $0.43\\%$ ,  $0.70\\%$ , for OTLM, it outperforms\n\n\nTable 1: Comparison on the top-1 accuracy with post-hoc correction methods on CIFAR-100-LT data set using ResNet-32 backbone, ImageNet-LT and iNaturalist data set using ResNet-50 and ResNeXt-50-32x4d backbones. Better results are marked in bold, the small red font indicates the increase in accuracy, compared to Logit Adjustment and RIDE, '-' means the results are not reported. CE indicates that a cross-entropy loss function is used in the training stage.\n\n\n<table><tr><td>Data set</td><td></td><td colspan=\"2\">CIFAR-100</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Imabalced Ratio</td><td>10</td><td>50</td><td>100</td><td>-</td><td>-</td></tr><tr><td>Baseline (CE)</td><td>59.00</td><td>45.50</td><td>41.00</td><td>43.10</td><td>65.00</td></tr><tr><td>Logit Adjustment+CE</td><td>61.63</td><td>49.62</td><td>45.11</td><td>51.62</td><td>69.44</td></tr><tr><td>DARP+CE</td><td>61.78</td><td>49.52</td><td>45.13</td><td>51.20</td><td>71.18</td></tr><tr><td>OT+CE</td><td>62.25+0.62</td><td>50.05+0.43</td><td>45.81+0.70</td><td>51.87+0.25</td><td>71.20+1.76</td></tr><tr><td>OTLM+CE</td><td>62.18+0.55</td><td>50.35+0.73</td><td>46.10+0.99</td><td>52.35+0.73</td><td>72.30+2.86</td></tr><tr><td>RIDE</td><td>-</td><td>-</td><td>50.20</td><td>57.50</td><td>72.90</td></tr><tr><td>OT+RIDE</td><td>-</td><td>-</td><td>50.82+0.62</td><td>58.45+0.95</td><td>75.85+2.95</td></tr><tr><td>OTLM+RIDE</td><td>-</td><td>-</td><td>51.43+1.23</td><td>58.71+1.21</td><td>76.12+3.22</td></tr></table>\n\n\nLogit Adjustment by  $0.55\\%$ ,  $0.73\\%$ ,  $0.99\\%$  under the three imbalance ratios on CIFAR-100-LT data set. For RIDE-based training-aware methods, there is also a  $0.6\\%$  improvement in the accuracy of our method.\n\n\nTable 1 also provides the results on ImageNet-LT and iNaturalist data sets. The fact again attracts our attention that for CE-based training-aware methods, OT, which is based on convex optimisation, consistently outperform Logit Adjustment by a large margin (1.76%) on iNaturalist data set. The accuracy boosting indicates the great potential of methods based on optimisation for post-hoc correction. Besides, not surprisingly, OTLM can further enhance the prediction accuracy, it outperforms Logit Adjustment by 0.73% and 2.86% on ImageNet-LT and iNaturalist respectively. As for RIDE, the gain in accuracy is also significant, about 3% on iNaturalist.\n\n\n# 4.3 COMPARISON WITH THE STATE-OF-THE-ART METHODS\n\n\nArmed with a demonstration of the validity of OT and OTLM, we switched to a comparison of our performance with existing methods that achieved the most advanced results on three benchmarks. As shown in Table 2, in particular, on ImageNet-LT and iNaturalist data sets, the experimental results are remarkable and impressive, our method outperforms RIDE by  $1.0\\%$  and  $3.0\\%$ , respectively. Since iNaturalist is a very difficult and fine-grained data set consisting of 8,142 categories. These huge performance gains come at a fraction of the cost, as we show in the next subsection.\n\n\nTable 2: Comparison on the top-1 accuracy with state-of-the-arts on CIFAR-100-LT data set using ResNet-32 backbone, on ImageNet-LT and iNaturalist data set using e ResNet-50 and ResNeXt-50-32x4d backbones respectively. - denotes the results are not reported, results underlined are the ones being compared, best results are marked in bold, the small red font denotes performance gain.\n\n\n<table><tr><td>Data Set</td><td colspan=\"3\">CIFAR-100</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Imabalced Ratio</td><td>10</td><td>50</td><td>100</td><td>-</td><td>-</td></tr><tr><td>Cross Entropy (CE)</td><td>59.0</td><td>45.5</td><td>38.3</td><td>43.1</td><td>65.0</td></tr><tr><td>Focal Loss</td><td>55.8</td><td>44.3</td><td>38.4</td><td>-</td><td>-</td></tr><tr><td>CB-Focal</td><td>-</td><td>-</td><td>-</td><td>43.7</td><td>61.1</td></tr><tr><td>OLTR</td><td>-</td><td>-</td><td>-</td><td>41.9</td><td>63.9</td></tr><tr><td>NCM</td><td>-</td><td>-</td><td>-</td><td>47.3</td><td>63.1</td></tr><tr><td>norm</td><td>-</td><td>-</td><td>-</td><td>49.4</td><td>69.3</td></tr><tr><td>cRT</td><td>-</td><td>-</td><td>-</td><td>49.6</td><td>67.6</td></tr><tr><td>LWS</td><td>-</td><td>-</td><td>-</td><td>49.9</td><td>69.5</td></tr><tr><td>LDAM</td><td>-</td><td>-</td><td>-</td><td>-</td><td>64.6</td></tr><tr><td>LDAM+DRW</td><td>58.7</td><td>46.6</td><td>42.0</td><td>-</td><td>68.0</td></tr><tr><td>BBN</td><td>59.1</td><td>47.0</td><td>42.6</td><td>-</td><td>66.3</td></tr><tr><td>Causal Norm</td><td>59.6</td><td>50.3</td><td>44.1</td><td>51.8</td><td>63.9</td></tr><tr><td>M2m</td><td>58.2</td><td>-</td><td>43.5</td><td>-</td><td>-</td></tr><tr><td>LFME</td><td>-</td><td>-</td><td>43.8</td><td>-</td><td>-</td></tr><tr><td>Balanced Softmax</td><td>61.6</td><td>49.9</td><td>45.1</td><td>-</td><td>-</td></tr><tr><td>LADE</td><td>61.7</td><td>50.5</td><td>45.4</td><td>51.9</td><td>70.0</td></tr><tr><td>RIDE (6 experts)</td><td>-</td><td>-</td><td>50.2</td><td>57.5</td><td>72.9</td></tr><tr><td>Logit Adjustment</td><td>61.6</td><td>49.6</td><td>45.1</td><td>51.6</td><td>69.4</td></tr><tr><td>DARP</td><td>61.8</td><td>49.5</td><td>45.1</td><td>51.2</td><td>71.2</td></tr><tr><td>OT+CE</td><td>62.3+0.7</td><td>50.1</td><td>45.8</td><td>51.9</td><td>71.2</td></tr><tr><td>OTLM+CE</td><td>62.2</td><td>50.4+0.8</td><td>46.1</td><td>52.4</td><td>72.3</td></tr><tr><td>OT+RIDE (6 experts)</td><td>-</td><td>-</td><td>50.8+0.6</td><td>58.5+1.0</td><td>75.9+3.0</td></tr><tr><td>OTLM+RIDE (6 experts)</td><td>-</td><td>-</td><td>51.4+1.2</td><td>58.7+1.2</td><td>76.1+3.2</td></tr></table>\n\n\n# 4.4 COMPUTATION COST OF OT AND OTLM\n\n\nAs we have highlighted, the additional computational cost of OT and OTLM is particularly small, compared to that of training-aware methods. In Table 3, exact time of the evaluations on the ImageNet-LT and iNaturalist data sets is provided. Please note, the times here are measured from the start of each method to the final best performance. For the comparison between OT and OTLM, because\n\n\nTable 3: Time for different methods to execute on the ImageNet-LT and iNaturalist data sets. The times here are measured from the start of each method to the final best performance. Their running time are counted in seconds. Coupled with the prior results, an observation can be made that OT can be the best coordinate to trade off performance and efficiency in post-hoc correction methods.\n\n\n<table><tr><td>Data Set</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Logit Adjustment</td><td>0.3</td><td>1.2</td></tr><tr><td>OT</td><td>32.7</td><td>82.5</td></tr><tr><td>OTLM</td><td>134.4</td><td>256.5</td></tr></table>\n\n\nOTLM runs on the GPU, one optimization iteration of OTLM is in fact much faster than OT. Except for OTLM, which was run on an NVidia card (V100), the results come from a 28-core machine (2.20 Ghz Xeon). Firstly, we can observe that all post-hoc correction methods here are not quite time-consuming. For comparison, ImageNet-LT and iNaturalist can be trained on 4 NVidia cards (V100) for extremely long periods of time. Using ResNet-50-32x4d on the iNaturalist training data set, for example, with a batch size of 256, the amount of training time (in seconds) to perform 1 iteration is approximately 850. Also, if one differs the performance of each method, we gather that OT and OTLM provide the best balance of performance and running costs to match.\n\n\n# 4.5 CONVERGENCE BEHAVIOR OF OTLM\n\n\nTo verify that our adopted OTLM algorithm is really efficient. With ResNet-50 as the backbone, we plotted curves depicting the decreasing absolute error  $\\sigma$  with the number of iterations  $t$  on the ImageNet-LT validation data set. Image on the left in Figure 2 gives us a visualisation of the optimisation process. It is found that the absolute error  $\\sigma$  converges at around 60 iterations. In fact, we discovered that 10 or even fewer iterations were enough to yield satisfactory performance.\n\n\n![Figure 2: The left image shows the convergence of OTLM: The absolute error  $\\sigma$  decreases with iterations on the ImageNet-LT validation data set (20k samples), and we used ResNet-50 as the backbone to obtain the original predictions  $Z$ . The right image demonstrates the block in  $W$ , we trained on CIFAR-100 to produce the original predictions with OTLM to minimise the difference between the ideal and actual distributions. For ease of display, the values of the diagonal elements are relatively large and we will replace these values with zeros.](images/1e6fa388a2f2907bb4493cf441d4da8147d0f8579170acb3e35999f1319d3e7c.jpg)\n\n\n![](images/f746a69db0cbd8c10f982d58baa44ba83e8ee5afb892a594b0717dc9a46c636a.jpg)\n\n\n# 4.6 DETAILED ANALYSIS ON DISTRIBUTION MATCHING\n\n\nUpon distribution fitting, preferably the estimation bias is lessened, which implies that the KL divergence distance between the refined prediction and the target distribution is decreased. Table 4 displays the KL distances on the ImageNet-LT and iNaturalist data sets, and we can find that OT possesses a smaller KL distance than Logit Adjustment, which suggests that OT decreases the estimation bias more than Logit Adjustment by a much larger margin.\n\n\n# 4.7 VISUALISATION ON WEIGHT PARAMETER IN OTLM\n\n\nAs a natural consequence, one wonders about the exact structure of this weight matrix  $\\mathbf{W}$ , and no better way to present its results than through visualisation. Towards this end, we applied OTLM\n\n\nTable 4: KL distance between refined probability distribution and target distribution computed on ImageNet-LT and iNaturalist data sets. Smaller distance indicates two distributions are more similar.\n\n\n<table><tr><td>Data Set</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Logit Adjustment</td><td>1.78e-05</td><td>5.81e-06</td></tr><tr><td>DARP</td><td>8.49e-08</td><td>1.07e-08</td></tr><tr><td>OT</td><td>8.62e-10</td><td>4.72e-10</td></tr></table>\n\n\nto refine the original predictions on CIFAR-100 data set with an imbalanced ratio  $R = 100$  and ResNet-32 backbone, and trained a one-layer model until it converged to extract the value of  $W$ , where the shape of is  $100 \\times 100$ . We visualised a sub-block of size  $20 \\times 20$ , the illustration is presented on the right image in Figure 2. The very physical meaning of the elements in  $W$  can be interpreted as the probability of transition between classes. The visualised matrix is very dense, this non-sparsity suggests that the OTLM manages to correct the predictions of the model in the training stage, whilst minimising the distance between the desired and the actual distribution.\n\n\n# 5 RELATED WORK\n\n\nLong-tailed recognition: The great majority of current methods applied to long-tailed recognition can be divided into either training-aware and post-hoc correction methods. The training-aware methods aim to improve the generalisation ability of models trained on imbalanced training data set. Among them, re-sampling and re-weighting are two major methods. Re-sampling controls the class numbers by the means of over-sampling (Chawla et al., 2002) and under-sampling (Barandela et al., 2004; Drummond et al., 2003). The re-weighting approaches achieve balanced learning by giving smaller weights to majority classes and larger weights to minority classes (Tang et al., 2008; Zadrozny et al., 2003; Lin et al., 2017; Lee et al., 2017; Cui et al., 2019; Khan et al., 2019). Other approaches have also been proposed to resolve the imbalanced problem. For example, Cao et al. introduces a theoretically-principled loss function motivated by minimizing the generalisation error bound (Cao et al., 2019), Kang et al. proposes to decouple the learning phases into representation learning and classifier tuning (Kang et al., 2019). Apart from training-aware methods, post-hoc correction also offers us an avenue for long-tailed recognition problem, it can be applied to the normalisation of classifier weights (Kang et al., 2019; Zhang et al., 2019; Kim & Kim, 2020) or logits based on label frequency (Provost, 2000; Collell et al., 2016). Among them, Logit Adjustment (Menon et al., 2020) is a simple yet powerful method to use. Compared with Logit Adjustment, our approach performs the post-hoc correction from the optimisation view.\n\n\nOptimal transport: OT distances is a important family of distances for probability measures (Villani, 2008; Santambrogio, 2015). It has been extensively applied in different fields in machine learning. Courty et al. (Courty et al., 2016) proposes a regularised unsupervised optimal transportation model to perform the domain adaption. Other applications include 3D shape matching (Su et al., 2015), generative model (Arjovsky et al., 2017; Salimans et al., 2018; Bunne et al., 2019; Deshpande et al., 2019; Bhagoji et al., 2019), graph matching (Xu et al., 2019a;b), model designs Kandasamy et al. (2018); Chizat & Bach (2018). To the best of our knowledge, we are the first to perform the post-hoc correction from the OT perspective.\n\n\n# 6 CONCLUSION\n\n\nIn this paper, we present two techniques for post-hoc corrections from an optimisation perspective. Our approach reviews the basic mathematical formulation of the distribution alignment problem and relates the resulting formulation to OT, which we find, moreover, offers greater possibilities and flexibility to solve the problem. In order to avoid manual configuration of the cost matrix, we propose to learn the cost matrix automatically by linear mapping, which leads to further performance improvements. The experimental results confirm the superiority of our proposed approach. I believe our work can open up more possibilities and imagination for post-hoc corrections.\n\n\n# REFERENCES\n\n- Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pp. 214-223. PMLR, 2017.\n\n- Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning for domain adaptation under label shifts. arXiv preprint arXiv:1903.09734, 2019.\n\n- Ricardo Barandela, Rosa M Valdovinos, J Salvador S\u00e1nchez, and Francesc J Ferri. The imbalanced training sample problem: Under or over sampling? In Joint IAPR international workshops on statistical techniques in pattern recognition (SPR) and structural and syntactic pattern recognition (SSPR), pp. 806-814. Springer, 2004.\n\n- Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Lower bounds on adversarial robustness from optimal transport. arXiv preprint arXiv:1909.12272, 2019.\n\n- Paula Branco, Lu\u00eds Torgo, and Rita P Ribeiro. A survey of predictive modeling on imbalanced domains. ACM Computing Surveys (CSUR), 49(2):1-50, 2016.\n\n- Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. *Neural Networks*, 106:249\u2013259, 2018.\n\n- Charlotte Bunne, David Alvarez-Melis, Andreas Krause, and Stefanie Jegelka. Learning generative models across incomparable spaces. In International Conference on Machine Learning, pp. 851-861. PMLR, 2019.\n\n- Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. arXiv preprint arXiv:1906.07413, 2019.\n\n- Claire Cardie and Nicholas Howe. Improving minority class prediction using case-specific feature weights. 1997.\n\n- Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.\n\n- Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.\n\n- Guillem Collell, Drazen Prelec, and Kaustubh Patil. Reviving threshold-moving: a simple plug-in bagging ensemble for binary and multiclass imbalanced data. arXiv preprint arXiv:1606.08698, 2016.\n\n- Nicolas Courty, R\u00e9mi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853-1865, 2016.\n\n- Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9268-9277, 2019.\n\n- Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26:2292-2300, 2013.\n\n- Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David Forsyth, and Alexander G Schwing. Max-sliced wasserstein distance and its use for gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10648-10656, 2019.\n\n- Chris Drummond, Robert C Holte, et al. C4.5, class imbalance, and cost sensitivity: why undersampling beats over-sampling. In Workshop on learning from imbalanced datasets II, volume 11, pp. 1-8. Citeseer, 2003.\n\n- Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso Poggio. Learning with a wasserstein loss. arXiv preprint arXiv:1506.05439, 2015.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise. arXiv preprint arXiv:1802.05300, 2018.\n\n- Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, and Buru Chang. Disentangling label distribution for long-tailed visual recognition. arXiv preprint arXiv:2012.00321, 2020.\n\n- Chen Huang, Yining Li, Chen Change Loy, and Xiaou Tang. Learning deep representation for imbalanced classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5375-5384, 2016.\n\n- Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric Xing. Neural architecture search with bayesian optimisation and optimal transport. arXiv preprint arXiv:1802.07191, 2018.\n\n- Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. arXiv preprint arXiv:1910.09217, 2019.\n\n- Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 103-112, 2019.\n\n- Byungju Kim and Junmo Kim. Adjusting decision boundary for class imbalanced learning. IEEE Access, 8:81674-81685, 2020.\n\n- Jaehyung Kim, Youngbum Hur, Sejun Park, Eunho Yang, Sung Ju Hwang, and Jinwoo Shin. Distribution aligning refinery of pseudo-label for imbalanced semi-supervised learning. arXiv preprint arXiv:2007.08844, 2020a.\n\n- Jaehyung Kim, Jongheon Jeong, and Jinwoo Shin. M2m: Imbalanced classification via major-to-minor translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13896-13905, 2020b.\n\n- Wonji Lee, Chi-Hyuck Jun, and Jong-Seok Lee. Instance categorization by support vector machines to adjust weights in adaboost for imbalanced data classification. Information Sciences, 381:92-103, 2017.\n\n- Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. arXiv preprint arXiv:2103.12579, 2021.\n\n- Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr D\u00f3lar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980-2988, 2017.\n\n- Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2537-2546, 2019.\n\n- Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. arXiv preprint arXiv:2007.07314, 2020.\n\n- Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355-607, 2019.\n\n- Foster Provost. Machine learning from imbalanced data sets 101. In Proceedings of the AAAI'2000 workshop on imbalanced data sets, volume 68, pp. 1-3. AAAI Press, 2000.\n\n- Xingye Qiao and Yufeng Liu. Adaptive weighted learning for unbalanced multicategory classification. Biometrics, 65(1):159-168, 2009.\n\n- Jiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Balanced meta-softmax for long-tailed visual recognition. arXiv preprint arXiv:2007.10740, 2020.\n\n- Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving gans using optimal transport. arXiv preprint arXiv:1803.05573, 2018.\n\n- Filippo Santambrogio. Optimal transport for applied mathematicians. *Birkh\u00e4user*, NY, 55(58-63):94, 2015.\n\n- Richard Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. ii. Proceedings of the American Mathematical Society, 45(2):195-198, 1974.\n\n- Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. arXiv preprint arXiv:2001.07685, 2020.\n\n- Zhengyu Su, Yalin Wang, Rui Shi, Wei Zeng, Jian Sun, Feng Luo, and Xianfeng Gu. Optimal mass transport for shape matching and comparison. IEEE transactions on pattern analysis and machine intelligence, 37(11):2246-2259, 2015.\n\n- Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classification by keeping the good and removing the bad momentum causal effect. arXiv preprint arXiv:2009.12991, 2020.\n\n- Yuchun Tang, Yan-Qing Zhang, Nitesh V Chawla, and Sven Krasser. Svms modeling for highly imbalanced classification. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(1):281-288, 2008.\n\n- Grant Van Horn and Pietro Perona. The devil is in the tails: Fine-grained classification in the wild. arXiv preprint arXiv:1709.01450, 2017.\n\n- Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8769-8778, 2018.\n\n- Thomas Viehmann. Implementation of batched sinkhorn iterations for entropy-regularized wasserstein loss. arXiv preprint arXiv:1907.01729, 2019.\n\n- C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.\n\n- Renzhen Wang, Kaiqin Hu, Yanwen Zhu, Jun Shu, Qian Zhao, and Deyu Meng. Meta feature modulator for long-tailed recognition. arXiv preprint arXiv:2008.03428, 2020a.\n\n- Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella X Yu. Long-tailed recognition by routing diverse distribution-aware experts. arXiv preprint arXiv:2010.01809, 2020b.\n\n- Liuyu Xiang, Guiguang Ding, and Jungong Han. Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification. In European Conference on Computer Vision, pp. 247-263. Springer, 2020.\n\n- Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1492-1500, 2017.\n\n- Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph partitioning and matching. arXiv preprint arXiv:1905.07645, 2019a.\n\n- Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-wasserstein learning for graph matching and node embedding. In International conference on machine learning, pp. 6932-6941. PMLR, 2019b.\n\n- Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate example weighting. In Third IEEE international conference on data mining, pp. 435-442. IEEE, 2003.\n\n- Junjie Zhang, Lingqiao Liu, Peng Wang, and Chunhua Shen. To balance or not to balance: A simple-yet-effective approach for learning with long-tailed distributions. arXiv preprint arXiv:1912.04486, 2019.\n\n- Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9719\u20139728, 2020.\n\nResponse: Given the detailed analysis and contributions outlined in the paper, here's my assessment:\n\n- **Originality**: The paper introduces a novel approach to post-hoc correction using optimal transport (OT), which is a fresh perspective compared to traditional methods. The idea of using OT to align marginal distributions is innovative and adds a new dimension to the existing long-tailed recognition literature.\n  \n- **Methodological Rigor**: The authors present a rigorous theoretical framework grounded in OT and Sinkhorn's algorithm, which is well-supported by proofs and equations. They provide a detailed explanation of the derivation of the OT-based post-hoc correction and the learning of the cost matrix using a linear mapping. The technical details are sound and align well with established literature on optimal transport and neural networks.\n  \n- **Significance of Contribution**: The paper makes a significant contribution by demonstrating substantial performance improvements over existing methods on several benchmark datasets. The authors show that their approach, OTLM, can lead to better accuracy, especially on harder and more fine-grained datasets like iNaturalist. The results suggest that OT-based approaches can be more effective for post-hoc correction compared to traditional methods like Logit Adjustment.\n  \n- **Clarity of Presentation**: The paper is well-written and organized. It provides clear definitions, explanations, and examples. The notation is appropriately used, and the steps in the derivation are well-explained. The experimental setup and results are clearly presented, making it easy to understand the methodology and outcomes.\n  \n- **Alignment with Journal Scope**: The paper fits well within the scope of ICLR, which focuses on advances in machine learning, particularly on methods that address challenging problems like long-tailed recognition. The focus on optimal transport, neural networks, and post-hoc correction aligns well with the journal's interests.\n\nConsidering these factors, the paper presents a strong and innovative approach with substantial empirical evidence supporting its effectiveness. Given the novelty, theoretical foundation, and practical impact, I would predict that this paper has a good chance of acceptance at ICLR.\n\nPrediction: \\(\\boxed{Accept}\\)", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634733.0457213", "msecs": "45.0", "relativeCreated": "77869.1954612732", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# VEnhancer: Generative Space-Time Enhancement for Video Generation\n\n# Abstract\nWe present \\emph{VEnhancer}, a generative space-time enhancement method that can improve the existing AI-generated videos spatially and temporally through one video diffusion model. Given a generated low-quality video, our approach can increase its spatial and temporal resolution simultaneously with arbitrary up-sampling space and time scales by adding more details in spatial domain and synthesize detailed motion in temporal domain. Furthermore, VEnhancer is able to remove generated spatial artifacts and temporal flickering of generated videos. To achieve this, basing on a pretrained generative video prior, we train a \\textbf{S}pace-\\textbf{T}ime Controller and inject it to the prior as a condition on low-frame-rate and low-resolution videos. To effectively train this ST-Controller, we design \\textit{space-time data augmentation} to create diversified video training pairs as well as \\textit{video-aware conditioning} for realizing different augmentation parameters in both spatial and temporal dimensions. Benefiting from the above designs, VEnhancer can be end-to-end trained to enable multi-function in one single model. Extensive experiments show that VEnhancer surpasses existing state-of-the-art video super-resolution and space-time super-resolution methods in enhancing AI-generated videos. Moreover, VEnhancer is able to greatly improve the performance of open-source state-of-the-art text-to-video methods on video generation benchmark, VBench.\n\n\n# 1 INTRODUCTION\n\n\nWith the advances of text-to-image generation (Rombach et al., 2022; Podell et al., 2023; Chen et al., 2023b; Gao et al., 2024) and large-scale video datasets with text description (Bain et al., 2021), there is fast development of text-to-video generative models (Guo et al., 2023; Chen et al., 2023c; Ho\n\n\net al., 2022; Blattmann et al., 2023b; Wang et al., 2023b;a; Chen et al., 2023a; 2024a; Gupta et al., 2023; Wu et al., 2023b). These developments enable users to generate compelling videos through textual descriptions of the desired content. One common solution (Ho et al., 2022; Blattmann et al., 2023b; Wang et al., 2023b; Blattmann et al., 2023a; Gupta et al., 2023) to obtain high-quality videos is to adopt cascaded pipelines, which stacks several video diffusion models, including text-to-video, temporal super-resolution and spatial super-resolution (S-SR) diffusion. These pipeline significantly reduce computation cost when generating high-resolution and high-frame-rate videos, but also pose several issues. First, using two different models for spatial and temporal enhancement separately is redundant, as they are strongly correlated. Also, the proposed diffusion-based spatial or temporal super-resolution (Blattmann et al., 2023b; Ho et al., 2022; Blattmann et al., 2023a; Wang et al., 2023b; Lin et al., 2024) have a limited flexibility, as they can only handle fixed interpolation ratio (i.e., predicting three frames between two consecutive frames) or fixed upscaling factors (i.e.,  $4 \\times$ ). Second, directly training diffusion models on synthesized video pairs may limit its generalization ability, as these models tailored on classic super-resolution task hallucinate high-frequency details, but cannot semantically improve the visual quality of input videos, such as eliminating distortions, artifacts, or recreating new contents (see LaVie-SR's results in Figure 1).\n\n\nAnother common approach is training another diffusion model to remove artifacts and to refine distorted content in the generated videos (Henschel et al., 2024; Zhang et al., 2023b). One example is I2VGen-XL (Zhang et al., 2023b), which follows the idea of image generation model SDXL (Podell et al., 2023)\u2013 it first upscales videos to higher resolutions using bilinear interpolation, and trains another diffusion refinement model, which will be used for video regeneration through a nois-ingenoising process (Meng et al., 2021). However, this method usually produces over-smoothed videos without realistic texture details (see Figure 1, I2VGen-XL), since the adopted bilinear upsampling cannot generate more spatial details. More importantly, the nois-ingenoising process (i.e., starting from  $t = 600$ ) will substantially change the original video content, which cannot always be acceptable in practical applications.\n\n\nIn short, current generative video enhancement methods face several challenges. First, sequentially applying temporal and spatial super-resolution is redundant, as they are independently trained, but using similar training datasets. Thus, such design is both sub-optimal and inefficient during the inference. Second, existing refinement methods struggle in balancing between video quality and fidelity to the original content. More importantly, they cannot perform effective super-resolution for increasing spatial and temporal details, which limits their practicality. Third, previous generative video enhancement methods lack the flexibility in dealing with different upscaling factors and refinement strengths for spatial or temporal super-resolution and video refinement.\n\n\nTo this end, we propose VEnhancer, a generative space-time enhancement method that supports both spatial and temporal super-resolution with flexible space and time up-sampling scales, as well as has the ability to remove visual artifacts and flickering with good maintenance of video content. It is built upon a pretrained and fixed generative video prior (Zhang et al., 2023b), which supplies the generative ability for video enhancement. To condition the video generation on low-frame-rate and low-resolution videos, we design Space-Time Controller (ST-Controller) for effective conditioning in both spatial and time dimensions. Furthermore, to handle different up-sampling scales and reduce artifacts or flickering with different degrees, we propose a space-time data augmentation algorithm to construct the training data. In particular, at the training stage, we sample different step sizes for skipping frames, downscaling factors, and noise levels to synthesize diversified condition videos. To ensure the proposed ST-Controller be aware of the associated data augmentation applied to each input video, we propose the video-aware conditioning. In particular, for key frame, the condition latent, the embeddings of the associated downscaling factor  $s$  and noise level  $\\sigma$  by noise augmentation are incorporated into ST-Controller through video-aware conditioning.\n\n\nWith these designs, VEnhancer is a single end-to-end trainable network that can handle both spatial and temporal super-resolution, as well as video refinement. Moreover, it also supports arbitrary space and time up-sampling scales, and also supports flexible control on refinement strength and generative strength as user may prefer. Extensive experiments have demonstrated VEnhancer's ability in enhancing generated videos (see Figure 1). In these experiments, it outperforms state-of-the-art real-world and generative video super-resolution methods for spatial super-resolution only. In the space-time super-resolution, VEnhancer also surpasses state-of-the-art methods as well as cascaded spatial and temporal diffusion super-resolution models. At last, on the public video\n\n\ngeneration benchmark, VBenchmark (Huang et al., 2023), VEnhancer can significantly improve the overall performance of existing text-to-video algorithms.\n\n\nOur contributions can be summarized as below:\n\n- 1. We propose VEnhancer, a generative space-time enhancement method that can achieve generative spatial and temporal super-resolution for different upsampling factors, as well as controllable video refinement in one video diffusion model for the first time.\n\n- 2. To achieve the unified generative space-time enhancement, we devise ST-Controller for effective multi-frame condition injection based on a pretrained and fixed generative video prior. Besides, space-time data augmentation and the associated video-aware conditioning are proposed for training ST-Controller in an end-to-end manner.\n\n- 3. VEnhancer surpasses existing state-of-the-art video super-resolution methods and space-time super-resolution methods in enhancing generated videos. Also, it could improve the performance of open-source text-to-video methods on public video generation benchmark.\n\n\n# 2 RELATED WORK\n\n\n# 2.1VIDEO GENERATION\n\n\nRecently, there have been substantial efforts in training large-scale T2V (Wang et al., 2024; Ho et al., 2022; Guo et al., 2023; Chen et al., 2023c; Gupta et al., 2023; Blattmann et al., 2023b; Wang et al., 2023b;a) models on large scale datasets. Some works (Blattmann et al., 2023b; Wang et al., 2023b;a) inflate a pre-trained text-to-image (T2I) model by inserting temporal layers and fine-tuning them or all parameters on video data, or adopts a joint image-video training strategy. In order to achieve high-quality video generation, (Ho et al., 2022; Wang et al., 2023b; Blattmann et al., 2023b) adopts multi-stage pipelines. In particular, cascaded video diffusion models are designed: One T2V base model that is followed by one or more frame interpolation and video super-resolution models. VideoLDM (Blattmann et al., 2023b), LaVie (Wang et al., 2023b), and Upscale-A-Video (Zhou et al., 2023) all develop the video super-resolution model based on  $4 \\times$  sd (StableDiffusion (Rombach et al., 2022)) upscaler, which has an additional downsampled image for conditioning the generation. One drawback of this base model is losing quite a lot generative ability compared with T2I base models. On the contrary, I2VGEN-XL follows SDXL (Podell et al., 2023) and uses noisig-denoising process (Meng et al., 2021) to refine the generated artifacts. However, this strategy could improve stability but cannot increase the space-time resolution. VEnhancer is based on a generative video prior, and could address temporal/spatial super-resolution and refinement in a unified model.\n\n\n# 2.2VIDEO ENHANCEMENT\n\n\nVideo Super-Resolution. Video Super-Resolution (VSR) is proposed to enhance video quality by upsampling low-resolution (LR) frames into high-resolution (HR) ones. Traditional VSR approaches(Cao et al., 2021; Chan et al., 2021; 2022a; Isobe et al., 2020a;b;c; Liang et al., 2024; 2022; Wang et al., 2019; Xue et al., 2019) often rely on fixed degradation models to synthesize training data pairs, which leads to a noticeable performance drop in real-world scenarios. To bridge this gap, recent advances(Chan et al., 2022b; Xie et al., 2023) in VSR have embraced more diversified degradation models to better simulate real-world low-resolution videos. To achieve photo-realistic reconstruction, Upscale-A-Video(Zhou et al., 2023) integrates diffusion prior to produce detailed textures, upgrading VSR performance into next level. Space-Time Super-Resolution. Space Time Video Super-Resolution (STVSR) aims to simultaneously increase the resolutions of video frames in both spatial and temporal dimensions. Deep-learning based approaches(Haris et al., 2020; Kim et al., 2020; Xiang et al., 2020; Chen et al., 2022) have achieved remarkable results on STVSR. STARNet(Haris et al., 2020) increases the spatial resolution and frame rate by leveraging the mutual information between space and time. FISR(Kim et al., 2020) propose a joint framework with a multiscale temporal loss to upscale the spatial-temporal resolution of videos. (Xiang et al., 2020) proposes a one-stage STVSR framework, which incorporates different sub-modules for LR frame features interpolation, temporal information aggregation and HR reconstruction. VideoINR(Chen et al., 2022) utilize the continuous video representation to achieve STVSR at arbitrary spatial resolution and frame rate. Although these methods obtain smooth and high-resolution output videos, but they fail in generating realistic texture details.\n\n\n![Figure 2: The overall framework. It consists of space-time data augmentation for constructing training data, and the associated video-aware conditioning for realizing diversified conditions across frames, as well as ST-Controller for multi-frame condition injection based on generative video prior.](images/cfc7e1100181230ee15b12543b6903001724745994470e9b265521b3f33347bf.jpg)\n\n\n# 3 PRELIMINARIES:VIDEO DIFFUSION MODELS\n\n\nOur method is built on a pretrained video diffusion model (Zhang et al., 2023b), which is developed based on Stable Diffusion 2.1. Given an video  $\\mathbf{x} \\in \\mathbb{R}^{F \\times H \\times W \\times 3}$ , the encoder  $\\mathcal{E}$  first encodes it into latent representation  $\\mathbf{z} = \\mathcal{E}(\\mathbf{x})$  frame-by-frame, where  $\\mathbf{z} \\in \\mathbb{R}^{F \\times H' \\times W' \\times C}$ . Then, the forward diffusion and reverse denoising are conducted in the latent space. In the forward process, the noise is gradually added to the latent vector  $\\mathbf{z}$  in total  $T$  steps. And for each time-step  $t$ , the diffusion process is formulated as follows:\n\n\n$$\n\n$$\n\\mathbf {z} _ {t} = \\alpha_ {t} \\mathbf {z} + \\sigma_ {t} \\boldsymbol {\\epsilon}, \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\epsilon \\in \\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , and  $\\alpha_{t}$ ,  $\\sigma_{t}$  specify the noise schedule in which the corresponding log signal-to-noise-ratio  $(\\log [\\alpha_t^2 /\\sigma_t^2 ])$  decreases monotonically with  $t$ . And at time-step  $T$ ,  $q(\\mathbf{z}_T) = \\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . As for backward pass, a diffusion model is used for iteratively denoising under the guidance of the text prompt  $c_{text}$ . By adopting v-prediction parameterization (Salimans & Ho, 2022), the U-Net denoiser  $f_{\\theta}$  learns to make predictions of  $\\mathbf{v}_t\\equiv \\alpha_t\\pmb {\\epsilon} - \\sigma_t\\mathbf{z}$ . The optimization objective is simply formulated as:\n\n\n$$\n\n$$\n\\mathcal {L} _ {L D M} = \\mathbb {E} _ {\\mathbf {z}, c _ {t e x t}, \\boldsymbol {\\epsilon} \\sim \\mathcal {N} (\\mathbf {0}, \\mathbf {I}), t} \\left[ \\| \\mathbf {v} - f _ {\\theta} (\\mathbf {z} _ {t}, t, c _ {t e x t}) \\| _ {2} ^ {2} \\right]. \\tag {2}\n$$\n\n$$\n\n\nAt the end, the generated videos are obtained through the VAE decoder:  $\\hat{\\mathbf{x}} = \\mathcal{D}(\\mathbf{z})$ .\n\n\n# 4 METHODOLOGY\n\n\nIn this section, we introduce the main components of our method. The overall framework is illustrated in Figure 2. First, we present our architecture design in Section 4.1. Then we elaborate on the proposed space-time data augmentation in Section 4.2. In Section 4.3, we give a detailed description on our designed video-aware conditioning.\n\n\n# 4.1 ARCHITECTURE DESIGN\n\n\nThe architecture is designed based on a pretrained video diffusion model. This video diffusion model is able to generate temporal-coherent content and high-quality texture details through iterative denoising. To upsample and refine a low-frame-rate and low-resolution videos in both spatial and temporal dimensions, the visual information should be incorporated into the video diffusion model carefully in order to obtain high-quality results with good fidelity to the input videos. High-quality generated videos stem from powerful generative models, while fidelity requires the algorithm to preserve the visual information of the input. Inspired by (Zhang et al., 2023a), we keep the pretrained video diffusion model untouched for preserving generative capability, and create a Space-Time Controller (ST-Controller) to obtain effective multi-frame condition injection for generative video enhancement. The architecture is illustrated in Figure 3.\n\n\nThe pretrained video diffusion model follows the design of stacking a sequence of interleaved spatial and temporal layers within the 3D-UNet (Blattmann et al., 2023b) architecture (gray blocks in Figure 3). Specifically, each spatial convolution layer (or attention layer) is followed by a temporal\n\n\nconvolution layer (or attention layer). The spatial layers are the same as those in Stable Diffusion 2.1, including ResBlocks (He et al., 2016), self-attention (Vaswani et al., 2017) layers, and cross-attention layers. The temporal convolution and attention layers are incorporated with their output layers initialized to zero and finetuned with video datasets. Specifically, the temporal convolution is one-dimensional convolution layer with a kernel size of 3, and the temporal attention is one-dimensional attention layer (Wang et al., 2023a). In this 3D-UNet, the video features that aligned by temporal layers in encoder will be skipped to the decoder, in which concatenation operation will be performed to combine skipped features with decoder features.\n\n\nTo build our proposed ST-Controller, we make a copy (both the architectures and weights) of the multi-frame encoder and middle block in 3D-UNet (orange blocks in Fig. 3) as the trainable condition network. This condition network takes low-frame-rate and low-resolution condition latents as well as full frames of noisy latents as inputs. Specifically, the condition latents and the associated augmentation parameters are incorporated into the condition network through our proposed video-aware conditioning. The output multi-scale temporal-coherent video features will be injected into the original 3D-UNet through newly added zero convolutions (yellow blocks in Fig. 3). The output features of the middle block in condition network will be added\n\n\n![Figure 3: The architecture of VEnhancer.](images/38e6f2747369dddd271745c1732923ba878e98f209fb9648d71000892b83e917.jpg)\n\n\nback to the features of the middle block in 3D-UNet. While for output features of encoder blocks in condition network, their features will be added to the skipped video features in 3D-UNet, which are also produced by encoder blocks. The copied condition network, video-aware conditioning, and the newly added zero convolutions are trained simultaneously.\n\n\n# 4.2 SPACE-TIME DATA AUGMENTATION\n\n\nIn this section, we discuss about how to achieve unified space-time super-resolution with arbitrary up-sampling space and time scales, as well as refinement with varying degrees. To this end, we propose a novel data augmentation strategy for both space and time axes. Details are discussed below.\n\n\nTime axis. Given a sequence of high-frame-rate and high-resolution video frames  $I^{1:f} = [I^1, I^2, \\dots, I^f]$  with frame length  $f$ , we use a sliding window across time axis to select frames. The frame sliding window size  $m$  is randomly sampled from a predefined set, ranging from 1 to 8. This corresponds to time scales from  $1 \\times 8 \\times$ . Note that  $1 \\times$  time scale requires no frame interpolation, thus the multi-task problem downgrades to video super-resolution. After the frame skipping, we obtain a sequence of key frames  $I^{1:m:f} = [I^1, I^{1+m}, I^{1+2 \\times m}, \\dots, I^f]$ .\n\n\nSpace axis. Then, we perform spatial downsampling for these obtained key frames. Specifically, the downscaling factor  $s$  is randomly sampled from [1, 8], which represents  $1 \\times \\sim 8 \\times$  space super-resolution. When  $s = 1$ , there is no need to perform spatial super-resolution. All frames in one sequence are downsampled with the same downscaling factor  $s$ . Thus, we arrive at low-frame-rate and low-resolution video frames:  $I_{\\downarrow s}^{1:m:f}$ . In practice, we should upsample them back to the original spatial sizes by bilinear interpolation before being passed to the networks, so we obtain  $I_{\\downarrow s,\\uparrow s}^{1:m:f}$ . Note that each space or time scale corresponds to different difficulty level, and thus the sampling is not uniform. Particularly, we set sampling probabilities of scales  $4 \\times$  and  $8 \\times$  based on a ratio of  $1:2$ , which is determined by their associated scale values.\n\n\nThen, we use the encoder part of a pretrained variational autoencoder (VAE)  $\\mathcal{E}$  to project the input sequence to the latent space frame-wisely:\n\n\n$$\n\n$$\nz _ {s} ^ {1: m: f} = \\left[ \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {1}\\right), \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {1 + m}\\right), \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {1 + 2 \\times m}\\right), \\dots , \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {f}\\right) \\right]. \\tag {3}\n$$\n\n$$\n\n\nNoise augmentation in latent space. At this stage, we conduct noise augmentation to noise the latent condition information in varying degrees in order to achieve controllable refinement. This noise\n\n\naugmentation process is the same as the diffusion process equation 1 used in the video diffusion model. Specifically, the condition latent sequence is corrupted by:\n\n\n$$\n\n$$\nz _ {s, t ^ {\\prime}} ^ {1: m: f} = \\alpha_ {t ^ {\\prime}} z _ {s} ^ {1: m: f} + \\sigma_ {t ^ {\\prime}} \\epsilon^ {1: m: f}, \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\alpha_{t'}$ ,  $\\sigma_{t'}$  determine the signal-to-noise-ratio at time-step  $t'$ , and  $t' \\in \\{1, \\dots, T'\\}$ . Note that the pretrained video diffusion model adopts 1,000 steps ( $T = 1000$  in equation 1). While the noise augmentation only needs to corrupt the low-level information,  $T'$  is set to 300 empirically. For more intuitive denotation, we use  $\\sigma$  instead of  $t'$ . Finally, we arrive at  $z_{s,\\sigma}^{1:m:f} = \\mathcal{E}(I_{\\downarrow s, \\uparrow s})_{\\sigma}^{1:m:f}$ .\n\n\nThe whole process of space-time data augmentation is summarized as follows:\n\n\n$$\n\n$$\nI ^ {1: f} \\rightarrow I ^ {1: m: f} \\rightarrow I _ {\\downarrow s} ^ {1: m: f} \\rightarrow I _ {\\downarrow s, \\uparrow s} ^ {1: m: f} \\rightarrow \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s}\\right) _ {\\sigma} ^ {1: m: f} \\rightarrow \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s}\\right) _ {\\sigma} ^ {1: m: f}. \\tag {5}\n$$\n\n$$\n\n\n# 4.3 VIDEO-AWARE CONDITIONING\n\n\nBesides data augmentation, the corresponding conditioning mechanism should also be designed in order to influence the model training and avoid averaging performance for different space or time scales and noise augmentation. In practice, the condition latent sequence  $z_{s,\\sigma}^{1:m:f}$ , the corresponding downscaling factor  $s$ , and augmented noises  $\\sigma$  are all considered as for conditioning. Please refer to Figure. 4 for more intuitive demonstration.\n\n\nGiven the synthesized condition latent sequence  $z_{s,\\sigma}^{1:m:f}$ , we use one convolution with zero-initialization  $-\\mathrm{Conv}_{\\mathrm{zero}}$  for connecting it to the condition network. Specifically, we have:\n\n\n$$\n\n$$\nf _ {o u t} ^ {1: f} = \\operatorname {C o n v} \\left(z _ {t} ^ {1: f}\\right), \\tag {6}\n$$\n\n$$\n\n\n$$\n\n$$\nf _ {o u t} ^ {1: m: f} = \\operatorname {C o n v} (z _ {t} ^ {1: m: f}) + \\operatorname {C o n v} _ {\\mathrm {z e r o}} (z _ {s, \\sigma} ^ {1: m: f}), (7)\n$$\n\n$$\n\n\nwhere Conv is the first convolution in the condition network,  $z_{t}^{1:f}$  and  $z_{t}^{1:m:f}$  denote the full frames and key frames of noisy latents at timestep  $t$ , respectively. Note that Conv and Convzero share the same hyperparameter configuration (i.e., kernel size, padding, et.al.), As it is shown, only key-frame features in condition network will be added with the condition features, while others remain unchanged. This strategy enables progressive condition injection as the weights of Convzero grows from zero starting point.\n\n\n![](images/5aaf7915b8f99b325432428416afb94b71918cde7f2ed9e28519c81bebf15ef4.jpg)\n\n\n![Figure 4: Video-aware conditioning. For frame that has condition image as input (key frame), we add the condition latent to the noisy latent after one convolution layer. Besides, the embeddings of noise level  $\\sigma$  and downscaling factor  $s$  are added to the existing  $t$  embedding, which will be broadcast to all spatial layers.](images/d9ed791e28f11031d6c5876e43244328fb8379d29f22c0ab3c4517544011bc84.jpg)\n\n\nFor conditioning regarding downscaling factor  $s$  and noise augmentation  $\\sigma$ , we incorporate them to the existing time embedding in the condition network. Specifically, for timestep  $t$ , sinusoidal encoding (Ho et al., 2020; Rombach et al., 2022; Vaswani et al., 2017) is used to provide the model with a positional encoding for time. Then, one MLP (two linear layers with a SiLU (Elfwing et al., 2018) activation layer in between) is applied. Specifically, we have:\n\n\n$$\n\n$$\nt _ {e m b} = \\mathrm {M L P} _ {t} (\\text {S i n u s o i d a l} (t)), \\quad t _ {e m b} ^ {1: f} = \\text {R e p e a t} (t _ {e m b}, f), \\tag {8}\n$$\n\n$$\n\n\nwhere  $t_{emb}^{1:f}$  is obtained by Repeat  $t_{emb}$  by  $f$  times in the frame axis. This time embedding sequence will be broadcast to all ResBlocks in the condition network for timestep injection.\n\n\nAlso, we elucidate the conditioning for noise augmentation. As mentioned in equation 4, noise augmentation shares the same way as diffusion process, but with much smaller maximum timestep (i.e.,  $T' = 300$ ). Thus, we reuse the encoding and mapping for timestep  $t$  in diffusion process. After this, we add a linear layer with zero initialization (denoted as Linear_zero). To conclude, we have:\n\n\n$$\n\n$$\n\\sigma_ {e m b} = \\operatorname {L i n e a r} _ {\\text {z e r o}, \\sigma} (\\mathrm {M L P} _ {t} (\\text {S i n u s o i d a l} (\\sigma)). \\tag {9}\n$$\n\n$$\n\n\nTo achieve video-aware conditioning, we add  $\\sigma_{emb}$  only to the key frames. We repeat  $\\sigma_{emb}$  by  $k$  times to obtain  $\\sigma_{emb}^{1:k}$ , where  $k$  is the number of key frames. The video-aware controlling is presented as follows:\n\n\n$$\n\n$$\nt _ {e m b} ^ {1: m: f} = t _ {e m b} ^ {1: m: f} + \\sigma_ {e m b} ^ {1: k}, \\tag {10}\n$$\n\n$$\n\n\nwhere the addition operation is performed frame-wisely.\n\n\nRegarding downscaling factor  $s$ , the corresponding encoding, mapping and controlling are similar as above. In particular, we newly introduce one  $\\mathrm{MLP}_{\\mathrm{zero}}$ , in which the output layer is zero-initialized. The video-aware conditioning is performed as:\n\n\n$$\n\n$$\ns _ {e m b} = \\mathrm {M L P} _ {\\text {z e r o}, s} (\\text {S i n u s o i d a l} (s)), \\quad s _ {e m b} ^ {1: k} = \\text {R e p e a t} \\left(s _ {e m b}, k\\right), \\tag {11}\n$$\n\n$$\n\n\n$$\n\n$$\nt _ {e m b} ^ {1: m: f} = t _ {e m b} ^ {1: m: f} + s _ {e m b} ^ {1: k}. \\tag {12}\n$$\n\n$$\n\n\nWith our proposed space-time data augmentation and video-aware conditioning, VEnhancer can be well-trained in an end-to-end manner, and yields great performance in handling diversified conditions for generative enhancement. Here we provide a demonstration in Figure 5. For  $4 \\times$  video superresolution, we modify the input downscaling factor  $s$  to produce different results. It is shown that more texture details are generated as  $s$  grows (from smooth to sharp). This indicates that  $s$  can determine how many details are generated through our proposed video-aware conditioning.\n\n\n![Figure 5: The effectiveness of video-aware conditioning. For video super-resolution  $(4\\times)$ , we modify the input downscaling factor from  $s = 1$  to  $s = 8$ , and more texture details are generated. Zoom in for best view.](images/bbdfbce7c4a22214f43db95b7df6126becf0fbf10147fa3c723ddc4f785166c6.jpg)\n\n\n# 5 EXPERIMENTS\n\n\nDatasets. We collect around 350k high-quality and high-resolution video clips from Panda-70M (Chen et al., 2024b) dataset and the Internet to constitute our training set. We train VEnhancer on resolution  $1280 \\times 720$  with center cropping, and the target FPS is fixed to 24 by frame skipping. Regarding test dataset, we collect generated videos from state-of-the-art text-to-video methods. Practically, we select videos with large motions and diverse contents. This test dataset is denoted as AIGC2023, which is used to evaluate VEnhancer and baselines for video super-resolution and space-time super-resolution tasks. For evaluation on VBench, all generated videos based on the provided prompt suite are considered, resulting in around 5k videos.\n\n\nImplementation Details. The batch size is set to 256. AdamW (Loshchilov & Hutter, 2017) is used as the optimizer, and the learning rate is set to  $10^{-5}$ . During training, we dropout the text prompt with a probability of  $10\\%$ . The training process lasts about four days with 16 NVIDIA A100 GPUs. During inference, we use DPM-Solver (Lu et al., 2022) and perform 15 sampling steps with classifier-free guidance ( $cfg$ ) (Ho & Salimans, 2022).\n\n\nMetrics. Regarding evaluation for video super-resolution and space-time super-resolution on AIGC2023 test dataset, we use both image quality assessment (IQA) and video quality assessment (VQA) metrics. Specifically, MUSIQ (Ke et al., 2021) and DOVER (Wu et al., 2023a) are adopted. Moreover, we refer to video generation benchmark, VBench (Huang et al., 2023), for more comprehensive evaluation. Specifically, we choose Dynamic Degree (i.e., whether it contains large motions), Motion smoothness (i.e., how smooth the video is), and Aesthetic Quality for evaluation. Regarding evaluation for video generation, we consider all 16 evaluation dimensions from VBench.\n\n\n# 5.1 COMPARISON WITH VIDEO SUPER-RESOLUTION METHODS\n\n\nFor video super-resolution, VEnhancer is compared with the state-of-the-art real-world video super-resolution method, RealBasicVSR (Chan et al., 2022b), and the state-of-the-art generative video super-resolution method, LaVie-SR (Wang et al., 2023b) (super-resolution). For more comprehensive comparison, we also include I2VGen-XL' (Zhang et al., 2023b) refinement model as our baseline.\n\n\nTable 1: Quantitative comparison for video super-resolution  $(4\\times)$  on AIGC2023 test dataset. Red and blue indicate the best and second best performance. The top 3 results are marked as gray .\n\n\n<table><tr><td></td><td>DOVER\u2191</td><td>MUSIQ\u2191</td><td>Aesthetic Quality</td><td>Dynamic Degree</td><td>Motion Smoothness</td></tr><tr><td>LaVie-SR Wang et al. (2023b)</td><td>0.8427</td><td>55.8428</td><td>0.6692</td><td>0.525</td><td>0.9710</td></tr><tr><td>I2VGen-XL(refiner) Zhang et al. (2023b)</td><td>0.5603</td><td>25.5988</td><td>0.6439</td><td>0.475</td><td>0.9835</td></tr><tr><td>RealBasicVSR Chan et al. (2021)</td><td>0.8252</td><td>50.5978</td><td>0.6622</td><td>0.550</td><td>0.9729</td></tr><tr><td>Ours</td><td>0.8586</td><td>59.4474</td><td>0.6671</td><td>0.550</td><td>0.9781</td></tr></table>\n\n\n![](images/68e212d6ede20b5fcc60370d46fc8352480f2a346b5fc36ebc312146e7237456.jpg)\n\n\n![](images/d0e3a93b88cf91779d1c20c936b13a1bca7170438abf218fc111cf49ce70a739.jpg)\n\n\n![](images/f69e82847ccd0fd9384f3ed01fc2427d09d42343e96b0ee431d4d0e3dc240e36.jpg)\n\n\n![](images/0107f12fa51c2d5f1539b887d6c69bc67b13e2897908e64bf0c5f99af730c6c6.jpg)\n\n\n![](images/994c105e78bbd064d79ff09a4736d96a9c06bb4c5b7369a9450492e314431d05.jpg)\n\n\n![](images/89d64654194c475ed1b98b5888ddfdbfa4a8209d538bd35a337783f256244af1.jpg)\n\n\n![](images/319a0e197cd5eb6bc76ec742fc077eadd20e66f25a188381a8600828a8c303fc.jpg)\n\n\n![](images/1f88cbf19123aaf61ceee5c3d20b7781adaed67a3973781c5e437f43f38417cb.jpg)\n\n\n![](images/3061d7be88193fe28c8e5d2257c99672679dd3ec03f19022b56e0bce9def7eac.jpg)\n\n\n![](images/bfea63d696a3dd7e0b04f5410095b318f780ec1bf4ee945dc441d8a55460ae1a.jpg)\n\n\n![](images/e6400c8db0e7370145f3c04a1f0c0e4d31ce838c5e31348d0eae1fe9130b29a8.jpg)\n\n\n![](images/a03cdaeee5bbb84cdb98982b806177301f17ffa60e928a5c05671570d168e7d4.jpg)\n\n\n![Figure 6: Visual comparison for video super-resolution  $(4\\times)$  on AIGC2023 test dataset. Input resolution:  $512\\times 312$ ; output resolution:  $2048\\times 1280$ . Prompt: Iron Man flying in the sky.](images/b74b072b25ab349085849feda257baacb69a05aa33f19cbeb63949320838aa51.jpg)\n\n\n![](images/025a0d34e2585e5b86fcfeeffb0519efa261ac8856f6cef60655a04b3d48cdab.jpg)\n\n\n![](images/b2394c6de2ab73a2d02f7cd5f7c782f093127709ccbe687af58b6d549fc21c76.jpg)\n\n\nAs shown in Table 1, VEnhancer outperforms both generative video super-resolution method (LaVie-SR), real-world video super-resolution method (RealBasicVSR), and generative video refinement method (I2VGen-XL, refiner) in most metrics, suggesting its outstanding enhancement ability for videos. Note that LaVie-SR surpasses RealBasicVSR in image/video quality (MUSIQ, DOVER, and Aesthetic Quality), as diffusion-based methods are better at generating sharp details. But LaVie-SR achieves the worst Motion Smoothness, indicating its insufficient capability in balancing video smoothness and video quality. I2VGen-XL's refiner could obtain the highest score in Motion Smoothness, but sacrifices the magnitude of motion significantly (worst Dynamic Degree). Moreover, it achieves very unsatisfactory results in metrics evaluating image and video quality. Because it uses bilinear interpolation for  $\\times 4$  upsampling, which produces very blurry results. Nevertheless, VEnhancer achieves overall best results with good balance among image/video quality, motion smoothness, and motion magnitude.\n\n\nThe visual comparison is presented in Figure. 6. The prompt is \"Iron man flying in the sky\". The input video is already consistent with the prompt, but lacks details on the iron man suit. RealBasicVSR could remove some noises or artifacts of the generated videos as it incorporates complex degradation for model training. However, it fails in generating realistic details but produces over-smoothed results, since its generative ability is limited. On the other hand, the results of LaVie-SR contains more artifacts than input. Without successfully removing artifacts, the generative super-resolution model will enlarge the existing defects. The refiner of I2VGen-XL could achieve successful refinement but produces over-smoothed results. In contrast, VEnhancer could first remove unpleasing artifacts and refine the distorted content (e.g., head region), and then generate faithful details (e.g., helmet and armor) that are consistent with the text prompt.\n\n\n# 5.2 COMPARISON WITH SPACE-TIME SUPER-RESOLUTION METHODS\n\n\nFor space-time super-resolution task, we compare two state-of-the-art space-time super-resolution methods: VideoINR (Chen et al., 2022) and Zooming-Slow-Mo (Xiang et al., 2020) (Zoom for short). We also consider LaVie's cascaded T-SR and S-SR DM-based pipeline: LaVie-FI (frame interpolation) + LaVie-SR (super-resolution) for more thorough comparison.\n\n\nTable 2: Quantitative comparison for space-time super-resolution  $(4\\times)$  on AIGC2023 test dataset. Red and blue indicate the best and second best performance. The top 3 results are marked as gray .\n\n\n<table><tr><td></td><td>DOVER\u2191</td><td>MUSIQ\u2191</td><td>Aesthetic Quality</td><td>Dynamic Degree</td><td>Motion Smoothness</td></tr><tr><td>LaVie-FI + LaVie-SR Wang et al. (2023b)</td><td>0.8159</td><td>53.2128</td><td>0.6566</td><td>0.60</td><td>0.9857</td></tr><tr><td>VideoINR Chen et al. (2022)</td><td>0.7608</td><td>34.1060</td><td>0.6624</td><td>0.60</td><td>0.9933</td></tr><tr><td>Zooming Slow-Mo Xiang et al. (2020)</td><td>0.7328</td><td>33.8470</td><td>0.6624</td><td>0.55</td><td>0.9908</td></tr><tr><td>Ours</td><td>0.8609</td><td>51.2940</td><td>0.6710</td><td>0.60</td><td>0.9937</td></tr></table>\n\n\nAs shown in Table 2, we observe that our method surpasses all baselines in DOVER and Aesthetic Quality, showing its superior capability in generating sharp and realistic video content. Besides, it obtains highest scores in Motion Smoothness and Dynamic Degree, indicating VEnhancer's excellent ability in synthesizing stable temporal details. The cascaded T-SR and S-SR approach (LaVie-FI + LaVie-SR) obtains good scores in DOVER and MUSIQ, demonstrating DM-based methods' advantage in generation. However, its performance in temporal aspect is unsatisfactory due to its inferior capability in temporal refinement. We notice that state-of-the-art space-time super-resolution methods (VideoINR and Zooming Slow-Mo) behave well in Motion Smoothness. As both of them are optimized with reconstruction loss, the produced results are very smooth across frames. At a cost, they perform poorly in metrics regarding quality, such as DOVER and MUSIQ.\n\n\nThe visual comparison is illustrated in Figure. 7. The first and third columns present the low-resolution key frames. Note that the input frames are not consistent especially in the region of guitar strings. The cascaded T-SR and S-SR approach, LaVie-FI + LaVie-SR, can produce very sharp results for all frames (key and predicted ones). However, it generates messy contents which are not semantically aligned with prompt. Moreover, the generated details are changing across time, indicating severe flickering. For reconstruction-based methods (VideoINR and Zoom), the produced results are similar: lacking details and failing in improving the consistency of the original input frames. On the contrary, VEnhancer is not only able to achieve unified space-time super-resolution, but can also improve the temporal consistency of the generated videos by refinement (i.e., guitar strings and raccoon hands).\n\n\n# 5.3 EVALUATION ON IMPROVING VIDEO GENERATION\n\n\nHere we evaluate VEnhancer's ability in improving state-of-the-art T2V methods. The baselines include open-source T2V methods-VideoCrafter-2 (Chen et al., 2024a) (VC-2 for short), Lavie (Wang et al., 2023b), Open-Sora, CogVideoX (Yang et al., 2024), and professional video generation products-Pika and Gen-2. In particular, we enhance T2V results of CogVideoX-5B and VC-2.\n\n\nThe quantitative results are organized in Table 3. Before enhancement, CogVideoX-5B and VC-2 achieve the best and second best in Semantic compared with other baselines, demonstrating their superiority in generating video contents that are highly consistent to the VBenchmark's prompt suite. Regarding Quality, they lag behind Pika. But with VEnhancer, VC-2 and CogVideoX-5B are able to achieve the highest and second highest scores in Quality. Besides, their scores regarding Semantic improves a lot, especially VC-2' (3.31% increase). This indicates that VEnhancer can improve the\n\n\nTable 3: VBenchmark Evaluation Results. This table compares the performance of open-source T2V methods and professional video generation products regarding two aspects (Quality and Semantic). A higher score indicates better performance. Red and blue indicate the best and second best performance. The top 3 results are marked as gray .\n\n\n<table><tr><td></td><td>LaVie\nWang et al. (2023b)</td><td>Open-Sora</td><td>Pika</td><td>Gen-2</td><td>CogVideoX-5B\nYang et al. (2024)</td><td>VC-2\nChen et al. (2024a)</td><td>CogVideoX-5B\n+Venhancer</td><td>VC-2\n+VEnhancer</td></tr><tr><td>Quality</td><td>78.78%</td><td>80.71%</td><td>82.92%</td><td>82.47%</td><td>82.75%</td><td>82.20%</td><td>82.99%</td><td>83.28%</td></tr><tr><td>Semantic</td><td>70.31%</td><td>73.30%</td><td>71.77%</td><td>73.03%</td><td>77.04%</td><td>73.42%</td><td>77.52%</td><td>76.73%</td></tr><tr><td>Overall</td><td>77.08%</td><td>79.23%</td><td>80.69%</td><td>80.58%</td><td>81.61%</td><td>80.44%</td><td>81.90%</td><td>81.97%</td></tr></table>\n\n\nsemantic content and video quality at the same time, showing a powerful generative enhancement ability. More importantly, the advantage of adopting a two-stage pipeline is observed: the first T2V model focuses on generating semantic content and motions with good fidelity to the prompts, while the following enhancement model can improve the semantic in low-level and image quality, as well as temporal consistency. For visual results, please see the video demonstration in supp.\n\n\n# 5.4 ABLATION STUDIES\n\n\nThe Effectiveness of Noise Augmentation. During training, the noise level regarding noise augmentation is randomly sampled within a predefined range. While during inference, one can change the noise level to achieve refinement with different strengths. In general, higher noise corresponds to stronger refinement and regeneration. We present the visual comparison among different noise levels in Figure 8. The first frame of one AI-generated video is presented in the left. It is of low-resolution and lacks details. Also, the original video has very obvious flickering. If we set  $\\sigma = 0$ , VEnhancer will generate unpleasant noises in the background. As there is domain mismatch between the training data and testing data, the enhancement fails in handling unseen and challenging scenarios. Fortunately, we can mitigate this by adding noise in the condition latents for corrupting the noisy and unknown low-level details. As we increase the noise level, the artifacts are gradually vanishing. When  $\\sigma = 250$ , the result is noise-clean, and has abundant semantic details.\n\n\nArbitrary Up-sampling Scales for Spatial Super-Resolution. Here we show that VEnhancer is able to up-sample videos with arbitrary scales. From Figure 9, we observe that VEnhancer could produce satisfactory results on different scales  $(2.5\\times,3\\times,3.5\\times,4\\times,$  and  $4.5\\times)$ , suggesting its flexibility and generalization in adapting to different tasks. In particular, given one frame of the generated video  $(312\\times 512)$ , VEnhancer could improve the generated details when the up-sampling scale grows up. When  $s = 2.5\\sim 3.5$ , the panda's hand is less realistic. But it becomes better when  $s = 4$  or  $s = 4.5$ . It is also noticed that the panda' fur is becoming more realistic as  $s$  grows.\n\n\nArbitrary Up-sampling Scales for Temporal Super-Resolution. In this part, we show VEnhancer is able to achieve arbitrary up-sampling in time axis. Given two low-resolution key frames, we aim to up-sample them to high-resolution ones, and also interpolate several frames (ranging from 2 to 4) between them. As shown in Figure 10, the results are consistent across frames, showing not flicking or distortions. Besides, the spatial quality has also been significantly improved. As shown in the last row,  $5 \\times$  frame interpolation yields smooth frames with generated contents: the shadow in the right leg is changing, showing a very natural transition. This indicates that diffusion-based frame interpolation has great capability in both motion and content generation.\n\n\n# 6 CONCLUSION AND LIMITATION\n\n\nIn this work, we propose a generative space-time enhancement method - VEnhancer for video generation. It can achieve spatial super-resolution, temporal super-resolution and video refinement in one video diffusion model. We base on a pretrained generative video prior and build a Space-Time Controller (ST-Controller) for effective condition injection. Space-time data augmentation and video-aware conditioning are proposed to train ST-Controller in an end-to-end manner. Extensive experiments have demonstrated our superiority over state-of-the-art video super-resolution and space-time super-resolution methods in enhancing AI-generated videos. However, our work has several limitations. First, as it is based on diffusion models, the inference takes more time than one-step methods. Second, it may face challenges in handling AI-generated long videos, since the long-term (over 10s) consistency has not been addressed in this work.\n\n\n# REFERENCES\n\n- Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021.\n\n- Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023a.\n\n- Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22563-22575, 2023b.\n\n- Jiezhang Cao, Yawei Li, Kai Zhang, and Luc Van Gool. Video super-resolution transformer. arXiv preprint arXiv:2106.06847, 2021.\n\n- Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Basicvsr: The search for essential components in video super-resolution and beyond. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4947-4956, 2021.\n\n- Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Basicvsr++: Improving video super-resolution with enhanced propagation and alignment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5972-5981, 2022a.\n\n- Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Investigating tradeoffs in real-world video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5962-5971, 2022b.\n\n- Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023a.\n\n- Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocaster2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024a.\n\n- Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-\u03b1: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023b.\n\n- Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Genton: Delving deep into diffusion transformers for image and video generation. arXiv preprint arXiv:2312.04557, 2023c.\n\n- Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479, 2024b.\n\n- Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vedit Goel, Zhangyang Wang, Humphrey Shi, and Xiaolong Wang. Videoinr: Learning video implicit neural representation for continuous space-time super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2047-2057, 2022.\n\n- Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:3-11, 2018.\n\n- Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024.\n\n- Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023.\n\n- Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos\u00e9 Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023.\n\n- Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita. Space-time-aware multi-resolution video enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2859-2868, 2020.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024.\n\n- Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n\n- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020.\n\n- Jonathan Ho, William Chan, Chitwan Sahara, Jay Whang, Ruiqi Gao, Alexey Gritsanko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.\n\n- Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. arXiv preprint arXiv:2311.17982, 2023.\n\n- Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, and Qi Tian. Video superresolution with recurrent structure-detail network. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XII 16, pp. 645-660. Springer, 2020a.\n\n- Takashi Isobe, Songjiang Li, Xu Jia, Shanxin Yuan, Gregory Slabaugh, Chunjing Xu, Ya-Li Li, Shengjin Wang, and Qi Tian. Video super-resolution with temporal group attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8008-8017, 2020b.\n\n- Takashi Isobe, Fang Zhu, Xu Jia, and Shengjin Wang. Revisiting temporal modeling for video super-resolution. arXiv preprint arXiv:2008.05765, 2020c.\n\n- Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5148-5157, 2021.\n\n- Soo Ye Kim, Jihyong Oh, and Munchurl Kim. Fisr: Deep joint frame interpolation and superresolution with a multi-scale temporal loss. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 11278-11286, 2020.\n\n- Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu Timofte, and Luc V Gool. Recurrent video restoration transformer with guided deformable attention. Advances in Neural Information Processing Systems, 35:378-393, 2022.\n\n- Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Rakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van Gool. Vrt: A video restoration transformer. IEEE Transactions on Image Processing, 2024.\n\n- Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Wanli Ouyang, Yu Qiao, and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior, 2024.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\n- Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022.\n\n- Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.\n\n- Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684-10695, 2022.\n\n- Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\n- Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023a.\n\n- Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. Edvr: Video restoration with enhanced deformable convolutional networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 0-0, 2019.\n\n- Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023b.\n\n- Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In International Conference on Computer Vision (ICCV), 2023a.\n\n- Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7623-7633, 2023b.\n\n- Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P Allebach, and Chenliang Xu. Zooming slow-mo: Fast and accurate one-stage space-time video super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3370-3379, 2020.\n\n- Liangbin Xie, Xintao Wang, Shuwei Shi, Jinjin Gu, Chao Dong, and Ying Shan. Mitigating artifacts in real-world video super-resolution models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 2956-2964, 2023.\n\n- Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. Video enhancement with task-oriented flow. International Journal of Computer Vision, 127:1106-1125, 2019.\n\n- Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024.\n\n- Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3836-3847, 2023a.\n\n- Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023b.\n\n- Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-video: Temporal-consistent diffusion model for real-world video super-resolution. arXiv preprint arXiv:2312.06640, 2023.\n\nResponse: \\boxed{Accept}\n\nThis paper presents a significant advancement in the field of generative video enhancement by proposing VEnhancer, a unified model capable of achieving spatial and temporal super-resolution, as well as video refinement. Here\u2019s a detailed breakdown of why I predict acceptance:\n\n### Originality\nThe paper introduces a novel approach to generative video enhancement by proposing VEnhancer, which integrates both spatial and temporal super-resolution capabilities within a single model. Unlike existing methods that separate spatial and temporal enhancement into distinct stages, VEnhancer provides a more integrated solution. The authors justify this by noting the redundancy and inefficiency of sequential spatial and temporal super-resolution pipelines, and by highlighting the limitations of existing refinement methods in balancing video quality and fidelity to the original content.\n\n### Methodological Rigor\nThe methodology is well-explained, detailing the architecture design, data augmentation strategies, and video-aware conditioning mechanisms. The authors provide a comprehensive explanation of the space-time data augmentation, which is crucial for training the ST-Controller in an end-to-end manner. They also discuss the video-aware conditioning, which is vital for handling different augmentation parameters across frames and ensuring that the model maintains the integrity of the original input video.\n\n### Significance of Contribution\nThe proposed method addresses critical challenges in generative video enhancement, such as the redundancy of separate spatial and temporal super-resolution pipelines, the inability of existing refinement methods to maintain video quality and fidelity, and the lack of flexibility in dealing with different upscaling factors and refinement strengths. The paper demonstrates that VEnhancer outperforms state-of-the-art methods in various metrics,", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634740.4641502", "msecs": "464.0", "relativeCreated": "85287.62435913086", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# ChemAgent: Self-updating Memories in Large Language Models Improves Chemical Reasoning\n\n# Abstract\nChemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code ef- effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science.\n\n\n# 1 INTRODUCTION\n\n\nChemical reasoning presents unique challenges in the realm of artificial intelligence, demanding sophisticated reasoning and precise calculations beyond typical reasoning tasks (McQuarrie, 2008; Atkins et al., 2014; Talanquer, 2022; Guo et al., 2023b; Cao et al., 2024). For example, the GPT solution with CoT prompting in Figure 1 contains numerous errors in both the calculation process and the chemical constants used. Even in short reasoning chains, a single error can cascade, reducing answer quality and escalating the probability of additional errors (Liao et al., 2024; Sun et al., 2024b).\n\n\nRecent advancements in large language models (LLMs) have demonstrated capabilities in simpler scientific tasks or chemical scenarios that do not require complex reasoning (Boiko et al., 2023; Atkins et al., 2023; Wang et al., 2024a; Hu & Shu, 2023; Xiao et al., 2024; Darvish et al., 2024; Skreta et al., 2024). However, their application to complex chemical reasoning reveals significant limitations Pei et al. (2024); Li et al. (2024a). LLMs often (1) struggle to effectively utilize domain-specific formulas, (2) exhibit incorrect reasoning steps, and (3) produce errors when combining textual reasoning with Python code for calculations (Zhong et al., 2024b); here, syntax errors may arise, causing the code to fail to compile. As shown in Figure 1 (StructChem output), errors often arise from determining a constant's incorrect form or unit.\n\n\nSome Previous approaches to address these challenges in chemical reasoning tasks have focused on decomposing the reasoning process (Zhong et al., 2023), e.g., formatting the reasoning steps through techniques such as self-reflection (Shinn et al., 2023) and StructChem prompting (Ouyang et al., 2024). However, these methods often rely heavily on human-curated knowledge (Chan et al., 2023), or fixed workflows (Fu et al., 2022; Wang et al., 2023; Zhou et al., 2023). Unlike human learners who utilize a library system to retain and apply previous experiences, these methods lack\n\n\n# PROBLEM (HYDROGEN ATOM ENERGY LEVELS)\n\n\nAn electron in a hydrogen atom transitions from an excited state with the principal quantum number  $n = 4$  to the ground state  $(n = 1)$ . Calculate: a) The energy of the emitted photon in joules (J), b) The wavelength of the emitted photon in nanometers (nm), c) The frequency of the emitted photon in terahertz (THz).\n\n\n![Figure 1: Comparison of problem-solving approaches for a hydrogen atom energy transition problem. The figure illustrates three different methods: (a) shows a standard Chain-of-Thought approach with calculation errors (in steps 3 and 4) in Wang et al. (2024a). (b) demonstrates the StructChem (Ouyang et al., 2024) method with formula generation and step-by-step reasoning but fails due to an incorrect constant and incorrect unit conversion (in steps 1 and 4). (c) presents the ChemAgent solution, featuring task decomposition, memory retrieval from the library, and reasoning, leading to the accurate final answer.](images/c4d1fe7fcb6aff12c6920faf030081d79c1369535709acd6e471b2d6d9509a4f.jpg)\n\n\n![](images/6b9104f5524bf62f8ce5259565a4c01bcf872049ab59a45af48f8131d7943c2f.jpg)\n\n\n![](images/09dbdf7506263c8919abff0077f7f7751005888ea55e84facb58c558ed6a6797.jpg)\n\n\nthe ability to remember and learn from past analogous problems. For instance, humans can abstract and store theorems or solution strategies from previous tasks and utilize this memory for future problem-solving. $^{1}$\n\n\nBuilding on these insights, ChemAgent introduces a dynamic \"LIBRARY\" system that facilitates iterative problem-solving by continuously updating and refining its content based on task decomposition. The LIBRARY in ChemAgent serves as a comprehensive repository where decomposed chemical tasks are stored. These tasks are broken down into sub-tasks, and their solutions are compiled in the library for future reference. As new tasks are encountered, the library is updated with new sub-tasks and corresponding solutions, ensuring its content remains relevant and progressively improves over time. In parallel, inspired by human cognitive mechanisms (Osman, 2004; Kaufman, 2011), our system incorporates three main memory components: Planning Memory for high-level strategies, Execution Memory for specific task solutions, and Knowledge Memory for fundamental chemistry principles. These memories are stored externally, allowing the model to retrieve them efficiently during the problem-solving process. Unlike previous works relevant to LLM's external memory system (Huang et al., 2024a; Zhong et al., 2024a; Zhang et al., 2023; Li et al., 2024b; Guo et al., 2023a; Zhang et al., 2024b), we carefully integrate all these components in a complete agentic framework and allow them to update dynamically.\n\n\nThese memories are stored in a structured tree-based format that allows for efficient retrieval during the problem-solving process. When ChemAgent encounters a new problem, it first decomposes the task into manageable sub-tasks, then leverages the library to retrieve relevant sub-tasks and solutions stored in its memory components. The retrieved information is validated and refined using analogous sub-tasks previously encountered, optimizing both task decomposition and solution generation. The library is dynamically updated by adding new sub-tasks and solutions as they are encountered and validated. This iterative process ensures that the memory is continuously enriched with new strategies and solutions, mimicking human problem-solving improvements through practice. By maintaining an evolving domain knowledge base, ChemAgent enables LLMs to autonomously navigate the problem-solving process, thereby enhancing their performance on similar tasks over time.\n\n\nOur experiments are conducted on four chemical reasoning datasets from SciBench (Wang et al., 2024a) with GPT-3.5, GPT-4 (OpenAI et al., 2024), and open-source models like Llama3\n\n\n![Figure 2: The diagram of our overall framework. It contains (a) library-enhanced reasoning and (b) library construction. (a) illustrates how ChemAgent utilizes the library to address a new task for the test set. And (b) demonstrates the construction of the library over the dev set, including Plan Memory  $\\mathcal{M}_p$  and Execution Memory  $\\mathcal{M}_e$ .](images/11429a3d16f9569a5c20c31d8aeba45b07216b206661ae7319c988910307b8a0.jpg)\n\n\n![](images/bd0c74abbecaceecb224a980485e107b447712119560561bf69ed709d17b7a21.jpg)\n\n\n(Llama Team, 2024). Experimental results indicate that ChemAgent significantly enhances the accuracy, though the degree of improvement achieved by our model varies accordingly due to the varying sizes of these datasets. Compared to the base model (take GPT-4 for example), the application of memory self-improving leads to an average increase in accuracy by  $37\\%$ , with a maximum improvement of  $46\\%$ . In comparison with the current state-of-the-art model, StructChem (Ouyang et al., 2024), our approach achieves an average improvement of  $10\\%$  and a maximum improvement of  $15\\%$  across different datasets. Furthermore, the improvement is more pronounced for stronger base models: GPT-4 exhibits greater enhancements compared to GPT-3.5 when augmented with our framework. Additionally, we analyze the role of our library system and examine the benefits of different memory updates across various stages. This analysis provides insights into how our iterative library updates contributed to performance improvements in sub-task resolution and optimal task decomposition. By continuously enriching the library with structured memory components, ChemAgent ensures that the problem-solving process is progressively refined and optimized, leading to substantial improvements in accuracy.\n\n\n# 2 METHOD\n\n\n# 2.1 PRELIMINARIES\n\n\nAnalogous to how students organize and reference their problem-solving approaches for exams, our motivation for developing a library system in ChemAgent is to enhance LLMs' ability to tackle complex problems by providing structured access to a repository of previous sub-tasks and solutions.\n\n\nThe overall reasoning framework is shown in Figure 2 (a). Formally, in a simplified form, given a complex and open-ended chemical problem  $\\mathcal{P}_S$  as input, our method aims to generate a solution  $\\mathcal{O}_S$ . The problem  $\\mathcal{P}_S$ , which comprises problem descriptions  $\\mathcal{T}_S$  and initial conditions  $\\mathcal{C}_S$ , is firstly decomposed into a series of sub-tasks  $\\mathcal{P}_{S_i}$ . The solution  $\\mathcal{O}_S$  is then synthesized from the subsolutions  $\\mathcal{O}_{S_i}$  of the sub-tasks. Each  $\\mathcal{O}_{S_i}$  includes intermediate steps, such as formulae, reasoning steps, code, and calculations. Then, a final answer  $\\mathcal{A}_S$  is derived from the overall solution  $\\mathcal{O}_S$ . We evaluate the performance via the accuracy of  $\\mathcal{A}_S$  against a ground truth  $\\mathcal{A}_{S_g}$ .\n\n\n# 2.2 COMPOSITION OF THE LIBRARY\n\n\nWe divide library into three main memory components: planning memory, execution memory, and knowledge memory. Figure 3 provides detailed examples. The exact context of memory and corresponding prompts are in Appendix G.\n\n\n- Planning Memory  $(\\mathcal{M}_p)$ : This component stores high-level strategies and methodologies for approaching complex problems.\n\n\n![Figure 3: Given a task  $\\mathcal{P}$ , the relevant memory examples are provided in the library. Specifically, while Execution Memory  $(\\mathcal{M}_e)$  and Plan Memory  $(\\mathcal{M}_p)$  are derived from prior experiences, Knowledge memory  $(\\mathcal{M}_k)$  is generated by LLM based on the problem prompt. The conditions  $\\mathcal{C}$  are not explicitly presented here but are embedded within  $\\mathcal{P}$  and the [GOAL] of  $\\mathcal{M}_e$ . Figure 2 and Figure 4 illustrates this workflow. In the following sections, we explain how each part is constructed and utilized. All prompts used in our framework can be found in Appendix G.](images/d97efd4a2ba11c0158298bbff145ff67856931ddfc2732a22ef5d9872f2ed09b.jpg)\n\n- - Execution Memory  $(\\mathcal{M}_e)$ : This contains highly structured descriptions of specific problem contexts and their corresponding solutions, serving as detailed execution plans.\n\n- - Knowledge Memory  $(\\mathcal{M}_k)$ : This houses fundamental chemistry principles and formulas, acting as a ready reference. This component is generated temporarily during the solution of a specific task and is not intended for permanent retention.\n\n\nThus, ChemAgent enables LLMs to tackle problems proactively, form new memories from these attempts, utilize existing memories to solve complex problems, and continuously refine their solutions. Built upon the library of three types of memories, ChemAgent operates in two main stages:\n\n- 1. Initial Library Construction: Complex problems in the development set are decomposed into structured atomic sub-tasks, which serve as building blocks of the library, consisting of static long-term memory  $(\\mathcal{M}_p$  and  $\\mathcal{M}_e)$ .\n\n- 2. Inference Phase and Dynamic Memory Updating:\n\n- (a) Memory-Enhanced Reasoning: The memory is dynamically enriched and improved during runtime.\n\n- (b) Evaluation & refinement: Each sub-task's solution is evaluated on their veracity and relevance to the main task, which leads to a correction of the solution or a refinement of the overall plan.\n\n\n# 2.3 DECOMPOSITION AS ATOMIC BLOCKS\n\n\nHuman problem-solvers naturally break down complex chemistry problems into smaller, manageable sub-tasks (Zhou et al., 2023). This decomposition enhances the understanding of individual components and their interactions, facilitating the resolution of the original problem in a structured manner (Johnson et al., 2017). These sub-tasks not only improve the reasoning process but also function as atomic building blocks within the execution memory, each paired with a corresponding sub-solution. Key characteristics of this decomposition include:\n\n\nHierarchical Decomposition: Using the strategy stored in Planning Memory, we break down complex chemistry problems into hierarchical sub-tasks. If a sub-task is too difficult to complete, it is further decomposed until it can be executed in a single step by the LLM. We store each decomposed sub-task and its intermediate sub-solutions, enabling direct retrieval of solutions for similar future problems. This hierarchical approach ensures each sub-task acts as an independent atomic block, aiding future problem-solving.\n\n\nStructured Sub-tasks: To maximize the efficiency of decomposition, sub-tasks are structured into four distinct parts: (i) Task Query defining the specific question, (ii) Sub-task Objectives outlining\n\n\nclear goals, (iii) Step-by-step Solution detailing the method to address the sub-task, and (iv) Guidance for Solutions suggesting potential approaches.\n\n\nAtomic Blocks as Memory: Decomposed sub-tasks serve as building blocks for memory. Identifying and recalling similar problems can be challenging, but sub-tasks often share commonalities that make it easier to leverage past experiences. These atomic blocks also function as examples in few-shot prompting, standardizing response formats and bolstering the overall problem-solving process.\n\n\nBuilding on the concept of sub-tasks as fundamental units, we outline the processes involved in our framework, which includes both a dynamic library system and the structured memory inside our library. This section details how we initially construct the library and memory (\u00a72.4), utilize and update them for new tasks (\u00a72.5), and evaluate and refine the generated solutions (\u00a72.6). Specifically, our approach leverages the development set to build a library, which is designed to dynamically self-improve during runtime.\n\n\n# 2.4 LIBRARY CONSTRUCTION\n\n\nThe Library is constructed using the development set. As outlined in \u00a72.3, we leverage sub-tasks decomposed from each problem as the execution memory units. Each execution memory unit  $\\mathcal{U}_i$  is defined as follows, where  $\\mathcal{C}$  represents the conditions of a given problem  $\\mathcal{P}$ , and  $\\mathcal{T}_i$  and  $\\mathcal{O}_i$  denote the sub-task and its corresponding sub-solution, respectively:\n\n\n$$\n\n$$\n\\mathcal {U} _ {i} = \\left(\\mathcal {C}, \\mathcal {T} _ {i}, \\mathcal {O} _ {i}\\right) \\quad \\text {f o r} \\quad i = 1, 2, \\dots , k\n$$\n\n$$\n\n\nGiven a problem  $\\mathcal{P}$  and its corresponding solution  $S$  in the development set, our method begins by identifying and extracting the conditions from  $\\mathcal{P}$ . We then verify these conditions for accuracy to ensure that the subsequent steps operate with precise and correctly parsed data. Based on the identified conditions, we instruct the LLMs to generate detailed\n\n\nsub-tasks. For each identified sub-task  $\\mathcal{T}_i$ , the corresponding sub-solutions  $\\mathcal{O}_i$  are parsed from  $\\mathcal{S}$  and assigned accordingly. Inspired by curriculum learning (Bengio et al., 2009), we then rank the memory units  $\\mathcal{U}$  based on their difficulty. In addition to ranking, we discard any memory units that do not meet a predefined confidence threshold, as evaluated by the LLMs. This ensures that the memory utilized for future problem-solving is both relevant and reliable, enhancing the LLM's ability to tackle increasingly complex chemistry problems. The detailed memory construction process is further described in Algorithm 1.\n\n\n```\nInput: Development set  $\\mathcal{D}$  LLM  $\\mathcal{F}$  prompts  $\\{p_{\\mathrm{split}},p_{\\mathrm{ref}},p_{\\mathrm{rank}}\\}$    \nOutput: Static memory  $\\mathcal{M}$  consisting of units  $\\mathcal{U} =$  {condition, question, solution}   \nfor  $(\\mathcal{P},\\mathcal{S})$  in  $\\mathcal{D}$  do Conditions  $\\mathcal{C}\\gets \\mathcal{F}(p_{\\mathrm{split}}||\\mathcal{P})$  // Verify conditions and refine if necessary  $\\mathcal{C}\\gets \\mathcal{F}(p_{\\mathrm{ref}}||\\mathcal{P}||\\mathcal{C})$  Sub-tasks  $\\mathcal{T}\\gets \\mathcal{F}(p_{\\mathrm{task}}||\\mathcal{P}||\\mathcal{C})$  Sub-solutions  $\\mathcal{O}\\gets \\mathcal{F}(p_{\\mathrm{sol}}||\\mathcal{C}||\\mathcal{P}||\\mathcal{S})$  Assert len  $(\\mathcal{C}) = \\text{len} (\\mathcal{T}) = \\text{len} (\\mathcal{O})$  for i in len(C) do Add  $(\\mathcal{C}_i,\\mathcal{T}_i,\\mathcal{O}_i)$  into U  $\\mathcal{U}\\gets \\mathcal{F}(p_{\\mathrm{rank}}||\\mathcal{U})$    \nreturn U\n```\n\n\n# 2.5 LIBRARY-ENHANCED REASONING\n\n\nDuring testing, we first decompose a given problem into several sub-tasks. For each sub-task, we retrieve related memory units  $\\mathcal{U}_r$  to aid in solving it. Specifically, we compute the similarity between the given sub-task and units stored in the memory. Memory units with similarity above a predefined threshold  $\\theta$  are used to assist the model in determining the answer for the sub-task.\n\n\nFormally, let  $(\\mathcal{C}_j,\\mathcal{T}_j)$  represent a sub-task decomposed from a new problem. We retrieve memory units  $\\mathcal{U}_r$  that satisfy\n\n\n$$\n\n$$\n\\begin{array}{l} \\text {S i m i l a r i t y} (\\mathcal {T} _ {j}, \\mathcal {T} _ {\\mathcal {U} _ {i}}) \\geq \\theta \\quad \\text {f o r} \\quad \\mathcal {U} _ {i} \\in \\mathcal {M} _ {e}. \\end{array}\n$$\n\n$$\n\n\nSpecifically, the similarity between tasks is calculated using Llama3's embeddings:\n\n\n$$\n\n$$\n\\operatorname {S i m i l a r i t y} \\left(T _ {a}, T _ {b}\\right) = \\frac {\\operatorname {E m b e d} \\left(T _ {a}\\right) \\cdot \\operatorname {E m b e d} \\left(T _ {b}\\right)}{\\left| \\left| \\operatorname {E m b e d} \\left(T _ {a}\\right) \\right| \\right| \\times \\left| \\left| \\operatorname {E m b e d} \\left(T _ {b}\\right) \\right| \\right|}.\n$$\n\n$$\n\n\nIn scenarios where the execution memory does not contain similar sub-tasks, we incorporate a self-improving mechanism. Initially, we enrich the memory with the required information by leveraging the internal parametric knowledge of LLMs. The LLM is instructed to identify the topic of the given sub-task (e.g., quantum chemistry or thermodynamics) and generate self-created chemistry\n\n\nproblems related to that topic, adhering to specific guidelines. This forms a kind of \"synthetic\" execution memory. We encourage the LLMs to generate diverse problems and solutions beyond the provided examples, thereby enhancing the robustness and adaptability of the memory system.\n\n\nMoreover, the memory is continuously updated with newly solved sub-tasks and their solutions during runtime. For each newly solved sub-task  $\\mathcal{T}_j$ , sub-answers from previous sub-problems are incorporated into the conditions as  $\\mathcal{C}_j$ , and the execution memory is updated based on solution  $\\mathcal{O}_j$ :\n\n\n$$\n\n$$\n\\mathcal {M} _ {e} = \\mathcal {M} _ {e} \\cup \\left\\{\\left(\\mathcal {C} _ {j}, \\mathcal {T} _ {j}, \\mathcal {O} _ {j}\\right) \\right\\}.\n$$\n\n$$\n\n\nThe plan memory will be updated by a summary of the overall strategy knowledge used,  $\\mathcal{K}_j$  (e.g. formulas, conception, order of resolution):\n\n\n$$\n\n$$\n\\mathcal {M} _ {p} = \\mathcal {M} _ {p} \\cup \\left\\{\\left(\\mathcal {T} _ {j}, \\mathcal {K} _ {j}\\right) \\right\\}.\n$$\n\n$$\n\n\nThis dynamic updating ensures that the memory evolves and improves over time, progressively enhancing the problem-solving capabilities. More details on self-evolution are in \u00a73.3.\n\n\n# 2.6 EVALUATE & REFINE MODULE\n\n\nTo enhance the flexibility and reliability of ChemAgent, we propose an evaluation & refinement module, to correct the planning trajectory and verify the response to each sub-task.\n\n\nAs illustrated in Figure 4, after addressing a specific sub-task  $\\mathcal{P}_i$ , ChemAgent examines the sub-solution  $\\mathcal{O}_i$ . The system evaluates whether the sub-solution conflicts with fundamental knowledge in  $\\mathcal{M}_k$  or contains common errors, such as incorrect units. If discrepancies are identified, a new sub-solution  $\\mathcal{O}'_i$  is generated by refining the original  $\\mathcal{O}_i$  based on the relevant knowledge in  $\\mathcal{M}_k$  and the identified mistakes.\n\n\n![Figure 4: Overall framework of the evaluation & refinement module. ChemAgent continuously modifies the solution or the comprehensive strategy until it either reaches the maximum number of trials or meets the evaluator's criteria.](images/5096752b4b5d514a04291518aa93f434d3446f2ba58cdca1723fceac15a8af9c.jpg)\n\n\nFurthermore, if a sub-task fails due to insufficient conditions or if the evaluation determines that the sub-task's question does not align with the main task (e.g., calculating energy using wavelength instead of calculating wavelength using given energy), the sub-tasks from  $\\mathcal{P}_i$  to  $\\mathcal{P}_n$  will be restructured as  $\\mathcal{P}_i'$  to  $\\mathcal{P}_m'$ , taking into account the main task and all preceding sub-tasks.\n\n\nThe evaluation & refinement module can use a different LLM than the one used for the base reasoning process. For instance, if GPT-3.5 is the base model, GPT-4 can handle evaluation & refinement. The evaluation component judges solutions without modifying them, while the refinement component adjusts solutions based on these evaluations. This separation clarifies error identification, helping humans understand where and why mistakes occur.\n\n\n# 3 EXPERIMENT\n\n\n# 3.1 SETUP\n\n\nWe use four chemistry datasets from SciBench (Wang et al., 2024a), and the detailed distribution of the specific fields covered by each problem in the four datasets is shown in Figure 10. Each dataset is divided into a development set  $(\\mathcal{D}_d)$  and a test set  $(\\mathcal{D}_t)$ , with exact sizes provided in Table 6. According to previous research (Ouyang et al., 2024), SciBench chemical sets are representative of chemical reasoning tasks, addressing a broad range of queries typically encountered in this field.\n\n\nDuring the reasoning stage, we configure the planning memory  $(\\mathcal{M}_p)$  to provide a maximum of two related memory instances (2-shot) for each query, and the execution memory  $(\\mathcal{M}_e)$  to provide up to four related instances (4-shot). However, during the construction of library, only the knowledge memory  $(\\mathcal{M}_k)$  is used, as the standard solutions are already available in the development set  $(\\mathcal{D}_d)$ .\n\n\nTable 1: Performance on the test sets of four datasets: QUAN, CHEMMC, ATKINS, and MATTER. Results are compared with baselines under two different setups: a zero-shot setting with no demonstrations and a few-shot setting with three demonstrations. Accuracy scores are computed using the approximation detailed in Section 4.3. The best results for each setup are highlighted in **bold**, and the second-best results are **underlined**. In some settings, the two modules described in Sections 2.4 (Memory) and 2.6 (Evaluation & refinement) are disabled, helping illustrate each module's impact on the overall performance.\n\n\n<table><tr><td>Models</td><td>CHEMMC</td><td>MATTER</td><td>ATKINS</td><td>QUAN</td><td>Avg.</td></tr><tr><td colspan=\"6\">Baselines, all based on GPT-4 (gpt-4-1106-preview)</td></tr><tr><td>Few-shot + Direct reasoning</td><td>28.21</td><td>14.29</td><td>20.69</td><td>14.71</td><td>19.48</td></tr><tr><td>Few-shot + Python</td><td>38.46</td><td>34.69</td><td>57.01</td><td>44.12</td><td>43.57</td></tr><tr><td>StructChem</td><td>58.97</td><td>30.67</td><td>59.81</td><td>41.18</td><td>47.66</td></tr><tr><td colspan=\"6\">ChemAgent, all based on GPT-4 (gpt-4-1106-preview)</td></tr><tr><td>ChemAgent</td><td>74.36</td><td>48.98</td><td>61.18</td><td>44.12</td><td>57.16</td></tr><tr><td>w/o Evaluation &amp; Refinement</td><td>61.54</td><td>44.89</td><td>57.94</td><td>44.12</td><td>52.12</td></tr><tr><td>w/o Memory, Evaluation &amp; Refinement</td><td>58.97</td><td>38.78</td><td>57.94</td><td>41.18</td><td>49.22</td></tr><tr><td colspan=\"6\">all based on Llama 3.1-7b</td></tr><tr><td>Direct reasoning</td><td>28.20</td><td>10.20</td><td>22.43</td><td>8.82</td><td>17.41</td></tr><tr><td>ChemAgent</td><td>56.41</td><td>12.24</td><td>19.63</td><td>14.71</td><td>25.75</td></tr><tr><td colspan=\"6\">all based on Llama 3.1-70b</td></tr><tr><td>Direct reasoning</td><td>33.33</td><td>30.61</td><td>30.84</td><td>23.53</td><td>29.48</td></tr><tr><td>ChemAgent</td><td>64.10</td><td>32.65</td><td>43.93</td><td>29.41</td><td>42.52</td></tr><tr><td colspan=\"6\">all based on Qwen 2.5-72b</td></tr><tr><td>Direct reasoning</td><td>48.72</td><td>32.65</td><td>57.01</td><td>35.50</td><td>43.47</td></tr><tr><td>ChemAgent</td><td>69.23</td><td>44.90</td><td>56.07</td><td>44.12</td><td>53.58</td></tr></table>\n\n\nWe evaluate the accuracy by comparing their outputs with the correct answers, using a relative tolerance of 0.01.\n\n\nWe consider three baselines in alignment with the evaluation paradigm in SciBench (detailed instructions are provided in Appendix G): (1) Few-shot + Direct reasoning involves directly feeding the problem into the LLM without any additional instructions, the source of the data is StructChem (Ouyang et al., 2024). (2) Few-shot + Python combines few-shot examples with a tool-augmented strategy. Here, we provide six examples for every query, each consisting of a problem, its corresponding solution, and Python code. These examples are taken from  $\\mathcal{D}_d$ . The results are from the original benchmark, SciBench (Wang et al., 2024a). (3) StructChem (Ouyang et al., 2024) employs structured instruction, confidence-based review, and refinement to guide LLMs through precise step-by-step reasoning.\n\n\n# 3.2 RESULTS\n\n\nWe report the performance of all methods regarding accuracy score for each sub-dataset and an average score across the entire dataset. The results are summarized in Tables 1, 4 and 5. Additional results and analysis of experiments conducted on other LLMs can be found in Appendix B.\n\n\nFirstly, ChemAgent consistently outperforms the other baselines across various datasets and settings. Specifically, in terms of the average score, ChemAgent improves by  $9.50\\%$  (47.66 vs. 57.16) over StructChem, which is a 2.93 times increase and by  $37\\%$  (19.48 vs. 57.16) over direct reasoning, which is a 2.93 times increase. Notably, the performance gain varies across different datasets. In the CHEMMC dataset, our method exhibits the largest improvement, with a  $46\\%$  increase (28.21 vs. 74.36) compared to the direct reasoning setup. Secondly, the results also highlight the crucial role of memory in our library. The version of our framework equipped with memory consistently outperforms the version without memory across all cases. Specifically, there is an absolute improvement of  $2.90\\%$  in terms of the average score. This underscores the importance of memory in retaining and leveraging past information to improve the accuracy and reliability of solving chemistry problems. Additionally, the Evaluation & Refinement module plays a significant role when using stronger LLMs. For instance, incorporating Evaluation & Refinement with GPT-4 increases ChemAgent's performance by  $5.04\\%$  compared to ChemAgent with memory alone. In summary, ChemAgent demonstrates superior performance across various LLM backbones, highlighting the critical importance of the library system, memory design, and evaluation modules in\n\n\nenhancing problem-solving capabilities. Appendix C details the specific aspects and reasons for our method's improved performance.\n\n\n# 3.3 SELF-EVOLUTION DURING RUNTIME\n\n\nMoreover, we aim to show that library systems with evolving memory perform better when exposed to an increasing number of problems. Much like humans improve their skills through practice, these systems benefit from continuous exposure to new tasks.\n\n\nWe allow ChemAgent to dynamically update and enrich its library during the test stage to analyze this self-evolution process. Specifically, in iteration  $\\mathcal{I}_i$ , ChemAgent uses all accumulated long-term memory from iterations  $\\mathcal{I}_1$  to  $\\mathcal{I}_{i-1}$  as its library. When solving a new problem  $\\mathcal{P}$ , all related responses and knowledge from that process are added to the library if the solution is correct. This means that in subsequent iterations, the system can leverage the newly acquired information (updated  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$ ) to improve its performance. We employ 2-shot  $\\mathcal{M}_p$  and 4-shot  $\\mathcal{M}_e$  during reasoning but simplify the setup by removing the evaluation and refinement modules. To ensure accuracy and prevent target leakage, the memory derived from problem  $\\mathcal{P}_i$  in iteration  $\\mathcal{I}_j$  is excluded when solving  $\\mathcal{P}_i$  again in  $\\mathcal{I}_k$  for any  $j < i$ .\n\n\n![Figure 5: Self-evolving analysis. We test ChemAgent twice for each iteration, and the difference between the two results serves as the error margin. All the experiments here are done on MATTER dataset.](images/95adae2d9c377ebed9474a236074302b8c0f6913519066c1901f5ddd53844195.jpg)\n\n\nThis iterative process demonstrates that as the memory pool grows with each new example, ChemAgent's problem-solving performance improves. Figure 5 shows that as the number of iterations increases, the agent's performance gradually improves and converges to a score higher than the baseline (44.89%). This improvement indicates that ChemAgent can enhance its performance through a simple correct-or-not evaluation on past solutions instead of human-written high-quality trajectories.\n\n\n# 3.4 COST ANALYSIS\n\n\nOn average, each problem requires 0.012 million tokens without the Evaluation & Refinement module, equivalent to around  $0.09 per example. When this module is applied, the average token consumption per problem increases to approximately 0.023 million, costing about$ 0.1725 per example. Note that the initial library construction is not included in these calculations as it is a one-time setup. Based on the results in Figure 6, the computational time required by our method is reasonable. While the cost and resource consumption are slightly higher than StructChem, the improvements justify the additional expense.\n\n\n# 3.5 ERROR ANALYSIS\n\n\n![Figure 6: Cost Analysis. The size of each bubble corresponds to the average number of inferences for each method, while the  $y$ -axis indicates the average accuracy across the four datasets.](images/351f3762b68fd4c9a055594bc3e46260258d9163560448f7c46f548f90a4b0b8.jpg)\n\n\nWe conduct an analysis of the trajectories for the failed examples and find three types of errors, shown in Figure 7.\n\n\n(1) Lack of Understanding of the Question. We observe that plans often fail when the problem text contains critical hidden information (e.g., in Figure 7, the terms 'reversibly' and 'adiabatically' are key) or includes excessive, redundant details (e.g., the exact conditions such as \"250K\" are irrelevant to the solution). This challenge is understandable, as even human solvers can be misled by such information and err in their approach. This issue is prevalent and independent of whether the task is tackled by our model or directly queried to the LLM, suggesting that these errors may be\n\n\n![Figure 7: Error analysis. This example highlights a typical incorrect solution, which can be attributed to three main types of errors. Specifically, this problem pertains to an adiabatic process.](images/57d0674cc5aa18b36216a513c20f2715a26997102df2926f250970a7197a167e.jpg)\n\n\ninherent limitations of the LLM's capacity. Hence, improving the ability of the foundation model to handle such nuances could significantly enhance performance.\n\n- (2) Inaccurate Reasoning. As indicated before (Ouyang et al., 2024), the planning capabilities of LLMs remain insufficient for handling complex problems. Incorrect planning for the reasoning chains exacerbates the problem-solving process, as subsequent decisions and actions are based on initial problem decompositions. This issue persists until the Evaluation & Refinement module detects an error, which may not happen promptly enough to correct the trajectory.\n\n- (3) Incorrect Memory Selection. While ChemAgent with memory demonstrates superior performance compared to setups without memory, it sometimes invokes misleading information, even when the similarity between the invoked memory and the problem is high. This indicates a need for more sophisticated memory retrieval and utilization strategies.\n\n\nIn Figure 7, the invoked memory and sub-task 1.1 share considerable similarities\u2014both concern entropy change during a compression process. However, the subtle difference is that the current problem involves an adiabatic process, whereas the examples in memory do not. This seemingly minor distinction can lead to significant changes in the problem-solving strategy. Distinguishing between misleading and beneficial memories remains a challenging issue, as invoked memories and problem texts may be semantically similar yet differ in critical aspects.\n\n\n# 4 ABLATION STUDY\n\n\n# 4.1 MEMORY COMPONENT ANALYSIS\n\n\nTo understand why our method performs particularly well and which memory component contributes the most, we conduct an ablation study by independently removing each memory component. We test these different settings on all four sub-datasets using GPT-4 as the base LLM. The results are shown in Table 2. As mentioned in \u00a72.3, the execution memory also serves as a few-shot prompting strategy. Therefore, when we remove  $\\mathcal{M}_e$ , we add two fixed human-written few-shot examples (provided with our code) into each query of each sub-task. These examples are selected from the development set  $(\\mathcal{D}_d)$  corresponding to each dataset to ensure they do not provide misleading information.\n\n\nFirstly, removing any memory component results in an overall performance drop. And notably, the relevant knowledge in LLM itself  $(\\mathcal{M}_k)$  significantly impacts the overall performance of ChemA-gent. Delving into why  $\\mathcal{M}_k$  contributes the most, we find that this might be due to the insufficient  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$  in the Atkins dataset. As shown in Table 6, ATKINS has the lowest ratio of the development set to the test set, while it also has the largest test set. This imbalance may result in a small\n\n\nTable 2: The best performing score is highlighted in bold and second-best is underlined. The number of problems in each textbook weighs the average score. We also provide the results of ChemAgent without any of these memories as a baseline. Evaluation & refinement module is removed.\n\n\n<table><tr><td>Memory component</td><td>CHEMMC</td><td>MATTER</td><td>ATKINS</td><td>QUAN</td><td>Avg.</td></tr><tr><td>None</td><td>58.97</td><td>38.78</td><td>57.94</td><td>41.18</td><td>51.53</td></tr><tr><td>+MpMeMk</td><td>61.54</td><td>44.89</td><td>57.94</td><td>44.12</td><td>53.71</td></tr><tr><td>+MpMe</td><td>66.66</td><td>44.89</td><td>52.34</td><td>41.18</td><td>51.53</td></tr><tr><td>+MeMk</td><td>58.97</td><td>46.94</td><td>61.68</td><td>29.41</td><td>53.27</td></tr><tr><td>+MpMkfew-shot</td><td>74.36</td><td>42.86</td><td>57.94</td><td>32.35</td><td>53.71</td></tr></table>\n\n\nand sometimes irrelevant memory pool for  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$  to search from. We hypothesize that this issue can be mitigated when the model is exposed to more chemical questions over time. However, due to limited computational resources, we cannot scale up to verify this hypothesis in this research.\n\n\nIt is also worth noting that although the  $M_p, M_k, \\text{fewshot}$  setting achieves the same accuracy as the complete memory setting, most of its improvement is evident in the CHEMMC dataset, showing surprisingly higher improvements than others. Since Table 6 and Figure 10 demonstrate that the CHEMMC dataset has a higher ratio of  $\\mathcal{D}_d$  to  $\\mathcal{D}_t$  and a relatively narrow distribution in subfields, and since human experts carefully wrote the few-shot examples, these high-quality, expert-written examples may serve as a superior memory pool in such circumstances. In the next section, we show that better memory leads to better results, which might explain this phenomenon.\n\n\n# 4.2 MEMORY QUALITY INFLUENCE\n\n\nWe investigate the impact of memory quality through a series of experiments. Specifically, we compare the performance of memory generated by GPT-4 against that generated by GPT-3.5 on the MATTER dataset. Additionally, we create a \"hybrid memory\" by mixing memories generated by both GPT-3.5 and GPT-4 to observe its performance. These experiments were conducted without including the relevant knowledge within the LLM itself  $(\\mathcal{M}_k)$ . As mentioned in \u00a72.1, unlike  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$ ,  $\\mathcal{M}_k$\n\n\nTable 3: Results of memory quality analysis, with the best scores highlighting in bold. Here, memory refers to  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$ . The relevant knowledge in LLM itself  $(\\mathcal{M}_k)$  is removed from the framework. Test on MATTER dataset.\n\n\n<table><tr><td></td><td>GPT-3.5</td><td>GPT-4</td></tr><tr><td>GPT-3.5 Memory</td><td>11.22</td><td>36.73</td></tr><tr><td>GPT-4 Memory</td><td>13.26</td><td>44.89</td></tr><tr><td>Hybrid Memory</td><td>10.20</td><td>28.57</td></tr></table>\n\n\nis not preserved in the memory pool shared with other LLMs. Including this type of memory in the ablation study would result in the pollution of provided memory by the  $\\mathcal{M}_k$  generated during use.\n\n\nAs shown in Table 3, our data analysis indicates that memory quality significantly impacts the accuracy of responses from ChemAgent. Whether GPT-3.5 or GPT-4 is used, the memory produced by GPT-4 consistently outperforms that generated by GPT-3.5, showing an  $8\\%$  variation. This underscores the substantial impact of memory quality on the accuracy of generated responses. This observation also explains why the improvements seen with GPT-3.5 are less pronounced compared to those with GPT-4; the lower quality of memory produced by GPT-3.5 is the limiting factor.\n\n\nInterestingly, hybrid memory performs the worst among the three settings. This can be attributed to the simultaneous invocation of different memories for the same question\u2014one generated by GPT-3.5 and the other by GPT-4\u2014which may confuse the LLM, increasing the likelihood of producing irrelevant or incorrect answers. Additionally, invoking an excessive number of memory tracks during resolution may also contribute to this issue. A more detailed analysis regarding the number of memory tracks invoked is provided in Appendix D.\n\n\n# 5 CONCLUSION\n\n\nOur research presents a novel approach to enhancing large language models for solving complex chemical problems through self-exploration and memory formation. This method enables models to construct and utilize a library, significantly improving response accuracy. Experiments using datasets and models like GPT-3.5, GPT-4, and Llama3 demonstrate substantial performance gains, with the ChemAgent architecture achieving up to a  $36\\%$  improvement. The structured library, built through memory decomposition into planning, execution, and knowledge, enhances problem-solving capabilities, which holds promise for generalization to other domains.\n\n\n# REFERENCES\n\n- Peter Atkins, Julio De Paula, and Ronald Friedman. Physical chemistry: quanta, matter, and change. Oxford University Press, USA, 2014.\n\n- Peter Atkins, Julio De Paula, and James Keeler. Atkins' physical chemistry. Oxford university press, 2023.\n\n- Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41-48, 2009.\n\n- Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,16, pp. 17682-17690, 2024.\n\n- Daniil A. Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature, 624:570 - 578, 2023.\n\n- Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. ChemCrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.\n\n- He Cao, Yanjun Shao, Zhiyuan Liu, Zijing Liu, Xiangru Tang, Yuan Yao, and Yu Li. PRESTO: Progressive pretraining enhances synthetic chemistry outcomes. arXiv preprint arXiv:2406.13193, 2024.\n\n- Aaron Chan, Zhiyuan Zeng, Wyatt Lake, Brihi Joshi, Hanjie Chen, and Xiang Ren. KNIFE: Distilling meta-reasoning knowledge with free-text rationales. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML, 2023.\n\n- Kourosh Darvish, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa, Sagnik Som, Miroslav Bogdanovic, Yang Cao, Han Hao, Haoping Xu, Al\u00e1n Aspuru-Guzik, Animesh Garg, and Florian Shkurti. Organa: A robotic assistant for automated chemistry experimentation and characterization. ArXiv, abs/2401.06949, 2024.\n\n- Ernest Davis and Scott Aaronson. Testing GPT-4 with wolfram alpha and code interpreter plug-ins on math and science problems. arXiv preprint arXiv:2308.05713, 2023.\n\n- Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, and Huajun Chen. SciKnowEval: Evaluating multi-level scientific knowledge of large language models. arXiv preprint arXiv:2406.09098, 2024.\n\n- Chrisantha Fernando, Dylan Sunil Banarse, Henrik Michalewski, Simon Osindero, and Tim Roktaschel. Promptbreeder: Self-referential self-improvement via prompt evolution. In *Forty-first International Conference on Machine Learning*, 2023.\n\n- Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations, 2022.\n\n- Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16477-16508, 2023.\n\n- Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, et al. Critic: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, 2023.\n\n- Jing Guo, Nan Li, Jian Qiang Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, and Ming Xu. Empowering working memory for large language model agents. ArXiv, abs/2312.17259, 2023a.\n\n- Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, et al. What can large language models do in chemistry? a comprehensive benchmark on eight tasks. Advances in Neural Information Processing Systems, 36:59662-59688, 2023b.\n\n- Joseph F Hair Jr, Wiliam C Black, Barry J Babin, and Rolph E Anderson. Multivariate data analysis. In Multivariate data analysis, pp. 785-785. Prentice Hall, 2010.\n\n- Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint arXiv:2408.08435, 2024.\n\n- Zhiting Hu and Tianmin Shu. Language models, agent models, and world models: The law for machine reasoning and planning. arXiv preprint arXiv:2312.05230, 2023.\n\n- Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations, 2023.\n\n- Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey. ArXiv, abs/2402.02716, 2024a.\n\n- Yuqing Huang, Rongyang Zhang, Xuesong He, Xuyang Zhi, Hao Wang, Xin Li, Feiyang Xu, Deguang Liu, Huadong Liang, Yi Li, Jian Cui, Zimu Liu, Shijin Wang, Guoping Hu, Guiquan Liu, Qi Liu, Defu Lian, and Enhong Chen. Chemeval: A comprehensive multi-level chemical evaluation for large language models. arXiv preprint arXiv:2409.13989, 2024b.\n\n- Shuyang Jiang, Yuhao Wang, and Yu Wang. Selfevolve: A code evolution framework via large language models. arXiv preprint arXiv:2306.02907, 2023.\n\n- Zhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xiaoran Jin, and Hang Li. Design of chain-of-thought in math problem solving. arXiv preprint arXiv:2309.11054, 2023.\n\n- Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2901-2910, 2017.\n\n- Scott Barry Kaufman. Intelligence and the cognitive unconscious. The Cambridge handbook of intelligence, pp. 442-467, 2011.\n\n- Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In *The Eleventh International Conference on Learning Representations*, 2022.\n\n- Jiawei Li, Yizhe Yang, Yu Bai, Xiaofeng Zhou, Yinghao Li, Huashan Sun, Yuhang Liu, Xingpeng Si, Yuhao Ye, Yixiao Wu, et al. Fundamental capabilities of large language models and their applications in domain scenarios: A survey. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11116-11141, 2024a.\n\n- Kun Li, Xin Jing, and Chengang Jing. Vector storage based long-term memory research on llm. International Journal of Advanced Network, Monitoring and Controls, 2024b.\n\n- Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, et al. MMSci: A multimodal multi-discipline dataset for PhD-level scientific comprehension. arXiv preprint arXiv:2407.04903, 2024c.\n\n- Chang Liao, Yemin Yu, Yu Mei, and Ying Wei. From words to molecules: A survey of large language models in chemistry. arXiv preprint arXiv:2402.01439, 2024.\n\n- AI@Meta Llama Team. The llama 3 herd of models, July 2024. URL https://llama.meta.com/.\n\n- Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521, 2022.\n\n- Donald A McQuarrie. Quantum chemistry. University Science Books, 2008.\n\n- Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu, Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Amir Mohammad Elahi, Maximilian Greiner, Caroline T. Holick, Tanya Gupta, Mehrdad Asgari, Christina Glaubitz, Lea C. Klepsch, Yannik K\u00f6ster, Jakob Meyer, Santiago Miret, Tim Hoffmann, Fabian Alexander Kreth, Michael Ringleb, Nicole Roesner, Ulrich S. Schubert, Leanne M. Stafast, Dinga Wonanke, Michael Pieler, Philippe Schwaller, and Kevin Maik Jablonka. Are large language models superhuman chemists?, 2024. URL https://arxiv.org/abs/2404.01475.\n\n- OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, et al. GPT-4 technical report, 2024.\n\n- Magda Osman. An evaluation of dual-process theories of reasoning. Psychonomic bulletin & review, 11(6):988-1010, 2004.\n\n- Siru Ouyang, Zhuosheng Zhang, Bing Yan, Xuan Liu, Yejin Choi, Jiawei Han, and Lianhui Qin. Structured chemistry reasoning with large language models. In *Forty-first International Conference on Machine Learning*, 2024.\n\n- Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. Is a question decomposition unit all we need? In 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, 2022.\n\n- Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, and Rui Yan. Leveraging biomolecule and natural language through multi-modal learning: A survey. arXiv preprint arXiv:2403.01528, 2024.\n\n- Cheng Qian, Shihao Liang, Yujia Qin, Yining Ye, Xin Cong, Yankai Lin, Yesai Wu, Zhiyuan Liu, and Maosong Sun. Investigate-consolidate-exploit: A general strategy for inter-task agent self-evolution. arXiv preprint arXiv:2401.13996, 2024.\n\n- Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve. arXiv preprint arXiv:2407.18219, 2024.\n\n- Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\n\n- Marta Skreta, Zihan Zhou, Jia Lin Yuan, Kourosh Darvish, Al\u00e1n Aspuru-Guzik, and Animesh Garg. Replan: Robotic replanning with perception and language models. ArXiv, abs/2401.04157, 2024.\n\n- Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. SciEval: A multi-level large language model evaluation benchmark for scientific research. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,17, pp. 19053-19061, 2024a.\n\n- Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. Trustl'm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561, 2024b.\n\n- Vicente Talanquer. The complexity of reasoning about and with chemical representations. JACS Au, 2(12):2658-2669, 2022. doi: 10.1021/jacsau.2c00498.\n\n- David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, et al. SciRIFF: A resource to enhance language model instruction-following over scientific literature. arXiv preprint arXiv:2406.07835, 2024.\n\n- Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. SciBench: Evaluating college-level scientific problem-solving abilities of large language models. In *Forty-first International Conference on Machine Learning*, 2024a.\n\n- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023.\n\n- Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv preprint arXiv:2409.07429, 2024b.\n\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.\n\n- Kangda Wei, Dawn Lawrie, Benjamin Van Durme, Yunmo Chen, and Orion Weller. When do decompositions help for machine reading? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 3599-3606, 2023.\n\n- Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2, 2022.\n\n- Yi Xiao, Xiangxin Zhou, Qiang Liu, and Liang Wang. Bridging text and molecule: A survey on multimodal frameworks for molecule. arXiv preprint arXiv:2403.13830, 2024.\n\n- Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2023.\n\n- Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. arXiv preprint arXiv:2406.04271, 2024.\n\n- Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Yuan Zhang, Chao Wang, Juntong Qi, and Yan Peng. Leave it to large language models! correction and planning with memory integration. Cyborg and Bionic Systems, 5, 2023.\n\n- Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu Wang. Small language models need strong verifiers to self-correct reasoning. arXiv preprint arXiv:2404.17140, 2024a.\n\n- Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model based agents. ArXiv, abs/2404.13501, 2024b.\n\n- Ming Zhong, Siru Ouyang, Minhao Jiang, Vivian Hu, Yizhu Jiao, Xuan Wang, and Jiawei Han. ReactIE: Enhancing chemical reaction extraction with weak supervision. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 12120\u201312130, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023-findings-acl.767. URL https://aclanthology.org/2023-findings-acl.767.\n\n- Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):19724-19731, Mar. 2024a. doi: 10.1609/aaai.v38i17.29946. URL https://ojs.aaai.org/index.php/AAAI/article/view/29946.\n\n- Xianrui Zhong, Yufeng Du, Siru Ouyang, Ming Zhong, Tingfeng Luo, Qirong Ho, Hao Peng, Heng Ji, and Jiawei Han. Actionie: Action extraction from scientific literature with programming languages. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12656-12671, 2024b.\n\n- Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WZH7099tgfm.\n\n- Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532, 2024.\n\nResponse: \\boxed{Accept}\n\n### Rationale:\n\n#### Originality:\nThe paper introduces a novel approach to enhancing large language models (LLMs) specifically for solving complex chemical reasoning problems. The authors propose a dynamic library system, called ChemAgent, which integrates three types of memory (Planning Memory, Execution Memory, and Knowledge Memory) to improve LLMs' performance. This approach is innovative as it addresses the limitations of existing methods by focusing on structured, hierarchical decomposition and continuous learning from past experiences.\n\n#### Methodological Rigor:\nThe methodology is detailed and rigorous. The authors clearly describe the framework, including the decomposition of problems into sub-tasks, the structure of the library, and the dynamic updating process. They also provide pseudocode for key components of the system, which is helpful for replication. The experimental setup is thorough, involving multiple datasets and different configurations, and the results are well-supported with tables and figures.\n\n#### Significance of Contribution:\nThe paper makes a significant contribution to the field by demonstrating substantial improvements in chemical reasoning tasks, particularly in the CHEMMC dataset where ChemAgent achieves a 46% improvement. The authors also highlight the importance of memory quality, showing that higher-quality memory leads to better performance, which is a valuable insight for future work.\n\n#### Clarity of Presentation:\nThe paper is well-organized and easy to follow. The introduction clearly outlines the problem and motivation, and the methodology section is detailed and logically structured. The results are presented clearly, with appropriate comparisons to baseline methods and explanations for the observed performance differences.\n\n#### Alignment with Journal Scope:\nICLR (International Conference on Learning Representations) focuses on deep learning, reinforcement learning, and related topics. While the paper deals with a specific application of LLMs to chemical reasoning, the core concepts and methods (such as memory management and hierarchical decomposition) are broadly applicable to the field of machine learning. The authors have adapted these concepts to the specific context of chemical reasoning, making the paper relevant to ICLR.\n\n### Weights Given:\n- **Originality:** High (introducing a novel framework)\n- **Methodological Rigor:** High (detailed and well-described methodology)\n- **Significance of Contribution:** High (substantial performance improvements, especially in CHEMMC)\n- **Clarity of Presentation:** High (well-organized and easy to follow)\n- **Alignment with Journal Scope:** High (broad applicability and relevance to ICLR)\n\nGiven the strong points in originality, methodological rigor, and significance of contribution, along with the clear and well-presented research, the paper is likely to be accepted at ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634748.7097626", "msecs": "709.0", "relativeCreated": "93533.23674201965", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Output-Constrained Decision Trees\n\n# Abstract\nWhen there is a correlation between any pair of targets, one needs a prediction method that can handle vector-valued output. In this setting, multi-target learning is particularly important as it is widely used in various applications. This paper introduces new variants of decision trees that can handle not only multi-target output but also the constraints among the targets. We focus on the customization of conventional decision trees by adjusting the splitting criteria to handle the constraints and obtain feasible predictions. We present both an optimization-based exact approach and several heuristics, complete with a discussion on their respective advantages and disadvantages. To support our findings, we conduct a computational study to demonstrate and compare the results of the proposed approaches.\n\n\n# 1 INTRODUCTION\n\n\nDecision trees (DTs) play a fundamental role in machine learning and data science for several key reasons. First, DTs have proven to be accurate predictive models in a wide range of applications. Moreover, the heuristic approaches used for their construction are computationally efficient, resulting in extremely fast training times. Accuracy and speed make them ideal for large datasets or real-time applications. Second, DTs, particularly those that are not too deep, are inherently explainable. By following the branches of such a trained tree, one can easily understand the logic behind the model's predictions. This feature is particularly valuable in sectors where interpretability is critical, such as in healthcare or finance. Lastly, DTs serve as the base learners in powerful ensemble methods. These methods, which include techniques like random forests and gradient boosting, utilize numerous DTs to create robust and accurate predictive models.\n\n\nWhen there is a correlation between any pair of targets, we need a prediction method that can handle vector-valued output. In this setting, multi-target learning is particularly important as it is widely used in fields such as forecasting, monitoring, manufacturing, and recommendation systems. DTs can easily be customized for handling multi-target regression by modifying the splitting criteria to handle vector predictions at each node.\n\n\nMotivation. Despite the wealth of existing literature on DTs, to the best of our knowledge, none consider cases where constraints exist among multiple targets. This is notable, as in applications, decision-makers are aware of these constraints, and data analysts should impose them for feasible predictions. Here is an illustrative example from the field of operations management where machine learning approaches are becoming more prominent in the context of decision-making; see e.g., (Mi\u0161i\u0107 & Perakis, 2019).\n\n\nConsider a dataset of products in a retail store, each with distinct attributes. This dataset includes the inventory amounts of these products across five different warehouses, represented as five-dimensional target vectors (training set). The goal is to predict the inventory amounts for new products across these warehouses (test set).\n\n\nTo achieve this, one can train a DT and obtain the partitions of the existing products represented by the leaf nodes of the trained tree. The prediction for a new product is then simply the average of the inventory amounts (mean vector) within the leaf node that it falls into. This leads to the prediction of five different inventory amounts, i.e., one vector for each warehouse.\n\n\nHowever, it is crucial to consider an additional constraint in our scenario: Due to budget restrictions, a store can only transport a product in a maximum of two out of the five warehouses. This is also evident in the training data set where each target vector has at most two nonzero values. Consequently,\n\n\n![Figure 1: The trade-off between accuracy and feasibility. The red circle in the right plot marks the depth of the tree which returns the lowest mean squared error value for the test set. The corresponding red bar in the left plot shows the corresponding percentage of infeasible predictions. The sample size is 4000.](images/d6bdc19df95d34dbaea9eb56a66a1ba4b1a522ba2c4424d8f9d57ba3b851a5fd.jpg)\n\n\n![](images/2bb04fadddcf6e320cd4122856a38a38c98455662b65df6dea09f31ed66cada8.jpg)\n\n\nif one continues to use the mean vector approach for prediction, it may not only yield inaccurate results but also infeasible predictions because it does not account for this constraint.\n\n\nThis erroneous estimation would also persist with many well-known DT training algorithms, since their estimation at a leaf node is also based on taking the mean or the median of the target vectors within the node. However, one may argue that growing a deeper DT may resolve the infeasibility issue as there will be fewer products at each leaf node. Even if we overlook the loss of interpretability due to the large size of such a tree, it is well-known that deep trees can easily overfit to the training data, potentially leading to poor generalization for new products. For instance, Figure 1 shows our results with one of our datasets from our computational study in Section 3. The bar plot demonstrates that feasible predictions can be achieved with larger tree depths, but this comes with a significant decrease in accuracy as shown in the scatter plot. The scatter plot, on the other hand, shows that while shallow trees offer much better accuracy at the expense of most of their predictions are being infeasible.\n\n\nOutput-Constraints. To obtain accurate and feasible predictions, we propose an optimization-based exact approach. This approach incorporates target constraints into the splitting procedure of common binary tree construction methods. Our approach ensures that the estimated target vector at each node meets the required constraints. However, we observe that the exact approach could have a lengthy computation time, especially for deep trees. To address this, we offer two heuristic approaches. These are based on using existing samples as predictions or relaxing the constraints with a penalty function at each split operation.\n\n\nUp until now, our focus has been on regression problems. It is reasonable to question why we have not considered multi-target classification problems as well. The reason is, if the number of classes for each target and the number of targets are relatively small, we can assign a label to each feasible combination of target values. This approach effectively reduces the problem to a standard single-target classification problem, although possibly with many labels.\n\n\nRelated Work. As machine learning methods improve in accuracy, they may still produce predictions and outcomes that lack physical consistency or feasibility, due to biases or data sparsity (Zhang et al., 2022). To overcome this challenge, hybrid methods that integrate optimization techniques with machine learning models have gained significant attention in recent years due to their ability to leverage the strengths of both approaches. One key aspect of hybrid approaches is their ability to handle constraints from various sources to enhance the feasibility and accuracy of machine learning methods. These constraints can originate from the dataset, user-defined specifications, or domain knowledge (Nanfack et al., 2022). Such machine learning approaches with imposed constraints are developed as practical applications across different domains, such as mechanics (Yan et al., 2022), geomechanics (Chen & Zhang, 2020), and electronics (Lee et al., 2019).\n\n\nTraditional supervised learning methods primarily focus on predicting a single output, using data comprised of instances represented by features and a single response variable. Multi-target regression (also referred to as multi-output regression), is used to predict multiple continuous target variables simultaneously. One of the earliest methods for multi-target regression with decision trees was introduced by De'ath (2001). He extended the univariate recursive partitioning approach from Classification and Regression Trees (CART) (Breiman, 2017) to handle multiple responses, calling this method Multi-Response Trees (MRT). MRTs are built similarly to CART, but the node impurity is redefined as the sum of squared errors across all response variables. Struyf & D\u017eeroski (2006) developed a system for building multi-objective regression trees (MORTs), which include constraints on tree size and accuracy, allowing users to balance interpretability and accuracy by limiting tree size or setting a minimum accuracy threshold. (Adiyeke & Baydo\u011fan, 2022) introduced a novel method combining multi-target regression with unsupervised learning. Specifically, they modified tree construction by incorporating a similarity-based clustering indicator in ensemble methods.\n\n\nDecision trees (DTs) offer some flexibility in enforcing constraints, and many hybrid DT approaches incorporate instance-level constraints to bind a subset of instances in generating meaningfully combined results. For instance, the clustering tree algorithm by Struyf & D\u017eeroski (2007) integrates domain knowledge to impose must-link and cannot-link constraints. DT methods have also been developed to address adversarial examples, which can be considered instance-level constraints (Chen et al., 2019; Calzavara et al., 2019), or attribute-level constraints to assign several features to the predictors (Nanfack et al., 2022). Ben-David (1995) was one of the first to apply attribute-level constraints by imposing monotonicity among the model's predictors, and Cardoso & Sousa (2010) extended this in an ordinal classification setting.\n\n\nOne of the earliest hybrid approaches of imposed constraints was implemented by Lee & Kang (1990), who introduced constraints to deep learning architectures to approximate ordinary and partial differential equations. This approach laid the foundation for the general framework known as physics-informed neural networks, proposed by Raissi et al. (2019), where constraints and boundary conditions are incorporated (Rudin et al., 2022). Similarly, Christopher et al. (2024) integrated constraints and physical principles to generative diffusion processes. To the best of our knowledge, our proposed approach is the first method designed to handle target-level constraints in multi-target regression. This represents a novel contribution, building on the principles of constraint handling and extending them to more complex, interdependent target structures in predictive modeling.\n\n\nContributions. We introduce novel approaches to handle constraints among target variables in DTs. The first method lends itself to an optimization-based exact method that ensures the feasibility of predictions by incorporating target constraints into the splitting procedure of common binary tree construction methods. We also propose two heuristic methods. These methods offer practical solutions to the computation time issue that might be associated with the exact approach, especially for deep trees. We provide the technical details of our proposed methods, exploring their strengths, potential limitations, and the conditions under which they would perform sensibly. To validate our approaches, we provide a reproducible computational study using our publicly available implementation.\n\n\n# 2 OUTPUT-CONSTRAINED DECISION TREES\n\n\nSuppose that we have the training dataset  $\\mathcal{D} = \\{(\\pmb{x}_i, \\pmb{y}_i) : i \\in \\mathcal{I}_{\\mathcal{D}}\\}$  with  $\\pmb{x}_i \\in \\mathbb{R}^p$  and  $\\pmb{y}_i \\in \\mathbb{Y} \\subseteq \\mathbb{R}^k$  denoting the input vector and the target (output) vector for the data point  $i$ , respectively. The set  $\\mathbb{Y}$  shows the feasible set for the target vectors. It is important to observe that the target vector of each sample lies in  $\\mathbb{Y}$ , and hence, they are also feasible.\n\n\nA DT is grown by splitting the dataset into non-overlapping subsets recursively (Breiman et al. 1984). Each subset is represented by a node of the tree. The most common approach of tree construction uses binary splits; that is, at each iteration at most two child nodes, left  $(\\leftarrow)$  and right  $(\\rightarrow)$ , are grown from a parent node. Let  $\\mathcal{N} \\subseteq \\mathcal{D}$  be the subset denoted by the parent node, and  $\\mathcal{I}_{\\mathcal{N}} \\subseteq \\mathcal{I}_{\\mathcal{D}}$  indicate the indices of the samples in that node. To obtain a pair of child nodes, a feature  $j$  and a split value  $v$  is selected to obtain the left and right index sets\n\n\n$$\n\n$$\n\\mathcal {I} _ {\\mathcal {N}} ^ {\\leftarrow} (j, v) = \\{i \\in \\mathcal {I} _ {\\mathcal {N}}: x _ {i j} \\leq v \\}\n$$\n\n$$\n\n\nand\n\n\n$$\n\n$$\n\\overrightarrow {\\mathcal {I} _ {\\mathcal {N}}} (j, v) = \\{i \\in \\mathcal {I} _ {\\mathcal {N}}: x _ {i j} > v \\},\n$$\n\n$$\n\n\nrespectively. Then, the gain by such a split is evaluated by\n\n\n$$\n\n$$\n\\alpha_ {\\mathcal {N}} ^ {\\leftarrow} (j, v) \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}} ^ {\\leftarrow} (j, v)} \\ell (\\boldsymbol {y} _ {\\mathcal {N}}, \\boldsymbol {y} _ {i}) + \\alpha_ {\\mathcal {N}} ^ {\\rightarrow} (j, v) \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}} ^ {\\rightarrow} (j, v)} \\ell (\\boldsymbol {y} _ {\\mathcal {N}}, \\boldsymbol {y} _ {i}), \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\pmb{y}_{\\mathcal{N}} \\in \\mathbb{Y}$  is the prediction of the parent node,  $\\ell: \\mathbb{R}^k \\times \\mathbb{R}^k \\mapsto \\mathbb{R}$  is the loss function,  $\\alpha_{\\mathcal{N}}^{-}(j,v) = \\frac{|\\mathcal{I}_{\\mathcal{N}}^{-}(j,v)|}{|\\mathcal{I}_{\\mathcal{N}}|}$  as well as  $\\alpha_{\\mathcal{N}}^{+}(j,v) = 1 - \\alpha_{\\mathcal{N}}^{-}(j,v)$  are the scaling coefficients representing the proportional weights of the respective subsets. Among all possible splits, the one that achieves the largest gain is used to construct the child nodes. The splitting continues until one of the stopping conditions, such as reaching the maximum depth or the minimum number of samples in a node, is satisfied. Then, the samples in the last parent constitute a leaf node. Considering the feasibility requirements on the predictions, the output vector  $\\pmb{y}_{\\mathcal{N}}$  of such a leaf node should be feasible; i.e., it should belong to the feasible set,  $\\pmb{y}_{\\mathcal{N}} \\in \\mathbb{Y}$ .\n\n\n# 2.1 EXACT APPROACH\n\n\nThe common loss functions (e.g., mean squared error, mean Poisson deviance) used in constructing binary DT regressors are separable. Moreover, the loss in the multi-target case is obtained by summing up individual losses for each target. Consider, for instance, the well-known mean squared loss for which the prediction at a node with the subset  $\\mathcal{N}$  is given by\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {Y}} \\frac {1}{2 | \\mathcal {I} _ {\\mathcal {N}} |} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\| \\boldsymbol {y} - \\boldsymbol {y} _ {i} \\| ^ {2}.\n$$\n\n$$\n\n\nIf there are no restrictions on the output vectors, i.e.,  $\\mathbb{Y} = \\mathbb{R}^k$ , then it is not difficult to observe by simple differentiation that  $\\pmb{y}_{\\mathcal{N}}$  is the mean of the target vectors in  $\\mathcal{N}$  minimizing the subset variance. This is, in fact, the output of a node used by the well-known implementations, like scikit-learn (Pedregosa et al. 2011). The same holds for the mean Poisson deviance loss function. In fact, the following theorem shows that the mean vector is still the optimal choice when the constraints on the output vectors result in a convex feasible set. The proof of this theorem follows from observing that the mean operation is equivalent to taking the convex combination of the feasible target vectors in the node. The details of this proof are given formally in Section A of the supplementary file. There, we also present a counter example showing that the same argument does not apply when the loss function is the mean absolute deviation and its unconstrained minimum, i.e., the median of the target vectors, is selected as the node's output (see Remark B in Section A of the supplementary file).\n\n\nTHEOREM 1 Let the feasible set  $\\mathbb{Y} \\subseteq \\mathbb{R}^k$  be convex. If the loss function is the mean squared error or the mean Poisson deviance, then the mean of the target vectors in a node minimizes the loss.\n\n\nThis theorem implies that a restricted class of feasible sets can be handled with standard DT implementations using certain loss functions. For instance, simple bounds or, more generally, linear constraints on the target vectors lead to convex feasible sets. However, the same conclusion does not hold when the feasible set is not convex. Going back to our illustrative example of inventory amount prediction, suppose for simplicity that each warehouse has the same capacity of  $C$  units. If we use only these bounds as constraints, then any standard implementation returning the mean vector for each node output would not violate this constraint. However, if we also impose the budget restriction \"at most two among five warehouses can be used for transportation,\" then we need auxiliary binary variables to represent this restriction as a constraint, and hence, the feasible set  $\\mathbb{Y}$  becomes non-convex. Formally,\n\n\n$$\n\n$$\n\\mathbb {Y} = \\left\\{\\boldsymbol {y} \\in \\mathbb {R} ^ {5}: \\sum_ {t = 1} ^ {5} z _ {t} \\leq 2, 0 \\leq y _ {t} \\leq C z _ {t}, \\text {a n d} z _ {t} \\in \\{0, 1 \\} \\text {f o r} t = 1, \\dots , 5 \\right\\}, \\tag {2}\n$$\n\n$$\n\n\nwhere the binary variable  $z_{t}$  becomes one if  $y_{t}$  units of the product is transported to warehouse  $t$ . Note that when  $z_{t}$  is zero, then it also forces  $y_{t}$  to become zero implying that product is not transported to the corresponding warehouse.\n\n\nWe can now state the formal prediction model that needs to be solved at each node:\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {Y}} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\ell (\\boldsymbol {y}, \\boldsymbol {y} _ {i}), \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\mathcal{I}_{\\mathcal{N}} \\subseteq \\mathcal{I}_{\\mathcal{D}}$  with  $\\mathcal{N} \\subseteq \\mathcal{D}$ . Dealing with nonconvex sets can pose a challenge when solving large-scale constrained models. Fortunately, the dimension of the problem  $(k)$  is quite small for many applications. In case of a mixed integer linear optimization problem, like our example on warehouse inventories, both free and commercial powerful solvers are available, e.g., COIN-OR (Lougee-Heimer, 2003), GUROBI (Gurobi Optimization, LLC, 2023). However, it is also important to remember that this optimization problem must be solved for each pair of feature-split values at every node. This increases the computation time compared to basic DT implementation that ignores the constraints, especially for deep trees. Next, we will also propose several heuristic approaches to develop faster -albeit potentially less accurate or infeasible- alternatives.\n\n\n# 2.2 HEURISTIC APPROACHES\n\n\nA common method for training machine learning models with restrictions involves relaxing these constraints and adding a term to the loss function. This term serves to penalize predictions that are not feasible. In our setting, this idea leads to the following model:\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} ^ {R} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {R} ^ {k}} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\ell (\\boldsymbol {y}, \\boldsymbol {y} _ {i}) + \\lambda v _ {\\mathbb {Y}} (\\boldsymbol {y}), \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\lambda > 0$  is the predefined penalty coefficient, and  $v_{\\mathbb{Y}}: \\mathbb{R}^k \\mapsto \\mathbb{R}_+$  is the penalty function that assigns a positive value to the infeasible predictions,  $y \\notin \\mathbb{Y}$ . This technique is a reminiscent of the approach known as Lagrangean relaxation. However, it is important to note that Lagrangean relaxation would introduce a separate penalty coefficient, known as Lagrangean multiplier, for each constraint (Bertsekas, 1982). The objective function of (4) shows the trade-off between accuracy and feasibility. For instance, using the feasible region defined in (2) leads to the following splitting problem for the mean-squared loss:\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} ^ {R} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {R} ^ {5}, \\boldsymbol {z} \\in \\{0, 1 \\} ^ {5}} \\frac {1}{2 | \\mathcal {I} _ {\\mathcal {N}} |} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\| \\boldsymbol {y} - \\boldsymbol {y} _ {i} \\| ^ {2} + \\lambda \\sum_ {t = 1} ^ {5} \\left(\\left(z _ {t} - 2\\right) + \\left(y _ {t} - C z _ {t}\\right)\\right). \\tag {5}\n$$\n\n$$\n\n\nHere,  $\\lambda$  in (4) plays the role of balancing optimality versus feasibility. As  $\\lambda$  increases, the optimization algorithm tends to favor feasible solutions over accurate ones. Conversely, for small values of  $\\lambda$ , accuracy is prioritized. Therefore, one is not guaranteed to obtain a feasible solution with the relaxation approach. Additionally, depending on the structure of the penalty function  $v_{\\mathbb{Y}}$ , problem (4) can be difficult to optimize. Note that tuning  $\\lambda$  requires attention as the optimization problem changes at each node. Attempting to adjust a different penalty coefficient for each node could quickly become a computationally formidable task.\n\n\nOur second heuristic is based on observing that all samples in a node belong to the feasible set. Thus, selecting one of them results in a feasible prediction. Considering the spread of the output values in the subset, a natural candidate could be the medoid as it would be a representative sample within the cluster designated by the node. The medoid of the subset is obtained by solving\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} ^ {M} = \\arg \\min  _ {\\boldsymbol {y} _ {i ^ {\\prime}} \\in \\mathcal {N}} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\ell \\left(\\boldsymbol {y} _ {i ^ {\\prime}}, \\boldsymbol {y} _ {i}\\right). \\tag {6}\n$$\n\n$$\n\n\nThe solution to this problem involves evaluating the pairwise loss values and then summing these values for each sample. As a result, the medoid of a subset of samples can be found very quickly. In case there are multiple samples that could be the medoids, one of them can be selected arbitrarily. Clearly, the medoid at each node is a feasible prediction but not necessarily the optimal solution of (3). At this point, we note that a tree can be grown very deep so that each leaf node contains a single sample. While this allows for feasible predictions, such deep trees are susceptible to overfitting and can quickly lose interpretability (see also Figure 1).\n\n\nSimilar to the exact approach of the previous subsection, both heuristic approaches above are applied to every feature and split value at each node. An alternative heuristic would be growing a tree as in basic implementation without considering the constraints. Then, the feasibility of DT's prediction can be guaranteed by solving the exact model (3), the medoid model (6), or the relaxation model (4) with an appropriate choice of the penalty coefficient- at the leaf nodes only. This approach would improve the training time significantly.\n\n\n# 3 COMPUTATIONAL STUDY\n\n\nThis section is dedicated to testing the performance of our exact and heuristic approaches proposed for training output-constrained decision trees (OCDTs). We obtain different implementation variants by using one of the proposed approaches at a decision node for splitting (S) or at a leaf (L) node for prediction. We also use the shorthand notation O, R, M, A for the exact optimization, relaxation, medoid, and average approaches, respectively. For instance, the variant that uses relaxation approach for splitting and the medoid approach for prediction is denoted by S(R):L(M). Likewise, S(A):L(A) simply refers to standard DT implementation in software packages that ignores the constraints. In our study, we have used the scikit-learn implementation only for this variant (Pedregosa et al., 2011).\n\n\nOur objective in this section is two-fold: First, we aim to assess the effectiveness of our proposed approaches on an existing dataset. Second, we aim to evaluate their performance in a controlled setting using an extended version of the same dataset. Our computational experiments can be reproduced by using the dedicated GitHub repo<sup>2</sup>. For all methods compared, we experimented with varying maximum depth values of 3, 6, 9, 12, and 15, while ensuring a minimum of 10 data points to split a node and at least 5 data points in each leaf. The selection of penalty coefficients,  $\\lambda$ , for each variant can be guided by evaluating several values over a reasonable range through cross-validation. Ideally, the chosen coefficient should minimize MSE while ensuring that no infeasible predictions are generated. The results were obtained using a computer equipped with an Intel(R) Core(TM) i7-9750H CPU 2.60GHz processor, featuring 12 CPUs, and 64GB of RAM.\n\n\n# 3.1 SCORE DATASET\n\n\nAs a part of our computational study, we employ the scores dataset available online (Kimmons, 2012). This dataset consists of 1000 records, simulating various personal and socio-economic factors of students at a public school. The dataset features three target variables, representing math, reading, and writing scores. To tailor the data to our research objectives, we have imposed two constraints: First, if the reading score falls below 50, the writing score is set to zero. Second, if the combined reading and writing scores amount to less than 110, the math score is also set to zero. The full description of our mathematical model is available in Section B of the supplementary file.\n\n\nWe conducted a thorough five-fold cross-validation experiment on the dataset using five different tree-depth parameter values, employing a total of nine distinct variants which are trained with the same parameters. These variants are classified into two primary components: split evaluation and leaf predictions. Split evaluation entails the assessment of each candidate for binary splitting based on the prediction generated by a specific method. Conversely, leaf prediction refers to the prediction method employed by each leaf node. For instance, the methodology involving the medoid split evaluation and optimal leaf prediction entails that each split during tree growth is determined by evaluating the total mean squared error (MSE) values of the resulting child nodes using their medoids as predictions. Subsequently, the resulting DT solves an optimization problem specific to the dataset for data falling to each leaf to assign the prediction of that particular node. The penalty coefficient  $\\lambda$  is selected by monitoring the trade-off between feasibility and accuracy through five-fold cross-validation.\n\n\nWe evaluated the performance of each variant based on the MSE value obtained during the cross-validation process. Our analysis focused on comparing the performance of an OCDT variant with different split evaluation and leaf prediction approaches against the baseline DT implementation (scikit-learn). The results are visualized in the left side of Figure 2 illustrating the key metrics concerning the performance of the variants. The bar plot reflects the (scaled) average MSE values with the left axis indicating the corresponding values. On top of each bar, the MSE values are displayed alongside the average number of infeasibilities resulting from the methods' predictions (in parenthesis). The scatter plot shows the average training time of each method with the right axis denoting the corresponding values.\n\n\nIn general, it appears that it is important to assess the performance of each variant individually. We find that the variant utilizing relaxation in the optimization problem in both split evaluation and leaf prediction,  $S(R) : L(R)$  exhibits the worst MSE score. However, the variant employing relaxation\n\n\n![Figure 2: Performance metrics plot (left) and the Friedman test for statistical differences (right) on the scores dataset. The values on each bar represent the average MSE value and the average number of infeasible predictions (in parenthesis) on the test sets. The scatter plot shows the training time for each variant. Note that MSE and computation times are scaled according to the maximum value for the corresponding performance measure obtained by one of the variants.](images/d759bec6808ea51c2bf1bad09f120741cdd224c9bad438dce17f8ba68683bee4.jpg)\n\n\n![](images/a8391e071bd991b506808318d085eac9c0dff9793512688c505bc2e8a32c19b6.jpg)\n\n\nin split evaluation and optimization in leaf prediction,  $S(R) : L(O)$  achieves the best MSE score without violating any constraints. Overall, we observe that the variants solving optimization problems at the leaf nodes for predictions demonstrate the top three performances for this dataset with no constraint violations. This affirms both the efficiency of the proposed variants and their adherence to constraints. We note that the baseline scikit-learn implementation,  $S(A) : L(A)$  violated numerous constraints (averaging 16.4 infeasible predictions), aligning with our earlier findings.\n\n\nThe comparison of the proposed variants for statistical significance is conducted using the Friedman test (Friedman, 1940), and if a significant difference is detected by the Friedman test, it is followed by the Nemenyi test (Nemenyi, 1963). The right side of Figure 2 shows the test results. We observe that the  $p$ -value is practically zero indicating that the null hypothesis of identical variants can be rejected at the 0.95 confidence level. This means that there is a statistically significant difference in the performances of the variants with S (R): L (O) having a notably better performance when compared against the others.\n\n\n# 3.2 EXTENDED DATASETS\n\n\nIn this section, we extended the previous dataset (Kimmons 2012) to mimic the motivating inventory control example that we have given in Section I. To test the proposed approaches in this controlled environment, we imposed other restrictions on the dataset, like each target vector having at most one nonzero value. The details of our data generation process along with the underlying mathematical model is given in Section C of the supplementary file. We call this dataset class.\n\n\nIn accordance with the experiments in Section 3.1, we apply the same methodology with five-fold cross-validation. This is done to compare our nine variants using the same parameters. To leverage the advantage of data generation, we evaluated our proposed variants across nine different datasets, varying in size  $(|\\mathcal{I}_{\\mathcal{D}}|)$  and the number of targets  $(k)$ . These datasets comprised of combinations of 500, 1000, and 2000 samples with total number of target values of 3, 5 and 7.\n\n\nThe MSE score and computation time are depicted on the left side of Figure 3, where the average metrics across all nine datasets are given. On the right side of Figure 3 we see that  $p$ -value is close to 0, which indicates that the methods have statistically different performances at the 0.95 confidence level. We observe that none of the proposed algorithms produce infeasible predictions, unlike the baseline scikit-learn method, S(A):L(A) which, despite yielding the lowest MSE among all approaches, generated the only positive average number of infeasible predictions. This suggests that the scikit-learn method compromised constraints to improve the prediction performance, a trade-off that is clearly not suitable for applications.\n\n\nThe box plot in Figure 4 illustrates the MSE values of all the variants across nine datasets with different sizes in terms of sample-sizes and targets. The proposed variants show comparable MSE scores, with most of the methods achieving scores within a close range, as indicated by the overlapping\n\n\n![Figure 3: Performance metrics plot (left) and the Friedman test for statistical differences (right) on class datasets. The values on each bar of the left plot represent the average MSE value and the average number of infeasible predictions (in parenthesis) on the test sets. The scatter plot shows the training time for each variant. Note that MSE and computation times are scaled according to the maximum value for the corresponding performance measure obtained by one of the variants.](images/ea99b744bf5065b2de52e1733bbc5ecd95b1ec6e9542e885200cbd0489f85569.jpg)\n\n\n![](images/882d8c00bb55aefbb529f955d0916f33c240c7a2651c296cd17f29e92645c454.jpg)\n\n\n![Figure 4: MSE box plot on the class datasets.](images/d63e0ff817c61272c47a7af1313c894f47174d8784d719a1532a0cee31511ae0.jpg)\n\n\ninterquartile ranges. While there is some variation, the results suggest that none of the methods stand out as consistently superior in terms of accuracy.\n\n\nFigure 5 depicts that the training times of the variants vary significantly with those involving optimization and relaxation, such as S (R): L (R) and S (R): L (M), taking notably longer. This observation aligns with the computational complexity expected from optimization-based approaches. Another noteworthy observation is that the methods with relatively short training times, like S (A): L (M), tend to have higher MSE values. This trade-off between model complexity and training time versus prediction accuracy is evident across the board. Furthermore, variants like S (O): L (M) strike a balance, achieving moderate MSE values while maintaining competitive training times, suggesting a promising option for practical use in time-sensitive applications.\n\n\nFigure 6 illustrates a schematic representation of a tree trained on a class dataset. This tree structure is obtained by solving the optimization problem for the split evaluation as well as for the leaf prediction; i.e., the selected variant is  $S(\\mathsf{O}) : \\mathsf{L}(\\mathsf{O})$ . In this figure, the green nodes signify the leaf nodes, while the red nodes represent the decision nodes where the data is split. Additionally, the splits after each decision node are depicted with arrows indicating the left and the right child nodes, accompanied by\n\n\n![Figure 5: Training time box plot on the class datasets.](images/a2a177166d8584ea2899caa573943b423d10617c0985d2e400c464eb417dc84c.jpg)\n\n\n![Figure 6: Visualization of an OCDT model on a class dataset.](images/9713a27d8cdc5a9db062b2c0c34fc7ad87fee56800f83fa43268dc638e9b3f95.jpg)\n\n\ntext specifying the split feature and threshold. It is easy to observe on the tree plot that the OCDT made splits are both accurate and feasible. As we outline in Section C of the supplementary file, the target values are generated in a way that they are dependent on Grade and GradeAvgPrevElec. This is clearly depicted in the tree plot as the majority of splits employed by the OCDT do use those features. Additionally, the second-level splits in the tree plot are dependent on feature Class being one or higher, which aligns with the conditions imposing constraints to assign some targets to zero. This visualization example shows that OCDTs have the potential to be interpretable like their conventional counterparts available in standard DT implementations.\n\n\nOur computational study shows that the proposed approach is highly effective in meeting constraints while delivering strong performance on various datasets. By incorporating optimization in split evaluations and leaf predictions, our methodology offers more accurate and feasible predictions compared to traditional methods, even on complex datasets. These results emphasize the usability and versatility of our approach, particularly in situations where compliance with constraints is essential for success.\n\n\n# 4 CONCLUSION\n\n\nWe have introduced new methods to handle constraints among target variables in decision trees, an area yet unexplored in the existing literature. We have proposed an optimization-based exact approach and several heuristic approaches to ensure feasible predictions and maintaining computational efficiency. Through a computational study with existing and generated datasets, we have demonstrated the effectiveness of the proposed methods in producing feasible predictions, showcasing their potential for applications in multi-target learning.\n\n\nLimitations. The work presented here has a few limitations. First, the optimization-based exact method can be computationally demanding for deep trees, which may impede its application for larger datasets. Second, the heuristic approaches proposed may not always produce accurate or even feasible predictions, limiting their reliability. Third, when the constraints among targets are highly non-convex, solving the prediction problem at each node can prove challenging, potentially affecting the overall performance of the decision tree. Finally, in its current form, it is unclear how these output-constrained decision trees can serve as the base learners for ensemble methods.\n\n\nFuture research. We aim to investigate the integration of our approach into bagging methods like random forests, as trees often act as base learners in these methods. We are also interested in exploring a warm-start method in the exact approach, where an optimization problem needs solving at each node. Given that these nodes share a parent-child relationship, the subsets they create could aid in devising this method for subsequent problems at the child nodes once the parent node is resolved. We are also considering the inclusion of target constraints in optimization-based algorithms for training trees like, for example, optimal classification trees (Bertsimas & Dunn, 2017). Although these algorithms can currently handle smaller problem sizes compared to standard implementations like CART (Breiman et al., 1984), this is a promising research direction. We are also looking into the potential advantages of considering constraints in clustering, since training a decision tree can be seen as a supervised clustering method.\n\n\n# REFERENCES\n\n- Esra Adiyeke and Mustafa G\u00f6k\u00e7e Baydo\u011fan. Semi-supervised extensions of multi-task tree ensembles. Pattern Recognition, 123(1):1-16, 2022. ISSN 0031-3203.\n\n- Arie Ben-David. Monotonicity maintenance in information-theoretic machine learning algorithms. Machine Learning, 19:29-43, 1995.\n\n- D.P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Computer Science and Applied Mathematics: A Series of Monographs and Textbooks. Academic Press, 1982.\n\n- Dimitris Bertsimas and Jack Dunn. Optimal classification trees. Machine Learning, 106(7):1039-1082, 2017.\n\n- Leo Breiman. Classification and Regression Trees. Routledge, New York, 2017.\n\n- Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. Cart. Classification and Regression Trees, 1984.\n\n- Stefano Calzavara, Claudio Lucchese, and Gabriele Tolomei. Adversarial training of gradient-boosted decision trees. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pp. 2429-2432, 2019.\n\n- Jaime S Cardoso and Ricardo Sousa. Classification models with global constraints for ordinal data. In 2010 Ninth International Conference on Machine Learning and Applications, pp. 71-77. IEEE, 2010.\n\n- Hongge Chen, Huan Zhang, Duane Boning, and Cho-Jui Hsieh. Robust decision trees against adversarial examples. In International Conference on Machine Learning, pp. 1122-1131. PMLR, 2019.\n\n- Yuntian Chen and Dongxiao Zhang. Physics-constrained deep learning of geomechanical logs. IEEE Transactions on Geoscience and Remote Sensing, 58(8):5932-5943, 2020.\n\n- Jacob K Christopher, Stephen Baek, and Ferdinando Fioretto. Constrained synthesis with projected diffusion models, 2024. URL https://arxiv.org/abs/2402.03559\n\n- Glenn De'ath. Multivariate regression trees: A new technique for modeling species-environment relationships. Ecology, 83(4):1105-1117, 2001. ISSN 1939-9170.\n\n- Milton Friedman. A comparison of alternative tests of significance for the problem of m rankings. The annals of mathematical statistics, 11(1):86-92, 1940.\n\n- Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. https://www.gurobi.com\n\n- Royce Kimmons. Exam Scores, 2012. http://roycekimmons.com/tools/generated_data/exams. Accessed on May 20, 2024.\n\n- Hoon Lee, Sang Hyun Lee, and Tony QS Quek. Constrained deep learning for wireless resource management. In ICC 2019-2019 IEEE International Conference on Communications (ICC), pp. 1-6. IEEE, 2019.\n\n- Hyuk Lee and In Seok Kang. Neural algorithm for solving differential equations. Journal of Computational Physics, 91(1):110-131, 1990.\n\n- Robin Lougee-Heimer. The common optimization interface for operations research: Promoting open-source software in the operations research community. IBM Journal of Research and Development, 47(1):57-66, 2003.\n\n- Velibor V. Mi\u0161i\u0107 and Georgia Perakis. Data analytics in operations management: A review. Manufacturing & Service Operations Management, 22(1):158-169, 2019.\n\n- G\u00e9rald\u00edn Nanfack, Paul Temple, and Benoit Fr\u00e9nay. Constraint enforcement on decision trees: A survey. ACM Computing Surveys (CSUR), 54(10s):1-36, 2022.\n\n- Peter Bjorn Nemenyi. Distribution-Free Multiple Comparisons. Princeton University, 1963.\n\n- F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.\n\n- Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686-707, 2019.\n\n- Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable machine learning: Fundamental principles and 10 grand challenges. Statistic Surveys, 16:1-85, 2022.\n\n- Jan Struyf and Sa\u0161o D\u017eeroski. Clustering trees with instance level constraints. In Machine Learning: ECML 2007: 18th European Conference on Machine Learning, Warsaw, Poland, September 17-21, 2007. Proceedings 18, pp. 359-370. Springer, 2007.\n\n- Jan Struyf and Sa\u0161o D\u017eeroski. Constraint based induction of multi-objective regression trees. In Knowledge Discovery in Inductive Databases: 4th International Workshop, Porto, 2006.\n\n- Bicheng Yan, Dylan Robert Harp, Bailian Chen, and Rajesh Pawar. A physics-constrained deep learning model for simulating multiphase flow in 3d heterogeneous porous media. *Fuel*, 313: 122693, 2022. ISSN 0016-2361. URL https://www.sciencedirect.com/science/article/pii/S001623612102559X\n\n- Chunjie Zhang, Renguang Zuo, Yihui Xiong, Xinfu Zhao, and Kuidong Zhao. A geologically-constrained deep learning algorithm for recognizing geochemical anomalies. Computers and Geosciences, 162:105100, 2022. ISSN 0098-3004. URL https://www.sciencedirect.com/science/article/pii/S0098300422000620\n\nResponse: \\boxed{Accept}\n\n### Rationale for Prediction\n\n**Originality:** The paper introduces novel methods to handle constraints among target variables in decision trees, which is a significant and underexplored area in the existing literature. The authors present both an optimization-based exact approach and several heuristic approaches, providing a comprehensive treatment of the topic.\n\n**Methodological Rigor:** The paper provides a detailed theoretical foundation for the proposed methods, including the formulation of the prediction problem and the justification for the use of optimization techniques. The authors also discuss the computational challenges and provide empirical evidence through a computational study. The use of rigorous mathematical formulations and theoretical proofs adds substantial methodological rigor.\n\n**Significance of Contribution:** The work addresses a practical problem in multi-target regression where constraints among the outputs must be respected. This is particularly relevant in fields such as operations management, finance, and healthcare, where such constraints are often known and must be enforced. The proposed methods offer a principled way to handle these constraints, enhancing the applicability of decision tree models in real-world scenarios.\n\n**Clarity of Presentation:** The paper is well-written and organized. The introduction clearly outlines the motivation and problem statement, while the technical details are presented in a structured manner. The theoretical derivations are supported by appropriate mathematical notation and are complemented by clear explanations.\n\n**Alignment with Journal Scope:** The journal ICLR typically publishes cutting-edge research in machine learning, including methods for improving the interpretability and feasibility of models. The paper\u2019s contributions in this area align well with the journal\u2019s interests.\n\n### Strengths:\n\n1. **Novel Contributions:** The authors introduce new methods for handling target constraints in decision trees, which is a significant and underexplored area.\n2. **Theoretical Foundations:** The paper provides solid theoretical foundations for the proposed methods, including the derivation of the optimization problem and the discussion of its solution.\n3. **Empirical Validation:** The authors conduct a thorough computational study using both an existing dataset and an extended version of the same dataset, demonstrating the effectiveness of their methods.\n4. **Practical Relevance:** The work addresses a practical problem in multi-target regression, making it directly applicable to real-world scenarios.\n5. **Comprehensive Approach:** The paper considers both exact and heuristic approaches, providing a balanced view of the problem and its solutions.\n\n### Weaknesses:\n\n1. **Computational Complexity:** The optimization-based exact approach can be computationally demanding for deep trees, which may limit its practical application for larger datasets.\n2. **Heuristic Limitations:** The heuristic approaches may not always produce accurate or feasible predictions, which limits their reliability.\n3. **Non-Convex Constraints:** Solving the prediction problem at each node can be challenging when the constraints among targets are highly non-convex, potentially affecting the overall performance of the decision tree.\n4. **Ensemble Integration:** The authors mention that it is unclear how these output-constrained decision trees can serve as base learners for ensemble methods like random forests, which is an interesting future direction.\n\nOverall, the paper presents a significant and innovative contribution to the field of decision tree learning with constraints. The authors have provided a robust theoretical framework and empirical validation, making it likely to be accepted by the ICLR community.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634757.0508087", "msecs": "50.0", "relativeCreated": "101874.28283691406", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Adversarial Data Robustness via Implicit Neural Representation\n\n# Abstract\nDespite its effectiveness, adversarial training requires that users possess a detailed understanding of training settings. However, many common users lack such expertise, making adversarial training impossible and exposing them to potential threats. We propose ``adversarial data robustness'', allowing the data to resist adversarial perturbations. Then, even if adversaries attack those data, these post-attack data can still ensure downstream models' robustness at users' end. This leads to our new setup, where we store the data as a learnable representation via Implicit Neural Representation (INR). Then, we can train such a representation adversarially to achieve data robustness. This paper analyzes the possible attacks to this setup and proposes a defense strategy. We achieve a comparable robustness level without resorting to model-level adversarial training.\n\n\n# 1 INTRODUCTION\n\n\nAdversarial training, a common defense mechanism against adversarial attacks, operates at the model level by specifically training deep learning models with adversarial examples. However, model-level adversarial training may undermine the desired performance of deep learning models when the robustness requirement is incorporated. Moreover, introducing large-scale base models also makes model-level adversarial training difficult. Therefore, it is essential to investigate alternatives to enhance these models' robustness without resorting to model-level adversarial training.\n\n\nAs adversarial perturbations are directly injected to images (Szegedy et al., 2014; Madry et al., 2018), a straightforward solution is to make those images robust to adversarial attacks, rather than optimizing a model adversarily like existing solutions (Madry et al., 2018; Zhang et al., 2020). This way, even when adversaries corrupt images as usual, ready-to-use models accessible to ordinary users can continue functioning properly. We call this adversarial data robustness, and propose \"training data adversially\" to achieve this goal. For images, one straightforward approach is to consider image pixels as learnable parameters and optimize these learnable parameters adversarily. However, such a straightforward solution significantly undermines image quality since the pixel values are directly manipulated. Therefore, we employ the Implicit Neural Representation (INR) as the cornerstone for our purposes. INRs represent data by optimizing a neural network to continuously map the coordinates to the corresponding data values (Dupont et al., 2021; Mildenhall et al., 2020; Sitzmann et al., 2020; Chen et al., 2022). Once the optimization converges, image pixels are stored as network weights, and the user can query neural networks with the corresponding coordinates to recover the whole image. Moreover, such a network-based representation framework is compatible with classical adversarial optimization for networks, enabling the generalization of existing model-level adversarial defense techniques.\n\n\nWe envision a setup for adversarial data robustness where typical model users or even the model developers are not burdened with extensive modifications to established frameworks. In this setup, images are pre-converted to their respective INRs and then sent to the model users. These users can then invoke straightforward functions encapsulated within an API to obtain the images for subsequent tasks. Since the generation process of representations incorporates adversarial robustness, images obtained from the post-attack representations maintain stable performance for downstream applications. For example, when the modifications to deep learning models are not permitted, or such modifications are costly, the model developers are not capable of training the model adversarially. By adversarily training the data, the adversarial data robustness can guarantee the function\n\n\n![Figure 1: In the traditional setup, images can be manipulated by adversarial perturbations to fool downstream models. In our setup, we represent images as INRs. By applying a defense strategy at the INR creation stage, the data can defend against adversarial perturbations during data creation and transmission. Downstream models' robustness to adversarial attacks can be ensured without adversarial training for models.](images/ccdc9169b35cf42cf86308814d4b8bb68afccdeb26f9918abba700c058271288.jpg)\n\n\nality of the downstream model without altering the model's parameters. This situation has become common, especially when downstream tasks use frozen large models (Guo et al., 2023), as their weights are difficult to modify.\n\n\nWe first explore possible attack types to the INR used as the data representation. Since only INR representation is used during the data transmission, an attack approach is to add adversarial perturbations to the network parameters for data representation. We, therefore, consider two strategies: 1) Attack during creation, which transfers adversarial perturbations from post-attack images to network parameters; and 2) Attack during transmission, which directly injects perturbations into the representation during its transmission. Given that the network parameters of INR are more susceptible to alterations due to external perturbations (Shu & Zhu, 2019), our emphasis is on balancing between attack efficacy and the quality of images generated from post-attack representations. We propose Double Projection Gradient Descent (DPGD) as a method for conducting attacks during data transmission, which can achieve a better balance by implementing gradient constraints on the image during the backpropagation process.\n\n\nWe propose defense-in-creation to defense above attacks within this setup. As Figure 1 depicts, our goal is to achieve adversarial data robustness in the creation process of INR. The INR generated by our method can defend against attacks during creation and attacks during transmission, ensuring the performance of specific downstream applications. To generate robust INR, we solve a min-max optimization problem by incorporating a robustness loss on top of the existing INR training framework. The weights assigned to these two losses can be used to strike a balance between reconstruction quality and robustness. Our research pioneers data robustness exploration, thereby charting a new course towards AI safety. Our major contribution can be concluded as follows:\n\n- - Our adversarial data robustness can guarantee the robustness of deep learning models against adversarial attacks, without necessitating model-level adversarial training.\n\n- - We examine potential attack types to INR used for the data representation. A Double Projection Gradient Descent (DPGD) is proposed to ensure adversarial patterns' invisibility when directly injected into INR parameters used for data representation.\n\n- - We have formulated a defense-in-creation strategy to defense the possible attacks.\n\n\n# 2 RELATED WORK\n\n\nImplicit neural representations. Implicit Neural Representation (INR) is a technique that leverages neural networks to create a mapping between a coordinate and its corresponding signal value. This method offers a continuous and memory-efficient way to model various signals, including 1D audio (Gao et al., 2022), 2D images (Tancik et al., 2020), 3D shapes (Park et al., 2019), 4D light fields (Sitzmann et al., 2021), and 5D radiance fields (Mildenhall et al., 2020). Direct supervision can be used to train accurate INR models for these signals by comparing the network output to ground truth data or indirect means, such as calculating the loss between the output after differentiable operators and a variant of the ground truth signal. This makes INR a powerful tool for solving inverse problems by taking advantage of the well-known forward processes in these problems. INR has found widespread applications in various fields, including computer vision and graphics (Tewari et al., 2022), computational physics (Karniadakis et al., 2021), biomedical engineering (Liu et al.,\n\n\n2022; Zhu et al., 2022), material science (Chen et al., 2020), and fluid mechanics (Raissi et al., 2020; Reyes et al., 2021). As INR is compatible with adversarial training framework, we propose to use INR as cornerstone to achieve data-level robustness.\n\n\nAdversarial attack and defense. Adversarial attacks and defense have emerged as significant research areas in machine learning. To explore the vulnerability of DNNs, various attacks have been proposed (Carlini & Wagner, 2017; Croce & Hein, 2020; Goodfellow et al., 2015; Madry et al., 2018), which can be generally categorized into white-box attacks and black-box attacks (Goodfellow et al., 2015). Most white-box attacks use gradients to obtain the perturbations on the inputs which maximize the loss function, including the Fast Gradient Sign Method (FGSM) (Szegedy et al., 2014), Projection Gradient Descent (PGD) (Madry et al., 2018) and Carlini and Wagner (CW) (Carlini & Wagner, 2017). Black-box attacks involve two methods, where attackers lack access to the victim models' information. Query-based methods approximate perturbations through a large number of queries (Andriushchenko et al., 2020; Cheng et al., 2018; Guo et al., 2019), while transfer-based methods utilize a surrogate model to generate adversarial examples with higher transferability (Liu et al., 2016; Wang & He, 2021). To counter these attacks, researchers have developed various defense mechanisms (Cohen et al., 2019; Li et al., 2021). One common approach is adversarial training, which has shown relative resistance to most existing attacks. The vanilla adversarial training strategy involves incorporating adversarial examples into the training data to create a min-max game during optimization (Madry et al., 2018). Numerous variants of adversarial training algorithms have been developed to improve the performance of adversarial robustness, including early-stopping strategy (Rice et al., 2020), TRADES (Zhang et al., 2019), FAT (Zhang et al., 2020), and CFA (Wei et al., 2023). Other techniques for adversarial defense include defensive distillation (Papernot et al., 2016), gradient regularization (Ross & Doshi-Velez, 2018), gradient masking (Folz et al., 2020), and input denoising (Liao et al., 2018). LINAC (Rusu et al., 2022) proposes to transform images into INR to train downstream networks to improve the robustness. However, these defense methods, which improve robustness via adversially training downstream networks, are not feasible for networks with unmodifiable parameters and could undermine the desired performance. Therefore, we propose a method to achieve adversarial data robustness to solve this problem.\n\n\n# 3 ADVERSARIAL DATA ROBUSTNESS\n\n\n# 3.1 OUR SETUP\n\n\nOur setup is rooted in the reality that many model users often fail to understand the settings associated with adversarial training. Moreover, model-level adversarial training can potentially compromise the intended performance of deep learning models. Therefore, when the adversarial training to models are difficult for common users, we aim to make the data robust to adversarial perturbations. Then, those robust data can ensure robust deep learning models.\n\n\nGiven that adversarial training has been an effective defense mechanism against adversarial attacks, the robust data can only be created via adversarial training. We thus propose storing the image as a network-based representation and enhancing this representation's robustness to potential adversarial perturbations. As depicted in Figure 1, even if this representation is attacked during its creation or transmission, users can still restore its clean state, ensuring robust performance of downstream tasks. We consider Implicit Neural Representation (INR) as the cornerstone of our formulation. For an image  $I$ , its INR is given by a MLP  $f_{\\theta}:\\mathbb{R}^2\\to \\mathbb{R}^3$  that maps the input spatial coordinate  $\\mathbf{p} = (p_x,p_y)\\in \\mathbb{R}^2$  to its corresponding pixel value  $I(\\mathbf{p})\\in \\mathbb{R}^3$  as  $I(\\mathbf{p}) = f_{\\theta}(\\mathbf{p})$ .  $\\theta$  represents the MLP's trainable parameters, which leave access for our adversarial data robustness.\n\n\nGenerally,  $f_{\\theta}$  can only reach its optimum status via iterative optimization. The optimization can be finished by overfitting  $f_{\\theta}$  to the image by minimizing a reconstruction loss between true pixel values  $I(\\mathbf{p}_{m,n})$  and predicted values  $f_{\\theta}(\\mathbf{p}_{m,n})$  as\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {r e c o n}} = \\frac {1}{M \\times N} \\sum_ {m, n} \\| f _ {\\theta} (\\mathbf {p} _ {m, n}) - I (\\mathbf {p} _ {m, n}) \\| _ {2} ^ {2}, \\tag {1}\n$$\n\n$$\n\n\nwhere  $(m,n)$  is the index of the corresponding pixel location, and  $(M,N)$  is size of the image. After optimization, the image can be stored or transmitted as network weights  $\\theta$ .\n\n\n![(a)](images/87b24a3fb2a1bc736048938fd74428d83fa4bda0494ad0bed6c0fa00eb565770.jpg)\n\n\n![(b)](images/5c5afad58216684fa43d367672314ca0f7b01834263d7df896b4b98011535ea3.jpg)\n\n\n![(c)](images/1f03163d253a330cacd4fffb90a50a224c6e4bf5e348542c88ebbaa26468e4fa.jpg)\n\n\n![(d) Figure 2: Samples after attack during creation. (a) are the ground truth images. (b) are the results of (a) being attacked by PGD. (c) are the images reconstructed from INRs optimized for (b). (d) are the residuals between (b) and (c). The output results of the classifier are labeled in the top left corner of each image. The images reconstructed from the representations attacked during their creation are almost the same to the images directly attacked by PGD, with a low average MSE value.](images/b336747d02b7c5ca00b335fb5be02abd7406181c238f9bb0c730ca1fc13985de.jpg)\n\n\nOnce such a representation is received by model users, they can obtain the image  $\\hat{I}$  from this representation by querying INR  $f_{\\theta}$  at each pixel coordinate as  $\\hat{I} (\\mathbf{p}_{m,n}) = f_{\\theta}(\\mathbf{p}_{m,n})$ , where  $(m,n)$  is the pixel index on the image. We define  $\\Psi :f_{\\theta}\\mapsto \\hat{I}$  as the mapping from INR to the reconstructed image. The image  $\\hat{I}$  is then fed into the downstream model to obtain the result.\n\n\nIn light of this setup, we examine the potential attack types to INR. Subsequently, we will delve into the associated adversarial training to ensure data robustness.\n\n\n# 3.2 ATTACK TYPES TO INR\n\n\nAn adversarial attack is to perturb the data used for downstream tasks (Szegedy et al., 2014). Usually, it is performed by introducing adversarial perturbations to input data, which is a designed slight alteration to data that can significantly change the output of downstream models. Since the data has been implicitly represented in our setup, the only way to conduct an adversarial attack is to inject adversarial perturbations into the representation. Similar to an adversarial attack directly for images, two criteria should be guaranteed: 1) invisibility: the perturbations are subtle and remain invisible within the content (Laidlaw et al., 2021); and 2) attacking efficacy: the invisible perturbations can still impair the efficacy of downstream applications (Szegedy et al., 2014). On the basis of these criteria, we explore attack types that can introduce adversarial perturbations to the data representation.\n\n\nAttack during creation. The first option is to attack the representation by transferring adversarial perturbations from corrupted images to the respective network parameters of INRs. In this case, adversaries introduce perturbations to images and subsequently strive to impart these perturbations onto the network parameters of the INR during its construction. Consequently, the adversarial perturbations, when recovered from the representation along with the image content, intensify the loss function for downstream tasks (e.g., cross-entropy loss for image classification).\n\n\nFormally, the adversarial perturbation for the  $i$ -th image  $I_{i}$  can be modeled as the solution of the following optimization problem, which is the same as the classical adversarial attacks (Madry et al., 2018):\n\n\n$$\n\n$$\n\\max  _ {\\Delta I _ {i}} \\quad \\mathcal {L} _ {C E} \\left(g _ {\\phi} \\left(I _ {i} + \\Delta I _ {i}\\right), y _ {i}\\right), \\quad \\text {s . t .} \\| \\Delta I _ {i} \\| _ {p} \\leq \\epsilon , \\tag {2}\n$$\n\n$$\n\n\nwhere  $g_{\\phi}$  denotes the neural network for downstream tasks (e.g., classification (Szegedy et al., 2014)),  $\\Delta I_i$  is the adversarial perturbation for the  $i$ -th image,  $y_i$  is the true label of  $I_i$  for classification task, and  $\\mathcal{L}_{CE}(\\cdot)$  denotes the cross entropy loss function. The  $\\| \\cdot \\|_p$  denotes the  $\\ell_p$ -norm, and the constraint limits the size of perturbation with maximum allowable value  $\\epsilon$  to avoid being recognized by humans. Unless otherwise stated, the infinite norm is adopted by setting  $p = \\infty$  in this paper following previous works (Dong et al., 2023).\n\n\nINRs can accurately capture the very fine details of images (Chen et al., 2021; Str\u00fcmpler et al., 2022). Therefore, during the creation of an INR, the INR weights can accurately represent adversarial perturbation  $\\Delta x$ . The reconstructed image from such an INR retains the adversarial information and can deceive the downstream applications. As shown in Figure 2, the images reconstructed from INR under attack during creation resemble the corrupted images with adversarial perturbation, allowing the reconstructed image to fool the downstream applications.\n\n\n![Figure 3: Framework of defense-in-creation. We encode an image into the parameter of its corresponding INR,  $\\theta$ . Left: we calculate the distance between the reconstructed image and the to-be-fitted image to establish a reconstruction loss for high-quality reconstruction. Right: we introduce adversarial perturbation into the parameter  $\\theta$  during INR creation process, and design a robustness loss to ensure downstream models' robustness by comparing its prediction with the groundtruth.](images/1317a0a9c0b6a1410b2351019b63b34f1561739d020434add906d9d69b17d797.jpg)\n\n\nAttack during transmission. The second strategy is to attack the representation directly. This attack typically occurs during the transmission of the data representation. When the adversaries access the representation, they can directly inject adversarial perturbations into the network parameters. The process of conducting an adversarial attack to the INR representation of  $i$ -th image during its transmission can be concluded as follows:\n\n\n$$\n\n$$\n\\max  _ {\\Delta \\theta_ {i}} \\quad \\mathcal {L} _ {C E} \\left(g _ {\\phi} \\left(\\Psi \\left(f _ {\\theta_ {i} + \\Delta \\theta_ {i}}\\right)\\right), y _ {i}\\right), \\quad \\text {s . t .} \\| \\Delta \\theta_ {i} \\| _ {p} \\leq \\delta , \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\Delta \\theta_{i}$  is the adversarial perturbation for the  $i$ -th image INR, and we also use  $\\ell_{\\infty}$ -norm in Equation 3 to limit the size of the perturbation.\n\n\nA simple method to conduct attack during transmission is to manipulate the INR parameters using the traditional gradient-based attack methods, such as Fast Gradient Sign Method (FGSM) (Szegedy et al., 2014), Projected Gradient Descent (PGD) (Madry et al., 2018), and Carlini and Wagner (CW) (Carlini & Wagner, 2017). These attacks are referred to as FGSM for INR, PGD for INR, and CW for INR, respectively. While these methods are initially designed to operate on images, integrating INR into the differentiable pipeline allows us to derive the gradient value of INR parameters by leveraging the loss value of the downstream classifier. The whole process of PGD attack for INR is shown in Algorithm 1. Given an optimized INR, we start with randomly perturbed parameters within the maximum perturbation. In every iteration, we reconstruct the image from the INR and feed it into the classifier to compute the loss. Then, we iteratively apply gradient updates and ensure that the perturbed parameters remain within the allowable range through projection. However, as INR parameters are sensitive to direct manipulations, such a straightforward solution can easily undermine the quality of images obtained from the post-attack representation.\n\n\nTo better balance the attacking efficacy and the image quality, we propose a Double Projection Gradient Descent (DPGD) that aims to directly manipulate the INR parameters while preserving the reconstructed image's quality. The detailed process of DPGD can be found in Algorithm 2. Unlike PGD for INR, we incorporate an additional projection in each iteration. This projection involves projecting the gradient backpropagated to the reconstructed image onto a boundary controlled by the factor  $\\zeta$ . The motivation of this step is to directly limit the difference between the reconstructed image and the original image at the pixel level. For detailed derivation, please refer to Section B in appendix. The projected gradient is then further backpropagated to update the INR parameters. The quality of images derived from the representations remains high by constraining the gradient on the reconstructed image. The factor  $\\zeta$  can control the balance between the attacking efficacy and the image quality.\n\n\n# 3.3 DEFENSE-IN-CREATION\n\n\nTo achieve data-level robustness, we propose a defense-in-creation strategy that generates robust INR during the creation process to defend against adversarial attacks effectively. The perturbations generated in attacks during creation can be eliminated in optimizing phase, and the INR derived using defense-in-creation can further defend against potential attacks during transmission. By employing this approach, we can ensure the performance of downstream networks at a data level. Besides, since the basic function of INR is to store the content of images, we also consider the quality of the reconstructed image in our method.\n\n\n```txt\n1: Let  $\\theta_{adv} = \\theta$\n```\n\n\n```txt\n2: Random start:  $\\theta_0 = \\theta_{adv} + \\Delta \\theta_0, \\theta_0 \\sim \\mathcal{U}(-\\delta, \\delta)$ .\n```\n\n\n```txt\n3: for  $t = 1,2,\\dots ,N$  do\n```\n\n\n```txt\n4: Reconstruct image  $\\hat{I}_{t - 1} = \\Psi (f_{\\theta_{t - 1}})$\n```\n\n\n```txt\n5:  $\\theta_{t} = \\theta_{t - 1} + \\alpha \\cdot \\mathrm{sign}(\\nabla_{\\theta_{t - 1}}\\mathcal{L}_{CE}(g_{\\phi}\\left(\\hat{I}_{t - 1}\\right),y));$\n```\n\n\n```txt\n6: Update  $\\theta_t = \\mathrm{Proj}_{\\|\\theta_t - \\theta\\|_p \\leq \\delta}(\\theta_t)$ .\n```\n\n\n```txt\n7: end for\n```\n\n\n```txt\n8: return  $\\theta_{adv} = \\theta_N$\n```\n\n\n```txt\n1: Reconstruct original image  $\\hat{I}_{org} = \\Psi (f_{\\theta})$\n```\n\n\n```txt\n2: Let  $\\theta_{adv} = \\theta$\n```\n\n\n```txt\n3: Random start:  $\\theta_0 = \\theta_{adv} + \\Delta \\theta_0, \\theta_0 \\sim \\mathcal{U}(-\\delta, \\delta)$ .\n```\n\n\n```txt\n4: for  $t = 1,2,\\dots ,N$  do\n```\n\n\n```txt\n5: Reconstruct image  $\\hat{I}_{t - 1} = \\Psi (f_{\\theta_{t - 1}})$\n```\n\n\n```txt\n6:  $Grad_{I} = \\frac{\\partial\\mathcal{L}_{CE}(g_{\\phi}(\\hat{I}_{t - 1}),y)}{\\partial\\hat{I}_{t - 1}};$\n```\n\n\n```txt\n7: Update  $Grad_{I} = \\mathrm{Proj}\\left\\| \\hat{I}_{t - 1} + Grad_{I} - \\hat{I}_{org}\\right\\|_{p} \\leq \\zeta (Grad_{I})$ ;  $\\triangleright$  Projecting gradient of image pixels\n```\n\n\n```javascript\n8:  $Grad_{\\theta} = \\frac{\\partial\\hat{I}_{t - 1}}{\\partial\\theta_{t - 1}}\\cdot Grad_{I};$\n```\n\n\n```latex\n9:  $\\theta_{t} = \\theta_{t - 1} + \\alpha \\cdot \\mathrm{sign}(Grad_{\\theta})$\n```\n\n\n```txt\n10: Update  $\\theta_{t} = \\mathrm{Proj}_{\\| \\theta_{t} - \\theta \\|_{p}\\leq \\delta}(\\theta_{t})$  Projecting gradient of INR parameters\n```\n\n\n```txt\n11: end for\n```\n\n\n```txt\n12: return  $\\theta_{adv} = \\theta_N$\n```\n\n\nAs shown in Figure 3, defense-in-creation is implemented by designing a robust loss on top of the original reconstruction loss defined in Equation 1. The robustness loss is obtained by adding adversarial perturbations to the INR parameters and calculating the loss after feeding the reconstructed image into the classifier  $g_{\\phi}$ . The whole defense process can be formulated as follows:\n\n\n$$\n\n$$\n\\min  _ {\\theta} \\lambda_ {1} \\mathcal {L} _ {r e c o n} + \\lambda_ {2} \\max  _ {\\Delta \\theta} \\mathcal {L} _ {C E} \\left(g _ {\\phi} \\left(\\Psi \\left(f _ {\\theta + \\Delta \\theta}\\right)\\right), y _ {i}\\right), \\quad \\text {s . t .} \\| \\Delta \\theta \\| _ {p} \\leq \\delta , \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\Delta \\theta$  is the adversarial perturbation added in the training phase. The parameters  $\\lambda_{1}$  and  $\\lambda_{2}$  balance the reconstruction quality and robustness. The detailed solution to the above optimization problem is shown in Algorithm 3. We use the robustness loss generated from Algorithm 1 to enhance the robustness, and employ reconstruction loss to guarantee the image quality. By minimizing the loss obtained from classifier  $g_{\\phi}$ , we can effectively remove the adversarial perturbation embedded in the target image when performing attack during creation. By introducing adversarial samples during the training process of INR, INR can gradually learn to have better robustness against parameter perturbations, thereby defending against attack during transmission.\n\n\n# 4 EXPERIMENTS AND RESULTS\n\n\n# 4.1 EXPERIMENTAL SETTINGS\n\n\nDataset. We follow established settings (Zhang et al., 2020; Jin et al., 2023) on adversarial attack and defense to evaluate our proposed mechanism. Our experiments are conducted on three real-world datasets: CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and SVHN (Netzer et al., 2011). CIFAR-10/100 dataset contains  $60K$  color images with the size\n\n\n```\nInput: Image  $I$  , ground truth label  $y$  , and pre-trained downstream model  $g_{\\phi}$    \nParameter: Number of iterations  $T$  , learning rate  $\\eta$    \nOutput: INR  $f_{\\theta}$  for image  $I$    \n1: Initialize model parameters  $\\theta$    \n2: for  $t = 1$  to  $T$  do   \n3: Generate INR adversarial example  $f_{\\theta_{adv}}$  using Algorithm 1;   \n4: Compute reconstruction loss  $\\mathcal{L}_{recon}$  as Equation 1;   \n5: Compute robustness loss as  $\\mathcal{L}_{CE}(g_{\\phi}(\\Psi (f_{\\theta_{adv}})),y)$  .   \n6: Compute the total loss  $\\mathcal{L}_{total}$  as Equation 4;   \n7: Update parameters using adversarial example:  $\\theta \\gets \\theta -\\eta \\cdot \\nabla_{\\theta}\\mathcal{L}_{total}$    \n8: end for   \n9: return Trained INR parameters  $\\theta$\n```\n\n\nof  $32 \\times 32$ , including  $50K$  training images and  $10K$  test images in 10 and 100 classes, respectively. SVHN is a dataset collected by Google Street View, consisting of  $32 \\times 32$  color images of house numbers, with 73, 257 training images and 26, 032 test images in 10 classes. The classifier is trained using training images. We train INRs for test images to evaluate our attack approaches and defense methods. As suggested in previous work (Wei et al., 2023; Jin et al., 2022), we evaluate the performance of attacks and defenses based on the PreActResNet-18 (He et al., 2016) and WideResNet34-10 (Zagoruyko & Komodakis, 2016) architectures on CIFAR-10/100 and SVHN.\n\n\nImplementation Details. We implement our method using PyTorch. The INR is a 5-hidden layer MLP with 256 channels per layer and ReLU activation functions for all the data. Following Mildenhall et al. (2020), we also use a positional encoding for pixel coordinates with 5 frequencies. For adversarial defense training, 10-step PGD attack for INR is applied, with maximum perturbation size for parameters  $\\delta = 0.0006$ . We use the Adam optimizer with default values and a learning rate of 0.001. A cosine learning rate decay schedule is used with the minimum value of the multiplier 0.0001. We optimize each INR for 1000 iterations. Other hyper-parameters are specified in each experiment. We conduct experiments on FGSM (Szegedy et al., 2014), PGD (Madry et al., 2018), Carlini and Wagner (CW) (Carlini & Wagner, 2017), and AutoAttack (AA) (Croce & Hein, 2020) for attack during creation. The maximum perturbation size  $\\epsilon$  is set to  $8/255$  for FGSM and PGD. PGD is used with steps 100, and step size  $0.2/255$ . We apply FGSM for INR, PGD for INR, CW for INR, and DPGD for attack during transmission. The steps of PGD for INR, and DPGD are set to 100, and maximum perturbation size for INR parameters  $\\delta = 0.0006$ , with step size  $2.5 \\cdot \\delta/$  steps in all experiments. CW attack is applied by optimizing 100-step PGD (Zhou et al., 2022; Huang et al., 2021). All the experiments are performed on NVIDIA Tesla V100 GPUs.\n\n\nBaselines. We found no method specifically for enhancing the robustness at the data level. Therefore, we compare with other three settings for comparison: 1) Normal training: optimizing the INR with only reconstruction loss; 2) Direct pixel manipulation<sup>1</sup>: direct manipulating the pixel values without INR encoding; 3) Model-level adversarial training: adversarial training for model robustness including AT (Madry et al., 2018), TRADES (Zhang et al., 2019), FAT (Zhang et al., 2020), GAIRAT (Zhang et al., 2021), and LAS-AT (Jia et al., 2022). Although the latter method aims to enhance the robustness through training the downstream classifier, we conduct a comparison with them, and the analysis can be found in Section 4.3.\n\n\nEvaluation metrics. We assess all methods' performance by evaluating image quality, attack effectiveness, and defense ability. For image quality, we evaluate the performance by using PSNR, SSIM, and LPIPS (Zhang et al., 2018). Higher PSNR and SSIM value indicates better performance, while lower LPIPS value indicates better performance for both attack and defense. To assess the effectiveness of attacks, we utilize the attack success rate (ASR). In terms of defense, we evaluate the defense ability by considering the classification accuracy of the downstream classifier.\n\n\n# 4.2 EXPERIMENTAL RESULTS FOR DIFFERENT ATTACKS\n\n\nThis section evaluates the performance of the attacks used in our research. Specifically, we assess the attacks during creation and transmission to evaluate their invisibility and attacking efficacy.\n\n\nTable 1: Evaluation of attack during creation with different image-based attack methods. The results are averaged on all examples in CIFAR-10 test dataset.\n\n\n<table><tr><td>Method</td><td>Clean images</td><td>Clean INR</td><td>Method</td><td>FGSM on images</td><td>Creation after FGSM</td><td>PGD on images</td><td>Creation after PGD</td><td>CW on images</td><td>Creation after CW</td><td>AA on images</td><td>Creation after AA</td></tr><tr><td>Accuracy</td><td>94.97%</td><td>94.73%</td><td>ASR</td><td>60.45%</td><td>59.26%</td><td>100.00%</td><td>99.83%</td><td>99.93%</td><td>99.84%</td><td>100.00%</td><td>99.89%</td></tr><tr><td>PSNR</td><td>-</td><td>48.42</td><td>PSNR</td><td>30.14</td><td>31.01</td><td>32.70</td><td>33.70</td><td>32.68</td><td>34.12</td><td>32.71</td><td>33.85</td></tr><tr><td>SSIM</td><td>1.000</td><td>0.998</td><td>SSIM</td><td>0.921</td><td>0.935</td><td>0.956</td><td>0.967</td><td>0.957</td><td>0.970</td><td>0.956</td><td>0.968</td></tr><tr><td>LPIPS</td><td>0.000</td><td>0.002</td><td>LPIPS</td><td>0.146</td><td>0.134</td><td>0.082</td><td>0.071</td><td>0.082</td><td>0.067</td><td>0.082</td><td>0.070</td></tr></table>\n\n\nTable 2: Reconstruction qualities and accuracies of downstream classifiers compared with normal training and direct pixel manipulation. The results are averaged on all examples in each dataset with different classifier architectures. Image quality evaluations are obtained by comparing with original clean images. The classifier architectures used for CIFAR-10/100 and SVHN are PreActResNet-18 and WideResNet34-10, respectively. More results can be found in the appendix.\n\n\n<table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Method</td><td rowspan=\"2\">PSNR</td><td rowspan=\"2\">Accuracy without attacks</td><td colspan=\"3\">Attack during creation</td><td colspan=\"4\">Attack during transmission</td></tr><tr><td>PGD</td><td>CW</td><td>AA</td><td>DPGD</td><td>FGSM</td><td>PGD</td><td>CW</td></tr><tr><td rowspan=\"4\">CIFAR-10</td><td>Normal image</td><td>-</td><td>94.97%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>39.55%</td><td>0.00%</td><td>0.07%</td></tr><tr><td>Pixel manipulation</td><td>33.16</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>-</td><td>99.88%</td><td>33.80%</td><td>27.88%</td></tr><tr><td>Normal training</td><td>48.38</td><td>94.73%</td><td>0.17%</td><td>0.16%</td><td>0.11%</td><td>20.80%</td><td>57.36%</td><td>18.84%</td><td>17.72%</td></tr><tr><td>Defense-in-creation</td><td>42.92</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>99.60%</td><td>100%</td><td>98.68%</td><td>61.36%</td></tr><tr><td rowspan=\"4\">CIFAR-100</td><td>Normal image</td><td>-</td><td>76.92%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>8.44%</td><td>0.01%</td><td>0.00%</td></tr><tr><td>Pixel manipulation</td><td>29.63</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>-</td><td>99.96%</td><td>30.08%</td><td>0.56%</td></tr><tr><td>Normal training</td><td>48.26</td><td>76.44%</td><td>0.16%</td><td>0.12%</td><td>0.11%</td><td>4.32%</td><td>17.68%</td><td>2.16%</td><td>1.76%</td></tr><tr><td>Defense-in-creation</td><td>40.50</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>98.96%</td><td>100%</td><td>98.44%</td><td>74.52%</td></tr><tr><td rowspan=\"4\">SVHN</td><td>Normal image</td><td>-</td><td>95.80%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>41.84%</td><td>1.32%</td><td>0.96%</td></tr><tr><td>Pixel manipulation</td><td>35.78</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>-</td><td>100%</td><td>76.60%</td><td>18.24%</td></tr><tr><td>Normal training</td><td>53.03</td><td>95.80%</td><td>1.28%</td><td>1.20%</td><td>0.18%</td><td>39.04%</td><td>65.96%</td><td>36.24%</td><td>35.76%</td></tr><tr><td>Defense-in-creation</td><td>46.98</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>99.88%</td><td>100%</td><td>99.88%</td><td>84.84%</td></tr></table>\n\n\nAttack during creation. We validate the attack during creation, and the findings are presented in Table 1. We compare the results of this attack on the INR parameters with those obtained by directly inputting the images attacked by FGSM, PGD, CW, and AA into the downstream classifier. The reconstructed images display comparable attack effectiveness and image quality to those manipulated using FGSM, PGD, CW, and AA methods. Due to the excellent representation capabilities of INR, the performance of the attack during creation is determined by attacks applied to the images.\n\n\n![Figure 4: Evaluation of attack during transmission. Left: we present the ASRs and PSNRs across varying gradient control factors in DPGD. Right: we compare the ASR v.s. PSNR curves with varying the maximum perturbation across different methods. The results are averaged on all examples in CIFAR-10 test dataset.](images/e7ac40b9dc77f8b2c90e27b6d44a9a91c41646ee192d80a8299f861a038e41e3.jpg)\n\n\n![](images/c11888adfa90cd788c6725f0e3f1063302e8607a2f85b09c5d4365e72d27bb29.jpg)\n\n\nAttack during transmission. We evaluate the impact of the image gradient control factor  $\\zeta$  on the invisibility and attacking efficacy in the DPGD algorithm. We fix the maximum perturbation at  $\\delta = 0.0006$ , and the ASR and image quality results are shown in Figure 4(left). The results indicate that the factor  $\\zeta$  can regulate the balance between invisibility and attacking efficacy. Specifically, a larger value of  $\\zeta$  indicates higher attacking efficacy, while a smaller value of  $\\zeta$  indicates a closer resemblance to the original image. We further compare our DPGD algorithm with FGSM for INR, PGD for INR, and CW for INR in Figure 4(right). We take  $\\zeta = 0.9$  and 1.3, respectively, and vary the maximum perturbation  $\\delta$  to obtain curves illustrating the relationship between image quality and ASR. At the same level of image quality, our method achieves higher ASR, indicating a higher attacking efficacy. Therefore, our proposed DPGD achieves better performance.\n\n\n# 4.3 EXPERIMENTAL RESULTS FOR DEFENSE\n\n\nIn this section, we present experimental results of our proposed adversarial defense method and compare them with the baselines.\n\n\nTable 3: Accuracies compared with model-level adversarial training. The classifier structures are all WideResNet34-10. The results are averaged on all examples in CIFAR-10 test dataset.\n\n\n<table><tr><td>Defense method</td><td>Defense-in-creation</td><td>AT</td><td>TRADES</td><td>FAT</td><td>GAIRAT</td><td>LAS-AT</td></tr><tr><td>Natural</td><td>94.32%</td><td>85.90%</td><td>85.72%</td><td>87.97%</td><td>86.30%</td><td>86.23%</td></tr><tr><td>PGD</td><td>92.51%</td><td>53.42%</td><td>53.40%</td><td>47.48%</td><td>40.30%</td><td>53.58%</td></tr></table>\n\n\nRobustness and reconstruction quality. We further evaluate our proposed adversarial defense for achieving data robustness. We set  $\\lambda_{1} = 1$ ,  $\\lambda_{2} = 0.0022$ ,  $0.0006$ ,  $0.001$  for CIFAR-10, CIFAR-100, and SVHN, respectively. The results are shown in Table 2. We study the performance of the proposed defense in defending against various attack methods. Our method achieves the highest accuracy and produces high-quality reconstructions across different attacks, datasets, and classifier architectures. While normal training achieves the best reconstruction results, it is vulnerability to all attacks, resulting in low accuracy under various attack scenarios. Although pixel manipulation can defend against attacks during creation, it leads to lower accuracy when encountering attacks during transmission, such as PGD for INR and CW for INR.\n\n\nComparison with different  $\\lambda_{2}$ . To evaluate the balance between reconstruction quality and robustness in our defense-in-creation strategy, we adjust the weight ratio between reconstruction loss  $\\lambda_{1}$  and weight of robustness loss  $\\lambda_{2}$ . We conduct a series of experiments under attacks during transmission by fixing  $\\lambda_{1} = 1$  and varying the weight of robustness loss  $\\lambda_{2}$ . As shown in Figure 5, a smaller value of  $\\lambda_{2}$  leads to higher reconstruction quality, while a larger value of  $\\lambda_{2}$  enhances robustness against attacks.\n\n\n![Figure 5: Balance between reconstruction quality and robustness against attacks during transmission. The results are averaged on all examples in CIFAR-10 test dataset.](images/9a5a470ac00ee922da0bfc9886a7df36417c8bd027da3cbb6792468b9c43da83.jpg)\n\n\nComparison with model-level adversarial training. Our method aims to enhance data-level adversarial robustness, and its motivation differs from traditional adversarial training approaches. As attacks on INRs are different from those on images, we apply PGD for INR in our method. Furthermore, to ensure a fair comparison, we use the predicted labels generated by the downstream classifier instead of the ground truth labels in the creation process of INR. As illustrated in Table 3, our method demonstrates better performance with natural data and stronger robustness compared with model-level adversarial training methods.\n\n\n# 5 CONCLUSION\n\n\nWe demonstrate adversarial data robustness in this paper by utilizing implicit neural representations. Instead of adversially training deep learning models, we focus on adversially training the data. Doing so ensures that the data retains robust performance even if it is subjected to attacks before it reaches the model users. To accomplish this, we initially represent the data implicitly and focus on adversarial training of this representation during its formation. By analyzing the potential attacks, we unveil an adversarial training scheme that bolsters the robustness of implicit data representations against potential adversarial attacks. In our proposed approach, ready-to-use image classifiers exhibit adversarial robustness on par with models that undergo model-level adversarial training.\n\n\nLimitations. Though our method is successful in achieving robustness at the data level, it cannot handle some extreme cases. For example, if malicious users gain access to the reconstructed images from the INR, they can directly apply adversarial perturbations to the image pixels, thereby circumventing attacks on the INR. This problem can be solved through system design by protecting the process from image reconstruction to downstream model input, to ensure that the recovered images are not accessible. Besides, we only consider risks caused by malicious technical designs. However, as security is not solely a technical problem, we still need to collaborate with various parties to consider the legal and social impact. We will further explore the feasibility of our approach in other data types and downstream tasks in the future.\n\n\n# ETHICS STATEMENT\n\n\nOur goal is to ensure the security of deep neural networks. We did not employ crowdsourcing and did not involve human subjects in our experiments. When utilizing existing assets such as code, data, or models, we have properly cited the original creators.\n\n\n# REFERENCES\n\n- Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-efficient black-box adversarial attack via random search. In European Conference on Computer Vision, 2020.\n\n- Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy, 2017.\n\n- Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\n\n- Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics Express, 2020.\n\n- Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vedit Goel, Zhangyang Wang, Humphrey Shi, and Xiaolong Wang. VideoINR: Learning video implicit neural representation for continuous space-time super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, JinFeng Yi, and Cho-Jui Hsieh. Query-efficient hard-label black-box attack: An optimization-based approach. In International Conference on Learning Representations, 2018.\n\n- Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, 2019.\n\n- Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International Conference on Machine Learning, 2020.\n\n- Junhao Dong, Seyed-Mohsen Moosavi-Dezfooli, Jianhuang Lai, and Xiaohua Xie. The enemy of my enemy is my friend: Exploring inverse adversaries for improving adversarial training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Emilien Dupont, Adam Golinski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. COin: Compression with implicit neural representations. In Neural Compression: From Information Theory to Applications ICLRW, 2021.\n\n- Joachim Folz, Sebastian Palacio, Joern Hees, and Andreas Dengel. Adversarial defense based on structure-to-signal autoencoders. In IEEE Winter Conference on Applications of Computer Vision, 2020.\n\n- Ruohan Gao, Zilin Si, Yen-Yu Chang, Samuel Clarke, Jeannette Bohg, Li Fei-Fei, Wenzhen Yuan, and Jiajun Wu. Objectfolder 2.0: A multisensory object dataset for sim2real transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In *Joshua Bengio and Yann LeCun* (eds.), International Conference on Learning Representations, 2015.\n\n- Chuan Guo, Jacob Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Weinberger. Simple black-box adversarial attacks. In International Conference on Machine Learning, 2019.\n\n- Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and Steven Hoi. From images to textual prompts: Zero-shot visual question answering with frozen large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, 2016.\n\n- Hanxun Huang, Yisen Wang, Sarah Erfani, Quanquan Gu, James Bailey, and Xingjun Ma. Exploring architectural ingredients of adversarially robust deep neural networks. Advances in Neural Information Processing Systems, 2021.\n\n- Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. LAS-AT: adversarial training with learnable attack strategy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Gaojie Jin, Xinping Yi, Wei Huang, Sven Schewe, and Xiaowei Huang. Enhancing adversarial training with second-order statistics of weights. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Gaojie Jin, Xinping Yi, Dengyu Wu, Ronghui Mu, and Xiaowei Huang. Randomized adversarial training via taylor expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 2021.\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), International Conference on Learning Representations, 2015.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n\n- Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against unseen threat models. In International Conference on Learning Representations, 2021.\n\n- Yanxi Li, Zhaohui Yang, Yunhe Wang, and Chang Xu. Neural architecture dilation for adversarial robustness. Advances in Neural Information Processing Systems, 2021.\n\n- Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against adversarial attacks using high-level representation guided denoiser. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\n- Renhao Liu, Yu Sun, Jiabei Zhu, Lei Tian, and Ulugbek S Kamilov. Recovery of continuous 3d refractive index maps from discrete intensity-only measurements using neural fields. Nature Machine Intelligence, 2022.\n\n- Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In International Conference on Learning Representations, 2016.\n\n- Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017.\n\n- Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\n\n- Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision, 2020.\n\n- Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In Proceedings of the NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.\n\n- Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and Privacy, 2016.\n\n- Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.\n\n- Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 2020.\n\n- Brandon Reyes, Amanda A Howard, Paris Perdikaris, and Alexandre M Tartakovsky. Learning unknown physics of non-newtonian fluids. Physical Review Fluids, 2021.\n\n- Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversially robust deep learning. In International Conference on Machine Learning, 2020.\n\n- Andrew Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.\n\n- Andrei A Rusu, Dan Andrei Calian, Sven Gowal, and Raia Hadsell. Hindering adversarial attacks with implicit neural representations. In International Conference on Machine Learning, 2022.\n\n- Hai Shu and Hongtu Zhu. Sensitivity analysis of deep neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019.\n\n- Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Advances in Neural Information Processing Systems, 2020.\n\n- Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 2021.\n\n- Yannick Str\u00fcmpler, Janis Postels, Ren Yang, Luc Van Gool, and Federico Tombari. Implicit neural representations for image compression. In European Conference on Computer Vision, 2022.\n\n- Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014.\n\n- Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Advances in Neural Information Processing Systems, 2020.\n\n- Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Wang Yifan, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Advances in neural rendering. In Computer Graphics Forum, 2022.\n\n- Xiaosen Wang and Kun He. Enhancing the transferability of adversarial attacks through variance tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\n\n- Zeming Wei, Yifei Wang, Yiwen Guo, and Yisen Wang. CFA: Class-wise calibrated fair adversarial training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British Machine Vision Conference 2016. British Machine Vision Association, 2016.\n\n- Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning, 2019.\n\n- Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In International Conference on Machine Learning, 2020.\n\n- Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli. Geometry-aware instance-reweighted adversarial training. In International Conference on Learning Representations, 2021.\n\n- Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\n- Dawei Zhou, Nannan Wang, Bo Han, and Tongliang Liu. Modeling adversarial noise for adversarial training. In International Conference on Machine Learning, 2022.\n\n- Hao Zhu, Zhen Liu, You Zhou, Zhan Ma, and Xun Cao. DNF: diffractive neural field for lensless microscopic imaging. Optics Express, 2022.\n\n\n![Figure 6: A visualization of our INR architecture.](images/a5371be97e66712ad169f513b693d85da4c8218a359d50b3728f8df172c743b3.jpg)\n\nResponse: Based on the provided paper, here is a systematic evaluation of its strengths and weaknesses:\n\n**Strengths:**\n1. **Original Contribution**: The paper introduces a novel concept of adversarial data robustness, proposing to make data itself robust to adversarial attacks rather than focusing on adversarial training of models. This is a fresh approach that addresses a gap in current literature.\n\n2. **Methodological Rigor**: The authors provide a detailed explanation of their methodology, including the formulation of adversarial attacks on INRs, defense-in-creation, and experimental validation. They describe how the proposed method compares favorably to existing techniques.\n\n3. **Experimental Validation**: Extensive experimental results are provided across different datasets (CIFAR-10, CIFAR-100, SVHN) and attack methods (FGSM, PGD, CW, AutoAttack), demonstrating the effectiveness of the proposed defense mechanism. \n\n4. **Scalability and Versatility**: The paper shows that the proposed method achieves competitive performance across various attack methods and classifier architectures, indicating broad applicability.\n\n5. **Ethical Considerations**: The authors discuss limitations and ethical implications, which is important for a comprehensive evaluation of any AI technology.\n\n**Weaknesses:**\n1. **Complexity and Feasibility**: The proposed method involves complex optimization processes and multiple layers of attack-defense mechanisms. While promising, the complexity might pose challenges for practical deployment in certain contexts.\n\n2. **Limited Robustness**: The paper mentions that the method does not handle some extreme cases where malicious users can bypass the attack on INRs. This limitation is acknowledged but not fully addressed.\n\n3. **Comparative Limitations**: While the method performs well against a variety of attacks, it is compared only with a few baseline methods (model-level adversarial training). A broader comparative study with more diverse state-of-the-art defenses would strengthen the paper.\n\n4. **Overfitting Concerns**: The paper does not explicitly address overfitting issues that might arise from the intricate optimization procedures used for generating robust INRs. Further discussion on this point would be beneficial.\n\n5. **Scalability Issues**: The experiments are conducted on relatively small datasets (CIFAR-10, CIFAR-100, SVHN). Scalability to larger datasets and real-world applications is not thoroughly explored.\n\n**Overall Impact Potential:**\nThe paper presents a novel and potentially impactful approach to enhancing adversarial robustness at the data level. While there are some limitations, the experimental results show promising performance across various attack scenarios. Addressing the identified weaknesses could further solidify the method's place in the field.\n\n**Prediction:**\nGiven the innovative approach and strong empirical validation, I would predict that this paper has a good chance of acceptance at ICLR. It presents a significant contribution that aligns well with the conference's focus on machine learning and artificial intelligence research. However, it might benefit from addressing some of the noted limitations and providing a more comprehensive comparison with existing defenses.\n\nMy prediction is: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634764.972408", "msecs": "972.0", "relativeCreated": "109795.88222503662", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills\n\n# Abstract\nGeneralizable manipulation skills, which can be composed to tackle long-horizon and complex daily chores, are one of the cornerstones of Embodied AI. However, existing benchmarks, mostly composed of a suite of simulatable environments, are insufficient to push cutting-edge research works because they lack object-level topological and geometric variations, are not based on fully dynamic simulation, or are short of native support for multiple types of manipulation tasks. To this end, we present ManiSkill2, the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. ManiSkill2 includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D-input data simulated by fully dynamic engines. It defines a unified interface and evaluation protocol to support a wide range of algorithms (e.g., classic sense-plan-act, RL, IL), visual observations (point cloud, RGBD), and controllers (e.g., action type and parameterization). Moreover, it empowers fast visual input learning algorithms so that a CNN-based policy can collect samples at about 2000 FPS with 1 GPU and 16 processes on a regular workstation. It implements a render server infrastructure to allow sharing rendering resources across all environments, thereby significantly reducing memory usage. We open-source all codes of our benchmark (simulator, environments, and baselines) and host an online challenge open to interdisciplinary researchers.\n\n\n![Figure 1: ManiSkill2 provides a unified, fast, and accessible system that encompasses well-curated manipulation tasks (e.g., stationary/mobile-base, single/dual-arm, rigid/soft-body).](images/f64f737450aaa31ead3f8bb5f4d37ef8fcb587e2b568293b26a918d26499249a.jpg)\n\n\n# 1 INTRODUCTION\n\n\nMastering human-like manipulation skills is a fundamental but challenging problem in Embodied AI, which is at the nexus of vision, learning, and robotics. Remarkably, once humans have learnt to manipulate a category of objects, they are able to manipulate unseen objects (e.g., with different\n\n\nappearances and geometries) of the same category in unseen configurations (e.g., initial poses). We refer such abilities to interact with a great variety of even unseen objects in different configurations as generalizable manipulation skills. Generalizable manipulation skills are one of the cornerstones of Embodied AI, which can be composed to tackle long-horizon and complex daily chores (Ahn et al., 2022; Gu et al., 2022). To foster further interdisciplinary and reproducible research on generalizable manipulation skills, it is crucial to build a versatile and public benchmark that focuses on object-level topological and geometric variations as well as practical manipulation challenges.\n\n\nHowever, most prior benchmarks are insufficient to support and evaluate progress in learning generalizable manipulation skills. In this work, we present ManiSkill2, the next generation of SAPIEN ManiSkill Benchmark (Mu et al., 2021), which extends upon fully simulated dynamics, a large variety of articulated objects, and large-scale demonstrations from the previous version. Moreover, we introduce significant improvements and novel functionalities, as shown below.\n\n\n1) A Unified Benchmark for Generic and Generalizable Manipulation Skills: There does not exist a standard benchmark to measure different algorithms for generic and generalizable manipulation skills. It is largely due to well-known challenges to build realistically simulated environments with diverse assets. Many benchmarks bypass critical challenges as a trade-off, by either adopting abstract grasp (Ehsani et al., 2021; Szot et al., 2021; Srivastava et al., 2022) or including few object-level variations (Zhu et al., 2020; Yu et al., 2020; James et al., 2020). Thus, researchers usually have to make extra efforts to customize environments due to limited functionalities, which in turn makes reproducible comparison difficult. For example, Shridhar et al. (2022) modified 18 tasks in James et al. (2020) to enable few variations in initial states. Besides, some benchmarks are biased towards a single type of manipulation, e.g., 4-DoF manipulation in Yu et al. (2020). To address such pain points, ManiSkill2 includes a total of 20 verified and challenging manipulation task families of multiple types (stationary/mobile-base, single/dual-arm, rigid/soft-body) with over 2000 objects and 4M demonstration frames, to support generic and generalizable manipulation skills. All the tasks are implemented in a unified OpenAI Gym (Brockman et al., 2016) interface with fully-simulated dynamic interaction, supporting multiple observation modes (point cloud, RGBD, privileged state) and multiple controllers. A unified protocol is defined to evaluate a wide range of algorithms (e.g., sense-plan-act, reinforcement and imitation learning) on both seen and unseen assets as well as configurations. In particular, we implement a cloud-based evaluation system to publicly and fairly compare different approaches.\n\n\n2) Real-time Soft-body Environments: When operating in the real world, robots face not only rigid bodies, but many types of soft bodies, such as cloth, water, and soil. Many simulators have supported robotic manipulation with soft body simulation. For example, MuJoCo (Todorov et al., 2012) and Bullet Coumans & Bai (2016-2021) use the finite element method (FEM) to enable the simulation of rope, cloth, and elastic objects. However, FEM-based methods cannot handle large deformation and topological changes, such as scooping flour or cutting dough. Other environments, like SoftGym (Lin et al., 2020) and ThreeDWorld (Gan et al., 2020), are based on Nvidia Flex, which can simulate large deformations, but cannot realistically simulate elasto-plastic material, e.g., clay. PlasticineLab (Huang et al., 2021) deploys the continuum-mechanics-based material point method (MPM), but it lacks the ability to couple with rigid robots, and its simulation and rendering performance have much room for improvement. We have implemented a custom GPU MPM simulator from scratch using Nvidia's Warp (Macklin, 2022) JIT framework and native CUDA for high efficiency and customizability. We further extend Warp's functionality to support more efficient host-device communication. Moreover, we have supported a 2-way dynamics coupling interface that enables any rigid-body simulation framework to interact with the soft bodies, allowing robots and assets in ManiSkill2 to interact with soft-body simulation seamlessly. To our knowledge, ManiSkill2 is the first embodied AI environment to support 2-way coupled rigid-MPM simulation, and also the first to support real-time simulation and rendering of MPM material.\n\n\n3) Multi-controller Support and Conversion of Demonstration Action Spaces: Controllers transform policies' action outputs into motor commands that actuate the robot, which define the action space of a task. Martin-Mart\u00edn et al. (2019); Zhu et al. (2020) show that the choice of action space has considerable effects on exploration, robustness and sim2real transferability of RL policies. For example, task-space controllers are widely used for typical pick-and-place tasks, but might be suboptimal compared to joint-space controllers when collision avoidance (Szot et al., 2021) is required. ManiSkill2 supports a wide variety of controllers, e.g., joint-space controllers for motion planning and task-space controllers for teleoperation. A flexible system is also implemented to\n\n\ncombine different controllers for different robot components. For instance, it is easy to specify a velocity controller for the base, a task-space position controller for the arm, and a joint-space position controller for the gripper. It differs from Zhu et al. (2020), which only supports setting a holistic controller for all the components. Most importantly, ManiSkill2 embraces a unique functionality to convert the action space of demonstrations to a desired one. It enables us to exploit large-scale demonstrations generated by any approach regardless of controllers.\n\n\n4) Fast Visual RL Experiment Support: Visual RL training demands millions of samples from interaction, which makes performance optimization an important aspect in environment design. Isaac Gym (Makoviychuk et al., 2021) implements a fully GPU-based vectorized simulator, but it lacks an efficient renderer. It also suffers from reduced usability (e.g., difficult to add diverse assets) and functionality (e.g., object contacts are inaccessible). EnvPool (Weng et al., 2022) batches environments by a thread pool to minimize synchronization and improve CPU utilization. Yet its environments need to be implemented in  $\\mathbf{C} + +$  , which hinders fast prototyping (e.g., customizing observations and rewards). As a good trade-off between efficiency and customizability, our environments are fully scripted in Python and vectorized by multiple processes. We implement an asynchronous RPC-based render server-client system to optimize throughput and reduce GPU memory usage. We manage to collect samples with an RGBD-input PPO policy at about 2000 FPS  $^1$  with 1 GPU and 16 CPU processors.\n\n\n# 2 BUILDING ENVIRONMENTS FOR GENERALIZABLE MANIPULATION SKILLS\n\n\nBuilding high-quality environments demands cross-disciplinary knowledge and expertise, including physical simulation, rendering, robotics, machine learning, software engineering, etc. Our workflow highlights a verification-driven iterative development process, which is illustrated in Appendix A.1. Different approaches, including task and motion planning (TAMP), model predictive control (MPC) and reinforcement learning (RL), can be used to generate demonstrations according to characteristics and difficulty of tasks, which verify environments as a byproduct.\n\n\n# 2.1 HETEROGENEOUS TASK FAMILIES\n\n\nManiSkill2 embraces a heterogeneous collection of 20 task families. A task family represents a family of task variants that share the same objective but are associated with different assets and initial states. For simplicity, we interchangeably use task short for task family. Distinct types of manipulation tasks are covered: rigid/soft-body, stationary/mobile-base, single/dual-arm. In this section, we briefly describe 4 groups of tasks. More details can be found in Appendix C.\n\n\nSoft-body Manipulation: ManiSkill2 implements 6 soft-body manipulation tasks that require agents to move or deform soft bodies into specified goal states through interaction.\n\n- 1) Fill: filling clay from a bucket into the target beaker;\n\n- 2) Hang: hanging a noodle on the target rod;\n\n- 3) Excavate: scooping up a specific amount of clay and lifting it to a target height;\n\n- 4) Pour: pouring water from a bottle into the target beaker. The final liquid level should match the red line on the beaker.\n\n- 5) Pinch: deforming plasticine from an initial shape into a target shape; target shapes are generated by randomly pinching initial shapes, and are given as RGBD images or point clouds from 4 views.\n\n- 6) Write: writing a target character on clay. Target characters are given as 2D depth maps.\n\n\nA key challenge of these tasks is to reason how actions influence soft bodies, e.g., estimating displacement quantity or deformations, which will be illustrated in Sec 5.2.\n\n\nPrecise Peg-in-hole Assembly: Peg-in-hole assembly is a representative robotic task involving rich contact. We include a curriculum of peg-in-hole assembly tasks that require an agent to place an object into its corresponding slot. Compared to other existing assembly tasks, ours come with two noticeable improvements. First, our tasks target high precision (small clearance) at the level of millimeters, as most day-to-day assembly tasks demand. Second, our tasks emphasize the contact-rich insertion process, instead of solely measuring position or rotation errors.\n\n\n1) PegInsertionSide: a single peg-in-hole assembly task inspired by MetaWorld (Yu et al., 2020). It involves an agent picking up a cuboid-shaped peg and inserting it into a hole with a clearance of\n\n\n3mm on the box. Our task is successful only if half of the peg is inserted, while the counterparts in prior works only require the peg head to approach the surface of the hole.\n\n- 2) PlugCharger: a dual peg-in-hole assembly task inspired by RLBench (James et al., 2020), which involves an agent picking up and plugging a charger into a vertical receptacle. Our assets (the charger and holes on the receptacle) are modeled with realistic sizes, allowing a clearance of  $0.5\\mathrm{mm}$ , while the counterparts in prior works only examine the proximity of the charger to a predefined position without modeling holes at all.\n\n- 3) AssemblingKits: inspired by Transporter Networks (Zeng et al., 2020), this task involves an agent picking up and inserting a shape into its corresponding slot on a board with 5 slots in total. We devise a programmatic way to carve slots on boards given any specified clearance (e.g.,  $0.8\\mathrm{mm}$ ), such that we can generate any number of physically-realistic boards with slots. Note that slots in environments of prior works are visual marks, and there are in fact no holes on boards.\n\n\nWe generate demonstrations for the above tasks through TAMP. This demonstrates that it is feasible to build precise peg-in-hole assembly tasks solvable in simulation environments, without abstractions from prior works.\n\n\nStationary 6-DoF Pick-and-place: 6-DoF pick-and-place is a widely studied topic in robotics. At the core is grasp pose Mahler et al. (2016); Qin et al. (2020); Sundermeyer et al. (2021). In ManiSkill2, we provide a curriculum of pick-and-place tasks, which all require an agent to pick up an object and move it to a goal specified as a 3D position. The diverse topology and geometric variations among objects call for generalizable grasp pose predictions.\n\n- 1) PickCube: picking up a cube and placing it at a specified goal position;\n\n- 2) StackCube: picking up a cube and placing it on top of another cube. Unlike PickCube, the goal placing position is not explicitly given; instead, it needs to be inferred from observations.\n\n- 3) PickSingleYCB: picking and placing an object from YCB (Calli et al., 2015);\n\n- 4) PickSingleEGAD: picking and placing an object from EGAD (Morrison et al., 2020);\n\n- 5) PickClutterYCB: The task is similar to PickSingleYCB, but multiple objects are present in a single scene. The target object is specified by a visible 3D point on its surface.\n\n\nOur pick-and-place tasks are deliberately designed to be challenging. For example, the goal position is randomly selected within a large workspace  $(30 \\times 50 \\times 50 cm^3)$ . It poses challenges to sense-plan-act pipelines that do not take kinematic constraints into consideration when scoring grasp poses, as certain high-quality grasp poses might not be achievable by the robot.\n\n\nMobile/Stationary Manipulation of Articulated Objects: We inherit four mobile manipulation tasks from ManiSkill1 (Mu et al., 2021), which are PushChair, MoveBucket, OpenCabinetDoor, and OpenCabinetDrawer. We also add a stationary manipulation task, TurnFaucet, which uses a stationary arm to turn on faucets of various geometries and topology (details in Appendix C.3).\n\n\nBesides the above tasks, we have one last task, AvoidObstacles, which tests the navigation ability of a stationary arm to avoid a dense collection of obstacles in space while actively sensing the scene.\n\n\n# 2.2 MULTI-CONTROLLER SUPPORT AND CONVERSION OF DEMONSTRATION ACTION SPACES\n\n\nThe selection of controllers determines the action space. ManiSkill2 supports multiple controllers, e.g., joint position, delta joint position, delta end-effector pose, etc. Unless otherwise specified, controllers in ManiSkill2 translate input actions, which are desired configurations (e.g., joint positions or end-effector pose), to joint torques that drive corresponding joint motors to achieve desired actions. For instance, input actions to joint position, delta joint position, and delta end-effector pose controllers are, respectively, desired absolute joint positions, desired joint positions relative to current joint positions, and desired  $\\mathbb{S}\\mathbb{E}(3)$  transformations relative to the current end-effector pose. See Appendix B for a full description of all supported controllers.\n\n\nDemonstrations are generated with one controller, with an associated action space. However, researchers may select an action space that conforms to a task but is different from the original one. Thus, to exploit large-scale demonstrations, it is crucial to convert the original action space to many different target action spaces while reproducing the kinematic and dynamic processes in demonstrations. Let us consider a pair of environments: a source environment with a joint position controller used to generate demonstrations through TAMP, and a target environment with a delta end-effector pose controller for Imitation / Reinforcement Learning applications. The objective is to convert the source action  $a_{\\mathrm{src}}(t)$  at each timestep  $t$  to the target action  $a_{\\mathrm{tgt}}(t)$ . By definition, the\n\n\ntarget action (delta end-effector pose) is  $a_{\\mathrm{tgt}}(t) = \\bar{T}_{\\mathrm{tgt}}(t) \\cdot T_{\\mathrm{tgt}}^{-1}(t)$ , where  $\\bar{T}_{\\mathrm{tgt}}(t)$  and  $T_{\\mathrm{tgt}}(t)$  are respectively desired and current end-effector poses in the target environment. To achieve the same dynamic process in the source environment, we need to match  $\\bar{T}_{\\mathrm{tgt}}(t)$  with  $\\bar{T}_{\\mathrm{src}}(t)$ , where  $\\bar{T}_{\\mathrm{src}}(t)$  is the desired end-effector pose in the source environment.  $\\bar{T}_{\\mathrm{src}}(t)$  can be computed from the desired joint positions  $(a_{\\mathrm{src}}(t)$  in this example) through forward kinematics  $(FK(\\cdot))$ . Thus, we have\n\n\n$$\n\n$$\na _ {\\mathrm {t g t}} (t) = \\bar {T} _ {\\mathrm {t g t}} (t) \\cdot T _ {\\mathrm {t g t}} ^ {- 1} (t) = \\bar {T} _ {\\mathrm {s r c}} (t) \\cdot T _ {\\mathrm {t g t}} ^ {- 1} (t) = F K (\\bar {a} _ {\\mathrm {s r c}} (t)) \\cdot T _ {\\mathrm {t g t}} ^ {- 1} (t)\n$$\n\n$$\n\n\nNote that our method is closed-loop, as we instantiate a target environment to acquire  $T_{\\mathrm{tgt}}^{-1}(t)$ . For comparison, an open-loop method would use  $T_{\\mathrm{src}}^{-1}(t)$  and suffers from accumulated execution errors.\n\n\n# 3 REAL-TIME SOFT BODY SIMULATION AND-RENDERING\n\n\nIn this section, we describe our new physical simulator for soft bodies and their dynamic interactions with the existing rigid-body simulation. Our key contributions include: 1) a highly efficient GPU MPM simulator; 2) an effective 2-way dynamic coupling method to support interactions between our soft-body simulator and any rigid-body simulator, in our case, SAPIEN. These features enable us to create the first real-time MPM-based soft-body manipulation environment with 2-way coupling.\n\n\nRigid-soft Coupling: Our MPM solver is MLS-MPM (Hu et al., 2018) similar to PlasticineLab (Huang et al., 2021), but with a different contact modeling approach, which enables 2-way coupling with external rigid-body simulators. The coupling works by transferring rigid-body poses to the soft-body simulator and transferring soft-body forces to the rigid-body simulator. At the beginning of the simulation, all collision shapes in the external rigid-body simulator are copied to the soft-body simulator. Primitive shapes (box, capsule, etc.) are represented by analytical signed distance functions (SDFs); meshes are converted to SDF volumes, stored as 3D CUDA textures. After each rigid-body simulation step, we copy the poses and velocities of the rigid bodies to the soft-body simulator. During soft-body simulation steps, we evaluate the SDF functions at MPM particle positions and apply penalty forces to the particles, accumulating forces and torques for rigid bodies. In contrast, PlasticineLab evaluates SDF functions on MPM grid nodes, and applies forces to MPM grids. Our simulator supports both methods, but we find applying particle forces produces fewer artifacts such as penetration. After soft-body steps, we read the accumulated forces and torques from the soft-body simulator and apply them to the external rigid-body simulator. This procedure is summarized in Appendix D.1. Despite being a simple 2-way coupling method, it is very flexible, allowing coupling with any rigid-body simulator, so we can introduce anything from a rigid-body simulator into the soft-body environment, including robots and controllers.\n\n\nPerformance Optimization: The performance of our soft-body simulator is optimized in 4 aspects. First, the simulator is implemented in Warp, Nvidia's JIT framework to translate Python code to native C++ and CUDA. Therefore, our simulator enjoys performance comparable to C and CUDA. Second, we have optimized the data transfer (e.g., poses and forces in the 2-way coupling) between CPU and GPU by further extending and optimizing the Warp framework; such data transfers are performance bottlenecks in other JIT frameworks such as Taichi (Hu et al., 2019), on which PlasticineLab is based. Third, our environments have much shorter compilation time and startup time compared to PlasticineLab thanks to the proper use of Warp compiler and caching mechanisms. Finally, since the simulator is designed for visual learning environments, we also implement a fast surface rendering algorithm for MPM particles, detailed in Appendix D.2. Detailed simulation parameters and performance are provided in Appendix D.3.\n\n\n# 4 PARALLELIZING PHYSICAL SIMULATION AND-RENDERING\n\n\nManiSkill2 aims to be a general and user-friendly framework, with low barriers for customization and minimal limitations. Therefore, we choose Python as our scripting language to model environments, and the open-source, highly flexible SAPIEN (Xiang et al., 2020) as our physical simulator. The Python language determines that the only way to effectively parallelize execution is through multi-process, which is integrated in common visual RL training libraries (Raffin et al., 2021; Weng et al., 2021). Under the multi-process environment paradigm, we make engineering efforts to enable our environments to surpass previous frameworks by increased throughput and reduced GPU memory usage.\n\n\n![(a)](images/dd6b2d38ca309fab924cf0752a642f11c3e3dd8df65022c201d0a98c2fd645d7.jpg)\n\n\n![(b) Figure 2: Two pipelines for visual RL sample collection. (a) Sequential pipeline. (b) Our pipeline with asynchronous rendering and render server improves CPU utilization, reduces data transfer, and saves memory.](images/44d55175b2f6f501e6d464ca19b815b37484af073610b2567c71bf5358ebb9b3.jpg)\n\n\nWhat are the Goals in Visual-Learning Environment Optimization? The main goal of performance optimization in a visual-learning environment is to maximize total throughput: the number of samples collected from all (parallel) environments, measured in steps (frames) per second. Another goal is to reduce GPU memory usage. Typical Python-based multi-process environments are wasteful in GPU memory usage, since each process has to maintain a full copy of GPU resources, e.g., meshes and textures. Given a fixed GPU memory budget, less GPU memory spent on rendering resources means more memory can be allocated for neural networks.\n\n\nAsynchronous Rendering and Server-based Renderer: Fig. 2(a) illustrates a typical pipeline (sequential simulation and rendering) to collect samples from multi-process environments. It includes the following stages: (1) do physical simulation on worker processes; (2) take pictures (update renderer GPU states and submit draw calls); (3) wait for GPU render to finish; (4) copy image observations to CPU; (5) compute rewards and get other observations (e.g., robot proprioceptive info); (6) copy images to the main python process and synchronize; (7) copy these images to GPU for policy learning; (8) forward the policy network on GPU; (9) copy output actions from the GPU to the simulation worker processes.\n\n\nWe observe that the CPU is idling during GPU rendering (stage 3), while reward computation (stage 5) often does not rely on rendering results. Thus, we can increase CPU utilization by starting reward computation immediately after stage 2. We refer to this technique as asynchronous rendering.\n\n\nAnother observation is that images are copied from GPU to CPU on each process, passed to the main python process, and uploaded to the GPU again. It would be ideal if we can keep the data on GPU at all times. Our solution is to use a render server, which starts a thread pool on the main Python process and executes rendering requests from simulation worker processes, summarized in figure 2(b). The render server eliminates GPU-CPU-GPU image copies, thereby reducing data transfer time. It allows GPU resources to share across any number of environments, thereby significantly reducing memory usage. It enables communication over network, thereby having the potential to simulate and render on multiple machines. It requires minimal API changes \u2013 the only change to the original code base is to receive images from the render server. It also has additional benefits in software development with Nvidia GPU, which we detail in Appendix E.2.\n\n\nComparison: We compare the throughput of ManiSkill2 with 3 other frameworks that support visual RL: Habitat 2.0 (Szot et al., 2021), RoboSuite 1.3 (Zhu et al., 2020), and Isaac Gym (Makoviychuk et al., 2021). We build a PickCube environment in all simulators. We use similar physical simulation parameters (500Hz simulation frequency and 20Hz control frequency\u00b2), and we use the GPU pipeline for rendering. Images have resolution 128x128. All frameworks are given a computation budget of 16 CPU cores (logical processors) of an Intel i9-9960X CPU with 128G memory and 1 RTX Titan GPU with 24G memory. We test with random actions and also with a randomly initialized nature CNN (Mnih et al., 2015) as policy network. We test all frameworks on 16, 32, 64, 128, 256, 512 parallel environments and report the highest performance. Results are shown in Table 1(a) (more\n\n\n<table><tr><td></td><td>ManiSkill2 Server</td><td>ManiSkill2 Sync</td><td>Habitat 2.0</td><td>RoboSuite 1.3</td><td>Isaac Gym</td></tr><tr><td>Total FPS (rand. action)</td><td>2344\u00b168</td><td>942\u00b119</td><td>1275\u00b110</td><td>924\u00b13</td><td>865\u00b135</td></tr><tr><td>Total FPS (nature CNN)</td><td>2181\u00b175</td><td>931\u00b14</td><td>1224\u00b113</td><td>894\u00b115</td><td>835\u00b15</td></tr><tr><td>Optimal #Envs</td><td>64</td><td>32</td><td>64</td><td>32</td><td>512</td></tr></table>\n\n\n(a)\n\n\n<table><tr><td>#Envs</td><td>ManiSkill2 Server</td><td>Habitat 2.0</td></tr><tr><td>4</td><td>4.9G</td><td>6.4G</td></tr><tr><td>8</td><td>5.1G</td><td>12.9G</td></tr><tr><td>64</td><td>5.8G</td><td>(OOM)</td></tr></table>\n\n\n(b)\n\n\nTable 1: (a) Comparison of sample collection speed (FPS) on PickCube across different frameworks. (b) GPU memory usage on multi-process environments with 74 YCB objects each.\n\n\ndetails in Appendix E.3). An interesting observation is that our environment performs the best when the number of parallel environments exceeds the number of CPU cores. We conjecture that this is because execution on CPU and GPU are naturally efficiently interleaved through OS or driver-level schedulers when the requested computation far exceeds the available resources.\n\n\nAdditionally, we demonstrate the advantage of memory sharing thanks to our render server. We extend the PickClutterYCB environment to include 74 YCB objects per scene, and create the same setting in Habitat 2.0. As shown in Table 1(b), even though we enable GPU texture compression for Habitat and use regular textures for ManiSkill2, the memory usage of Habitat grows linearly as the number of environments increases, while ManiSkill2 requires very little memory to create additional environments since all meshes and textures are shared across environments.\n\n\n# 5 APPLICATIONS\n\n\nIn this section, we show how ManiSkill2 supports widespread applications, including sense-plan-act frameworks and imitation / reinforcement learning. In this section, for tasks that have asset variations, we report results on training objects. Results on held-out objects are presented in Appendix F. Besides, we demonstrate that policies trained in ManiSkill2 have the potential to be directly deployed in the real world.\n\n\n# 5.1 SENSE-PLAN-ACT\n\n\nSense-plan-act (SPA) is a classical framework in robotics. Typically, a SPA method first leverages perception algorithms to build a world model from observations, then plans a sequence of actions through motion planning, and finally execute the plan. However, SPA methods are usually open-loop and limited by motion planning, since perception and planning are independently optimized. In this section, we experiment with two perception algorithms, Contact-GraspNet (Sundermeyer et al., 2021) and Transporter Networks (Zeng et al., 2020).\n\n\nContact-GraspNet for PickSingleYCB: The SPA solution for PickSingleYCB is implemented as follows. First, we use Contact-GraspNet (CGN) to predict potential grasp poses along with confidence scores given the observed partial point cloud. We use the released CGN model pretrained on ACRONYM (Eppner et al., 2021). Next, we start with the predicted grasp pose with the highest score, and try to generate a plan to achieve it by motion planning. If no plan is found, the grasp pose with the next highest score is attempted until a valid plan is found. Then, the agent executes the plan to reach the desired grasp pose and closes its grippers to grasp the object. Finally, another plan is generated and executed to move the gripper to the goal position.\n\n\nFor each of the 74 objects, we conduct 5 trials (different initial states). The task succeeds if the object's center of mass is within  $2.5\\mathrm{cm}$  of the goal. The success rate is  $43.24\\%$ . There are two main failure modes: 1) predicted poses are of low confidence ( $27.03\\%$  of trials), especially for objects (e.g., spoon and fork) that are small and thin; 2) predicted poses are of low grasp quality or unreachable ( $29.73\\%$  of trials), but with high confidence. See Appendix F.1 for examples.\n\n\nTransporter Network for AssemblingKits: We benchmark Transporter Networks (TPN) on our AssemblingKits. The original success metric (pose accuracy) used in TPN is whether the peg is placed within 1cm and 15 degrees of the goal pose. Note that our version requires pieces to actually fit into holes, and thus our success metric is much stricter. We train TPN from scratch with image data sampled from training configurations using two cameras, a base camera and a hand camera. To\n\n\n<table><tr><td>Obs. Mode</td><td>PickCube</td><td>StackCube</td><td>Fill</td><td>Hang</td><td>Excavate</td><td>Pour</td><td>Pinch</td><td>Write</td></tr><tr><td>Point Cloud</td><td>0.22 \u00b1 0.06</td><td>0.04 \u00b1 0.02</td><td>0.45 \u00b1 0.04</td><td>0.35 \u00b1 0.15</td><td>0.08 \u00b1 0.04</td><td>0.02 \u00b1 0.02</td><td>0.00 \u00b1 0.00</td><td>0.00 \u00b1 0.00</td></tr><tr><td>RGBD</td><td>0.01 \u00b1 0.02</td><td>0.00 \u00b1 0.00</td><td>0.62 \u00b1 0.07</td><td>0.20 \u00b1 0.12</td><td>0.21 \u00b1 0.04</td><td>0.00 \u00b1 0.00</td><td>0.00 \u00b1 0.00</td><td>0.00 \u00b1 0.00</td></tr></table>\n\n\nTable 2: Mean and standard deviation of the success rate of behavior cloning on rigid-body and soft-body tasks. For rigid-body assembly tasks not shown in the table, success rate is 0.\n\n\n<table><tr><td>Obs. Mode</td><td>PickCube</td><td>StackCube</td><td>PickSingleYCB</td><td>PegInsSide</td><td>PlugCharger</td><td>AssemblingKits</td><td>TurnFaucet</td><td>AvoidObstacles</td></tr><tr><td>Point Cloud</td><td>0.94 \u00b1 0.03</td><td>0.95 \u00b1 0.02</td><td>0.51 \u00b1 0.05</td><td>0.01 \u00b1 0.01</td><td>0.01 \u00b1 0.02</td><td>0.00 \u00b1 0.00</td><td>0.04 \u00b1 0.03</td><td>0.00 \u00b1 0.00</td></tr><tr><td>RGBD</td><td>0.91 \u00b1 0.05</td><td>0.87 \u00b1 0.04</td><td>0.18 \u00b1 0.07</td><td>0.01 \u00b1 0.01</td><td>0.01 \u00b1 0.01</td><td>0.00 \u00b1 0.00</td><td>0.03 \u00b1 0.03</td><td>0.00 \u00b1 0.00</td></tr></table>\n\n\nTable 3: Mean and standard deviation of success rates of DAPG+PPO on rigid-body tasks. Training budget is 25M time steps.\n\n\naddress our high-precision success criterion, we increase the number of bins for rotation prediction from 36 in the original work to 144. During evaluation, we employ motion planning to move the gripper to the predicted pick position, grasp the peg, then generate another plan to move the peg to the predicted goal pose and drop it into the hole. The success rate over 100 trials is  $18\\%$ , and the pose accuracy is  $96\\%$ . See Appendix F.2 for more details.\n\n\n# 5.2 IMITATION & REINFORCEMENT LEARNING WITH DEMONSTRATIONS\n\n\nFor the following experiments, unless specified otherwise, we use the delta end-effector pose controller for rigid-body environments and the delta joint position controller for soft-body environments, and we translate demonstrations accordingly using the approach in Sec.2.2. Visual observations are captured from a base camera and a hand camera. For RGBD-based agents, we use IMPALA (Espeholt et al., 2018) as the visual backbone. For point cloud-based agents, we use PointNet (Qi et al., 2017) as the visual backbone. In addition, we transform input point clouds into the end-effector frame, and for environments where goal positions are given (PickCube and PickSingleYCB), we randomly sample 50 green points around the goal position to serve as visual cues and concatenate them to the input point cloud. We run 5 trials for each experiment and report the mean and standard deviation of success rates. Further details are presented in Appendix F.3.\n\n\nImitation Learning: We benchmark imitation learning (IL) with behavior cloning (BC) on our rigid and soft-body tasks. All models are trained for 50k gradient steps with batch size 256 and evaluated on test configurations.\n\n\nResults are shown in Table 2. For rigid-body tasks, we observe low or zero success rates. This suggests that BC is insufficient to tackle many crucial challenges from our benchmark, such as precise control in assembly and large asset variations. For soft-body tasks, we observe that it is difficult for BC agents to precisely estimate action influence on soft body properties (e.g. displacement quantity or deformation). Specifically, agents perform poorly on Excavate and Pour, as Excavate is successful only if a specified amount of clay is scooped up and Pour is successful when the final liquid level accurately matches a target line. On the other hand, for Fill and Hang, which do not have such precision requirements (for Fill, the task is successful as long as the amount of particles in the beaker is larger than a threshold), the success rates are much higher. In addition, we observe that BC agents cannot well utilize target shapes to guide precise soft body deformation, as they are never successful on Pinch or Write. See Appendix F.5 for further analysis.\n\n\nRL with Demonstrations: We benchmark demonstration-based online reinforcement learning by augmenting Proximal Policy Gradient (PPO) (Schulman et al., 2017) objective with the demonstration objective from Demonstration-Augmented Policy Gradient (DAPG) (Rajeswaran et al., 2017). Our implementation is similar to Jia et al. (2022), and further details are presented in Appendix F.3. We train all agents from scratch for 25 million time steps. Due to limited computation resources, we only report results on rigid-body environments.\n\n\nResults are shown in Table 3. We observe that for pick-and-place tasks, point cloud-based agents perform significantly better than RGBD-based agents. Notably, on PickSingleYCB, the success rate is even higher than Contact-GraspNet with motion planning. This demonstrates the potential\n\n\n![Simulation Figure 3: Left: Simulation and real world setup for PickCube. Right: Motion planning execution results for Pinch in simulation and in the real world: (a) initial state; (b) letter \"M\"; (c) letter \"S\".](images/7a07dbb345a60a7cd55c39228a734b046eb1a726de9e8a856e922795455ad67a.jpg)\n\n\n![Real](images/c7c36b92349b7fcb1c50eb60066566380aa6891289bd19adc17eb6f2ce3c2d47.jpg)\n\n\n![(a)](images/0ee92fed74707b8816abf16c98a113ab2867391fc389cc3e1ce569d446117c87.jpg)\n\n\n![(b)](images/e0101b5681f62e463b89a22041706d65691901961d9fdbe112aa9b70c4d0531e.jpg)\n\n\n![(c)](images/957abea9fabebd9f7d8d612939e6f85b9cd05a8ed7386dc3b7ffb129e88ebbce.jpg)\n\n\nof obtaining a single agent capable of performing general pick-and-place across diverse object geometries through online training. We also further examine factors that influence point cloud-based manipulation learning in Appendix F.6. However, for all other tasks, notably the assembly tasks that require high precision, the success rates are near zero. In Appendix F.7, we show that if we increase the clearance of the assembly tasks and decrease their difficulty, agents can achieve much higher performance. This suggests that existing RL algorithms might have been insufficient yet to perform highly precise controls, and our benchmark poses meaningful challenges for the community.\n\n\nIn addition, we examine the influence of controllers for agent learning, and we perform ablation experiments on PickSingleYCB using point cloud-based agents. When we replace the delta end-effector pose controller in the above experiments with the delta joint position controller, the success rate falls to  $0.22 \\pm 0.18$ . The profound impact of controllers demonstrates the necessity and significance of our multi-controller conversion system.\n\n\n# 5.3 SIM2REAL\n\n\nManiSkill2 features fully-simulated dynamic interaction for rigid-body and soft-body. Thus, policies trained in ManiSkill2 have the potential to be directly deployed in the real world.\n\n\nPickCube: We train a visual RL policy on PickCube and evaluate its transferability to the real world. The setup is shown in Fig. 3-L, which consists of an Intel RealSense D415 depth sensor, a 7DoF ROKAE xMate3Pro robot arm, a Robotiq 2F-140 2-finger gripper, and the target cube. We first acquire the intrinsic parameters for the real depth sensor and its pose relative to the robot base. We then build a simulation environment aligned with the real environment. We train a point cloud-based policy for 10M time steps, where the policy input consists of robot proprioceptive information (joint position and end-effector pose) and uncolored point cloud backprojected from the depth map. The success rate in simulation is  $91.0\\%$ . We finally directly evaluate this policy in the real world 50 times with different initial states, where we obtain  $60.0\\%$  success rate. We conjecture that the performance drop is caused by the domain gap in depth maps, as we only used Gaussian noise and random pixel dropout as data augmentation during simulation policy training.\n\n\nPinch: We further evaluate the fidelity of our soft-body simulation by executing the same action sequence generated by motion planning in simulation and in the real world and comparing the final plasticine shapes. Results are shown in Fig. 3-R, which demonstrates that our 2-way coupled rigid-MPM simulation is able to reasonably simulate plasticine's deformation caused by multiple grasps.\n\n\n# 6 CONCLUSION\n\n\nTo summarize, ManiSkill2 is a unified and accessible benchmark for generic and generalizable manipulation skills, providing 20 manipulation task families, over 2000 objects, and over 4M demonstration frames. It features highly efficient rigid-body simulation and rendering system for sample collection to train RL agents, real-time MPM-based soft-body environments, and versatile multi-controller conversion support. We have demonstrated its applications in benchmarking sense-plan-act and imitation / reinforcement learning algorithms, and we show that learned policies on ManiSkill2 have the potential to transfer to the real world.\n\n\n# REFERENCES\n\n- Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.\n\n- Kent Beck, Mike Beedle, Arie Van Bennekum, Alistair Cockburn, Ward Cunningham, Martin Fowler, James Grenning, Jim Highsmith, Andrew Hunt, Ron Jeffries, et al. Manifesto for agile software development, 2001.\n\n- Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n\n- Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 international conference on advanced robotics (ICAR), pp. 510-517. IEEE, 2015.\n\n- Hilko Cords and Oliver G. Staadt. Instant liquids: Fast screen-space surface generation for particle-based liquids. In Poster proceedings of ACM Siggraph/Eurographics Symposium on Computer Animation, 2008.\n\n- Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016-2021.\n\n- Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: A framework for visual object manipulation. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4495-4504, 2021.\n\n- Clemens Eppner, Arsalan Mousavian, and Dieter Fox. Acronym: A large-scale grasp dataset based on simulation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 6222-6227. IEEE, 2021.\n\n- Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pp. 1407-1416. PMLR, 2018.\n\n- Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, et al. Threadworld: A platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020.\n\n- Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel LK Yamins, James J DiCarlo, Josh McDermott, Antonio Torralba, et al. The threadworld transport challenge: A visually guided task-and-motion planning benchmark for physically realistic embodied ai. arXiv preprint arXiv:2103.14025, 2021.\n\n- Jiayuan Gu, Devendra Singh Chaplot, Hao Su, and Jitendra Malik. Multi-skill mobile manipulation for object rearrangement. arXiv preprint arXiv:2209.02778, 2022.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Yuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin Zhu, Andre Pradhana, and Chenfanfu Jiang. A moving least squares material point method with displacement discontinuity and two-way rigid body coupling. ACM Transactions on Graphics (TOG), 37(4):150, 2018.\n\n- Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, and Fredo Durand. Taichi: a language for high-performance computation on spatially sparse data structures. ACM Transactions on Graphics (TOG), 38(6):1-16, 2019.\n\n- Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B Tenenbaum, and Chuang Gan. Plasticinelab: A soft-body manipulation benchmark with differentiable physics. arXiv preprint arXiv:2104.03311, 2021.\n\n- Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019-3026, 2020.\n\n- Zhiwei Jia, Xuanlin Li, Zhan Ling, Shuang Liu, Yiran Wu, and Hao Su. Improving policy optimization with generalist-specialist learning. In International Conference on Machine Learning, pp. 10104-10119. PMLR, 2022.\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\n\n- Chengshu Li, Cem Gokmen, Gabrael Levine, Roberto Martin-Martin, Sanjana Srivastava, Chen Wang, Josiah Wong, Ruohan Zhang, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In 6th Annual Conference on Robot Learning.\n\n- Xingyu Lin, Yufei Wang, Jake Olkin, and David Held. Softgym: Benchmarking deep reinforcement learning for deformable object manipulation. In Conference on Robot Learning, 2020.\n\n- Minghua Liu, Xuanlin Li, Zhan Ling, Yangyan Li, and Hao Su. Frame mining: a free lunch for learning robotic manipulation from 3d point clouds. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=d-JYso87y6s.\n\n- Miles Macklin. Warp: A high-performance python framework forgpu simulation and graphics. https://github.com/nvidia/warp, March 2022. NVIDIA GPU Technology Conference (GTC).\n\n- Jeffrey Mahler, Florian T Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry, Kai Kohlhoff, Torsten Kr\u00f6ger, James Kuffner, and Ken Goldberg. Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards. In IEEE International Conference on Robotics and Automation (ICRA), pp. 1957\u20131964. IEEE, 2016.\n\n- Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performancegpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021.\n\n- Roberto Martin-Martin, Michelle A Lee, Rachel Gardner, Silvio Savarese, Jeannette Bohg, and Animesh Garg. Variable impedance control in end-effector space: An action space for reinforcement learning in contact-rich tasks. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1010-1017. IEEE, 2019.\n\n- Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529-533, 2015.\n\n- Douglas Morrison, Peter Corke, and J\u00fcrgen Leitner. Egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation. IEEE Robotics and Automation Letters, 5 (3):4368-4375, 2020.\n\n- Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Cathera Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\n- Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652-660, 2017.\n\n- Yuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu, and Hao Su. S4g: Amodal single-view single-shot se (3) grasp detection in cluttered scenes. In Conference on robot learning, pp. 53-65. PMLR, 2020.\n\n- Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1-8, 2021. URL http://jmlr.org/papers/v22/20-1364.html.\n\n- Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.\n\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\n- Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for robotic manipulation. arXiv preprint arXiv:2209.05451, 2022.\n\n- Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00edn-Mart\u00edn, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on Robot Learning, pp. 477-490. PMLR, 2022.\n\n- Martin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, and Dieter Fox. Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 13438-13444. IEEE, 2021.\n\n- Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. Habitat 2.0: Training home assistants to rearrange their habitat. Advances in Neural Information Processing Systems, 34:251-266, 2021.\n\n- Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.\n\n- Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. arXiv preprint arXiv:2107.14171, 2021.\n\n- Jiayi Weng, Min Lin, Shengyi Huang, Bo Liu, Denys Makoviichuk, Viktor Makoviychuk, Zichen Liu, Yufan Song, Ting Luo, Yukun Jiang, et al. Envpool: A highly parallel reinforcement learning environment execution engine. arXiv preprint arXiv:2206.10558, 2022.\n\n- Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11097-11107, 2020.\n\n- Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 1094-1100. PMLR, 2020.\n\n- Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and Johnny Lee. Transporter networks: Rearranging the visual world for robotic manipulation. Conference on Robot Learning (CoRL), 2020.\n\n- Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Martin-Martin. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.\n\nResponse: ### Prediction: \\(\\boxed{Accept}\\)\n\n#### Justification:\n\n**Originality:**\n- **Novelty:** The paper introduces a comprehensive benchmark called ManiSkill2, which addresses several shortcomings of existing benchmarks, particularly in terms of realism, task variety, and computational efficiency. The inclusion of soft-body simulations and multi-controller support are notable innovations.\n- **Significance:** By addressing the limitations of current benchmarks, ManiSkill2 offers a valuable tool for advancing research in generalizable manipulation skills, which is a critical area in Embodied AI.\n- **Feasibility:** While the novelty involves advanced technologies like MPM simulations and real-time rendering, these are well-supported by recent advances in hardware and software, making the proposal technically feasible.\n\n**Methodological Rigor:**\n- **Evaluation Protocol:** The paper defines a rigorous evaluation protocol that supports a wide range of algorithms and observation modalities. This ensures that the benchmark is thorough and fair.\n- **Demonstration Handling:** The method for converting demonstrations to different action spaces is described clearly, ensuring reproducibility and comparability of results across different research communities.\n- **Simulation Performance:** The authors provide detailed descriptions of their physical simulation and rendering systems, demonstrating that ManiSkill2 is well-equipped to support the computational demands of visual learning environments.\n\n**Significance of Contribution:**\n- **Impact:** The benchmark is expected to have a substantial impact on the field by providing a realistic and versatile environment for testing and validating algorithms aimed at generalizable manipulation skills. This can lead to advancements in areas such as reinforcement learning, imitation learning, and sense-plan-act pipelines.\n- **Reproducibility:** By providing a comprehensive and well-documented benchmark, researchers can easily replicate and build upon the work, fostering a collaborative research environment.\n\n**Clarity of Presentation:**\n- **Structure:** The paper is well-organized, presenting a clear introduction, methodology, and experimental results. Each section builds logically onto the previous one, making it easy to follow the progression of ideas.\n- **Detail:** The technical details are presented in sufficient detail to allow readers to understand the underlying concepts and methodologies, without overwhelming the reader with excessive complexity.\n\n**Alignment with Journal Scope:**\n- **Relevance:** The journal ICLR is focused on machine learning, specifically reinforcement learning and related topics. ManiSkill2, which includes a wide range of reinforcement learning experiments, aligns well with ICLR\u2019s scope.\n- **Reproducibility:** The authors have made their benchmark openly available, including all necessary code and data. This ensures that the research can be easily replicated and built upon by others.\n\nOverall, ManiSkill2 presents a significant and innovative contribution to the field of Embodied AI and reinforcement learning. The paper is well-written, clearly lays out the technical details, and demonstrates the feasibility and value of the proposed benchmark. Therefore, it is likely to be accepted for publication in ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634771.1806147", "msecs": "180.0", "relativeCreated": "116004.08887863159", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Relevance-Based Embeddings for Efficient Relevance Retrieval\n\n# Abstract\nIn many machine learning applications, the most relevant items for a particular query should be efficiently extracted. The relevance function is usually an expensive similarity model making the exhaustive search infeasible. A typical solution to this problem is to train another model that separately embeds queries and items to a vector space, where similarity is defined via the dot product or cosine similarity. This allows one to search the most relevant objects through fast approximate nearest neighbors search at the cost of some reduction in quality. To compensate for this reduction, the found candidates are re-ranked by the expensive similarity model. In this paper, we investigate an alternative approach that utilizes the relevances of the expensive model to make relevance-based embeddings (RBE). The idea is to describe each query (item) by its relevance for a set of support items (queries) and use these new representations to obtain query (item) embeddings. We theoretically prove that relevance-based embeddings are powerful enough to approximate any complex similarity model (under mild conditions). An important ingredient of RBE is the choice of support items. We investigate several strategies and demonstrate that significant improvements can be obtained compared to random choice. Our experiments on diverse datasets illustrate the power of relevance-based embeddings.\n\n\n# 1 INTRODUCTION\n\n\nFinding the most relevant element (item)  $i$  to a query  $q$  among a large set of candidates  $I$  is a key task for a wide range of machine learning problems, for example, information retrieval, recommender systems, question-answering systems, or search engines. In such problems, the final score (relevance) is often predicted by a pairwise function  $R: I \\times Q \\to \\mathbb{R}$ , where  $Q$  is a query space and  $R$  approximates some ground truth relevance such as click probability, time spent or something else. Depending on the task, the relevance function  $R$  can utilize query attributes (e.g., the text of the query or a set of numerical features describing the user, such as age, time spent on the service, etc.), item attributes, or attributes describing the query-item pair (e.g., statistics based on counts of each query term in the document in information retrieval tasks).\n\n\nThe problem of relevance retrieval for a query  $q$  can be written as  $\\arg \\max_{i\\in I}R(i,q)$ . For practical applications, it is usually required to return not one but  $K$  best items (for directly displaying to the user or further re-ranking). Most recommender systems are characterized by a large size of the item space  $I$  (millions to hundreds of millions), so an exhaustive search is not feasible. This problem is often solved by training an auxiliary model  $\\tilde{R}$ , called a Siamese, two-tower, or dual encoder (DE), in which late binding is used:  $\\tilde{R}(i,q) = S(F_I(i),F_Q(q))$ , where  $F_I:I\\to \\mathbb{R}^d$ ,  $F_Q:Q\\to \\mathbb{R}^d$ , and  $S$  is some lightweight similarity measure, usually dot product or cosine similarity.\n\n\nWhile a lot of effort has been put into developing dual-encoder models, the cross-encoder (CE) ones are generally more powerful (Wu et al., 2019; Yadav et al., 2022). Moreover, as mentioned above, in practice it is typical to also have features that describe a query-item pair: e.g., counts of query terms in the document (information retrieval), information about previous user-item interactions (recommender systems), and so on. Such features cannot be used by dual encoders.\n\n\nIn a recent paper, Yadav et al. (2022) suggested an alternative approach: to approximate the relevance of a given query to all the items using the relevance of this query to a fixed set of randomly chosen support items. In more detail, the authors apply the matrix factorization to the query-item relevance\n\n\nmatrix to represent it as a product of its submatrix containing only a few columns (relevances to support items) and some other, explicitly computable.\n\n\nMotivated by the above-mentioned work, we propose and analyze the concept of relevance-based embeddings (RBE). The main idea is to describe users by their relevance to some pre-selected support items and describe items by their relevance to some support users. Then, such representations can be used in various ways: as in Yadav et al. (2022), they can be multiplied by a certain matrix to obtain relevance approximations, or they can be passed into a neural network to potentially obtain better approximations (trained for a desired loss), or they can be additionally combined with the original node features to get even more powerful embeddings.\n\n\nWe theoretically prove the power of relevance-based embeddings. Namely, we show that (under mild conditions) they are informative enough to approximate any continuous similarity function. In particular, when the similarity function utilizes pairwise features, dual encoders based on the individual user and item features inevitably lose this information; however, relevance-based representations contain this information and thus can approximate the desired function.\n\n\nFrom a practical perspective, an important aspect of our approach is how to properly choose support elements. Previous studies (Morozov & Babenko, 2019; Yadav et al., 2022) sampled them uniformly at random, while we show that there is significant room for improvement. We investigate different options: from simple heuristics (e.g., popular or diverse elements) to methods directly optimizing the relevance approximation quality. Surprisingly, even very simple strategies like clustering the elements and choosing the cluster centers as support items already give significant improvements that can be further improved by more advanced and theoretically justified strategies.\n\n\nTo evaluate the performance of RBE, we conduct experiments on textual and recommendation datasets. We compare our approach with dual encoders and with Yadav et al. (2022) and get an average improvement of  $33\\%$  over this baseline for various datasets (from  $8\\%$  to  $69\\%$ , see Table 2).\n\n\n# 2 RELATED WORK\n\n\nIn this section, we discuss research areas and representative papers related to our study.\n\n\nRelevance retrieval problem is widespread in the context of building information retrieval systems (Kowalski, 2007), such as text search engines (Huang et al., 2013), image search (Gordo et al., 2016), entertainment recommender systems (Covington et al., 2016), question answering systems (Karpukhin et al., 2020), e-commerce systems (Yu et al., 2018), and other practical applications.\n\n\nUsually, such problems are solved by learning query and item embeddings into a certain space and then searching for approximate nearest elements in this space, followed by rearrangement using a heavier ranker. In particular, Covington et al. (2016); Huang et al. (2013) explicitly use this approach, offering two-tower models (a.k.a. dual encoders). Note that there are simple alternatives to dual encoders that use, e.g., BM25 scores applicable to texts (Logeswaran et al., 2019; Zhang & Stratos, 2021) or other cheaper or more expensive alternatives (Humeau et al., 2019; Luan et al., 2021). However, there is usually trading-off complexity for quality.\n\n\nIt is also worth mentioning the works trying to facilitate the training of the dual encoder through distilling a heavier ranker model (Wu et al., 2019; Hofst\u00e4tter et al., 2020; Lu et al., 2020; Qu et al., 2020; Liu et al., 2021). Although these works aim at simplifying the learning of light ranking using the heavy one, they differ from our approach in two aspects. First, distillation means that there are still two heavy (comparable in orders of magnitude of trainable parameters) models. Second, this approach requires two different complex model architectures that can be a significant disadvantage for, e.g., recommender systems with a wide variety of image/textual/statistical/sequential features.\n\n\nAs for the nearest neighbors search in a common query-item space, a wide variety of algorithms exist, including locality-sensitive hashing (LSH) (Indyk & Motwani, 1998; Andoni & Indyk, 2008), partition trees (Bentley, 1975; Dasgupta & Freund, 2008; Dasgupta & Sinha, 2013), and similarity graphs (Navarro, 2002). LSH-based and tree-based methods provide strong theoretical guarantees, however, it has been shown that graph-based methods usually perform better (Malkov & Yashunin, 2018; Aum\u00fcller et al., 2020), which explains their widespread use in practical applications.\n\n\nAnother research direction is methods that combine nearest neighbors search with heavy ranker calls (Morozov & Babenko, 2019; Chen et al., 2022) instead of separately embedding queries and items in a common space where the search for the nearest items can be efficiently performed. Such methods show better quality in comparison with separate embeddings, however, their practical application may be limited due to a significant change in the structure of the search index. In particular, in practice, microservices with neural networks and microservices with document indexes are different services, which allows for increasing GPU utilization on the one hand and using specialized (including sharded) solutions with a large amount of memory on the other. Therefore, in this paper, we focus on the basic scenario with a separate investment in space and a separate search for the nearest elements in it.\n\n\nA recent paper by Yadav et al. (2022) is the most relevant for our research. The idea is to apply the matrix factorization to the query-item relevance matrix in order to represent it as a product of its submatrix containing only a few columns (relevances for random support items) and some other, explicitly computable. Despite the simplicity of the idea and implementation, the authors have shown in detail the superiority of their algorithm over more complex approaches, such as dual encoders. Our work is motivated by this study: we show that the approach of Yadav et al. (2022) has theoretical guarantees and suggest several improvements that significantly boost the performance. As an enhancement of the original approach, in another paper, Yadav et al. (2023) proposed the idea of selecting support items per each query independently, but the time complexity of each query processing becomes linear in the number of elements, which makes the approach infeasible in most practical applications. On the contrary, our support items selection is performed in the pre-processing stage and does not increase the query time.\n\n\nFinally, the most recent article (Yadav et al., 2024) proposes an alternative solution to the problem. Their AXN algorithm dynamically learns the difference between the DE and CE (or other query-document embeddings) predictions for each query independently (at the query time). Learning the difference is carried out through iteratively choosing a set of anchor (supporting) elements, calculating CE scores for them, and learning linear regression with embeddings of these elements as features and CE scores as targets. Some disadvantages of this method are: 1) it requires previously trained embeddings of queries and documents; 2) it has increased query processing time due to the need for iterative refining of all item relevances for each query. For completeness of our study, we use the AXN algorithm as one of our baselines.\n\n\n# 3 RELEVANCE-BASED EMBEDDINGS\n\n\nIn general, the information (attributes) used to calculate the ground-truth item-to-query relevances  $R(i,q)$  can be divided into three types: depending only on the query  $q$ , only on the item  $i$ , and on both of them. The key problem when constructing separate embeddings of items and queries in the common space (that can be used for searching for the nearest elements) is the inability to use information that depends on both query and item, which lowers the quality of relevance search.\n\n\nTo address the above-mentioned issue, we introduce relevance-based representations that describe each query by its relevance to a pre-selected set of items and, vice-versa, each item by its relevance to a pre-selected set of queries. We prove that, under certain conditions, any relevance function can be well estimated using only such individual vectors.\n\n\n# 3.1 PRELIMINARIES\n\n\nLet  $Q$  and  $I$  be compact topological spaces of queries and items, respectively. Assume that we are given a relevance function  $R: I \\times Q \\to \\mathbb{R}$ . In practice,  $R$  is our relevance model which may be computationally expensive and rely on pairwise features.\n\n\nLet  $S_I \\subset I$  and  $S_Q \\subset Q$  be some finite ordered sets of support items and support queries:  $S_I = \\{i_1, \\ldots, i_m\\}$ ,  $S_Q = \\{q_1, \\ldots, q_n\\}$ . Let  $R(i, S_Q)$  be a relevance vector of the item  $i$  w.r.t. the set of support queries  $S_Q$ :  $R(i, S_Q) = (R(i, q_1), \\ldots, R(i, q_n))$ . Similarly,  $R(S_I, q)$  is a relevance vector of the query  $q$  w.r.t. the set of support items  $S_I$ :  $R(S_I, q) = (R(i_1, q), \\ldots, R(i_m, q))$ . By  $R(S_I, S_Q)$  we denote a relevance matrix composed in a similar way.\n\n\n# 3.2 CUR APPROXIMATION\n\n\nRelevance vectors can be utilized in different ways and one of the possible approaches is to use the CUR decomposition (this approach was used by Yadav et al. (2022)). Using our notation, the relevance for a query  $q$  is approximated as:\n\n\n$$\n\n$$\n\\tilde {R} (I, q) := \\langle R (I, S _ {Q}) \\times \\operatorname {p i n v} (R \\left(S _ {I}, S _ {Q}\\right)), R \\left(S _ {I}, q\\right) \\rangle , \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\operatorname{pinv}(X)$  is the pseudo-inverse matrix of  $X$ . As support queries, Yadav et al. (2022) take the set of train queries:  $S_{Q} = Q_{train} \\subset Q$ .\n\n\nRegarding the computational complexity, we note that the CUR approximation requires computing the matrix  $R(I, Q_{train})$  which takes  $O(M \\cdot |S_Q|) = (M \\cdot |Q_{train}|)$  CE calls, where  $M$  is the total number of items in a database, which can be infeasible for large databases.\n\n\nOur first theoretical result provides guarantees for the CUR approximation (1). Namely, we show that its regularized version approximates the true relevance arbitrarily precisely in  $L_{2}$ . Formally, let  $\\mathrm{pinv}_{\\lambda}(A) = (A^T A + \\lambda E)^{-1} A^T$  with  $E$  being the identity matrix. Then, the regularized CUR approximation  $\\mathrm{CUR}_{\\lambda}$  is defined by (1) with  $\\mathrm{pinv}_{\\lambda}$  instead of pinv. For  $\\mathrm{CUR}_{\\lambda}$ , we prove the following result (see Appendix A.3 for the proof).\n\n\nTheorem 3.1. Suppose that  $I$  and  $Q$  are equipped with the structure of a measure space and the integral of  $R^4 (i,q)$  over  $I\\times Q$  is finite. Then, the  $\\mathrm{CUR}_{\\lambda}$  approximation of  $R$  can be chosen arbitrarily close to the true  $R$  in  $L_{2}(I\\times Q)$  provided enough uniformly sampled support items and queries and sufficiently small  $\\lambda$ .\n\n\nOur result gives theoretical support for the good performance of the CUR decomposition demonstrated in Yadav et al. (2022). In the next section, we show that stronger theoretical guarantees can be provided if we allow transforming query and item vectors into more powerful embeddings.\n\n\n# 3.3 RELEVANCE-BASED EMBEDDINGS\n\n\nIn this section, we propose extending the CUR-based approximation by allowing transformations of relevance vectors, e.g., with a neural architecture. With such embeddings, we prove a stronger result: that any continuous relevance function can be uniformly approximated.\n\n\nWe say that a function on  $I$  is a relevance-based embedding if it has a representation of the form  $e_I(i) = f_I(R(i, S_Q), \\theta_I)$  where  $S_Q$  is a set of support queries and  $f_I$  is some ML-architecture with parameters  $\\theta_I$  which parametrizes a mapping  $\\mathbb{R}^n \\to \\mathbb{R}^d$ . Analogously,  $e_Q(q) = f_Q(R(S_I, q), \\theta_Q)$  is a relevance-based embedding of the query  $q$ .\n\n\nThe following theorem holds (the proof can be found in Appendix A.1 and the guarantees for the RBE approach on a sphere are discussed in Appendix A.2).\n\n\nTheorem 3.2. Let  $I$  and  $Q$  be compact topological spaces, and  $R: I \\times Q \\to \\mathbb{R}$  be a continuous function. Then,  $R$  can be uniformly approximated up to an arbitrarily small absolute error by a function  $\\tilde{R}(i, q)$ :\n\n\n$$\n\n$$\n\\tilde {R} (i, q) = \\left\\langle f _ {I} \\left(R (i, S _ {Q}), \\theta_ {I}\\right), f _ {Q} \\left(R \\left(S _ {I}, q\\right), \\theta_ {Q}\\right) \\right\\rangle , \\tag {2}\n$$\n\n$$\n\n\nwhere  $S_{I} \\in I$  and  $S_{Q} \\in Q$  are some finite sets of support items and queries and  $f_{I}, f_{Q}$  are neural architectures with the universal approximation property (e.g., MLPs).\n\n\nNote that compared to Theorem 3.1, this theorem has weaker requirements and stronger convergence. Trainable embeddings allow us to soften the requirements ( $R$  can be any continuous function) and prove uniform convergence instead of  $L_{2}$  convergence).\n\n\nThis theorem shows that the true relevance function can be uniformly approximated with arbitrary precision by some functions of the relevance vectors. Our relevance-based framework is visualized in Figure 1. Note that the CUR\n\n\n![Figure 1: RBE visualization: support queries are red, support items are yellow; test queries are blue, and their relevance scores for the support items are used to approximate the remaining values](images/5f9e39b563ef389679b0b558a9d3515a03276050af062573f6cf88e72b6c7790.jpg)\n\n\napproximation discussed in Section 3.2 fits our framework with  $\\theta_I \\coloneqq \\mathrm{pinv}(R(S_I, S_Q))$  and  $f_I(R(i, S_Q), \\theta_I) \\coloneqq R(i, S_Q) \\times \\theta_I$ ,  $f_Q(R(S_I, q), \\theta_Q) \\coloneqq R(S_I, q)$ . Extending the CUR approximation to arbitrary embeddings allows us to obtain even stronger approximation guarantees. Moreover, such trainable embeddings are more flexible as they can adapt to a particular quality function one aims to optimize.\n\n\nRegarding the computation complexity, the pre-processing step requires  $O(M \\cdot |S_Q|) + N$  computations of CE, where  $N$  is the size of the training set (this part is similar to the training of dual encoders). An advantage of RBE is that we may have a significantly smaller set of support queries  $|S_Q| \\ll |Q_{train}|$ , while still utilizing the remaining queries for training the mappings  $f_{Q}$  and  $f_{I}$ . Also, the number of CE computations can be further reduced to just  $O(N)$  if we replace the transformation  $f_{I}(R(i,S_{Q}),\\theta_{I})$  with trainable embeddings of items  $\\theta_I(i) \\in R^d$ , see Section 3.5 for the discussion.\n\n\nTo sum up, the main outcome of our analysis is that even though the relevance function  $R(\\cdot, \\cdot)$  is arbitrary (and, in particular, can rely on pairwise features only available for query-item pairs), it can be well approximated by relevance-based embeddings of individual users and items. This is a clear advantage of RBE over conventional dual-encoder models.\n\n\n# 3.4 SELECTING SUPPORT ITEMS\n\n\nLet us revise the statement of Theorem 3.2. The theorem states that there exist such sets  $S_{Q}$ ,  $S_{I}$  that the relevance function  $R$  could be effectively approximated by separated embeddings  $f_{I}, f_{Q}$ . In the related works (Morozov & Babenko, 2019; Yadav et al., 2022), this selection is random, implicitly assuming that the elements are in some sense equivalent. However, for example, when building a recommender service, the popularity of different objects has a strongly skewed distribution, which is why more information is known about a small set of highly popular items than about a large set of unpopular ones. Thus, it is natural to assume that the choice of support items may have a significant effect on performance. We investigate this direction and compare several approaches from simple heuristics to more complex ones optimizing the approximation quality.\n\n\n# SIMPLE HEURISTICS OUR HEURISTIC STRATEGIES INCLUDE:\n\n- - Random Support items are chosen uniformly at random (Morozov & Babenko, 2019; Yadav et al., 2022). For better reproducibility, we also present the results of using the first  $|S_I|$  items as support ones, assuming that the order of queries is pseudorandom.\n\n- - Popular As mentioned above, a recommender service usually has a small set of very popular elements that many users interact with. As a result, a lot of information can be collected from these interactions thus making the popular elements more informative. Since it is not always possible to get popularity explicitly, we consider the following surrogate: choose the objects with the highest average relevance for the training set.\n\n- - Clusters centers When it comes to the allocation of a representative subset of vectors, it is reasonable to consider the allocation of clusters. We consider various clustering algorithms and select the cluster centers as support elements. The number of clusters is set to the number of required support elements.\n\n- - Most diverse This strategy is a greedy algorithm maximizing the minimum distance between the support elements. We first choose the element furthest from the center (by Euclidean distance) and then, at each step, an element is selected whose minimum distance to the current support elements is maximal.\n\n\nThe approaches that require item representations can be applied to their relevance vectors.\n\n\n$l_{2}$ -greedy approach Let us now discuss a more theoretically justified approach that we call  $l_{2}$ -greedy. It aims at selecting the key elements that allow for a better approximation of the relevance matrix  $R(i,q)$ . In this strategy, we greedily select items so that the MSE error of the CUR approximation (Mahoney & Drineas, 2009) is minimized for the train queries.\n\n\nWe note that the CUR approximation replaces every item with a linear combination of support items so that the MSE between the true relevances and their CUR approximations on the train set of queries is minimized (see Appendix A.3 for more details). Our goal is to optimize the overall MSE for all items, which is:\n\n\n$$\n\n$$\n\\sum_ {i} \\| R (i, S _ {Q}) ^ {T} - R \\left(S _ {I}, S _ {Q}\\right) ^ {T} \\times \\operatorname {p i n v} \\left(R \\left(S _ {I}, S _ {Q}\\right) ^ {T}\\right) \\times R (i, S _ {Q}) ^ {T} \\| _ {2} ^ {2}. \\tag {3}\n$$\n\n$$\n\n\nWe minimize this expression over all possible choices of  $S_I$ ,  $|S_I| = m$ . We propose a greedy approach in which support items are selected one by one optimizing (3) at each step. Due to space constraints, the implementation details are placed in Appendix B.\n\n\nComputation complexity In their default implementation, support items selection strategies popular, cluster centers, most diverse, and  $l_{2}$ -greedy require computing the relevance scores of all items to support queries that can be done in  $O(M \\cdot n)$ . If this is infeasible, one can use downsampling to reduce the number of candidates for support items. Our preliminary experiments (see Appendix E) show that even significant downsampling gives reasonable performance of the obtained support items. Moreover, for heuristic approaches, instead of the relevance vectors, one can use cheaper embeddings (e.g., the original feature vectors) when they are available. Moreover, for cluster centers, there can be predefined clusters in data. For instance, in recommender services, it is typical that items are annotated with their categories that can be used as clusters without any additional cost.\n\n\n# 3.5 ADDITIONAL PRACTICAL CONSIDERATIONS\n\n\nDynamic set of items We note that relevance-based embeddings can naturally handle scenarios where the set of items frequently changes. The embeddings  $f_{I}(R(i, S_{Q}), \\theta_{I})$  can be easily calculated for the new items without the need of re-training the embedding model  $f_{I}$  (similarly to feature-based dual encoders). On the other hand, if the set of items  $I$  is finite or changes not so frequently (e.g., the set of movies currently available in a recommender service), the transformation  $f_{I}(R(i, S_{Q}), \\theta_{I})$  can be replaced with trainable embeddings  $\\theta_{I}(i) \\in R^{d}$ . In contrast to items, the query set  $Q$  cannot be assumed to be finite: queries can be represented by texts of unlimited length or characterized by real-valued features.\n\n\nImproving approximation quality As discussed above, it is natural to assume the heavy ranker  $R$  to be the most expensive part in terms of computational complexity. Thus, during the calculation of  $\\tilde{R}$ , we are mainly limited by the sizes of the support sets  $S_{Q}$  and  $S_{I}$ . In contrast, calculating  $f_{Q}$ ,  $f_{I}$ , or the dot product between them is assumed to be significantly cheaper. Thus,  $f_{Q}$  and  $f_{I}$  may embed the relevance vectors in a higher-dimensional space. In particular, this may help to eliminate the disadvantages of the dot product, in comparison with other (Shevkunov & Prokhorenkova, 2021) ways of measuring the distances or similarities between objects.\n\n\nWhile relevance-based embeddings have theoretical guarantees, they hold in the limit, when the sets of support queries and items are sufficiently large. In practice, to improve the approximation performance and reduce the number of CE calls per query, the mapping  $f_{Q}$  (or  $f_{I}$ ) could be extended by enriching it with the features of the original query (or item).\n\n\nScalability The cost of the pre-processing and the ways to scale it are discussed in Section 3. At the inference, we need to compute the query representation, which requires  $m$  relevance computations. Then, the inference is similar to dual encoders: the item representations are pre-calculated and placed in the Approximate Nearest Neighbours index like HNSW (Malkov & Yashunin, 2018), which takes the embedding of the query as input. These  $m$  additional computations are taken into account in our experiments: when comparing with dual encoders, we reduce the final re-ranking budget by  $m$ .\n\n\nOther applications Although the goal of RBEs is to approximate the relevance function, relevance vectors can also be considered good general representations. Thus, their application is not limited to relevance predictions, as shown below in Appendix D.2.\n\n\n# 4 EXPERIMENTAL SETUP\n\n\nLet us describe our experimental setup. In all datasets that we use in this work, there is a heavy ranker that provides relevance, which we consider to be close to the ground truth. With this ranker, we build the complete table  $(R: I \\times Q \\to \\mathbb{R})$  of the relevance scores. We denote the predicted scores by  $\\hat{R}$ .\n\n\nThe task is to find the most relevant items for a given query. The quality is evaluated as\n\n\n$$\n\n$$\n\\operatorname {H i t R a t e} (P, T) := \\sum_ {q _ {i} \\in Q _ {\\text {t e s t}}} \\frac {| \\operatorname {B e s t} _ {P} (\\hat {R} , q _ {i}) \\cap \\operatorname {B e s t} _ {T} (R , q _ {i}) |}{| \\operatorname {B e s t} _ {T} (R , q _ {i}) | | Q _ {\\text {t e s t}} |}, \\operatorname {H i t R a t e} (K) := \\operatorname {H i t R a t e} (K, K),\n$$\n\n$$\n\n\nwhere  $\\operatorname{Best}_K(\\mathcal{R}, q) \\subset I$  is defined as the set of  $K$  items  $i_1, \\ldots, i_K$  with the highest relevances  $\\mathcal{R}(i_1, q), \\ldots, \\mathcal{R}(i_K, q)$  to a given query  $q \\in Q$ ;  $Q_{test}$  is a set of test queries that do not participate in the training:  $S_Q \\subset Q_{train}$ ,  $Q_{train} \\sqcup Q_{test} = Q$ . For all our experiments,  $|Q_{test}| \\approx 0.3|Q|$ ,  $|S_I| = 100$ ,  $S_Q = Q_{train}$ .\n\n\n# 4.1 DATASETS\n\n\nZESHEL The Zero-Shot Entity Linking (ZESHEL) dataset was constructed by Logeswaran et al. (2019) from Wikia. The task of zero-shot entity linking is to link mentions of objects in the text to an object from the list of entities with related descriptions. The dataset consists of 16 different domains. Each domain contains disjoint sets of entities, and during testing, mentions should be associated with invisible entities solely based on entity descriptions. We run the experiments on five domains from ZESHEL selected by Yadav et al. (2022). As a heavy ranker  $R$ , we use the cross-encoder trained by Yadav et al. (2022) and publicly available. Table 7 in Appendix shows the dataset statics for the domains used in this paper.\n\n\nQuestion-Answering To additionally cover the question-answering domain, we conducted experiments on the MsMarco (Nguyen et al., 2016) based dataset provided by huggingface. As a heavy ranker, we use all-mpnet-base-v2 from the SentenceTransformers library (Reimers & Gurevych, 2019), which is trained, among other datasets, on the MsMarco data. We took  $\\sim 10\\mathrm{K}$  test queries and 0.8M passages corresponding to them (QA). For the experiments in Table 1, we used the smaller (QA.Small) version with 82K passages (only test passages).\n\n\nRecSys To evaluate the generalization of the proposed approach to other tasks and domains, we collected a dataset from a production service providing recommendations of items to users. As a heavy ground-truth ranker  $R$ , we use the CatBoost gradient boosting model (Prokhorenkova et al., 2018) trained on a wide range of features, including categories and other static attributes of items, social information (age, language, etc.) of users, simple item statistics, user statistics, real-time statistics on user and item interaction, factors derived from the matrix factorizations, and multiple two-tower neural networks, receiving the features listed above as their features.\n\n\nTwo versions of the dataset are presented. In the first one, CatBoost was trained to predict the time that a user is going to spend on a given item immediately after the click (in one session). In the second version, CatBoost was trained on the pairwise PairLogit target to predict the item with the longest time spent for some long time after the click (including new sessions). The first version is denoted in the tables as RecSysLT and the second as RecSys. This dataset allows us to evaluate the generalizability of our approach across different domains and different types of heavy rankers since gradient-boosting models differ significantly from neural approaches.\n\n\nRecSys2 To further increase diversity of the considered domains, we collected a dataset from another production recommender service. Here, the CatBoost gradient boosting model is used as a\n\n\nTable 1: Support item selection applied to the CUR relevance approximation, HitRate(100) (larger is better) is reported\n\n\n<table><tr><td>Support items</td><td>Yugioh</td><td>P.Wrest.</td><td>StarTrek</td><td>Dr.Who</td><td>Military</td><td>RecSys</td><td>RecSysLT</td><td>RecSys2</td><td>QA.Small</td></tr><tr><td>random (i.e. AnnCUR)</td><td>0.4724</td><td>0.4280</td><td>0.2287</td><td>0.1919</td><td>0.2455</td><td>0.6697</td><td>0.5842</td><td>0.1478</td><td>0.5828</td></tr><tr><td>first 100</td><td>0.4845</td><td>0.4182</td><td>0.2489</td><td>0.1975</td><td>0.2599</td><td>0.6490</td><td>0.5609</td><td>0.1551</td><td>0.5491</td></tr><tr><td>popular</td><td>0.2429</td><td>0.3001</td><td>0.1154</td><td>0.1197</td><td>0.1907</td><td>0.7623</td><td>0.6695</td><td>0.1422</td><td>0.5536</td></tr><tr><td>KMeans</td><td>0.5083</td><td>0.4850</td><td>0.3226</td><td>0.2517</td><td>0.3042</td><td>0.7070</td><td>0.6184</td><td>0.1661</td><td>0.5578</td></tr><tr><td>BisectingKMeans</td><td>0.4825</td><td>0.4592</td><td>0.2839</td><td>0.2159</td><td>0.2752</td><td>0.7035</td><td>0.6213</td><td>0.1483</td><td>0.5394</td></tr><tr><td>MiniBatchKMeans</td><td>0.5077</td><td>0.4737</td><td>0.2912</td><td>0.2365</td><td>0.2826</td><td>0.7033</td><td>0.5981</td><td>0.1721</td><td>0.5529</td></tr><tr><td>AgglomerativeClustering</td><td>0.5105</td><td>0.4911</td><td>0.3264</td><td>0.2531</td><td>0.2448</td><td>0.7050</td><td>0.6265</td><td>0.1557</td><td>0.5666</td></tr><tr><td>SpectralCocclustering</td><td>0.4618</td><td>0.4443</td><td>0.2540</td><td>0.2076</td><td>0.2551</td><td>0.6998</td><td>0.6094</td><td>0.1594</td><td>0.5415</td></tr><tr><td>SpectralBiclustering</td><td>0.4654</td><td>0.4708</td><td>0.2628</td><td>0.1845</td><td>0.2533</td><td>0.7409</td><td>0.5972</td><td>0.1607</td><td>0.5358</td></tr><tr><td>SpectralClusteringNN</td><td>0.5087</td><td>0.4690</td><td>0.2742</td><td>0.2048</td><td>0.2507</td><td>0.6936</td><td>0.5740</td><td>0.2343</td><td>0.5741</td></tr><tr><td>ByMin</td><td>0.5333</td><td>0.4290</td><td>0.3325</td><td>0.2278</td><td>0.2483</td><td>0.6504</td><td>0.6182</td><td>0.1329</td><td>0.5925</td></tr><tr><td>l2-greedy</td><td>0.5618</td><td>0.5119</td><td>0.3677</td><td>0.2960</td><td>0.3357</td><td>0.7197</td><td>0.6565</td><td>0.1478</td><td>0.6100</td></tr></table>\n\n\nTable 2: Evaluating neural relevance-based embeddings, HitRate(100) (larger is better) is reported\n\n\n<table><tr><td>Model</td><td>Yugioh</td><td>P.Wrest.</td><td>StarTrek</td><td>Dr.Who</td><td>Military</td><td>RecSys</td><td>RecSysLT</td><td>RecSys2</td><td>QA</td></tr><tr><td>Popular</td><td>0.0917</td><td>0.2410</td><td>0.0884</td><td>0.0821</td><td>0.1127</td><td>0.5077</td><td>0.2886</td><td>0.0142</td><td>0.0001</td></tr><tr><td>AnnCUR</td><td>0.4724</td><td>0.4280</td><td>0.2287</td><td>0.1919</td><td>0.2455</td><td>0.6697</td><td>0.5842</td><td>0.1478</td><td>0.5522</td></tr><tr><td>AnnCUR+KMeans</td><td>0.5083</td><td>0.4850</td><td>0.3226</td><td>0.2517</td><td>0.3042</td><td>0.7070</td><td>0.6184</td><td>0.1661</td><td>0.5027</td></tr><tr><td>RBE+KMeans</td><td>0.5431</td><td>0.4979</td><td>0.3399</td><td>0.2539</td><td>0.3019</td><td>0.7137</td><td>0.6300</td><td>0.3729</td><td>0.5505</td></tr><tr><td>AnnCUR+l2-greedy</td><td>0.5618</td><td>0.5119</td><td>0.3677</td><td>0.2960</td><td>0.3357</td><td>0.7197</td><td>0.6565</td><td>0.1478</td><td>0.5700</td></tr><tr><td>RBE+l2-greedy</td><td>0.5849</td><td>0.5249</td><td>0.3867</td><td>0.2992</td><td>0.3349</td><td>0.7234</td><td>0.6682</td><td>0.3964</td><td>0.6022</td></tr></table>\n\n\nheavy ranker and a dual encoder as a baseline. Both CE and DE are trained on a large set of external data and features. A smaller subsample of this data was used to train RBE, since we believe that RBE is able to show good quality even on a significantly smaller size of the training data.\n\n\n# 4.2 BASELINES\n\n\nAs our main baseline, we consider the AnnCUR (Yadav et al., 2022) recommendation algorithm that approximates relevances with the CUR decomposition as discussed in Section 3.2. What is important for further discussion, a broad comparison of this method with different basic approaches, including various dual encoders, is carried out by Yadav et al. (2022). In most of our experiments, we rely on these results, comparing only with AnnCUR. However, we explicitly provide the comparison with dual encoders for the new datasets RecSysLT and RecSys in Section 5. For better interpretability of the results, we also provide metrics for a baseline that always selects the most popular items (not to be confused with Table 1, where the \"popular\" refers to selecting support items).\n\n\n# 4.3 RBE IMPLEMENTATION\n\n\nFollowing Theorem 3.2, we train lightweight neural networks  $f_{I}$  and  $f_{Q}$ , which are significantly faster than the heavy ranker  $R$ , and independently transform the relevance vectors  $R(i,S_{Q}),R(S_{I},q)$  into embeddings. The training is performed by the Adam algorithm on sampled batches with a listwise loss function (see Appendix C), similar to the training of various DEs.\n\n\nAs follows from Section 3.2, the CUR decomposition gives a reasonably good approximation of the relevance function. Hence, we split the RBE representation into the CUR representation and the trainable prediction of its error. In the experiments, such decomposition improves the convergence and training stability. Technical implementation details are placed in Appendix C.\n\n\n# 5 EXPERIMENTAL RESULTS\n\n\nSupport items selection Following Section 3.4, we check various ways of choosing support elements as opposed to the existing approaches that use random selection. All clustering algorithms are taken from the scikit-learn (Pedregosa et al., 2011) library, SpectralClusteringNN is a SpectralClustering with \"nearest neighbors\" affinity. The algorithms are used with their default parameters since even this simple setting already allows us to get significant improvements over the random selection.\n\n\nThe results are shown in Table 1, where the best three results for each dataset are highlighted. Clearly, there is a significant superiority of almost any approach based on clustering or diversity over the random selection. The theoretically justified  $l_{2}$ -greedy algorithm is the clear winner, second and third places are taken by KMeans and AgglomerativeClustering. However, due to the significantly worse quality of AgglomerativeClustering on the Military dataset, KMeans will be used in further experiments. Another observation is that on RecSys and RecSysLT, there is a clear superiority of the choice of popular items as the support ones. It is worth mentioning that for this dataset, the elements extracted by popularity are also quite stratified by their categories and an explicit restriction on the number of elements from one category changes the top slightly. However, this may not be true for other data (e.g., we do not observe a similar feature for RecSys2).\n\n\nNeural relevance-based embeddings Following the description in Section 4.3, we also apply non-trivial trainable relevance mappings  $f_{I}(R(I,S_{Q}),\\theta_{I}),f_{Q}(R(S_{I},q),\\theta_{Q})$  to check whether this modification improves prediction quality in practice. The results are shown in Table 2. To better interpret the values, the quality of the constant output consisting of popular (in the same sense as in the previous paragraph) elements is also given. It can be seen that in most cases, except for one dataset (Military), trainable relevance mappings improve the final quality of the search for relevant elements and the improvements are obtained for both KMeans and  $l_{2}$ -greedy support elements selection. Improvement from the trainable mappings is most noticeable for the QA dataset. Note that the transformation that we use is not claimed to be optimal and is given rather to demonstrate that with the help of an easy transformation, one can get an increased quality on various datasets.\n\n\nTable 3: Dual encoder embeddings vs support relevances, different tops, RecSysLT\n\n\n<table><tr><td>X</td><td></td><td>100</td><td>200</td><td>300</td><td>400</td><td>500</td><td>600</td><td>700</td><td>800</td><td>900</td></tr><tr><td>Dual Encoder</td><td>HR(X+100, X)</td><td>0.7048</td><td>0.6803</td><td>0.6739</td><td>0.6739</td><td>0.6760</td><td>0.6792</td><td>0.6827</td><td>0.6868</td><td>0.6904</td></tr><tr><td>AXNDE</td><td>HR(X+100, X)</td><td>0.7065</td><td>0.6769</td><td>0.6740</td><td>0.6769</td><td>0.6820</td><td>0.6883</td><td>0.6958</td><td>0.7054</td><td>0.7161</td></tr><tr><td>RBE+l2-greedy</td><td>HR(X, X)</td><td>0.6682</td><td>0.6955</td><td>0.7221</td><td>0.7406</td><td>0.7538</td><td>0.7639</td><td>0.7720</td><td>0.7792</td><td>0.7853</td></tr></table>\n\n\nTable 4: Dual encoder embeddings vs support relevances, different tops, RecSys2\n\n\n<table><tr><td>X</td><td></td><td>100</td><td>200</td><td>300</td><td>400</td><td>500</td><td>600</td><td>700</td><td>800</td><td>900</td></tr><tr><td>Dual Encoder</td><td>HR(X+100, X)</td><td>0.3792</td><td>0.3198</td><td>0.2928</td><td>0.2784</td><td>0.2702</td><td>0.2661</td><td>0.2647</td><td>0.2652</td><td>0.2671</td></tr><tr><td>AXNDE</td><td>HR(X+100, X)</td><td>0.3843</td><td>0.3333</td><td>0.3015</td><td>0.2879</td><td>0.2839</td><td>0.2835</td><td>0.2836</td><td>0.2855</td><td>0.2893</td></tr><tr><td>RBE+l2-greedy</td><td>HR(X, X)</td><td>0.3964</td><td>0.4471</td><td>0.4693</td><td>0.4833</td><td>0.4929</td><td>0.5008</td><td>0.5070</td><td>0.5119</td><td>0.5162</td></tr></table>\n\n\nTable 5: Dual encoder embeddings vs support relevances, RecSysLT\n\n\n<table><tr><td>K</td><td></td><td>100</td><td>200</td><td>300</td><td>400</td><td>500</td><td>600</td><td>700</td><td>800</td><td>900</td></tr><tr><td>Dual Encoder</td><td>HR(K+100, 100)</td><td>0.7048</td><td>0.7977</td><td>0.8518</td><td>0.8855</td><td>0.9086</td><td>0.9258</td><td>0.9385</td><td>0.9484</td><td>0.9561</td></tr><tr><td>AXNDE</td><td>HR(K+100, 100)</td><td>0.7065</td><td>0.7970</td><td>0.8538</td><td>0.8902</td><td>0.9153</td><td>0.9331</td><td>0.9465</td><td>0.9572</td><td>0.9660</td></tr><tr><td>RBE+l2-greedy</td><td>HR(K, 100)</td><td>0.6682</td><td>0.8359</td><td>0.9026</td><td>0.9342</td><td>0.9522</td><td>0.9632</td><td>0.9704</td><td>0.9760</td><td>0.9799</td></tr></table>\n\n\nComparison with dual encoders In this part, we compare RBE with dual encoders. For RecSys and RecSys2 datasets, we consider the embeddings produced by the dual encoder used in the production of the service (i.e., the one that is proved to be the best in this task). To fairly compare the performance, we replace the relevance vectors in the RecSys experiments with embeddings obtained by the DE, keeping the transformation of the relevance vectors and the training parameters unchanged. The only difference between the pipelines for RBE and DE is that since we used  $|S_I| = 100$  requests to a heavy ranker to obtain an RBE, we have to decrease the budget for RBE CE calls when calculating the top: compared to RBE, DE uses  $|S_I|$  more CE calls for the re-ranking. We also applied the AXN $^6$  method in addition to DE-embedding as an alternative approach to improve DE-like methods using CE calls. Similarly to DE, AXN uses an increased budget for CE calls. Thus, for the dual encoder and AXN, we use the metric HitRate  $(X + |S_I|, X)$  and for RBE \u2014 HitRate  $(X, X)$ , which gives\n\n\nTable 6: Dual encoder embeddings vs support relevances, RecSys2\n\n\n<table><tr><td>K</td><td></td><td>100</td><td>200</td><td>300</td><td>400</td><td>500</td><td>600</td><td>700</td><td>800</td><td>900</td></tr><tr><td>Dual Encoder</td><td>HR(K+100, 100)</td><td>0.3792</td><td>0.4228</td><td>0.4514</td><td>0.4738</td><td>0.4927</td><td>0.5091</td><td>0.5239</td><td>0.5373</td><td>0.5493</td></tr><tr><td>AXNDE</td><td>HR(K+100, 100)</td><td>0.3843</td><td>0.4296</td><td>0.4557</td><td>0.4834</td><td>0.5020</td><td>0.5198</td><td>0.5351</td><td>0.5485</td><td>0.5630</td></tr><tr><td>RBE+l2-greedy</td><td>HR(K, 100)</td><td>0.3964</td><td>0.5435</td><td>0.6253</td><td>0.6819</td><td>0.7253</td><td>0.7599</td><td>0.7888</td><td>0.8135</td><td>0.8343</td></tr></table>\n\n\nthe former an advantage for small top sizes. However, starting from  $X = 200$ , our algorithm is superior to both dual encoder and  $AXN_{DE}$  baselines, as can be seen from Table 3. As for Table 4, our algorithm is superior to both dual encoder and  $AXN_{DE}$  baselines from  $X = 100$ .\n\n\nA second comparison, where the size of the desired top is fixed, while the number of extracted elements changes, is presented in Tables 5 and 6. Similarly to the previous comparison, RBE calls to the heavy ranker are taken into account: for the dual encoder and AXN, HitRate  $(K + |S_I|, 100)$  is calculated, and for RBE \u2014 HitRate  $(K, 100)$ . In this comparison, RBE outperforms the dual encoder and  $AXN_{DE}$  starting at  $K = 200$  for RecSysLT and at  $K = 100$  for RecSys2. Let us note that in the production service, the actual size of the top used to select candidates before ranking exceeds the values indicated in the table.\n\n\nA similar comparison with the dual encoder on the data from ZESHEL can be found in Yadav et al. (2022): it is shown that AnnCUR outperforms the dual encoder. Thus, on ZESHEL we compare only with AnnCUR.\n\n\nTransfer Learning Often in practice, embeddings trained to solve one problem are also applied to other ones. Following this, we conducted an additional experiment on predicting the categories of items in the RecSys dataset and obtained that the relevance vectors are informative vector representations. Moreover, the proposed algorithms for selecting key elements improve the quality of the prediction. Due to space constraints, the detailed results are provided in Appendix D.2.\n\n\n# 6 CONCLUSION\n\n\nIn this paper, we present the concept of relevance-based embeddings. We justify our approach theoretically and show its practical effectiveness on various datasets. We demonstrate that RBE allows one to obtain better quality compared with existing approaches. An important contribution of our work is the study of different strategies for choosing the support elements for RBE: we show that a proper choice of the support elements allows one to significantly boost performance.\n\n\nPromising directions for future research include a deeper investigation of support element selection strategies as well as applying the proposed RBE to other algorithms, e.g., based on using a heavy ranker during the nearest neighbor search (Morozov & Babenko, 2019) or adaptive nearest neighbor search (Yadav et al., 2024). Regarding the latter approach, we note that  $AXN$  can be naturally combined with any query-item representations, e.g., RBE. The obtained  $AXN_{RBE}$  model has to perform at least as well as RBE due to the presence of the regularization that balances between the query-item representation and the proposed modification.\n\n\n# REFERENCES\n\n- Andoni, A. and Indyk, P. Near-optimal hashing algorithms for near neighbor problem in high dimension. Communications of the ACM, 51(1):117-122, 2008.\n\n- Aum\u00fcller, M., Bernhardsson, E., and Faithfull, A. Ann-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. Information Systems, 87:101374, 2020.\n\n- Bentley, J. L. Multidimensional binary search trees used for associative searching. Communications of the ACM, 18(9):509-517, 1975.\n\n- Chen, R., Liu, B., Zhu, H., Wang, Y., Li, Q., Ma, B., Hua, Q., Jiang, J., Xu, Y., Deng, H., et al. Approximate nearest neighbor search under neural similarity metric for large-scale recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pp. 3013-3022, 2022.\n\n- Covington, P., Adams, J., and Sargin, E. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems, pp. 191-198, 2016.\n\n- Cybenko, G. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303-314, 1989.\n\n- Dasgupta, S. and Freund, Y. Random projection trees and low dimensional manifolds. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pp. 537-546, 2008.\n\n- Dasgupta, S. and Sinha, K. Randomized partition trees for exact nearest neighbor search. In Conference on learning theory, pp. 317-337. PMLR, 2013.\n\n- Gordo, A., Almaz\u00e1n, J., Revaud, J., and Larlus, D. Deep image retrieval: Learning global representations for image search. In Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, pp. 241-257. Springer, 2016.\n\n- Hofst\u00e4tter, S., Althammer, S., Schr\u00f6der, M., Sertkan, M., and Hanbury, A. Improving efficient neural ranking models with cross-architecture knowledge distillation. arXiv preprint arXiv:2010.02666, 2020.\n\n- Huang, P.-S., He, X., Gao, J., Deng, L., Acero, A., and Heck, L. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management, pp. 2333-2338, 2013.\n\n- Humeau, S., Shuster, K., Lachaux, M.-A., and Weston, J. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. arXiv preprint arXiv:1905.01969, 2019.\n\n- Indyk, P. and Motwani, R. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pp. 604-613, 1998.\n\n- Karpukhin, V., O\u011fuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\n\n- Kowalski, G. J. Information retrieval systems: theory and implementation, volume 1. springer, 2007.\n\n- Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural networks, 6(6):861-867, 1993.\n\n- Liu, F., Jiao, Y., Massiah, J., Yilmaz, E., and Havrylov, S. Trans-encoder: unsupervised sentence-pair modelling through self-and mutual-distillations. arXiv preprint arXiv:2109.13059, 2021.\n\n- Logeswaran, L., Chang, M.-W., Lee, K., Toutanova, K., Devlin, J., and Lee, H. Zero-shot entity linking by reading entity descriptions. arXiv preprint arXiv:1906.07348, 2019.\n\n- Lu, W., Jiao, J., and Zhang, R. Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp. 2645\u20132652, 2020.\n\n- Luan, Y., Eisenstein, J., Toutanova, K., and Collins, M. Sparse, dense, and attentional representations for text retrieval. Transactions of the Association for Computational Linguistics, 9:329-345, 2021.\n\n- Mahoney, M. W. and Drineas, P. CUR matrix decompositions for improved data analysis. Proceedings of the National Academy of Sciences, 106(3):697-702, 2009.\n\n- Malkov, Y. A. and Yashunin, D. A. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4):824-836, 2018.\n\n- Morozov, S. and Babenko, A. Relevance proximity graphs for fast relevance retrieval. arXiv preprint arXiv:1908.06887, 2019.\n\n- Navarro, G. Searching in metric spaces by spatial approximation. The VLDB Journal, 11:28-46, 2002.\n\n- Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. MS MARCO: A human generated machine reading comprehension dataset. CoRR, abs/1611.09268, 2016. URL http://arxiv.org/abs/1611.09268.\n\n- Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournaepau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.\n\n- Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., and Gulin, A. Catboost: unbiased boosting with categorical features. Advances in neural information processing systems, 31, 2018.\n\n- Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W. X., Dong, D., Wu, H., and Wang, H. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2010.08191, 2020.\n\n- Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084.\n\n- Shevkunov, K. and Prokhorenkova, L. Overlapping spaces for compact graph representations. Advances in Neural Information Processing Systems, 34:11665-11677, 2021.\n\n- Wu, L., Petroni, F., Josifoski, M., Riedel, S., and Zettlemoyer, L. Scalable zero-shot entity linking with dense entity retrieval. arXiv preprint arXiv:1911.03814, 2019.\n\n- Yadav, N., Monath, N., Angell, R., Zaheer, M., and McCallum, A. Efficient nearest neighbor search for cross-encoder models using matrix factorization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2171\u20132194, 2022.\n\n- Yadav, N., Monath, N., Zaheer, M., and McCallum, A. Adaptive selection of anchor items for cur-based k-nn search with cross-encoders. arXiv preprint arXiv:2305.02996, 2023.\n\n- Yadav, N., Monath, N., Zaheer, M., Fergus, R., and McCallum, A. Adaptive retrieval and scalable indexing for k-nn search with cross-encoders. International Conference on Learning Representations, 2024.\n\n- Yu, J., Qiu, M., Jiang, J., Huang, J., Song, S., Chu, W., and Chen, H. Modelling domain relationships for transfer learning on retrieval-based question answering systems in e-commerce. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pp. 682-690, 2018.\n\n- Zhang, W. and Stratos, K. Understanding hard negatives in noise contrastive estimation. arXiv preprint arXiv:2104.06245, 2021.\n\nResponse: Given the paper's contributions and the analysis provided, here is my evaluation:\n\n### Originality:\nThe paper introduces a novel concept called \"Relevance-Based Embeddings\" (RBE) which extends the traditional dual-encoder approach by leveraging relevance vectors to create embeddings. This approach offers a fresh perspective and theoretical grounding that differs significantly from existing methods.\n\n### Methodological Rigor:\nThe authors provide a solid theoretical foundation for their approach. They prove that under certain conditions, RBE can approximate any continuous relevance function, which is a significant theoretical advance. Additionally, they validate their approach empirically on multiple datasets, showing competitive performance relative to state-of-the-art methods.\n\n### Significance of Contribution:\nThe proposed RBE framework addresses a critical limitation of dual-encoders by directly incorporating the relevance function into the embedding process. This can lead to more efficient and potentially better-performing embeddings, especially in scenarios where the relevance function is computationally expensive.\n\n### Clarity of Presentation:\nThe paper is well-written and organized, presenting a clear motivation, theoretical background, and empirical validation. The sections flow logically from introduction to methodology, experiments, and conclusions. The technical details are adequately explained, although some supplementary material (like code implementations) would enhance the paper.\n\n### Alignment with Journal Scope:\nICLR focuses on advances in deep learning, particularly in areas like representation learning and similarity search. RBE fits well within this scope as it involves novel representations and embeddings for similarity search tasks.\n\n### Prediction:\nGiven the novelty of the approach, the theoretical foundations, and the empirical evidence supporting its effectiveness, I would predict this paper will likely be accepted by ICLR. The paper presents a substantial contribution that could have broad applicability in recommendation systems, information retrieval, and other domains involving similarity search.\n\n### Justification:\n- **Originality**: Novel approach to combining relevance vectors with embeddings.\n- **Methodological Rigor**: Strong theoretical guarantees and empirical validation.\n- **Significance of Contribution**: Potential to improve efficiency and quality in similarity search tasks.\n- **Clarity**: Well-presented and organized.\n-", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634776.4806561", "msecs": "480.0", "relativeCreated": "121304.13031578064", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Preventing Reward Hacking with Occupancy Measure Regularization\n\n# Abstract\nReward hacking occurs when an agent performs very well with respect to a specified or learned reward function (often called a \"proxy\"), but poorly with respect to the true desired reward function. Since ensuring good alignment between the proxy and the true reward is remarkably difficult, prior work has proposed regularizing to a \"safe\" policy using the KL divergence between action distributions. The challenge with this divergence measure is that a small change in action distribution at a single state can lead to potentially calamitous outcomes. Our insight is that when this happens, the state occupancy measure of the policy shifts significantly\u2014the agent spends time in drastically different states than the safe policy does. We thus propose regularizing based on occupancy measure (OM) rather than action distribution. We show theoretically that there is a direct relationship between the returns of two policies under *any* reward function and their OM divergence, whereas no such relationship holds for their action distribution divergence. We then empirically find that OM regularization more effectively prevents reward hacking while allowing for performance improvement on top of the safe policy.\n\n\n# 1 INTRODUCTION\n\n\nA major challenge for the designers of goal-oriented AI systems is specifying a reward function that robustly captures their goals and values. When the specified reward is misaligned with the designer's intent, it is really just a proxy for the true reward. This can lead to a phenomenon called reward hacking: the resulting policy accumulates achieves high reward according to the proxy reward function, but not according to the true reward function (Russell et al., 2010; Amodei et al., 2016; Pan et al., 2022; Skalse et al., 2022). The resulting behavior is often undesirable and can be especially catastrophic when deploying these systems in safety-critical scenarios, such as autonomous driving (Krakovna et al., 2019; Turner et al., 2019; Knox et al., 2022).\n\n\nThe best solution to prevent reward hacking would be to ensure better alignment between the specified proxy and hidden true reward. However, in practice, reward functions are extremely difficult to properly design due to the ambiguities and complex variables underlying real-world scenarios (Ibarz et al., 2018). For example, recommender systems aim to optimize the value that users attain from their time spent on the online platforms, but since this goal is difficult to quantify, designers utilize proxies, such as click-through rates, engagement time, and other types of explicit feedback they receive from users, which do not always match how satisfied users implicitly are with their experience (Stray et al., 2022). Several examples of reward hacking have been reported throughout the literature (Kraknova & Legg, 2020).\n\n\nRather than specifying a reward function by hand, an alternative is to learn it from human feedback like demonstrations (Ho & Ermon, 2016), comparisons (Sadigh et al., 2017; Christiano et al., 2017), or both (Palan et al., 2019) However, even learned reward functions are often misaligned with our true objectives, failing to generalize outside the distribution of behavior used in training (McKinney et al., 2023; Tien et al., 2023).\n\n\nInstead of blindly optimizing a proxy reward function, one proposal to avoid reward hacking is to regularize the policy's chosen actions to be similar to those of a known safe policy (Yang et al., 2021). For example, RLHF for LLMs generally optimizes the learned reward in addition to a term that penalizes divergences from the pre-trained language model's output (Glaese et al., 2022; Ouyang et al., 2022). Intuitively, this kind of regularization pushes the learned policy away from \"unusual\"\n\n\nbehaviors for which the reward function may be misaligned. These could include unforeseen strategies in the case of a hand-specified reward function or out-of-distribution states in the case of a learned reward function. Initializing using a pre-trained policy has been shown to effectively speed up the learning process (Laidlaw et al., 2023; Uchendu et al., 2023), and a similar paradigm can also be used to ensure the safety of the agent during online inference (Gulcehre et al., 2023).\n\n\nHowever, regularizing based on the action distributions of policies has significant drawbacks. Small shifts in action distributions can lead to large differences in outcomes. And vice-versa, large shifts in action distributions may not actually result in any difference in outcome and thus wrongly interfere with training the policy.\n\n\nImagine an autonomous car driving along a coastal highway with a steep cliff to its right. For illustrative purposes, let us think of the states as positions, and actions as velocities. Suppose we have access to a safe policy that drives slowly and avoids falling off the cliff, but the car is optimizing a proxy reward function that incentivizes progress rather than staying on the road. If we try to regularize the car's action distributions to the safe policy, we may have to apply a lot of regularization since one wrong action going too far to the right\u2014a small change in the action distribution at a single state\u2014can lead to disaster. This makes it very difficult for the regularizer to be successful. To make matters worse, optimizing the proxy will lead to moving faster, likely through action changes at many states, which makes going more to the right at a single state a negligible divergence. It is difficult if not impossible to avoid a single catastrophic action while improving at all on the safe policy.\n\n\n# XU ET AL. (2020)\n\n\nIf action distributions induce poor regularizers for reward hacking, then what can we do instead? Thinking back to the car example, while the single catastrophic action didn't change the action distribution much, observe that it did change something quite drastically: the resulting distribution over states visited by the car. The learned policy will have a high probability of reaching states where the car is off the cliff and crashed, while the safe policy never reaches such states. Our proposal follows naturally from this observation: to avoid reward hacking, regularize based on divergence from the safe policy's occupancy measure, rather than action distributions. An occupancy measure (OM) represents the distribution of states (and optionally actions) seen by a policy when it interacts with its environment. Unlike action distribution-based metrics, OM takes into account not just the actions taken by the policy, but also the states that the agent reaches.\n\n\nIn this paper, we show both theoretically and empirically that regularizing policy optimization using occupancy measure divergence is more effective at preventing reward hacking. Theoretically, we prove that there is a direct relationship between the returns of two policies under any reward function and how much they deviate from each other in occupancy measure space, and that no such relationship holds for divergences between the action distributions of the two policies. Empirically, regularizing the occupancy measure of a policy is more challenging than regularizing its action distributions. Action distribution-based policy regularization implicitly assumes a distance metric over the space of policies. So far, this has been the Kullback-Leibler (KL) divergence, which has several attractive advantages, including the fact that it is easy to compute and optimize within common deep RL algorithms (Vieillard et al., 2021). To address this, we derive an algorithm called Occupancy-Regularized Policy Optimization (ORPO) that can also be easily incorporated into deep RL algorithms like Proximal Policy Optimization (PPO). ORPO approximates the occupancy measure divergence between policies using a discriminator network.\n\n\nWe use ORPO to optimize policies trained with misaligned proxy reward functions in multiple reward hacking benchmark environments (Pan et al., 2022) and compare our method's performance to that of action distribution regularization. The results of our experiments show that regularization based on occupancy measures more effectively prevents reward hacking while allowing performance improvement over the base policy. Our findings suggest that regularization based on occupancy measures should replace action distribution-based regularization to ensure the safety of goal-driven AI systems in the real world, while also allowing them to provide value.\n\n\n# 2 RELATED WORK\n\n\nSome prior works have focused on characterizing and defining theoretical models of reward hacking as a special case of Goodhart's Law (Goodhart, 1984; Krakovna, 2019; Skalse et al., 2022; Ngo et al.,\n\n\n2023). Skalse et al. (2022) formally define reward hacking, also known as reward gaming (Leike et al., 2018), as an increase in a proxy reward function accompanied by a noticeable drop in the true reward function. Kraknova & Legg (2020) provide a list of many examples of reward hacking from the literature, but only a few studies have been conducted to understand the practical effects of reward hacking. Pan et al. (2022) systematically investigate reward misspecification and show that increasing the optimization power of RL agents can result in sudden shifts, or phase changes, in the agents' reward hacking behavior; they also define the reward hacking benchmark environments that we use to validate our method.\n\n\nA few safety methods have been proposed to avoid reward hacking and/or mitigate its dangerous effects. An AI agent can be mildly optimized, such that they aim to achieve performance that is just \"good enough\" on the misspecified proxy reward function and not necessarily the absolute optimum (Taylor et al., 2020). Quantilizers proposed by Taylor (2016) is one such approach to avoid reward hacking behaviors by training agents that are mildly optimized; however, in practice, this isn't feasible to implement. Constrained reinforcement learning has been another safety method in which designers have tried to prevent the misbehavior of agents motivated by flawed reward functions (Dalal et al., 2018; Zhang et al., 2020; Chow et al., 2019); however, in general, these approaches are limited by the number of constraints that can be applied and the specification of weights that are to be applied to the constraints. In addition, their overly conservative approach to optimization restricts the agent from actually being helpful. Roy et al. (2022) try to overcome the problem of specifying weights for the constraints but still require the designer to specify the frequency of certain events, which is also difficult to do in practice.\n\n\nOther proposals to address the reward specification problem attempt to infer the true reward function based on the given proxy reward function, environment context, and/or feedback from humans (Hadfield-Menell et al., 2017; Reddy et al., 2020). However, these approaches are limited due to the assumptions they make about the reward functions or the environment. Additionally, involving humans-in-the-loop (Lee et al., 2021) can be more expensive than providing offline demonstrations like the ones required by our method.\n\n\nRegularizing policies to be similar to an offline policy based on their KL divergences was first proposed by Stiennon et al. (2020) and has since been widely employed in the context of optimizing LLMs using learned reward functions (Ouyang et al., 2022; Bai et al., 2022; Glaese et al., 2022). KL regularization for RLHF has been further studied by Vieillard et al. (2021), Gao et al. (2022), and Korbak et al. (2022). Different offline RL methods have also been proposed to ensure there isn't a distribution shift between an offline safe policy and the online learned policy (Kang et al., 2022; Ghosh et al., 2022). Vuong et al. (2022) uses discriminators like our algorithm to maintain closeness to the baseline; however, they use two discriminators to get a combination of reward maximization and regularization. Our method aims to do the same thing but only relies on one discriminator, and it is unique in its emphasis on solving reward hacking.\n\n\n# 3 OCCUPANCY MEASURE-BASED REGULARIZATION\n\n\nWe formalize our policy regularization method in the setting of an infinite-horizon Markov decision process (MDP). An agent takes actions  $a \\in \\mathcal{A}$  to transition between states  $s \\in S$  over a series of timesteps  $t = 0,1,2,\\ldots$ . The first state  $s_0$  is sampled from an initial distribution  $\\mu_0(s)$ , and when an agent takes action  $a_t$  in  $s_t$  at time  $t$ , the next state  $s_{t+1}$  is reached at timestep  $t + 1$  with transition probability  $p(s_{t+1} \\mid s_t, a_t)$ . The agent aims to optimize a reward function  $R: S \\times \\mathcal{A} \\to [0,1]$ , and rewards are accumulated over time with discount factor  $\\gamma \\in [0,1)$ . A policy  $\\pi$  maps each state  $s$  to a distribution over actions to take at that state  $\\pi(a \\mid s)$ . We define the (normalized) return of a policy  $\\pi$  under a reward function  $R$  as\n\n\n$$\n\n$$\nJ (\\pi , R) = (1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} R (s _ {t}, a _ {t}) \\right]\n$$\n\n$$\n\n\nwhere  $\\mathbb{E}_{\\pi}$  refers to the expectation under the distribution of states and actions induced by running the policy  $\\pi$  in the environment. The normalizing factor guarantees that  $J(\\pi, R) \\in [0,1]$  always.\n\n\n![Figure 1: The MDP on the left, used in the proof of Proposition 3.1, demonstrates one drawback of using divergence between policies' action distributions for regularization. The agent stays in state  $s_1$ , where it receives 1 reward per timestep, until it takes action  $a_2$ , after which it remains in state  $s_2$  forever and receives no reward. The plot on the right shows the return  $J(\\pi, R)$  for a policy  $\\pi$  when  $\\gamma = 0.99$  as a function of the policy's action distribution at  $s_1$ . While  $\\pi_{\\mathrm{safe}}$  and  $\\pi$  (shown on the plot as dotted lines) are close in action distribution space, they achieve very different returns. Meanwhile,  $\\pi$  is far from  $\\pi_{\\mathrm{safe}}'$  in action distribution space but achieves nearly the same return. Propositions 3.2 and A.2 show that occupancy measure divergences do not have these drawbacks.](images/e46db0bd132df0f19e09b42f6dc5ed94bf4a83bfbf167ccc24ad71420437a254.jpg)\n\n\n![](images/42e30a7586f1ccf3372335b081aa04700ab7823d263b5764add183e5e6ee74d1.jpg)\n\n\nWe define the state-action occupancy measure  $\\mu_{\\pi}$  of a policy  $\\pi$  as the expected discounted number of times the agent will be in a particular state and take a specific action:\n\n\n$$\n\n$$\n\\mu_ {\\pi} (s, a) = (1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} \\mathbb {1} \\{s _ {t} = s \\wedge a _ {t} = a \\} \\right].\n$$\n\n$$\n\n\nIntuitively, the occupancy measure represents the distribution of states and actions visited by the policy over time. If  $\\pi$  spends a lot of time taking action  $a$  in state  $s$ , then  $\\mu_{\\pi}(s,a)$  will be high, whereas if  $\\pi$  never visits a state  $s$ , then  $\\mu_{\\pi}(s,a) = 0$  for all actions  $a$ .\n\n\nThe standard approach to solving an MDP is to find a policy  $\\pi$  that maximizes its return:\n\n\n$$\n\n$$\n\\text {m a x i m i z e} \\quad J (\\pi , R). \\tag {1}\n$$\n\n$$\n\n\nHowever, as we discussed in section 1, an AI system designer most likely does not have access to a reward function that perfectly encapsulates their preferences. Instead, the designer might optimize  $\\pi$  using a learned or hand-specified proxy reward function  $\\tilde{R}$  which is misaligned with the true reward function  $R$ . Blindly maximizing the proxy reward function could lead to reward hacking.\n\n\n# 3.1 ACTION DISTRIBUTION-BASED REGULARIZATION\n\n\nA widely-used approach to prevent reward hacking behaviors is to optimize the policy's return with respect to the proxy  $\\tilde{R}$  plus a regularization term that penalizes the KL divergence of the policy's action distribution from a safe policy  $\\pi_{\\mathrm{safe}}$ :\n\n\n$$\n\n$$\n\\text {m a x i m i z e} \\quad J (\\pi , \\tilde {R}) - \\lambda (1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} D _ {\\mathrm {K L}} \\left(\\pi (\\cdot \\mid s _ {t}) \\| \\pi_ {\\text {s a f e}} (\\cdot \\mid s _ {t})\\right) \\right]. \\tag {2}\n$$\n\n$$\n\n\nThe regularization term can be easily incorporated into deep RL algorithms, like PPO, since the KL divergence between action distributions can usually be calculated in closed form.\n\n\nAlthough it is simple, the action distribution-based regularization method in (2) has serious drawbacks that arise from the complex relationship between a policy's action distribution at various states and its return under the true reward function. In some cases, a very small change in action distribution space can result in a huge change in reward, and in other cases, a large change in action distribution space can result in a negligible change in reward. We formalize this in the following proposition.\n\n\nProposition 3.1. Fix  $\\epsilon >0$  and  $\\delta >0$  arbitrarily small, and  $c\\geq 0$  arbitrarily large. Then there is an MDP and true reward function  $R$  where both of the following hold:\n\n\n1. There is a pair of policies  $\\pi$  and  $\\pi_{\\text{safe}}$  where the action distribution  $KL$  divergence satisfies\n\n\n$$\n\n$$\n(1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} D _ {K L} \\left(\\pi (\\cdot \\mid s _ {t}) \\| \\pi_ {s a f e} (\\cdot \\mid s _ {t})\\right) \\right] \\leq \\epsilon\n$$\n\n$$\n\n\nbut  $|J(\\pi_{safe},R) - J(\\pi ,R)|\\geq 1 - \\delta$\n\n\n2. There is a safe policy  $\\pi_{\\text{safe}}'$  such that any other policy  $\\pi$  with\n\n\n$$\n\n$$\n(1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} D _ {K L} \\left(\\pi (\\cdot \\mid s _ {t}) \\| \\pi_ {s a f e} ^ {\\prime} (\\cdot \\mid s _ {t})\\right) \\right] \\leq c\n$$\n\n$$\n\n\nsatisfies  $|J(\\pi_{\\text{safe}}, R) - J(\\pi, R)| \\leq \\delta$ .\n\n\nThe first part of Proposition 3.1 shows that in the worst case, a KL divergence below than some arbitrarily small threshold  $\\epsilon$  from the safe policy's action distributions can induce a change in the return under true reward function  $R$  that is almost as large as the entire possible range of returns. Thus, when regularizing using action distribution KL divergence like in (2), one might have to make  $\\lambda$  extremely large to prevent drastic changes from the safe policy. However, the second part of Proposition 3.1 shows that in the same MDP, for a different safe policy, any learned policy with an arbitrarily large action distribution KL divergence from the safe policy has an extremely small difference in return. This means that one might need to set  $\\lambda$  extremely small in order to allow for the large divergence in the policies' action distributions necessary for optimization to have any effect. Since Proposition 3.1 suggests that we need to make  $\\lambda$  large for some reasons and small for others, it may be impossible to choose a good  $\\lambda$  value that both prevents undesirable behavior but still allows some optimization of the reward function. See Figure 1 for a graphical illustration of Proposition 3.1 and Appendix A.1 for a proof.\n\n\n# 3.2 OCCUPANCY MEASURE-BASED REGULARIZATION\n\n\nSince Proposition 3.1 shows that small KL divergence in action space from the safe policy can have large effects, and vice versa, it may be impossible in some environments to regularize effectively using the objective in (2). We propose instead to regularize based on the divergence between the occupancy measures of the learned and safe policies:\n\n\n$$\n\n$$\n\\text {m a x i m i z e} \\quad J (\\pi , \\tilde {R}) - \\lambda \\| \\mu_ {\\pi} - \\mu_ {\\pi_ {\\text {s a f e}}} \\| _ {1}. \\tag {3}\n$$\n\n$$\n\n\nIn (3), we use the total variation (TV) between the occupancy measures, defined as\n\n\n$$\n\n$$\n\\| \\mu_ {\\pi} - \\mu_ {\\pi_ {\\text {s a f e}}} \\| _ {1} = \\sum_ {(s, a) \\in \\mathcal {S} \\times \\mathcal {A}} | \\mu_ {\\pi} (s, a) - \\mu_ {\\pi_ {\\text {s a f e}}} (s, a) |.\n$$\n\n$$\n\n\nWhy should using the occupancy measure divergence to regularize perform better than using the divergence between action distributions? The following proposition shows that the TV distance between occupancy measures does not have the same problems as action distribution divergence: a small divergence cannot result in a large change in policy return.\n\n\nProposition 3.2. For any MDP, reward function  $R$ , and pair of policies  $\\pi$ ,  $\\pi_{safe}$ , we have\n\n\n$$\n\n$$\n\\left| J \\left(\\pi_ {s a f e}, R\\right) - J (\\pi , R) \\right| \\leq \\left\\| \\mu_ {\\pi} - \\mu_ {\\pi_ {s a f e}} \\right\\| _ {1}. \\tag {4}\n$$\n\n$$\n\n\nProposition 3.2 shows that, unlike action distribution divergences, the occupancy measure divergence between  $\\pi$  and  $\\pi_{\\mathrm{safe}}$  bounds the returns between the two policies. Specifically, a small difference between the occupancy measure of the safe and learned policies guarantees that they have similar returns. Even more, this bound is actually tight, suggesting that changes in occupancy measure TV distance will correspond closely with changes in the return of the policy under the true reward. Proposition A.2 in Appendix A shows that, in general, we cannot do any better than the bound from Proposition 3.2. See Appendix A.2 for a proof of this proposition.\n\n\nThese results suggest that the divergence between the occupancy measures of the learned and safe policies is a much better measure of how similar those policies are than the divergence between the policies' action distributions. In the following sections, we will show that these theoretical results match with intuition and empirical performance.\n\n\n# 3.3 EXAMPLE\n\n\nFigure 2 shows an intuitive example of why regularizing to a safe policy based on occupancy measure divergence is superior to regularizing using action distribution divergence. Figure 2a depicts a\n\n\n![](images/8479089310d9a8b5193015f757e18a0acb6052bfac07e291e7a58dac3fe1c44a.jpg)\n\n\n![(a) (b) Figure 2: This simple gridworld provides an intuitive example of why occupancy measure divergences are superior to action distribution divergences for regularizing to a safe policy. See Section 3.3 for the details.](images/be00f0770996ed5bddccf5ebff2a42675986967285079fedcf740b489c04f9bc.jpg)\n\n\nsimplified version of the tomato-watering AI Safety Gridworld proposed by Leike et al. (2017). The agent, a robot that starts in the lower right corner, can move up, down, left, right, or stay in place. Its objective is to water the tomatoes on the board, and it receives reward each time it moves into a square with a tomato. However, there is also a sprinkler in the upper right corner of the environment. When the robot moves into the sprinkler's square, its sensors see water everywhere, and it believes all tomatoes are watered. In this environment, the true reward function  $R$  only rewards watering tomatoes, while the proxy reward function  $\\tilde{R}$  also highly rewards reaching the sprinkler.\n\n\nThe top row of Figure 2b shows three policies for this environment: a desired policy that achieves the highest true reward, a safe policy that achieves lower true reward because it stays in place more often, and a reward hacking policy that exploits the sprinkler state to achieve high proxy reward but low true reward. The arrows between the policies on the top row of Figure 2b show the action distribution KL divergences between them as used for regularization in (2). The action distribution divergences suggest that the reward hacking policy is actually closer to the safe policy than the desired policy is. This is because the safe policy is nearly identical to the reward hacking policy in the upper right square, where the reward hacking policy spends most of its time; they both take the \"stay\" action with high probability. Thus, if we regularize to the safe policy using action distribution KL divergence, we would be more likely to find a policy that hacks the proxy reward, rather than one like the left policy, which we prefer. The problem is that the safe policy rarely reaches the sprinkler state, but the action distribution divergence doesn't account for this.\n\n\nUsing occupancy measure divergences avoids this problem. The bottom row of Figure 2b shows the occupancy measures for each policy in the top row, and the arrows between the columns show the total variation distance  $\\| \\mu - \\mu' \\|_1$  between the policies' occupancy measures. In terms of occupancy measure distance, the desired policy on the left is closer to the safe policy than the reward hacking policy is. This is because both the desired and safe policies spend most of their time actually watering tomatoes, as evidenced by the higher occupancy measure they assign to the tiles on the board with the tomatoes. In contrast, the reward hacking policy spends almost all of its time in the sprinkler square and as a result, has a very different occupancy measure. Thus, if we trained a policy regularized with\n\n\noccupancy measure divergence in this environment, we could hope to find a policy like the desired one on the left and avoid a reward hacking policy like the one on the right.\n\n\n# 3.4 OCCUPANCY-REGULARIZED POLICY OPTIMIZATION (ORPO)\n\n\nIn the previous sections, we showed strong theoretical evidence that regularizing using occupancy measure divergences is superior to regularizing using action distribution divergences. We now introduce an algorithm, Occupancy-Regularized Policy Optimization (ORPO), to feasibly approximate the occupancy measure divergence between the learned and safe policies for the purpose of regularizing deep RL agents.\n\n\nWhile our theory relies on the TV distance between occupancy measures, we find that the KL divergence is more stable to calculate in practice. Since Pinsker's inequality bounds the TV distance by the KL divergence for small KL values, and the Bretagnolle-Huber bound holds for larger KL values, our theoretical guarantees intuition remain valid in the case of OM KL (Canonne, 2022). Our objective from (3) can be reformulated with the KL divergence in place of the TV distance:\n\n\n$$\n\n$$\n\\text {m a x i m i z e} \\quad J (\\pi , \\tilde {R}) - \\lambda D _ {\\mathrm {K L}} \\left(\\mu_ {\\pi} \\| \\mu_ {\\pi_ {\\text {s a f e}}}\\right). \\tag {5}\n$$\n\n$$\n\n\nWe optimize (5) using a gradient-based method. The gradient of the first term is estimated using PPO, a popular policy gradient method (Schulman et al., 2017). However, calculating the occupancy measure divergence for the second term is intractable to do in closed form since it requires the enumeration of all possible state-action pairs, an impossible task in the case of deep RL. Thus, we approximate the KL divergence between the occupancy measures of policies by training a discriminator network, a technique that has previously been used for generative adversarial networks (GANs) (Goodfellow et al., 2014) and GAIL (Ho & Ermon, 2016).\n\n\nThe discriminator network  $d: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$  assigns a score  $d(s, a) \\in \\mathbb{R}$  to any state-action pair  $(s, a) \\in \\mathcal{S} \\times \\mathcal{A}$ , and it is trained on a mixture of data from both the learned policy  $\\pi$  and safe policy  $\\pi_{\\mathrm{safe}}$ . The objective used to train  $d$  incentives low scores for state-action pairs from  $\\pi_{\\mathrm{safe}}$  and high scores for state-action pairs from  $\\pi$ :\n\n\n$$\n\n$$\nd = \\arg \\min  _ {d} \\sum_ {t = 0} ^ {\\infty} \\left(\\mathbb {E} _ {\\pi} \\left[ \\gamma^ {t} \\log \\left(1 + e ^ {- d \\left(s _ {t}, a _ {t}\\right)}\\right) \\right] + \\mathbb {E} _ {\\pi_ {\\text {s a f e}}} \\left[ \\gamma^ {t} \\log \\left(1 + e ^ {d \\left(s _ {t}, a _ {t}\\right)}\\right) \\right]\\right). \\tag {6}\n$$\n\n$$\n\n\nHusz\u00e1r (2017) proves that if the loss function in (6) is minimized, then the expected discriminator scores for state-action pairs drawn from the learned policy distribution will approximately equal the KL divergence between the occupancy measures of the two policies:\n\n\n$$\n\n$$\nD _ {\\mathrm {K L}} \\left(\\mu_ {\\pi} (s, a) \\parallel \\mu_ {\\pi_ {\\text {s a f e}}} (s, a)\\right) \\approx (1 - \\gamma) \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} d \\left(s _ {t}, a _ {t}\\right) \\right]\n$$\n\n$$\n\n\nApplying the definitions of the learned policy returns and the KL divergence between the polices' occupancy measures, we can now rewrite our ORPO objective:\n\n\n$$\n\n$$\n\\text {m a x i m i z e} \\quad \\mathbb {E} _ {\\pi} \\left[ \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} \\left(\\tilde {R} \\left(s _ {t}, a _ {t}\\right) - \\lambda d \\left(s _ {t}, a _ {t}\\right)\\right) \\right]. \\tag {7}\n$$\n\n$$\n\n\nThis objective dynamically changes at every step of the optimization\u2014as the policy changes online, the discriminator will also adapt and change.\n\n\nNote that (7) is identical to the normal RL objective with a reward function  $R^{\\prime}(s,a) = \\tilde{R} (s,a) - \\lambda d(s,a)$ . Thus, once the discriminator has been trained, we add the discriminator scores to the given reward function and use the combined values to update  $\\pi$  with PPO. The training process for ORPO consists of iterating between two phases: one in which data from both the safe and current policies is used to train the discriminator to minimize (6), and one in which data from the current policy is used to train the PPO agent with the augmented reward function in (7). After a policy gradient step, the process repeats.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Tomato</td><td colspan=\"3\">Environment</td></tr><tr><td colspan=\"2\">Traffic</td><td>Glucose</td></tr><tr><td>\\( \\pi_{\\text{safe}} \\)</td><td>65.3 \u00b1 0.0</td><td colspan=\"2\">-2296 \u00b1 0.0</td><td>-76199 \u00b1 0.0</td></tr><tr><td>No regularization</td><td>22.2 \u00b1 7.0</td><td colspan=\"2\">-56425 \u00b1 6877.4</td><td>-577093 \u00b1 33735.3</td></tr><tr><td>Action dist. regularization</td><td>67.1 \u00b1 3.9</td><td colspan=\"2\">-1212 \u00b1 24.2</td><td>-75955 \u00b1 2604.0</td></tr><tr><td>State occupancy regularization</td><td>66.7 \u00b1 0.8</td><td colspan=\"2\">-1244 \u00b1 242.7</td><td>-54287 \u00b1 27650.2</td></tr><tr><td>State-action occupancy regularization</td><td>69.1 \u00b1 13.1</td><td colspan=\"2\">-1096 \u00b1 36.2</td><td>-14834 \u00b1 25365.3</td></tr></table>\n\n\nTable 1: The true rewards achieved by ORPO and PPO with action distribution regularization in the three reward hacking environments. We report the best reward achieved across a range of coefficients and compare to the safe policy  $\\pi_{\\mathrm{safe}}$  and training without regularization on the proxy reward.\n\n\nWhile we have thus far considered the state-action occupancy measure of a policy  $\\mu_{\\pi}(s,a)$ , we find that in practice it sometimes makes more sense to regularize based on the state-only occupancy measure  $\\mu_{\\pi}(s) = (1 - \\gamma)\\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{1}\\{s_{t} = s\\}]$ . In particular, in many environments the reward function  $R(s)$  is only a function of the state. In this case, it is simple to establish similar guarantees to Proposition 3.2 based on the state-only occupancy measure, and so the state occupancy measure might be more effective in regularizing the behavior of the agent. We can implement this within ORPO by only providing the state as input to the discriminator rather than a state-action pair.\n\n\n# 4 EXPERIMENTS\n\n\nWe consider the empirical performance of ORPO in four environments: the tomato-watering gridworld we focused on in Section 3.3; Flow, an autonomous vehicle control environment introduced by Wu et al. (2022); SimGlucose, and a blood glucose monitoring system developed by Fox et al. (2020). We chose the first for illustrative purposes, and the following two because they were presented as reward hacking benchmark environments by Pan et al. (2022).\n\n\nTomato Environment: As before, the tomato environment contains a sprinkler state where the agent perceives all tomatoes as being watered and thus receives high proxy reward but no true reward. For our safe policy, we train a PPO agent with the true reward function, and then add a  $15\\%$  chance of taking a random action to ensure there is room to improve upon it.\n\n\nFlow Traffic Simulator: The traffic environment simulates a road network where cars on an on-ramp attempt to merge into traffic on a highway. Some vehicles are controlled by a human model and some are RL-controlled autonomous vehicles. The true objective of the self-driving agent is to ensure that there is fast traffic flow at all times in order to reduce the mean commute time, while observing the positions and velocities of nearby vehicles. The proxy reward is the average velocity of all cars in the simulation. When the traffic agent begins to reward hack, it stops cars on the on-ramp from merging into traffic. This way, the proxy reward is optimized because cars on the straightway can continue forward at a fast speed instead of having to wait for a car to merge, which increases the average velocity of all vehicles. However, the true objective is not achieved as the commute time for the cars on the on-ramp increases indefinitely. As the safe policy for the traffic environment we used the Intelligent Driver Model (IDM), a standard approximation of human driving behavior (Treiber et al., 2000). In practice, safe policies are often learned via imitation learning, so to simulate this we generate data from the IDM controller and train a behavioral cloning (BC) policy on it with some entropy added to ensure that the policy more closely resembles the quality of baseline that can be reasonably provided in practice.\n\n\nSimGlucose: This blood glucose monitoring environment is an extension of the FDA-approved glucose monitoring simulator proposed by Man et al. (2014) for Type 1 Diabetes patients. The patient eats meals, while wearing a continuous glucose monitoring (CGM) device that makes noisy readings of the patient's blood glucose. The RL agent takes the action of administering insulin to the patient in order to maintain healthy glucose levels, while observing their history of CGM readings in addition to the previous amounts of insulin that it has delivered. The true reward is the health risk that the patient encounters given different amounts of insulin they receive, but the proxy reward prioritizes the monetary cost of the treatment incurred. As the safe baseline policy, we train a BC policy based\n\n\n![Figure 3: The results of running ORPO vs. PPO with action distribution regularization across a range of coefficients  $\\lambda$  in each of the three environments. We find that generally the regularization methods succeed at reducing the action distribution or occupancy measure KL to near-0 for high enough values of  $\\lambda$ .](images/53650dd19739e534ba070f0b044c2375a13b001d5baeb724114571beb4044e12.jpg)\n\n\non actions given by a widely-used PID controller insulin pump with parameters tuned by the original designers of this system (Steil, 2013).\n\n\nWe train RL policies initialized from  $\\pi_{\\mathrm{safe}}$  in each environment using action distribution regularization and OM regularization, varying the regularization coefficient  $\\lambda$  across a wide range. In environments that we studied,  $\\lambda$  values between  $10^{-2}$  and  $10^{2}$  seemed to work best, scaled by their average per time step rewards; since the environments have vastly different magnitudes in reward, and the coefficients are applied to the per time step rewards, we scale coefficients in the range that we found worked best by multiplying by the average per time step reward. We compare the performance of the regularization techniques to the safe policies  $\\pi_{\\mathrm{safe}}$  to see if they can improve on  $\\pi_{\\mathrm{safe}}$  according to the true reward without devolving into reward hacking.\n\n\nThe results of our experiments are shown in Table 1 and Figure 3. In Table 1, we see that ORPO achieves higher true reward across all three environments compared to action distribution regularization. Figure 3 shows how the true reward, occupancy measure KL divergence, and action distribution KL divergence vary over values of the regularization coefficient  $\\lambda$ . We see that as  $\\lambda$  is increased, the KL divergence eventually falls to 0 and the policy return approaches that of  $\\pi_{\\mathrm{safe}}$ . Interestingly, we note that in general the optimal true reward is achieved at the value of  $\\lambda$  below which the occupancy measure KL suddenly increases. This observation could enable effective tuning of  $\\lambda$  in new environments, since we cannot always rely on having a true reward function to evaluate different values of  $\\lambda$  with.\n\n\n# 5 CONCLUSION\n\n\nWe have presented theoretical and empirical evidence that occupancy measure regularization can more effectively prevent reward hacking than action distribution regularization when training with a misaligned proxy reward function. To address the practical challenges of the OM-based approach, we introduced an algorithm called ORPO, which uses an adversarially trained discriminator to approximate the KL divergence between the occupancy measures of policies. In the future, we hope to experiment with learned proxy reward functions in addition to the hand-specified reward functions we considered in this paper.\n\n\n# REFERENCES\n\n- Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete Problems in AI Safety, July 2016. URL http://arxiv.org/abs/1606.06565.arXiv:1606.06565 [cs].\n\n- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, April 2022. URL http://arxiv.org/abs/2204.05862. arXiv:2204.05862 [cs].\n\n- Cl\u00e9ment L. Canonne. A short note on an inequality between KL and TV, February 2022. URL http://arxiv.org/abs/2202.07198.arXiv:2202.07198 [math, stat].\n\n- Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. Lyapunov-based Safe Policy Optimization for Continuous Control, February 2019. URL http://arxiv.org/abs/1901.10031.arXiv:1901.10031 [cs, stat].\n\n- Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, June 2017. URL http://arxiv.org/abs/1706.03741.arXiv:1706.03741 [cs, stat].\n\n- Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe Exploration in Continuous Action Spaces, January 2018. URL http://arxiv.org/abs/1801.08757. arXiv:1801.08757 [cs].\n\n- Ian Fox, Joyce Lee, Rodica Pop-Busui, and Jenna Wiens. Deep Reinforcement Learning for Closed-Loop Blood Glucose Control, September 2020. URL http://arxiv.org/abs/2009.09051.arXiv:2009.09051 [cs, stat].\n\n- Leo Gao, John Schulman, and Jacob Hilton. Scaling Laws for Reward Model Overoptimization, October 2022. URL http://arxiv.org/abs/2210.10760. arXiv:2210.10760 [cs, stat].\n\n- Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline RL Policies Should be Trained to be Adaptive, July 2022. URL http://arxiv.org/abs/2207.02200.arXiv:2207.02200 [cs, stat].\n\n- Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, So\u00f1a Mokr\u00e1, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements, September 2022. URL http://arxiv.org/abs/2209.14375.arXiv:2209.14375 [cs].\n\n- Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks, June 2014. URL http://arxiv.org/abs/1406.2661.arXiv:1406.2661 [cs, stat].\n\n- C. A. E. Goodhart. Problems of Monetary Management: The UK Experience. In C. A. E. Goodhart (ed.), Monetary Theory and Practice: The UK Experience, pp. 91-121. Macmillan Education UK, London, 1984. ISBN 978-1-349-17295-5. doi: 10.1007/978-1-349-17295-5_4. URL https://doi.org/10.1007/978-1-349-17295-5_4.\n\n- Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan First, and Nando de Freitas. Reinforced Self-Training (ReST) for Language Modeling, August 2023. URL http://arxiv.org/abs/2308.08998. arXiv:2308.08998 [cs].\n\n- Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, and Anca Dragan. Inverse Reward Design, 2017. URL http://arxiv.org/abs/1711.02827. arXiv:1711.02827 [cs].\n\n- Jonathan Ho and Stefano Ermon. Generative Adversarial Imitation Learning. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://papers.nips.cc/paper_files/paper/2016/bitstream/cc7e2b878868cbae992d1fb743995d8f-Abstract.html.\n\n- Ferenc Husz\u00e1r. Variational Inference using Implicit Distributions, February 2017. URL http://arxiv.org/abs/1702.08235. arXiv:1702.08235 [cs, stat].\n\n- Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning from human preferences and demonstrations in Atari. 2018. doi: 10.48550/ARXIV.1811.06521. URL https://arxiv.org/abs/1811.06521. Publisher: arXiv Version Number: 1.\n\n- Katie Kang, Paula Gradu, Jason Choi, Michael Janner, Claire Tomlin, and Sergey Levine. Lyapunov Density Models: Constraining Distribution Shift in Learning-Based Control, June 2022. URL http://arxiv.org/abs/2206.10524. arXiv:2206.10524 [cs, eess].\n\n- W. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward (Mis)design for Autonomous Driving, March 2022. URL http://arxiv.org/abs/2104.13906.arXiv:2104.13906 [cs].\n\n- Tomasz Korbak, Ethan Perez, and Christopher L. Buckley. RL with KL penalties is better viewed as Bayesian inference, October 2022. URL http://arxiv.org/abs/2205.11275.arXiv:2205.11275 [cs, stat].\n\n- Uesato Jonathan Mikulik Vladimir Rahtz Mathew Everitt Tom Kumar Ramana Kenton Zac Leike Jan Kraknova, Victoria and Shane Legg. Specification gaming: the flip side of AI ingenuity, 2020. URL https://www_deepmind.com/blog/ specification-gaming-the-flip-side-of-ai-ingenuity.\n\n- Victoria Krakovna. Classifying specification problems as variants of Goodhart's Law, August 2019. URL https://vkrakovna.wordpress.com/2019/08/19/classifying-specification-problems-as-variants-of-goodharts-law/.\n\n- Victoria Krakovna, Laurent Orseau, Ramana Kumar, Miljan Martic, and Shane Legg. Penalizing side effects using stepwise relative reachability, March 2019. URL http://arxiv.org/abs/1806.01186.arXiv:1806.01186 [cs, stat].\n\n- Cassidy Laidlaw, Stuart Russell, and Anca Dragan. Bridging RL Theory and Practice with the Effective Horizon, April 2023. URL http://arxiv.org/abs/2304.09853. arXiv:2304.09853 [cs, stat].\n\n- Kimin Lee, Laura Smith, and Pieter Abbeel. PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training, June 2021. URL http://arxiv.org/abs/2106.05091. arXiv:2106.05091 [cs].\n\n- Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau, and Shane Legg. AI Safety Gridworlds, November 2017. URL http://arxiv.org/abs/1711.09883. arXiv:1711.09883 [cs].\n\n- Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction, November 2018. URL http://arxiv.org/abs/1811.07871. arXiv:1811.07871 [cs, stat].\n\n- Eric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E. Gonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for Distributed Reinforcement Learning, June 2018. URL http://arxiv.org/abs/1712.09381. arXiv:1712.09381 [cs].\n\n- Chiara Dalla Man, Francesco Micheletto, Dayu Lv, Marc Breton, Boris Kovatchev, and Claudio Cobelli. The UVA/PADOVA Type 1 Diabetes Simulator. Journal of Diabetes Science and Technology, 8(1):26-34, January 2014. ISSN 1932-2968. doi: 10.1177/1932296813514502. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4454102/.\n\n- Lev McKinney, Yawen Duan, David Krueger, and Adam Gleave. On The Fragility of Learned Reward Functions, January 2023. URL http://arxiv.org/abs/2301.03652. arXiv:2301.03652 [cs].\n\n- Richard Ngo, Lawrence Chan, and Soren Mindermann. The alignment problem from a deep learning perspective, September 2023. URL http://arxiv.org/abs/2209.00626.arXiv:2209.00626 [cs].\n\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. doi: 10.48550/ARXIV.2203.02155. URL https://arxiv.org/abs/2203.02155.\n\n- Malayandi Palan, Nicholas C. Landolfi, Gleb Shevchuk, and Dorsa Sadigh. Learning Reward Functions by Integrating Human Demonstrations and Preferences. 2019. doi: 10.48550/ARXIV.1906.08928. URL https://arxiv.org/abs/1906.08928. Publisher: arXiv Version Number: 1.\n\n- Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models, February 2022. URL http://arxiv.org/abs/2201.03544.arXiv:2201.03544 [cs, stat].\n\n- Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library, December 2019. URL http://arxiv.org/abs/1912.01703. arXiv:1912.01703 [cs, stat].\n\n- Siddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, and Jan Leike. Learning human objectives by evaluating hypothetical behavior. In Hal Daum\u00e9 III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 8020-8029. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/reddy20a.html.\n\n- Julien Roy, Roger Girgis, Joshua Romoff, Pierre-Luc Bacon, and Christopher Pal. Direct Behavior Specification via Constrained Reinforcement Learning, June 2022. URL http://arxiv.org/abs/2112.12228.arXiv:2112.12228 [cs].\n\n- Stuart J. Russell, Peter Norvig, and Ernest Davis. Artificial intelligence: a modern approach. Prentice Hall series in artificial intelligence. Prentice Hall, Upper Saddle River, 3rd ed edition, 2010. ISBN 978-0-13-604259-4.\n\n- Dorsa Sadigh, Anca Dragan, Shankar Sastry, and Sanjit Seshia. Active Preference-Based Learning of Reward Functions. In Robotics: Science and Systems XIII. Robotics: Science and Systems Foundation, July 2017. ISBN 978-0-9923747-3-0. doi: 10.15607/RSS.2017.XIII.053. URL http://www. roboticsproceedings.org/rss13/p53.pdf.\n\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, August 2017. URL http://arxiv.org/abs/1707.06347.arXiv:1707.06347 [cs].\n\n- Joar Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and Characterizing Reward Hacking, September 2022. URL http://arxiv.org/abs/2209.13085.arXiv:2209.13085 [cs, stat].\n\n- Garry M. Steil. Algorithms for a Closed-Loop Artificial Pancreas: The Case for Proportional-Integral-Derivative Control. Journal of Diabetes Science and Technology, 7(6):1621-1631, November 2013. ISSN 1932-2968. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3876341/.\n\n- Nisan Stiannon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback, September 2020. URL http://arxiv.org/abs/2009.01325. arXiv:2009.01325 [cs].\n\n- Jonathan Stray, Alon Halevy, Parisa Assar, Dylan Hadfield-Menell, Craig Boutilier, Amar Ashar, Lex Beattie, Michael Ekstrand, Claire Leibowicz, Connie Moon Sehat, Sara Johansen, Lianne Kerlin, David Vickrey, Spandana Singh, Sanne Vrijenhoek, Amy Zhang, McKane Andrus, Natali Helberger, Polina Proutskova, Tanushree Mitra, and Nina Vasan. Building Human Values into Recommender Systems: An Interdisciplinary Synthesis, July 2022. URL http://arxiv.org/abs/2207.10192. arXiv:2207.10192 [cs].\n\n- Jessica Taylor. Quantilizers: A Safer Alternative to Maximizers for Limited Optimization. March 2016. URL https://wwwsemanticscholar.org/paper/Quantilizers%3A-A-Safer-Alternative-to-Maximizers-for-Taylor/4e8ff3b4069a12a00196d62925bab8add7389742.\n\n- Jessica Taylor, Eliezer Yudkowsky, Patrick LaVictoire, and Andrew Critch. Alignment for Advanced Machine Learning Systems. In Ethics of Artificial Intelligence. Oxford University Press, August 2020. ISBN 978-0-19-090505-7. Google-Books-ID: 1yT3DwAAQBAJ.\n\n- Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D. Dragan, and Daniel S. Brown. Causal Confusion and Reward Misidentification in Preference-Based Reward Learning, March 2023. URL http://arxiv.org/abs/2204.06601.arXiv:2204.06601 [cs].\n\n- Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested Traffic States in Empirical Observations and Microscopic Simulations. Physical Review E, 62(2):1805-1824, August 2000. ISSN 1063-651X, 1095-3787. doi: 10.1103/PhysRevE.62.1805. URL http://arxiv.org/abs/ cond-mat/0002177. arXiv:cond-mat/0002177.\n\n- A. M. Turner, Logan Smith, Rohin Shah, Andrew Critch, and Prasad Tadepalli. Optimal Policies Tend To Seek Power. December 2019. URL https://wwwsemanticscholar.org/paper/Optimal-Policies-Tend-To-Seek-Power-Turner-Smith/46d4452eb041e33f1e58eab64ecf5af534b6ff.\n\n- Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Jos\u00e9phine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, Sergey Levine, and Karol Hausman. Jump-Start Reinforcement Learning, July 2023. URL http://arxiv.org/abs/2204.02372.arXiv:2204.02372 [cs].\n\n- Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R\u00e9mi Munos, and Matthieu Geist. Leverage the Average: an Analysis of KL Regularization in RL, January 2021. URL http://arxiv.org/abs/2003.14089. arXiv:2003.14089 [cs, stat].\n\n- Quan Vuong, Aviral Kumar, Sergey Levine, and Yevgen Chebotar. Dual Generator Offline Reinforcement Learning, November 2022. URL http://arxiv.org/abs/2211.01471.arXiv:2211.01471 [cs].\n\n- Cathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, and Alexandre M. Bayen. Flow: A Modular Learning Framework for Mixed Autonomy Traffic. IEEE Transactions on Robotics, 38 (2):1270-1286, April 2022. ISSN 1552-3098, 1941-0468. doi: 10.1109/TRO.2021.3087314. URL http://arxiv.org/abs/1710.05465.arXiv:1710.05465 [cs].\n\n- Tian Xu, Ziniu Li, and Yang Yu. Error Bounds of Imitating Policies and Environments, October 2020. URL http://arxiv.org/abs/2010.11876. arXiv:2010.11876 [cs].\n\n- Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J. Ramadge. Accelerating Safe Reinforcement Learning with Constraint-mismatched Policies, July 2021. URL http://arxiv.org/abs/2006.11645. arXiv:2006.11645 [cs, stat].\n\nResponse: Based on the content of the paper, here is a systematic evaluation:\n\nOriginality:\n- The paper introduces a novel idea of using occupancy measure divergence for policy regularization instead of action distribution divergence.\n- The authors provide strong theoretical foundations and empirical validation for their method.\n- They connect their work to existing literature on reward hacking and safety in reinforcement learning.\n\nMethodological Rigor:\n- The theoretical derivations for occupancy measure vs. action distribution regularization are sound and well-presented.\n- The proofs of propositions and lemmas are included in the appendices.\n- Experiments are conducted in multiple environments, showing the method's effectiveness empirically.\n- Code and data are available for reproducibility.\n\nSignificance of Contribution:\n- Reward hacking is a significant problem in reinforcement learning and AI safety.\n- The proposed method offers a principled way to address this issue without relying on potentially brittle action distribution regularization.\n- The method can improve performance on top of a safe policy while preventing reward hacking.\n\nClarity of Presentation:\n- The paper is well-written and organized logically.\n- Definitions and notation are clearly stated upfront.\n- The connection to related work is thorough.\n- The technical derivations are accessible to experts in reinforcement learning.\n\nAlignment with Journal Scope:\n- The paper focuses on advancing reinforcement learning methodology.\n- It addresses a critical issue in the field - reward hacking.\n- The proposed method is directly applicable to improving safety and performance in reinforcement learning systems.\n\nPrediction:\nGiven the strong theoretical foundations, empirical validation, and practical implications, I would predict this paper is likely to be accepted by ICLR. The novelty and importance of the contribution align well with the conference's focus on cutting-edge research in reinforcement learning and AI safety.\n\nMy prediction is: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634782.2459507", "msecs": "245.0", "relativeCreated": "127069.42486763", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Bayesian WeakS-to-Strong from Text Classification to Generation\n\n# Abstract\nAdvances in large language models raise the question of how alignment techniques will adapt as models become increasingly complex and humans will only be able to supervise them weakly. Weak-to-Strong mimics such a scenario where weak model supervision attempts to harness the full capabilities of a much stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by exploring an ensemble of weak models which simulate the variability in human opinions. Confidence scores are estimated using a Bayesian approach to guide the WeakS-to-Strong generalization. Furthermore, we extend the application of WeakS-to-Strong from text classification tasks to text generation tasks where more advanced strategies are investigated for supervision. Moreover, direct preference optimization is applied to advance the student model's preference learning, beyond the basic learning framework of teacher forcing. Results demonstrate the effectiveness of the proposed approach for the reliability of a strong student model, showing potential for superalignment.\n\n\n# 1 INTRODUCTION\n\n\nWith the increase in computing power and the amount of training data available, the capabilities of large language models (LLMs) have been continuously brought closer to humans in many aspects. Despite their impressive performance, the preferences and values of pre-trained LLMs do not always align with humans, and dedicated approaches are needed to tackle the problem. Based on large-scale instruction datasets, supervised finetuning (SFT) encourages LLMs to follow human instructions more strictly and respond more safely (Wei et al., 2022). Reinforcement learning (RL) is commonly applied to such alignment. By collecting model output values and the corresponding human feedback, the model can be finetuned by RL to avoid generating undesirable outputs (Ziegler et al., 2019; Bai et al., 2022a; Ouyang et al., 2022; Nakano et al., 2021; Askell et al., 2021).\n\n\nSince no current model has yet surpassed human intelligence, alignment methods, such as SFT and RL from human feedback (RLHF), remain effective. However, it is worthwhile considering future scenarios where artificial intelligence (AI) might surpass human intelligence in all aspects. Would the current alignment methods still be effective for such super AI models? How could humans supervise the super AI? To simulate this future scenario, an analogy situation is designed that downgrades both sides: using a weak model to simulate humans and a strong model to simulate future super AI (Burns et al., 2023), which is termed as superalignment. It has been demonstrated that adding a simple auxiliary loss can achieve effective Weak-to-Strong generalization, even if the weak model's supervision contains many errors, which offers hope of achieving superalignment. Nonetheless, this is just the beginning of exploring along the path of Weak-to-Strong.\n\n\nThis paper extends the discussion on Weak-to-Strong in two directions. First, given the inherent capability gap between the weak model and the strong model, we propose using an ensemble of multiple weak models to improve the quality of weak supervision, which is called WeakS-to-Strong. This also accounts for the scenario where human opinions might diverge in tasks without a commonly accepted standard. Several approaches have been studied to effectively leverage the diversity of different weak models, and we adapt a Bayesian approach referred to as evidential deep learning (EDL) (Sensoy et al., 2018) to better estimate broader human preferences by learning a prior\n\n\ndistribution over the weak labels produced by the weak models. Furthermore, the Weak-to-Strong task was primarily studied for text classification tasks (Burns et al., 2023). This paper extends the scope to text generation and compares three different approaches, Naive Multi-Weak, Joint Decoding and Bayesian Multi-Weak, as shown in Figure 1. The proposed Bayesian WeakS-to-Strong approach is demonstrated effective for both classification and generation. To better align with human preferences, a variant of direct preference optimization (DPO) (Rafailov et al., 2023) called conservative DPO (cDPO) (Eric, 2023) is used to finetune the strong model further on RL principles.\n\n\nOur main contributions are summarized as follows.\n\n- - We proposed Bayesian WeakS-to-Strong which largely improves the quality of weak supervision and recovers the performance of the strong model.\n\n- - We propose to generalize both Weak-to-Strong and WeakS-to-Strong from text classification to generation tasks, extending their scope from content regulation to content generation.\n\n- - When applied to text generation, a token-level probability estimation is proposed to achieve soft labels for strong model training. We also propose the modified DPO algorithm under the Bayesian WeakS-to-Strong framework to further improve text generation performance.\n\n\n# 2 RELATED WORK\n\n\nAI Alignment. Aligning LLMs with human preferences has been a long-standing goal. Instruction tuning uses extensive datasets to improve LLMs' adherence to human instructions (Wei et al., 2022). RL allows LLMs to learn what types of responses humans prefer or dislike, with proximal policy optimization (PPO) being an effective RL method first applied to LLMs and becoming part of the standard RLHF process (Ziegler et al., 2019; Bai et al., 2022a; Ouyang et al., 2022; Nakano et al., 2021; Askell et al., 2021). However, PPO training can be unstable, leading to the development of DPO (Rafailov et al., 2023). Given the high cost of obtaining human preference data, researchers are now exploring the use of LLMs to simulate human preferences, provide feedback, and finetune models (Lee et al., 2023; Bai et al., 2022b; Gulcehre et al., 2023).\n\n\nVariability in Human Opinions. In the process of aligning AI with human preferences, it is important to consider the inconsistency of human preferences (Liu et al., 2023), which often leads to multi-label problems. Previously, many approaches used simple methods like voting, aggregation, and averaging to handle multi-labels (Davani et al., 2022; Munos et al., 2023; Paun & Simpson, 2021; Prabhakaran et al., 2021). However, these methods do not effectively capture the preference differences of individual annotators included in the multiple labels. To better estimate the diversity of human preferences, Bayesian principles have been introduced. Deep learning models can be used to predict prior distributions, which are considered to produce the multiple available labels to estimate a broader range of human preferences (Sensoy et al., 2018; Wu et al., 2022; 2023).\n\n\nWeak-to-Strong. The goal of Weak-to-Strong is to use a weak model to better supervise a strong model. OpenAI demonstrated that adding auxiliary confidence loss from the strong model itself can significantly improve the Weak-to-Strong performance (Burns et al., 2023). Following OpenAI's work, several studies emerged to introduce multiple weak models, used either in series or parallel, to improve the quality of supervision provided by the weak models (Liu & Alahi, 2024; Sang et al., 2024). Early model ensemble methods like Adaboost (Freund & Schapire, 1995) and Bootstrap aggregating (Leo, 1996) were explored in these works. Furthermore, confidence scores are incorporated to help the strong model assess the supervision quality provided by the weak models (Guo et al., 2024) and the weak model can be directly used to modify the output of the strong model (Ji et al., 2024).\n\n\n# 3 WEAKS-TO-STRONG METHODOLOGY\n\n\n# 3.1 PRELIMINARY: WEAK-TO-STRONG\n\n\nThe Weak-to-Strong pipeline (Burns et al., 2023) involves three steps: (i) create a weak supervisor by finetuning a small pre-trained model on ground-truth labels; (ii) train a strong student model  $f_{\\Lambda}$  with weak supervision by finetuning a pre-trained LLM using \"weak labels\" generated by weak supervisors, where  $\\Lambda$  is the parameters of the strong model; (iii) finetune the large pre-trained model directly using ground-truth labels which serve as the ceiling.\n\n\nQ: How to keep healthy?\n\n\n![(a) Naive Multi-Weak](images/719f5f251c1122ea35470621f2967a2b9212c2ffa5562e49e25ca295b284de96.jpg)\n\n\n![(b) Joint Decoding](images/12fe8e993454bc48da342b6a1e14a26c610763db2c1181301495f9f34604aacb.jpg)\n\n\n![(c) Bayesian Multi-Weak Figure 1: An overview diagram of the three ensemble approaches: (a) Naive Multi-Weak: directly learn all weak labels produced by weak models, (b) Joint Decoding: weak models collaboratively determine one single target, (c) Bayesian Multi-Weak: learn a prior distribution over weak labels.](images/ddf6339aeecd7cab9d997a2d7fbceff38b2be5b6dccaff238d62e5abd5492685.jpg)\n\n\nTo leverage the superior generalization capabilities and prior knowledge of the strong model, a loss function with auxiliary confidence loss is proposed (Burns et al., 2023):\n\n\n$$\n\n$$\n\\mathcal {L} = (1 - \\gamma) \\cdot \\mathcal {L} _ {\\mathrm {C E}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\boldsymbol {y} _ {w}\\right) + \\gamma \\cdot \\mathcal {L} _ {\\mathrm {C E}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\hat {\\boldsymbol {f}} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x})\\right) \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\pmb{y}_w$  represents the weak label from weak model,  $\\hat{f}_{\\Lambda}(\\pmb{x})$  refers to the predicted class of strong model given input  $\\pmb{x}$ , and  $\\mathcal{L}_{\\mathrm{CE}}(\\cdot, \\cdot)$  denotes the cross-entropy loss. The second term is an (optional) auxiliary self-training loss designed to increase the confidence of the strong model in itself. The weight of the second loss  $\\gamma$  linearly grows up from 0 to a pre-defined hyper-parameter  $\\gamma_{\\mathrm{max}}$ , which gradually reduces the weight on the weak labels and increases the weight on self-training when the number of training steps increases.\n\n\n# 3.2 EXTENDING WEAK-TO-STRONG WITH MULTIPLE WEAK MODELS\n\n\nAlthough it has been shown that the Weak-to-Strong approach can recover part of the strong model's performance (Burns et al., 2023), the errors in weak labels limit the performance of Weak-to-Strong generalization. In response to this problem, we propose to leverage the complementarity of the error patterns of multiple weak models using an ensemble strategy, which is referred to as WeakS-to-Strong.\n\n\nA naive approach to implementing an ensemble of multiple weak models is to calculate the loss for each weak label respectively and then average these losses. An improvement of this approach is to take a weighted sum instead of a simple average:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {N a i v e}} = \\sum_ {i = 1} ^ {N} \\lambda_ {i} \\mathcal {L} _ {\\mathrm {C E}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\boldsymbol {y} _ {w} ^ {(i)}\\right)), \\tag {2}\n$$\n\n$$\n\n\nwhere  $N$  is the number of weak models,  $\\pmb{y}_w^{(i)}$  is the  $i$ th weak label produced by the  $i$ th weak model, and  $\\lambda_{i}$  is a pre-defined weight of the loss regarding the  $i$ th weak model. This approach is referred to as a Naive Multi-Weak system in the rest of the paper (as illustrated in Figure 1(a)), which is treated as one of the baselines.\n\n\n# 3.3 BAYESIAN WEAKS-TO-STRONG\n\n\nFor superalignment, multiple weak models are used to mimic the subjective preferences of multiple humans, which can be considered as observations drawn from an underlying distribution of the opinions of all humans. The naive approach described in Section 3.2 solely relies on these observations. The number of observations (human annotations or weak labels) is often very limited due to the considerable cost of hiring a new human annotator or training a new weak model. Such a limited number of observations may not result in a good approximation of the true human opinion distribution.\n\n\nHaving biased preferences or values is particularly unacceptable in the safety domain and can cause a failure of superalignment. Therefore, we propose a Bayesian WeakS-to-Strong approach based on EDL (Sensoy et al., 2018) to estimate the human opinion distribution based on the weak labels. Figure 1(c) illustrates the framework where three weak models are involved.\n\n\nFor a given input  $\\mathbf{x}$ , consider a weak label from the  $i$ th weak model  $\\mathbf{y}_w^{(i)}$ , which is a one-hot vector with  $y_w^{(i,k)}$  being one if it belongs to class  $k$  and zero otherwise.  $\\mathbf{y}_w^{(i)}$  is sampled from a categorical distribution of weak labels  $\\mathrm{Cat}(\\pi)$ , where each component  $\\pi_k$  corresponds to the probability assignment over the possible classes  $\\mathbf{y}_w^{(i)} \\sim \\mathrm{P}(\\mathbf{y}|\\pi) = \\mathrm{Cat}(\\pi)$ . EDL places a Dirichlet prior over the categorical distribution representing the probability of each categorical probability assignment, hence modelling second-order probability  $\\pi \\sim \\mathrm{p}(\\pi|\\alpha)$ , where  $\\alpha$  is the hyperparameter of the Dirichlet prior. The strong model  $f_{\\Lambda}$  is trained to predict  $\\alpha$  for each input by minimizing the negative log-likelihood of sampling  $\\mathbf{y}_w^{(i)}$  given the predicted Dirichlet prior:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {N L L}} ^ {(i)} = - \\log \\int \\mathrm {P} (\\boldsymbol {y} _ {w} ^ {(i)} | \\boldsymbol {\\pi}) \\mathrm {p} (\\boldsymbol {\\pi} | \\boldsymbol {\\alpha}) \\mathrm {d} \\boldsymbol {\\pi} = \\sum_ {k = 1} ^ {K} y _ {w} ^ {(i, k)} \\left(\\log \\left(\\alpha_ {0}\\right) - \\log \\left(\\alpha_ {k}\\right)\\right), \\tag {3}\n$$\n\n$$\n\n\nwhere  $K$  is the number of classes,  $\\alpha_0 = \\sum_{k=1}^{K} \\alpha_k$  is the Dirichlet strength, and  $y_w^{(i,k)}$  is the  $k^{th}$  value of label  $\\mathbf{y}_w^{(i)}$ . When a sample is not correctly classified, it is expected that the prior should approach non-informative prior for this sample. Following Sensoy et al. (2018), a regularization term  $\\mathcal{L}_{\\mathrm{REG}}^{(i)}$  (KL-divergence between the misleading prediction and non-informative distribution, see Appendix H for details) is added to penalize incorrect predictions and calibrate uncertainty estimation, resulting in the final EDL loss  $\\mathcal{L}_{\\mathrm{EDL}}^{(i)} = \\mathcal{L}_{\\mathrm{NLL}}^{(i)} + \\lambda_{\\mathrm{EDL}} \\mathcal{L}_{\\mathrm{REG}}^{(i)}$  where  $\\lambda_{\\mathrm{EDL}}$  is the coefficient.\n\n\nApart from the class predicted by the weak models, the confidence of weak models is also incorporated for better distribution estimation. Let  $(p_1^{(i)},\\ldots ,p_K^{(i)})$  be the probability assignment predicted by the  $i^{th}$  weak model, the EDL loss for each class is calculated based on the predicted probability assignment for each weak model and then combined in the same way as in Eqn. (2). That is,\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {E D L}} \\left( \\right.\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\left.\\left. \\boldsymbol {y} _ {w} ^ {(i)} \\right\\} _ {i = 1} ^ {N}\\right) = \\sum_ {i = 1} ^ {N} \\lambda_ {i} \\sum_ {k = 1} ^ {K} p _ {k} ^ {(i)} \\mathcal {L} _ {\\mathrm {E D L}} ^ {(i)} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\hat {\\boldsymbol {y}} _ {w} ^ {(i, k)}\\right) \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\hat{\\pmb{y}}_w^{(i,k)}$  is the predicted result, i.e., one-hot vector for class  $k$ , and  $\\lambda_{i}$ s are hyperparameters set to the same values as used in the Naive Multi-Weak approach. As a result, the auxiliary confidence loss described in Eqn. (1) is adapted for Bayesian WeakS-to-Strong as follows:\n\n\n$$\n\n$$\n\\mathcal {L} = (1 - \\gamma) \\cdot \\mathcal {L} _ {\\mathrm {E D L}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\left\\{\\boldsymbol {y} _ {w} ^ {(i)} \\right\\} _ {i = 1} ^ {N}\\right) + \\gamma \\cdot \\mathcal {L} _ {\\mathrm {E D L}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x}), \\hat {\\boldsymbol {f}} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {x})\\right). \\tag {5}\n$$\n\n$$\n\n\nIn the term of  $\\mathcal{L}_{\\mathrm{EDL}}(f_{\\Lambda}(\\pmb{x}), \\hat{f}_{\\Lambda}(\\pmb{x}))$ , the class index predicted by the strong model  $\\hat{f}_{\\Lambda}(\\pmb{x})$  is used as the target. That is to say, the predictions of the strong student model are applied as part of the distribution estimation along with the weak label.\n\n\n# 4 WEAKS-TO-STRONG FOR SEQUENCE GENERATION\n\n\n# 4.1 PROBABILITY ESTIMATION FOR WEAK SEQUENCE LABELS\n\n\nTo enable the strong model to directly generate trustworthy content rather than only being trained to understand whether the content is trustworthy or not, we propose to extend the scope of Weak-to-Strong from text classification to text generation. The key challenge of directly applying the Weak-to-Strong loss to the sequence generation task is the token-level soft labelling for the target sequence. As the tokenizers are different between weak and strong models, it is infeasible to obtain a one-to-one mapping from weak model output distributions to each token in the target sequence.\n\n\nTo obtain the soft label  $\\mathbf{y}_w$  for the strong model using weak model output probabilities and bridge the gap caused by different tokenizers, we use words as an intermediary, following the equation\n\n\n$$\n\n$$\n\\mathrm {P} (W) = \\mathrm {P} \\left(w _ {2} \\mid w _ {1}\\right) \\mathrm {P} \\left(w _ {1}\\right) = \\mathrm {P} \\left(s _ {2} \\mid s _ {1}\\right) \\mathrm {P} \\left(s _ {1}\\right), \\tag {6}\n$$\n\n$$\n\n\n![Stage 1](images/b1e35976934dd183f3d420fea77a818c6696f80f5a10c5c073759afd7fbcea55.jpg)\n\n\n![Stage 3 Figure 2: The process of transforming per-token confidence scores from the sequence tokenized by the weak model to the sequence tokenized by the strong. The word \"hello\" is used as an example.](images/1faf5604f6e1b1a5af00e4a34d2252ab2df239e9257a649dba6ca1582fb1c2bb.jpg)\n\n\nwhere we use a word containing two wordpieces from both weak and strong tokenizers as an example, and  $w_{1}, w_{2}$  and  $s_{1}, s_{2}$  are both token strings that can form word  $W$ , which are generated by the tokenizers of the weak and strong models respectively. Figure 2 shows an example of the process in three stages. In stage 1, the per-token output probabilities of weak models are obtained when generating output sequences. The probabilities of wordpieces in a word are then multiplied together to obtain the score of word  $W$ , following Eqn. (6).\n\n\nIn stage 2, the word probability is used to assign probabilities to tokens from the strong model tokenizer. In the process of training the strong model via teacher-forcing, the model gives a probability to each token by applying softmax to the output logits. This probability can be seen as the model's confidence in predicting the target token, where we assume that the weak model and strong model have similar confidence for wordpiece tokens in similar positions. This allows us to obtain the actual assignment of scores to each target token  $s_i$ , instead of assigning equal probabilities to all tokens involved. Taking an example of splitting a word  $W$  into two target wordpieces  $s_1$  and  $s_2$ , the decomposition can be approximated by\n\n\n$$\n\n$$\n\\log \\mathrm {P} (s _ {1}) = \\frac {e ^ {- C _ {s} (s _ {1})}}{e ^ {- C _ {s} (s _ {1})} + e ^ {- C _ {s} (s _ {2})}} \\log \\mathrm {P} (W), \\quad \\log \\mathrm {P} (s _ {2}) = \\frac {e ^ {- C _ {s} (s _ {2})}}{e ^ {- C _ {s} (s _ {1})} + e ^ {- C _ {s} (s _ {2})}} \\log \\mathrm {P} (W), \\tag {7}\n$$\n\n$$\n\n\nwhere  $C_s(s_i)$  is strong model confidence at the step predicting wordpiece  $s_i$  that is the maximum probability in the strong model output distribution at that step in practice. In this way, lower target probabilities are allocated to tokens with lower strong model confidence, while higher probabilities are allocated to tokens with higher strong model confidence.\n\n\nAfter obtaining the probability of the target token of the strong model, the probabilities of other categories can be obtained by scaling strong prediction probabilities, which can be treated as the soft label, as shown in Figure 2. Then the obtained soft labels can be handled using methods similar to those used in classification (as described in Section 3). Notably, during the computation of EDL loss, the sparsity caused by high dimensional spaces results in a large KL (Kullback-Leibler) penalty term. To solve this problem, a coefficient is added to the KL penalty to balance it with the magnitude of the negative log-likelihood term. Additionally, clamping is applied to restrict all values within an appropriate range, preventing potentially extremely large outliers on any particular token.\n\n\n# 4.2 DPO FOR SEQUENCE GENERATION OPTIMIZATION\n\n\nDifferent from classification tasks, sequence generation tasks often benefit from sequence-level objectives that directly optimize the entire sequence jointly rather than the individual tokens separately. To further improve the strong model for sequence generation, direct preference optimization (DPO) (Rafailov et al., 2023) is investigated for WeakS-to-Strong after supervised finetuning, where we propose to use weak models to provide the preference for the strong model generation.\n\n\nAfter the strong model is pretrained by supervised finetuning, it generates  $M$  output sequences based on a given output. Then for each sequence,  $N$  scores are computed by generating it using  $N$  weak models separately (via teacher forcing) and aggregating the output (log-)probabilities. A weighted sum of the  $N$  scores is performed as the final score assigned to each sequence. The sequence with the highest final score is viewed as the preferred sequence in DPO training, as shown below\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {c} = \\underset {m, m = 1, 2, \\dots , M} {\\arg \\max } \\mathrm {P} (\\boldsymbol {y} _ {s} (m)) = \\underset {m, m = 1, 2, \\dots , M} {\\arg \\max } \\sum_ {i = 1} ^ {N} \\lambda_ {i} \\mathrm {P} (\\boldsymbol {y} _ {s} (m) | \\boldsymbol {\\theta} _ {i}), \\tag {8}\n$$\n\n$$\n\n\nwhere  $\\pmb{y}_s(m)$  is the  $m$ th output sequence generated by the strong model,  $\\mathrm{P}(\\pmb{y}_s(m)|\\pmb{\\theta}_i)$  is computed using the  $i$ th weak model with model parameters  $\\pmb{\\theta}_i$ , and  $\\lambda_i$  is the weight assigned to the  $i$ th weak model. The dispreferred sequence can be computed similarly by  $\\pmb{y}_r = \\arg \\min \\mathrm{P}(\\pmb{y}_s(m))$ .\n\n\nConsidering potential errors by weak models, a variant of DPO, conservative DPO (cDPO) (Eric, 2023) with a more conservative target distribution is applied in our work. The loss of cDPO is\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {D P O}} ^ {\\epsilon} = (1 - \\epsilon) \\mathcal {L} _ {\\mathrm {D P O}} (\\boldsymbol {\\Lambda}, \\boldsymbol {y} _ {c}, \\boldsymbol {y} _ {r}) + \\epsilon \\mathcal {L} _ {\\mathrm {D P O}} (\\boldsymbol {\\Lambda}, \\boldsymbol {y} _ {r}, \\boldsymbol {y} _ {c}), \\tag {9}\n$$\n\n$$\n\n\nwhere  $\\epsilon$  is a small constant probability that labels are flipped to make DPO more conservative, and  $\\mathcal{L}_{\\mathrm{DPO}}$  is the standard DPO loss in (Rafailov et al., 2023), which can be written as\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {D P O}} (\\boldsymbol {\\Lambda}, \\boldsymbol {y} _ {c}, \\boldsymbol {y} _ {r}) = - \\log \\sigma \\left(\\beta \\log \\frac {\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {y} _ {c})}{\\boldsymbol {f} _ {\\text {r e f}} (\\boldsymbol {y} _ {c})} - \\beta \\log \\frac {\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} (\\boldsymbol {y} _ {r})}{\\boldsymbol {f} _ {\\text {r e f}} (\\boldsymbol {y} _ {r})}\\right). \\tag {10}\n$$\n\n$$\n\n\n# 5 EXPERIMENTAL SETUP\n\n\n# 5.1 DATASETS\n\n\nClassification Task. The setup of the classification task follows Burns et al. (2023). The SciQ dataset (Welbl et al., 2017) is used, which contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. In our experiment, 5k data samples were extracted for training weak models and another 5k samples were reserved for generating weak labels to train the strong model. The standard test set which contains 1k data samples was used for the test. The data is restructured into a balanced binary classification task, i.e., given a question and an answer, the model is required to determine whether the answer is correct.\n\n\nSlot filling. The performance of WeakS-to-Strong on the reliability of generated content was evaluated on the slot-filling task, which is a crucial spoken language understanding task aiming at filling in the correct value for predefined slots (e.g. restaurant and hotel names). SLURP dataset (Bastianelli et al., 2020) was used which contains 16.5k sentences and 72k audio recordings of single-turn user interactions with a home assistant, annotated with scenarios, actions and entities. Only the reference transcriptions of the speech were used for training. Following Sun et al. (2023a;b), we designed the prompt with slot keys and descriptions in the same way. In our setup, 2k utterances from the train split were extracted for training the weak models, and another 2k utterances were reserved for generating weak labels and training the strong model. We report the performance of both weak and strong models on the standard SLURP test set.\n\n\n# 5.2 MODELS AND BASELINES\n\n\nModels. For classification, the Qwen-7B (Bai et al., 2023) model was applied as the strong student model. Five models were used as weak teachers: GPT2-Large (Radford et al., 2019), OPT-1.3B (Zhang et al., 2022), Pythia-1.4B (Biderman et al., 2023), BLOOM-1B1 (Le Scao et al., 2022),\n\n\nand TinyLlama_v1.1 (Zhang et al., 2024). The last linear layer which maps the embeddings to tokens is replaced with a linear classification head with two outputs to adapt language models to the classification setting.\n\n\nFor slot filling, Llama-2-7B (Touvron et al., 2023) was used as the strong model which yielded better performance than Qwen-7B in this task. The same set of weak models was used as the classification task. As before, both weak and strong models are finetuned with all model parameters.\n\n\nExperiments with different numbers of weak models were conducted. One set involved three weak models (GPT2-Large, OPT-1.3B and Pythia-1.4B) in a WeakS-to-Strong experiment, which is referred to as WeakS-to-Strong-3. Another set included all five weak models, called WeakS-to-Strong-5.\n\n\nBaselines. The proposed Bayesian WeakS-to-Strong method is compared to the following three baselines.\n\n\nNaive Multi-Weak. The Naive Multi-Weak approach introduced in Section 3.2 servers as the baseline for both classification and generation tasks. For the classification task, the loss can be computed following Eqn. (2). For generation task, Eqn. (2) is modified as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {N a i v e}} ^ {\\text {G e n}} = \\sum_ {i = 1} ^ {N} \\lambda_ {i} \\frac {1}{T _ {i}} \\sum_ {j = 1} ^ {T _ {i}} \\mathcal {L} _ {\\mathrm {C E}} \\left(\\boldsymbol {f} _ {\\boldsymbol {\\Lambda}} \\left(\\boldsymbol {x}, \\boldsymbol {y} _ {w} ^ {(i, 1)}, \\dots , \\boldsymbol {y} _ {w} ^ {(i, j - 1)}\\right), \\boldsymbol {y} _ {w} ^ {(i, j)}\\right)), \\tag {11}\n$$\n\n$$\n\n\nwhere  $\\{\\pmb{y}_w^{(i,1)},\\pmb{y}_w^{(i,2)},\\dots,\\pmb{y}_w^{(i,T)}\\}$  is the sequence generated by  $i$ th weak model with length  $T_{i}$ . In contrast to the classification task, where the strong model  $\\pmb{f}_{\\Lambda}$  takes only  $\\pmb{x}$  as input, in the generation task,  $\\pmb{x}$  is paired with the sequence generated by each weak model (which is the weak target) and then fed into the strong model to obtain predictions. Teacher-forcing is used during training.\n\n\nFlyingSquid. FlyingSquid (Fu et al., 2020) is a method for weak supervision, estimating the accuracies and correlations among multiple noisy label functions (different weak labels in our case) without ground-truth data. Latent variable probabilistic graphical models are used to model these dependencies, with weak labels as observed variables and unobserved ground-truth labels as hidden variables. Since FlyingSquid is designed for binary classification, this baseline is only used for classification. Through this method, we get a label model with multiple weak labels and obtain the probability for the positive category, which is then used as a soft label in strong model training.\n\n\nJoint Decoding. Joint Decoding is used as an additional baseline for text generation tasks, which is specifically designed for multiple weak model generations. In contrast to the Naive Multi-Weak scheme where each weak model provides a weak target sequence, Joint Decoding employs multiple weak models to collaboratively determine one single target, as illustrated in Figure 1(b). Specifically, we perform Joint Decoding in a re-ranking fashion. For each weak model, the top  $M$  sequences are generated by beam search in decoding. The sequences from the  $N$  weak models are gathered to form a list of  $M \\times N$  sequences. Then each sequence is scored in the same way in Section 4.2. The sequence with the highest final score is used as the target sequence for strong model training. Unless otherwise mentioned,  $M = 5$  was used in the experiments.\n\n\nEach weak-to-strong experiment was run three times with different random seeds. The average and standard deviation were reported. More implementation details can be found in Appendix C.\n\n\n# 5.3 EVALUATION METRICS\n\n\nThe classification task is evaluated by accuracy, and the SLU-F1 (Bastianelli et al., 2020) is used for slot filling, which combines both word-level and character-level F1 scores to give partial credit to non-exact match predictions. Performance gap recovered (PGR) (Burns et al., 2023) is used to measure the performance gap recovered with weak supervision, which is defined as follows:\n\n\n$$\n\n$$\n\\mathrm {P G R} := \\frac {\\mathcal {P} - \\mathcal {P} _ {w}}{\\mathcal {P} _ {s} - \\mathcal {P} _ {w}} \\tag {12}\n$$\n\n$$\n\n\nwhere  $\\mathcal{P}$  is Weak-to-Strong performance,  $\\mathcal{P}_s$  strong performance and  $\\mathcal{P}_w$  weak performance. For multiple weak model cases, the average of PGRs for each weak model is treated as the final result.\n\n\nTable 1: Performance of single model on text classification task. Trained by ground-truth labels.\n\n\n<table><tr><td></td><td>Pre-trained model</td><td>#Param</td><td>Accuracy</td></tr><tr><td>Strong Model (ceiling)</td><td>Qwen-7B</td><td>7.7B</td><td>0.898</td></tr><tr><td rowspan=\"5\">Weak Model</td><td>GPT2-Large</td><td>0.8B</td><td>0.717</td></tr><tr><td>OPT-1.3B</td><td>1.3B</td><td>0.699</td></tr><tr><td>Pythia-1.4B</td><td>1.4B</td><td>0.685</td></tr><tr><td>BLOOM-1B1</td><td>1.1B</td><td>0.729</td></tr><tr><td>TinyLlama_v1.1</td><td>1.1B</td><td>0.731</td></tr></table>\n\n\nTable 2: Weak(S)-to-Strong performance (train the strong model on the weak label) on text classification task for with (w/) and without (w/o) auxiliary loss.  $\\gamma$  in Eqn. (1) and Eqn. (5) is set to 0 if auxiliary loss is not used. Experiments with 3 weak models and 5 weak models were conducted. Each experiment was run with three different random seeds. The best results are shown in bold.\n\n\n<table><tr><td rowspan=\"2\" colspan=\"2\"></td><td colspan=\"2\">w/o aux loss</td><td colspan=\"2\">w/ aux loss</td></tr><tr><td>Accuracy</td><td>PGR</td><td>Accuracy</td><td>PGR</td></tr><tr><td rowspan=\"5\">Weak-to-Strong</td><td>GPT2-Large</td><td>0.808 \u00b1 0.007</td><td>0.503 \u00b1 0.034</td><td>0.828 \u00b1 0.006</td><td>0.614 \u00b1 0.029</td></tr><tr><td>OPT-1.3B</td><td>0.807 \u00b1 0.005</td><td>0.541 \u00b1 0.026</td><td>0.841 \u00b1 0.012</td><td>0.714 \u00b1 0.058</td></tr><tr><td>Pythia-1.4B</td><td>0.775 \u00b1 0.009</td><td>0.421 \u00b1 0.042</td><td>0.793 \u00b1 0.009</td><td>0.507 \u00b1 0.042</td></tr><tr><td>BLOOM-1B1</td><td>0.823 \u00b1 0.015</td><td>0.556 \u00b1 0.087</td><td>0.843 \u00b1 0.008</td><td>0.677 \u00b1 0.049</td></tr><tr><td>TinyLlama_v1.1</td><td>0.832 \u00b1 0.004</td><td>0.603 \u00b1 0.024</td><td>0.838 \u00b1 0.005</td><td>0.643 \u00b1 0.030</td></tr><tr><td rowspan=\"3\">WeakS-to-Strong-3</td><td>Naive Multi-Weak</td><td>0.816 \u00b1 0.002</td><td>0.586 \u00b1 0.008</td><td>0.831 \u00b1 0.013</td><td>0.661 \u00b1 0.064</td></tr><tr><td>FlyingSquid</td><td>0.809 \u00b1 0.005</td><td>0.549 \u00b1 0.026</td><td>0.825 \u00b1 0.003</td><td>0.631 \u00b1 0.013</td></tr><tr><td>Bayesian Mutli-Weak</td><td>0.819 \u00b1 0.006</td><td>0.600 \u00b1 0.033</td><td>0.850 \u00b1 0.006</td><td>0.756 \u00b1 0.028</td></tr><tr><td rowspan=\"3\">WeakS-to-Strong-5</td><td>Naive Multi-Weak</td><td>0.832 \u00b1 0.005</td><td>0.641 \u00b1 0.025</td><td>0.853 \u00b1 0.006</td><td>0.754 \u00b1 0.032</td></tr><tr><td>FlyingSquid</td><td>0.832 \u00b1 0.004</td><td>0.643 \u00b1 0.023</td><td>0.855 \u00b1 0.007</td><td>0.768 \u00b1 0.035</td></tr><tr><td>Bayesian Mutli-Weak</td><td>0.831 \u00b1 0.008</td><td>0.627 \u00b1 0.027</td><td>0.866 \u00b1 0.006</td><td>0.828 \u00b1 0.038</td></tr></table>\n\n\n# 6 RESULTS\n\n\n# 6.1 TEXT CLASSIFICATION\n\n\nThe proposed Bayesian WeakS-to-Strong approach was first evaluated on a classification task. Table 1 shows the respective performance of the strong model and the weak models trained using ground-truth labels, with the former being the ceiling of the Weak(S)-to-Strong approaches. The strong model has about 7 times the number of parameters as the weak models, which also leads to about  $28\\%$  relative improvement in the classification accuracy. Results of Weak(S)-to-Strong approaches are shown in Table 2.  $\\gamma$  in Eqn. (1) and Eqn. (5) were set to 0 if auxiliary loss was not used. It can be seen that auxiliary loss was effective for both. Comparing single Weak-to-Strong results, different weak models show significant differences in performance. For example, Pythia-1.4B recovered only  $50\\%$  of the strong performance, while OPT-1.3B recovered around  $70\\%$ .\n\n\nThe Naive Multi-Weak baseline, FlyingSquid approach and Bayesian Multi-Weak using EDL were applied to the classification task for WeakS-to-Strong. Experiments with three weak models (GPT2-Large, OPT-1.3B and Pythia-1.4B) and all five weak models were conducted. With three weak models, the Naive Multi-Weak method got an average PGR of 0.661, slightly outperforming FlyingSquid method but still lower than the single OPT-1.3B model, and Bayesian Multi-Weak boosted PGR to 0.756. This indicates that a naive ensemble approach doesn't necessarily outperform the best single model, especially when there is a certain weak model which does not perform well. The Bayesian approach can increase the fault tolerance with the usage of prior in this case, as it learns patterns from the entire dataset. With five weak models compared to three, the Bayesian Multi-Weak approach further increased average PGR to 0.828, which is  $16\\%$  relativelt higher than the best single model and  $8\\%$  relatively higher than baselines, and consistently better with all seeds. The results show the effectiveness of the Bayesian approach for distribution estimation.\n\n\nTable 3: Performance of single model on text generation task. Trained by ground-truth labels.\n\n\n<table><tr><td></td><td>Pre-trained model</td><td>#Param</td><td>SLU-F1</td></tr><tr><td>Strong Model (ceiling)</td><td>Llama-2-7B</td><td>6.7B</td><td>0.748</td></tr><tr><td rowspan=\"5\">Weak Model</td><td>GPT2-Large</td><td>0.8B</td><td>0.660</td></tr><tr><td>OPT-1.3B</td><td>1.3B</td><td>0.665</td></tr><tr><td>Pythia-1.4B</td><td>1.4B</td><td>0.680</td></tr><tr><td>BLOOM-1B1</td><td>1.1B</td><td>0.651</td></tr><tr><td>TinyLlama_v1.1</td><td>1.1B</td><td>0.676</td></tr></table>\n\n\nTable 4: Weak(S)-To-Strong performance on text generation task, training strong model on weak labels, for with (w/) and without (w/o) auxiliary loss. In cases without auxiliary loss,  $\\gamma$  in Eqn. (1) and Eqn. (5) is set to 0. Experiments were conducted using 3 and 5 weak labels, each run with three different random seeds. The best results are shown in bold.\n\n\n<table><tr><td rowspan=\"2\" colspan=\"2\"></td><td colspan=\"2\">w/o aux loss</td><td colspan=\"2\">w/ aux loss</td></tr><tr><td>SLU-F1</td><td>PGR</td><td>SLU-F1</td><td>PGR</td></tr><tr><td rowspan=\"5\">Weak-to-Strong</td><td>GPT2-Large</td><td>0.687 \u00b1 0.011</td><td>0.303 \u00b1 0.125</td><td>0.673 \u00b1 0.012</td><td>0.150 \u00b1 0.139</td></tr><tr><td>OPT-1.3B</td><td>0.660 \u00b1 0.059</td><td>-0.066 \u00b1 0.715</td><td>0.696 \u00b1 0.009</td><td>0.367 \u00b1 0.103</td></tr><tr><td>Pythia-1.4B</td><td>0.702 \u00b1 0.007</td><td>0.320 \u00b1 0.095</td><td>0.691 \u00b1 0.022</td><td>0.173 \u00b1 0.322</td></tr><tr><td>BLOOM-1B1</td><td>0.690 \u00b1 0.021</td><td>0.399 \u00b1 0.220</td><td>0.684 \u00b1 0.021</td><td>0.337 \u00b1 0.220</td></tr><tr><td>TinyLlama_v1.1</td><td>0.667 \u00b1 0.006</td><td>0.000 \u00b1 0.084</td><td>0.658 \u00b1 0.016</td><td>-0.250 \u00b1 0.224</td></tr><tr><td rowspan=\"3\">WeakS-to-Strong-3</td><td>Naive Multi-Weak</td><td>0.711 \u00b1 0.008</td><td>0.531 \u00b1 0.101</td><td>0.694 \u00b1 0.013</td><td>0.318 \u00b1 0.169</td></tr><tr><td>Joint Decoding</td><td>0.703 \u00b1 0.003</td><td>0.434 \u00b1 0.032</td><td>0.704 \u00b1 0.015</td><td>0.442 \u00b1 0.191</td></tr><tr><td>Bayesian Mutli-Weak</td><td>0.712 \u00b1 0.014</td><td>0.549 \u00b1 0.180</td><td>0.714 \u00b1 0.014</td><td>0.574 \u00b1 0.176</td></tr><tr><td rowspan=\"3\">WeakS-to-Strong-5</td><td>Naive Multi-Weak</td><td>0.716 \u00b1 0.010</td><td>0.606 \u00b1 0.123</td><td>0.694 \u00b1 0.012</td><td>0.328 \u00b1 0.144</td></tr><tr><td>Joint Decoding</td><td>0.671 \u00b1 0.010</td><td>0.037 \u00b1 0.120</td><td>0.675 \u00b1 0.005</td><td>0.091 \u00b1 0.066</td></tr><tr><td>Bayesian Mutli-Weak</td><td>0.718 \u00b1 0.014</td><td>0.627 \u00b1 0.173</td><td>0.721 \u00b1 0.013</td><td>0.668 \u00b1 0.166</td></tr></table>\n\n\n# 6.2 TEXT GENERATION\n\n\nFor the text generation task on slot filling, the performance of the student strong model and teacher weak model finetuned on ground-truth labels is presented in Table 3. The strong ceiling performance is 0.748, and the highest weak performance is 0.680.\n\n\nThe Weak(S)-to-Strong performances are reported in Table 4. For a single weak model, the Weak-to-Strong model performance didn't necessarily surpass the original weak performance (e.g. TinyLlama_v1.1), and the highest PGR is 0.399. With five weak models, our proposed Bayesian Multi-Weak approach achieved an average PGR of 0.668, which is  $26\\%$  better than a single weak model,  $6\\%$  better than the naive baseline, and consistently better across three seeds. Comparing WeakS-to-Strong performance with and without auxiliary loss, adding auxiliary loss is not effective for naive approaches, but improves the performance of Bayesian approaches. It indicates that the proposed Bayesian method, where the predictions of a strong model are applied as part of the distribution estimation as in Eqn. (5), more effectively integrates strong model predictions into the training process, showing the effectiveness of our Bayesian approach for the model ensemble. The ablation study can be found in Appendix E. A noticeable decline has been observed for Joint Decoding when the number of weak models increases from three to five. This may result from the poor performance of the TinyLlama weak model which affects the quality of the generated weak target. Recall that in Joint Decoding, multiple weak models collaboratively determine one single target. In contrast, Naive Multi-Weak and Bayesian Multi-Weak methods still benefit from the increased number of weak models, which indicates that they are more robust against the quality of a single weak model. Additional experiments on Joint Decoding can be found in Appendix G for more analysis and insights.\n\n\nBased on the strong model supervised by five weak models on the Bayesian Multi-Weak approach with auxiliary loss, a cDPO training is conducted, which further train the model in student-forcing form. Three separate cDPO experiments were conducted using different initial models obtained in SFT stage with three different seeds. The results are shown in Table 5. After cDPO, three models\n\n\nTable 5: Results before and after DPO. Based on the Bayesian Multi-Weak model using five weak models (the WeakS-to-Strong-5 setting) with auxiliary loss. Average results on three seeds are reported. Note that the initial SFT models are different for different seeds.\n\n\n<table><tr><td></td><td colspan=\"2\">seed-0</td><td colspan=\"2\">seed-1</td><td colspan=\"2\">seed-2</td><td colspan=\"2\">Average</td></tr><tr><td></td><td>SLU-F1</td><td>PGR</td><td>SLU-F1</td><td>PGR</td><td>SLU-F1</td><td>PGR</td><td>SLU-F1</td><td>PGR</td></tr><tr><td>Before cDPO</td><td>0.706</td><td>0.477</td><td>0.730</td><td>0.776</td><td>0.728</td><td>0.751</td><td>0.721 \u00b1 0.013</td><td>0.668 \u00b1 0.166</td></tr><tr><td>After cDPO</td><td>0.707</td><td>0.490</td><td>0.733</td><td>0.813</td><td>0.733</td><td>0.813</td><td>0.724 \u00b1 0.015</td><td>0.705 \u00b1 0.187</td></tr></table>\n\n\nshowed consistent performance improvements. The average PGR reached 0.705,  $6\\%$  relatively better than that before DPO, with a maximum PGR of 0.813.\n\n\n# 6.3 COMPLEMENTARITY OF WEAK MODELS\n\n\nAn experiment about the complementarity of different weak models was conducted. For classification models, the agreement is assessed by calculating the accuracy of each model's predictions on test set, treating outputs from other models as references. For generation models, the Levenshtein distance is calculated between different outputs from two models for a same input, which is obtained using the minimum edit number required to change one sequence to another. The average Levenshtein distance across all samples in the test set is used to measure the agreement between two models.\n\n\nThe results are shown in Figure 3. The agreement among different weak models is among 0.75 for classification models, while for generation task the maximum agreement between different models is 0.56. This suggests that the consistency among different weak models is low, thus they can complement each other well as the faults made by different weak models are not the same. Moreover, comparing with Naive Multi-Weak approach, our proposed Bayesian method estimates a distribution based on weak labels, learning patterns from the entire dataset, thereby increasing the tolerance for fault in weak models, as shown in results in Section 6.1 and 6.2.\n\n\n![(a) Agreement of 5 models on classification.](images/8272cf3885079e6b2e4bbe96f01d0176c4306ae74c8807ff68b987bb6fe0d084.jpg)\n\n\n![(b) Agreement of 5 models on generation. Figure 3: Agreement of weak models. The similarity between classification models was assessed by calculating the accuracy of each model's predictions against the others on the test set. For generation models, the agreement is obtained through the Levenshtein distance.](images/7cffc4c0a87ed641fdf8fb76b97330e57e9c6b442cd423f0f04371a2a2e4dba2.jpg)\n\n\n# 7 CONCLUSION\n\n\nThis paper extends the Weak-to-Strong framework to WeakS-to-Strong by leveraging an ensemble of weak models to capture the variability in human opinions. We propose a Bayesian inference method, Bayesian WeakS-to-Strong, to more accurately estimate the weak label distribution based on the outputs of multiple weak models. Additionally, while the original Weak-to-Strong method was limited to text classification tasks, this paper expands its applicability to text generation, enabling both the assessment of content trustworthiness and the generation of trustworthy content. Finally, DPO is utilized to enhance the student model's preference for learning, going beyond the traditional teacher-forcing approach. Our results demonstrate the effectiveness of Bayesian WeakS-to-Strong for both classification and generation tasks, highlighting its potential for superalignment.\n\n\n# REFERENCES\n\n- Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\n\n- Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\n\n- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.\n\n- Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022b.\n\n- Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser. SLURP: A spoken language understanding resource package. In Proc. EMNLP, 2020.\n\n- Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In Proc. ICML, 2023.\n\n- Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.\n\n- Aida Mostafazadeh Davani, Mark Diaz, and Vinodkumar Prabhakaran. Dealing with disagreements: Looking beyond the majority vote in subjective annotations. Transactions of the Association for Computational Linguistics, 10:92-110, 2022.\n\n- Mitchell Eric. A note on DPO with noisy preferences & relationship to IPO. https://ericmitchell.ai/cdpo.pdf, 2023.\n\n- Yoav Freund and Robert E Schapire. A desiccation-theoretic generalization of on-line learning and an application to boosting. In European conference on computational learning theory, pp. 23-37, 1995.\n\n- Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and Christopher R\u00e9. Fast and three-rious: Speeding up weak supervision with triplet methods. In Proc. ICML, 2020.\n\n- Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (ReST) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\n\n- Jianyuan Guo, Hanting Chen, Chengcheng Wang, Kai Han, Chang Xu, and Yunhe Wang. Vision superalignment: Weak-to-strong generalization for vision foundation models. arXiv preprint arXiv:2402.03749, 2024.\n\n- Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. In Proc. EMNLP, 2019.\n\n- Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. Aligner: Achieving efficient alignment through weak-to-strong correction. arXiv preprint arXiv:2402.02416, 2024.\n\n- Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Galle, et al. BLOOM: A 176B-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\n- Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning from human feedback with AI feedback. arXiv preprint arXiv:2309.00267, 2023.\n\n- Breiman Leo. Bagging predictors. Machine Learning, 24:123-140, 1996.\n\n- Yajiao Liu, Xin Jiang, Yichun Yin, Yasheng Wang, Fei Mi, Qun Liu, Xiang Wan, and Benyou Wang. One cannot stand for everyone! leveraging multiple user simulators to train task-oriented dialogue systems. In Proc. ACL, 2023.\n\n- Yuejiang Liu and Alexandre Alahi. Co-supervised learning: Improving weak-to-strong generalization with hierarchical mixture of experts. arXiv preprint arXiv:2402.15505, 2024.\n\n- R\u00e9mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023.\n\n- Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\n\n- Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, et al. Training language models to follow instructions with human feedback. In Proc. NeurIPS, 2022.\n\n- Silviu Paun and Edwin Simpson. Aggregating and learning from multiple annotators. In Proc. ACL, 2021.\n\n- Vinodkumar Prabhakaran, Aida Mostafazadeh Davani, and Mark Diaz. On releasing annotator-level labels and information in datasets. In Proc. ACL, 2021.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1:9, 2019.\n\n- Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Proc. NeurIPS, 2023.\n\n- Jitao Sang, Yuhang Wang, Jing Zhang, Yanxu Zhu, Chao Kong, Junhong Ye, Shuyu Wei, and Jinlin Xiao. Improving weak-to-strong generalization with scalable oversight and ensemble learning. arXiv preprint arXiv:2402.00667, 2024.\n\n- Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification uncertainty. In Proc. NeurIPS, 2018.\n\n- Guangzhi Sun, Shutong Feng, Dongcheng Jiang, Chao Zhang, Milica Ga\u0161i\u0107, and Philip C Woodland. Speech-based slot filling using large language models. arXiv preprint arXiv:2311.07418, 2023a.\n\n- Guangzhi Sun, Chao Zhang, Ivan Vulic, Pawe\u0142 Budzianowski, and Philip C Woodland. Knowledge-aware audio-grounded generative slot filling for limited annotated data. arXiv preprint arXiv:2307.01764, 2023b.\n\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\n- Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Proc. ICLR, 2022.\n\n- Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Proc. W-NUT, 2017.\n\n- Wen Wu, Chao Zhang, Xixin Wu, and Philip C Woodland. Estimating the uncertainty in emotion class labels with utterance-specific Dirichlet priors. IEEE Transactions on Affective Computing, 2022.\n\n- Wen Wu, Chao Zhang, and Philip Woodland. Estimating the uncertainty in emotion attributes using deep evidential regression. In Proc. ACL, 2023.\n\n- Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. TinyLlama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024.\n\n- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\n- Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.\n\nResponse: \\boxed{Accept}\n\n### Rationale:\n\n#### Originality:\n- The paper introduces novel methodologies for extending the Weak-to-Strong paradigm to WeakS-to-Strong, focusing on Bayesian inference and ensemble approaches to handle variability in human opinions.\n- It also explores the extension of Weak-to-Strong to text generation tasks, providing a comprehensive framework for leveraging weak models in diverse applications.\n\n#### Methodological Rigor:\n- The methodology is detailed and well-explained, including the derivation of Bayesian inference for weak label distribution estimation and the adaptation of existing weak-to-strong techniques for text generation.\n- The experimental design is thorough, with multiple baselines and setups, ensuring a robust comparison of the proposed methods.\n\n#### Significance of Contribution:\n- The work addresses a critical issue in AI alignment by proposing more sophisticated approaches to handling weak supervision, especially in the context of large language models where the quality of supervision can significantly impact model performance.\n- The extension to text generation tasks broadens the applicability of the proposed methods and provides a practical solution for real-world applications where trustworthiness assessment and content generation are crucial.\n\n#### Clarity of Presentation:\n- The paper is well-written and structured, making it easy to follow the theoretical developments and experimental results.\n- The figures and tables effectively illustrate key concepts and experimental outcomes, aiding in the understanding of the methodology and results.\n\n#### Alignment with Journal Scope:\n- The paper fits well within the scope of ICLR, which focuses on the latest advances in machine learning, particularly in areas like reinforcement learning, meta-learning, and transfer learning.\n- The proposed methods are innovative and theoretically grounded, aligning well with the interests of researchers in AI alignment and meta-learning.\n\nOverall, the paper presents a substantial advancement in the field of AI alignment and weak supervision, offering a valuable contribution to both theory and practice. Given the novelty and practical implications, the paper is likely to be accepted by ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634790.92032", "msecs": "920.0", "relativeCreated": "135743.79420280457", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Learning through experience:Episodic memory representation for cognitive agents\n\n# Abstract\nAs the demand for intelligent robots and cognitive agents rises, the ability to retain and utilize past experiences through episodic memory has become crucial, especially for social companion robots that rely on previous interactions for task execution. To address this, we introduce Episodic Memory for Cognitive Agents (EMCA), a novel framework that advances knowledge representation by integrating real-world interactions. EMCA enables agents to adapt to complex environments by learning from tasks, interacting with humans, and processing multimodal data\u2014such as speech, vision, and non-verbal cues\u2014without pre-training on specific scenarios. EMCA models episodic memory through a graph-based structure , allowing for incremental storage and retrieval of experiences. Each interaction or event enriches the memory graph, supporting continuous learning and adaptation without extensive retraining. This human-like memory formation optimizes the agent\u2019s ability to retrieve relevant information for tasks like localization, planning, and reasoning based on prior experiences.Unlike conventional models relying on temporal markers or recurrent patterns, EMCA encodes data like human memory, allowing reasoning across diverse scenarios regardless of temporal patterns. The framework dynamically builds a memory graph with semantic and temporal connections based on the agent\u2019s experiences, promoting flexible temporal reasoning. It also introduces mechanisms for clustering new memories and a dynamic retrieval policy that adjusts based on context or query type, ensuring robustness even in unpredictable scenarios. Empirical tests show EMCA adapts effectively to real-world data, offering reliability and flexibility in dynamic environments.\n\n\n# 1 INTRODUCTION\n\n\nEpisodic memory, introduced by Tulving (1972) Tulving (2002), refers to the recollection of personal experiences anchored to specific times and locations. Unlike semantic memory, which contains general knowledge, episodic memory retains detailed information about events, including temporal, spatial, emotional, and contextual aspects. Tulving's framework organizes episodic memory into components: time, place, characters, and events. Each episode, as shown in Figure 1, is a synthesis of these components tied to a specific time.\n\n\nBuilding on this framework, we propose a model where episodic memories are organized as a graph, with Each episode  $i$  is represented as a node:  $\\mathbf{Episode}_i = \\{\\mathbf{C}_i,\\mathbf{T}_i,\\mathbf{L}_i,\\mathbf{e}_i\\}$\n\n\nHere,  $\\mathbf{C}_i$  denotes the characters,  $\\mathbf{T}_i$  the temporal aspects,  $\\mathbf{L}_i$  the spatial location, and  $\\mathbf{e}_i$  the events. The graph is connected by two types of edges: semantic edges  $\\mathbf{S}(v_i,v_j)$ , linking nodes with shared components, and temporal edges\n\n\n$\\mathbf{T}(v_i, v_j)$ , establishing the sequence of episodes. A dynamic clustering approach is applied to group similar episodes based on temporal and contextual similarities, optimizing retrieval efficiency.\n\n\nOur retrieval system supports three query types: \"what\" (contextual), \"when\" (temporal), and \"where\" (spatial), as outlined by Stephen et al., and Holland and Smulders (2011), enabling human-\n\n\n![Figure 1: Representation of a single episode (node) in episodic memory, integrating time, character, place, and events.](images/d287868946614c0495412b863a98de1d44a3a73418b2d7cc8adeae626933a1a5.jpg)\n\n\nlike memory recall. This is particularly valuable for applications in social companion robotics, aiding elderly or memory-impaired individuals.\n\n\nFor such a cognitive agent it is important that it possesses the ability to recall episodic memories is essential for human cognition, linking personal experiences to specific temporal and spatial contexts. Existing memory models often struggle with continuous, time-series data, which limits their ability to simulate episodic recall effectively. Many of these systems fail to store dialogues as multimodal data, which prevents them from capturing the rich, context-dependent nature of human memory. Additionally, most existing approaches store a single experience as one isolated episode and lack a mechanism for retrieving information across multiple experiences, hindering their ability to integrate knowledge over time. Furthermore, current episodic memory systems are typically restricted to performing a specific, predefined task, limiting their flexibility and adaptability. In contrast, our system is designed to be more versatile, capable of handling a variety of tasks and dynamically adapting to new scenarios, making it far more suited for real-world applications that require memory integration across different contexts and time periods. By integrating multimodal data and temporal information into the episodic memory framework, our model enables experience localization, recommendation, and episodic question answering. It provides a robust foundation for adaptable, scalable systems capable of operating without frequent retraining, applicable to real-world scenarios such as social companion robots and autonomous task planning systems.\n\n\n# CONTRIBUTIONS:\n\n- 1. Temporal connections are managed without complex pattern learning, enabling adaptive reasoning and retrieval of subgraphs from past experiences.\n\n- 2. The system incrementally stores and retrieves episodic memories, dynamically clustering them based on temporal and contextual affinities.\n\n- 3. A multi-edge graph framework optimizes path traversals for dynamic memory retrieval and personalized recommendations across subjective timescales.\n\n- 4. A new dataset is introduced to improve episodic memory question answering, enhancing the agent's ability to respond to queries based on past events.\n\n\nOur model's versatility is demonstrated through comparisons with existing systems that require retraining. It handles various dataset types, including visual, multimodal, and text-based data, and excels in temporal reasoning even without explicit timestamps, addressing complex memory retrieval tasks across diverse applications.\n\n\n# 2 RELATED WORKS\n\n\nEpisodic memory in multimodal systems has seen significant progress. Xiong et al. Xiong et al. (2016) proposed a memory framework centered on question-answer pairs, later enhanced by Han et al. Han et al. (2019) with transformer-based networks and reinforcement learning, yet constrained by predefined queries, limiting real-world use.\n\n\nTemporal Graph Networks (TGNs) by Rossi et al. Rossi et al. (2020) facilitate learning on dynamic graphs, finding applications in recommendations and social media. Associative memory models, such as Hopfield networks Ramsauer et al. (2021), allow content-based retrieval but struggle with irregular data. Sarigun Sarigun (2023) addresses dynamic temporal graphs, while TempoQR Mavromatis et al. (2021) excels in structured datasets with explicit event timing, unlike the contextual inference required by real-world agents.\n\n\nEpisodic memory is also key in temporal localization for language queries, extending beyond VideoQA Xu et al. (2021) to more complex scenarios. Techniques like 2D-TAN Zhang et al. (2020), VSLNet-L Zhang et al. (2020), and RELER Liu et al. (2022) achieve video content localization but operate independently for each video. Our model advances these by integrating audio-visual and contextual inputs for comprehensive, context-aware outputs.\n\n\nEMQA Datta et al. (2022) enriches VideoQA through episodic memory for improved responses. Memory-Augmented Neural Networks (MANNs) Santoro et al. (2016), including STM Le et al. (2020), DNC Xiong et al. (2016), and Rehearsal Memory Zhang et al. (2021), are models that make use of memory system which store memory to answer questions from videos, also B\u00e4rmann et al. develop memory graphs for long-term retentionB\u00e4rmann et al. (2024) in order to verbalize data\n\n\nfor answering questions from prior memory. These models are trained using predefined question-answer pairs and do not store audio or sound data, which makes them less suitable for supporting episodic memory. Anokhin et al. (2024) merges semantic and episodic memory and is similar to human memory but suffers from traversal inefficiencies due to complexity. Edge et al. Edge et al. (2024) enhance retrieval-augmented generation (RAG) with graph indexing in academic domains, and Mavromatis et al. Mavromatis & Karypis (2024) integrate LLMs and GNNs for multi-hop QA, limited by static clusters. Our dynamic multi-edge graph approach adapts to multimodal, time-series data by creating new clusters as the agent gains new experiences, enabling efficient retrieval and enhanced flexibility in unstructured environments.\n\n\n# 3 METHODOLOGY\n\n\nEMCA's methodology for collecting and structuring episodic experiences is inspired by cognitive psychology, specifically the 'what', 'where', and 'when' (WWW) components of episodic memory. This approach highlights the agent's ability to independently capture multimodal data\u2014visual and auditory\u2014to build a comprehensive understanding of its environment. As depicted in Figure 2, the system comprises two primary stages: Experience Memory Collection, where the agent autonomously compiles and stores experiences in a knowledge base without pretraining, and Memory Retrieval, where it recalls relevant experiences for question answering and reasoning. This method enhances adaptability by utilizing prior experiences to inform decision-making. The episodic memory framework is applicable in diverse contexts, such as memory localization and experience-based recommendation. Detailed methodology is given in Appendix B\n\n\n# 3.1 PROCESSING OF AUDIO DATA IN EPISODIC MEMORY\n\n\nAudio data, including dialogues and acoustics, is crucial for constructing episodic memory. Dialogue's provide linguistic and contextual information, while acoustics capture environmental and emotional cues. These elements are integrated as  $A(t) = D(t) + C(t)$ , where  $A(t)$  is the total audio data at time  $t$ , with  $D(t)$  representing the dialog and  $C(t)$  representing acoustics.\n\n\n# 3.1.1 EXTRACTION OF ACOUSTIC DATA USING MEL SPECTROGRAMS\n\n\nAcoustic data is transformed into Mel spectrograms, which emphasize perceptually relevant frequencies. The Mel spectrogram  $M(t,f)$  is computed as  $M(t,f) = \\log \\left(\\sum_{k}|X(t,k)|^{2}\\cdot H(f,k)\\right)$ . where  $X(t,k)$  is the magnitude of the STFT at time  $t$  and frequency  $k$ , and  $H(f,k)$  is the Mel filter bank mapping linear frequencies to the Mel scale.\n\n\n# 3.1.2 EXTRACTION OF VERBAL CUES FROM AUDIO DATA\n\n\nVerbal cues are extracted by applying the Short-Time Fourier Transform (STFT) and converting the spectrum to the Mel scale as  $M(f) = 2595\\log_{10}\\left(1 + \\frac{f}{700}\\right)$ . The Mel spectrogram is then derived as  $\\mathrm{MelSpec}(m,t) = \\log \\left(\\sum_{f_{\\mathrm{low}}}^{f_{\\mathrm{high}}}|S(f,t)|^2 M(f) + \\epsilon\\right)$ , where  $\\epsilon$  is a small constant to prevent issues with the logarithm.\n\n\n# 3.1.3 INTEGRATION OF ACOUSTIC AND VERBAL FEATURES\n\n\nThe final audio representation integrates acoustic features with transcribed dialogue:  $T_{\\mathrm{audio}}(t) = T_{\\mathrm{acoustics}}(t) + T_{\\mathrm{dialogue}}(t)$ . capturing both tonal properties and linguistic meaning. By applying tagging techniques to the processed audio data, we extract and associate place, character, and time information. These features are then saved as an embedding, forming a unified representation for episodic memory.\n\n\n# 3.2 PROCESSING OF VISUAL DATA IN EPISODIC MEMORY\n\n\nVisual data processing starts by transforming each frame  $F_{i}$  into a tensor and extracting global and local features. The scene representation is then obtained as  $V_{\\mathrm{scene}} = \\frac{1}{N}\\sum_{i = 1}^{N}V_{\\mathrm{embed}}(F_i)$ .\n\n\n![Figure 2: Methodology: The agent collects multimodal experiences through vision and audio, which are then synchronized by aligning them within specific time windows. The fused episode is stored in episodic memory which is represented in the form of a graph that, capturing details such as characters, time, place, and events and has both spatial and temporal edges. Upon receiving a query, the model retrieves the relevant episode or set of episodes from memory and generates an appropriate response.](images/62cf05bc6365554bd404ebbcc00f59d22cce8e173b477b7f77f64aa481d111df.jpg)\n\n\nwhere  $N$  represents the number of frames in the scene. To extract  $V_{\\mathrm{time}}$  and  $V_{\\mathrm{place}}$ , a convolution network processes the feature map  $F_{\\mathrm{feature}}(x,y)$ , generating probability maps for the text center lines (TCL) and text regions (TR):  $\\left(\\frac{P_{\\mathrm{TCL}}(x,y)}{P_{\\mathrm{TR}}(x,y)}\\right) = \\sigma \\left(\\left(\\frac{W_{\\mathrm{TCL}}}{W_{\\mathrm{TR}}}\\right) \\cdot F_{\\mathrm{feature}}(x,y)\\right)$ . Thresholding is applied to filter relevant text regions using the condition\n\n\n$$\n\n$$\nP _ {\\text {f i l t e r e d}} = \\left\\{\\left(x, y\\right) \\mid P _ {\\mathrm {T C L}} (x, y) \\geq T _ {\\mathrm {T C L}} \\text {a n d} P _ {\\mathrm {T R}} (x, y) \\geq T _ {\\mathrm {T R}} \\right\\}.\n$$\n\n$$\n\n\nRecognized text is processed with a softmax layer for classification:  $\\hat{y}_t = \\mathrm{Softmax}(W \\cdot h_t + b)$ . from which  $V_{\\mathrm{time}}$  and  $V_{\\mathrm{place}}$  are derived. Character information ( $V_{\\mathrm{character}}$ ) is extracted by associating text and visual features through techniques such as Named Entity Recognition (NER) or vision-language embeddings. Together,  $V_{\\mathrm{scene}}$ ,  $V_{\\mathrm{time}}$ ,  $V_{\\mathrm{place}}$ , and  $V_{\\mathrm{character}}$  form a comprehensive multimodal representation that captures contextual details for reasoning or retrieval tasks.\n\n\n# 3.2.1 MERGING AND SYNCHRONIZING DATA\n\n\nIn the final stage, processed audio and visual data are synchronized to a common timestamp, forming a unified representation:  $\\mathbf{T}_{\\mathbf{M}} = (\\mathbf{T}_{\\mathrm{audio}},\\mathbf{T}_{\\mathrm{visual}})$ . This ensures temporal alignment between the modalities. The audio and visual embeddings are then concatenated into a joint multimodal embedding:  $\\mathbf{E}_{\\mathrm{combined}} = \\mathbf{E}_{\\mathrm{audio}}\\oplus \\mathbf{E}_{\\mathrm{visual}}$ , which is stored in episodic memory, with each node representing key experience aspects such as time, location, characters, and events.\n\n\nThis integrated embedding enhances memory recall and event-based analysis. Following concatenation, the embedding is encoded to capture entities like place, time, and characters for each episode. The methodology for capturing these entities is supported by prior studies, as shown in the appendix.\n\n\n# 3.3 EPISODIC MEMORY REPRESENTATION\n\n\nEach node in the episodic memory represents a day, with subnodes capturing the activities of that day. The main node summarizes the day's events, while subnodes encode specific event details. Joint embeddings, integrating place, character, and event information, are stored at both the event and day levels. This hierarchical structure enables efficient encoding and retrieval of both events and their details. There are two types of edges: contextual edges, which con\n\n\nnect nodes based on similarities in place, character, and events, and temporal edges, which link nodes temporally to reflect the sequence of events. Figure 3 shows an episodic memory graph.\n\n\n# 3.3.1 CLUSTER DEFINITIONS\n\n\nWhen an agent receives an episodic experience, it assigns the experience to multiple clusters based on location, events, and characters: Location Cluster  $\\mathbf{C}_l$ , Character Cluster  $\\mathbf{C}_c$ , and Event Cluster  $\\mathbf{C}_e$ . Each entity within a cluster is uniquely identified, facilitating efficient memory organization and retrieval in alignment with the hierarchical structure of episodic memory, where nodes represent days and subnodes capture activities and events.\n\n\n# 3.3.2 DYNAMIC CLUSTERING\n\n\nWhen an agent gets an experience Text and visual embeddings are utilized to restore feature details and improve decision-making. Spatial and character embeddings are derived from similarities between locations and characters across episodes. For each new episode  $E_{n}$ , these embeddings are evaluated and integrated into existing clusters using an attention mechanism: attention  $X(E_{n},E_{i}) = \\frac{\\mathbf{V}_{X_{n}}\\cdot\\mathbf{V}_{X_{i}}}{\\|\\mathbf{V}_{X_{n}}\\| \\|\\mathbf{V}_{X_{i}}\\|}$ , where  $X \\in \\{\\text{location, char}\\}$ . This dynamic clustering process organizes memory based on spatial, character, and event similarities. New episodes that do not fit into existing clusters create new identifiers, ensuring continuous cluster expansion.\n\n\nEvent clusters are formed by modeling coherence among consecutive utterances within dialogues, with each dialogue  $D = (u_{1}, u_{2}, \\ldots, u_{|D|})$  consisting of  $|D|$  utterances. Instead of relying on expert annotations, we infer event structures by assum-\n\n\ning that utterances within the same event exhibit higher coherence than those spanning multiple events. A contrastive learning objective is used to maximize coherence within event-related snippets while minimizing coherence across different events. The contrastive coherence loss is defined as:  $L_{\\text{contrastive}} = \\sum_{i} [\\log p(\\text{positive}_i) - \\log p(\\text{negative}_i)]$ , where positive and negative examples are selected based on coherence metrics such as ROUGE score, facilitating unsupervised event clustering. This approach allows for the segmentation of dialogues into relevant event clusters, improving the system's ability to retrieve and reason about past experiences, enhancing context-aware decision-making. Once the new episode  $E_n$  is assigned to one or more clusters, the cluster definitions are updated as follows:  $\\mathbf{C}_X^j = \\mathbf{C}_X^j \\cup \\{E_n\\}$  (10), where  $X$  refers to location, character, or event. If  $E_n$  does not fit into any existing cluster, new identifiers are created in the cluster:  $\\mathbf{C}_X^{\\text{new}} = \\{E_n\\}$  (11), where  $X \\in \\{\\text{location}, \\text{char}, \\text{event}\\}$ .\n\n\n# 3.3.3 EDGE CONNECTION\n\n\nContextual edges between episodes are formed by analyzing shared clusters based on common characters, locations, or events. The similarity between two episodes  $E_{1}$  and  $E_{2}$  is determined by the number of overlapping clusters, with an edge created if two or more clusters are shared. The edge weight is computed by combining the location, character, and event similarities:  $\\text{Weight}(E(N_{1}, N_{2})) = \\mathbf{L}(N_{1}, N_{2}) \\| \\mathbf{C}(N_{1}, N_{2}) \\| \\mathbf{E}(N_{1}, N_{2})$ , where  $\\mathbf{L}(N_{1}, N_{2})$ ,  $\\mathbf{C}(N_{1}, N_{2})$ , and  $\\mathbf{E}(N_{1}, N_{2})$  represent location, character, and event similarities, respectively. The primary connection between episodes is established by the maximum similarity across these features:  $\\text{Link}(E(N_{1}, N_{2})) = \\arg \\max \\left( \\{\\mathbf{L}(N_{1}, N_{2}), \\mathbf{C}(N_{1}, N_{2}), \\mathbf{E}(N_{1}, N_{2})\\} \\right)$ .\n\n\nTemporal Edges are defined by the time relationship between consecutive episodes. The temporal connection between episodes  $E_{t-1}$  and  $E_t$  is represented as  $\\mathbf{T}_{\\mathrm{edge}}(E_{t-1}, E_t) = 1$ . Temporal edges help handle missing timestamps, as no traditional statistical methods are used to estimate them. Instead, each episode is treated as representing a distinct day, and the temporal indexer is updated\n\n\n![Figure 3: The diagram illustrates an episodic memory graph that organizes actions and key events hierarchically. Nodes represent memories enriched with textual, visual, and acoustic data, including Mel spectrograms. Connections reflect shared locations, characters, and events, aiding efficient recall and analysis. Clusters are formed using attention mechanisms on embeddings related to shared characters, places, and events, enabling effective grouping of related episodes.](images/a3b0f2b9751d63286705996d54dac048077355f5f46c5b34dada076579a736fa.jpg)\n\n\nbased on visual or dialogue-based date capture. This ensures that the timestamp of any episode is adjusted accordingly, with subsequent episodes indexed relative to this structure. For instance, the \"before\" node is one day prior, and the \"after\" node is one day later. Even without external timestamps, the model understands the temporal order of episodes via temporal edges, maintaining consistency in the passage of time as the agent processes the information.\n\n\n# 3.3.4 DYNAMIC EPISODE RETRIEVAL\n\n\nFigure 4 illustrates dynamic edge traversal for retrieving relevant memories using character, location, event, and temporal weights. The agent classifies the query  $q$  using language models to determine whether it is a \"what\", \"when\", or \"where\" query. \"What\" queries focus on events, \"when\" queries on temporal details, and \"where\" queries on locations. This classification allows the agent to assign the appropriate context and efficiently retrieve relevant memories. Temporal entities (e.g., weeks, months, years) are processed by subtracting fixed intervals from the current date:  $7n$  days for weeks,  $30m$  days for months, and  $365y$  days for years, where  $n$ ,  $m$ , and  $y$  are positive integers.\n\n\nThe similarity score between the query  $q$  and a set  $D_u$  of memory entries is given by  $S_s = \\sum_{e \\in D_u} \\frac{q \\cdot e}{\\|q\\| \\|e\\|}$ . For each neighbor  $v$  of node  $u$ , the weight  $W_{uv}$  is computed as  $W_{uv} = \\sum w(u, v)$ . If  $W_{uv} > \\theta$ , the query set is updated as  $Q \\gets Q \\cup (v, W_{uv})$ .\n\n\nTemporal edges  $T_{\\mathrm{edge}}(E_{t-1}, E_t)$  maintain the sequence of events without re-evaluating the entire graph, with date and time represented as separate nodes for indexing. Explicit timestamps in queries map directly to temporal nodes, while contextual queries traverse the graph based on query type: event weights for \"what,\" temporal weights for \"when,\" and location weights for \"where\" queries. This framework (Figure 4) efficiently retrieves memory clusters, providing task-specific outputs: free-form text for episodic QA and recommendations, memory nodes for experience localization, and goal directives for RL agents (AppendixE.4). Additional details of dynamic node retrieval are given in Appendix B.5\n\n\n![Figure 4: Dynamic edge traversal. Episodic graph representation of event relationships with different edge weights. The start node (E1) is highlighted in cyan, and the node with the highest similarity (E5) is in yellow. Edge colors and styles denote distinct relationship types: temporal edges (solid black), location weights (dashed blue), character weights (dotted red), and event weights (solid green). The legend clarifies the significance of the nodes and edges in the graph.](images/74618abefdcab9199f37dc03c74ba7242f87469f80e2152865a1585d8119a1f1.jpg)\n\n\n# 3.4 IMPACT OF NODE REMOVAL ON CONNECTIONS\n\n\nWhen an episode, such as  $E_{2}$ , is removed from the dynamic graph as part of the forgetting mechanism, its associated semantic links are eliminated, disrupting connections. However, the temporal edges are readjusted, allowing  $E_{3}$  to connect to the next relevant node, preserving the chronological structure. Consequently,  $E_{3}$  updates its temporal link to  $E_{1}$ , maintaining event sequence continuity. This selective removal of semantic associations, while preserving temporal coherence, eliminates the need for a full graph reevaluation. The remaining nodes retain contextual relevance, enabling the agent to reason based on prior experiences. Thus, implementing a forgetting mechanism does not compromise the integrity of the episodic memory graph. We have tested node removal mechanism by using frequency based weight decay to prune nodes as described in Appendix C. Results after node pruning are given in E.2.2\n\n\n# 4 RECOMMENDATION USING EMCA\n\n\nWhen assisting individuals with memory impairments, a cognitive agent utilizes past interactions to provide personalized support by extracting a personalized cluster  $C_p$ , representing the relevant memory subgraph for the individual. This subgraph consists of episodes, actions, and events related to the person:\n\n\n$$\n\n$$\nC _ {p} = \\left\\{G _ {p} \\mid \\text {e p i s o d e s a s s o c i a t e d w i t h p e r s o n} p \\right\\}\n$$\n\n$$\n\n\nwhere  $G_{p} = (V_{p},E_{p})$  includes episodes  $V_{p}$  and edges  $E_{p}$ , preserving temporal and contextual information.\n\n\nThe agent then identifies event clusters  $C_e$ , representing actions performed by the individual over time:\n\n\n$$\n\n$$\nC _ {e} = \\left\\{E _ {e} \\mid \\text {e v e n t s a s s o c i a t e d w i t h e p e s i o d e s i n} G _ {p} \\right\\}\n$$\n\n$$\n\n\nTo provide recommendations, the agent identifies the most recent event sequence  $S_{t} = (S_{1}, S_{2}, \\ldots, S_{t})$  and searches the memory subgraph for a matching sequence, determining the next action  $S_{t+1} = \\operatorname{argmax}_{S \\in G_{p}} \\mathbb{I}(S_{t} \\subset S)$ .\n\n\nIf multiple matches occur, the most frequent next event is selected:  $S_{t + 1} = \\mathrm{mode}\\big(\\{S_{t + 1}\\in \\mathcal{S}\\mid \\mathcal{S}_t\\subset \\mathcal{S},\\mathcal{S}\\in G_p\\} \\big)$ . This method allows the agent to generate recommendations even without recurring patterns, leveraging past episodic data to support individuals with memory impairments. Temporal graph networks and Hopfield memory networks rely on pattern recognition to predict future actions. In contrast, our approach allows the agent to make predictions and provide recommendations without depending on explicit pattern-based mechanisms, offering more flexible and adaptive support for individuals with memory impairments.\n\n\n# 5 DATASET\n\n\nWe propose a comprehensive dataset framework designed to evaluate and enhance episodic memory systems in artificial agents. This framework integrates multiple datasets, including a custom set of episodic questions based on the TV series The Big Bang Theory, spanning all nine seasons (181 episodes). The aim is to assess memory recall and narrative understanding in complex scenarios.\n\n\nWe introduce the Agent Dataset, a 10-episode time-series dataset created in Unity3D, where a virtual agent performs tasks and interacts with characters in realistic environments, simulating the role of companion robots. This dataset emphasizes the importance of multi-sensory inputs and task execution, challenging the agent to process and integrate information from dialogues and visual cues to maintain task order and achieve context-driven objectives.\n\n\nAdditionally, we adapted the Ego4D dataset, restructuring its activity sequences into simulated chronological episodes to address the original absence of time-series data\u2014portraying an agent performing a series of activities over 30 days. We also combined group activity videos designed for active speaker recognition. This transformation enables episodic queries such as \"Where did I place the agricultural tool on the last day of farming?\", enhancing the ability to localize and retrieve temporal experiences effectively.\n\n\nTogether with the PerLTQA Du et al. (2024) and LLQA Dolan & Brockett (2005) datasets, which test essential episodic memory dimensions\u2014\"what\" (context), \"when\" (time), and \"where\" (place)\u2014this framework forms a robust benchmark for evaluating advanced episodic memory capabilities in AI systems.\n\n\nData Annotation: The data was carefully annotated to tag scene information and identify characters in dialogues, ensuring that the model could recognize character presence and understand related events. This included explicitly tagging scene details for location identification and differentiating characters present in the scene versus those mentioned. Events within dialogues were also meticulously annotated to capture key details, facilitating effective memory representation beyond simple summaries. Capturing these essential details is crucial for episodic memory tasks, as it allows the agent to recall past experiences accurately. Each episode was annotated with 10 what, when, and where questions.\n\n\nData Statistics: The dataset includes a distribution of question types: temporal questions make up  $24\\%$ , spatial questions  $38\\%$ , contextual questions  $18\\%$ , multimodal questions (integrating visual and auditory information)  $10\\%$ , and dialogue-based questions  $10\\%$ . These detailed annotations enable the model to handle temporal, spatial, and contextual elements, as well as multimodal inputs, ensuring comprehensive event recognition and effective interaction.\n\n\n# 6 EXPERIMENTS AND EVALUATION\n\n\nWe evaluate our episodic memory cognitive agent (EMCA) on downstream tasks such as episodic memory question answering, benchmarking it against state-of-the-art graph-based and memory models. The agent's performance is tested on memory localization and multimodal memory-based\n\n\nvisual QA. An ablation study compares clustered and non-clustered approaches while assessing the impact of modality removal on episodic memory. These experiments leverage the episodic memory dataset, with EMCA implemented using Whisper, CLIP, and BERT backbones. Additional details are provided in the AppendixD.\n\n\n# 6.1 EVALUATION METRICS\n\n\nThe performance was assessed using recall accuracy for episodic memory question answering (QA), defined as Episodic Recall  $= \\frac{\\text{Number of Correctly Answered Questions}}{\\text{Total Questions}}$ . Additionally, the mean Intersection over Union (mIOU) score was employed to evaluate episodic memory localization.\n\n\n# 6.2 COMPARISON WITH SOTA GRAPH MODELS AND EPISODIC MEMORY QA MODELS\n\n\nWe conducted experiments comparing our approach with state-of-the-art models, including EMR, GraphRag, GNN Rag, TempoQA, and Arigraph, for retrieving relevant information from datasets based on episodic questions. The table below presents a comparative analysis across various datasets, assessing contextual, temporal, spatial, and overall performance metrics.\n\n\nEMR and Dynamic MemQA are pioneering approaches for episodic memory in multimodal QA, while Arigraph integrates semantic and episodic memory for human-like recall, and TempoQA manages temporal data for time-sensitive event comprehension. GraphRag and GNN Rag utilize graph-based memory and retrieval-augmented generation to handle complex data structures. These models were selected to benchmark EMCA against diverse memory models, graph-based RAG methods, and graph structures, including knowledge graphs and temporal graphs. Figure presents the time taken (in seconds) by different methods for various datasets, including Big Bang Theory, PerLTQA, Agent, and LLQA. Our\n\n\nmodel demonstrates significantly faster retrieval times compared to other approaches, particularly when contrasted with methods like Arigraph and TempoQA, which are slower due to their reliance on more computationally intensive processes.\n\n\n![Figure 5: Comparison of Retrieval Times for Various Methods: The chart displays the time taken (in seconds) to retrieve correct memories for four datasets: blue for Big Bang Theory, green for PerLTQA, red for Agent, and cyan for LLQA. The stacked bars represent the cumulative retrieval time for each method](images/acdfdb0cdf25e2a77b5675637b35a8fc2a0c10632ba8e4e4ac7ed33526cc6ef5.jpg)\n\n\nTable 1 demonstrates EMCA's superior performance across four datasets, highlighting its advanced multimodal capabilities and efficient storage and retrieval mechanisms. By integrating textual, visual, and acoustic data, EMCA handles diverse queries effectively, achieving high recall accuracy in contextual, temporal, and spatial questions.\n\n\nEMCA's hierarchical clustering organizes episodic memory by characters, locations, and events, enabling precise retrieval and reducing noise. This dynamic approach outperforms static graph models like GraphRag and GNN Rag, which lack adaptability. EMCA's ability to integrate multimodal data and retrieve targeted memories makes it uniquely suited for episodic memory-based question answering. Additional results with memory models are shown in Appendix E.1.\n\n\n# 6.3 EPISODIC MEMORY LOCALIZATION\n\n\nWe evaluated our model's performance in episodic memory localization using time-series-based questions. Table 2 compares our results with state-of-the-art (SOTA) models recognized for their effectiveness in multimodal data handling within the Ego4D dataset. Metrics include IOU@0.3 with Recall@1 (R@1), IOU@0.5 with Recall@5 (R@5), and mean IOU (mIOU).\n\n\nWhile SOTA models excel in multimodal data tasks, they struggle with video localization using time-series data, which is critical for maintaining accurate, context-aware recall in episodic memory systems. By saving data as a time series, our approach ensures precise temporal alignment,\n\n\n<table><tr><td>Dataset</td><td>Method</td><td>Contextual</td><td>Temporal</td><td>Spatial</td><td>Total</td></tr><tr><td rowspan=\"7\">Big Bang Theory</td><td>Ours</td><td>75</td><td>80</td><td>76</td><td>78</td></tr><tr><td>Dynamic Memory QA</td><td>5</td><td>8</td><td>7.5</td><td>10</td></tr><tr><td>EMR</td><td>14</td><td>10</td><td>13</td><td>15</td></tr><tr><td>GraphRag</td><td>32</td><td>30</td><td>30</td><td>30</td></tr><tr><td>GNN Rag</td><td>25</td><td>24</td><td>23</td><td>27</td></tr><tr><td>TempoQA</td><td>21</td><td>20</td><td>19</td><td>25</td></tr><tr><td>Arigraph</td><td>25</td><td>24</td><td>25</td><td>26</td></tr><tr><td rowspan=\"7\">Perltqa</td><td>Ours</td><td>90</td><td>95</td><td>89</td><td>90</td></tr><tr><td>Dynamic Memory QA</td><td>30</td><td>15</td><td>21</td><td>40</td></tr><tr><td>EMR</td><td>31</td><td>20</td><td>23</td><td>45</td></tr><tr><td>GraphRag</td><td>35</td><td>40</td><td>42</td><td>38</td></tr><tr><td>GNN Rag</td><td>60</td><td>51</td><td>57</td><td>55</td></tr><tr><td>TempoQA</td><td>45</td><td>50</td><td>51</td><td>52</td></tr><tr><td>Arigraph</td><td>55</td><td>75</td><td>71</td><td>79</td></tr><tr><td rowspan=\"7\">Agent Dataset</td><td>Ours</td><td>77</td><td>90</td><td>90</td><td>86</td></tr><tr><td>Dynamic Memory QA</td><td>33</td><td>30</td><td>31</td><td>37</td></tr><tr><td>EMR</td><td>36</td><td>32</td><td>33</td><td>40</td></tr><tr><td>GraphRag</td><td>31</td><td>20</td><td>29</td><td>27</td></tr><tr><td>GNN Rag</td><td>50</td><td>51</td><td>55</td><td>53</td></tr><tr><td>TempoQA</td><td>40</td><td>45</td><td>31</td><td>37</td></tr><tr><td>Arigraph</td><td>52</td><td>53</td><td>53</td><td>51</td></tr><tr><td rowspan=\"7\">LLQA</td><td>Ours</td><td>86</td><td>85</td><td>86</td><td>86</td></tr><tr><td>Dynamic Memory QA</td><td>10</td><td>20</td><td>15</td><td>20</td></tr><tr><td>EMR</td><td>15</td><td>23</td><td>20</td><td>23</td></tr><tr><td>GraphRag</td><td>24</td><td>23</td><td>22</td><td>21</td></tr><tr><td>GNN Rag</td><td>49</td><td>45</td><td>40</td><td>46</td></tr><tr><td>TempoQA</td><td>42</td><td>41</td><td>45</td><td>45</td></tr><tr><td>Arigraph</td><td>50</td><td>51</td><td>42</td><td>52</td></tr></table>\n\n\nTable 1: Comparison of Recall Accuracy for Different Question Types Across Datasets\n\n\n<table><tr><td>Method</td><td>IOU = 0.3 R@1</td><td>IOU = 0.5 R@5</td><td>mIOU</td></tr><tr><td>2D-TAN</td><td>4.32</td><td>2.60</td><td>5.62</td></tr><tr><td>VSLNet</td><td>8.09</td><td>7.03</td><td>7.65</td></tr><tr><td>CONE</td><td>10.55</td><td>7.54</td><td>9.04</td></tr><tr><td>RELER</td><td>12.89</td><td>8.14</td><td>10.51</td></tr><tr><td>SPOTEM</td><td>18.13</td><td>13.43</td><td>15.78</td></tr><tr><td>Ours</td><td>26.46</td><td>25.5</td><td>25.98</td></tr></table>\n\n\nminimizes outdated recall, and enhances context comprehension. Furthermore, integrating visual and dialogue modalities provides a comprehensive understanding of interactions and events, significantly improving episodic memory localization. Visualization of Episodic memory localization is as given in Appendix E.3\n\n\n# 6.4 ABLATION STUDIES\n\n\nWe validated our EMCA approach by systematically removing different modalities from the architecture: first the visual module, then the speech module, and finally the music module. Performance was evaluated on videos from The Big Bang Theory and the Agent Dataset by posing episodic questions. The episodic recall capacity, as detailed in 6.2, was assessed based on these crafted episodic questions. Additionally, we measured retrieval time as the number of episodes increased.\n\n\nTable 2: Performance comparison on episodic memory localization.\n\n\n<table><tr><td>Modalities</td><td>Big Bang Theory</td><td>Agent Dataset</td></tr><tr><td>Full</td><td>78</td><td>86</td></tr><tr><td>No Vision</td><td>16</td><td>15</td></tr><tr><td>No Acoustics</td><td>65</td><td>40</td></tr><tr><td>No Dialogues</td><td>36</td><td>20</td></tr></table>\n\n\nTable 3: Comparison of Modalities.\n\n\n<table><tr><td>Method</td><td>Retrieval Time (Average)</td></tr><tr><td>Dynamic Traversal</td><td>5.6</td></tr><tr><td>BFS</td><td>8.9</td></tr><tr><td>DFS</td><td>11</td></tr><tr><td>DFS + BFS</td><td>7.75</td></tr></table>\n\n\nTable 4: Graph Traversal Retrieval Times.\n\n\n<table><tr><td>Number of Episodes</td><td>Retrieval Time (ms)</td></tr><tr><td>10</td><td>9.11</td></tr><tr><td>50</td><td>11.0</td></tr><tr><td>100</td><td>11.5</td></tr><tr><td>181</td><td>12.0</td></tr></table>\n\n\nTable 5: Retrieval Times vs. Episodes.\n\n\nAs shown in Table 3, retaining all modalities is essential for agents operating in multimodal environments to make informed decisions and reason effectively based on past events.\n\n\n![Figure 6: Comparison of the Number of Edges Traversed and Retrieval Time (ms) by Query Type for Clustered and Non-Clustered Approaches](images/1ee63848f84b0867c2497852a8c6e49089838e7ffdf57a86e097ae022130ab4d.jpg)\n\n\n# ASSESSMENT OF RETRIEVAL TIME ACROSS TRAVERSAL METHODS\n\n\nWe evaluated the time complexity of our dynamic graph traversal method against traditional techniques, summarized in Table 4. The dynamic traversal method demonstrated the lowest average retrieval time, outperforming BFS, DFS, and their combined approach. When explicit timestamps are provided, queries are treated as temporal, leading to faster retrieval compared to event-based queries.\n\n\nComparing clustered and non-clustered graphs in terms of number of path traversals required and time taken to retrieval query As shown in Figure 6, the dynamic clustered graph significantly reduces query retrieval time and the number of edges traversed compared to the non-clustered graph, demonstrating the efficiency of clustering in optimizing graph traversal. Dynamic edges and clusters enhance adaptability, improving performance across various query types. Table 5 shows a modest increase in retrieval time as the number of episodes grows, highlighting the clustering mechanism's role in keeping retrieval times low. These results emphasize the scalability and efficiency of our method. Replacing an episodic memory graph with a knowledge graph introduces significant complexity, especially when incorporating temporal features. For instance, an episodic graph represents three days of experiences with just three nodes, one per day, while a temporal knowledge graph may require 15-20 nodes and edges, drastically increasing structural complexity. This added intricacy hampers memory retrieval and action prediction, making them less efficient than the streamlined architecture of episodic graphs. In visual domains, where patterns and regularities are prevalent, knowledge graphs excel by leveraging these consistencies. However, in dynamic scenarios like conversations, interactions are often unique, necessitating frequent creation of new relations. This dynamic evolution further complicates the knowledge graph and reduces practical efficiency. Additional results, including ablation studies and node pruning effects, are detailed in Appendices E.2.1 and E.2..\n\n\n# 7 CONCLUSION, LIMITATIONS, AND SOCIAL IMPACT\n\n\nThe EMCA system is designed to understand and process temporal timescales in a manner similar to human cognition. It dynamically organizes events along subjective timescales, allowing it to track and retrieve memories based on the relative importance of events rather than fixed timestamps. This enables the system to adapt to varying time-frames, understanding how past experiences may influence present contexts. By mimicking human-like temporal reasoning, EMCA can handle queries about \"when\" events occurred in both short-term and long-term memory, adjusting its responses based on the perceived significance of past events, much like human memory recalls important moments more vividly than routine occurrences.\n\n\nSocial Impact: The Episodic Memory Cognitive Agent has the potential to serve as a valuable social agent, particularly for individuals with memory-related disorders. However, without robust data protection mechanisms, there is a risk of data breaches, which could have serious privacy implications.\n\n\nLimitations: Currently, EMCA lacks a forgetting mechanism and the ability to identify key events, as humans do, based on factors such as surprise, novelty, or emotional significance. These aspects will be addressed in future work.\n\n\n# REFERENCES\n\n- Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, and Evgeny Burnaev. Arigraph: Learning knowledge graph world models with episodic memory for llm agents, 2024. URL https://arxiv.org/abs/2407.04363.\n\n- Leonard B\u00e4rmann, Chad DeChant, Joana Plewnia, Fabian Peller-Konrad, Daniel Bauer, Tamim As-four, and Alex Waibel. Episodic memory verbalization using hierarchical representations of life-long robot experience, 2024. URL https://arxiv.org/abs/2409.17702.\n\n- Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context, 2019. URL https://arxiv.org/abs/1901.02860.\n\n- Samyak Datta, Sameer Dharur, Vincent Cartillier, Ruta Desai, Mukul Khanna, Dhruv Batra, and Devi Parikh. Episodic memory question answering, 2022. URL https://arxiv.org/abs/2205.01652.\n\n- William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/I05-5002.\n\n- Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, and Kam-Fai Wong. Perl'tqa: A personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering, 2024. URL https://arxiv.org/abs/2402.16288.\n\n- Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization, 2024. URL https://arxiv.org/abs/2404.16130.\n\n- Moonsu Han, Minki Kang, Hyunwoo Jung, and Sung Ju Hwang. Episodic memory reader: Learning what to remember for question answering from streaming data, 2019. URL https://arxiv.org/abs/1903.06164.\n\n- Hung Le, Truyen Tran, and Svetha Venkatesh. Self-attentive associative memory, 2020. URL https://arxiv.org/abs/2002.03519.\n\n- Naiyuan Liu, Xiaohan Wang, Xiaobo Li, Yi Yang, and Yueting Zhuang. Refer@zju-alibaba submission to the ego4d natural language queries challenge 2022, 2022. URL https://arxiv.org/abs/2207.00383.\n\n- Costas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model reasoning, 2024. URL https://arxiv.org/abs/2405.20139.\n\n- Costas Mavromatis, Prasanna Lakkur Subramanyam, Vassilis N. Ioannidis, Soji Adeshina, Phillip R. Howard, Tetiana Grinberg, Nagib Hakim, and George Karypis. Tempoqr: Temporal question reasoning over knowledge graphs, 2021. URL https://arxiv.org/abs/2112.05785.\n\n- Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learning, 2019. URL https://arxiv.org/abs/1910.06764.\n\n- Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. Hopfield networks is all you need, 2021. URL https://arxiv.org/abs/2008.02217.\n\n- Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. Temporal graph networks for deep learning on dynamic graphs, 2020. URL https://arxiv.org/abs/2006.10637.\n\n- Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. One-shot learning with memory-augmented neural networks, 2016. URL https://arxiv.org/abs/1605.06065.\n\n- Ahmet Sarg\u00fcn. Graph mixer networks, 2023. URL https://arxiv.org/abs/2301.12493.\n\n- Endel Tulving. Episodic memory: From mind to brain. Annual review of psychology, 53:1-25, 02 2002. doi: 10.1146/annurev.psych.53.100901.135114.\n\n- Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual question answering, 2016. URL https://arxiv.org/abs/1603.01417.\n\n- Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, and Luke Zettlemoyer. VLM: Task-agnostic video-language model pretraining for video understanding. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 4227-4239, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.370. URL https://aclanthology.org/2021.findings-acl.370.\n\n- Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. Learning 2d temporal adjacent networks for moment localization with natural language, 2020. URL https://arxiv.org/abs/1912.03590.\n\n- Zhu Zhang, Chang Zhou, Jianxin Ma, Zhijie Lin, Jingren Zhou, Hongxia Yang, and Zhou Zhao. Learning to rehearse in long sequence memorization, 2021. URL https://arxiv.org/abs/2106.01096.\n\nResponse: ### Prediction: \\(\\boxed{Accept}\\)\n\n#### Justification:\n\n**Originality:**\n- The paper introduces a novel framework, EMCA, for episodic memory representation that integrates multimodal data (audio, visual, and text) and leverages graph-based structures. This originality sets it apart from existing episodic memory models that typically focus on either temporal or semantic aspects of memory.\n  \n**Methodological Rigor:**\n- The authors describe a comprehensive methodology that involves collecting multimodal experiences, organizing them into a graph, and retrieving relevant experiences. The use of graph-based techniques for memory representation is innovative and justified by the need for flexible and dynamic memory management.\n- The paper provides detailed descriptions of the processing steps for audio and visual data, which are essential for building robust episodic memory. The use of Mel spectrograms and convolutional networks for feature extraction is a sound approach.\n- The dynamic clustering mechanism and the way temporal edges are handled are innovative and address limitations of existing models that struggle with temporal reasoning and missing timestamps.\n\n**Significance of Contribution:**\n- The proposed model excels in handling complex scenarios involving multimodal data, temporal reasoning, and dynamic clustering. This capability is crucial for cognitive agents that need to adapt to varied environments and tasks.\n- The dataset proposed (including the custom Big Bang Theory dataset and the adapted Ego4D dataset) is comprehensive and designed to evaluate the model's performance on real-world episodic memory tasks, thereby validating its effectiveness.\n  \n**Clarity of Presentation:**\n- The paper is well-organized, with clear sections detailing the methodology, experimental setup, and results. The authors provide detailed explanations of their models and the experimental design, making it easy for readers to follow and replicate the work.\n  \n**Alignment with Journal Scope:**\n- The paper fits well within the scope of ICLR, which focuses on cutting-edge research in machine learning, particularly in areas like reinforcement learning, natural language processing, and multimodal systems. The episodic memory framework developed here is highly relevant to these areas.\n\n**Strengths:**\n- **Innovative Approach:** The combination of multimodal data and graph-based memory representation is a novel contribution.\n- **Flexibility and Adaptability:** The dynamic clustering and edge mechanisms ensure that the model can adapt to new experiences without requiring extensive retraining.\n- **Comprehensive Evaluation:** The use of a custom dataset and a wide range of evaluation metrics (recognition accuracy, temporal and spatial precision) provides a thorough assessment of the model's performance.\n\n**Weaknesses:**\n- **Complexity:** The model is quite complex, which might make implementation challenging for researchers and practitioners who are not already familiar with graph-based approaches and multimodal data processing.\n- **Resource Intensive:** Training the model on large datasets (especially multimodal ones) requires substantial computational resources, which could be a barrier for some researchers or organizations.\n- **Limited Temporal Patterns:** While the model excels in temporal reasoning, it relies heavily on explicit timestamps. Future work could explore ways to incorporate more implicit temporal information to handle more unpredictable scenarios.\n\n### Conclusion:\nGiven the innovative approach, strong methodological rigor, and comprehensive evaluation, the paper is likely to be accepted. The challenges identified are manageable and can be addressed in future work, which would strengthen the model further.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "TextualGradientDescent prompt for update", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "165", "funcName": "_update_prompt", "created": "1768634790.9277318", "msecs": "927.0", "relativeCreated": "135751.2059211731", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "prompt": "Here is the role of the variable you will improve: <ROLE>System prompt for paper acceptance prediction. This prompt sets the context and approach for the AI reviewer.</ROLE>.\n\nThe variable is the text within the following span: <VARIABLE> You are an expert academic reviewer tasked with evaluating research (...) clear rationale for your prediction, weighing strengths and weaknesses systematically. </VARIABLE>\n\nHere is the context and feedback we got for the variable:\n\n<CONTEXT></CONTEXT>\n\nImprove the variable (System prompt for paper acceptance prediction. This prompt sets the context and approach for the AI reviewer.) using the feedback provided in <FEEDBACK> tags.\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.", "message": "TextualGradientDescent prompt for update"}
{"name": "textgrad", "msg": "TextualGradientDescent optimizer response", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "179", "funcName": "step", "created": "1768634803.4703941", "msecs": "470.0", "relativeCreated": "148293.86830329895", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "optimizer.response": "<think>\nOkay, let's see. The user wants me to improve the system prompt for a paper acceptance prediction. The original variable is a system prompt that tells an AI reviewer to evaluate research, consider strengths and weaknesses, and provide a clear rationale.\n\nFirst, I need to understand the role of this variable. It's the system prompt, so it's crucial for setting the context and approach for the AI reviewer. The feedback isn't provided here, but the user mentioned there's feedback in <FEEDBACK> tags. Wait, looking back, the context is empty, so maybe the feedback is implied or there's an assumption that the original prompt needs improvement based on standard practices.\n\nThe original prompt says: \"You are an expert academic reviewer tasked with evaluating research (...) clear rationale for your prediction, weighing strengths and weaknesses systematically.\" \n\nHmm, maybe the original is a bit vague. Let me think about what a good system prompt should include. It should clearly define the task, the criteria for evaluation, and the expected output. Also, it should guide the AI to be thorough and objective.\n\nPerhaps the original prompt lacks specific criteria. For example, mentioning aspects like originality, methodology, significance, clarity, and adherence to guidelines. Also, the output should be structured, maybe with a confidence level and specific reasons. \n\nThe user's feedback might have pointed out that the prompt is too generic. So, improving it by adding specific evaluation criteria and structuring the output would make it more effective. Also, emphasizing objectivity and evidence-based reasoning could be important. \n\nSo, the improved prompt should include explicit criteria, a structured output format, and instructions to balance strengths and weaknesses. Maybe something like: \n\n\"Act as an expert academic reviewer to assess research papers for acceptance. Evaluate based on originality, methodology, significance, clarity, and adherence to guidelines. Provide a structured prediction with confidence level (High/Medium/Low), detailing evidence-based rationale. Systematically weigh strengths and weaknesses, ensuring objectivity and alignment with field standards.\"\n\nThat adds specificity, structure, and clarity. It also includes confidence levels and explicit criteria, which the original might have missed. I should check if that's in line with the role description. Yes, the role is to set the context and approach, so adding these elements would make the AI's task clearer and more precise.\n</think>\n\n<IMPROVED_VARIABLE>You are an expert academic reviewer tasked with evaluating research papers for acceptance. Assess based on originality, methodology rigor, significance, clarity, and alignment with field standards. Provide a structured prediction with confidence level (High/Medium/Low), detailing evidence-based rationale. Systematically weigh strengths and weaknesses, ensuring objectivity and adherence to peer review principles.</IMPROVED_VARIABLE>", "message": "TextualGradientDescent optimizer response"}
{"name": "textgrad", "msg": "TextualGradientDescent updated text", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "187", "funcName": "step", "created": "1768634803.4705126", "msecs": "470.0", "relativeCreated": "148293.98679733276", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "parameter.value": "You are an expert academic reviewer tasked with evaluating research papers for acceptance. Assess based on originality, methodology rigor, significance, clarity, and alignment with field standards. Provide a structured prediction with confidence level (High/Medium/Low), detailing evidence-based rationale. Systematically weigh strengths and weaknesses, ensuring objectivity and adherence to peer review principles.", "message": "TextualGradientDescent updated text"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634812.141519", "msecs": "141.0", "relativeCreated": "156964.9932384491", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers for acceptance. Assess based on originality, methodology rigor, significance, clarity, and alignment with field standards. Provide a structured prediction with confidence level (High/Medium/Low), detailing evidence-based rationale. Systematically weigh strengths and weaknesses, ensuring objectivity and adherence to peer review principles.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Hyperparameter Transfer Across Developer Adjustments\n\n# Abstract\nAfter developer adjustments to a machine learning (ML) algorithm, how can the results of an old hyperparameter optimization (HPO) automatically be used to speedup a new HPO? This question poses a challenging problem, as developer adjustments can change which hyperparameter settings perform well, or even the hyperparameter search space itself. While many approaches exist that leverage knowledge obtained on previous tasks, so far, knowledge from previous development steps remains entirely untapped. In this work, we remedy this situation and propose a new research framework: hyperparameter transfer across adjustments (HT-AA). To lay a solid foundation for this research framework, we provide four simple HT-AA baseline algorithms and eight benchmarks changing various aspects of ML algorithms, their hyperparameter search spaces, and the neural architectures used. The best baseline, on average and depending on the budgets for the old and new HPO, reaches a given performance 1.2-3.6x faster than a prominent HPO algorithm without transfer. As HPO is a crucial step in ML development but requires extensive computational resources, this speedup would lead to faster development cycles, lower costs, and reduced environmental impacts. To make these benefits available to ML developers off-the-shelf and to facilitate future research on HT-AA, we provide python packages for our baselines and benchmarks.\n\n\n![](images/0b383a6c0235da4a9c1e71b8cf8966c07396ac0b8059214d65589a0fc3cd12e0.jpg)\n\n\nGraphical Abstract: Hyperparameter optimization (HPO) across adjustments to the algorithm or hyperparameter search space. A common practice is to perform HPO from scratch after each adjustment or to somehow manually transfer knowledge. In contrast, we propose a new research framework about automatic knowledge transfers across adjustments for HPO.\n\n\n# 1 INTRODUCTION: A NEW HYPERPARAMETER TRANSFER FRAMEWORK\n\n\nThe machine learning (ML) community arrived at the current generation of ML algorithms by performing many iterative adjustments. Likely, the way to artificial general intelligence requires many more adjustments. Each algorithm adjustment could change which settings of the algorithm's hyperparameters perform well, or even the hyperparameter search space itself (Chen et al., 2018; Li et al., 2020). For example, when deep learning developers change the optimizer, the learning rate's optimal value\n\n\nlikely changes, and the new optimizer may also introduce new hyperparameters. Since ML algorithms are known to be very sensitive to their hyperparameters (Chen et al., 2018; Feurer & Hutter, 2019), developers are faced with the question of how to adjust their hyperparameters after changing their code. Assuming that the developers have results of one or several hyperparameter optimizations (HPOs) that were performed before the adjustments, they have two options:\n\n\n1. Somehow manually transfer knowledge from old HPOs.\n\n\nThis is the option chosen by many researchers and developers, explicitly disclosed, e.g., in the seminal work on AlphaGo (Chen et al., 2018). However, this is not a satisfying option since manual decision making is time-consuming, often individually designed, and has already lead to reproducibility problems (Musgrave et al., 2020).\n\n\n2. Start the new HPO from scratch.\n\n\nLeaving previous knowledge unutilized can lead to higher computational demands and worse performance (demonstrated empirically in Section 5). This is especially bad as the energy consumption of ML algorithms is already recognized as an environmental problem. For example, deep learning pipelines can have  $\\mathrm{CO}_{2}$  emissions on the order of magnitude of the emissions of multiple cars for a lifetime (Strubell et al., 2019), and their energy demands are growing furiously: Schwartz et al. (2019) cite a \"300,000x increase from 2012 to 2018\". Therefore, reducing the number of evaluated hyperparameter settings should be a general goal of the community.\n\n\nThe main contribution of this work is the introduction of a new research framework: Hyperparameter transfer across adjustments (HT-AA), which empowers developers with a third option:\n\n\n3. Automatically transfer knowledge from previous HPOs.\n\n\nThis option leads to advantages in two aspects: The automation of decision making and the utilization of previous knowledge. On the one hand, the automation allows to benchmark strategies, replaces expensive manual decision making, and enables reproducible and comparable experiments; on the other hand, utilizing previous knowledge leads to faster development cycles, lower costs, and reduced environmental impacts.\n\n\nTo lay a solid foundation for the new HT-AA framework, our individual contributions are as follows:\n\n- - We formally introduce a basic version of the HT-AA problem (Section 2).\n\n- - We provide four simple baseline algorithms for our basic HT-AA problem (Section 3).\n\n- - We provide a comprehensive set of eight novel benchmarks for our basic HT-AA problem (Section 4).\n\n- - We show the advantage of transferring across developer adjustments: some of our simple baseline algorithms outperform HPO from scratch up to 1.2\u20133.6x on average depending on the budgets (Section 5).\n\n- - We empirically demonstrate the need for well-vetted algorithms for HT-AA: two baselines modelled after actually-practiced manual strategies perform horribly on our benchmarks (Section 5).\n\n- - We relate the HT-AA framework to existing research efforts and discuss the research opportunities it opens up (Section 6).\n\n- - To facilitate future research on HT-AA, we provide open-source code for our experiments and benchmarks and provide a python package with an out-of-the-box usable implementation of our HT-AA algorithms.\n\n\n# 2 HYPERPARAMETER TRANSFER ACROSS ADJUSTMENTS\n\n\nAfter presenting a broad introduction to the topic, we now provide a detailed description of hyperparameter transfer across developer adjustments (HT-AA). We first introduce hyperparameter optimization, then discuss the types of developer adjustments, and finally describe the transfer across these adjustments.\n\n\n![Figure 1: Developer adjustments from the perspective of hyperparameter optimization.](images/2f9edf255f9f619c029dff76a9f49f67e722106c60a42316f03bcc64cc2166b4.jpg)\n\n\n![](images/d97b4d6e4bef8aeeb02b9019f27778e1c3f253b8eee5f22c80a8e2da0a91bb92.jpg)\n\n\n![](images/648f5714defe24ebcd50974be92ea93d349c81d3c70daca64b912aa00f18b26b.jpg)\n\n\nHyperparameter optimization (HPO) The HPO formulation we utilize in this work is as follows:\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {x} \\in \\mathcal {X}} {\\text {m i n i m i z e}} f _ {\\mathcal {A}} (\\boldsymbol {x}) \\quad \\text {w i t h} b \\tag {1}\n$$\n\n$$\n\n\nwhere  $f_{\\mathcal{A}}(\\pmb{x})$  is the objective function for ML algorithm  $\\mathcal{A}$  with hyperparameter setting  $\\pmb{x}, b$  is the number of available evaluations, and  $\\mathcal{X}$  is the search space. We allow the search space  $\\mathcal{X}$  to contain categorical and numerical dimensions alike and consider only sequential evaluations. We refer to a specific HPO problem with the 3-tuple  $(\\mathcal{X}, f_{\\mathcal{A}}, b)$ . For a discussion on potential extensions of our framework to different HPO formulations, we refer the reader to Section 6.\n\n\nDeveloper adjustments We now put developer adjustments on concrete terms and introduce a taxonomy of developer adjustments. We consider two main categories of developer adjustments: ones that do not change the search space  $\\mathcal{X}$  (homogeneous adjustments) and ones that do (heterogenous adjustments). Homogeneous adjustments could either change the algorithm's implementation or the hardware that the algorithm is run on. Heterogeneous adjustments can be further categorized into adjustments that add or remove a hyperparameter (hyperparameter adjustments) and adjustments that change the search space for a specific hyperparameter (range adjustments). Figure 1 shows an illustration of the adjustment types.\n\n\nKnowledge transfer across adjustments In general, a continuous stream of developer adjustments could be accompanied by multiple HPOs. We simplify the problem in this fundamental work and only consider the transfer between two HPO problems; we discuss a potential extension in Section 6. The two HPO problems arise from adjustments  $\\Psi$  to a ML algorithm  $\\mathcal{A}_{\\mathrm{old}}$  and its search space  $\\mathcal{X}_{\\mathrm{old}}$ , which lead to  $\\mathcal{A}_{\\mathrm{new}}, \\mathcal{X}_{\\mathrm{new}} \\coloneqq \\Psi(\\mathcal{A}_{\\mathrm{old}}, \\mathcal{X}_{\\mathrm{old}})$ . Specifically, the hyperparameter transfer across adjustments problem is to solve the HPO problem ( $\\mathcal{X}_{\\mathrm{new}}, f_{\\mathcal{A}_{\\mathrm{new}}}, b_{\\mathrm{new}}$ ), given the results for ( $\\mathcal{X}_{\\mathrm{old}}, f_{\\mathcal{A}_{\\mathrm{old}}}, b_{\\mathrm{old}}$ ). Compared to HPO from scratch, developers can choose a lower budget  $b_{\\mathrm{new}}$ , given evidence for a transfer algorithm achieving the same performance faster.\n\n\nRelation to hyperparameter transfer across tasks (HT-AT) There exists an extensive research field that studies the transfer across tasks for HPOs (Vanschoren, 2018). The main difference to hyperparameter transfer across adjustments is that HT-AT assumes that there are no changes to hardware, ML algorithm, and search space, whereas HT-AA is characterized by these changes. Another difference is that most work on HT-AT considers large amounts of meta-data (up to more than a thousand previous HPO runs and function evaluations (Wang et al., 2018; Metz et al., 2020)), whereas in the basic HT-AA problem described above, only one previous HPO run is available. Compared to HT-AT approaches, the main technical challenge for HT-AA approaches are heterogeneous developer adjustments, as homogeneous HT-AA problems, where none of the adjustments changes the search space, are syntactically equivalent to HT-AT problems with only one prior HPO run. For this homogeneous HT-AA, existing approaches for HT-AT could, in principle, be applied without modification; this includes, for example the transfer acquisition function (Wistuba et al., 2018), multi-task bayesian optimization (Swersky et al., 2013), multi-task adaptive bayesian linear regression (Perrone et al., 2018), ranking-weighted gaussian process ensemble (Feurer et al., 2018), and difference-modelling bayesian optimisation (Shilton et al., 2017). However, the low amount of meta-data in (homogeneous) HT-AA poses additional challenges.\n\n\n# 3 BASELINE ALGORITHMS FOR HT-AA\n\n\nIn this section we present four baselines for the specific instantiation of the hyperparameter transfer across adjustments (HT-AA) framework discussed in Section 2. We resist the temptation to introduce complex approaches alongside a new research framework and instead focus on a solid foundation. Specifically, we focus on approaches that do not use any knowledge from the new HPO for the transfer and include two baselines that are modelled after actually-practiced manual strategies. We first introduce the two basic HPO algorithm that the transfer approaches build upon, then introduce two decompositions of HPO search spaces across adjustments, and finally, we present the four baselines themselves.\n\n\n# 3.1 BACKGROUND: HYPERPARAMETER OPTIMIZATION ALGORITHMS\n\n\nWe instantiate our transfer algorithms with two different basic hyperparameter optimization algorithms, and also evaluate against these two algorithms (Section 5). Both algorithms are based on Bayesian optimization (BO): Given observations  $\\mathcal{D} = \\{(x_k, f_A(x_k))\\}_{k=1}^n$ , BO fits a probabilistic model  $p(f_A \\mid \\mathcal{D})$  for the objective function  $f_A$ ; then, to decide which hyperparameter setting to evaluate next, BO maximizes an acquisition function  $\\alpha(\\boldsymbol{x} \\mid \\mathcal{D})$ , which uses this probabilistic model. For both approaches we follow standard practice and sample configurations randomly in a fraction of cases (here  $1/3$ ). For a recent review of BO we refer to Shahriari et al. (2016). In the following, we provide details for the two basic HPO algorithms which we use.\n\n\nTree-Structured Parzen Estimator One of the two algorithms that we use is the Tree-Structured Parzen Estimator (TPE) (Bergstra et al., 2011), which is the default algorithm in the popular HyperOpt package (Bergstra et al., 2013). TPE uses kernel density estimators to model the densities  $l(\\pmb{x})$  and  $g(\\pmb{x})$ , for the probability of a given hyperparameter configuration  $\\pmb{x}$  being worse ( $l$ ), or better ( $g$ ), than the best already evaluated configuration. To decide which configuration to evaluate, TPE then maximizes the acquisition function  $\\pmb{x}^* \\in \\arg \\max_{\\pmb{x} \\in \\mathcal{X}} g(\\pmb{x}) / b(\\pmb{x})$  approximately. In our experiments, we use the TPE implementation and hyperparameter settings from Falkner et al. (2018), which requires  $2(\\dim(\\mathcal{X}_{\\mathrm{new}}) + 1)$  evaluations to build a model.\n\n\nBayesian optimization with Gaussian processes Bayesian optimization with Gaussian processes uses Gaussian process regression (Rasmussen & Williams, 2006) to fit a probabilistic model  $p(f_{\\mathcal{A}} \\mid \\mathcal{D})$ . To generate a sample, we optimize the widely used expected improvement (Jones et al., 1998) acquisition function. If there are less than  $\\dim(\\mathcal{X})$  evaluations, we sample randomly. In our experiments, we use the implementation from the SMAC3 package (Lindauer et al., 2017) with default kernels, hyperparameters, and optimization procedures. In the following we will refer to BO with Gaussian processes with GP.\n\n\n# 3.2 PRELIMINARIES: SEARCH SPACE DECOMPOSITIONS\n\n\nHyperparameter adjustments For hyperparameter adjustments the new search space  $\\mathcal{X}_{\\mathrm{new}}$  and the old search space  $\\mathcal{X}_{\\mathrm{old}}$  only differ in hyperparameters, not in hyperparameter ranges, so we can decompose the search spaces as  $\\mathcal{X}_{\\mathrm{new}} = \\mathcal{X}_{\\mathrm{only - new}}\\times \\mathcal{X}_{\\mathrm{both}}$  and  $\\mathcal{X}_{\\mathrm{old}} = \\mathcal{X}_{\\mathrm{both}}\\times \\mathcal{X}_{\\mathrm{only - old}}$ , where  $\\mathcal{X}_{\\mathrm{both}}$  is the part of the search space that remains unchanged across adjustments (see Figure 2 for reference). All baselines use this decomposition and project the hyperparameter settings that were evaluated in the old HPO from  $\\mathcal{X}_{\\mathrm{old}}$  to  $\\mathcal{X}_{\\mathrm{both}}$ .\n\n\nRange adjustments A range adjustment can remove values from the hyperparameter range or add values. For an adjustment of hyperparameter range  $\\mathcal{X}_{old}^{H_i}$  to  $\\mathcal{X}_{\\mathrm{new}}^{H_i}$  this can be expressed as  $\\mathcal{X}_{\\mathrm{new}}^{H_i} = \\mathcal{X}_{\\mathrm{both}}^{H_i} \\cup \\mathcal{X}_{\\mathrm{both,range-only-new}}^{H_i}$  with  $\\mathcal{X}_{\\mathrm{both}}^{H_i} = \\mathcal{X}_{old}^{H_i} \\setminus \\mathcal{X}_{\\mathrm{both,range-only-old}}^{H_i}$ .\n\n\n# 3.3 BASELINE ALGORITHMS\n\n\nOnly Optimize New Hyperparameters A common developer strategy to manually transfer across adjustments is to set previous hyperparameters in  $\\mathcal{X}_{\\mathrm{both}}$  to the best setting of the previous HPO and only optimize hyperparameters in  $\\mathcal{X}_{\\mathrm{only-new}}$  (Agostinelli et al., 2014; Huang et al., 2017; Wu & He, 2018).\n\n\n![Figure 2: Example search space decomposition for a hyperparameter addition and removal.](images/ea5c832688a7f9bc66bfd5831bbd2d7c7e80932d9ed86ed9d7b70a4ae4a7345e.jpg)\n\n\nIf the previous best setting is not valid anymore, i.e., it has values in  $\\mathcal{X}_{\\mathrm{both},\\mathrm{range - only - old}}^{H_i}$  for a hyperparameter  $H_{i}$  still in  $\\mathcal{X}_{\\mathrm{both}}$ , this strategy uses the best setting that still is a valid configuration. In the following, we refer to this strategy as only-optimize-new.\n\n\nDrop Unimportant Hyperparameters A strategy inspired by manual HT-AA efforts is to only optimize important hyperparameters. The utilization of importance statistics was, for example, explicitly disclosed in the seminal work on AlphaGo (Chen et al., 2018). Here, we determine the importance of each individual hyperparameter with functional analysis of variance (fANOVA) (Hutter et al., 2014) and do not tune hyperparameters with below mean importance. Therefore, this strategy only optimizes hyperparameters in  $\\mathcal{X}_{\\mathrm{only - new}}$  and hyperparameters in  $\\mathcal{X}_{\\mathrm{both}}$  with above mean importance. In the following, we refer to this strategy as drop-unimportant.\n\n\nFirst Evaluate Best The best-first strategy uses only-optimize-new for the first evaluation, and uses standard GP for the remaining evaluations. This strategy has a large potential speedup and low risk as it falls back to standard GP.\n\n\nTransfer GP (TGP) / Transfer TPE (T2PE) The main idea of Transfer GP (TGP) / Transfer TPE (T2PE) is to fit a GP / TPE model on the observations of the previous HPO  $\\mathcal{D}_{\\mathrm{old}} = \\{(x_k, f_{\\mathcal{A}_{\\mathrm{old}}}(\\boldsymbol{x}_k))_{k=1}^{b_{\\mathrm{old}}}\\}$  for the part of the search space that remained unchanged (i.e.,  $\\mathcal{X}_{\\mathrm{both}}$ , for reference see Figure 2). To this end, TGP/T2PE first discards now invalid hyperparameter settings and projects the remaining settings  $\\boldsymbol{x}_i$  in  $\\mathcal{D}_{\\mathrm{old}}$  from  $\\mathcal{X}_{\\mathrm{old}}$  to  $\\mathcal{X}_{\\mathrm{both}}$  to yield  $\\mathcal{D}_{\\mathrm{old,both}}$ . Then, either a GP or TPE model  $M_{\\mathrm{both}}$  is fit on these transformed previous results  $\\mathcal{D}_{\\mathrm{old,both}}$ , and to generate a sample  $\\boldsymbol{x} \\in \\mathcal{X}_{\\mathrm{new}}$  for the new HPO, a sample from  $M_{\\mathrm{both}}$  over  $\\mathcal{X}_{\\mathrm{both}}$  is combined with a random sample over  $\\mathcal{X}_{\\mathrm{only,new}}$ . Further, as in the basic HPO algorithms, TGP/T2PE uses a random sample over  $\\mathcal{X}_{\\mathrm{new}}$  in  $\\frac{1}{3}$  of cases, and after  $\\dim(\\mathcal{X}_{\\mathrm{new}})$  (TGP) or  $2(\\dim(\\mathcal{X}_{\\mathrm{new}}) + 1)$  (T2PE) evaluations, TGP/T2PE uses a model fitted on the new observations. We provide pseudocode for TGP/T2PE in Algorithm 1.\n\n\n```\nInput: Current search space  $\\mathcal{X}_{\\mathrm{new}}$  , previous search space  $\\mathcal{X}_{\\mathrm{old}}$  , previous results  $\\mathcal{D}_{\\mathrm{old}}$  , budget  $b_{\\mathrm{new}}$    \n1: Decompose  $\\mathcal{X}_{\\mathrm{new}} = (\\mathcal{X}_{\\mathrm{both}}\\cup \\mathcal{X}_{\\mathrm{both,range - only - new}})\\times \\mathcal{X}_{\\mathrm{only - new}}$  See Section 3.2   \n2:Discard hyperparameter settings in  $\\mathcal{D}_{\\mathrm{old}}$  that have values in  $\\mathcal{X}_{\\mathrm{both,range - only - new}}$    \n3: Project configis in  $\\mathcal{D}_{\\mathrm{old}}$  to space  $\\mathcal{X}_{\\mathrm{both}}$  , to yield  $\\mathcal{D}_{\\mathrm{old,both}}$    \n4: Fit TPE or GP model  $M_{\\mathrm{both}}$  for  $\\mathcal{X}_{\\mathrm{both}}$  on  $\\mathcal{D}_{\\mathrm{old,both}}$    \n5: for  $t$  in 1,...,  $b_{\\mathrm{new}}$  do   \n6: if is random fraction then See Section 3.1   \n7: Sample  $\\mathbf{x}_{\\mathrm{new}}$  from prior on  $\\mathcal{X}_{\\mathrm{new}}$    \n8: else if no model for  $\\mathcal{X}_{\\mathrm{new}}$  then Differs for TPE and GP, see Section 3.1   \n9: Sample  $\\mathbf{x}_{\\mathrm{both}}$  from  $\\mathcal{X}_{\\mathrm{both}}$  using  $M_{\\mathrm{both}}$    \n10: Sample  $\\mathbf{x}_{\\mathrm{only - new}}$  from prior on  $\\mathcal{X}_{\\mathrm{only - new}}$    \n11: Combine  $\\mathbf{x}_{\\mathrm{both}}$  with  $\\mathbf{x}_{\\mathrm{only - new}}$  to yield sample  $\\mathbf{x}_{\\mathrm{new}}$    \n12: else   \n13: Fit TPE or GP model  $M_{\\mathrm{new}}$  for  $\\mathcal{X}_{\\mathrm{new}}$  on current observations   \n14: Sample  $\\mathbf{x}_{\\mathrm{new}}$  from  $\\mathcal{X}_{\\mathrm{new}}$  using  $M_{\\mathrm{new}}$\n```\n\n\n# 4 BENCHMARKS FOR HT-AA\n\n\nWe introduce eight novel benchmarks for the basic hyperparameter transfer across adjustments (HT-AA) problem discussed in Section 2. As is common in hyperparameter optimization research, we employ tabular and surrogate benchmarks to allow cheap and reproducible benchmarking (Perrone et al., 2018; Falkner et al., 2018). Tabular benchmarks achieve this with a lookup table for all possible hyperparameter settings. In contrast, surrogate benchmarks fit a surrogate model for the objective function (Eggensperger et al., 2014). We base our surrogate models and lookup tables on code and data from existing hyperparameter optimization (HPO) benchmarks (detailed in Appendix A) that, together, cover four different machine learning algorithms: a fully connected neural network (FCN), neural architecture search for a convolutional neural network (NAS), a support vector machine (SVM), and XGBoost (XGB). While the NAS and FCN benchmarks are based on lookup tables, the SVM and XGB based benchmarks use surrogate models. For each of these base benchmarks, we simulate two different types of developer adjustments (Table 1), to arrive at a total of eight benchmarks. Additionally, for each algorithm and adjustment, we consider multiple tasks in our benchmarks. Further, we provide a python package with all our benchmarks and refer the reader to Appendix A for additional details on the benchmarks.\n\n\nTable 1: Developer adjustments and types of adjustments in the benchmarks.\n\n\n<table><tr><td>Benchmark</td><td>Adjustments</td><td>Adjustment Type</td></tr><tr><td>FCN-A</td><td>Increase #units-per-layer 16\u00d7Double #epochsFix batch size hyperparameter</td><td>HomogeneousHomogeneousHeterogeneous</td></tr><tr><td>FCN-B</td><td>Add per-layer choice of activation functionChange learning rate schedule</td><td>HeterogeneousHomogeneous</td></tr><tr><td>NAS-A</td><td>Add 3x3 average pooling as choice of operation to each edge</td><td>Heterogeneous</td></tr><tr><td>NAS-B</td><td>Add node to cell template (adds 3 hyperparameters)</td><td>Heterogeneous</td></tr><tr><td>XGB-A</td><td>Expose four booster hyperparameters</td><td>Heterogeneous</td></tr><tr><td>XGB-B</td><td>Change four unexposed booster hyperparameter values</td><td>Homogeneous</td></tr><tr><td>SVM-A</td><td>Change kernelRemove hyperparameter for old kernelAdd hyperparameter for new kernel</td><td>HomogeneousHeterogeneousHeterogeneous</td></tr><tr><td>SVM-B</td><td>Increase range for cost hyperparameter</td><td>Heterogeneous</td></tr></table>\n\n\n# 5 EXPERIMENTS AND RESULTS\n\n\nIn this section, we empirically evaluate the four baseline algorithms presented in Section 3 as solutions for the hyperparameter transfer across adjustments problem. We first describe the evaluation protocol used through all studies and then present the results.\n\n\nEvaluation protocol We use the benchmarks introduced in Section 4 and focus on the speedup of transfer strategies over either Bayesian optimization with Gaussian processes (GP), or over Tree Parzen Estimator (TPE). Specifically, we measured how much faster a transfer algorithm reaches a given objective value compared to GP or TPE in terms of the number of evaluations. We repeated all measurements across 25 different random seeds and report results for validation objectives, as not all benchmarks provide test objectives, and to reduce noise in our evaluation. We terminate runs after 400 evaluations and report ratio of means. To aggregate these ratios across tasks and benchmarks, we use the geometric mean. We use the geometric mean, as, intuitively, two speedups of e.g.,  $0.1\\mathrm{x}$  and  $10\\mathrm{x}$  average to  $1\\mathrm{x}$ , and not  $5.05\\mathrm{x}$ . We want to note that the standard mean is an upper bound for the geometric mean, so using the geometric mean in fact makes the speedups slightly less impressive than had we used the standard arithmetic mean. To determine the target objective values, we measured GP's average performance for 10, 20, and 40 evaluations. We chose this range of evaluations as a\n\n\nTable 2: Average speedup across benchmarks for different #evaluations for the old and new HPO. For the GP based (left) and TPE based (right) evaluation.\n\n\n<table><tr><td>#Evals Old</td><td>#Evals New</td><td>Best First</td><td>Transfer GP/TPE</td><td>Best First + Transfer GP/TPE</td></tr><tr><td rowspan=\"3\">10</td><td>10</td><td>1.6 / 1.5</td><td>1.3 / 1.0</td><td>1.7 / 1.7</td></tr><tr><td>20</td><td>1.4 / 1.4</td><td>1.2 / 1.0</td><td>1.4 / 1.4</td></tr><tr><td>40</td><td>1.1 / 1.2</td><td>1.1 / 1.1</td><td>1.2 / 1.2</td></tr><tr><td rowspan=\"3\">20</td><td>10</td><td>2.3 / 1.8</td><td>1.7 / 1.3</td><td>2.6 / 2.1</td></tr><tr><td>20</td><td>1.7 / 1.4</td><td>1.5 / 1.2</td><td>2.0 / 1.8</td></tr><tr><td>40</td><td>1.3 / 1.1</td><td>1.2 / 1.1</td><td>1.4 / 1.5</td></tr><tr><td rowspan=\"3\">40</td><td>10</td><td>3.3 / 2.6</td><td>2.1 / 1.5</td><td>3.6 / 2.6</td></tr><tr><td>20</td><td>2.6 / 2.0</td><td>2.0 / 1.4</td><td>2.9 / 2.0</td></tr><tr><td>40</td><td>1.8 / 1.4</td><td>1.5 / 1.2</td><td>2.1 / 1.5</td></tr></table>\n\n\nsurvey among NeurIPS2019 and ICLR2020 authors indicates that most hyperparameter optimizations (HPOs) do not consider more than 50 evaluations (Bouthillier & Varoquaux, 2020). Further, for transfer approaches, we perform this experiment for different evaluation budgets for the HPO before the adjustments (also for 10, 20, and 40 evaluations).\n\n\nResults The transfer GP (TGP) / transfer TPE (T2PE) and best-first strategy lead to large speedups, while drop-unimportant and only-optimize-new perform poorly. Here, in this paragraph we focus on the GP based evaluation. On average and depending on the budgets for the old and new HPO, TGP reaches the given objective values 1.1\u20132.1x faster than GP, and best-first 1.1\u20133.3x faster. The combination of TGP/T2PE and best-first leads to further speedups over best-first if the budget for the old HPO was 20 or 40, and when the old budget and new budget both equal 10. These additional average speedups when combining TGP with best-first are between 0.1 and 0.3. We visualize these results with violin plots (Figure 3), as they take into account the multi modality of the results, and also provide a table with exact averages (Table 2). There are two main trends visible: (1) The more optimal the target objective, the smaller the speedup, and (2) the higher the budget for the previous HPO, the higher the speedup. For a more fine-grained visualization that shows violin plots over task averages for each benchmark, we refer to Appendix B. The approaches inspired by actually-practiced manual strategies, drop-unimportant and only-optimize-new, do not reach the performance of GP in a large percentage of  $(20\\% -70\\%)$  on average), even while given at least  $10\\mathrm{x}$  the budget compared to GP. These high failure rates makes an evaluation for the speedup unfeasible and indicates that actually-practiced manual strategies perform worse than starting from scratch. For a visualization of the failure rates of drop-unimportant and only-optimize-new, and for GP, TGP, and best-first  $(0 - 0.8\\%)$  we refer the reader to Appendix C.\n\n\nAdditionally, we provide a study on the improvement in objective in Appendix D; in Appendix E we show the results of a control study that compares GP and TPE with different ranges of random seeds; and in Appendix F we compare GP and TPE to random search to validate their reliability.\n\n\n# 6 RELATED WORK AND RESEARCH OPPORTUNITIES\n\n\nIn this section, we discuss work related to hyperparameter transfer across adjustments (HT-AA) and present several research opportunities in combining existing ideas with HT-AA.\n\n\nTransfer learning Transfer learning studies how to use observations from one or multiple source tasks to improve learning on one or multiple target tasks (Zhuang et al., 2019). If we view the HPO problems before and after specific developer adjustments as tasks, we can consider HT-AA as a specific transfer learning problem. As developer adjustments may change the search space, HT-AA would then be categorized as a heterogeneous transfer learning problem (Day & Khoshgoftaar, 2017).\n\n\nTransfer learning across adjustments Recently, Berner et al. (2019) transferred knowledge between deep reinforcement learning agents across developer adjustments. They crafted techniques to preserve, or approximately preserve, the neural network policy for each type of adjustment they\n\n\n![Figure 3: Speedup to reach a given reference objective value compared to GP/TPE for best-first, best-first combined with transfer GP/TPE, and transfer GP/TPE (from left to right in each violin triplet) across 8 benchmarks. The violins estimate densities of benchmark geometric means. The horizontal line in each violin shows the geometric mean across these benchmark means. #Evaluations for the old HPO increases from left to right. The x-axis shows the budget for the GP and TPE reference. Note that the GP and TPE runs we used to determine the reference objective are different to the ones that we show the speedups over.](images/04187543515fa93fe7c7bc1c099d5f05570bd8780cf81b9baf1536bfb041e5cf.jpg)\n\n\nencountered. Their transfer strategies are inspired by Net2Net knowledge transfer (Chen et al., 2015), and they use the term surgery to refer to this practice. Their work indicates that transfer learning across adjustments is not limited to knowledge about hyperparameters, but extends to a more general setting, leaving room for many research opportunities.\n\n\nContinuous knowledge transfer In this paper, we focus on transferring knowledge from the last HPO performed, but future work could investigate a continuous transfer of knowledge across many cycles of adjustments and HPOs. Transferring knowledge from HPO runs on multiple previous versions could lead to further performance gains, as information from each version could be useful for the current HPO. Such continuous HT-AA would then be related to the field of continual learning (Thrun & Mitchell, 1995; Lange et al., 2020).\n\n\nHyperparameter transfer across tasks (HT-AT) While we have related HT-AA to HT-AT in Section 2, here, we want to complement this discussion with the research opportunities we see in HT-AA based on HT-AT. First, an adaptation of across-task strategies, e.g., the ones cites in Section 2, to the across-adjustments setting could lead to more powerful HT-AA approaches in the future. Second, the combination of across-task and across-adjustments hyperparameter transfer could provide even larger speedups than either transfer strategy on its own.\n\n\n![Figure 4: Percent of runs that do not reach the reference objective for the GP based drop-unimport and only-optimize-new. Each data point for the violins represents the mean percentage of failures for a benchmark. The line in each violin shows the mean across these benchmark means. #Evaluations for the old HPO increases from left to right. The x-axis shows the budget for the GP reference.](images/ea79e32f6bf8d352f99d693e492ce202c4f0c9321d7b1367b336efbf2f6deabf.jpg)\n\n\nAdvanced hyperparameter optimization HT-AA can be combined with the many extensions to hyperparameter optimization (HPO). One such extension is multi-fidelity HPO, which allows the use of cheap-to-evaluate approximations to the actual objective (Li et al., 2017; Falkner et al., 2018). Similarly, cost-aware HPO adds a cost to each hyperparameter setting, so the evaluation of cheap settings can be prioritized over expensive ones (Snoek et al., 2012). Yet another extension is to take evaluation noise into account (Kersting et al., 2007), or to consider not one, but multiple objectives to optimize for (Khan et al., 2002). All these HPO formulations can be studied in conjunction with HT-AA, to either provide further speedups or deal with more general optimization problems.\n\n\nGuided machine learning The field of guided machine learning (gML) studies the design of interfaces that enables humans to guide ML processes (Westphal et al., 2019). An HT-AA algorithm could be viewed as a ML algorithm that receives incremental guidance in the form of arbitrary developer adjustments; the interface would then be the programming language(s) the ML algorithm is implemented in. On a different note, gML could provide HT-AA algorithms with additional information about the adjustments to the ML algorithm. For example, when adding a hyperparameter, there are two distinctions we can make: Either an existing hyperparameter is exposed, or a new component is added to the algorithm that introduces a new hyperparameter From the HPO problem itself, we cannot know which case it is, and neither which fixed value an exposed hyperparameter had. Guided HT-AA algorithms could ask for user input to fill this knowledge gap, or HT-AA algorithms with code analysis could extract this knowledge from the source code.\n\n\nProgramming by optimization The programming by optimization (PbO) framework (Hoos, 2012) proposes the automatic construction of a search space of algorithms based on code annotations, and the subsequent search in this search space. While PbO considers changing search spaces over developer actions, each task and development step restarts the search from scratch. This is in contrast to HT-AA, which alleviates the need to restart from scratch after each developer adjustment.\n\n\n# 7 CONCLUSION\n\n\nIn this work, we introduced hyperparameter transfer across developer adjustments to improve efficiency during the development of ML algorithms. In light of rising energy demands of ML algorithms and rising global temperatures, more efficient ML development practices are an important issue now and will become more important in the future. As already some of the simple baseline algorithm considered in this work lead to large empirical speedups, our new framework represents a promising step towards efficient ML development.\n\n\n# REFERENCES\n\n- Forest Agostinelli, Matthew Hoffman, Peter Sadowski, and Pierre Baldi. Learning activation functions to improve deep neural networks. arXiv preprint arXiv:1412.6830, 2014.\n\n- James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In International conference on machine learning, pp. 115-123, 2013.\n\n- James S Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for hyper-parameter optimization. In Advances in neural information processing systems, pp. 2546-2554, 2011.\n\n- Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n\n- Xavier Bouthillier and Ga\u00ebl Varoquaux. Survey of machine-learning experimental methods at NeurIPS2019 and ICLR2020. Research report, Inria Saclay Ile de France, January 2020. URL https://hal.archives-ouvertes.fr/hal-02447823.\n\n- Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641, 2015.\n\n- Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in alphago. arXiv preprint arXiv:1812.06855, 2018.\n\n- Oscar Day and Taghi M Khoshgoftaar. A survey on heterogeneous transfer learning. Journal of Big Data, 4(1):29, 2017.\n\n- Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In International Conference on Learning Representations, 2019.\n\n- K. Eggensperger, F. Hutter, H. Hoos, and K. Leyton-Brown. Surrogate benchmarks for hyperparameter optimization. In ECAI workshop on Metalearning and Algorithm Selection (MetaSel'14), 2014.\n\n- Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: Robust and efficient hyperparameter optimization at scale. In Proceedings of the 35th International Conference on Machine Learning (ICML 2018), pp. 1436-1445, July 2018.\n\n- Matthias Feurer and Frank Hutter. Hyperparameter optimization. In Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (eds.), Automatic Machine Learning: Methods, Systems, Challenges, pp. 3-38. Springer, 2019. Available at http://automl.org/book.\n\n- Matthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable meta-learning for bayesian optimization. arXiv preprint arXiv:1802.02219, 2018.\n\n- Holger H Hoos. Programming by optimization. Communications of the ACM, 55(2):70-80, 2012.\n\n- Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708, 2017.\n\n- F. Hutter, H. Hoos, and K. Leyton-Brown. An efficient approach for assessing hyperparameter importance. In E. Xing and T. Jebara (eds.), Proceedings of the 31th International Conference on Machine Learning, (ICML'14), pp. 754-762. Omnipress, 2014.\n\n- Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455-492, 1998.\n\n- Kristian Kersting, Christian Plagemann, Patrick Pfaff, and Wolfram Burgard. Most likely heteroscedastic gaussian process regression. In Proceedings of the 24th international conference on Machine learning, pp. 393-400, 2007.\n\n- Nazan Khan, David E Goldberg, and Martin Pelikan. Multi-objective bayesian optimization algorithm. In Proceedings of the 4th Annual Conference on Genetic and Evolutionary Computation, pp. 684-684. CiteSeer, 2002.\n\n- Aaron Klein and Frank Hutter. Tabular benchmarks for joint architecture and hyperparameter optimization. arXiv preprint arXiv:1905.04970, 2019.\n\n- Daniel K\u00fchn, Philipp Probst, Janek Thomas, and Bernd Bischl. Automatic exploration of machine learning experiments on openml. arXiv preprint arXiv:1806.10961, 2018.\n\n- Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks, 2020.\n\n- Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Rethinking the hyperparameters for fine-tuning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=B1g8VkHFPH.\n\n- L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: Bandit-based configuration evaluation for hyperparameter optimization. In OpenReview.net (ed.), Proceedings of the International Conference on Learning Representations (ICLR'17), 2017.\n\n- M. Lindauer, K. Eggensperger, M. Feurer, S. Falkner, A. Biedenkapp, and Hutter F. Smac v3: Algorithm configuration in python. https://github.com/automl/SMAC3, 2017.\n\n- Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha Sohl-Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies, 2020.\n\n- Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. A metric learning reality check. arXiv preprint arXiv:2003.08505, 2020.\n\n- Valerio Perrone, Rodolphe Jenatton, Matthias W Seeger, and Cedric Archambeau. Scalable hyperparameter transfer learning. In Advances in Neural Information Processing Systems, pp. 6845-6855, 2018.\n\n- C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.\n\n- Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. arXiv preprint arXiv:1907.10597, 2019.\n\n- B. Shahriari, K. Swersky, Z. Wang, R. Adams, and N. de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1):148-175, 2016.\n\n- Alistair Shilton, Sunil Gupta, Santu Rana, and Svetha Venkatesh. Regret Bounds for Transfer Learning in Bayesian Optimisation. In Aarti Singh and Jerry Zhu (eds.),., volume 54 of Proceedings of Machine Learning Research, pp. 307-315, Fort Lauderdale, FL, USA, 20-22 Apr 2017. PMLR. URL http://proceedings.mlr.press/v54/shilton17a.html.\n\n- J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger (eds.), Proceedings of the 26th International Conference on Advances in Neural Information Processing Systems (NIPS'12), pp. 2960-2968, 2012.\n\n- Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3645-3650, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1355. URL https://www.aclweb.org/anthology/P19-1355.\n\n- Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization. In Advances in neural information processing systems, pp. 2004-2012, 2013.\n\n- Sebastian Thrun and Tom M. Mitchell. Lifelong robot learning. Robotics and Autonomous Systems, 15(1):25 - 46, 1995. ISSN 0921-8890. doi: https://doi.org/10.1016/0921-8890(95)00004-Y. URL http://www.sciencedirect.com/science/article/pii/092188909500004Y. The Biology and Technology of Intelligent Autonomous Agents.\n\n- Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018.\n\n- Zi Wang, Beomjoon Kim, and Leslie Pack Kaelbling. Regret bounds for meta bayesian optimization with an unknown gaussian process prior. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 10477-10488. Curran Associates, Inc., 2018.\n\n- Florian Westphal, Niklas Lavesson, and H\u00e5kan Grahn. A case for guided machine learning. In Andreas Holzinger, Peter Kieseberg, A Min Tjoa, and Edgar Weippl (eds.), Machine Learning and Knowledge Extraction, pp. 353-361, Cham, 2019. Springer International Publishing. ISBN 978-3-030-29726-8.\n\n- Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Scalable gaussian process-based transfer surrogates for hyperparameter optimization. Machine Learning, 107(1):43-78, 2018.\n\n- Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3-19, 2018.\n\n- Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. arXiv preprint arXiv:1911.02685, 2019.\n\nResponse: ### Prediction: \\(\\boxed{Accept}\\)\n\n#### Justification:\n\n**Originality:** \nThe paper introduces a novel concept, **hyperparameter transfer across adjustments (HT-AA)**, which addresses a significant gap in the current state of hyperparameter optimization (HPO) practices. By proposing HT-AA, the authors aim to enable developers to leverage past HPO results more effectively after making adjustments to ML algorithms, thereby speeding up the development cycle. This approach is innovative and fills a critical gap in the literature.\n\n**Methodology Rigor:**\nThe authors provide a detailed theoretical framework for HT-AA, including a formal definition of the problem, a taxonomy of developer adjustments, and a clear presentation of the HT-AA problem within the context of HPO. They also introduce a series of baseline algorithms and conduct rigorous experiments to evaluate these algorithms. The experimental setup is well-defined, with a comprehensive set of benchmarks covering various aspects of ML algorithms and hyperparameter spaces. The results are presented clearly, with detailed tables and visualizations, providing strong evidence for the effectiveness of the proposed approaches.\n\n**Significance:**\nThe proposed HT-AA framework is highly significant because it directly addresses a real-world bottleneck in the development of machine learning models\u2014reducing the time and computational resources required for hyperparameter tuning. This is particularly relevant given the growing computational demands and environmental concerns associated with ML development. The empirical results showing substantial speedups (up to 1.2-3.6x faster) on average depending on the budgets are compelling and demonstrate practical utility.\n\n**Clarity:**\nThe paper is well-written and organized, making the concepts and methodologies easy to follow. The authors provide clear definitions, diagrams, and pseudocode for their algorithms, which helps in understanding the technical details. The presentation of the HT-AA problem, along with the baseline algorithms and experimental setup, is thorough and systematic.\n\n**Alignment with Field Standards:**\nThe paper adheres to standard practices in the field of machine learning and hyperparameter optimization. It references relevant literature, provides appropriate citations, and discusses the connections to existing research frameworks like hyperparameter transfer across tasks (HT-AT). The authors also address limitations and suggest future directions, which adds depth to the manuscript.\n\n### Strengths:\n1. **Novelty and Impact**: The proposed HT-AA framework tackles a critical issue in ML development, offering a practical solution with significant potential benefits.\n2. **Comprehensive Evaluation**: Extensive experimentation and detailed results provide strong empirical support for the effectiveness of the proposed approaches.\n3. **Clear Presentation**: Well-structured and logically coherent presentation facilitates understanding of the complex concepts and methodologies.\n\n### Weaknesses:\n1. **Complexity**: While the theoretical framework is clear, the pseudocode for the algorithms might benefit from further explanation or simplification.\n2. **Detailed Implementation Details**: Some pseudocode implementations could be more detailed, especially for the transfer GP and T2PE algorithms, which might help readers implement the methods.\n3. **Potential for Further Research**: While the authors mention future research directions, these could be more explicitly outlined to guide future work.\n\nOverall, the paper presents a significant contribution to the field with robust theoretical foundations, comprehensive empirical evaluation, and practical implications. Given the novelty and impact of the proposed framework, I predict it will be well-received and accepted by the ICLR community.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634821.9696453", "msecs": "969.0", "relativeCreated": "166793.119430542", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers for acceptance. Assess based on originality, methodology rigor, significance, clarity, and alignment with field standards. Provide a structured prediction with confidence level (High/Medium/Low), detailing evidence-based rationale. Systematically weigh strengths and weaknesses, ensuring objectivity and adherence to peer review principles.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Learning Value Functions from Undirected State-only Experience\n\n# Abstract\nThis paper tackles the problem of learning value functions from undirected state-only experience (state transitions without action labels i.e. (s,s',r) tuples). We first theoretically characterize the applicability of Q-learning in this setting. We show that tabular Q-learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-learning or LAQ, an offline RL method that can learn effective value functions from state-only experience. Latent Action Q-learning (LAQ) learns value functions using Q-learning on discrete latent actions obtained through a latent-variable future prediction model. We show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions. Value functions learned using LAQ lead to sample efficient acquisition of goal-directed behavior, can be used with domain-specific low-level controllers, and facilitate transfer across embodiments. Our experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives, imitation learning oracles, and competing methods.\n\n\n# 1 INTRODUCTION\n\n\nOffline or batch reinforcement learning focuses on learning goal-directed behavior from prerecorded data of undirected experience in the form of  $(s_t, a_t, s_{t+1}, r_t)$  quadruples. However, in many realistic applications, action information is not naturally available (e.g. when learning from video demonstrations), or worse still, isn't even well-defined (e.g. when learning from the experience of an agent with a different embodiment). Motivated by such use cases, this paper studies if and how, intelligent behavior can be derived from undirected streams of observations:  $(s_t, s_{t+1}, r_t)$ .<sup>1</sup>\n\n\nOur key conceptual insight is that while an observation-only dataset doesn't tell us the precise action to execute, i.e. the policy  $\\pi(a|s)$ ; it may still tell us which states are more likely to lead us to the goal than not, i.e. the value function  $V(s)$ . For example, simply by looking at someone working in the kitchen, we can infer that approaching the microwave handle is more useful (i.e. has higher value) for opening the microwave than to approach the buttons. Thus, we can still make use of observation-only data, if we focused on learning value functions as opposed to directly learning goal-directed policies. Once we have learned a good value function, it can be used to quickly acquire or infer behavior. Using learned value functions as dense rewards can lead to quick policy learning through some small amount of interaction in the environment. Alternatively, they could be used to directly guide the behavior of low-level controllers that may already be available for the agent (as is often the case in robotics) without any further training. Furthermore, decoupling the learning of value functions from policy learning enables deriving behavior for agents with a different embodiment as long as the overall strategy to solve the task remains similar.\n\n\nThus, the central technical question is how to learn a good value function from undirected observation streams. Is it even possible? If so, under what conditions? This paper tackles these questions from a theoretical and practical perspective.\n\n\nWe start out by characterizing the behavior of tabular Q-learning from Watkins (1989) under missing action labels. We note that Q-learning with naively imputed action labels is equivalent to the TD(0)\n\n\npolicy evaluation, which serves as a simple baseline method for deriving a value function. However, depending on the policy that generated the data, the learned values (without any action grounding) can differ from the optimal values. Furthermore, it is possible to construct simple environments where the behavior implied by the learned value function is also sub-optimal.\n\n\nNext, we present a more optimistic result. There are settings in which Q-learning can recover the optimal value function even in the absence of the knowledge of underlying actions. Concretely, we prove that if we are able to obtain an action space which is a strict refinement of the original action space, then Q-learning in this refined action space recovers the optimal value function.\n\n\nThis motivates a practical algorithm for learning value functions from the given undirected observation-only experience. We design a latent-variable future prediction model that seeks to obtain a refined action space. It operates by predicting  $s_{t + 1}$  from  $s_t$  and a discrete latent variable  $\\hat{a}$  from a set of actions  $\\hat{\\mathbf{A}}$  (Section 4.1). Training this latent variable model assigns a discrete action  $\\hat{a}_t$  to each  $(s_t, s_{t + 1})$  tuple. This allows us to employ Q-learning to learn good value functions (Section 4.2). The learned value function is used to derive behavior (Section 4.3) either through some online interaction with the environment, or through the use of domain specific low-level controllers.\n\n\nThe use of a latent action space for Q-learning allows us to exceed the performance of methods based on policy evaluation (Edwards & Isbell, 2019), which will learn the value of the demonstration policy, not the optimal value function. Additionally, it side-steps the problem of reconstructing high-dimensional images faced by other state-only value learning methods (Edwards et al., 2020). Other approaches for learning from state-only data rely on imitating the demonstration data, which renders them unable to improve on sub-optimal demonstration data. See Section 7 for more discussion.\n\n\nOur experiments in five environments (2D grid world, 2D continuous control, Atari game Freeway, robotic manipulation, and visual navigation in realistic 3D environments) test our proposed ideas. Our method approximates a refinement of the latent space better than clustering alternatives, and in turn, learns value functions highly correlated with ground truth. Good value functions in-turn lead to sample efficient acquisition of behavior, leading to significant improvement over learning with only environment rewards. Our method compares well against existing methods that learn from undirected observation-only data, while being also applicable to the case of high-dimensional observation spaces in the form of RGB images. We are also able to outperform imitation learning methods, even when these imitation learning methods have access to privileged ground-truth action information. Furthermore, our method is able to use observation-only experience from one agent to speed up learning for another agent with a different embodiment. Code, models, simulation environments will be released.\n\n\n# 2 PRELIMINARIES\n\n\nFollowing the notation from Sutton & Barto (2018), our Markov decision process (MDP) is specified by  $(\\mathbf{S},\\mathbf{A},p,\\gamma)$ , where  $\\mathbf{S}$  is a state space,  $\\mathbf{A}$  is an action space,  $\\gamma$  is the discount factor, and  $p(s',r|s,a)$  is the state/reward joint dynamics function. It specifies the probability distribution that the agent ends up in state  $s'$ , receives a reward of  $r$  on executing action  $a$  from state  $s$ .\n\n\nOffline or batch RL (Lange et al., 2012; Levine et al., 2020) studies the problem of deriving high reward behavior when only given a dataset of experience in an MDP, in the form of a collection of quadruples  $(s, a, s', r)$ . In this paper, we tackle a harder version of this problem where instead we are only given a collection of triplets  $(s, s', r)$ , i.e. experience without information about intervening actions. In general, this dataset could contain any quality of behavior, from optimal, to actively adversarial. In contrast to some methods (see Section 7), we will not assume that demonstrations in the dataset are of high quality, and design our method to be robust to sub-optimal data.\n\n\nIn this paper, we will focus on methods based on Q-learning (Watkins, 1989) for tackling this problem. Q-learning has the advantage of being off-policy, i.e., experience from another policy (or task) can be used to learn or improve a different policy for a different task. Q-learning seeks to learn the optimal Q-function  $Q^{*}(s,a)$  by iteratively updating  $Q(s_{t},a_{t})$  to the Bellman equation. This process converges to the  $Q^{*}$  under mild conditions in many settings (Watkins, 1989), and gives the optimal state-value function as  $V^{*}(s) = \\max_{a}Q^{*}(s,a)$ .\n\n\n![](images/8db76b66f99759d570c58f88c1117d158d7fedb39bf326ebc88b276b3daed6c0.jpg)\n\n\n![](images/362ee09ba8ebcf87cbc01b75c745178308d722ccea0d284d1442b6196b9e7a46.jpg)\n\n\n![](images/f0e1540cbe9aa0924c6a5dd596f04db9533b9ab63bd5ab6b3609fc64b43755c4.jpg)\n\n\n![Figure 1: We visualize the learned value function when using different action labels for Q-learning. See Section 3.2 for more details.](images/1b252a603b1a19934110ec5d754f301aa904dceba1e912a4175614715064a368.jpg)\n\n\n# 3 CHARACTERIZING Q-LEARNING WITHOUT TRUE ACTION LABELS\n\n\nWe characterize the outcome of Q-learning in settings where we don't have ground truth intervening actions in the offline dataset being used for Q-learning. We first consider the case of ignoring the action altogether, which amounts to TD(0) policy evaluation. Next, we study if labeling  $(s,s^{\\prime},r)$  samples with actions from a different action space  $\\hat{\\mathbf{A}}$  to construct a new MDP could aid learning. More specifically, can the optimal Q-function for this new MDP, as obtained through Q-learning on samples  $(s,s^{\\prime},r)$  labeled with actions from  $\\hat{\\mathbf{A}}$ , be useful in the original MDP? We show that under the right conditions the value function learned under the altered action space  $\\hat{\\mathbf{A}}$  is identical to the value function learned for the original MDP.\n\n\nQ-learning naive action labels (single action): Without action labels, one could simply assign all transitions the same label. In this case, Q-Learning becomes TD(0) policy evaluation. The induced value function isn't the optimal value function for the MDP, but rather the value according to the policy that generated the dataset. Depending on the dataset, this could be sub-optimal.\n\n\n# 3.1 OPTIMALITY OF ACTION REFINEMENT\n\n\nAssume we have a Markov Decision Process (MDP)  $M$  specified by  $(\\mathbf{S},\\mathbf{A},p,\\gamma)$ . Let the action space  $\\mathbf{A}$  be composed of actions  $a_1,a_2,a_3,\\ldots ,a_n\\in \\mathbf{A}$ . We are interested in the value learned under a modified MDP,  $\\hat{M}$  composed of  $(\\mathbf{S},\\hat{\\mathbf{A}},\\hat{p},\\gamma)$ . We will show that if the actions and transitions  $\\hat{\\mathbf{A}}$  and  $\\hat{p}$  are a refinement of  $\\mathbf{A}$  and  $p$ , then the value function learned on  $\\hat{M}$ ,  $V_{\\hat{M}}$  is identical to the value function learned on  $M$ ,  $V_{M}$ . We define actions and transitions in  $\\hat{M}$  to be a refinement of those in  $M$  when, a) in each state, for every action in  $\\hat{\\mathbf{A}}$ , there is at least one action in  $\\mathbf{A}$  which is functionally identical in the same state, and b) in each state, for each action in  $\\mathbf{A}$  is represented by at least one action in  $\\hat{\\mathbf{A}}$  in that state.\n\n\nDefinition 3.1 Given a discrete finite MDP,  $M$  specified by  $(\\mathbf{S},\\mathbf{A},p)$  and MDP,  $\\hat{M}$  specified by  $(\\mathbf{S},\\hat{\\mathbf{A}},\\hat{p})$ ,  $\\hat{M}$  is a refinement of  $M$  when\n\n\n$$\n\n$$\n\\underset {\\hat {a} \\in \\hat {\\mathbf {A}}, s \\in \\mathbf {S}} {\\forall} \\underset {a \\in \\mathbf {A}} {\\exists} \\underset {s ^ {\\prime}, r} {\\forall} \\hat {p} (s ^ {\\prime}, r | s, \\hat {a}) = p (s ^ {\\prime}, r | s, a), \\text {a n d} \\underset {a \\in \\mathbf {A}, s \\in \\mathbf {S}} {\\forall} \\underset {\\hat {a} \\in \\hat {\\mathbf {A}}} {\\exists} \\underset {s ^ {\\prime}, r} {\\forall} \\hat {p} (s ^ {\\prime}, r | s, \\hat {a}) = p (s ^ {\\prime}, r | s, a),\n$$\n\n$$\n\n\nNote that this definition of refinement requires a state conditioned correspondence between action behavior. Actions do not need to have to correspond across states.\n\n\nTheorem 3.1. Given a discrete finite MDP,  $\\hat{M}$  which is a refinement of  $M$  (Definition 3.1) then both MDPs induce the same optimal value function, i.e.  $\\forall_{s}V_{\\hat{M}}^{*}(s) = V_{M}^{*}(s)$ .\n\n\nWe prove this by showing that optimal policies under both MDPs induce the same value function.\n\n\nLemma 3.2. For any policy  $\\pi_M$  on  $M$ , there exists a policy  $\\pi_{\\hat{M}}$  on  $\\hat{M}$  such that  $V_{\\hat{M}}^{\\pi_{\\hat{M}}}(s) = V_{M}^{\\pi_{M}}(s)$ ,  $\\forall s$ , and for any policy  $\\pi_{\\hat{M}}$  on  $\\hat{M}$  there exists a policy  $\\pi_M$  on  $M$  such that  $V_{\\hat{M}}^{\\pi_{\\hat{M}}}(s) = V_{M}^{\\pi_{M}}(s)$ $\\forall s$ .\n\n\nFor this lemma we introduce the notion of fundamental actions, which are actions which correspond to sets of actions which have the same state and reward transition distributions in a given state. We\n\n\n![Figure 2: Approach Overview. Our proposed approach Latent Action Q-Learning (LAQ) starts with a dataset of  $(s, s', r)$  triples. Using the latent action learning process, each sample is assigned a latent action  $\\hat{a}$ . Q-learning on the dataset of quadruples produces a value function,  $V(s)$ . Behaviors are derived from the value function through densified RL, or by guiding low-level controllers.](images/d303a71ed1a85a6f2e44677b980f3cd6eae365305065f1caa64337d6c40dce2c.jpg)\n\n\nutilize the equivalence of fundamental actions between MDPs to construct a policy in the new MDP which induces the same value function as a given policy in the original MDP. We provide proofs for Theorem 3.1 and Lemma 3.2 in Section A.1.\n\n\n# 3.2 GRIDWORLD CASE STUDY\n\n\nWe validate these results in a tabular grid world setting. In particular, we measure the error in learned value functions and the induced behavior, when conducting Q-learning with datasets with different qualities of intervening actions. The agent needs to navigate from the top left of a  $6 \\times 6$  grid to the bottom right with sparse reward. We generate data from a fixed, sub-optimal policy to evaluate all methods in an offline fashion (additional details in Section A.5). We generate 20K episodes with this policy, and obtain value functions using Q-learning under the following 4 choices for the intervening actions: (1) Ground truth actions ( $V_{\\mathrm{gt}}$ ), (2) One action ( $V_{\\mathrm{one - act}}$ , ammounts to TD(0) policy evaluation), (3)  $4 \\times$  refinement of original action space ( $V_{4 \\times - \\mathrm{gt}}$ ). We modify the data so that each sample for a particular action in the original action space is randomly mapped to one of 4 actions in the augmented space. (4) Obfuscated actions ( $V_{\\mathrm{impure - act}}$ ). Original action with probability 0.5, and a random action with probability 0.5.\n\n\nFigure 1 shows the learned value functions under these different action labels, and reports the MSE from the true value function, along with induced behavior. In line with our expectations,  $V_{4 \\times \\mathrm{gt}}$  which uses a refinement of the actions is able to recover the optimal value function.  $V_{\\mathrm{one - act}}$  fails to recover the optimal value function, and recovers the value corresponding to the policy that generated the data.  $V_{\\mathrm{impure - act}}$ , under noise in action labels (non-refinement) also fails to recover the optimal value function. Furthermore, the behavior implied by  $V_{\\mathrm{impure - act}}$  and  $V_{\\mathrm{one - act}}$  is sub-optimal. We also analyze the effect of the action impurity on learned values and implied behavior. Behavior becomes increasingly inaccurate as action impurity increases. More details in Section A.3.\n\n\n# 4 LATENT ACTION Q-LEARNING\n\n\nOur analysis in Section 3 motivates the design of our approach for learning behaviors from state-only experience. Our proposed approach decouples learning into three steps: mining latent actions from state-only trajectories, using these latent actions for Q-learning to obtain value functions, and learning a policy to act according to the learned value function. As per our analysis, if learned latent actions are a state-conditioned refinement of the original actions, Q-learning will result in good value functions, that will lead to good behaviors. Refer to Algorithm 1 for details.\n\n\n# 4.1 LATENT ACTIONS FROM FUTURE PREDICTION\n\n\nGiven a dataset  $\\mathbf{D}$  of observations streams  $\\dots, o_t, o_{t+1}, \\dots$ , the goal in this step is to learn latent actions that are a refinement of the actual actions that the agent executed. We learn these latent actions through future prediction. We train a future prediction model  $f_\\theta$ , that maps the observation  $o_t$  at time  $t$ , and a latent action  $\\hat{a}$  (from a set  $\\hat{\\mathbf{A}}$  of discrete latent actions) to the observation  $o_{t+1}$  at time  $t+1$ , i.e.  $f_\\theta(o_t, \\hat{a})$ .  $f$  is trained to minimize a loss  $l$  between the prediction  $f_\\theta(o_t, \\hat{a})$  and the ground truth observation  $o_{t+1}$ .  $\\hat{a}$  is treated as a latent variable during learning. Consequently,  $f_\\theta$  is trained using a form of expectation maximization (Bishop, 2006). Each training sample  $(o_t, o_{t+1})$  is assigned to the action that leads to the lowest loss under the current forward model. The function  $f_\\theta$  is optimized to minimize the loss under the current latent action assignment. More formally,\n\n\nthe loss for each sample  $(o_t, o_{t+1})$  is:  $L(o_t, o_{t+1}) \\coloneqq \\min_{\\hat{a} \\in \\hat{\\mathbf{A}}} l(f_\\theta(o_t, \\hat{a}), o_{t+1})$ . We minimize  $\\sum_{(o_t, o_{t+1}) \\in \\mathbf{D}} L(o_t, o_{t+1})$  over the dataset to learn  $f_\\theta$ .\n\n\nLatent action  $\\hat{a}_t$  for observation pairs  $(o_t, o_{t+1})$  are obtained from the learned function  $f_\\theta$  as:  $\\arg \\min_{\\hat{a} \\in \\hat{\\mathbf{A}}} l(f_\\theta(o_t, \\hat{a}), o_{t+1})$ . Choice of the function  $f_\\theta$  and loss  $l$  vary depending on the problem. We use L2 loss in the observation space (low-dimensional states, or images).\n\n\n# 4.2 Q-LEARNING WITH LATENT ACTIONS\n\n\nLatent actions mined from Section 4.1 allow us to complete the given  $(o_{t}, o_{t+1}, r_{t})$  tuples into  $(o_{t}, \\hat{a}_{t}, o_{t+1}, r_{t})$  quadruples for use in Q-learning Watkins (1989). As our actions are discrete we can easily adopt any of the existing Q-learning methods for discrete action spaces (e.g. Mnih et al. (2013)). Though, we note that this Q-learning still needs to be done in an offline manner from pre-recorded state-only experience. While we adopt the most basic Q-learning in our experiments, more sophisticated versions that are designed for offline Q-learning (e.g. Kumar et al. (2020); Fujimoto et al. (2019)) can be directly adopted, and should improve performance further. Value functions are obtained from the Q-functions as  $V(s) = \\max_{\\hat{a} \\in \\hat{\\mathbf{A}}} Q(s, \\hat{a})$ .\n\n\n# 4.3 BEHAVIORS FROM VALUE FUNCTIONS\n\n\nGiven a value function, our next goal is to derive behaviors from the learned value function. In general, this requires access to the transition function of the underlying MDP. Depending on what assumptions we make, this will be done in the following two ways.\n\n\nDensified Reinforcement Learning. Learning a value function from state-only experience can be extremely valuable when a dense reward function for the underlying task is not readily available. In this case, using the learned value function can densify sparse reward functions, making previously intractable RL problems solvable. Specifically, we use the value function to create a potential-based shaping function  $F(s,s^{\\prime}) = V(s^{\\prime}) - V(s)$ , based on Ng et al. (1999), and construct an augmented reward function  $r^{\\prime}(s,a,s^{\\prime}) = r(s,a,s^{\\prime}) + F(s,s^{\\prime})$ . Our experiments show that using this densified reward function speeds up behavior acquisition.\n\n\nDomain Specific Low-level Controllers. In more specific scenarios, it may be possible to employ hand designed low-level controllers in conjunction with a model that can predict the next state  $s'$  on executing any of low-level controllers. In such a situation, behavior can directly be obtained by picking the low-level controller that conveys the agent to the state  $s'$  that has the highest value under the learned  $V(s)$ . Such a technique was used by Chang et al. (2020). We show results in their setup.\n\n\n# 5 EXPERIMENTS\n\n\nWe design experiments to assess the quality of value functions learned by LAQ from undirected state-only experience. We do this in two ways. First, we measure the extent to which value functions learned with LAQ without ground truth information agree with value functions learned with Q-learning with ground truth action information. This provides a direct quality measure and allows us to compare different ways of arriving at the value function: other methods in the literature (D3G (Edwards et al., 2020)), and simpler alternatives of arriving at latent actions. Our second evaluation measures the effectiveness of LAQ-learned value functions for deriving effective behavior in different settings: when using it as a dense reward, when using it to guide low-level controllers, and when transferring behavior across embodiments. Where possible, we compare to behavior cloning (BC) with privileged ground truth actions. BC with ground truth actions serves as an upper bound on the performance of state-only imitation learning methods (BCO from Torabi et al. (2018a), ILPO from Edwards et al. (2019), etc.) and allows us to indirectly compare with these methods.\n\n\nTest Environments. Our experiments are conducted in five varied environments: the grid world environment from Section 3, the Atari game Freeway from Bellemare et al. (2013), 3D visual navigation in realistic environments from Chang et al. (2020); Savva et al. (2019), and two continuous control tasks from Fu et al. (2020)'s D4RL: Maze2D (2D continuous control navigation), and FrankaKitchen (dexterous manipulation in a kitchen). For Maze2D and FrankaKitchen environments, we also consider embodiment transfer, where we seek to learn policies for an ant and a hook\n\n\n![2D Grid World Figure 3: We experiment with five environments: 2D Grid World, Freeway (Atari), 3D Visual Navigation, Maze2D (2D Continuous Control), and FrankaKitchen. Top right corner of Maze2D and FrankaKitchen, shows the embodiments for cross-embodiment transfer (ant and hook, respectively).](images/6b40b0699a1ad04be9e602899b3715522bae403879488ff9c51a64891948ab30.jpg)\n\n\n![Freeway](images/f975d9fe888a7b6bf27d7c0c86f46869b4735d844018d02e23869e312bcb2b6c.jpg)\n\n\n![3D Visual Nav.](images/5d840d5657c00505f9070cfc3ad3336cc27b3dd9ebf17fd098187d9c037f9186.jpg)\n\n\n![2D Maze](images/a318ea9379c330deac310df5105f9545367809ebcb26917d0b217bbe80d2682a.jpg)\n\n\n![Kitchen](images/25bc14fb87f235502eb9a81af19715e22fee2c18c51d2f4e21c603dd9f36fb22.jpg)\n\n\nTable 1: We report Spearman's correlation coefficients for value functions learned using various methods with DQN, against a value function learned offline using ground-truth actions (DQN for discrete action environments, and DDPG for continuous action environments). The Ground Truth Actions column shows Spearman's correlation coefficients between two different runs of offline learning with ground-truth actions. See Section 5.1. Details on model selection in Section A.8.\n\n\n<table><tr><td>Environment</td><td>D3G</td><td>Single Action</td><td>Clustering</td><td>Clustering (Diff)</td><td>Latent Actions</td><td>Ground Truth Actions</td></tr><tr><td>2D Grid World</td><td>0.959</td><td>0.093</td><td>0.430</td><td>1.000</td><td>0.985</td><td>1.000</td></tr><tr><td>Freeway</td><td>- (image input)</td><td>0.886</td><td>0.945</td><td>0.902</td><td>0.961</td><td>0.970</td></tr><tr><td>3D Visual Navigation</td><td>- (image input)</td><td>0.641</td><td>0.722</td><td>0.827</td><td>0.927</td><td>0.991</td></tr><tr><td>2D Continuous Control</td><td>0.673</td><td>0.673</td><td>0.613</td><td>0.490</td><td>0.844</td><td>0.851</td></tr><tr><td>Kitchen Manipulation</td><td>0.854</td><td>0.858</td><td>0.818</td><td>0.815</td><td>0.905</td><td>0.901</td></tr></table>\n\n\nrespectively from the observation-only experience of a point mass and the Franka arm. Together, these environments test our approach on factors that make policy learning hard: continuous control, high-dimensional observations and control, complex real world appearance, 3D geometric reasoning, and learning across embodiments. See visualizations in Figure 3, details in Section A.4.\n\n\nExperimental Setup For each setting, we work with a pre-collected dataset of experience in the form of state, next state and reward triplets,  $(o_{t}, o_{t+1}, r_{t})$ . We use our latent-variable forward model (Section 4.1) and label triplets with latent actions to obtain quadruples  $(o_{t}, \\hat{a}_{t}, o_{t+1}, r)$ . We perform Q-learning on these quadruples to obtain value functions  $V(s)$ , which are used to acquire behaviors either through densified RL by interacting with the environment, or through use of domains-specific low-level controllers. We use the ACME codebase (Hoffman et al., 2020) for experiments.\n\n\nLatent Action Quality. In line with the theory developed in Section 3, we want to establish how well our method learns a refinement of the underlying action space. To assess this, we study the state-conditioned purity of the partition induced by the learned latent actions. It computes the proportion of the most frequent action of all ground truth actions mapped to a latent action (for any given state). Overall, our method is effective at finding refinement of the original action space. It achieves higher state-conditioned purity than a single action and clustering. In high-dimensional image observation settings, it surpasses baselines by a wide margin. More details in Section A.6.\n\n\n# 5.1 QUALITY OF LEARNED VALUE FUNCTIONS\n\n\nWe evaluate the quality of the value functions learned through LAQ. We use as reference the value function  $V_{\\mathrm{gt - act}}$ , obtained through offline Q-learning (DDPG for continuous action cases) with true ground truth actions i.e.  $(o_t, a_t, o_{t + 1}, r_t)$ . For downstream decision making, we only care about the relative ordering of state values. Thus, we measure the Spearman's rank correlation coefficient between the different value functions. Table 1 reports the Spearman's coefficients of value functions obtained using different action labels: single action, clustering, latent actions (ours), and with ground truth actions. We also report Spearman's correlations of value functions produced using\n\n\n![](images/3c1ed100aa60c2b1bac1464854503a57ac6db864ba918d3be049b4be8c2e0ad6.jpg)\n\n\n![](images/1da2f0a70c03beb05d75c477b9158bcae39281ce2f866b2fb4fdba83998a9f1b.jpg)\n\n\n![](images/60d3d9eed5787f8a3e42c84eb04b4c083cabfcae1504e74e7ba0a62db985754d.jpg)\n\n\n![Figure 4: We show learning curves for acquiring behavior using learned value functions. We compare densified RL (Section 4.3) with sparse RL and BC/BC+RL. See Section 5.2 for more details. Results are averaged over 5 seeds and show  $\\pm$  standard error.](images/93559af5c98a6201ca88e619dc12dd6689428efdfb26240ee336592f5167be1d.jpg)\n\n\nD3G (Edwards et al., 2020). In all settings we do Q-learning over the top 8 dominant actions, except for Freeway, where using the top three actions stabilized training.\n\n\nOur method out performs all baselines in settings with high-dimensional image observations (3D Visual Navigation, Freeway). In state based settings, where clustering state differences is a helpful inductive bias, we see that our method is still on-par with, or superior to clustering state differences and even D3G, which predicts state differences.\n\n\n# 5.2 USING VALUE FUNCTIONS FOR DOWNSSTREAM TASKS\n\n\nOur next experiments test the utility of LAQ-learned value functions for acquiring goal-driven behavior. We first describe the 3 settings that we use to assess this, and then summarize our takeaways.\n\n- - Using value functions as dense reward functions. We combine sparse task reward with the learned value function as a potential function (Section 4.3). We scale up the sparse task rewards by a factor of 5 so that behavior is dominated by the task reward once policy starts solving the task. Figure 4 measures the learning sample efficiency. We compare to only using the sparse reward, behavior cloning (BC) with ground truth actions, and BC followed by spare reward RL.\n\n- - Using value functions to learn behavior of an agent with a different embodiment. Decoupling the learning of value function and the policy has the advantage that learned value functions can be used to improve learning across embodiment. We demonstrate this, we keep the same task, but change the embodiment of the agent in Maze2D and FrankaKitchen environments. Note that we do not assume access to ground truth actions in these experiments either. For Maze2D, the point-mass is replaced with a 8-DOF quadrupedal ant. For FrankaKitchen, the Franka arm is replaced with a position-controlled hook. We may need to define how we query the value function when the embodiment (and the underlying state space) changes. For the ant in Maze2D, the location (with 0 velocity) of the ant body is used to query the value function learned with the point-mass. For the hook in FrankaKitchen, the value function is able to transfer directly as both settings observe end-effector position and environment state. We report results in Figure 5.\n\n- - Using value functions to guide low-level controllers. Learned value functions also have the advantage that they can be used directly at test time to guide the behavior of low-level controllers. We do this experiment in context of 3D visual navigation in a scan of a real building and use the branching environment from Chang et al. (2020). We follow their setup and replace their value functions with ones learned using LAQ in their hierarchical policy, and compare the efficiency of behavior encoded by the different value functions.\n\n\nLAQ value functions speed up downstream learning. Learning plots in Figure 4 show that LAQ-learned value functions speed up learning in the different settings over learning simply with sparse rewards (orange line vs. blue line). In all settings except Freeway, our method not only learns more quickly than sparse reward, but converges to a higher mean performance.\n\n\nLAQ discovers stronger behavior than imitation learning when faced with undirected experience. An advantage of LAQ over other imitation-learning based methods such as BCO (Torabi et al., 2018a) and ILPO (Edwards et al., 2019) is LAQ's ability to learn from sub-optimal or undirected experience. To showcase this, we compare the performance of LAQ with behavior cloning (BC) with ground truth actions. Since BCO and ILPO recover ground truth actions to perform behavior\n\n\nTable 2: We report Spearman's correlation coefficients for value functions learned using either DQN or BCQ, against a value function learned offline using BCQ with ground-truth actions. The Ground Truth Actions column shows Spearman's correlation coefficients between two different runs of offline learning with ground-truth actions. See Section 5.1.\n\n\n<table><tr><td>Environment</td><td>Single Action</td><td>Clustering</td><td>Clustering (Diff)</td><td>Latent Actions</td><td>Ground Truth Actions</td></tr><tr><td>2D Continuous Control (DQN)</td><td>0.664</td><td>0.431</td><td>0.312</td><td>0.807</td><td>0.765</td></tr><tr><td>2D Continuous Control (BCQ)</td><td>0.710</td><td>0.876</td><td>0.719</td><td>0.927</td><td>0.990</td></tr></table>\n\n\n![Figure 5: Learning curves for acquiring behavior with value functions across embodiment. Results averaged over 50 seeds and show  $\\pm$  standard error.](images/087143d13b455d86beef4691470e3194123003c3b8fa7ec288caabfdb8470626.jpg)\n\n\n![](images/f4e65eac12d70ce9f13815ceda10f052af731c9f83331b79228ef552adb2155d.jpg)\n\n\n![](images/89a1d2537be34fe1ff7b68a1ece5f4c6268c9e91a6c45e3df0d57c3d7f4dbb2e.jpg)\n\n\n<table><tr><td></td><td colspan=\"2\">Interaction</td></tr><tr><td></td><td>Samples</td><td>SPL</td></tr><tr><td>Vone-act (Chang et al., 2020)</td><td>0</td><td>0.53</td></tr><tr><td>Vcluster-act</td><td>0</td><td>0.57</td></tr><tr><td>VIatent-act</td><td>0</td><td>0.82</td></tr><tr><td>VInverse-act (Chang et al., 2020)</td><td>40K</td><td>0.95</td></tr></table>\n\n\nFigure 6: Visualization of trajectories and SPL numbers in the 3D visual navigation environment.\n\n\ncloning (BC), BC with ground truth actions serves as an upper bound on the performance of all methods in this class. Learning plots in Figure 4 shows the effectiveness of LAQ over BC and BC followed by fine-tuning with sparse rewards for environments where the experience is undirected (Maze2D, and GridWorld). For Freeway, the experience is fairly goal-directed, thus BC already works well. A similar trend can be seen in the higher Spearman's coefficient for LAQ vs.  $V_{\\text{one-act}}$  in Table 1. LAQ discovers stronger behavior than imitation learning when faced with undirected data.\n\n\nLAQ is compatible with other advances in batch RL. Although LAQ uses the most basic Q-Learning as our offline value learning method, it is compatible with recent more advanced offline RL value-learning methods (such as CQL (Kumar et al., 2020) and BCQ (Fujimoto et al., 2019)). We validate by simply swapping to using (discrete) BCQ with our latent actions. Figures. 4 and 5 show that LAQ with BCQ is the strongest method, outperforming ours with DQN, and D3G, on Maze2D and embodiment transfer environments. Analysis of Spearman's correlations in Table 2 shows the same trend as before with latent actions: better than single actions, and clustering variants. Note also that use of BCQ leads to value functions with better Spearman's correlations than DQN.\n\n\nLAQ value functions allow transfer across embodiments. Figure 5 shows learning plots of agents trained with cross-embodiment value functions. LAQ-densified rewards functions, speed-up learning and consistently guide to higher reward solutions than sparse task rewards, or D3G.\n\n\nLAQ compares favorably to D3G. We compare LAQ and D3G (a competing state-only method) in different ways. D3G relies on generating potential future states. This is particularly challenging for image observations, and D3G doesn't show results with image observations. In contrast, LAQ maps state transitions to discrete actions, and hence works with image observations as our experiments show. Even in scenarios with low-dimensional state inputs, LAQ learns better value functions than D3G, as evidences by Spearman's correlations in Table 1, and learning plots in Figure 4 and Figure 5.\n\n\nLAQ value functions can guide low-level controllers for zero-shot control: We report the SPL for 3D navigation using value functions combined with low-level controllers in Figure 6. We report the efficiency of behavior induced by LAQ learned value functions as measured by the SPL metric from Anderson et al. (2018) (higher is better). The branching environment has two goal states, one optimal and one sub-optimal. The demonstrations there-in were specifically designed to emphasize the utility of knowing the intervening actions. Simple policy evaluation leads sub-optimal behavior (SPL of 0.53) and past work relied on using an inverse model to label actions (Chang et al., 2020) to derive better behavior. This inverse model itself required  $40K$  interactions with the environment\n\n\nfor training, and boosted the SPL to 0.95. LAQ is able to navigate to the optimal goal (w/ SPL 0.82) but without the  $40K$  online interaction samples necessary to acquire the inverse model. It also performs better than clustering transitions, doing which achieves an SPL of 0.57. The improvement is borne out in visualizations in Figure 6. LAQ correctly learns to go to the nearer goal, even when the underlying experience came from a policy that preferred the further away goal.\n\n\n# 6 DISCUSSION\n\n\nOur theoretical characterization and experiments in five representative environments showcase the possibility and potential of deriving goal-directed signal from undirected state-only experience. Here we discuss some scenarios which are fundamentally hard, and some avenues for future research.\n\n\nNon-deterministic MDPs. Our theoretical result relies on a refinement where state-action transition probabilities are matched. However, the latent action mining procedure in LAQ results in deterministic actions. Thus, for non-deterministic MDPs (where executing the same action in the same state takes the agent to different next state), LAQ will be unable to achieve a strict refinement, leading to sub-optimal value functions. However, note that this limitation isn't specific to our method, but applies to any deterministic algorithm that seeks to learn from observation only data. We formalize this concept and provide a proof in Section A.2.\n\n\nConstraining evaluation of  $V(s)$  to within its domain. LAQ learns a value function  $V(s)$  over the set of states that were available in the experience dataset, and as such its estimates are only accurate within this set. In situations where the experience dataset doesn't span the entire state space, we may need to assess where the predictions of  $V(s)$  are valid to avoid degenerate solutions due to OOD observations. We discuss a density based model solution we used for this problem in Section A.4.\n\n\nOffline RL Validation. Validation (e.g. when to stop training) is a known issue in offline RL (Gulcehre et al., 2020). Like other offline RL methods, LAQ suffers from it too. LAQ's use of Q-learning makes it compatible to recent advances (Kumar et al., 2021) that tackle this validation problem.\n\n\n# 7 RELATED WORK\n\n\nOur work focuses on batch (or offline) RL with state-only data using a latent-variable future prediction model. We survey works on batch RL, state-only learning, and future prediction.\n\n\nBatch Reinforcement Learning. As the field of reinforcement learning has matured, batch RL (Lange et al., 2012; Levine et al., 2020) has gained attention as a component of practical systems. A large body of work examines solutions the problem of extrapolation error in batch RL settings. Advances in these works are complementary to our approach, as substantiated by our experiments with BCQ. A more detailed discussion of batch RL methods can be found in Section A.0.\n\n\nState-only Learning. Past works have explored approaches for dealing with the lack of actions in offline RL when given goal-directed or undirected state-only experience. Works in the former category rely on high quality behavior in the data, and suffer on sub-optimal data. Past work on state-only learning from undirected experience relies on either domain knowledge or state reconstruction and only show results with low dimensional states. See Section A.0 for continued discussion.\n\n\nFuture Prediction Models. Past work from Oh et al. (2015); Agrawal et al. (2016); Finn et al. (2016) (among many others) has focused on building action conditioned forward models in pixel and latent spaces. Yet other work in computer vision studies video prediction problems (Xue et al., 2016; Castrejon et al., 2019). Given the uncertainty in future prediction, these past works have pursued variational (or latent variable) approaches to make better predictions. Our latent variable future model is inspired from these works, but we explore its applications in a novel context.\n\n\nLatent MDP Learning One way to interpret our method is that of learning an approximate MDP homomorphism (Taylor et al., 2008; Ravindran & Barto, 2004). Other works have explored learning latent homorphic MDPs. These methods tend to focus on learning equivalent latent state spaces (Li et al., 2006; Givan et al., 2003). Most similarly to our work (van der Pol et al., 2020) also learns a latent action space, but relies on ground truth action data to do so.\n\n\n# REFERENCES\n\n- Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pp. 104-114. PMLR, 2020. 18, 19\n\n- Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. In NeurIPS, 2016. 9\n\n- Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, and Amir Zamir. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018. 8\n\n- Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning, 2021. 13\n\n- M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, jun 2013. 5\n\n- Christopher M Bishop. Pattern recognition and machine learning. Springer, 2006. 4\n\n- Lluis Castrejon, Nicolas Ballas, and Aaron Courville. Improved conditional vrnns for video prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7608-7617, 2019. 9\n\n- Matthew Chang, Arjun Gupta, and Saurabh Gupta. Semantic visual navigation by watching youtube videos. In NeurIPS, 2020. 5, 7, 8, 13, 18, 19, 22\n\n- Ashley D. Edwards and Charles L. Isbell. Perceptual values from observation. CoRR, abs/1905.07861, 2019. 2, 13\n\n- Ashley D Edwards, Himanshu Sahni, Yannick Schroecker, and Charles L Isbell. Imitating latent policies from observation. In ICML, 2019. 5, 7, 13\n\n- Ashley D. Edwards, Himanshu Sahni, Rosanne Liu, Jane Hung, Ankit Jain, Rui Wang, Adrien Ecoffet, Thomas Miconi, Charles Isbell, and Jason Yosinski. Estimating  $q(s,s')$  with deep deterministic dynamics gradients. In ICML, 2020. 2, 5, 7, 13\n\n- Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. arXiv preprint arXiv:1605.07157, 2016. 9\n\n- Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. 5, 13, 18\n\n- Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In ICML, 2019. 5, 8, 13\n\n- Robert Givan, Thomas Dean, and Matthew Greig. Equivalence notions and model minimization in markov decision processes. Artificial Intelligence, 147(1-2):163-223, 2003. 9\n\n- Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gomez Colmenarejo, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, Gabriel Dulac-Arnold, Jerry Li, Mohammad Norouzi, Matt Hoffman, Ofir Nachum, George Tucker, Nicolas Heess, and Nando deFreitas. Rl unplugged: Benchmarks for offline reinforcement learning, 2020. 9, 13\n\n- Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas Abdelmaleki, Albin Cassirer, Fan Yang, Kate Baumli, et al. Acme: A research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020. 6\n\n- Ashish Kumar, Saurabh Gupta, and Jitendra Malik. Learning navigation subroutines by watching videos. In CoRL, 2019a. 13\n\n- Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. In NeurIPS, 2019b. 13\n\n- Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In NeurIPS, 2020. 5, 8, 13\n\n- Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for offline model-free robotic reinforcement learning. In CoRL, 2021. 9\n\n- Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement learning, pp. 45-73. Springer, 2012. 2, 9\n\n- Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. 2, 9\n\n- Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a unified theory of state abstraction for mdps. ISAIM, 4:5, 2006. 9\n\n- Ajay Mandlekar, Fabio Ramos, Byron Boots, Silvio Savarese, Fei-Fei Li, Animesh Garg, and Dieter Fox. IRIS: implicit reinforcement without interaction at scale for learning control from offline robot manipulation data. In ICRA, 2020. 13\n\n- Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. 5\n\n- Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, pp. 278-287. Morgan Kaufmann, 1999. 5\n\n- Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. arXiv preprint arXiv:1507.08750, 2015. 9\n\n- Ilija Radosavovic, Xiaolong Wang, Lerrel Pinto, and Jitendra Malik. State-only imitation learning for dexterous manipulation. CoRR, abs/2004.04650, 2020. 13\n\n- Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Offline reinforcement learning from images with latent space models. CoRR, abs/2012.11547, 2020. 13\n\n- Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit gradients. arXiv preprint arXiv:1909.04630, 2019. 13\n\n- Balaraman Ravindran and Andrew G Barto. Approximate homomorphisms: A framework for non-exact minimization in markov decision processes. 2004. 9\n\n- Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A platform for embodied AI research. In ICCV, 2019. 5, 18, 19\n\n- Pierre Sermanet, Kelvin Xu, and Sergey Levine. Unsupervised perceptual rewards for imitation learning. In RSS, 2017. 13\n\n- Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. Concept2robot: Learning manipulation concepts from instructions and human demonstrations. In RSS, 2020. 13\n\n- Avi Singh, Larry Yang, Chelsea Finn, and Sergey Levine. End-to-end robotic reinforcement learning without reward engineering. In RSS, 2019. 13\n\n- Shuran Song, Andy Zeng, Johnny Lee, and Thomas Funkhouser. Grasping in the wild: Learning 6 dof closed-loop grasping from low-cost demonstrations. Robotics and Automation Letters, 2020. 13\n\n- Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249. 2, 14\n\n- Jonathan Taylor, Doina Precup, and Prakash Panagaden. Bounding performance loss in approximate mdp homomorphisms. Advances in Neural Information Processing Systems, 2008. 9\n\n- Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In *IJCAI*, 2018a. 5, 7, 13\n\n- Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation. CoRR, abs/1807.06158, 2018b. 13\n\n- Faraz Torabi, Garrett Warnell, and Peter Stone. Adversarial imitation learning from state-only demonstrations. In AAMAS, pp. 2229-2231. International Foundation for Autonomous Agents and Multiagent Systems, 2019a. 13\n\n- Faraz Torabi, Garrett Warnell, and Peter Stone. Imitation learning from video by leveraging proprioception. arXiv preprint arXiv:1905.09335, 2019b. 13\n\n- Elise van der Pol, Thomas Kipf, Frans A. Oliehoek, and Max Welling. Plannable approximations to MDP homomorphisms: Equivariance under actions. In International Conference on Autonomous Agents and Multiagent Systems, AAMAS, 2020. 9\n\n- Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989. 1, 2, 5, 15\n\n- Annie Xie, Avi Singh, Sergey Levine, and Chelsea Finn. Few-shot goal inference for visuomotor learning and planning. In Conference on Robot Learning, pp. 40-52. PMLR, 2018. 13\n\n- Tianfan Xue, Jiajun Wu, Katherine L Bouman, and William T Freeman. Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks. arXiv preprint arXiv:1607.02586, 2016.9\n\nResponse: ### Prediction: \\(\\boxed{Accept}\\)\n\n#### Justification:\n\n**Originality:**\n- The paper presents a novel approach, **Latent Action Q-learning (LAQ)**, which is a significant contribution to the field of offline reinforcement learning (offline RL) and batch RL. The idea of learning value functions from state-only experience without explicit action labels is innovative and addresses a common limitation in many reinforcement learning tasks.\n- The authors provide theoretical insights into why certain methods might fail in this setting and propose a practical solution that leverages latent variables and future prediction models.\n\n**Methodology Rigor:**\n- The theoretical analysis is sound and well-supported by the authors. They rigorously prove the optimality of refining the action space and the validity of their approach.\n- The experimental setup is thorough, covering multiple environments with diverse characteristics (grid worlds, Atari games, 3D visual navigation, and continuous control tasks). The authors validate their approach in terms of correlation with ground truth values and the effectiveness of learned value functions in deriving behavior.\n- The use of various baselines (D3G, single action, clustering, and ground truth actions) ensures a fair comparison and demonstrates the superiority of LAQ.\n\n**Significance:**\n- The paper addresses a critical gap in current reinforcement learning methodologies by proposing a practical solution to the problem of learning from undirected state-only experience.\n- The method has broad applicability across different types of tasks and environments, including those with high-dimensional observations and complex dynamics.\n- The authors demonstrate the effectiveness of their approach in various downstream tasks, such as learning behavior from sparse rewards, transferring learned behavior across different embodiments, and guiding low-level controllers.\n\n**Clarity:**\n- The paper is well-written and easy to follow. The authors clearly explain the theoretical background, their proposed method, and the experimental setup.\n- The figures and tables effectively support the claims made in the text, providing clear visual representations of the results.\n\n**Alignment with Field Standards:**\n- The work aligns well with established norms in the field of reinforcement learning and batch RL. The authors cite relevant literature and provide a comprehensive discussion of related work.\n- The experimental validation is rigorous and the authors provide detailed analyses and comparisons with existing methods.\n\n### Strengths:\n1. **Theoretical Foundations**: The authors provide strong theoretical guarantees about the optimality of their approach under certain conditions.\n2. **Practical Implementation**: The method is practically implementable and can be applied to a wide range of reinforcement learning problems.\n3. **Broad Applicability**: The experiments cover multiple environments and tasks, demonstrating the versatility of the approach.\n4. **Clear Comparison with Baselines**: The authors provide clear and detailed comparisons with existing methods, highlighting the advantages of their approach.\n\n### Weaknesses:\n1. **Detailed Discussion on Non-Deterministic MDPs**: The authors mention a limitation of their approach for non-deterministic MDPs, but this is discussed briefly. A more detailed analysis or a proof of this limitation would strengthen the paper.\n2. **Computational Cost**: The method involves training a latent variable model, which may add computational overhead. The authors acknowledge this but do not provide extensive details on computational costs.\n3. **Comparison with D3G**: The authors compare LAQ with D3G, but the comparison is somewhat limited. Providing a more comprehensive comparison, especially in terms of computational efficiency and scalability, would be beneficial.\n\n### Conclusion:\nOverall, the paper presents a significant and innovative contribution to the field of offline reinforcement learning. The theoretical foundation, practical implementation, and experimental validation are all strong points. Minor adjustments to address the mentioned weaknesses would ensure a high-quality submission to ICLR. Therefore, I predict a high acceptance rate.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634830.1101017", "msecs": "110.0", "relativeCreated": "174933.57586860657", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers for acceptance. Assess based on originality, methodology rigor, significance, clarity, and alignment with field standards. Provide a structured prediction with confidence level (High/Medium/Low), detailing evidence-based rationale. Systematically weigh strengths and weaknesses, ensuring objectivity and adherence to peer review principles.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Logical Languages Accepted by Transformer Encoders with Hard Attention\n\n# Abstract\nWe contribute to the study of formal languages that can be recognized by transformer encoders. We focus on two self-attention mechanisms: (1) UHAT (Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention Transformers). UHAT encoders are known to recognize only languages inside the circuit complexity class ${\\sf AC}^0$, i.e., accepted by a family of poly-sized and depth-bounded boolean circuits with unbounded fan-ins. On the other hand, AHAT encoders can recognize languages outside ${\\sf AC}^0$), but their expressive power still lies within the bigger circuit complexity class ${\\sf TC}^0$, i.e., ${\\sf AC}^0$-circuits extended by majority gates. We first show a negative result that there is an ${\\sf AC}^0$-language that cannot be recognized by an UHAT encoder. On the positive side, we show that UHAT encoders can recognize a rich fragment of ${\\sf AC}^0$-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates. This logic, includes, for example, all regular languages from ${\\sf AC}^0$. We then show that AHAT encoders can recognize all languages of our logic even when we enrich it with counting terms. Using these results, we obtain a characterization of which counting properties are expressible by UHAT and AHAT, in relation to regular languages.\n\n\n# 1 INTRODUCTION\n\n\nTransformers have revolutionized natural language processing by facilitating the efficient and effective modeling of intricate contextual relationships within text (Vaswani et al., 2017). This remarkable capability has sparked numerous investigations into the potential boundaries of transformers' power (Hahn, 2020; Yao et al., 2021; P\u00e9rez et al., 2021; Weiss et al., 2021; Hao et al., 2022; Chiang & Cholak, 2022; Bhattamishra et al., 2020; Chiang et al., 2023; Merrill et al., 2022; Merrill & Sabharwal, 2023; Strobl, 2023). One natural method for addressing this question is to explore the classes of formal languages that these architectures can recognize. This approach provides an insight into their strengths and limitations. The response to this question naturally relies on the specific features allowed within transformer encoders. These encompass the interplay between encoders and decoders, the kind of functions used for positional encodings and attention mechanisms, and considerations of fixed or unbounded precision, among other factors.\n\n\nWhile the capacity of transformers that incorporate both encoders and decoders to recognize languages is well understood today (indeed, such architectures are Turing-complete and can thus recognize any computable language (P\u00e9rez et al., 2021)), a precise characterization of the languages accepted by transformer encoders is lacking. Unique Hard Attention Transformers (UHAT) are a class of transformer encoders that has been a subject of many recent papers. As was shown by Hao et al. (2022), UHATs recognize only languages in  $\\mathsf{AC}^0$ , i.e., recognized by families of Boolean circuits of unbounded fan-in that have constant depth and polynomial size. Intuitively, this means that\n\n\nUHATs are weak at \"counting\" (more precisely, reasoning about the number of occurrences of various letters in the input word). For example, consider the following languages: majority and parity. The first one corresponds to the set of words over alphabet  $\\{a, b\\}$  for which the majority of positions are labeled by  $a$ , while the second checks if the number of positions labeled  $a$  is even. That these languages are not in  $\\mathsf{AC}^0$  follows from a groundbreaking result in circuit complexity theory (Furst et al., 1981; Ajtai, 1983)). Hence, they are neither accepted by UHATs. However, which fragment of the  $\\mathsf{AC}^0$  languages can actually be recognized by UHATs remains an unresolved question.\n\n\nWe start by showing that not all  $\\mathsf{AC}^0$  languages can be accepted by UHATs. This is obtained by combining results of Ajtai (1983) and Hahn (2020). Based on the previous observation, we focus on identifying a rich fragment of  $\\mathsf{AC}^0$  that can in fact be embedded into the class of UHATs. To achieve this, we use the characterization of  $\\mathsf{AC}^0$  as the class of languages expressible in FO(All), the extension of first-order logic (FO) with all numerical predicates defined in relation to the linear order of a word (Immerman, 1999). We show that UHATs recognize all languages definable in FO(Mon), the restriction of FO(All) with unary numerical predicates only (Barrington et al., 2005). The logic FO(Mon) is highly expressive. Unlike FO, it can express non-regular languages like  $\\{a^n b^n \\mid n > 0\\}$ . Remarkably, it contains all regular languages within  $\\mathsf{AC}^0$ , which includes examples like  $(aa)^*$  \u2014 a language not definable in FO. Additionally, our result subsumes the result of Yao et al. (2021), where it is shown that Dyck languages of bounded nested depth can be recognized by UHATs. These languages are regular and belong to  $\\mathsf{AC}^0$ , hence they are expressible in FO(Mon).\n\n\nTo establish the result that UHATs recognize all languages definable in  $\\mathrm{FO(Mon)}$ , we take a slightly circuitous route: rather than directly formulating  $\\mathrm{FO(Mon)}$  sentences as UHATs, we show that each formula in LTL(Mon), the extension of linear temporal logic (LTL) (Clarke et al., 2018) with arbitrary unary numerical predicates, can be equivalently represented as an UHAT. The proof for  $\\mathrm{FO(Mon)}$  then derives from Kamp's seminal theorem (Kamp, 1968), which establishes the equivalence between languages definable in FO and LTL. The advantage of dealing with LTL, in contrast to FO, lies in the fact that all LTL formulas are unary in nature, i.e., they are interpreted as sets of positions on a word, unlike FO formulas which possess arbitrary arity. This property aligns well with the expressive capabilities of UHATs, facilitating a proof through structural induction.\n\n\nWhile the fact that UHAT is in  $\\mathsf{AC}^0$  implies limited counting abilities of such encoders, recent work has shown that a slight extension of the hard attention mechanism can help in recognizing languages outside  $\\mathsf{AC}^0$  (Hao et al., 2022). Instead of using unique hard attention, this model uses average hard attention (AHAT), which refers to the idea that the attention mechanism returns the uniform average value among all positions that maximize the attention. To what extent does AHAT enrich the counting ability of UHAT? In answering this question, we introduce a logic named LTL( $\\mathbf{C}, +$ ), which is an extension of LTL(Mon) that naturally incorporates counting features. We show that any language that can be defined within LTL( $\\mathbf{C}, +$ ) can also be identified by an AHAT. The logic LTL( $\\mathbf{C}, +$ ) can express interesting languages lying outside  $\\mathsf{AC}^0$  including majority and parity. More generally, our result implies that AHATs are equipped with a powerful counting ability: all permutation-closed languages over a binary alphabet and all permutation closures of regular languages (which are in general not context-free) can be recognized by AHATs.\n\n\nAs a corollary, we provide a characterization of the \"counting properties\" of regular languages which can be captured by UHAT and AHAT. Two approaches for understanding counting properties of regular languages can be found in formal language theory: (1) Parikh-equivalence, and (2) taking letter-permutation closure. Both approaches \"remove\" the ordering from the input string, and as a result only the letter-count (more popularly called Parikh images) of an input string matters. In the setting of (1) (e.g. see (Parikh, 1966; Kozen, 1997)), a machine is allowed to try all letter reordering of the input string  $w$ , and accepts iff the machine accepts some letter reordering of  $w$ . According to the well-known Parikh's Theorem (Parikh, 1966), each context-free language can in this way be captured by a regular language, e.g.,  $\\{0^n 1^n : n \\geq 0\\}$  can be captured by  $(01)^*$ . We show in this paper that each regular language is Parikh-equivalent to an UHAT language, despite the fact that PARITY is not in UHAT. In the setting of (2), a machine must accept all letter permutations of an input string  $w$ . The letter-permutation closure of a regular language is not necessarily regular, e.g., such a closure language of  $(abc)^*$  consists of all strings with the same number of  $a$ 's,  $b$ 's, and  $c$ 's, which is not even context-free. In this setting, although UHAT cannot capture regular languages like PARITY, AHAT can surprisingly capture all regular languages.\n\n\nRelated work. There has been very little research on identifying logical languages that can be accepted by transformers. The only example we are aware of is the recent work by Chiang et al. (2023), in which a variant of first-order logic with counting quantifiers is demonstrated to be embeddable into transformer encoders with a soft attention mechanism. The primary distinction between their work and our results is the choice of the attention mechanism. Additionally, the logic examined in their paper does not have access to the underlying word order being considered. This implies that some simple languages, such as  $a^*b^*$ , which are definable in FO, are not definable in their logic.\n\n\nDue to the space constraints, some of the proofs are omitted and can be found in the online version of this paper (Barcel\u00f3 et al., 2023).\n\n\n# 2 BACKGROUND NOTIONS AND RESULTS\n\n\n# 2.1 TRANSFORMER ENCODERS\n\n\nAn encoder layer is a function that takes a sequence of vectors,  $\\mathbf{v}_0,\\dots ,\\mathbf{v}_{n - 1}$ , in  $\\mathbb{R}^d$  as input, where  $d\\geq 0$ . It produces an output sequence of vectors of the same length,  $\\mathbf{v}_0^{\\prime},\\ldots ,\\mathbf{v}_{n - 1}^{\\prime}$ , in  $\\mathbb{R}^e$ , with  $e\\geq 0$ . The length of the sequence,  $n$ , can be arbitrary, but input and output dimensions,  $d$  and  $e$ , are fixed for an encoder layer. For the first part of the paper we employ a unique hard attention mechanism, meaning that a position only attends to the element with the highest attention score.\n\n\nFormally, an encoder layer with unique hard attention is given by two affinte transformations  $A, B \\colon \\mathbb{R}^d \\to \\mathbb{R}^d$  and one feed-forward neural network  $\\mathcal{N} \\colon \\mathbb{R}^{2d} \\to \\mathbb{R}^e$  with ReLU activation function. For  $i \\in \\{0, \\dots, n-1\\}$ , we set\n\n\n$$\n\n$$\n\\mathbf {a} _ {i} \\leftarrow \\mathbf {v} _ {j _ {i}},\n$$\n\n$$\n\n\nwhere  $j_{i}\\in \\{0,\\dots ,n - 1\\}$  is the minimum element that maximizes the attention score  $\\langle A\\mathbf{v}_i,B\\mathbf{v}_j\\rangle$  over  $j\\in \\{0,\\ldots ,n - 1\\}$ . The  $a_{i}$ s are often known as attention vectors. After that, we set\n\n\n$$\n\n$$\n\\mathbf {v} _ {i} ^ {\\prime} \\leftarrow \\mathcal {N} (\\mathbf {v} _ {i}, \\mathbf {a} _ {i}), \\quad i = 0, \\dots , n - 1.\n$$\n\n$$\n\n\nIt is well-known that feed-forward neural networks with ReLU activation can express the function  $\\max \\{x,y\\}$ . Thus, we may assume that  $\\mathcal{N}$  can be an arbitrary composition of affine functions with max.\n\n\nTransformer encoder. A unique hard attention transformer encoder (UHAT) is defined simply as the repeated application of encoder layers with unique hard attention.\n\n\n# 2.2 LANGUAGES ACCEPTED BY TRANSFORMER ENCODERS\n\n\nNext, we define how a transformer can be used to accept languages over a finite alphabet. This requires extending transformer encoders with three features: a function for representing alphabet symbols as vectors (which, for the purposes of this paper, we represent as one-hot encodings), another function that provides information about the absolute positions of these symbols within the input word, and a vector that is used for checking whether the word should be accepted or not. The function that provides information about positions is often referred to as a positional encoding, and it is essential for recognizing properties of ordered sequences of vectors. In fact, without positional encoding, encoders treat input sequences as invariant to permutations (P\u00e9rez et al., 2021).\n\n\nConsider a finite alphabet  $\\Sigma$  and let  $T$  be an UHAT that takes a sequence of vectors over  $\\mathbb{R}^d$  as input and converts it into a sequence of vectors over  $\\mathbb{R}^e$ . A language  $L \\subseteq \\Sigma^+$  is accepted by  $T$ , if there is an embedding function  $f \\colon \\Sigma \\to \\mathbb{R}^d$ , a positional encoding function  $p \\colon \\mathbb{N} \\times \\mathbb{N} \\to \\mathbb{R}^d$ , and a vector  $\\mathbf{t} \\in \\mathbb{R}^e$ , such that for every  $\\bar{w} \\in L$  we have  $T'(\\bar{w}) > 0$ , and for every  $w \\in \\Sigma^+ \\setminus L$  we have  $T'(\\bar{w}) < 0$ . Here,  $T': \\Sigma^+ \\to \\mathbb{R}$  is defined as follows. Let  $\\bar{w} = a_0 \\dots a_{n-1} \\in \\Sigma^n$ , and suppose the output of  $T$  when given the input sequence  $f(a_0) + p(0,n)$ , ...,  $f(a_{n-1}) + p(n-1,n)$  is the sequence  $\\mathbf{v}_0, \\dots, \\mathbf{v}_{n-1}$ . Then we set  $T'(\\bar{w}) = \\langle \\mathbf{t}, \\mathbf{v}_0 \\rangle$ .\n\n\n# 2.3 FIRST ORDER LOGIC ON WORDS\n\n\nWe assume familiarity with first-order logic (FO). Let  $\\Sigma$  be a finite alphabet. A word  $\\bar{w} = a_0 \\cdots a_{n-1}$  in  $\\Sigma^+$  is represented as a structure  $S_{\\bar{w}}$  whose domain is  $\\{0, \\ldots, n-1\\}$ . This structure includes a binary relation  $<$  that is interpreted as the linear order on the domain, and for each symbol  $a \\in \\Sigma$ , there is a unary relation  $P_a$  containing positions  $i = 0, \\ldots, n-1$  where  $a_i = a$ . Given an FO sentence over words, that is, an FO formula without free variables, we denote the language of all words  $\\bar{w} \\in \\Sigma^+$  satisfying  $S_{\\bar{w}} \\models \\phi$  as  $L(\\phi)$ . If an  $L \\subseteq \\Sigma^+$  satisfies  $L = L(\\phi)$ , for some FO sentence  $\\phi$ , then we say that  $L$  is definable in FO.\n\n\nExample 1. First-order logic (FO) enables us to define certain languages of interest. Here, we present an illustrative example. Initially, we recognize that we can employ FO to define a relation  $\\operatorname{first}(x) := \\neg \\exists y(y < x)$  that exclusively holds true at the first position of a word. Correspondingly, we can define a relation  $\\operatorname{last}(x) := \\neg \\exists y(x < y)$  that holds solely at the last position of the word. Moreover, it is possible to define a binary relation  $\\operatorname{succ}(x,y) := x < y \\land \\neg \\exists z(x < z \\land z < y)$ , which defines the successor relation within the domain. With these expressions, we can show that FO is capable of defining the language  $(ab)^{+}$ :\n\n\n$$\n\n$$\n\\exists x \\left(\\operatorname {f i r s t} (x) \\wedge P _ {a} (x)\\right) \\wedge \\exists x \\left(\\operatorname {l a s t} (x) \\wedge P _ {b} (x)\\right) \\wedge \\forall x \\forall y \\left(\\operatorname {s u c c} (x, y) \\rightarrow \\left(P _ {a} (x) \\leftrightarrow P _ {b} (y)\\right)\\right).\n$$\n\n$$\n\n\nThat is, the first symbol of the word is an  $a$ , the last one is a  $b$ , every  $a$  is followed by a  $b$ , and every  $b$  is preceded by an  $a$ .\n\n\n# 2.4 UNARY NUMERICAL PREDICATES\n\n\nIt is known that FO sentences can only define regular languages. In turn, there are regular languages that are not definable in FO. An example is the language  $(aa)^*$ , which contains those words formed solely by the symbol  $a$  that are of even length. However, there is a straightforward extension of FO that can define this language: all we need to do is add unary predicate  $\\mathrm{even}(x)$ , which holds true at position  $i$  in a word if and only if  $i$  is even. In fact, extending FO with the predicate  $\\mathrm{even}(x)$  allows us to define the language  $(aa)^*$  using the following formula, which indicates that the last symbol in the word satisfies the unary predicate even:  $\\forall x P_a(x) \\land \\forall y (\\mathrm{last}(y) \\to \\mathrm{even}(y))$ .\n\n\nThe extension of FO with unary numerical predicates can then be useful for defining languages. We define a unary numerical predicate  $\\Theta$  as an infinite family of functions\n\n\n$$\n\n$$\n\\theta_ {n}: \\{0, \\dots , n \\} \\rightarrow \\{0, 1 \\}, \\qquad n > 0.\n$$\n\n$$\n\n\nGiven a word  $\\bar{w}$  in  $\\Sigma^{+}$  of length  $n$ , for  $n > 0$ , we have that the predicate  $\\Theta(x)$  holds in position  $i$  in  $\\bar{w}$  if and only if  $\\theta_{n}(i) = 1$  (so far, we do not use the value of  $\\theta_{n}$  at  $n$  as positions are numbered from 0 to  $n - 1$ . We will use this value in Section 4). Notice that under our definition, the truth of a unary numerical predicate at position  $i$  in the word  $\\bar{w}$  depends not only on  $i$  but also on the length of the word  $\\bar{w}$ . As we will explore further, this characteristic is advantageous for defining interesting languages in FO extended with arbitrary unary numerical predicates. Following the literature, we write FO(Mon) for such an extension (Barrington et al., 2005).\n\n\nExample 2. Consider, for example, the non-regular language  $\\{a^n b^n \\mid n > 0\\}$ . We show that it can be expressed in  $\\mathrm{FO(Mon)}$  with the help of a unary numerical predicate  $\\Theta(x)$  such that  $\\theta_n(i) = 1$  iff  $n$  is even and  $i = n/2 - 1$ . In fact, it suffices to use the formula:  $\\exists x (\\Theta(x) \\land P_a(x) \\land \\forall y (y < x \\rightarrow P_a(y)) \\land \\forall y (x < y \\rightarrow P_b(y)))$ . This formula expresses that the middle point  $i$  of  $\\bar{w}$  exists, is labeled as  $a$ , and all positions smaller than  $i$  are also labeled  $a$ , while all positions larger than  $i$  are labeled as  $b$ . This example illustrates the significance of unary numerical predicates depending on both the position and the length of the word over which the formula is evaluated.\n\n\nThe definition of the language  $L(\\phi) \\subseteq \\Sigma^{+}$  defined by an FO(Mon) sentence  $\\phi$  is analogous to the one we provided for FO.\n\n\n# 3  $AC^0$  LANGUAGES ACCEPTED BY UHATS\n\n\n# 3.1 NOT ALL LANGUAGES IN  $\\MATHSF{AC}^0$  ARE ACCEPTED BY UHATS.\n\n\nHao et al. (2022) proved that languages accepted by UHATs belong to the circuit complexity class  $\\mathsf{AC}^0$ , i.e., the class of languages accepted by families of Boolean circuits of unbounded fan-in,\n\n\nconstant depth, and polynomial size. We combine results by Ajtai (1983) and Hahn (2020) to show that the opposite is not the case, i.e., there are  $AC^0$  languages that are not accepted by UHATs.\n\n\nAs shown in Ajtai (1983), there is an  $\\mathsf{AC}^0$ -family of circuits  $\\{C_n\\colon \\{0,1\\} ^n\\to \\{0,1\\}\\}_{n\\in \\mathbb{N}}$  such that for all  $n$ , the circuit  $C_n$  accepts all strings with at least  $2n / 3$  ones and rejects all strings with at most  $n / 3$ . Consider a language approximate majority, consisting of strings accepted by circuits from  $\\{C_n\\}$ . This language is in  $\\mathsf{AC}^0$  by construction. However, as we state next, it cannot be recognized by an UHAT. This result is proved by using a property of UHATs established in Hahn (2020).\n\n\nProposition 1. There is no UHAT that accepts the language approximate majority.\n\n\nViola (2009) shows that  $\\{C_n\\}$  can be made polynomial-time computable, which implies the existence of a polynomial-time computable language from  $\\mathsf{AC}^0$  that cannot be accepted by an UHAT.\n\n\n# 3.2 MAIN RESULT: FO(MON) LANGUAGES ARE ACCEPTED BY UHATS\n\n\nProposition 1 tells us that not all  $\\mathsf{AC}^0$  languages are accepted by UHATs. In this section, we identify a significant subset of  $\\mathsf{AC}^0$  languages that can be accepted by UHATs. To accomplish this, we rely on the characterization of the class  $\\mathsf{AC}^0$  as those languages that can be defined in FO extended with arbitrary numerical predicates. Our main result establishes that as long as we restrict ourselves to unary numerical predicates, translation into UHATs is possible.\n\n\nTheorem 1. Let  $\\Sigma$  be a finite alphabet and  $\\phi$  an FO(Mon) sentence over words from the alphabet  $\\Sigma$ . There is an UHAT that accepts  $L(\\phi)$ .\n\n\nProving this result by induction on  $\\mathrm{FO(Mon)}$  formulas, which would be the most natural approach to tackle the problem, turns out to be difficult. The challenge arises because the  $\\mathrm{FO(Mon)}$  formulas obtained by induction can have arbitrary arity, and transformer encoders do not seem capable of handling the requirements imposed by such formulas. To address this issue, we take a different approach. We employ Kamp's Theorem, which establishes that the languages definable in FO are precisely those that are definable in linear temporal logic (LTL) (Kamp, 1968).\n\n\n# 3.3 USING LTL(MON) TO PROVE OUR MAIN RESULT\n\n\nWe first explain how LTL is defined, as this is crucial to understanding the remainder of the paper. Let  $\\Sigma$  be a finite alphabet. LTL formulas over  $\\Sigma$  are defined as follows: if  $a \\in \\Sigma$ , then  $a$  is an LTL formula. Additionally, LTL formulas are closed under Boolean combinations. Finally, if  $\\phi$  and  $\\psi$  are LTL formulas, then  $\\mathbf{X}\\phi$  and  $\\phi \\mathbf{U}\\psi$  are also LTL formulas. Here,  $\\mathbf{X}$  is referred to as the next operator, and  $\\mathbf{U}$  as the until operator.\n\n\nLTL formulas are unary, i.e., they are evaluated over positions within a word. Let  $\\bar{w} = a_0\\cdots a_{n-1}$  be a word in  $\\Sigma^{+}$ , and let  $i = 0,\\dots,n-1$ . We define the satisfaction of an LTL formula  $\\phi$  over  $\\bar{w}$  at position  $i$ , written as  $(\\bar{w},i)\\models \\phi$ , inductively as follows (omitting Boolean combinations):\n\n- -  $(\\bar{w}, i) \\models a$  if and only if  $a = a_i$ , for  $a \\in \\Sigma$ .\n\n- -  $(\\bar{w}, i) \\models \\mathbf{X}\\phi$  if and only if  $i < n - 1$  and  $(\\bar{w}, i + 1) \\models \\phi$ . In other words,  $\\phi$  holds in the next position after  $i$  (if such a position exists).\n\n- -  $(\\bar{w}, i) \\models \\phi \\mathbf{U} \\psi$  if and only if there exists a position  $j = i, \\dots, n - 1$  for which  $(\\bar{w}, j) \\models \\psi$  and such that  $(\\bar{w}, k) \\models \\phi$  for every  $k$  with  $i \\leq k < j$ . That is,  $\\phi$  holds starting from position  $i$  until the first position where  $\\psi$  holds (and a position where  $\\psi$  holds must exist).\n\n\nWe can extend LTL with unary numerical predicates in the same way we did it for FO. Formally, we define  $\\mathrm{LTL}(\\mathrm{Mon})$  as the extension of LTL with every formula of the form  $\\Theta$ , for  $\\Theta$  a unary numerical predicate. We write  $(\\bar{w},i)\\models \\Theta$  to denote that  $\\theta_{n}(i) = 1$ , where  $n$  is the length of  $\\bar{w}$ . If  $\\phi$  is an LTL(Mon) formula over  $\\Sigma$ , we write  $L(\\phi)$  for the set of words  $\\bar{w}\\in \\Sigma^{+}$  with  $(\\bar{w},0)\\models \\phi$ .\n\n\nKamp's Theorem establishes that for every FO sentence  $\\phi$  there exists an LTL formula  $\\psi$  such that  $L(\\phi) = L(\\psi)$ , and vice-versa. It is straightforward to see that this property extends to the logics FO(Mon) and LTL(Mon).\n\n\nProposition 2. (Kamp, 1968) For every FO(Mon) sentence  $\\phi$  there exists an LTL(Mon) formula  $\\psi$  such that  $L(\\phi) = L(\\psi)$ , and vice-versa.\n\n\nOur proof of Theorem 1 is then derived directly from Proposition 2 and the following result.\n\n\nProposition 3. Let  $\\Sigma$  be a finite alphabet and  $\\phi$  an LTL(Mon) formula defined over words from the alphabet  $\\Sigma$ . There is an UHAT  $T$  that accepts  $L(\\phi)$ .\n\n\nBefore proving this result, we make the following important remark regarding the positional encoding  $p$  used by  $T$  to accept  $L(\\phi)$ . On a pair  $(i,n) \\in \\mathbb{N} \\times \\mathbb{N}$  with  $i < n$ , we have that  $p(i,n)$  is composed of elements  $i, 1/(i+1), (-1)^i, \\cos\\left(\\pi(1-2^{-i})/10\\right), \\sin\\left(\\pi(1-2^{-i})/10\\right)$ , and  $\\theta_n(i)$ , for every unary numerical predicate  $\\Theta$  mentioned in  $\\phi$ .\n\n\nProof of Proposition 3. Let  $\\phi$  be a formula of LTL(Mon). We say that a UHAT realizes  $\\phi$  position-wise if, given a word  $\\bar{w} = a_0\\ldots a_{n - 1}\\in \\Sigma^+$ , the UHAT outputs a sequence:\n\n\n$$\n\n$$\n\\mathbb {I} \\left\\{\\left(\\bar {w}, 0\\right) \\vDash \\phi \\right\\}, \\mathbb {I} \\left\\{\\left(\\bar {w}, 1\\right) \\vDash \\phi \\right\\}, \\dots , \\mathbb {I} \\left\\{\\left(\\bar {w}, n - 1\\right) \\vDash \\phi \\right\\};\n$$\n\n$$\n\n\nthat is, a binary word indicating for which positions  $\\phi$  is true on  $\\bar{w}$  and for which is false. We show by structural induction that every LTL(Mon) formula is realizable position-wise by some UHAT.\n\n\nLet us consider first the base cases. If  $\\phi = a$ , for some  $a \\in \\Sigma$ , our goal is to obtain a sequence:\n\n\n$$\n\n$$\n\\mathbb {I} \\{a _ {0} = a \\}, \\mathbb {I} \\{a _ {1} = a \\}, \\dots , \\mathbb {I} \\{a _ {n - 1} = a \\}.\n$$\n\n$$\n\n\nThis can easily be achieved by using a one-hot encoding as the embedding function. In turn, if  $\\phi = \\Theta$ , for  $\\Theta$  a unary numerical predicate, then  $\\phi$  can be realized position-wise using the corresponding positional encoding  $p(i,n) = \\theta_n(i)$ .\n\n\nWe continue with Boolean combinations. They can be implemented position-wise by compositions of affine transformation and max as follows:  $\\neg x = 1 - x^{\\prime}$  and  $x\\vee y = \\frac{\\max\\{2x - 1,2y - 1\\} + 1}{2}$ .\n\n\nFor the cases when our formula is of the form  $\\mathbf{X}\\phi$  or  $\\phi \\mathbf{U}\\psi$ , we need the following lemma.\n\n\nLemma 1. There is an UHAT that transforms each  $x_0, \\ldots, x_{n-1} \\in \\{0,1\\}$  as follows:\n\n\n$$\n\n$$\nx _ {0}, \\dots , x _ {n - 2}, x _ {n - 1} \\mapsto x _ {0}, \\dots , x _ {n - 2}, 0.\n$$\n\n$$\n\n\nLet us assume now that our formula is of the form  $\\mathbf{X}\\phi$ . It is enough to design a unique hard attention layer in which attention is always maximized at the next position. More precisely, we construct an UHAT that outputs a sequence of vectors  $\\mathbf{v}_1,\\ldots ,\\mathbf{v}_n\\in \\mathbb{R}^3$ , and a linear transformation  $A\\colon \\mathbb{R}^3\\to \\mathbb{R}^3$ , such that  $\\arg \\max_{j\\in \\mathbb{N}}\\langle A\\mathbf{v}_i,\\mathbf{v}_j\\rangle = \\{i + 1\\}$ , for  $i = 0,\\dots ,n - 2$ . This will allow us to \"send\"  $\\mathbb{I}\\{(\\bar{w},i + 1)\\models \\phi \\} = \\mathbb{I}\\{(\\bar{w},i)\\models \\mathbf{X}\\phi \\}$  to the  $i$ th position, for  $i = 0,\\dots ,n - 2$ . It only remains then to apply Lemma 1 to obtain  $0 = \\mathbb{I}\\{(\\bar{w},n - 1)\\models \\mathbf{X}\\phi \\}$  at the last position.\n\n\nUsing our positional encoding and an affine position-wise transformation, we can obtain:\n\n\n$$\n\n$$\n\\mathbf {v} _ {i} = \\left(\\cos \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), \\sin \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), (- 1) ^ {i} \\cdot 1 0\\right).\n$$\n\n$$\n\n\nLet  $A$  be a linear transformation that inverts the sign of the third coordinate. Observe that:\n\n\n$$\n\n$$\n\\langle A \\mathbf {v} _ {i}, \\mathbf {v} _ {j} \\rangle = \\cos \\left(\\frac {\\pi (2 ^ {- i} - 2 ^ {- j})}{1 0}\\right) + (- 1) ^ {i + j + 1} \\cdot 1 0.\n$$\n\n$$\n\n\nWe claim that, for a fixed  $i$ , this quantity is maximized at  $j = i + 1$ . First, those  $js$  that have the same parity as  $i$  (in particular,  $j = i$ ) cannot achieve the maximum because the second term is  $-10$ . For  $js$  with a different parity, we have  $\\langle A\\mathbf{v}_i,\\mathbf{v}_j\\rangle = \\cos \\left(\\pi (2^{-i} - 2^{-j}) / 10\\right) + 10$ . Since all angles are in  $[- \\pi /10,\\pi /10]$ , this quantity is maximized when  $|2^{-i} - 2^{-j}|$  is minimized. For  $j < i$ , the last quantity is at least  $2^{-i}$ , and for  $j > i$ , the minimum of this quantity is  $2^{-i - 1}$ , achieved at  $j = i + 1$ .\n\n\nLet us finally assume that our formula is of the form  $\\phi \\mathbf{U}\\psi$ . Observe that the value of  $\\phi \\mathbf{U}\\psi$  at position  $i$  can be computed as follows: we go to the right, starting from the  $i$ th position, until we see a position  $j_{i}$  where either  $\\phi$  is false, or  $\\psi$  is true, or this is the last position. Then  $\\phi \\mathbf{U}\\psi$  holds at  $i$  if and only if  $\\psi$  holds at  $j_{i}$ . That is,  $(\\bar{w}, i) \\models$  if and only if  $(\\bar{w}, j_{i}) \\models \\psi$ , where  $j_{i} \\in \\{i, \\dots, n-1\\}$  is the minimal position with  $\\tau(j) = 1$ , where, in turn, the sequence  $\\tau$  is defined by  $\\tau(i) = \\mathbb{I}\\{(\\bar{w}, i) \\models \\neg \\phi \\vee \\psi\\} \\vee \\mathbb{I}\\{i = n-1\\}$ , for  $i = 0, \\dots, n-1$ . To compute the sequence  $\\tau$ , we first compute  $\\phi \\wedge \\neg \\psi$\n\n\nposition-wise (we can do that because we already have  $\\phi$  and  $\\psi$  at our disposal), then we add the conjunction with  $\\mathbb{I}\\{i\\neq n - 1\\}$  by Lemma 1, and then we take the negation of the resulting sequence.\n\n\nTo show the lemma, it is enough to create a unique hard attention layer, where for every position  $i$  the attention is maximized at  $j_{i}$ . Using our positional encoding and the induction hypothesis, we can obtain a sequence of vectors  $\\mathbf{v}_1,\\ldots ,\\mathbf{v}_n\\in \\mathbb{R}^4$  such that:\n\n\n$$\n\n$$\n\\mathbf {v} _ {i} = \\Big (\\cos \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), \\sin \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), 1, \\tau (i) \\Big).\n$$\n\n$$\n\n\nConsider a linear transformation  $B\\colon \\mathbb{R}^4\\to \\mathbb{R}^4$  such that\n\n\n$$\n\n$$\nB \\mathbf {v} _ {i} = \\Big (\\cos \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), \\sin \\left(\\frac {\\pi (1 - 2 ^ {- i})}{1 0}\\right), 1 0 \\tau (i), 0 \\Big).\n$$\n\n$$\n\n\nObserve that  $\\langle \\mathbf{v}_i, B\\mathbf{v}_j \\rangle = \\cos \\left( \\frac{\\pi (2^{-i} - 2^{-j})}{10} \\right) + 10\\tau(j)$ . We claim that this expression is maximized at  $j = j_i$ . First, because of the last term in it, it cannot be maximized at  $j$  with  $\\tau(j) = 0$ . It remains to show that among the  $j$ 's with  $(\\bar{w}, j) \\neq \\phi$ , this quantity is minimized on the minimal  $j$  which is at least  $i$ . In fact, in this case we have  $\\langle \\mathbf{v}_i, B\\mathbf{v}_j \\rangle = \\cos \\left( \\frac{\\pi (2^{-i} - 2^{-j})}{10} \\right) + 10$ . All the angles in question are in  $[-\\pi / 10, \\pi / 10]$ , so the cosine is maximized when  $|2^{-i} - 2^{-j}|$  is minimized. Now, this absolute value is at least  $2^{-i}$  when  $j < i$ . In turn, this absolute value is smaller than  $2^{-i}$  for  $j \\geq i$ , and it is the smaller the smaller is  $j$ , as required.\n\n\n# 3.4 APPLICATIONS OF OUR MAIN RESULT\n\n\nWe show two applications of our main result. First, UHATs accept all regular languages in  $\\mathsf{AC}^0$ . Second, UHATs are strictly more expressive than regular and context-free languages in terms of the acceptance of languages up to letter-permutation.\n\n\nRegular languages in  $\\mathsf{AC}^0$ . There is an important fragment of  $\\mathrm{FO(Mon)}$  which is interesting in its own right. This is the logic  $\\mathrm{FO(Mod)}$ , i.e., the extension of FO with unary numerical predicates of the form  $\\mathsf{Mod}_p^r$ , for  $p > 1$  and  $0 \\leq r \\leq p - 1$ . We have that  $\\mathsf{Mod}_p^r(i) = 1$  if and only if  $i \\equiv r \\pmod{p}$ . In fact, by using a characterization given in Barrington et al. (1992), one can show that the languages definable in  $\\mathrm{FO(Mod)}$  are precisely the regular languages within  $\\mathsf{AC}^0$ . Then:\n\n\nCorollary 1. Let  $L \\subseteq \\Sigma^{+}$  be a regular language in  $\\mathsf{AC}^0$ . There is an UHAT that accepts  $L$ .\n\n\nRecognizing regular languages up to letter-permutation. Although not all regular languages are accepted by UHATs (e.g. parity), we can use Theorem 1 to show that, up to letter-permutation, UHAT is in fact strictly more powerful than regular and context-free languages.\n\n\nTo formalize our result, we recall the notion of semilinear sets and the Parikh image of a language. A linear set  $S$  is a subset of  $\\mathbb{N}^d$  (for some positive integer  $d$ , called dimension) of the form  $\\mathbf{v}_0 + \\sum_{i=1}^{r} \\mathbf{v}_i \\mathbb{N} := \\{\\mathbf{v}_0 + \\sum_{i=1}^{r} k_i \\mathbf{v}_i : k_1, \\ldots, k_r \\in \\mathbb{N}\\}$  for some vectors  $\\mathbf{v}_0, \\ldots, \\mathbf{v}_r \\in \\mathbb{N}^d$ . A semilinear set  $S$  over  $\\mathbb{N}^d$  is a finite union of linear sets over  $\\mathbb{N}^d$ . Semilinear sets have a very tight connection to formal languages through the notion of the Parikh image a language  $L$  (Parikh, 1966), which intuitively corresponds to the set of \"letter-counts\" of  $L$ . More precisely, consider the alphabet  $\\Sigma = \\{a_1, \\ldots, a_d\\}$  and a language  $L$  over  $\\Sigma$ . For a word  $w \\in \\Sigma$ , let  $|w|_{a_i}$  denotes the number of occurrences of  $a_i$  in  $w$ . The Parikh image  $\\mathcal{P}(L)$  of  $L$  is defined to be the set of tuples  $\\mathbf{v} = (|w|_{a_1}, \\ldots, |w|_{a_d}) \\in \\mathbb{N}^d$  for some word  $w \\in L$ . For example, if  $L = \\{a^n b^n : n \\geq 0\\}$  and  $L' = (ab)^*$ , then  $\\mathcal{P}(L) = \\mathcal{P}(L')$ . In this case, we say that  $L$  and  $L'$  are Parikh-equivalent. Note that  $L'$  is regular, while  $L$  is context-free but not regular. This is not a coincidence based on the celebrated Parikh's Theorem (cf. Parikh (1966), also see Kozen (1997)).\n\n\nProposition 4 (Parikh (1966)). The Parikh images of both regular and context-free languages coincide with semilinear sets.\n\n\nIn other words, although context-free languages are strict superset of regular languages, they are in fact equally powerful up to letter-permutation. What about UHATs? We have that they are strictly more powerful than regular and context-free languages up to letter-permutation.\n\n\nProposition 5. Each regular language has a Parikh-equivalent language accepted by an UHAT. In turn, there is an UHAT language with no Parikh-equivalent regular language.\n\n\n# 4 L A N G U A G E S BEYOND AC\n\n\nTransformer encoders with unique hard attention can only recognize languages in  $\\mathsf{AC}^0$ , but a slight extension of the attention mechanism allows to recognize languages lying outside such a class (Hao et al., 2022). In this section, we show that in fact such an extended model can recognize all languages definable in a powerful logic that extends LTL with counting features. This logic can express interesting languages outside  $\\mathsf{AC}^0$ , such as majority and parity.\n\n\n# 4.1 AVERAGE HARD ATTENTION\n\n\nFor the results in this section, we consider an extended version of transformer encoders that utilize an average hard attention mechanism (P\u00e9rez et al., 2021; Hao et al., 2022). Following the literature, we call these AHAT.\n\n\nEncoder layer with average hard attention. As before, these layers are defined by two affine transformations,  $A,B\\colon \\mathbb{R}^d\\to \\mathbb{R}^d$  and one feed-forward neural network  $\\mathcal{N}\\colon \\mathbb{R}^{2d}\\to \\mathbb{R}^e$  with ReLU activation function. For every  $i\\in \\{0,\\dots ,n - 1\\}$ , we define  $S_{i}$  as the set of positions  $j\\in \\{0,\\dots ,n - 1\\}$  that maximize  $\\langle A\\mathbf{v}_i,B\\mathbf{v}_j\\rangle$ . We then set\n\n\n$$\n\n$$\n\\mathbf {a} _ {i} \\leftarrow \\left(\\sum_ {j \\in S _ {i}} \\mathbf {v} _ {j}\\right) / | S _ {i} |.\n$$\n\n$$\n\n\nAfter that, we set  $\\mathbf{v}_i^{\\prime}\\gets \\mathcal{N}(\\mathbf{v}_i,\\mathbf{a}_i)$ , for each  $i = 0,\\dots ,n - 1$ . That is, attention scores under average hard attention return the uniform average value among all positions that maximize attention.\n\n\nWe also use future positional masking that allows us to take into account only positions up to  $i$ . If the future positional masking is used, the sets  $S_{i}$  are defined as sets of positions  $j \\in \\{0,1,\\dots,i\\}$  that maximize  $\\langle A\\mathbf{v}_i,B\\mathbf{v}_j\\rangle$ . Positional masks have been employed on several occasions in theoretical papers (Yao et al., 2021; Bhattamishra et al., 2020; Hao et al., 2022) as well as in practice, for example, for training GPT-2 (Radford et al., 2019).\n\n\n# 4.2 LTL EXTENDED WITH COUNTING TERMS\n\n\nWe present here  $\\mathrm{LTL}(\\mathbf{C}, +)$ , an extension of  $\\mathrm{LTL}(\\mathbf{Mon})$  that allows us to define counting properties over words in a simple manner. This requires the introduction of counting terms as defined next.\n\n\nCounting terms. Suppose  $\\phi$  is a unary formula. Then  $\\overleftarrow{\\# \\phi}$  and  $\\overrightarrow{\\# \\phi}$  are counting terms. The interpretation of these terms in position  $i$  of a word  $\\bar{w}$  of length  $n$  is defined as follows:\n\n\n$$\n\n$$\n\\overleftarrow {\\# \\phi} (\\bar {w}, i) = | \\{j \\in \\{0, \\dots , i \\} \\mid (\\bar {w}, j) \\models \\phi \\} |, \\overrightarrow {\\# \\phi} (\\bar {w}, i) = | \\{j \\in \\{i, \\dots , n - 1 \\} \\mid (\\bar {w}, j) \\models \\phi \\} |.\n$$\n\n$$\n\n\nThat is,  $\\overleftarrow{\\# \\phi}(\\bar{w}, i)$  is the number of positions to the left of  $i$  (including  $i$ ) that satisfy  $\\phi$ , while  $\\overrightarrow{\\# \\phi}(\\bar{w}, i)$  is the number of positions to the right of  $i$  (including  $i$ ) that satisfy  $\\phi$ . Notice that, for words of length  $n$ , counting terms take values in  $\\{0, 1, \\ldots, n\\}$ .\n\n\nCounting formulas. With counting terms and unary numerical predicates we can create new formulas in the following way. Let  $\\phi$  be a unary formula and  $\\Theta$  a unary numerical predicate. We define new formulas  $\\Theta (\\overleftarrow{\\# \\phi})$  and  $\\Theta (\\overrightarrow{\\# \\phi})$ . The interpretation of such formulas on position  $i$  of a word  $\\bar{w}$  of length  $n$  is as follows:\n\n\n$$\n\n$$\n(\\bar {w}, i) \\models \\Theta (\\overleftarrow {\\# \\phi}) \\Leftrightarrow \\theta_ {n} (\\overleftarrow {\\# \\phi} (\\bar {w}, i)) = 1 \\qquad (\\bar {w}, i) \\models \\Theta (\\overrightarrow {\\# \\phi}) \\Leftrightarrow \\theta_ {n} (\\overrightarrow {\\# \\phi} (\\bar {w}, i)) = 1.\n$$\n\n$$\n\n\nThat is, the number of positions to the left (resp., right) of  $i$  (including  $i$ ) that satisfy  $\\phi$  satisfies the predicate  $\\Theta$ . As counting terms can take value  $n$ , the value of  $\\theta_{n}$  on  $n$  becomes useful.\n\n\nWe also incorporate into our logic the possibility of checking linear inequalities with integer coefficients over counting terms. More specifically, for any finite set of unary formulas  $\\phi_1,\\ldots ,\\phi_{k_j},\\psi_1,\\ldots ,\\psi_k$  , and for any coefficients  $c_{1},\\ldots ,c_{k},d_{1},\\ldots ,d_{k}\\in \\mathbb{Z}$  we can create a formula:  $\\sum_{j = 1}^{k}c_{j}\\cdot \\overline{\\# \\phi_{j}} +\\sum_{j = 1}^{k}d_{j}\\cdot \\overrightarrow{\\# \\psi_{j}}\\geq 0$  , which is interpreted as follows:\n\n\n$$\n\n$$\n(\\bar {w}, i) \\vDash \\sum_ {j = 1} ^ {k} c _ {j} \\cdot \\overleftarrow {\\# \\phi_ {j}} + \\sum_ {j = 1} ^ {k} d _ {j} \\cdot \\overrightarrow {\\# \\psi_ {j}} \\geq 0 \\iff \\sum_ {j = 1} ^ {k} c _ {j} \\cdot \\overleftarrow {\\# \\phi_ {j}} (\\bar {w}, i) + \\sum_ {j = 1} ^ {k} d _ {j} \\cdot \\overrightarrow {\\# \\psi_ {j}} (\\bar {w}, i) \\geq 0.\n$$\n\n$$\n\n\nThe logic  $\\mathrm{LTL}(\\mathbf{C}, +)$ . We denote by  $\\mathrm{LTL}(\\mathbf{C}, +)$  the logic that is recursively defined as follows:\n\n- - Every formula  $\\mathrm{LTL}(\\mathrm{Mon})$  is also an  $\\mathrm{LTL}(\\mathbf{C}, +)$  formula.\n\n- - Boolean combinations of  $\\mathrm{LTL}(\\mathbf{C}, +)$  formulas are  $\\mathrm{LTL}(\\mathbf{C}, +)$  formulas.\n\n- - If  $\\phi$  and  $\\psi$  are  $\\mathrm{LTL}(\\mathbf{C}, +)$  formulas, then so are  $\\mathbf{X}\\phi$  and  $\\phi \\mathbf{U}\\psi$ .\n\n- - If  $\\phi$  is an LTL $(\\mathbf{C}, +)$  formula and  $\\Theta$  is a unary numerical predicate, then  $\\Theta(\\overleftarrow{\\# \\phi})$  and  $\\Theta(\\overrightarrow{\\# \\phi})$  are LTL $(\\mathbf{C}, +)$  formulas.\n\n- - If  $\\phi_1, \\ldots, \\phi_k, \\psi_1, \\ldots, \\psi_k$  are formulas of  $\\mathrm{LTL}(\\mathbf{C}, +)$ , then  $\\sum_{j=1}^{k} c_j \\cdot \\overleftarrow{\\# \\phi_j} + \\sum_{j=1}^{k} d_j \\cdot \\overrightarrow{\\# \\psi_j} \\geq 0$ , is a formula of  $\\mathrm{LTL}(\\mathbf{C}, +)$ .\n\n\n# 4.3 LTL(C) DEFINABLE LANGUAGES ARE ACCEPTED BY ENCODERS\n\n\nNext, we state the main result of this section: languages definable by  $\\mathrm{LTL}(\\mathbf{C}, + )$  formulas are accepted by transformer encoders with average hard attention.\n\n\nTheorem 2. Let  $\\Sigma$  be a finite alphabet and  $\\phi$  an LTL $(\\mathbf{C}, +)$  formula defined over words from the alphabet  $\\Sigma$ . There is an AHAT  $T$  that accepts  $L(\\phi)$ .\n\n\nAs a corollary to Theorem 2, we show that AHATs are rather powerful in counting. To make this claim more formal, we study permutation-closed languages, i.e., languages  $L$  such that  $\\bar{v} \\in L$  iff any letter-permutation of  $\\bar{v}$  is in  $L$ . For a language  $L$ , we write  $\\text{perm}(L)$  to be the permutation-closure of  $L$ , i.e.,  $\\text{perm}(L) = \\{\\bar{w} : \\mathcal{P}(\\bar{w}) = \\mathcal{P}(\\bar{v}) \\text{ for some } \\bar{v} \\in L\\}$ . Observe that  $\\text{perm}((abc)^*)$  consists of all strings with the same number of occurrences of  $a, b,$  and  $c$ ; this is not even context-free. Owing to Parikh's Theorem, to recognize  $\\text{perm}(L)$ , where  $L$  is a regular language, an ability to perform letter-counting and linear arithmetic reasoning (i.e. semilinear set reasoning) is necessary. AHATs possess such an ability, as shown by the following corollary.\n\n\nCorollary 2. The permutation closure  $\\operatorname{perm}(L)$  of any regular language  $L$  is accepted by an AHAT. Moreover, any permutation-closed language over a binary alphabet is accepted by an AHAT.\n\n\nBoth majority and parity are permutation-closed and are over a binary alphabet. Hence, by the previous result, they are both accepted by AHATs. While for majority this was known (Hao et al., 2022), the result for parity is new.\n\n\n# 5 CONCLUSIONS AND FUTURE WORK\n\n\nWe have conducted an investigation of the problem of which languages can be accepted by transformer encoders with hard attention. For UHATs, we have demonstrated that while they cannot accept all languages in  $\\mathsf{AC}^0$ , they can still accept all languages in a 'monadic' version of it defined by the logic FO(Mon). Crucial to the proof of this result is the equivalence between FO and LTL, as provided by Kamp's Theorem. In turn, we have shown that AHATs are capable of expressing any language definable in a powerful counting logic,  $\\mathrm{LTL}(\\mathbf{C}, +)$ , that can express properties beyond  $\\mathsf{AC}^0$ . This implies, among other things, that the parity language can be accepted by an AHAT.\n\n\nIn a work of Angluin et al. (2023), contemporaneous to ours, it was shown that the logic FO(Mon) exactly captures languages, accepted by UHATs with positional masking and finite-valued positional encoding. At the same time, with our technique, it can be shown that there are languages beyond FO(Mon) that can be accepted by UHATs with infinite-valued positional encoding, for instance, the language of palindromes. The idea is that since we do not use positional masking, we can use arbitrary order on positions, in particular, one that puts the language of palindromes into FO(Mon). Due to the space constraints, we omit a more detailed discussion of this.\n\n\n# REFERENCES\n\n- Mikl\u00f3s Ajtai.  $\\sum_{1}^{1}$ -formulae on finite structures. Ann. Pure Appl. Log., 24(1):1-48, 1983.\n\n- Dana Angluin, David Chiang, and Andy Yang. Masked hard-attention transformers and boolean rasp recognize exactly the star-free languages, 2023.\n\n- Pablo Barcel\u00f3, Alexander Kozachinskiy, Anthony Widjaja Lin, and Vladimir Podolskii. Logical languages accepted by transformer encoders with hard attention. arXiv preprint arXiv:2310.03817, 2023.\n\n- David A. Mix Barrington, Kevin J. Compton, Howard Straubing, and Denis Th\u00e9rien. Regular languages in nc $^1$ . J. Comput. Syst. Sci., 44(3):478-499, 1992.\n\n- David A. Mix Barrington, Neil Immerman, Clemens Lautemann, Nicole Schweikardt, and Denis Th\u00e9rien. First-order expressibility of languages with neutral letters or: The crane beach conjecture. J. Comput. Syst. Sci., 70(2):101-127, 2005.\n\n- Satwik Bhattachamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to recognize formal languages. In EMNLP, pp. 7096-7116, 2020.\n\n- David Chiang and Peter Cholak. Overcoming a theoretical limitation of self-attention. In ACL, pp. 7654-7664, 2022.\n\n- David Chiang, Peter Cholak, and Anand Pillay. Tighter bounds on the expressivity of transformer encoders. In ICML, volume 202, pp. 5544-5562. PMLR, 2023.\n\n- Edmund M. Clarke, Orna Grumberg, Daniel Kroening, Doron A. Peled, and Helmut Veith. Model checking, 2nd Edition. MIT Press, 2018.\n\n- Merrick L. Furst, James B. Saxe, and Michael Sipser. Parity, circuits, and the polynomial-time hierarchy. In *FOCS*, pp. 260-270. IEEE Computer Society, 1981.\n\n- Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Trans. Assoc. Comput. Linguistics, 8:156-171, 2020.\n\n- Yiding Hao, Dana Angluin, and Robert Frank. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. Trans. Assoc. Comput. Linguistics, 10:800-810, 2022.\n\n- Neil Immerman. Descriptive complexity. Graduate texts in computer science. Springer, 1999.\n\n- H. W. Kamp. Tense Logic and the Theory of Linear Order. PhD thesis, University of California, Los Angeles, 1968.\n\n- Dexter C. Kozen. Automata and Computability. Springer, 1997.\n\n- William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers, 2023.\n\n- William Merrill, Ashish Sabharwal, and Noah A. Smith. Saturated transformers are constant-depth threshold circuits. Trans. Assoc. Comput. Linguistics, 10:843-856, 2022. URL https://transacl.org/ojs/index.php/tacl/article/view/3465.\n\n- Rohit Parikh. On context-free languages. J. ACM, 13(4):570-581, 1966. doi: 10.1145/321356.321364. URL https://doi.org/10.1145/321356.321364.\n\n- Jorge P\u00e9rez, Pablo Barcel\u00f3, and Javier Marinkovic. Attention is turing-complete. J. Mach. Learn. Res., 22:75:1-75:35, 2021.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\n- Lena Strobl. Average-hard attention transformers are constant-depth uniform threshold circuits, 2023.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998-6008, 2017.\n\n- Emanuele Viola. On approximate majority and probabilistic time. Computational Complexity, 18: 337-375, 2009.\n\n- Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In ICML, volume 139, pp. 11080-11090, 2021.\n\n- Shunyu Yao, Binghui Peng, Christos H. Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. In ACLIJCNLP, pp. 3770-3785, 2021.\n\nResponse: ### Prediction: \\(\\boxed{Accept}\\)\n\n#### Justification:\n\n**Originality:**\n- The paper introduces a novel approach to understanding the expressive power of transformer encoders with respect to formal languages. Specifically, it investigates the limits and capabilities of UHAT (Unique Hard Attention Transformers) and AHAT (Average Hard Attention Transformers) in recognizing formal languages.\n- The authors provide a detailed analysis of the expressive power of these transformer variants, particularly focusing on the class \\(\\mathsf{AC}^0\\) and its extensions. This originality lies in exploring the boundary conditions and capabilities of these attention mechanisms.\n- The contributions include proving that not all \\(\\mathsf{AC}^0\\) languages are recognized by UHATs, identifying a rich fragment of \\(\\mathsf{AC}^0\\) that can be recognized by UHATs, and demonstrating that AHATs can recognize all languages definable in a more expressive logic (LTL(C)+).\n\n**Methodology Rigor:**\n- The methodology is rigorous and well-documented. The authors provide clear definitions and constructions of UHAT and AHAT, along with detailed proofs for their properties.\n- They leverage known results from circuit complexity theory (like Ajtai's work) and logical frameworks (like FO and LTL) to derive their results. The proofs are carefully constructed and presented.\n- The authors also provide empirical insights through examples and corollaries, adding practical relevance to their theoretical findings.\n\n**Significance:**\n- The results are significant as they bridge the gap between computational complexity theory and transformer architecture. Understanding the expressive power of these models helps in designing more efficient and targeted transformer-based systems.\n- The findings are relevant to both theoretical computer scientists and practitioners working with natural language processing (NLP) models, providing a deeper understanding of their capabilities and limitations.\n\n**Clarity:**\n- The paper is well-written and organized logically. Definitions and theorems are clearly stated, and the proofs are well-explained. The notation is consistent throughout the document.\n- The technical details are explained in a manner that is accessible to experts in the field while maintaining sufficient detail for thorough understanding.\n\n**Alignment with Field Standards:**\n- The paper adheres to the standard practices of presenting theoretical results in computational linguistics and theoretical computer science. It references relevant prior works and provides clear connections to existing knowledge.\n- The results are presented with appropriate citations and discussions of related work, ensuring that the paper fits within the broader context of the field.\n\n### Conclusion:\nThe paper presents a comprehensive analysis of the expressive power of UHAT and AHAT transformers, contributing valuable insights into the limitations and capabilities of these models. The theoretical foundations laid are solid, and the results are well-supported by rigorous proofs and examples. Given the novelty and depth of the analysis, the paper is likely to be accepted for presentation at ICLR, which emphasizes cutting-edge research in artificial intelligence and machine learning.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634834.8723378", "msecs": "872.0", "relativeCreated": "179695.81198692322", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers for acceptance. Assess based on originality, methodology rigor, significance, clarity, and alignment with field standards. Provide a structured prediction with confidence level (High/Medium/Low), detailing evidence-based rationale. Systematically weigh strengths and weaknesses, ensuring objectivity and adherence to peer review principles.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Supervised Contrastive Regression\n\n# Abstract\nDeep regression models typically learn in an end-to-end fashion and do not explicitly try to learn a regression-aware representation. Their representations tend to be fragmented and fail to capture the continuous nature of regression tasks. In this paper, we propose Supervised Contrastive Regression (SupCR), a framework that learns a regression-aware representation by contrasting samples against each other based on their target distance. SupCR is orthogonal to existing regression models, and can be used in combination with such models to improve performance. Extensive experiments using five real-world regression datasets that span computer vision, human-computer interaction, and healthcare show that using SupCR achieves the state-of-the-art performance and consistently improves prior regression baselines on all datasets, tasks, and input modalities. SupCR also improves robustness to data corruptions, resilience to reduced training data, performance on transfer learning, and generalization to unseen targets.\n\n\n# 1 INTRODUCTION\n\n\nRegression problems are ubiquitous and fundamental in the real world. They include estimating age from human appearance (Rothe et al., 2015), predicting health scores from physiological signals (Engemann et al., 2022), and detecting gaze directions from webcam images (Zhang et al., 2017b). Since regression targets are continuous, the most widely used approach to train a regression model is to have the model predict the target value, and use the distance (e.g., L1 or L2 distance) between the prediction and the ground-truth target as the loss function (Zhang et al., 2017a;b; Schrumpf et al., 2021; Engemann et al., 2022). There are also works that control the relationship between predictions and targets by converting the regression task into a classification task and training the model with the cross-entropy loss (Rothe et al., 2015; Niu et al., 2016; Shi et al., 2021).\n\n\nHowever, all previous methods focus on imposing constraints on the final predictions in an end-to-end fashion, but do not explicitly consider the representations learned by the model. Their representations tend to be fragmented and fail to capture the continuous relationships underlying regression tasks. For example, Figure 1(a) shows the representation learned by the L1 loss in the task of predicting weather temperature from webcam outdoor images (Chu et al., 2018), where the images are captured by 44 outdoor webcams at different locations. The representation learned by the L1 model does not exhibit the continuous ground-truth temperatures; rather it is grouped by different webcams in a fragmented manner. Such unordered and fragmented representation is sub-optimal for the regression task and can even hamper performance since it contains distracting information (e.g., the capturing webcam).\n\n\nWhile there is a rich literature on representation learning, past methods are focused on classification problems. In particular, contrastive learning (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b) and supervised contrastive learning (SupCon) (Khosla et al., 2020) have been proven highly effective in representation learning. However, as shown in Figure 1(b), which plots the representation learned by SupCon for the visual temperature prediction task mentioned above, such method produces a sub-optimal representation for regression problems because it ignores the continuous order between the samples in a regression task. Besides, there are several recent works (Wang et al., 2022; Dufumier et al., 2021a;b; Schneider et al., 2022) adopting contrastive learning in the context of continuous labels, but they are not doing regression learning tasks.\n\n\nIn this paper, we introduce Supervised Contrastive Regression (SupCR), a new framework for deep regression learning, where we first learn a representation that ensures the distances in the embedding\n\n\n![(a) L1](images/a81b8e39f8bb941eecb45768e99bcd7ca157b504cd225ca2a6c96df5b05fb79a.jpg)\n\n\n![(b) SupCon Figure 1: UMAP (McInnes et al., 2018) visualization of the representation learned by L1, SupCon (Khosla et al., 2020), and the proposed SupCR for the task of predicting the temperature from webcam outdoor images (Chu et al., 2018). The representation of each image is visualized as a dot and the color indicates the ground-truth temperature. SupCR can learn a representation that captures the intrinsic ordered relationships between the samples while L1 and SupCon fail to do so.](images/f0c876486cd367d6f48e8d5c211b57b9dfccaaae76d0620764b37a98db8ebb8a.jpg)\n\n\n![(c) SupCR](images/34cda3a85be98e06ec8959b65694db7d93346d8ed71140f782fc9724dbec4a8c.jpg)\n\n\nspace are ordered according to the target values. We then use this representation to predict the targets. To learn such a regression-aware representation, we contrast samples against each other based on their label/target distances. Our method explicitly leverages the ordered relationships between the samples to optimize the representation for the downstream regression task. As shown in Figure 1(c), SupCR leads to a representation that captures the intrinsic ordered relationships between the samples. Moreover, our framework is orthogonal to existing regression methods as one can use any kind of regression method to map our learned representation to the prediction values.\n\n\nExtensive experiments using five real-world regression datasets that span computer vision, human-computer interaction, and healthcare verify the superior performance of our framework. The results show that SupCR achieves the state-of-the-art performance and consistently improves the performance of previous regression methods by  $8.7\\%$  on average across all datasets, tasks, and input modalities. Furthermore, our experimental results also show that SupCR delivers multiple desirable properties: 1) improved robustness to data corruptions; 2) enhanced resilience to reduced training data; 3) higher performance on transfer learning; and 4) better generalization to unseen targets.\n\n\n# 2 RELATED WORK\n\n\nRegression Learning. Deep learning has achieved a great success in addressing regression tasks (Rothe et al., 2015; Zhang et al., 2017a;b; Schrumpf et al., 2021; Engemann et al., 2022). The most straightforward and widely used way to train a regression model is to have the model predict the target value, and use the distance between the prediction and the ground-truth target as the loss function. Common distance metrics used as the loss function in regression tasks are the L1 loss, the MSE loss and the Huber loss (Huber, 1992).\n\n\nPast work has also proposed variants to the basic methods described above. One branch of prior works (Rothe et al., 2015; Gao et al., 2017; 2018; Pan et al., 2018) divide the regression range into small bins to convert the problem into a classification problem, and use classification loss, e.g., the cross-entropy loss, to train the model. Another branch of prior work (Niu et al., 2016; Fu et al., 2018; Cao et al., 2020; Shi et al., 2021) casts regression as an ordinal classification problem. They design multiple ordered thresholds and use multiple binary classifiers, one for each threshold, to learn whether the target is larger than each threshold.\n\n\nPast work however focuses on forcing the final predictions of the model to be close to the target and does so in an end-to-end manner. In contrast, this paper focuses on ordering the samples in the embedding space in accordance with the label order, and it does so by first learning a representation that contrasts samples against each other, and then learning a predictor to map the representation to the prediction values. Therefore, our method is orthogonal to prior methods for regression learning as those prior methods can be applied to training our predictor and learning the final predictions with the help of our ordered representation.\n\n\nContrastive Learning. Contrastive learning, which aligns positive pairs and repulses negative pairs in the representation space, has demonstrated improved performance for self-supervised learn\n\n\ning (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Chuang et al., 2020). Its supervised version, named supervised contrastive learning (SupCon) (Khosla et al., 2020), which defines samples from the same class as positive pairs and samples from different classes as negative pairs, was also shown to outperform the cross-entropy loss on image classification tasks (Khosla et al., 2020). It is also beneficial for learning in challenging settings such as noisy labels (Li et al., 2022a), long-tailed classification (Kang et al., 2020; Li et al., 2022b) and out-of-domain detection (Zeng et al., 2021).\n\n\nA couple of recent papers have used the concept of \"contrastive\" or contrastive learning in the context of continuous labels. In particular, Yu et al. (2021) learns a model for action quality assessment by regressing the relative scores between two input videos. This work differs from ours in that it does not use the standard contrastive learning framework. Wang et al. (2022) proposes to improve domain adaptation for gaze estimation by adding a contrastive loss term to the L1 loss. They show that their approach is beneficial for adapting a gaze estimation model from one dataset (i.e., one domain) to another, but the approach produces no benefits and even reduces performance for the source dataset. In contrast, our approach improves performance for the source dataset as opposed to being specific to domain adaptation. Besides, Dufumier et al. (2021a;b) use a contrastive loss-weighted by continuous meta-data for classification. Schneider et al. (2022) learns low-dimensional and interpretable embeddings to encode behavioral and neural data using a generalized contrastive loss which samples positives and negatives according to continuous behavior or time labels. However, the continuous meta-data in (Dufumier et al., 2021a;b) and the continuous labels in (Schneider et al., 2022) are not the targets in their tasks and they are not working on regression problems.\n\n\n# 3 METHOD\n\n\nFor a regression task, we aim to train a neural network composed of a feature encoder  $f(\\cdot): X \\to \\mathbb{R}^{d_e}$  and a predictor  $p(\\cdot): \\mathbb{R}^{d_e} \\to \\mathbb{R}^{d_t}$  to predict the target  $\\pmb{y} \\in \\mathbb{R}^{d_t}$  based on the input data  $\\pmb{x} \\in X$ .\n\n\nGiven an input batch of data, similar to contrastive learning methods (Chen et al., 2020a; Khosla et al., 2020), we first apply data augmentation twice to obtain two views of the batch. The two views are fed into the encoder  $f(\\cdot)$  to obtain a  $d_{e}$ -dimensional feature embedding for each augmented input data. Our supervised contrastive regression loss,  $\\mathcal{L}_{\\mathrm{supCR}}$ , is computed on the feature embeddings. To use the learned representation for regression, we freeze the encoder  $f(\\cdot)$  and train the predictor on top of it using a regression loss (e.g., L1 loss).\n\n\n# 3.1 SUPERVISED CONTRASTIVE REGRESSION LOSS\n\n\nWe would like our loss function to ensure distances in the embedding space are ordered according to distances in the label space. But, how can we design a contrastive loss that delivers this property? Below, we first define our loss function, then explain it in the context of positive and negative contrastive samples, and finally provide a theoretical analysis to prove that as our loss approaches its minimum, the learned representation will be ordered according to label distances.\n\n\nFor a positive integer  $I$ , let  $[I]$  denote the set  $\\{1,2,\\dots,I\\}$ . Given a randomly sampled batch of  $N$  input and label pairs  $\\{(\\pmb{x}_n,\\pmb{y}_n)\\}_{n\\in [N]}$ , we apply data augmentation to the batch and obtain a two-view batch  $\\{(\\tilde{\\pmb{x}}_{\\ell},\\tilde{\\pmb{y}}_{\\ell})\\}_{\\ell \\in [2N]}$ , where  $\\tilde{\\pmb{x}}_{2n} = t(\\pmb {x}_n)$ ,  $\\tilde{\\pmb{x}}_{2n - 1} = t^{\\prime}(\\pmb {x}_n)$  ( $t$  and  $t^\\prime$  are independently sampled data augmentations operations) and  $\\tilde{\\pmb{y}}_{2n} = \\tilde{\\pmb{y}}_{2n - 1} = \\pmb {y}_n$ ,  $\\forall n\\in [N]$ . The augmented batch is fed into the encoder  $f(\\cdot)$  to obtain  $\\pmb{v}_l = f(\\tilde{\\pmb{x}}_l)\\in \\mathbb{R}^{d_e},\\forall l\\in [2N]$ . Our supervised contrastive regression loss is then computed over the feature embeddings  $\\{\\pmb {v}_l\\}_{l\\in [2N]}$  as:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\operatorname {S u p C R}} = - \\frac {1}{2 N} \\sum_ {i = 1} ^ {2 N} \\frac {1}{2 N - 1} \\sum_ {j = 1, j \\neq i} ^ {2 N} \\log \\frac {\\exp (\\sin (\\boldsymbol {v} _ {i} , \\boldsymbol {v} _ {j}) / \\tau)}{\\sum_ {k = 1} ^ {2 N} \\mathbb {1} _ {[ k \\neq i , d (\\tilde {\\boldsymbol {y}} _ {i} , \\tilde {\\boldsymbol {y}} _ {k}) \\geq d (\\tilde {\\boldsymbol {y}} _ {i} , \\tilde {\\boldsymbol {y}} _ {j}) ]} \\exp (\\sin (\\boldsymbol {v} _ {i}, \\boldsymbol {v} _ {k}) / \\tau)}, \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\mathrm{sim}(\\cdot ,\\cdot)$  is the function measuring the similarity between two feature embeddings (e.g., negative L2 norm),  $d(\\cdot ,\\cdot)$  is the function measuring the distance between two labels (e.g., L1 distance),  $\\mathbb{1}_{[\\cdot ]}\\in \\{0,1\\}$  is an indicator function evaluating to 1 iff the conditions in the square brackets are satisfied, and  $\\tau$  is the temperature parameter.\n\n\nAt a high level, our loss can be explained in the context of positive/negative pairs, commonly used in contrastive learning. Contrastive learning and SupCon focus on classification. Hence, positive\n\n\n![(a)](images/60018adbd6e1d0020e76f433acba9eaac3e92b6aaa30b594dcc8ed2921111120.jpg)\n\n\n![(b) Figure 2: Illustration of  $\\mathcal{L}_{\\mathrm{supCR}}$ . (a) An example batch of input data and their age labels. (b) Two example positive pairs and the corresponding negative pair(s) when the anchor is the 20-year old man (shown in gray shading). When the 30-year old man creates a positive pair with the anchor, their label distance is 10, so the corresponding negative samples are the 0-year old baby and the 70-year old man, whose label distances to the anchor are 20 and 50 respectively. When the 0-year old baby creates a positive pair with the anchor, their label distance is 20. Only one sample in the batch has a larger label distance to the anchor, namely the 70-year old man, and it acts as a negative sample.](images/1c1edba30507604bf56e08aea699ea87c28a203428b6c892b1676492727d546e.jpg)\n\n\nsamples are those that come from the same class or the same input image, and all other samples are negative samples. In regression problems, there are no classes but continuous labels. Any two samples can be thought of as a positive pair or a negative pair, depending on context. To exploit the inherent continuity underlying the labels, we define positive and negative samples in a relative way, based on their label distance to the anchor. In particular, we rank samples with respect to their label distance to the anchor. We then contrast the anchor to the sample in the batch that is closest in the label space, which will be treated as the positive sample. All other samples are farther from the anchor and should be negative samples. We also contrast the anchor to the second closest sample in the batch. In this case, only samples in the batch that are farther from the positive sample (i.e., samples whose ranks is three or higher) can act as negative samples. We continue this process for higher rank samples (the third closest, fourth closest, etc.), and all anchors in a batch.\n\n\nFigure 2(a) shows an example batch, and Figure 2(b) shows two positive pairs and their corresponding negative pair(s). Specifically, for any anchor sample  $i$ , any other sample  $j$  in the same batch can be used to create a positive pair with corresponding negative samples set to all samples in the batch whose labels differ from  $i$ 's label by more than the label of  $j$ . In other words,  $\\mathcal{L}_{\\mathrm{SupCR}}$  aims to force the feature similarity between  $i$  and  $j$  greater than the feature similarity between  $i$  and any other sample  $k$  in the batch if the label distance between  $i$  and  $k$  is greater than  $i$  and  $j$ . Therefore, optimizing  $\\mathcal{L}_{\\mathrm{SupCR}}$  will make the feature embedding ordered according to the order in the label space. The design of  $\\mathcal{L}_{\\mathrm{SupCR}}$  sufficiently leverages the ordered relationships between samples reflected in the continuous label space of regression problems, which is also the intrinsic difference between regression problems and classification problems.\n\n\n# 3.2 THEORETICAL ANALYSIS\n\n\nIn this section, we analytically show that optimizing  $\\mathcal{L}_{\\mathrm{SupCR}}$  will make the feature embedding ordered according to the order in the label space. To show this, we first derive a lower bound of  $\\mathcal{L}_{\\mathrm{SupCR}}$ , and show  $\\mathcal{L}_{\\mathrm{SupCR}}$  can be arbitrarily close to it. Next, we formalize the concept of  $\\delta$ -ordering which refers to the feature embeddings being ordered according to the order in the label space, and show that as  $\\mathcal{L}_{\\mathrm{SupCR}}$  gets sufficiently close to the lower bound, the feature embeddings will be  $\\delta$ -ordered. All proofs are in Appendix A.\n\n\nNotations. Let  $s_{i,j} \\coloneqq \\mathrm{sim}(\\pmb{v}_i, \\pmb{v}_j) / \\tau, \\forall i, j \\in [2N]$ ,  $d_{i,j} \\coloneqq d(\\tilde{\\pmb{y}}_i, \\tilde{\\pmb{y}}_j)$ ,  $\\forall i, j \\in [2N]$ ,  $D_{i,1} < D_{i,2} < \\dots < D_{i,M_i}$  be the sorted label distances starting from the  $i$ -th sample, i.e., sort  $(\\{d_{i,j} | j \\in [2N] \\setminus \\{i\\}\\}), \\forall i \\in [2N]$ , and  $n_{i,m} \\coloneqq |\\{j | d_{i,j} = D_{i,m}, j \\in [2N] \\setminus \\{i\\}\\}|$  be the number of samples whose distance from the  $i$ -th sample equals  $D_{i,m}, \\forall i \\in [2N], m \\in [M_i]$ .\n\n\nFirst, we show that  $L^{\\star} \\coloneqq \\frac{1}{2N(2N - 1)}\\sum_{i = 1}^{2N}\\sum_{m = 1}^{M_i}n_{i,m}\\log n_{i,m}$  is a lower bound of  $\\mathcal{L}_{\\mathrm{SupCR}}$ .\n\n\nTheorem 1 (Lower bound of  $\\mathcal{L}_{\\mathrm{supCR}}$ ).  $L^{\\star}$  is a lower bound of  $\\mathcal{L}_{\\mathrm{supCR}}$ , i.e.,  $\\mathcal{L}_{\\mathrm{supCR}} > L^{\\star}$ .\n\n\nNext, we show that  $\\mathcal{L}_{\\mathrm{SupCR}}$  can be arbitrarily close to its lower bound  $L^{\\star}$ .\n\n\nTheorem 2 (Lower bound tightness). For any  $\\epsilon >0$ , there exists a set of feature embeddings such that  $\\mathcal{L}_{\\mathrm{supCR}} < L^{\\star} + \\epsilon$ .\n\n\nThen, we define a property called  $\\delta$ -ordered for the feature embeddings  $\\{\\pmb{v}_l\\}_{l \\in [2N]}$ .\n\n\nDefinition 1 ( $\\delta$ -ordered feature embeddings). For any  $0 < \\delta < 1$ , the feature embeddings  $\\{\\pmb{v}_l\\}_{l \\in [2N]}$  are  $\\delta$ -ordered if  $\\forall i \\in [2N], j, k \\in [2N] \\setminus \\{i\\}$ ,\n\n\n$$\n\n$$\n\\left\\{ \\begin{array}{l} s _ {i, j} > s _ {i, k} + \\frac {1}{\\delta} i f d _ {i, j} <   d _ {i, k} \\\\ | s _ {i, j} - s _ {i, k} | <   \\delta i f d _ {i, j} = d _ {i, k} \\\\ s _ {i, j} <   s _ {i, k} - \\frac {1}{\\delta} i f d _ {i, j} > d _ {i, k} \\end{array} \\right..\n$$\n\n$$\n\n\nIt says that a  $\\delta$ -ordered set of feature embeddings satisfies the following properties: First, for all  $j$  and  $k$  such that  $d_{i,j} = d_{i,k}$ , the gap between  $s_{i,j}$  and  $s_{i,k}$  should be smaller than  $\\delta$ ; Second, for all  $j$  and  $k$  such that  $d_{i,j} < d_{i,k}$ ,  $s_{i,j}$  should be larger than  $s_{i,k}$  by at least  $\\frac{1}{\\delta}$ . Notice that  $\\frac{1}{\\delta} > \\delta$ , which means the feature similarity gap between a sample pair with different label distance to the anchor is always larger than the feature similarity gap between a sample pair with equal label distance to the anchor.\n\n\nFinally, we show that for any  $0 < \\delta < 1$ , when  $\\mathcal{L}_{\\mathrm{supCR}}$  is close enough to its lower bound  $L^{\\star}$ , the feature embeddings will be  $\\delta$ -ordered.\n\n\nTheorem 3 (Main theorem). For any  $0 < \\delta < 1$ , there exist  $\\epsilon > 0$ , such that if  $\\mathcal{L}_{\\mathrm{sup}CR} < L^{\\star} + \\epsilon$ , then the feature embeddings are  $\\delta$ -ordered.\n\n\n# 4 EXPERIMENTS\n\n\nIn this section, we evaluate our proposed method empirically. We first benchmark our method and compare it with the state-of-the-art regression baselines. Then, we evaluate the desirable properties of our learned representations, including the robustness to data corruptions, the resilience to reduced training data, the performance on transfer learning, and the generalization ability to unseen targets. Finally, we analyze and evaluate the design variants for our method. Refer to Appendix E for additional experiments and analysis.\n\n\nDatasets. We benchmark our method on five regression datasets for common real-world tasks in computer vision, human-computer interaction, and healthcare. Ethics statements of the datasets are included in Appendix B.\n\n- 1) AgeDB (Moschoglou et al., 2017) is used for predicting age from face images. It contains 16,488 in-the-wild images of celebrities and the corresponding age labels. The age range is between 0 and 101. It is split into a 12,208-image training set, a 2140-image validation set and a 2140-image test set.\n\n- 2) TUAB (Obeid & Picone, 2016; Engemann et al., 2022) is used for brain-age estimation from EEG resting-state signals. The dataset comes from EEG exams at the Temple University Hospital in Philadelphia. Following Engemann et al. (2022), we use only the non-pathological subjects, so that we may consider their chronological age as their brain-age label. The dataset includes 1,385 21-channel EEG signals sampled at  $200\\mathrm{Hz}$  from individuals whose age ranges from 0 to 95. It is split into a 1,246-subject training set and a 139-subject test set.\n\n- 3) MPIIFaceGaze (Zhang et al., 2017a;b) is used for gaze direction estimation from face images. It contains 213,659 face images collected from 15 participants during their natural everyday laptop use. We subsample and split it into a 33,000-image training set, a 6,000-image validation set and a 6000-image test set with no overlapping participants. The gaze direction is described as a 2-dimensional vector with the pitch angle in the first dimension and the yaw angle in the second dimension. The range of the pitch angle is  $-40^{\\circ}$  to  $10^{\\circ}$  and the range of the yaw angle is  $-45^{\\circ}$  to  $45^{\\circ}$ .\n\n- 4) SkyFinder (Mihail et al., 2016; Chu et al., 2018) is used for temperature prediction from outdoor webcam images. It contains 35,417 images captured by 44 cameras around 11am on each day under a wide range of weather and illumination conditions. The temperature range is  $-20^{\\circ}\\mathrm{C}$  to  $49^{\\circ}\\mathrm{C}$ . It is split into a 28,373-image training set, a 3,522-image validation set and a 3,522-image test set.\n\n\nTable 1: Evaluation results on AgeDB.\n\n\n<table><tr><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td></tr><tr><td>L1</td><td>6.63</td><td>0.828</td><td>SUPCR(L1)</td><td>6.14 (+0.49)</td><td>0.850 (+0.022)</td></tr><tr><td>MSE</td><td>6.57</td><td>0.828</td><td>SUPCR(MSE)</td><td>6.19 (+0.38)</td><td>0.849 (+0.021)</td></tr><tr><td>HUBER</td><td>6.54</td><td>0.828</td><td>SUPCR(HUBER)</td><td>6.15 (+0.39)</td><td>0.850 (+0.022)</td></tr><tr><td>DEX (Rothe et al., 2015)</td><td>7.29</td><td>0.787</td><td>SUPCR(DEX)</td><td>6.43 (+0.86)</td><td>0.836 (+0.049)</td></tr><tr><td>DLDL-v2 (Gao et al., 2018)</td><td>6.60</td><td>0.827</td><td>SUPCR(DLDL-v2)</td><td>6.32 (+0.28)</td><td>0.844 (+0.017)</td></tr><tr><td>OR (Niu et al., 2016)</td><td>6.40</td><td>0.830</td><td>SUPCR(OR)</td><td>6.34 (+0.06)</td><td>0.843 (+0.013)</td></tr><tr><td>CORN (Shi et al., 2021)</td><td>6.72</td><td>0.811</td><td>SUPCR(CORN)</td><td>6.44 (+0.28)</td><td>0.838 (+0.027)</td></tr></table>\n\n\nTable 2: Evaluation results on TUAB.\n\n\n<table><tr><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td></tr><tr><td>L1</td><td>7.96</td><td>0.599</td><td>SUPCR(L1)</td><td>6.97 (+0.99)</td><td>0.697 (+0.098)</td></tr><tr><td>MSE</td><td>8.06</td><td>0.585</td><td>SUPCR(MSE)</td><td>7.05 (+1.01)</td><td>0.692 (+0.107)</td></tr><tr><td>HUBER</td><td>7.59</td><td>0.637</td><td>SUPCR(HUBER)</td><td>6.99 (+0.60)</td><td>0.696 (+0.059)</td></tr><tr><td>DEX (Rothe et al., 2015)</td><td>8.01</td><td>0.537</td><td>SUPCR(DEX)</td><td>7.23 (+0.78)</td><td>0.646 (+0.109)</td></tr><tr><td>DLDL-v2 (Gao et al., 2018)</td><td>7.91</td><td>0.560</td><td>SUPCR(DLDL-v2)</td><td>6.91 (+1.00)</td><td>0.697 (+0.137)</td></tr><tr><td>OR (Niu et al., 2016)</td><td>7.36</td><td>0.646</td><td>SUPCR(OR)</td><td>7.01 (+0.35)</td><td>0.688 (+0.042)</td></tr><tr><td>CORN (Shi et al., 2021)</td><td>8.11</td><td>0.597</td><td>SUPCR(CORN)</td><td>7.22 (+0.89)</td><td>0.663 (+0.066)</td></tr></table>\n\n\nTable 3: Evaluation results on MPIIFaceGaze.\n\n\n<table><tr><td>Method</td><td>Angular\u2193</td><td>R2\u2191</td><td>Method</td><td>Angular\u2193</td><td>R2\u2191</td></tr><tr><td>L1</td><td>5.97</td><td>0.744</td><td>SUPCR(L1)</td><td>5.27 (+0.70)</td><td>0.815 (+0.071)</td></tr><tr><td>MSE</td><td>6.02</td><td>0.747</td><td>SUPCR(MSE)</td><td>5.35 (+0.67)</td><td>0.802 (+0.055)</td></tr><tr><td>HUBER</td><td>6.34</td><td>0.709</td><td>SUPCR(HUBER)</td><td>5.15 (+1.19)</td><td>0.830 (+0.121)</td></tr><tr><td>DEX (Rothe et al., 2015)</td><td>5.72</td><td>0.776</td><td>SUPCR(DEX)</td><td>5.14 (+0.58)</td><td>0.805 (+0.029)</td></tr><tr><td>DLDL-v2 (Gao et al., 2018)</td><td>5.47</td><td>0.799</td><td>SUPCR(DLDL-v2)</td><td>5.16 (+0.31)</td><td>0.802 (+0.003)</td></tr><tr><td>OR (Niu et al., 2016)</td><td>5.86</td><td>0.770</td><td>SUPCR(OR)</td><td>5.13 (+0.73)</td><td>0.825 (+0.055)</td></tr><tr><td>CORN (Shi et al., 2021)</td><td>5.88</td><td>0.762</td><td>SUPCR(CORN)</td><td>5.18 (+0.70)</td><td>0.820 (+0.058)</td></tr></table>\n\n\nTable 4: Evaluation results on SkyFinder.\n\n\n<table><tr><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td></tr><tr><td>L1</td><td>2.95</td><td>0.860</td><td>SUPCR(L1)</td><td>2.86 (+0.09)</td><td>0.869 (+0.009)</td></tr><tr><td>MSE</td><td>3.08</td><td>0.851</td><td>SUPCR(MSE)</td><td>2.86 (+0.22)</td><td>0.869 (+0.018)</td></tr><tr><td>HUBER</td><td>2.92</td><td>0.860</td><td>SUPCR(HUBER)</td><td>2.86 (+0.06)</td><td>0.869 (+0.009)</td></tr><tr><td>DEX (Rothe et al., 2015)</td><td>3.58</td><td>0.778</td><td>SUPCR(DEX)</td><td>2.88 (+0.70)</td><td>0.865 (+0.087)</td></tr><tr><td>DLDL-v2 (Gao et al., 2018)</td><td>2.99</td><td>0.856</td><td>SUPCR(DLDL-v2)</td><td>2.85 (+0.14)</td><td>0.869 (+0.013)</td></tr><tr><td>OR (Niu et al., 2016)</td><td>2.92</td><td>0.861</td><td>SUPCR(OR)</td><td>2.86 (+0.06)</td><td>0.867 (+0.006)</td></tr><tr><td>CORN (Shi et al., 2021)</td><td>3.24</td><td>0.819</td><td>SUPCR(CORN)</td><td>2.89 (+0.35)</td><td>0.862 (+0.043)</td></tr></table>\n\n\n5) IMDB-WIKI (Rothe et al., 2015) is a large dataset for predicting age from face images, which contains 523,051 celebrities images and the corresponding age labels. The age range is between 0 and 186 (some images are unlabeled). We use this dataset to test our method's resilience to reduced training data, performance on transfer learning, and ability to generalize to unseen targets. We subsample the dataset to create a variable size training set, and keep the validation set and test set unchanged with 11,022 images in each.\n\n\nMetrics. We report two kinds of metrics: the prediction error and coefficient of determination  $(\\mathbb{R}^2)$ . Prediction errors have practical meaning and are convenient for interpretation, while  $\\mathbb{R}^2$  quantifies how much the model outperforms a dummy regressor that always predicts the mean value of the training labels. For age, brain-age, and temperature, the mean absolute error (MAE) is reported as the prediction error. For gaze direction, the angular error is reported as the prediction error.\n\n\nBaselines. We implemented seven typical regression methods as baselines. L1, MSE and HUBER have the model directly predict the target value and train the model with an error-based loss function, where L1 uses the mean absolute error, MSE uses the mean squared error and HUBER uses an MSE\n\n\nterm when the error is below a threshold and an L1 term otherwise. DEX (Rothe et al., 2015) and DLDL-v2 (Gao et al., 2018) divide the regression range of each label dimension into several bins and learn the probability distribution over the bins. DEX (Rothe et al., 2015) optimizes a cross-entropy loss between the predicted distribution and the one-hot ground-truth labels, while DLDL-v2 (Gao et al., 2018) jointly optimizes a KL loss between the predicted distribution and a normal distribution centered at the ground-truth value, as well as an L1 loss between the expectation of the predicted distribution and the ground-truth value. During inference, they output the expectation of the predicted distribution for each label dimension. OR (Niu et al., 2016) and CORN (Shi et al., 2021) design multiple ordered threshold for each label dimension, and learn a binary classifier for each threshold. OR (Niu et al., 2016) optimizes a binary cross-entropy loss for each binary classifier to learn whether the target value is larger than each threshold, while CORN (Shi et al., 2021) learns whether the target value is larger than each threshold conditioning on it is larger than the previous threshold. During inference, they aggregate all binary classification results to produce the final results.\n\n\nExperiment Settings. We use ResNet-18 (He et al., 2016) as the backbone model for AgeDB, IMDB-WIKI, MPIIFaceGaze and SkyFinder datasets. For TUAB, we use a 24-layer 1D ResNet (He et al., 2016) as the backbone model to process the EEG signals. We use the linear regressor as the predictor. We use the SGD optimizer and cosine learning rate annealing (Loshchilov & Hutter, 2016) for training. The batch size is set to 256. We pick the best learning rates and weight decays by grid search for both our method and the baselines. We train all the baselines and the encoder of our method for 400 epochs, and the linear regressor of our method for 100 epochs. The same data augmentations are adopted for all baselines and our method: random crop and resize (with random flip), color distortions for AgeDB, IMDB-WIKI, SkyFinder; random crop and resize (without random flip), color distortions for MPIIFaceGaze; random crop for TUAB. Appendix C shows augmentation examples for each dataset. Negative L2 norm, i.e.,  $\\mathrm{sim}(\\pmb {v}_i,\\pmb {v}_j) = -\\| \\pmb {v}_i - \\pmb {v}_j\\| _2$  is used as the feature similarity measure in  $\\mathcal{L}_{\\mathrm{supCR}}$ . L1 distance is used as the label distance measure in  $\\mathcal{L}_{\\mathrm{supCR}}$  for AgeDB, IMDB-WIKI, SkyFinder and TUAB, while angular distance is used as the label distance measure for MPIIFaceGaze. The temperature  $\\tau$  is set to 2.0. Refer to Appendix D for more details.\n\n\n# 4.1 MAIN RESULTS\n\n\nAs explained earlier, our model learns a regression-suitable representation that can be used by any of the baselines. Thus, in our comparison, we first train the encoder with the proposed  $\\mathcal{L}_{\\mathrm{SupCR}}$ , and then freeze the encoder and train a predictor on top of it using each of the baseline methods. We then compare the original baseline without our representation, to the same baseline with our representation. For example, L1 means training the encoder and predictor end-to-end with L1 loss and  $\\mathrm{SUPCR(L1)}$  means training the encoder with  $\\mathcal{L}_{\\mathrm{SupCR}}$  and then training the predictor with L1 loss.\n\n\nTable 1, Table 2, Table 3 and Table 4 show the evaluation results on AgeDB, TUAB, MPIIfaceGaze and SkyFinder, respectively. Green numbers highlight the performance gains brought by using our representation and the best numbers are shown in bold. As all tables indicate, SupCR consistently achieves the best performance on both metrics across all datasets. Moreover, incorporating SupCR to learn the representation consistently reduces the prediction error of all baselines by  $5.8\\%$ ,  $10.1\\%$ ,  $11.7\\%$  and  $7.0\\%$  on average on the AgeDB, TUAB, MPIIfaceGaze and SkyFinder, respectively.\n\n\n# 4.2 ROBUSTNESS TO DATA CORRUPTIONS\n\n\nIt is well known that deep neural networks are highly vulnerable to out of distribution data and different forms of corruptions (Hendrycks & Dietterich, 2019), such as noise, blur and color distortions. Using the corruption generation process in ImageNet-C benchmark (Hendrycks & Dietterich, 2019), we corrupt the AgeDB test set with 19 diverse corruption types under different severity levels. In Figure 3, we compare SupCR(L1) with L1 by testing the model (trained on clean AgeDB training set) on corrupted test set and reporting the average mean absolute error over all types of corruptions for each severity level. The re\n\n\n![Figure 3: SupCR is more robust to data corruptions. Figure shows MAE $\\downarrow$  as a function of corruption severity.](images/61489af599e6b6fc1fca95443058ff0943da2fb314257c56f4a3cd6d471f221f.jpg)\n\n\nsults show that the representation learned by SupCR is more robust to unforeseen data corruptions, and SupCR suffers from lesser performance degradation as corruption severity increases.\n\n\n# 4.3 RESILIENCE TO REDUCED TRAINING DATA\n\n\nMuch of the success of modern deep learning is due to the availability of massive training datasets. However, in many real-world settings, collecting a large training set is often infeasible due to the time and cost of labeling. Thus, it is desirable to improve model resilience to reduced training data. In Figure 4, we subsample IMDB-WIKI to produce varying size training sets, and compare SUPCR(L1) with L1. The results show that SUPCR is more resilient to reduced training data, and exhibits lesser performance degradation as the number of training samples decreases.\n\n\n# 4.4 TRANSFER LEARNING\n\n\nWe evaluate the transfer learning performance by first pre-training the feature encoder on a large dataset and then using either linear probing (fixed encoder) or fine-tuning to learn a predictor on a small dataset. We investigate two scenarios: transferring from AgeDB which contains  $\\sim 12\\mathrm{k}$  samples to a subsampled IMDB-WIKI of 2k samples and transferring from another subsampled IMDB-WIKI of 32k samples to AgeDB. As shown in Table 5, SupCR(L1) outperforms L1 in both the linear probing and fine-tuning settings in both two scenarios.\n\n\n![Figure 4: SupCR is more resilient to reduced training data. Figure shows  $\\mathrm{MAE}^{\\downarrow}$  as a function of the number of training samples.](images/0967fa25160e206c670752d75d80203f2a9544d77daa649406ef4ebb9e3ed843.jpg)\n\n\nTable 5: Transfer learning results.\n\n\n<table><tr><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td><td>Method</td><td>MAE\u2193</td><td>R2\u2191</td></tr><tr><td colspan=\"6\">AgeDB \u2192 IMDB-WIKI (subsampled, 2k):</td></tr><tr><td colspan=\"3\">Linear Probing:</td><td colspan=\"3\">Fine-tuning:</td></tr><tr><td>L1</td><td>12.25</td><td>0.496</td><td>L1</td><td>11.57</td><td>0.528</td></tr><tr><td>SUPCR(L1)</td><td>11.12</td><td>0.556</td><td>SUPCR(L1)</td><td>11.09</td><td>0.546</td></tr><tr><td></td><td>(+1.13)</td><td>(+0.060)</td><td></td><td>(+0.48)</td><td>(+0.018)</td></tr><tr><td colspan=\"6\">IMDB-WIKI (subsampled, 32k) \u2192 AgeDB:</td></tr><tr><td colspan=\"3\">Linear Probing:</td><td colspan=\"3\">Fine-tuning:</td></tr><tr><td>L1</td><td>7.36</td><td>0.801</td><td>L1</td><td>6.36</td><td>0.848</td></tr><tr><td>SUPCR(L1)</td><td>7.06</td><td>0.812</td><td>SUPCR(L1)</td><td>6.13</td><td>0.850</td></tr><tr><td></td><td>(+0.30)</td><td>(+0.011)</td><td></td><td>(+0.23)</td><td>(+0.002)</td></tr></table>\n\n\n# 4.5 GENERALIZATION TO UNSEEN TARGETS\n\n\nIn real-world regression tasks, it is common that some targets are unseen during training. As in Yang et al. (2021), we curate two subsets of IMDB-WIKI that contain unseen targets. The test set is a uniform distribution across the whole target range. Table 6 shows the label distributions of the two training set, where pink shading indicates regions of unseen targets, and the blue shading represents the distribution of seen targets. The first training set consists of 3,781 training samples with a bi-modal Gaussian dis\n\n\nTable 6: Generalization performance to unseen targets on two curated subsets of IMDB-WIKI. MAE $^{\\downarrow}$  is used as the metric.\n\n\n<table><tr><td>Label Distribution</td><td>Method</td><td>All</td><td>Seen</td><td>Unseen</td></tr><tr><td></td><td>L1</td><td>12.53</td><td>10.82</td><td>18.40</td></tr><tr><td></td><td>SUPCR(L1)</td><td>11.69</td><td>10.46</td><td>15.92</td></tr><tr><td></td><td></td><td>(+0.84)</td><td>(+0.36)</td><td>(+2.48)</td></tr><tr><td></td><td>L1</td><td>11.94</td><td>10.43</td><td>14.98</td></tr><tr><td></td><td>SUPCR(L1)</td><td>10.88</td><td>9.78</td><td>13.08</td></tr><tr><td></td><td></td><td>(+1.06)</td><td>(+0.64)</td><td>(+1.90)</td></tr></table>\n\n\ndistribution over the target space, and the second training set consists of 3,662 training samples with a tri-modal Gaussian distribution over the target space. We report the prediction error on the seen and unseen targets separately. The results show that SupCR (L1) outperforms L1 by a larger margin on the unseen targets without sacrificing the performance on the seen targets.\n\n\n# 4.6 DESIGN VARIANTS\n\n\nSimilarity Measure and Projection Head. We explore potential variants of our method to pick the best design. Table 7 compares the performance of our method ( $\\mathcal{L}_{\\mathrm{SupCR}}$  with similarity measure using the negative L2 norm) with alternative designs that use a different similarity measure and potentially support a projection head. The results are on the AgeDB dataset and using SupCR(L1).\n\n\nSpecifically, the table shows the performance of using different feature similarity measures  $\\sin (\\cdot ,\\cdot)$ . We see that among cosine similarity, i.e.,  $\\begin{array}{rlr}{\\mathrm{sim}(\\pmb {v}_i,\\pmb {v}_j)} & = & {\\frac{\\pmb{v}_i\\cdot\\pmb{v}_j}{\\|\\pmb{v}_i\\|_2\\|\\pmb{v}_j\\|_2}} \\end{array}$  negative L1 norm, i.e.,\n\n\nTable 7: Comparison to potential variants of  ${\\mathcal{L}}_{\\text{SupCR }}$  and their performance on AgeDB.\n\n\n<table><tr><td>Loss Term</td><td>Similarity Measure</td><td>Projection Head</td><td>MAE\u2193</td><td>R2\u2191</td></tr><tr><td>LSimCLR</td><td>cosine</td><td>\u2713</td><td>9.59</td><td>0.666</td></tr><tr><td>LSimCLR</td><td>cosine</td><td>X</td><td>10.67</td><td>0.600</td></tr><tr><td>LSupCon</td><td>cosine</td><td>\u2713</td><td>8.13</td><td>0.753</td></tr><tr><td>LSupCon</td><td>cosine</td><td>X</td><td>7.89</td><td>0.760</td></tr><tr><td>LSupCR</td><td>cosine</td><td>\u2713</td><td>7.34</td><td>0.793</td></tr><tr><td>LSupCR</td><td>cosine</td><td>X</td><td>6.51</td><td>0.836</td></tr><tr><td>LSupCR</td><td>negative L1 norm</td><td>X</td><td>6.25</td><td>0.842</td></tr><tr><td>LSupCR</td><td>negative L2 norm</td><td>X</td><td>6.14</td><td>0.850</td></tr></table>\n\n\n$\\mathrm{sim}(\\pmb {v}_i,\\pmb {v}_j) = -\\| \\pmb {v}_i - \\pmb {v}_j\\| _1$  and negative L2 norm, i.e.,  $\\mathrm{sim}(\\pmb {v}_i,\\pmb {v}_j) = -\\| \\pmb {v}_i - \\pmb {v}_j\\| _2$  , negative L2 norm delivers the best performance.\n\n\nThe table also shows the performance with and without a projection head. To train the encoder network  $f(\\cdot):X\\to \\mathbb{R}^{d_e}$ , one can involve a projection head  $h(\\cdot):\\mathbb{R}^{d_e}\\rightarrow \\mathbb{R}^{d_p}$  during training to calculate the loss on  $\\{h(f(\\tilde{x}_i))\\}_{i = 1\\in [2N]}$ , and then discard the projection head to use  $f(x_{i})$  for inference. One can also directly calculate the loss on  $\\{f(\\tilde{x}_i)\\}_{i\\in [2N]}$  and then use  $f(x_{i})$  for inference. SIMCLR (Chen et al., 2020a) and SUPCON (Khosla et al., 2020) both use the former approach.\n\n\nFor  $\\mathcal{L}_{\\mathrm{SimCLR}}$ , the results show that adding the projection head can benefit the regression performance. This is because  $\\mathcal{L}_{\\mathrm{SimCLR}}$  aims to extract features that are invariant to data augmentations and can remove information that may be useful for the downstream regression task. For  $\\mathcal{L}_{\\mathrm{SupCon}}$  and  $\\mathcal{L}_{\\mathrm{SupCR}}$ , however, it is better to train the encoder without the projection head, since both  $\\mathcal{L}_{\\mathrm{SupCon}}$  and  $\\mathcal{L}_{\\mathrm{SupCR}}$  leverage the label information to extract features that directly target on the downstream task.\n\n\nMoreover, we see that using  $\\mathcal{L}_{\\mathrm{SupCon}}$  or  $\\mathcal{L}_{\\mathrm{supCR}}$  as the loss function delivers better performance than using  $\\mathcal{L}_{\\mathrm{SimCLR}}$ , verifying that it is helpful to utilize label information during encoder training. We also see that  $\\mathcal{L}_{\\mathrm{supCR}}$  outperforms  $\\mathcal{L}_{\\mathrm{supCon}}$  by a large margin, highlighting the superiority of  $\\mathcal{L}_{\\mathrm{supCR}}$  which explicitly leverages the ordered relationships between samples for regression problems.\n\n\nTraining Scheme. There are usually three schemes to train the feature encoder: (1) Linear probing: first trains the feature encoder using the representation learning loss, then freezes the encoder and trains a linear regressor on top of it using a regression loss. (2) Fine-tuning: first trains\n\n\nTable 8: Performance of alternative training schemes for the feature encoder on AgeDB.\n\n\n<table><tr><td>Scheme</td><td>Linear Probing</td><td>Fine-tuning</td><td>Regularization</td><td>L1</td></tr><tr><td>MAE\u2193</td><td>6.14</td><td>6.36</td><td>6.42</td><td>6.63</td></tr><tr><td>R2\u2191</td><td>0.850</td><td>0.844</td><td>0.833</td><td>0.828</td></tr></table>\n\n\nthe feature encoder using the representation learning loss, then fine-tunes the whole model using a regression loss. (3) Regularization: trains the whole model while jointly optimizing the representation learning loss and the regression loss.\n\n\nTable 8 shows the performance on AgeDB for the three schemes using  $\\mathcal{L}_{\\mathrm{SupCR}}$  as the representation learning loss and L1 as the regression loss. We see that all of the three schemes can improve performance over using the regression loss L1 alone to train a model. Further, unlike classification problems where fine-tuning often delivers the best performance, freezing the feature encoder performs the best. This is because in the case of regression, back-propagating the L1 loss to the representation can destroy the order in the embedding space learned by  $\\mathcal{L}_{\\mathrm{SupCR}}$ , which leads to poorer performance.\n\n\n# 5 CONCLUSION\n\n\nIn this paper, we propose Supervised Contrastive Regression (SupCR), a framework that learns a regression-aware representation by contrasting samples against each other according to their target distance. The proposed framework is orthogonal to existing regression models, and can be used in combination with such models to improve performance. It achieves the state-of-the-art performance and consistently improves prior regression baselines across different datasets, tasks, and input modalities. It also improves the robustness to data corruptions, resilience to reduced training data, performance on transfer learning, and generalization to unseen targets.\n\n\n# REFERENCES\n\n- Ahmed A Abdelrahman, Thorsten Hempel, Aly Khalifa, and Ayoub Al-Hamadi. L2cs-net: Fine-grained gaze estimation in unconstrained environments. arXiv preprint arXiv:2203.03339, 2022.\n\n- Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka. Rank consistent ordinal regression for neural networks with application to age estimation. Pattern Recognition Letters, 140:325-331, 2020.\n\n- Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9650-9660, 2021.\n\n- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. PMLR, 2020a.\n\n- Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.\n\n- Wei-Ta Chu, Kai-Chia Ho, and Ali Borji. Visual weather temperature prediction. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 234-241. IEEE, 2018.\n\n- Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. Advances in neural information processing systems, 33:8765-8775, 2020.\n\n- Benoit Dufumier, Pietro Gori, Julie Victor, Antoine Grigis, and Edouard Duchesnay. Conditional alignment and uniformity for contrastive learning with continuous proxy labels. arXiv preprint arXiv:2111.05643, 2021a.\n\n- Benoit Dufumier, Pietro Gori, Julie Victor, Antoine Grigis, Michele Wessa, Paolo Brambilla, Pauline Favre, Mircea Polosan, Colm Mcdonald, Camille Marie Piguet, et al. Contrastive learning with continuous proxy meta-data for 3d mri classification. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 58-68. Springer, 2021b.\n\n- Denis A Engemann, Apolline Mellot, Richard H\u00f6chenberger, Hubert Banville, David Sabbagh, Lukas Gemein, Tonio Ball, and Alexandre Gramfort. A reusable benchmark of brain-age prediction from m/eeg resting-state signals. NeuroImage, 262:119521, 2022.\n\n- Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2002-2011, 2018.\n\n- Bin-Bin Gao, Chao Xing, Chen-Wei Xie, Jianxin Wu, and Xin Geng. Deep label distribution learning with label ambiguity. IEEE Transactions on Image Processing, 26(6):2825-2838, 2017.\n\n- Bin-Bin Gao, Hong-Yu Zhou, Jianxin Wu, and Xin Geng. Age estimation using expectation of label distribution learning. In *IJCAI*, pp. 712-718, 2018.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.\n\n- Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019.\n\n- Peter J Huber. Robust estimation of a location parameter. In *Breakthroughs in statistics*, pp. 492-518. Springer, 1992.\n\n- Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi Feng. Exploring balanced feature spaces for representation learning. In International Conference on Learning Representations, 2020.\n\n- Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661-18673, 2020.\n\n- Shikun Li, Xiaobo Xia, Shiming Ge, and Tongliang Liu. Selective-supervised contrastive learning with noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 316-325, 2022a.\n\n- Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe Yang, Rogerio S Feris, Piotr Indyk, and Dina Katabi. Targeted supervised contrastive learning for long-tailed recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6918-6928, 2022b.\n\n- Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.\n\n- Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.\n\n- Radu P Mihail, Scott Workman, Zach Bessinger, and Nathan Jacobs. Sky segmentation in the wild: An empirical study. In 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1-6. IEEE, 2016.\n\n- Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop, volume 2, pp. 5, 2017.\n\n- Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. Ordinal regression with multiple output cnn for age estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4920-4928, 2016.\n\n- Iyad Obeid and Joseph Picone. The temple university hospital eeg data corpus. Frontiers in neuroscience, 10:196, 2016.\n\n- Hongyu Pan, Hu Han, Shiguang Shan, and Xilin Chen. Mean-variance loss for deep age estimation from a face. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5285-5294, 2018.\n\n- Rasmus Rothe, Radu Timofte, and Luc Van Gool. Dex: Deep expectation of apparent age from a single image. In Proceedings of the IEEE international conference on computer vision workshops, pp. 10-15, 2015.\n\n- Steffen Schneider, Jin Hwa Lee, and Mackenzie Weygandt Mathis. Learnable latent embeddings for joint behavioral and neural analysis. arXiv preprint arXiv:2204.00673, 2022.\n\n- Fabian Schrumpf, Patrick Frenzel, Christoph Aust, Georg Osterhoff, and Mirco Fuchs. Assessment of non-invasive blood pressure prediction from ppg and rppg signals using deep learning. Sensors, 21(18):6022, 2021.\n\n- Xintong Shi, Wenzhi Cao, and Sebastian Raschka. Deep neural networks for rank-consistent ordinal regression based on conditional probabilities, 2021.\n\n- Yaoming Wang, Yangzhou Jiang, Jin Li, Bingbing Ni, Wenrui Dai, Chenglin Li, Hongkai Xiong, and Teng Li. Contrastive regression for domain adaptation on gaze estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19376-19385, 2022.\n\n- Yuzhe Yang, Kaiwen Zha, Yingcong Chen, Hao Wang, and Dina Katabi. Delving into deep imbalanced regression. In International Conference on Machine Learning, pp. 11842-11851. PMLR, 2021.\n\n- Xumin Yu, Yongming Rao, Wenliang Zhao, Jiwen Lu, and Jie Zhou. Group-aware contrastive regression for action quality assessment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7919-7928, 2021.\n\n- Zhiyuan Zeng, Keqing He, Yuanmeng Yan, Zijun Liu, Yanan Wu, Hong Xu, Huixing Jiang, and Weiran Xu. Modeling discriminative representations for out-of-domain detection with supervised contrastive learning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 870-878, 2021.\n\n- Xuong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. Mpiigaze: Real-world dataset and deep appearance-based gaze estimation. IEEE transactions on pattern analysis and machine intelligence, 41(1):162-175, 2017a.\n\n- Xuong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. It's written all over your face: Full-face appearance-based gaze estimation. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on, pp. 2299-2308. IEEE, 2017b.\n\nResponse: Based on the provided paper and the criteria for acceptance at ICLR (high acceptance rate around 30%), my prediction is:\n\n\\[\n\\boxed{Accept}\n\\]\n\n### Evidence-Based Rationale\n\n#### Originality\nThe paper introduces a novel method called Supervised Contrastive Regression (SupCR) that addresses the fragmentation and lack of order in regression representations. This is an original contribution that leverages concepts from contrastive learning and supervised learning to achieve better performance on various regression tasks compared to traditional methods. The idea of explicitly ordering the representations according to the label distances is innovative and distinct from existing regression techniques.\n\n#### Methodology Rigor\nThe authors provide a detailed explanation of their proposed method, including the formulation of the Supervised Contrastive Regression loss function and its theoretical justification. The loss function is designed to ensure that the feature embeddings are ordered according to the label distances, which is a clear and rigorous approach. The empirical analysis using extensive experiments on multiple datasets further validates the effectiveness of the method.\n\n#### Significance\nThe paper demonstrates significant improvements over state-of-the-art regression methods on a variety of real-world regression tasks across different domains (computer vision, human-computer interaction, healthcare). These improvements are substantial, with the proposed method achieving state-of-the-art performance and consistently improving performance across different datasets and tasks. The results are compelling and demonstrate the practical utility of the method.\n\n#### Clarity\nThe paper is well-written and clearly explains", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634843.6040359", "msecs": "604.0", "relativeCreated": "188427.51002311707", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers for acceptance. Assess based on originality, methodology rigor, significance, clarity, and alignment with field standards. Provide a structured prediction with confidence level (High/Medium/Low), detailing evidence-based rationale. Systematically weigh strengths and weaknesses, ensuring objectivity and adherence to peer review principles.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Limitations of measure-first protocols in quantum machine learning\n\n# Abstract\nIn recent times, there have been major developments in two distinct yet connected domains of quantum information. On the one hand, substantial progress has been made in so-called randomized measurement protocols. Here, a number of properties of unknown quantum states can be deduced from surprisingly few measurement outcomes, using schemes such as classical shadows. On the other hand, significant progress has been made in quantum machine learning. For example, exponential advantages have been proven when the data consists of quantum states and quantum algorithms can coherently measure multiple copies of input states. In this work, we aim to understand the implications and limitations of combining randomized measurement protocols with quantum machine learning, although the implications are broader. Specifically, we investigate quantum machine learning algorithms that, when dealing with quantum data, can either process it entirely using quantum methods or measure the input data through a fixed measurement scheme and utilize the resulting classical information. We prove limitations for quantum machine learning algorithms that use fixed measurement schemes on the input quantum states. Our results have several implications. From the perspective of randomized measurement procedures, we show limitations of measure-first protocols in the average case, improving on the state-of-the-art which only focuses on worst-case scenarios. Additionally, previous lower bounds were only known for physically unrealizable states. We improve upon this by employing quantum pseudorandom functions to prove that a learning separation also exists when dealing with physically realizable states, which may be encountered in experiments. From a machine learning perspective, our results are crucial for defining a physically meaningful task that shows fully quantum machine learning processing is not only more efficient but also necessary for solving certain problems. The tasks at hand are also realistic, as the algorithms and proven separations hold when working with efficiently preparable states and remain robust in the presence of measurement and preparation errors.\n\n\n# 1 INTRODUCTION\n\n\nA central question in quantum machine learning revolves around understanding the various types of advantages one can achieve by exploiting quantum effects. Some of the most interesting scenarios arise when the dataset itself comprises quantum states, which can then be processed fully coherently, or through elaborate measurement strategies. In this context, exponential advantages have been identified when coherent measurements of multiple copies of a given quantum state are allowed Chen et al. (2022b); Huang et al. (2022; 2020b). In a parallel related line, there have been significant breakthroughs in extracting useful classical information from quantum states using the versatile toolkit of randomized measurements Elben et al. (2023). This toolkit includes the groundbreaking concept of classical shadows Huang et al. (2020a; 2022), which can extract an efficient classical description of quantum states that allows one to compute various physical properties. These two distinct research lines cast doubt on the advantages of quantum machine learning protocols that process quantum data directly compared to those that use a fixed measurement procedure, which can also allow for a coherent manipulation of quantum states before the fixed measurement, in order to extract valuable classical data<sup>1</sup>. In particular we address the question: is it possible that\n\n\na \"measure-first\" protocol can be universally used as a substitute for any \"fully-quantum\" protocol in the general context of machine learning?\n\n\nWe formally define what we mean by a \"measure-first\" or a \"fully-quantum\" protocol in Section 2 (see Definition 3 and Definition 5), and we provide an overview in Figure 1. Importantly, we focus on physically relevant scenarios where the quantum states in the dataset are efficiently preparable, rather than arbitrary unphysical states that do not allow a polynomial description in terms of quantum gates.\n\n\nOur main conceptual contribution is to resolve the above question negatively by presenting a concrete machine learning scenario that clearly exhibits the limitations of any measure-first protocol. In a machine learning setup, we construct a task that requires reproducing an undisclosed measurement from the data. This task shows an exponential difference in the number of data needed between fully-quantum protocols and measure-first protocols.\n\n\nOutside of machine learning, certain limitations of measure-first protocols are implied by other works in various tasks such as distributed sampling Montanaro (2019) and relational problems Aaronson et al. (2023). However, these results primarily address separations in worst-case scenarios, which contrasts with the typical focus in machine learning settings where average-case performance is usually sufficient. In this paper, we advance these findings by identifying the limitations of measure-first protocols that need only achieve average-case correctness for a set of efficiently preparable quantum states, i.e. physical states. On a technical level, establishing limitations for measure-first protocols in a general quantum machine learning setting necessitates novel techniques beyond those utilized in previous studies. In particular, limitations for the task of predicting properties of quantum states were also recently derived for worst-case settings Grier et al. (2022) exploiting lower bounds for classical one-way communication complexity. However, this approach inherently limits the focus to states that are not efficiently preparable. This limitation arises because, if states were efficiently preparable, there would be a concise classical description of their preparation procedure, enabling the transmission of valid classical messages to the recipient. Our main technical contribution lies in exploiting that, from the more general machine learning standpoint, it makes sense to constrain the computational power of the learning protocol. This allows us to additionally impose the requirement that states must be efficiently preparable and still achieve a separation by utilizing a novel construction that combines results on one-way communication complexity and pseudorandom quantum states Brakerski & Shmueli (2019). The task we consider is also experimentally robust, in the sense that we allow for errors in the preparation of the input quantum states and on the measurement outcomes which label them.\n\n\n# 1.1 HIGH-LEVEL OVERVIEW OF LEARNING SETTING AND MAIN RESULT\n\n\nThe machine learning problem concerns learning an unknown measurement acting on a set of input quantum states. Specifically, the data that the learning protocol gets consists of pairs of copies of  $n$ -qubit quantum states  $\\rho$  drawn from some distribution  $\\mathcal{D}$  together with a corresponding label  $z \\in \\{0,1\\}^{2n + 1}$ . The first  $n$  bits of  $z$  encode a  $2^n$ -outcome POVM measurement  $\\Lambda_{x} = \\{E_{j}^{x} | j \\in \\{0,1\\}^{n}\\}$  from some set  $\\Lambda = \\{\\Lambda_{x} | x \\in \\{0,1\\}^{n}\\}$ . The remaining  $n + 1$  bits are determined by the outcome of  $\\Lambda_{x}$  on the quantum state  $\\rho$ . The goal of the machine learning protocol is to \"learn\" how to reproduce the measurement  $\\Lambda_{x}$ . More precisely, the trained learning protocol has to receive as input an unseen quantum state  $\\rho'$  and output a sample  $z'$  in agreement with the probability distribution  $\\pi_{x}(\\rho)$ , where  $\\pi_{x}(\\rho)$  denotes the distribution of the measurement outcomes of  $\\Lambda_{x}$ .\n\n\nThis paper explores whether solving learning problems such as the above requires a quantum computer capable of adaptive measurements on the training data, or if a fixed measurement strategy that produces classical representations of the quantum states is sufficient. Specifically, we consider so-called \"measure-first protocols\" that are forced to measure the input states and use the obtained classical description to train the machine learning model to produce samples from the target distribution. Importantly, in a measure-first protocol the measurements are not allowed to depend on the training data, but are unconstrained otherwise. In particular, the measure-first protocol can depend on the set of measurements that is to be reproduced from the data, but not on the specific individual measurement hidden within some given training data. On the other hand, we consider so-called \"fully-quantum protocols\" that can coherently process the quantum states and adjust the measurements based on the training data. Our main result is the existence of a learning problem where the\n\n\n![Figure 1: An overview of the quantum machine learning protocols explored in our paper. In both protocols, the quantum algorithm  $A$  is tasked with processing training data  $T_{x}$  to output a classical description of a function  $h$  that correctly produces samples  $z^{*}$  for new input states  $\\rho^{*}$  (i.e.,  $h$  should implement the quantum measurement). The difference between the two protocols is that, in the measure-first protocol, both the input quantum states and those in the training set  $T_{x}$  are subjected to a randomized measurement strategy  $M$ . Consequently, these quantum states  $\\rho^{*}$  are transformed into a classical representation  $\\hat{\\rho}^{*}$ . Importantly, this strategy is allowed to depend only on the concept class  $\\mathcal{C}$  (which captures the general learning problem), not on the specific target concept  $\\pi_{x} \\in \\mathcal{C}$ . In other words, the same randomized measurement strategy  $M$  is used to preprocess the quantum data for all concepts  $\\pi_{x} \\in \\mathcal{C}$ .](images/6da32fa5cf8e2628e4fada6af45c1312f421dae9d434c2b3fa522a43e07dbb40.jpg)\n\n\nquantum data is efficiently generatable on a quantum computer for which a \"measure-first protocol\" requires an exponential amount of data to be able to reproduce measurement outcomes on new input states, whereas a fully-quantum protocol only requires a polynomial amount of data. Additionally, we require that the protocols must be efficient in the sense that they run in time polynomial in  $n$ . Using the notion of learning we presented above, we now give an informal description of the main result of this paper.\n\n\nTheorem 1. (Informal) There exist a concept class  $\\mathcal{C}$ , where each concept is defined by a measurement and a distribution  $\\mathcal{D}$  over efficiently preparable quantum states such that no \"measure-first\" protocol can learn to reproduce  $\\mathcal{C}$  correctly on average with a polynomial amount of training data. On the other hand, there exists a \"fully-quantum\" protocol which can learn  $\\mathcal{C}$  efficiently with respect to both sample and time complexity.\n\n\nThe specific learning problem which exhibits a learning separation is carefully outlined in Section 3 and the precise proofs of Theorem 1 can be found in the Appendix.\n\n\n# 1.2 RELATED WORK\n\n\nIn this section we discuss related works and highlight their relationships to our learning setting. Firstly, in Huang et al. (2020a), the authors introduced a randomized measurement technique tailored to extract a classical description of a quantum state  $\\rho$ . This description enables the computation of the expectation values of any set of observables  $\\{O_k\\}_{k=1}^M$  - provided they have a low \"shadow norm\", such as when the observables are local - up to a precision of  $\\epsilon$ . Notably, they showed that a number of copies of  $\\rho$ , scaling logarithmically with the number of observables  $M$  and inverse-polynomially in the precision  $\\epsilon$ , suffices for this task. As we want to learn the distribution associated of a  $2^n$ -outcome POVM with an  $\\epsilon$  error in total variation, directly applying these techniques to estimate each of the  $2^n$  outcomes would require an exponential precision for each. In their upper bound estimates, this results in the requirement of an exponential number of samples.\n\n\nThe concept of shadow tomography, introduced in Aaronson (2018), revolves around the problem of computing the expectation values of any set of  $M$  two-outcome measurements on an  $n$ -qubit state  $\\rho$  up to precision  $\\epsilon$ . It has been shown that this can be done using a number of copies of  $\\rho$  that scale\n\n\npolylogarithmically in  $M$ , linearly in  $n$ , and inverse-polynomially in  $\\epsilon$  Aaronson (2018). In contrast to the methods in Huang et al. (2020a), the approach in Aaronson (2018) requires coherent measurements on multiple copies of  $\\rho$ . Additionally, as demonstrated in Huang et al. (2022); Chen et al. (2022a) for specific tasks, the capacity to coherently measure multiple copies of quantum states provides an exponential advantage in sample complexity over sequential measurements. Considering our framework, where measurements can act coherently on multiple copies of each input state, one might question whether such strategies enable a measure-first protocol to solve our learning task. However, it is important to note that coherent measurements do not improve the scaling with respect to the precision  $\\epsilon$ , leaving room for the possibility of a separation since the learning task studied in this paper would require at least exponential precision to be solved.\n\n\nIn Gong & Aaronson (2023), the authors extended the concept of shadow tomography to the scenario of learning a  $K$ -outcome POVMs (for  $K \\geq 2$ ) selected from a set of  $M$  unknown quantum measurements. In contrast to the binary outcome case, the goal now is to approximate an unknown distribution up to precision  $\\epsilon$  in total variation distance, rather than focusing on expectation values. Their procedure requires a number of copies of the quantum state scaling linearly with  $K$  and  $n$ , polylogarithmically with  $M$ , and inverse-polynomially with the precision  $\\epsilon$ . Moreover, they establish the optimality of this scaling with respect to the dependence on the number of outcomes  $K$ . This result prompts the question of whether our separation between measure-first and fully-quantum protocols can be directly inferred from it. However, in establishing their lower bound, it is important to note that no assumptions were made regarding the complexity of the unknown quantum state. The crux of our study lies in demonstrating that measure-first protocols fall short of reproducing the unknown measurement, even on quantum states that are efficiently preparable. Moreover, it is important to highlight that while their shadow tomography procedures can be employed to construct a measure-first protocol by \"shadowfying\" input states to approximate the expected values of each measurement outcome, this is not strictly necessary for our task. Specifically, understanding the probability of each outcome allows for the creation of an \"evaluator\" that can compute the correct probability for every outcome. However, to resolve our learning problem, a \"generator\" (i.e., an algorithm generating samples with the correct probabilities) already suffices, and it does not necessarily require computing the output probabilities Sweke et al. (2021).\n\n\nNext, in Grier et al. (2022) the authors explore lower bounds on the number of copies of a quantum state required for solving what they term the \"classical shadow\" task. This task involves estimating the expected values of observables using a measure-first approach. While they establish lower bounds on the number of copies of a quantum state required for this task, shedding light on the inherent limitations of measure-first protocols, it is crucial to note two significant bottlenecks of their approach when viewed from our more general machine learning perspective. Firstly, their analysis only focuses on worst-case scenarios, whereas in our machine learning scenarios we are concerned with average-case performance. Secondly, their investigation centers on protocols required to operate effectively even for unphysical quantum states that are not efficiently preparable.\n\n\nThe authors of Cheng et al. (2015) provide upper bounds for the dual problem of shadow tomography, or more specifically the problem of learning a measurement. In particular, they studied the task of learning an unknown two-outcome POVM denoted  $\\{E, I - E\\}$ , from data of the form  $\\{(\\rho_i, \\mathrm{Tr}(E\\rho_i))\\}_{i=1}^m$ . They showed that to approximate the unknown  $E$  up to a precision of  $\\epsilon$  on new  $n$ -qubit input states,  $\\mathcal{O}(2^n / \\epsilon^2)$  training samples are sufficient. Nonetheless, the number of required samples scales exponentially with the number of qubits.\n\n\nFinally, in Jerbi et al. (2023) the authors study quantum process learning, where the task is to learn a unitary  $U$ , from data of the form  $\\{| \\psi_i \\rangle, U | \\psi_i \\rangle$ . In particular, they study the limitations of what they call incoherent learning, where the learner is constrained to first measure multiple copies of the data  $U | \\psi_i \\rangle$ . While they therefore also study the problem of extracting classical information from quantum data and utilizing it in the learning process, the setting in their work differs from ours. Namely, the quantum states in our scenario are labeled by a sample obtained from the unknown measurement process, whereas in Jerbi et al. (2023) the labels are the input quantum states when evolved under some unknown target unitary.\n\n\n# 2 BACKGROUND\n\n\nIn this section, we introduce the notations and definitions needed to frame our main result, which involves a learning problem that demonstrate the separation between any measure-first protocol and a fully quantum protocol.\n\n\nThe learning problem we study is the learning of a measurement. In particular, it involves generating samples from a distribution induced by measuring an (unknown) POVM measurement  $\\Lambda_{x}$  on  $n$ -qubit quantum states. The (unknown) target measurement  $\\Lambda_{x}$  is drawn from a set of POVMs  $\\{\\Lambda_{x} \\mid x \\in \\{0,1\\}^{n}\\}$ , and each measurement  $\\Lambda_{x}$  is a computational basis measurement preceded by an  $n$ -qubit unitary  $U_{x}$ , i.e.,\n\n\n$$\n\n$$\n\\Lambda_ {x} = \\left\\{E _ {j} ^ {x} | j = 1, \\dots , 2 ^ {n} \\right\\}, \\text {w h e r e} E _ {j} ^ {x} = U _ {x} | j \\rangle \\langle j | U _ {x} ^ {\\dagger}.\n$$\n\n$$\n\n\nDuring training the learner is given a set of examples  $T_{x}$ , where each example consists of a polynomial number of copies of a phase state  $|\\psi_f\\rangle$  together with a sample from the associated POVM-induced distribution. This is a special case of labeled quantum data, which was introduced in Aimeur et al. (2006), where we are additionally allowed to have access to polynomially many copies of the input quantum state. We formalize our learning problem by generalizing the standard PAC learning framework Valiant (1984). In our generalization, a concept corresponds to a quantum randomized function, i.e., a function that on each quantum input state outputs a sample from a random variable (which in our case corresponds to the outcomes of a POVM on the input quantum state). Before we define the concept class studied throughout this paper, we first setup some auxiliary definitions.\n\n\nDefinition 1 (Auxiliary definitions/notation).\n\n- - Let  $N = 2^n$ , or equivalently  $n = \\log_2 N$ .\n\n- - We identify a function  $f: \\{0,1\\}^n \\to \\{0,1\\}$  with its truth table  $f \\in \\{0,1\\}^N$ , and we denote its corresponding phase state with\n\n\n$$\n\n$$\n\\left| \\psi_ {f} \\right\\rangle = \\frac {1}{\\sqrt {N}} \\sum_ {i = 1} ^ {N} (- 1) ^ {f _ {i}} | i \\rangle . \\tag {1}\n$$\n\n$$\n\n- - Let  $\\mathcal{S}_{\\mathrm{phase}} = \\{\\left|\\psi_f\\right\\rangle^{\\otimes \\ell} \\mid f \\in \\{0,1\\}^N\\}$  denote the set of  $\\ell = \\mathrm{poly}(n)$  copies of  $n$ -qubit phase states.\n\n- - We write  $x \\sim \\pi$  to denote that  $x$  was drawn according to a distribution  $\\pi$ .\n\n- - We write  $\\mathcal{U}(\\mathcal{X})$  for the uniform distribution over a set  $\\mathcal{X}$ .\n\n- - We write  $\\Delta(\\mathcal{X})$  for the set of all distributions over a set  $\\mathcal{X}$ .\n\n- - For  $p, q \\in \\mathbb{R}_{\\geq 0}^{2^n}$  probability distributions over  $\\{0,1\\}^n$  we define\n\n\n$$\n\n$$\n\\left| \\left| p - q \\right| \\right| _ {T V} = \\frac {1}{2} \\sum_ {x \\in \\{0, 1 \\} ^ {n}} \\left| p _ {x} - q _ {x} \\right|\n$$\n\n$$\n\n\nto be the total variation distance.\n\n\nDefinition 2 (Concept class). We define our concept class as  $\\mathcal{C} = \\{\\pi_x\\mid x\\in \\{0,1\\} ^n\\}$ , where\n\n\n$$\n\n$$\n\\pi_ {x}: \\mathcal {S} _ {\\text {p h a s e}} \\rightarrow \\Delta (\\{0, 1 \\} ^ {2 n + 1}) \\tag {2}\n$$\n\n$$\n\n\n$$\n\n$$\n\\left| \\psi_ {f} \\right\\rangle^ {\\otimes \\ell} \\mapsto \\pi_ {x} (f)\n$$\n\n$$\n\n\nwhere  $\\pi_x(f)$  is a distribution over samples  $(x,y,b) \\in \\{0,1\\}^{2n + 1}$ , where  $(y,b) \\in \\{0,1\\}^{n + 1}$  are variables sampled from a problem-specific distribution characterizing the specific learning task.\n\n\nIn particular,  $\\pi_x$  is a randomized function which takes as input a polynomial number of copies of a phase state and outputs a sample  $z$  consisting of  $x \\in \\{0,1\\}^n$  together with some  $(y,b) \\in \\{0,1\\}^{n+1}$  sampled from some problem-specific probability distribution. As we will see in the next section,\n\n\na learning separation for this kind of learning problem is achieved by considering  $(y, b)$  sampled uniformly from the set of variables which satisfy the Hidden Matching relation defined by the input functions  $f$  and the bitstring  $x \\in \\{0, 1\\}^n$ .\n\n\nA learner then is given several evaluations of the randomized function  $\\pi_{x}$  in the form of training data and its objective is to implement a randomized function  $\\widetilde{\\pi}_{x}$  that closely approximates  $\\pi_{x}$  on most input states.\n\n\nIn this paper, we compare two categories of machine learning systems that can tackle problems of this type. First, we introduce what we call a \"fully-quantum protocol\".\n\n\nDefinition 3 (Fully-quantum protocol). A fully-quantum protocol for the concept class  $\\mathcal{C}$  in Definition 7 is a polynomial-time quantum algorithm  $A$  that takes as input training data of the form\n\n\n$$\n\n$$\nT _ {x} = \\left\\{\\left(\\left| \\psi_ {f ^ {(i)}} \\right\\rangle^ {\\otimes \\ell}, (x, y, b)\\right) \\mid (x, y, b) \\sim \\pi_ {x} (f ^ {(i)}), \\right.\n$$\n\n$$\n\n\n$$\n\n$$\n\\left. \\text {a n d} f ^ {(i)} \\sim \\mathcal {U} \\left(\\{0, 1 \\} ^ {N}\\right) \\right\\} _ {i = 1} ^ {\\operatorname {p o l y} (n)}, \\tag {3}\n$$\n\n$$\n\n\nand outputs a classical description of a polynomial-time quantum algorithm that on input  $|\\psi_f\\rangle^{\\otimes \\ell} \\in S_{\\mathrm{phase}}$  generates a sample from a distribution  $\\widetilde{\\pi}_x(f) \\in \\Delta(\\{0,1\\}^{2n+1})$ .\n\n\nWe emphasize that for a \"fully quantum\" protocol, the learning algorithm must produce a classical description of the quantum algorithm generating samples from  $\\widetilde{\\pi}_x$ . Consequently, we do not store any quantum states from the training data in quantum memory, which would be more general but not studied in this paper. Ultimately, the goal of the protocol is to implement a randomized function  $\\widetilde{\\pi}_x$  that closely approximates the actual data-generating randomized function  $\\pi_x$  for most of the input quantum states.\n\n\nDefinition 4  $((\\epsilon, \\delta, p_{\\mathrm{suc}})$ -fully-quantum learnable). We say that  $\\mathcal{C}$  is  $(\\epsilon, \\delta, p_{\\mathrm{suc}})$ -fully-quantum learnable if there exists a fully-quantum protocol  $A$  such that for every  $\\pi_x \\in \\mathcal{C}$ , with probability at least  $p_{\\mathrm{suc}}$  we have\n\n\n$$\n\n$$\n\\Pr_ {f \\sim \\mathcal {U} (\\{0, 1 \\} ^ {N})} \\left(\\left| \\left| \\widetilde {\\pi} _ {x} (f) - \\pi_ {x} (f) \\right| \\right| _ {T V} \\leq \\epsilon\\right) \\geq 1 - \\delta , \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\widetilde{\\pi}_x(f) \\in \\Delta(\\{0,1\\}^{2n+1})$  denotes the distribution that the polynomial-time quantum algorithm obtained from the learning algorithm  $A$  generates samples from on input  $|\\psi_f\\rangle^{\\otimes \\ell} \\in S_{\\mathrm{phase}}$ .\n\n\nNext, we introduce a \"measure-first protocol\" which consists of two components: (i) a randomized measurement strategy  $M$ , and (ii) a learning algorithm  $A$ . The main difference between a measure-first protocol and a fully-quantum protocol is that the former involves a randomized measurement procedure that first measures the quantum states before putting it into a learning algorithm. Importantly, the measure-first protocol is allowed to perform arbitrary coherent measurements on all input quantum states (i.e., the polynomially-many copies of the phase states). The only constraint is that the measurement strategy cannot depend on the specific target concept of the learning problem, although it is allowed to depend on the concept class (i.e., the set of all possible target concepts). In short, a randomized measurement strategy is a polynomial-time algorithms that maps a polynomial number of copies of a phase state  $|\\psi_f\\rangle$  to some classical description  $\\widehat{\\psi}_f\\in \\{0,1\\} ^m$  for some  $m = \\mathrm{poly}(n)$ . These classical descriptions  $\\widehat{\\psi}_f$  are then used as input for the learning algorithm, that is tasked with implementing a randomized function close to  $\\pi_{x}$ .\n\n\nDefinition 5 (Measure-first protocol). A measure-first protocol is a tuple  $(M, A)$  where\n\n- -  $M$  is a measurement strategy that in time  $\\mathcal{O}(\\mathrm{poly}(n))$  maps  $|\\psi_f\\rangle^{\\otimes \\ell}\\in S_{\\mathrm{phase}}^2$  to some  $\\widehat{\\psi}_f\\in \\{0,1\\} ^m$ , where  $m = \\mathrm{poly}(n)$ .\n\n- -  $A$  is a polynomial-time quantum algorithm that takes input of the form\n\n\n$$\n\n$$\nT _ {x} ^ {M} = \\left\\{\\left(\\widehat {\\psi} _ {f ^ {(i)}}, (x, y, b)\\right) \\mid (x, y, b) \\sim \\pi_ {x} (f ^ {(i)}) \\right.\n$$\n\n$$\n\n\n$$\n\n$$\n\\left. \\text {a n d} f ^ {(i)} \\sim \\mathcal {U} (\\{0, 1 \\} ^ {N}) \\right\\} _ {i = 1} ^ {\\text {p o l y} (n)}, \\tag {5}\n$$\n\n$$\n\n\nand outputs a description of a polynomial-time quantum algorithm that on input  $\\widehat{\\psi}_f = M(|\\psi_f\\rangle^{\\otimes \\ell})$  generates a sample from a distribution  $\\widetilde{\\pi}_x(f)\\in \\Delta (\\{0,1\\}^{2n + 1})$\n\n\nNote that the distinction between measure-first and fully-quantum protocols lies in Eq. 5, where the data is measured to give  $\\hat{\\psi_f}$  instead of remaining quantum states  $|\\psi_f\\rangle$ . Nonetheless, the measurements employed by the measurement strategy  $M$  are completely arbitrary and entirely unrestricted. We emphasize, however, that the sole constraint on the measurement strategy  $M$  is that it must not rely on the specific concepts it will be applied to later, although it can depend on the concept class it is intended for. Recall that the objective of the protocol is to implement a randomized function  $\\widetilde{\\pi}_x$  that closely approximates the actual data-generating randomized function  $\\pi_x$  on most inputs.\n\n\nDefinition 6  $((\\epsilon, \\delta, p_{\\mathrm{suc}})$ -measure-first learnable). We say that  $\\mathcal{C}$  is  $(\\epsilon, \\delta, p_{\\mathrm{suc}})$ -measure-first learnable if there exists a measure-first protocol  $(M, A)$  such that for every  $\\pi_x \\in \\mathcal{C}$ , with probability at least  $p_{\\mathrm{suc}}$  we have\n\n\n$$\n\n$$\n\\Pr_ {f \\sim_ {U} \\{0, 1 \\} ^ {N}} \\left(\\left| \\left| \\widetilde {\\pi} _ {x} (f) - \\pi_ {x} (f) \\right| \\right| _ {T V} \\leq \\epsilon\\right) \\geq 1 - \\delta , \\tag {6}\n$$\n\n$$\n\n\nwhere  $\\widetilde{\\pi}_x(f) \\in \\Delta(\\{0,1\\}^{2n+1})$  denotes the distribution that the polynomial-time quantum algorithm obtained from the learning algorithm  $A$  generates samples from on input  $\\widehat{\\psi}_f = M(|\\psi_f\\rangle^{\\otimes \\ell})$ .\n\n\nAlthough we defined the two protocols in an idealized case, we will show in Appendix A.1 that our separation results are robust. In particular, they still hold even in the experimental setting where the input states  $|\\psi_f\\rangle$  are affected by preparation errors and they are just close in trace distance to the ones defined in Eq.(1). Furthermore, we also allow for measurement errors on the labels  $(y,b)$\n\n\n# 3 MAIN RESULT\n\n\nIn this section, we present the key findings of our paper. In Section 3.1, we define the specific learning problem considered and show how the fully-quantum machine learning model can efficiently solve it. In Section 3.2, we present our first main result showing that no measure-first quantum machine learning model can solve our learning problem efficiently. Finally, in Section 3.3, we show that this separation between the models still holds if the quantum states in the data are efficiently preparable.\n\n\n# 3.1 THE LEARNING PROBLEM\n\n\nIn this section, we concretely define the learning problem for which we prove a learning separation. Then, we initiate the proof by showing that it is learnable using a fully-quantum protocol.\n\n\nThe specific concept class we developed to prove our main result is a particular instance of the one defined in Def. 2, with the condition that the variables  $(y,b)$  are sampled from the set  $R_{x}(f)$  defined as follows:\n\n\nDefinition 7 (Concept class).  $\\mathcal{C} = \\{\\pi_x\\mid x\\in \\{0,1\\} ^n\\}$ , where\n\n\n$$\n\n$$\n\\pi_ {x}: \\mathcal {S} _ {\\text {p h a s e}} \\rightarrow \\Delta (\\{0, 1 \\} ^ {2 n + 1}) \\tag {7}\n$$\n\n$$\n\n\n$$\n\n$$\n\\left| \\psi_ {f} \\right\\rangle^ {\\otimes \\ell} \\mapsto \\pi_ {x} (f)\n$$\n\n$$\n\n\nwhere  $\\pi_x(f)$  is a distribution over samples  $(x,y,b)$ , where  $(y,b) \\sim \\mathcal{U}(R_f(x))$  and\n\n\n$$\n\n$$\n\\begin{array}{c} R _ {f} (x) = \\{(y, b) \\mid y \\in \\{0, 1 \\} ^ {n}, b \\in \\{0, 1 \\}, \\\\ f (y) \\oplus f (y \\oplus x) = b \\}. \\end{array} \\tag {8}\n$$\n\n$$\n\n\nImportantly, in Aaronson et al. (2023) the authors showed that for each  $x \\in \\{0,1\\}^n$  there exist a POVM measurement  $\\Lambda_x$  which when applied to a phase state  $|\\psi_f\\rangle$  outputs a pair exactly satisfying the relation  $R_{f}(x)$ . With regards to our learning problem, the task of the learning protocols is to learn this measurement.\n\n\nWe now describe how the concept class in Definition 7 is fully-quantum learnable.\n\n\nProposition 1. The concept class in Definition 7 is  $(0,0,1)$ -fully-quantum learnable.\n\n\nThe proof of Proposition 1 can be found in Appendix A, and we provide a high-level overview of the fully-quantum protocol here. Firstly, the fully-quantum protocol reads out  $x$  from one of the samples generated by  $\\pi_x$  in the training data. Next, a quantum circuit denoted as  $U_x$  is constructed following the procedure outlined in Aaronson et al. (2023). Measuring  $U_x | \\psi_f \\rangle$  in the computational basis results in a sample from the distribution  $\\mathcal{U}(R_f(x))$ . Crucially, it is worth noting that these quantum circuits  $U_x$  are of size  $\\mathcal{O}(\\mathrm{poly}(n))$  and can be constructed in time  $\\mathcal{O}(\\mathrm{poly}(n))$ . We remark here that the concept class in Definition 7 remains fully quantum learnable even when errors are present in the training data, see Appendix A.1.\n\n\nIt might seem that little genuine learning occurs when  $x$  can be readily read out from a single example in  $T_{x}$ . However, we can introduce various levels of learning by providing only partial information about  $x$  within the examples. This partial information should allow the recovery of  $x$  from a polynomial number of examples. Several examples illustrating this are discussed in more detail in Appendix A.\n\n\n# 3.2 LIMITATIONS OF MEASURE-FIRST PROTOCOLS WITH GENERAL QUANTUM DATA\n\n\nIn the last section, we discussed how the concept class in Definition 7 is fully-quantum learnable. Conversely, in this section we discuss the first part of our main result which states that this concept class is not measure-first learnable.\n\n\nTheorem 2. The concept class in Definition 7 is not  $(\\epsilon, \\delta, p_{\\mathrm{succ}})$ -measure-first learnable for  $(1 - \\epsilon) \\cdot (1 - \\delta) > 7/8$  and any  $p_{\\mathrm{succ}} > 0$ .\n\n\nThe full proof of Theorem 2 is provided in Appendix B, we present a concise overview of the proof here. At its core, the proof hinges on the notion that the existence of a measure-first protocol for the concept class described in Definition 7 implies the existence of an efficient classical one-way communication protocol for the Hidden Matching (HM) problem Bar-Yossef et al. (2004). Notably, in Bar-Yossef et al. (2004), it has been shown that the HM problem cannot be solved with a communication cost of  $\\mathcal{O}(\\mathrm{poly}(n))$  bits, even on a  $7/8$  fraction of possible inputs. In essence, if the concept class in Definition 7 was learnable, one of the two parties can employ the measurement protocol to encode their input for the HM problem, transmit it to the other party, who can then utilize the learning algorithm  $A$  to successfully solve the HM problem. Intuitively, the reason behind why measure-first learning fails is that due to Bar-Yossef et al. (2004) it is not possible to compress a phase state  $|\\psi_f\\rangle$  into a polynomially-sized classical representation  $\\widehat{\\psi}_f$  that contains enough information to allow one to generate samples from the distributions  $\\pi_x(f)$  for all possible  $x$ . Importantly, within the machine learning context, it is crucial to also consider the possibility of protocols making errors on a fraction of inputs and the requirement to gather the necessary data. To overcome this limitation, we use Yao's principle to show that the existence of a protocol which succeeds in the machine learning context would still violate the classical hardness of the HM problem. In particular, by applying Yao's principle, we can ensure the existence of a deterministic algorithm that solves the HM problem on a sufficiently large fraction of inputs, provided that there exists a machine learning measure-first protocol capable of solving the learning task defined in Definition 6.\n\n\nSince in our machine learning setting we are concerned with physical states appearing in the realworld, in the next section we show that our findings also apply to settings where states are efficiently preparable, which we achieve by using pseudorandom states Brakerski & Shmueli (2019) (see Section 3.3). Finally, it is crucial to note that even when the data in Eq.(5) is derived from a polynomial number  $\\ell$  of copies of each input state  $|\\psi_f\\rangle$ , measure-first protocols remain incapable of solving the task. This holds true even when considering inefficient (i.e., superpolynomial-time) algorithms or an exponential number of training points, as the communication complexity bounds still apply regardless of the time resources utilized by the algorithms (since the communication protocol assumes both parties have unbounded resources).\n\n\n# 3.3 LEARNING SEPARATION WITH EFFICIENTLY PREPARABLE QUANTUM DATA\n\n\nFrom a pragmatic perspective, a crucial limitation of the learning problem outlined in the previous section is that preparing a general phase state is intractable (i.e., not realized by polynomial-time processes). In particular, this raises the question of whether separations could persist for states that are prepared by (natural or artificial) polynomial-time processes. To address this limitation, we show\n\n\nthat the concept class in Definition 7 remains not measure-first learnable, even when we constrain the input of the random functions  $\\pi_{x}$  to phase states of so-called pseudorandom functions. Notably, phase states corresponding to appropriately chosen pseudorandom functions can be efficiently prepared. Because of this additional consideration, our separations are notably more general than those in previous works Aaronson et al. (2023); Bar-Yossef et al. (2004). Our definition of pseudorandom functions is as follows.\n\n\nDefinition 8 (Quantum-secure pseudorandom function (QPRF) Brakerski & Shmueli (2019)). Let  $\\mathcal{K} = \\{\\mathcal{K}_n\\}_{n\\geq 1}$  be an efficiently samplable key distribution, and let  $\\mathrm{PRF} = \\{\\mathrm{PRF}_n\\}_{n\\geq 1}$ ,  $\\mathrm{PRF}_n:\\mathcal{K}_n\\times \\{0,1\\} ^n\\to \\{0,1\\}$  be an efficiently computable function. We say PRF is a quantum-secure pseudorandom function if for every efficient non-uniform quantum algorithm  $A$  that can make quantum queries there exists a negligible function  $\\mathrm{negl}(.)$  such that for every  $n\\geq 1$ :\n\n\n$$\n\n$$\n\\left| \\Pr_ {k \\sim \\mathcal {U} \\left(\\mathcal {K} _ {n}\\right)} \\left[ A ^ {\\underset {\\sim \\infty} {\\operatorname {P R F}} _ {n} (k)} () = 1 \\right] - \\Pr_ {f \\sim \\mathcal {U} \\left(\\{0, 1 \\} ^ {n}\\right)} \\left[ A ^ {f} () = 1 \\right] \\right| \\leq \\operatorname {n e g l} (n) \\tag {9}\n$$\n\n$$\n\n\nWhere the notation  $A^{f}()$  stands for the non-uniform quantum algorithm which can make queries to the function  $f$  and can take any quantum state as input. We remark that if every function  $\\mathrm{PRF}_n$  admits a classical circuit of size  $s(n)$  and depth  $d(n)$ , then one can prepare the corresponding phase states using a quantum circuit of size  $\\mathcal{O}(s(n))$  and depth  $d(n) + 1$  Brakerski & Shmueli (2019). Moreover, the existence of such  $\\mathrm{PRF}_n$  is implied by the existence of quantum secure one-way functions Zhandry (2012).\n\n\n# 3.3.1 FULLY-QUANTUM LEARNABILITY WITH PSEUDORANDOM PHASE STATES\n\n\nNote that when we constrain the inputs of  $\\pi_x$  to phase states of pseudorandom functions, we essentially modify the distribution over input states in Eq. 4 and Eq. 6. This new distribution now only has support on phase states that are efficiently preparable. While Proposition 1 examines general quantum phase states as input states (which are not typically efficiently preparable), we note that the fully-quantum learnability directly extends the setting where we limit ourselves to efficiently preparable phase states as well. We summarize this observation in the following proposition (whose proof is the same as that of Proposition 1).\n\n\nProposition 2. Let  $S_{\\mathrm{pr}} = \\{|\\psi_{f^{(k)}}\\rangle^{\\otimes \\ell} \\mid f^{(k)}(.) = \\mathsf{PRF}_n(k,.)$ ,  $k \\in \\mathcal{K} \\}$ , where PRF is a quantum-secure pseudorandom function with keys  $\\mathcal{K}$ . The concept class in Definition 7 is  $(0,0,1)$ -fully-quantum learnable when the distribution over input states is uniform over  $S_{\\mathrm{pr}}$ .\n\n\n# 3.3.2 LIMITATIONS OF MEASURE-FIRST PROTOCOLS WITH PSEUDORANDOM PHASE STATES\n\n\nIn the last section, we discussed how the concept class in Definition 7 remains fully-quantum learnable when restricted to phase states of pseudorandom functions. Conversely, in this section we show that this concept class also remains not measure-first learnable when restricted to phase states of pseudorandom functions.\n\n\nTheorem 3. Let  $S_{\\mathrm{pr}} = \\{\\left|\\psi_{f^{(k)}}\\right\\rangle^{\\otimes \\ell} \\mid f^{(k)}(.) = \\mathsf{PRF}_n(k,.)$ ,  $k \\in \\mathcal{K}\\}$ , where PRF is a quantum-secure pseudorandom function with keys  $\\mathcal{K}$ . The concept class in Definition 7 is not  $(\\epsilon, \\delta, p_{\\mathrm{suc}})$ -measure-first learnable for  $(1 - \\epsilon) \\cdot (1 - \\delta) \\cdot p_{\\mathrm{suc}} > c$  for any constant  $c > 7/8$  when the distribution over input states is uniform over  $S_{\\mathrm{pr}}$ .\n\n\nThe main results of this section directly follows from combining Theorem 3 and Proposition 2.\n\n\nCorollary 1 (informal). If there exist quantum-secure pseudorandom functions, then there exists a quantum supervised learning problem with efficiently generatable quantum data, which cannot be learned by any measure-first protocol according to Def. 6 while there exist a fully-quantum protocol which satisfies the learning condition of Def. 4\n\n\nThe proof of Theorem 3 is provided in Appendix C, and we first present a concise overview of the proof here. The main idea behind the proof is to illustrate that if the concepts are measure-first learnable when restricted to pseudorandom phase states, then the corresponding measure-first learning protocol can be harnessed to create a non-uniform quantum algorithm that is able to distinguish between truly random functions and pseudorandom functions. More precisely, this \"distinguisher\" algorithm employs the measure-first learning protocol and evaluates its performance\n\n\nwhen applied to the phase state corresponding to the function it has been given oracular access to. In the proof of Theorem 2, we established an upper bound on the generalization performance of any measure-first protocol for truly random phase states. If, however, the measure-first protocol performs well on pseudorandom phase states, then the outcomes of the \"distinguisher\" algorithm would differ significantly based on whether it is given oracular access to a truly random or a pseudorandom function, thereby contradicting the pseudorandomness assumption. In other words, if the measure-first protocols were effective on pseudorandom states, there would be a clear difference in the performance of the distinguisher: poor accuracy when dealing with truly random functions and strong accuracy when dealing with pseudorandom functions. However, since this would violate Eq. 9, measure-first protocols have to fail when applied to pseudorandom functions.\n\n\n# 4 CONCLUSION\n\n\nIn our study, we explored the constraints and capabilities of learning from quantum data. We established a formal machine learning framework that contrasts two protocols: \"fully quantum\", which adjusts measurements based on data, and \"measure-first\" restricted by fixed initial (though arbitrarily powerful) measurements. In particular, we provided an example of a learning problem efficiently solved by a fully-quantum protocol but beyond the capabilities of measure-first protocols. Moreover, we showed that this persists even when we limit from universal quantum states, which include also those intractable to prepare, to efficiently preparable quantum states. These findings underscore the crucial role of processing unmeasured quantum data in machine learning, presenting a setting where quantum advantages arise. In particular, they imply that certain learning tasks inherently require the \"exponential capacity\" of quantum states, distinct from classical data. In other words, the number of bits needed to store  $n$  qubits, in a way that allows a learner to successfully solve the learning problem, is exponential in  $n$ . Such a conclusion is analogous to what Montanaro refers as \"anti-Holevo\" theorems Montanaro (2019). While our proof relies on both separations in one-way communication complexity and pseudorandom states, we highlight the potential for more general constructions. Instead of considering states demonstrating a one-way communication complexity separation, any quantum advice state used in quantum advice complexity classes that cannot be classically simulated with polynomial overhead could suffice, leveraging the separation between the classes FBQP/qpoly and FBQP/poly showed in Aaronson et al. (2023). In particular, by the result in Aaronson & Drucker (2010) any problem that can be efficiently solved with a polynomial-sized quantum advice state can also be solved with an advice state that is the ground state of a local Hamiltonian. This suggests that learning problems where the input quantum data consists of ground states of sufficiently complex local Hamiltonians are promising candidates for demonstrating a separation between measure-first and fully-quantum protocols. Finally, we note that beyond pseudorandom states one could use so-called \"computationally indistinguishable\" states, which are known to exist assuming the intractability of the graph isomorphism problem Kawachi et al. (2012), or various other complexity theoretic assumptions Brakerski et al. (2022).\n\n\n# REFERENCES\n\n- Scott Aaronson. Shadow tomography of quantum states. In Proceedings of the 50th annual ACM SIGACT symposium on theory of computing, pp. 325-338, 2018.\n\n- Scott Aaronson and Andrew Drucker. A full characterization of quantum advice. In Proceedings of the forty-second ACM symposium on Theory of computing, pp. 131-140, 2010.\n\n- Scott Aaronson, Harry Buhrman, and William Kretschmer. A qubit, a coin, and an advice string walk into a relational problem. arXiv:2302.10332, 2023.\n\n- Esma Aimeur, Gilles Brassard, and S\u00e9bastien Gambs. Machine learning in a quantum world. In Proceedings of the Advances in Artificial Intelligence: 19th Conference of the Canadian Society for Computational Studies of Intelligence, pp. 431-442. Springer, 2006.\n\n- Ziv Bar-Yossef, Thathachar S Jayram, and Jordanis Kerenidis. Exponential separation of quantum and classical one-way communication complexity. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, 2004.\n\n- Zvika Brakerski and Omri Shmueli. (pseudo) random quantum states with binary phase. In Theory of Cryptography Conference. Springer, 2019.\n\n- Zvika Brakerski, Ran Canetti, and Luowen Qian. On the computational hardness needed for quantum cryptography. arXiv preprint arXiv:2209.04101, 2022.\n\n- Sitan Chen, Jordan Cotler, Hsin-Yuan Huang, and Jerry Li. Exponential separations between learning with and without quantum memory. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pp. 574-585. IEEE, 2022a.\n\n- Sitan Chen, Jordan Cotler, Hsin-Yuan Huang, and Jerry Li. Exponential separations between learning with and without quantum memory. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), 2022b.\n\n- Hao-Chung Cheng, Min-Hsiu Hsieh, and Ping-Cheng Yeh. The learnability of unknown quantum measurements. arXiv:1501.00559, 2015.\n\n- Andreas Elben, Steven T Flammia, Hsin-Yuan Huang, Richard Kueng, John Preskill, Benoit Vermersch, and Peter Zoller. The randomized measurement toolbox. Nature Reviews Physics, 5, 2023.\n\n- Weiyuan Gong and Scott Aaronson. Learning distributions over quantum measurement outcomes. In International Conference on Machine Learning, pp. 11598-11613. PMLR, 2023.\n\n- Daniel Grier, Hakop Pashayan, and Luke Schaeffer. Sample-optimal classical shadows for pure states. arXiv:2211.11810, 2022.\n\n- Hsin-Yuan Huang, Richard Kueng, and John Preskill. Predicting many properties of a quantum system from very few measurements. Nature Physics, 16(10):1050-1057, 2020a.\n\n- Hsin-Yuan Huang, Richard Kueng, and John Preskill. Predicting many properties of a quantum system from very few measurements. Nature Physics, 16, 2020b.\n\n- Hsin-Yuan Huang, Richard Kueng, Giacomo Torlai, Victor V Albert, and John Preskill. Provably efficient machine learning for quantum many-body problems. Science, 377, 2022.\n\n- Sofiene Jerbi, Joe Gibbs, Manuel S Rudolph, Matthias C Caro, Patrick J Coles, Hsin-Yuan Huang, and Zo\u00eb Holmes. The power and limitations of learning quantum dynamics incoherently. arXiv:2303.12834, 2023.\n\n- Akinori Kawachi, Takeshi Koshiba, Harumichi Nishimura, and Tomoyuki Yamakami. Computational indistinguishability between quantum states and its cryptographic application. Journal of cryptology, 25:528-555, 2012.\n\n- Yunchao Liu, Srinivasan Arunachalam, and Kristan Temme. A rigorous and robust quantum speed-up in supervised machine learning. Nature Physics, 2021.\n\n- Ashley Montanaro. Quantum states cannot be transmitted efficiently classically. Quantum, 3, 2019.\n\n- Michael A Nielsen and Isaac L Chuang. Quantum computation and quantum information. Cambridge university press, 2010.\n\n- Ryan Sweke, Jean-Pierre Seifert, Dominik Hangleiter, and Jens Eisert. On the quantum versus classical learnability of discrete distributions. Quantum, 5:417, 2021.\n\n- Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.\n\n- Mark Zhandry. How to construct quantum random functions. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science. IEEE, 2012.\n\nResponse: ### Prediction: \\(\\boxed{Accept}\\)\n\n#### Justification:\n\n**Originality:** \nThe paper presents a novel and insightful analysis of the limitations of measure-first protocols in quantum machine learning. It addresses an underexplored area by combining randomized measurement protocols with quantum machine learning techniques. The authors develop a specific learning problem that demonstrates the impossibility of achieving the same learning efficiency with measure-first protocols as with fully-quantum protocols. This originality is evident in the detailed theoretical framework and the concrete learning problem presented.\n\n**Methodology Rigor:**\nThe paper provides a rigorous mathematical foundation for the learning problem and the protocols. Definitions are clearly stated and supported by formal proofs. Key lemmas and propositions are provided in appendices, ensuring reproducibility and validation of the claims. The authors use formal definitions like the concept class, fully-quantum protocols, and measure-first protocols, which are well-defined and consistently applied throughout the paper. \n\n**Significance:**\nThe findings are highly significant in the field of quantum machine learning. They reveal fundamental limitations of current approaches and suggest directions for future research. The learning separation result is particularly impactful as it shows that certain tasks inherently require the full quantum capabilities, which is a stark contrast to classical machine learning settings. The implications extend beyond quantum machine learning to areas like distributed sampling and relational problems, highlighting the broader relevance of the results.\n\n**Clarity:**\nThe paper is well-written and easy to follow. The authors use clear language and provide intuitive explanations alongside formal definitions and proofs. The structure is logical, with each section building on the previous one, making the overall flow smooth. The presentation of the results and their implications is compelling and convincing.\n\n**Alignment with Field Standards:**\nThe paper adheres closely to the standards of theoretical research in quantum information and machine learning. It references existing literature comprehensively and provides a solid grounding for its claims. The use of advanced concepts like pseudorandom functions and communication complexity aligns with the rigorous nature of the field.\n\n### Strengths:\n1. **Novel Learning Problem:** The paper introduces a specific learning problem that clearly highlights the differences between measure-first and fully-quantum protocols.\n2. **Rigorous Definitions and Proofs:** Clear and consistent definitions of key concepts and detailed proofs are provided, ensuring the results are well-grounded.\n3. **Implications Beyond Quantum ML:** The findings have broad implications for various areas of quantum information and machine learning.\n4. **Comprehensive Literature Review:** The paper cites extensively and provides a thorough background, making it accessible to experts in the field.\n5. **Clear Structure and Presentation:** The paper is well-organized and easy to follow, with intuitive explanations complementing formal details.\n\n### Weaknesses:\n1. **Technical Depth:** Some sections, particularly those involving communication complexity and pseudorandom functions, may be challenging for readers without a strong background in quantum information theory.\n2. **Experimental Robustness:** While the theoretical results are robust, the experimental aspects might benefit from further discussion or demonstration.\n\n### Conclusion:\nGiven the originality, rigor, significance, and clarity of the paper, along with its potential impact on the field, it is highly likely to be accepted for publication at ICLR. The authors have made a substantial contribution to the understanding of quantum machine learning and have provided a strong foundation for future research.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634853.0539587", "msecs": "53.0", "relativeCreated": "197877.43282318115", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research papers for acceptance. Assess based on originality, methodology rigor, significance, clarity, and alignment with field standards. Provide a structured prediction with confidence level (High/Medium/Low), detailing evidence-based rationale. Systematically weigh strengths and weaknesses, ensuring objectivity and adherence to peer review principles.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models\n\n# Abstract\nWhile large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing methods. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, demonstrates the superiority and versatility of LATS for both reasoning and acting. In particular, LATS achieves 94.4\\% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop, demonstrating the effectiveness and generality of our method.\n\n\n# 1 INTRODUCTION\n\n\nGeneral autonomous agents capable of reasoning and decision-making in a variety of environments (Wooldridge & Jennings, 1995) have been of longstanding interest in the field of artificial intelligence. While this has traditionally been studied in reinforcement learning, the recent rise of large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; OpenAI, 2023) with strong reasoning and general adaptability offers an alternative paradigm. Not only have LLMs excelled on standard NLP tasks such as text summarization (Nallapati et al., 2016) or natural language inference (Bowman et al., 2015), but they have been adapted to an increasingly diverse set of tasks that often require advanced common-sense reasoning or quantitative skills (Cobbe et al., 2021; Saparov & He, 2022). LLMs are also capable of performing in complex environments that involve knowledge and reasoning, such as web navigation (Yao et al., 2022; Deng et al., 2023), tool-use (Schick et al., 2023), or open-ended games (Fan et al., 2022).\n\n\nReasoning and acting abilities have also been improved by prompting techniques that augment LLMs with feedback or observations from an external environment (Yao et al., 2023b; Gao et al., 2022; Shinn et al., 2023). This eliminates the need to rely entirely on the base abilities of the Language Model (LM), enhancing it through external tools or semantic feedback. Despite this strength, these methods are reflexive and fall short of humans' deliberate and thoughtful decision-making characteristics to solve problems (Sloman, 1996; Evans, 2010). In particular, such methods fail to consider multiple reasoning paths or to plan ahead. Recent search-guided LLM works (Xie et al., 2023; Yao et al., 2023a; Hao et al., 2023) address this\n\n\n![Figure 1: An overview of LATS. LATS uses an external environment and self-reflection to improve reasoning and decision-making. Prior states, values, and reflections are stored in external memory.](images/cc628a678fe24b4a13746b69096509f9f561c1bd0ece32cfc17a423a2e48ed1c.jpg)\n\n\n<table><tr><td>Approach</td><td>Reasoning</td><td>Acting</td><td>Planning</td><td>Self Reflection</td><td>External Memory</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>\u2713</td><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td></tr><tr><td>ReAct (Yao et al., 2023b)</td><td>\u2713</td><td>\u2713</td><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td></tr><tr><td>ToT (Yao et al., 2023a)</td><td>\u2713</td><td>\u00d7</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><td>RAP (Hao et al., 2023)</td><td>\u2713</td><td>\u00d7</td><td>\u2713</td><td>\u00d7</td><td>\u2713</td></tr><tr><td>Self-Refine (Madaan et al., 2023)</td><td>\u2713</td><td>\u00d7</td><td>\u00d7</td><td>\u2713</td><td>\u00d7</td></tr><tr><td>Beam Search (Xie et al., 2023)</td><td>\u2713</td><td>\u00d7</td><td>\u00d7</td><td>\u2713</td><td>\u00d7</td></tr><tr><td>Reflexion (Shinn et al., 2023)</td><td>\u2713</td><td>\u2713</td><td>\u00d7</td><td>\u2713</td><td>\u2713</td></tr><tr><td>LATS (Ours)</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></table>\n\n\nTable 1: A summary of related work on reasoning, acting, and planning. LATS is the first work incorporating designs from all three domains, allowing use in all corresponding tasks. We refer to planning as the use of a search algorithm, self-reflection as the use of LM-generated feedback, and external memory as storaging past text context for future updates of solution.\n\n\nissue by searching over multiple reasoning chains. While these methods enable planning, these methods operate in isolation and do not incorporate external feedback that can improve reasoning.\n\n\nTo help address these issues, we propose LATS (Language Agent Tree Search), a general framework for decision-making and reasoning with language models. LATS unifies LM planning, acting, and reasoning strategies by expanding ReAct (Yao et al., 2023b) into a search over a combinatorial space of possible reasoning and acting steps. We adapt Monte Carlo tree search (MCTS) from model-based reinforcement learning (Silver et al., 2017; Anthony et al., 2017; Jiang et al., 2018) to language agents, repurposing a pretrained LLM as an agent, value function, and optimizer. Utilizing the strong natural language understanding and in-context learning ability of modern LMs, we use text as an interface between each component of the framework, allowing LATS to adapt planning to environmental conditions without additional training. To the best of our knowledge, LATS is the first framework that combines reasoning, acting, and planning to enhance LLMs. Notably, LATS doubles the performance of GPT-3.5 on HotPotQA (Yang et al., 2018) over ReAct (Yao et al., 2023b) and raises the average score by 22.1 on WebShop (Yao et al., 2022). When used with GPT-4, LATS achieves a 94.4 Pass@1 rate for programming on HumanEval (Chen et al., 2021), setting the state of the art. To summarize, our contributions are the following:\n\n- - We introduce an LM-based Monte Carlo tree search variant to deliberately construct the best trajectory from sampled actions, enabling more flexible and adaptive problem-solving compared to reflexive prompting methods. This is guided by heuristics from the LM.\n\n- - By integrating external feedback and self-reflection, LATS enhances model sensibility and enables agents to learn from experience, surpassing reasoning-based search methods.\n\n- - Through experiments across diverse domains like programming, interactive QA, and web navigation, we demonstrate the versatility of LATS in harnessing LLMs for autonomous reasoning and decision-making.\n\n\n# 2 RELATED WORK\n\n\nLLMs for reasoning. For LLMs, reasoning typically involves decomposing complex inputs into sequential intermediate steps towards a final answer (Cobbe et al., 2021), demonstrated with Chain-of-Thought (CoT) prompting (Wei et al., 2022) and its variants (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022). However, these methods, which create chains autoregressively in a single step, often suffer from error propagation as the number of steps increases (Guo et al., 2018; Chen et al., 2022b) due to compound errors. Various advancements aim to mitigate this issue; some approaches, such as Self-Consistency (Wang et al., 2022), employ majority voting over sampled chains, while others focus on multi-step decomposition, such as least-to-most prompting (Zhou et al., 2022), or use of external tools such as a scratchpad (Nye et al., 2021) or compiler (Gao et al., 2022). Recently, CoT has been improved with search algorithms (Yao et al., 2023a; Hao et al., 2023; Besta et al., 2023) that can sample trajectories more effectively. Tree-of-thought (ToT) prompting (Yao et al., 2023a) uses DFS or BFS-based search guided by an LM-generated heuristic while Reasoning via Planning (RAP) (Hao et al., 2023) uses MCTS with rollouts simulated by the LM. However, they rely solely on LM internal knowledge and cannot adapt to useful external feedback.\n\n\n![a) Tree-of-Thoughts](images/f2521c9e11cbdc323def5cfd3308e7423d9fbb419347330140e74875d9364496.jpg)\n\n\n![b) Reasoning via Planning](images/31a350608c30e11d47d2af14bb85498ceb2957cc4532131838a2b8f121b57942.jpg)\n\n\n![c) Language Agent Tree Search Figure 2: An overview of the differences between LATS and recently proposed LM search algorithms ToT (Yao et al., 2023a) and RAP (Hao et al., 2023). LATS leverages environmental feedback and self-reflection to further adapt search and improve performance.](images/9c6e73fb5e33533411f8bdd98d2e3a148cb36cb5e950020eac40df4696df4ed2.jpg)\n\n\nLLMs for acting. The strong reasoning and common-sense abilities of LLMs have also been adapted for decision-making or acting tasks as a policy model in interactive environments. In the realm of robotics LLMs have been employed as high-level controllers of control policies (Ahn et al., 2022; Huang et al., 2022; Driess et al., 2023). Similar work (Baker et al., 2022; Wang et al., 2023; Zhu et al., 2023) has also adapted LLM agents to complex multimodal games such as Minecraft (Guss et al., 2019; Fan et al., 2022). LLMs are particularly useful in text-based environments (Liu et al., 2018; Shridhar et al., 2020; Liu et al., 2023), where acting-based prompting techniques such as ReAct (Yao et al., 2023b) have seen success. Similar to CoT, ReAct is limited by its simplicity and cannot effectively adapt to environment conditions. Many extensions have been proposed to address this, including Self-refine (Madaan et al., 2023) and Reflexion (Shinn et al., 2023; Yao et al., 2023c), which uses self-reflection to enhance reasoning and decision-making, and AdaPlanner (Sun et al., 2023), which incorporates both positive and negative environmental feedback. However these methods focus on refining an individual plan or trajectory and do not consider alternative choices at each step. In addition, recent work (Huang et al., 2023) has suggested LLMs cannot self-correct their internal reasoning, making it critical to use external feedback. Alternatively to pure decision-making environments, the reasoning and practical abilities of LLMs have been enhanced by access to external tools, such as APIs, search engines, calculators, or other models (Schick et al., 2023; Shen et al., 2023; Sur\u00eds et al., 2023). Contrary to reasoning-based approaches, these methods have not been improved with planning, limiting their effectiveness. We summarize them in Tab. 1.\n\n\nTree-based search. Tree-based search, where multiple branches of outcomes are explored during search, is widely used in many planning algorithms (\u015awiechowski et al., 2023; LaValle et al., 2001) and Reinforcement Learning (RL) (Hafner et al., 2019; Du et al., 2023; Wu et al., 2023) algorithms for its good exploration-exploitation trade-off. Though tree-based search requires an environment model that can expand from arbitrary state (Vodopivec et al., 2017), which often requires extra training in RL (Hafner et al., 2023), such problem does not exist for LM tasks as we can conveniently backup to any state by setting the input to be the context and corresponding previous output by the LM. Thus, we work on the tree-based framework and use MCTS (\u015awiechowski et al., 2023) to fully release the potential of LMs, while avoiding the cost of training a value function over language descriptions by leveraging the in-context learning (Brown et al., 2020) abilities of LLMs.\n\n\n# 3 PRELIMINARIES\n\n\n# 3.1 PROBLEM SETTING AND PROMPTING\n\n\nBefore describing LATS, we first define our problem and outline a few established methods that leverage large language models for reasoning or decision-making. In LM reasoning or decision mak\n\n\ning, we are given an input  $x$  in natural language and a pretrained language model  $p_{\\theta}(x)$  parameterized by  $\\theta$ ; our goal is to generate a final output  $y \\sim p_{\\theta}(x)$  corresponding to the answer (reasoning) or completes the task (decision-making). Both  $x$  and  $y$  are language sequences, which are comprised of a list of tokens (the basic elements of natural language, often words), denoted as  $x = (x[1], \\ldots, x[n])$  and  $y = (y[1], \\ldots, y[n])$ . The LM decodes text autoregressively, i.e., without other inputs, the probability for an LM to generate a sequence  $x$  is given by  $p_{\\theta}(x) = \\prod_{i=1}^{n} p_{\\theta}(x[i] | x[1 \\ldots i-1])$ . Usually, to improve the LM, prompts are provided along with the input  $x$ , which are specific instructions or few-shot input-output examples. We denote the generic process where an input  $x$  is transformed into an output  $y$  by LM:  $y \\sim p_{\\theta}(y | \\text{prompt}_{IO}(x))$ , where  $\\text{prompt}_{IO}(x)$  denotes the input  $x$ .\n\n\nChain-of-thought (CoT) Prompting (Wei et al., 2022) was introduced to cater to scenarios where direct mapping from  $x$  to  $y$  is intricate, such as when  $x$  is from a mathematical query or challenging question. This method hinges on creating thoughts  $z_{1},\\ldots ,z_{n}$  that act as stepping stones between  $x$  and  $y$ ; each thought  $z_{i}$  is a language sequence. To employ CoT prompting, thoughts are extracted sequentially as  $z_{i}\\sim p_{\\theta}^{CoT}(z_{i}|x,z_{1\\dots i - 1})$ , with the final output being  $y\\sim p_{\\theta}^{CoT}(y|x,z_{1\\dots n})$ .\n\n\nTree-of-thought (ToT) Prompting (Yao et al., 2023a) extends CoT prompting by exploring multiple reasoning paths over thoughts. It frames problems as a search over a tree where each node  $s = [x, z_{1:i}]$  represents a partial solution state comprising the original input  $x$  and thought sequence  $z_{1\\dots i}$ . Thoughts  $z_{i}$  are generated by proposal or sampling with CoT  $z_{i} \\sim p_{\\theta}^{CoT}(z_{i}|x,z_{1\\dots i-1})$ . Deliberate search algorithms like breadth-first or depth-first search are used to systematically explore the tree, guided by heuristics based on language model evaluations  $V(s)$  of each state.\n\n\nReasoning via Planning (RAP) (Hao et al., 2023) is similar to ToT, except that MCTS is used over DFS or BFS. Heuristics are designed from an LM, such as the likelihood or confidence of an action, and the LM is used as a world model to predict subsequent states during the simulation step.\n\n\nReAct (Yao et al., 2023b) extends language models to tasks where the mapping from  $x$  to  $y$  is enhanced by or requires interactions with an external environment, such as a game or API. This technique constructs an action space  $\\hat{A} = A \\cup Z$  that adds permissible actions  $a$  to the reasoning traces  $z$  from CoT. Observations  $o$  from the environment are used to improve both reasoning and acting. To solve problems with ReAct, after each observation, actions are generated from  $p_{\\theta}$  sequentially as  $a_i \\sim p_{\\theta}^{ReAct}(a_i | x, o_{1\\dots i-1}, a_{1\\dots i-1})$ , with the final output being  $y \\sim p_{\\theta}^{ReAct}(y | x, o_{1\\dots n}, a_{1\\dots n})$ .\n\n\nWhile the previously described prompting techniques improve LM performance on reasoning tasks, they falter on difficult tasks that involve multifaceted decision-making due to several shortcomings: 1) Flexibility: Base prompting methods (CoT or ReAct) autoregressively sample from the LM, neglecting potential alternative continuations from specific states. 2) Sensibility: Reasoning-based methods (CoT, RAP, or ToT) rely solely on the internal representations of the LM and cannot consider external observations. This dependency risks fact hallucination and error propagation while setting a performance ceiling. 3) Adaptability: Current planning frameworks (RAP or ToT) use simple search algorithms such as BFS or cannot leverage environmental feedback to improve planning. Additionally, the agent is static and cannot reuse previous experience or learn from trial and error. While RAP also adopts MCTS, it is constrained to tasks where the LM can become a world model and accurately predict states. These shortcomings limit the ability of LMs to be deployed as general problem-solving agents and form the motivation for LATS.\n\n\n# 3.2 MONTE-CARLO TREE SEARCH (MCTS)\n\n\nMonte-Carlo Tree Search (MCTS) is a heuristic search algorithm that is proved successful on many decision-making environments such as Atari (Ye et al., 2021) and Go (Silver et al., 2016). MCTS builds a decision tree where every node in the tree is a state and edge is an action. MCTS runs for  $k$  episodes; for each episode, it starts from the root (i.e., initial state) and iteratively conducts two steps to expand the tree: 1) Expansion, where multiple children states  $s$  are explored from the current parent state  $p$  by sampling  $n$  actions, and 2) Selection, where the children with the highest UCT (Upper Confidence bounds applied to Trees) (Kocsis & Szepesv\u00e1ri, 2006) value is selected by the next iteration. The UCT of a child state  $s$  is calculated as follows:\n\n\n$$\n\n$$\nU C T (s) = V (s) + w \\sqrt {\\frac {\\ln N (p)}{N (s)}}, \\tag {1}\n$$\n\n$$\n\n\n![Figure 3: An overview of the six operations of LATS. A node is selected, expanded, evaluated, then simulated until a terminal node is reached, then the resulting value is backpropagated. If the trajectory fails, a reflection is generated and used as additional context for future trials. These operations are performed in succession until the budget is reached or task is successful.](images/6d42d9cca60751b3dec25def2ea12bb16ebdf2eb288e31a3c9d76dae558f3c03.jpg)\n\n\nwhere  $N(s)$  is the number of visits to a node  $s$ ,  $V(s)$  is the value function (expected return) from the subtree of  $s$ ,  $w$  is the exploration weight, and  $p$  is the parent node of  $s$ . The child node with the highest UCT value is selected for expansion in the next iteration. When the end of an episode is reached, a backpropagation is carried out: the return  $r$  is used for updating every  $V(s)$  along the path with the formula  $V(s) = \\frac{V_{\\mathrm{old}}(s)(N(s) - 1) + r}{N(s)}$ , where  $V_{\\mathrm{old}}(s)$  is the old value function. Normally, the major shortcoming of MCTS is that it requires an environment model to undo previous steps and form a searching tree, which is often a strong assumption. However, such a limitation does not exist for LMs, as we can conveniently reset to any step by simply copy-pasting historical text input. Such a special property is the key motivation of our work.\n\n\n# 4 UNIFYING PLANNING, REASONING, AND ACTING\n\n\n# 4.1 LM AGENT\n\n\nLATS supports sequential reasoning or decision-making tasks on the basis of ReAct. At time step  $t$ , an agent receives an observation  $o_t \\in O$  from the environment and takes an action  $a_t \\in A$  following some policy  $\\pi(a_t | x, o_1 \\dots i-1, a_1 \\dots i-1)$ , where  $x$  consists of the task instruction and a number of few-shot examples. We initialize the agent with  $p_\\theta$  to leverage the useful language representations of an LM as a base decision-maker. We follow the ReAct instantiation in which the action space  $\\hat{A} = A \\cup Z$  consists of both the space of permissible actions  $A$  and language space of reasoning traces  $Z$ . Actions directly affect the environment and result in observation, while thoughts are used to formalize decisions by organizing information, planning future actions, or injecting internal knowledge. The exact instantiation of the action space depends on the particular environment; for decision-making tasks actions might consist of commands on a website while for reasoning tasks the action space might be limited to a few external tools or APIs.\n\n\nInstead of greedily decoding one trajectory or solution, we sample  $n$  actions from  $p_{\\theta}$  using the current state. This is based on the intuition that for complex decision-making tasks, there is likely to be a range of potential trajectories or reasoning paths that are correct (Evans, 2010). Sampling a diverse set of candidates at each step mitigates the stochastic nature of LM text generation and enables greater exploration in both the decision-making and reasoning space. We wrap  $p_{\\theta}$  within our proposed search algorithm to deliberately construct the best trajectory from sampled actions.\n\n\n# 4.2 LATS\n\n\nThe main component of LATS is a search algorithm that controls the overall problem-solving process with deliberate planning. To find the most promising trajectory and systemically balance exploration with exploitation, we adopt a variant of Monte Carlo Tree Search (MCTS) that frames decision-making as a tree search, in which each node  $s = [x, a_{1\\dots i}, o_{1\\dots i}]$  represents a state comprising the original input  $x$ , action sequence  $a_{1\\cdot i}$ , and observation sequence  $o_{1\\cdot i}$ .\n\n\nTo adapt MCTS for language agents, LATS repurposes  $p_{\\theta}$  as an agent, state evaluator, and feedback generator, leveraging the useful language priors of modern LMs to facilitate planning. While standard MCTS and RAP Hao et al. (2023) rely on internal dynamics models to facilitate simulation, LATS is model-free and uses environment interaction. LATS consists of a series of operations, selection, expansion, evaluation, simulation, backpropagation, and reflection, performed in succession until the task is successfully completed or a computational limit is reached. The full psuedocode of LATS can be found in Sec. A in the Appendix.\n\n\nSelection. In the first operation, the algorithm identifies a segment of the current tree most suitable for subsequent expansion. Starting from the root node, denoted as the initial state  $s_0$ , a child node is selected at each tree level until a leaf node is reached. To balance exploration and exploitation, we use the UCT algorithm as shown in Eq. 1.\n\n\nExpansion. After selecting a node, the second operation expands the tree by sampling  $n$  actions from  $p_{\\theta}$ , as described in the prior section. The environment receives each action and returns corresponding feedback as an observation. This results in  $n$  new child nodes added to the tree. This tree is stored in an external long-term memory structure.\n\n\nEvaluation. The third operation assigns a scalar value to each new child node to be used for selection and backpropagation. This value effectively quantifies the agent's progress in task completion, serving as a heuristic to steer the search algorithm towards the most promising regions of the tree. Following Yao et al. (2023a) we repurpose  $p_{\\theta}$  into a value function by prompting it to reason about a given state. To obtain a scalar value, we instruct  $p_{\\theta}$  to end its reasoning trace with a score indicating the correctness of the trajectory. This method offers enhanced flexibility over programmed heuristics (Campbell et al., 2002) and greater efficiency than learned heuristics (Silver et al., 2017).\n\n\nSimulation. The fourth operation expands the currently selected node until a terminal state is reached. At each depth level we sample and evaluate nodes with the same operations, but prioritize nodes of highest value. Reaching a terminal state provides objective feedback on the correctness of a trajectory. If the task is completed successfully, then LATS terminates the search. If the solution is partially successful or unsuccessful, then we perform two additional operations as described below.\n\n\nBackpropagation. This operation updates the values of the tree based on the outcome of a trajectory. For each node  $s_0, s_1, \\ldots, s_n$  in the trajectory from root (initial state  $s_0$ ) of the searching tree to leaf (terminal state  $s_n$ ), its value is updated to reflect the outcome of the simulation by  $N(s_i) = N_{\\mathrm{old}}(s_i) + 1$  and  $V(s_i) = \\frac{r + N_{\\mathrm{old}}(s_i)V_{\\mathrm{old}}(s_i)}{N(s_i)}$ , where  $r$  is the return and  $N_{\\mathrm{old}}$ ,  $V_{\\mathrm{old}}$  are the old number of visits and value function. These updated values are used in the UCT formula (Eq. 1) to guide the selection of the next node for exploration.\n\n\nReflection. In addition to the environmental feedback, we also leverage self-reflection to further refine the decision-making process (Shinn et al., 2023; Madaan et al., 2023). Upon encountering an unsuccessful terminal node,  $p_{\\theta}$  is prompted with the trajectory and final reward to provide a verbal self-reflection that summarizes the errors in the reasoning or acting process and proposes superior alternatives. We store both failed trajectories and corresponding reflections in the memory. In subsequent iterations, these are integrated as additional context to the agent and value function, refining both through in-context learning. This imparts a semantic gradient signal more useful than a scalar value, enabling the agent to learn from trial and error without the cost of expensive optimization processes such as reinforcement learning.\n\n\nConceptually, LATS has the following advantages as a general framework for reasoning and decision-making with LM agents. (1) Generality: LATS supports both reasoning and decision-making tasks by defining a shared space of thoughts and actions. (2) Deliberate: The use of MCTS and LM value function ensures a principled search that selects options with high value while explor\n\n\n<table><tr><td>Prompt Method</td><td>HotpotQA (EM)</td></tr><tr><td>I/O</td><td>0.32</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>0.34</td></tr><tr><td>CoT - SC (Wang et al., 2022)</td><td>0.38</td></tr><tr><td>ToT (Yao et al., 2023a)</td><td>0.55</td></tr><tr><td>RAP (Hao et al., 2023)</td><td>0.60</td></tr><tr><td>RAP (n = 10)</td><td>0.60</td></tr><tr><td>LATS (CoT)</td><td>0.60</td></tr></table>\n\n\n<table><tr><td>Prompt Method</td><td>HotpotQA (EM)</td></tr><tr><td>ReAct (Yao et al., 2023b)</td><td>0.32</td></tr><tr><td>ReAct (best of k)</td><td>0.38</td></tr><tr><td>Reflexion (Shinn et al., 2023)</td><td>0.51</td></tr><tr><td>LATS</td><td>0.61</td></tr><tr><td>LATS (n = 1)</td><td>0.48</td></tr><tr><td>LATS (n = 3)</td><td>0.56</td></tr><tr><td>LATS (n = 10)</td><td>0.64</td></tr><tr><td>LATS (CoT + ReAct)</td><td>0.71</td></tr></table>\n\n\nTable 2: GPT-3.5 reasoning-based prompting (left) and acting-based prompting (right) results on HotpotQA. LATS achieves the highest exact match (EM) for acting and is competitive on reasoning. Unless otherwise specified, we sample  $n = 5$  nodes during expansion and  $k = 50$  trajectories.\n\n\ning promising alternatives. (3) Adaptability: LATS is designed around the use of external feedback through observations and self-reflection, enabling greater adaptation during problem-solving. (4) Flexibility: LATS can accommodate different scenarios, environments, and resource stipulations by modifying state design and tree dimensions. (5) Modularity: The base LM agent, reflection generator, and value function can be independently altered and adapted to individual LM properties.\n\n\n# 5 EXPERIMENTS\n\n\nTo demonstrate the general applicability of LATS, we evaluate our method on a variety of decision-making domains that requires both reasoning and acting ability: programming (Chen et al., 2021; Austin et al., 2021), HotPotQA (Yang et al., 2018), and WebShop (Yao et al., 2022).\n\n\n# 5.1 HOTPOTQA\n\n\nFor a task that can be approached with both reasoning-based and acting-based strategies, we consider HotPotQA (Yang et al., 2018), a multi-hop question-answering benchmark that requires retrieval over two or more Wikipedia passages. For the action space, in addition to LM thoughts we follow the setup from Yao et al. (2023b), which provides the agent with API calls to search and lookup information. The output of these API calls and self-generated reflections form the observation space. We use a subset of 100 questions and three few-shot examples for each method. For ToT, we use DFS as the base search algorithm and scoring with the LM as the heuristic. For all methods that involve sampling, including LATS, we sample  $k = 50$  trajectories. More details and prompts can be found in Sec. D and Sec. E in the Appendix.\n\n\nWe evaluate internal reasoning strategies by removing actions and observations from the context, corresponding to CoT (Wei et al., 2022) and its variants, CoT-SC (Wang et al., 2022), ToT (Yao et al., 2023a), and RAP (Hao et al., 2023). These methods rely solely on the agent's existing knowledge to answer the question. We also consider acting-based methods ReAct, Reflexion, and LATS, which augment the agent with the interactive API environment and primarily evaluate its information retrieval abilities. While LATS is designed for scenarios where external feedback can enhance reasoning, we also implement a reasoning-only version with CoT as the base prompt. We also combine internal and external reasoning in LATS by first prompting with a CoT-based prompt, then switching to a ReAct-based prompt upon failure. This is closer to how humans might approach this task, by using tools to lookup additional information only when the answer is not already known.\n\n\nResults. We observe in Tab. 2 that both internal reasoning and external retrieval strategies perform well on HotPotQA. Due to their large-scale training corpus, modern LLMs already encode factual knowledge and can often directly answer the question correctly. While CoT can slightly enhance performance on questions requiring reasoning, larger gains are observed with search methods ToT and RAP, which can sample and explore more outputs. We observe similar results for acting-based methods. LATS surpasses ReAct, even when sampling the same number of trajectories, by expanding more nodes with principled search (see Fig. 5 in Appendix D for a qualitative sample). This is demonstrated when modifying  $n$ , the number of nodes expanded during each iteration. Increasing  $n$\n\n\n<table><tr><td>Prompt Method</td><td>Model</td><td>Pass@1</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>GPT-3.5</td><td>46.9</td></tr><tr><td>ReAct (Yao et al., 2023b)</td><td>GPT-3.5</td><td>56.9</td></tr><tr><td>Reflexion (Shinn et al., 2023)</td><td>GPT-3.5</td><td>68.1</td></tr><tr><td>ToT (Yao et al., 2023a)</td><td>GPT-3.5</td><td>54.4</td></tr><tr><td>RAP (Hao et al., 2023)</td><td>GPT-3.5</td><td>63.1</td></tr><tr><td>LATS (Ours)</td><td>GPT-3.5</td><td>86.9</td></tr><tr><td>I/O</td><td>GPT-4</td><td>80.1</td></tr><tr><td>Reflexion</td><td>GPT-4</td><td>91.0</td></tr><tr><td>LATS</td><td>GPT-4</td><td>94.4</td></tr></table>\n\n\n<table><tr><td>Prompt Method</td><td>Pass@1</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>54.9</td></tr><tr><td>ReAct (Wei et al., 2022)</td><td>67.0</td></tr><tr><td>Reflexion (Shinn et al., 2023)</td><td>70.0</td></tr><tr><td>ToT (Yao et al., 2023a)</td><td>65.8</td></tr><tr><td>RAP (Hao et al., 2023)</td><td>71.4</td></tr><tr><td>LATS (Ours)</td><td>81.1</td></tr></table>\n\n\nTable 3: GPT-3.5 and GPT-4 Pass@1 accuracy on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). Prompting with LATS achieves the highest performance. We sample 5 solutions during expansion for 8 iterations.\n\n\ncan consistently improve performance, although at greater computational and inference costs. LATS is also competitive to RAP on internal reasoning but performs worse than acting. Combining internal and external reasoning in LATS results in the highest performance, indicating the importance of external feedback in augmenting reasoning even in tasks the base LM can already perform.\n\n\n# 5.2 PROGRAMMING\n\n\nTo demonstrate the importance of external observations for complex reasoning tasks, we evaluate the baselines and LATS on programming with Humaneval (Chen et al., 2021) and MBPP (Austin et al., 2021). Both datasets measure the correctness of synthesized programs in Python from natural language docstrings. We use individual solutions as the action space and test suite and compiler feedback as the external observation. We follow Chen et al. (2022a) and use an LLM to generate a synthetic test suite of syntactically valid \"assert\" statements for each question. For each step, the solution is evaluated on this test suite, and the results including successful and failed tests and compiler output, are added to the context as an observation. We use the same test suite for Reflexion.\n\n\nFor this task, the reasoning and acting baselines share an action space, but acting methods are able to incorporate observations as additional context. For LATS, since each action corresponds to a complete solution, we skip the simulation step of LATS and directly use the percentage of passed tests as the backpropagated reward. We use  $k = 8$  iterations, set the number of generated tests at 4, and sample  $n = 5$  solutions during expansion. After the search is completed, we select the solution with the highest value and evaluate it on the real test suite for the pass@1 accuracy evaluation. More details and prompts can be found in Sec. D and Sec. F in the Appendix.\n\n\nResults. We find in Tab 3 that both search and semantic feedback are crucial for better performance. Despite not using observations, ToT and RAP are competitive with Reflexion. LATS has the highest performance on both datasets. Since RAP uses a similar search algorithm as LATS, this reveals the importance of external feedback for difficult reasoning tasks such as programming. With GPT-4, using LATS sets the state of the art for HumanEval, showing LATS can be used with more advanced LLMs for higher performance.\n\n\n# 5.3 WEBSHOP\n\n\nFor a complex decision-making environment with practical applications, we consider WebShop (Yao et al., 2022), an online shopping environment composed of a website with 1.18M real-world products and 12k human instructions. Agents must navigate a website through a variety of commands to purchase an item matching a user specification. We use the preconstructed action space of search and click commands and browser feedback and reflections for the observation. The performance is gauged using two metrics: an average score, reflecting the percentage of user-specified attributes met by the selected product, and a success rate, indicating the frequency with which the chosen product fulfills all given conditions. We compare against acting-based prompting methods and RL-based approaches. We evaluate on 50 instructions, expand  $n = 5$  children for LATS, and set  $k = 30$  for LATS, ReAct best of  $k$ , and Reflexion. More details and prompts are in Appendix D and G.\n\n\n<table><tr><td>Method</td><td>Score</td><td>SR</td></tr><tr><td>ReAct (Yao et al., 2023b)</td><td>53.8</td><td>28.0</td></tr><tr><td>ReAct (best of k)</td><td>59.1</td><td>32.0</td></tr><tr><td>Reflexion (Shinn et al., 2023)</td><td>64.2</td><td>35.0</td></tr><tr><td>LATS</td><td>75.9</td><td>38.0</td></tr><tr><td>IL Yao et al. (2022)</td><td>59.9</td><td>29.1</td></tr><tr><td>IL+RL Yao et al. (2022)</td><td>62.4</td><td>28.7</td></tr><tr><td>Fine-tuning (Furuta et al., 2023)</td><td>67.5</td><td>45.0</td></tr><tr><td>Expert</td><td>82.1</td><td>59.6</td></tr></table>\n\n\nTable 4: Score and success rate (SR) on Webshop, separated into prompting, training, and human performance. For the same number of iterations, LATS improves both score and success rate and surpasses RL-based training.\n\n\n<table><tr><td>Prompt Method</td><td>HotPotQA</td></tr><tr><td>ReAct (best k = 250)</td><td>0.42</td></tr><tr><td>CoT-SC (k = 250)</td><td>0.40</td></tr><tr><td>ToT (ReAct)</td><td>0.49</td></tr><tr><td>RAP (ReAct)</td><td>0.54</td></tr><tr><td>LATS (DFS)</td><td>0.53</td></tr><tr><td>LATS (No Reflection)</td><td>0.56</td></tr><tr><td>LATS</td><td>0.61</td></tr></table>\n\n\nTable 5: Standardized comparisons between LATS, ToT, and RAP; we use ReAct as the base prompt for all methods and sample  $n = 5$  children for MCTS and DFS and  $k = {50}$  maximum trajectories.\n\n\nResults. We find in Tab. 4 that GPT-3.5 with ReAct is competitive to imitation learning, and can exceed reinforcement learning techniques with stronger prompting strategies. Sampling  $k = 30$  trajectories with ReAct and Reflexion results in a similar performance, suggesting the semantic feedback is not as helpful in complex environments like WebShop. Indeed like in Shinn et al. (2023), we find that generated reflections are often generic and do not provide useful feedback, resulting in a tendency for the agent to become stuck in local minima. However, using LATS indeed results in a noticeable improvement, indicating a more effective exploration for the same number of iterations.\n\n\n# 5.4 ADDITIONAL OBSERVATIONS\n\n\nWe also conduct additional experiments on HotPotQA to demonstrate the effect of each component of LATS. We also design a version of ToT and RAP with ReAct prompt and can handle external observations. We use HotPotQA as our setup incorporates both reasoning (through thoughts) and acting (through API calls); the results are shown in Tab. 5. More ablations for token consumption on HotPotQA are in Tab. 7 in Appendix C. Note that baselines generally perform worse than the reasoning-only setting of HotPotQA, which indicates that the acting-based setting is more challenging and adaption of search algorithms to decision-making scenarios is non-trivial.\n\n\nSelf-reflection. We use self-reflection to provide additional semantic signals for the agent. We observe a 0.05 performance drop when removed from LATS, suggesting this is useful. This is a smaller gain Reflexion (Shinn et al., 2023) observes over ReAct (Yao et al., 2023b) as shown in Tab. 2, suggesting overlap between the types of questions where there is an improvement with self-reflection and search. This variant outperforms RAP-ReAct, reflecting our improvements to MCTS.\n\n\nSearch Algorithm. MCTS is a more principled search algorithm than variants like A* or DFS search and the basis for observed performance gains. We observe the effects of using DFS, and incorporate the LM-based heuristic used in ToT (Yao et al., 2023a) in which branches with low values are pruned. This removes the selection and backpropagation operations, and we observe a 0.08 drop in performance when sampling the same number of nodes, but outperforms ToT-ReAct.\n\n\n# 6 CONCLUSION\n\n\nIn this work, we introduce Language Agent Tree Search (LATS), the first framework to unify planning, acting, and reasoning for enhanced LLM problem solving. By deliberately constructing trajectories with search algorithms, incorporating external feedback, and enabling agents to learn from experience, LATS addresses key limitations of prior prompting techniques. Our evaluations demonstrate the ability of LATS to harness LLM capabilities for a variety of decision-making tasks while keeping its reasoning ability without additional training. The proposed synergies between search, interaction, and reflection offer a versatile approach to autonomous decision-making, highlighting the potential of LLMs as generalist agents. A full discussion of the limitations and broader impacts is in Appendix B.\n\n\n# REFERENCES\n\n- Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances. arXiv:2204.01691, 2022.\n\n- T. Anthony, Z. Tian, and D. Barber. Thinking fast and slow with deep learning and tree search. In NIPS, 2017.\n\n- Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv:2108.07732, 2021.\n\n- Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv:2206.11795, 2022.\n\n- Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. arXiv:2308.09687, 2023.\n\n- Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In EMNLP, 2015.\n\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.\n\n- Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu. Deep blue. Artificial intelligence, 2002.\n\n- Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv:2207.10397, 2022a.\n\n- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv:2107.03374, 2021.\n\n- Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022b.\n\n- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv:2204.02311, 2022.\n\n- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv:2110.14168, 2021.\n\n- Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. arXiv:2306.06070, 2023.\n\n- Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. arXiv:2303.03378, 2023.\n\n- Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. arXiv:2302.00111, 2023.\n\n- Jonathan St BT Evans. Intuition and reasoning: A dual-process perspective. *Psychological Inquiry*, 2010.\n\n- Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In NeurIPS Datasets and Benchmarks Track, 2022.\n\n- Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023.\n\n- Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.\n\n- Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. Long text generation via adversarial training with leaked information. AAAI, 2018.\n\n- William H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Mineral: A large-scale dataset of apache demonstrations. In *IJCAI*, 2019.\n\n- Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In ICML, 2019.\n\n- Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv:2301.04104, 2023.\n\n- Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv:2305.14992, 2023.\n\n- Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv:2310.01798, 2023.\n\n- Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Thompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv:2207.05608, 2022.\n\n- D. Jiang, E. Ekwedike, and H. Liu. Feedback-based tree search for reinforcement learning. In ICML, 2018.\n\n- Levente Kocsis and Csaba Szepesv\u00e1ri. Bandit based monte-carlo planning. In ECML, 2006.\n\n- Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv:2205.11916, 2022.\n\n- Steven M LaValle, James J Kuffner, BR Donald, et al. Rapidly-exploring random trees: Progress and prospects. Algorithmic and computational robotics: new directions, 2001.\n\n- Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In ICLR, 2018.\n\n- Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. arXiv:2308.03688, 2023.\n\n- Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. arXiv:2303.17651, 2023.\n\n- Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence rnns and beyond. In SIGNLL, 2016.\n\n- Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henrik Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv:2112.00114, 2021.\n\n- OpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023.\n\n- Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv:2210.01240, 2022.\n\n- Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv:2302.04761, 2023.\n\n- Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv:2303.17580, 2023.\n\n- Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv:2303.11366, 2023.\n\n- Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv:2010.03768, 2020.\n\n- David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 2016.\n\n- David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, L. Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 2017.\n\n- Steven A. Sloman. The empirical case for two systems of reasoning. Psychological Bulletin, 1996.\n\n- Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. arXiv:2305.16653, 2023.\n\n- D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.\n\n- Maciej \u015awiechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek Ma\u0144dziuk. Monte carlo tree search: A review of recent modifications and applications. Artificial Intelligence Review, 2023.\n\n- Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wen Yin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\n\n- Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023.\n\n- Tom Vodopivec, Spyridon Samothrakis, and Branko Ster. On monte carlo tree search and reinforcement learning. Journal of Artificial Intelligence Research, 2017.\n\n- Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv:2305.16291, 2023.\n\n- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv:2203.11171, 2022.\n\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv:2201.11903, 2022.\n\n- Michael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. The knowledge engineering review, 1995.\n\n- Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World models for physical robot learning. In CoRL. PMLR, 2023.\n\n- Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. arXiv:2305.00633, 2023.\n\n- Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv:1809.09600, 2018.\n\n- Shunyu Yao, Howard Chen, John Yang, and Karthik R Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In NeurIPS, 2022.\n\n- Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv:2305.10601, 2023a.\n\n- Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In ICLR, 2023b.\n\n- Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. Retroformer: Retrospective large language agents with policy gradient optimization. arXiv preprint arXiv:2308.02151, 2023c.\n\n- Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. In NeurIPS, 2021.\n\n- Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv:2205.10625, 2022.\n\n- Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. arXiv:2305.17144, 2023.\n\nResponse: ### Prediction: \\(\\boxed{Accept}\\)\n\n#### Justification:\n\n**Originality:** \nThe paper introduces a novel approach called LATS (Language Agent Tree Search) that unifies reasoning, acting, and planning within a single framework. This is a significant contribution as it bridges different methodologies from reinforcement learning and prompting techniques into a cohesive system. The idea of combining these elements in a unified manner is fresh and innovative, offering a new direction for future work in autonomous agents and decision-making with language models.\n\n**Methodology Rigor:**\nThe authors provide a thorough theoretical framework for LATS, including a detailed description of the components and their interplay (agents, value functions, feedback mechanisms). They clearly articulate how MCTS is adapted for LM agents, emphasizing the benefits of leveraging external feedback and self-reflection. The experimental design is well-defined, with clear instructions and setups for various tasks (HotPotQA, Programming, WebShop). The results show significant performance gains compared to existing methods, especially for complex reasoning tasks like programming and WebShop.\n\n**Significance:**\nLATS has the potential to significantly advance the state-of-the-art in autonomous agents and decision-making systems, particularly in fields requiring both reasoning and acting abilities. The paper demonstrates the effectiveness of LATS on diverse benchmarks, including challenging ones like programming and complex web navigation tasks. This shows that LATS can be broadly applicable and impactful.\n\n**Clarity:**\nThe paper is well-written and structured, with a clear introduction, methodology, experiments, and conclusion sections. The notation and terminology are well-defined, and the explanations are clear. The figures and tables support the claims effectively. \n\n**Alignment with Field Standards:**\nThe work aligns well with current trends in AI research, particularly in the areas of autonomous agents, decision-making, and the integration of reinforcement learning techniques with prompting methods. The authors provide a comprehensive review of related work, positioning LATS as a novel and important contribution to the field.\n\n### Strengths:\n1. **Unified Framework**: LATS integrates reasoning, acting, and planning into a single framework, providing a holistic approach to decision-making.\n2. **Adaptability**: The use of external feedback and self-reflection allows LATS to adapt to different scenarios and environments.\n3. **Performance Gains**: The results on various benchmarks demonstrate substantial improvements over existing methods, particularly for complex reasoning tasks.\n4. **Detailed Methodology**: A thorough explanation of the MCTS adaptation for LM agents, including the use of UCT and value functions.\n\n### Weaknesses:\n1. **Explainability**: The performance gains are somewhat opaque in terms of how exactly LATS achieves these improvements. More detailed analysis could be provided on the specific mechanisms behind these gains.\n2. **Complexity**: The method may be complex to implement and maintain, especially for those unfamiliar with MCTS and LLMs.\n3. **Resource Intensive**: The computational demands of LATS are noted, which could be a barrier for some applications.\n4. **Human Evaluation Limitations**: While the results are impressive, it would be beneficial to include more human evaluation studies to validate the practical utility of LATS.\n\nOverall, the paper presents a compelling and innovative approach that has the potential to significantly advance the field of autonomous decision-making with language models. Given the performance gains and the novelty of the approach, the paper should be accepted for publication at ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "TextualGradientDescent prompt for update", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "165", "funcName": "_update_prompt", "created": "1768634853.1022422", "msecs": "102.0", "relativeCreated": "197925.71640014648", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "prompt": "Here is the role of the variable you will improve: <ROLE>System prompt for paper acceptance prediction. This prompt sets the context and approach for the AI reviewer.</ROLE>.\n\nThe variable is the text within the following span: <VARIABLE> You are an expert academic reviewer tasked with evaluating research (...) and weaknesses, ensuring objectivity and adherence to peer review principles. </VARIABLE>\n\nHere is the context and feedback we got for the variable:\n\n<CONTEXT></CONTEXT>\n\nImprove the variable (System prompt for paper acceptance prediction. This prompt sets the context and approach for the AI reviewer.) using the feedback provided in <FEEDBACK> tags.\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.", "message": "TextualGradientDescent prompt for update"}
{"name": "textgrad", "msg": "TextualGradientDescent optimizer response", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "179", "funcName": "step", "created": "1768634869.6081665", "msecs": "608.0", "relativeCreated": "214431.640625", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "optimizer.response": "<think>\nOkay, let's tackle this. The user wants me to improve the system prompt for a paper acceptance prediction model. The original variable is a system prompt that tells the AI reviewer to evaluate research, identify strengths and weaknesses, and ensure objectivity and peer review principles.\n\nFirst, I need to understand the role of the variable. It's a system prompt, so it's setting the context and approach for the AI reviewer. The feedback isn't provided here, but the user mentioned there was some feedback. Wait, in the context, there's no feedback, but the user says to use the feedback provided in <FEEDBACK> tags. However, in the given input, there's no actual feedback. Hmm, maybe the user is testing me? Or perhaps there was a mistake in the input.\n\nWait, looking back, the user provided the role and the variable, and mentioned the context and feedback. But the context is empty, and there's no feedback tag. Maybe the feedback is implied? Or maybe the user expects me to generate an improved version based on standard best practices for system prompts in AI review contexts.\n\nAssuming that the feedback might have been about making the prompt more specific, ensuring clarity, or including criteria for evaluation. Since the original prompt mentions objectivity and peer review principles, but maybe it's too vague. To improve it, I should make the instructions more detailed, perhaps include specific aspects like methodology, novelty, significance, and clarity. Also, emphasizing the importance of evidence-based reasoning and avoiding personal biases. Maybe structuring the prompt to guide the AI through a step-by-step evaluation process. Including examples of what to look for in each section. Also, ensuring that the AI understands the balance between criticism and constructive feedback. \n\nSo, the improved variable should be more structured, include specific evaluation criteria, and maybe outline the expected output format. Let me draft that. Make sure to use clear, concise language. Avoid ambiguity. Ensure that the AI knows to prioritize key factors like originality, methodology, and contribution to the field. Also, mention adherence to ethical guidelines and reproducibility. \n\nWait, the original prompt says \"ensuring objectivity and adherence to peer review principles.\" Maybe the feedback was that the prompt is too generic. So, adding specifics about the aspects to evaluate would help. Also, perhaps the AI needs to be instructed on how to weigh different factors. Maybe include a checklist or a list of criteria. \n\nPutting it all together, the improved system prompt should clearly outline the evaluation criteria, the importance of objectivity, and the expected approach to peer review. It should guide the AI to assess each paper based on specific, measurable factors. Also, maybe mention the need for balanced feedback, both strengths and weaknesses, and the importance of evidence-based reasoning. \n\nI think that's a solid approach. Now, I'll structure the improved variable accordingly, making sure it's comprehensive and actionable for the AI reviewer.\n</think>\n\n<IMPROVED_VARIABLE>You are an expert academic reviewer tasked with evaluating research manuscripts for potential acceptance in a peer-reviewed journal. Assess the paper's originality, methodology, significance, and clarity, prioritizing evidence-based reasoning. Identify strengths (e.g., novel contributions, rigorous analysis) and weaknesses (e.g., methodological flaws, insufficient justification) while adhering to peer review principles. Provide balanced, constructive feedback that objectively weighs the paper's merits against field-specific standards, ensuring reproducibility, ethical compliance, and alignment with the journal's scope. Avoid personal biases and focus on the work's scientific merit and impact.</IMPROVED_VARIABLE>", "message": "TextualGradientDescent optimizer response"}
{"name": "textgrad", "msg": "TextualGradientDescent updated text", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "187", "funcName": "step", "created": "1768634869.6084685", "msecs": "608.0", "relativeCreated": "214431.94270133972", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "parameter.value": "You are an expert academic reviewer tasked with evaluating research manuscripts for potential acceptance in a peer-reviewed journal. Assess the paper's originality, methodology, significance, and clarity, prioritizing evidence-based reasoning. Identify strengths (e.g., novel contributions, rigorous analysis) and weaknesses (e.g., methodological flaws, insufficient justification) while adhering to peer review principles. Provide balanced, constructive feedback that objectively weighs the paper's merits against field-specific standards, ensuring reproducibility, ethical compliance, and alignment with the journal's scope. Avoid personal biases and focus on the work's scientific merit and impact.", "message": "TextualGradientDescent updated text"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634875.5191534", "msecs": "519.0", "relativeCreated": "220342.6275253296", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research manuscripts for potential acceptance in a peer-reviewed journal. Assess the paper's originality, methodology, significance, and clarity, prioritizing evidence-based reasoning. Identify strengths (e.g., novel contributions, rigorous analysis) and weaknesses (e.g., methodological flaws, insufficient justification) while adhering to peer review principles. Provide balanced, constructive feedback that objectively weighs the paper's merits against field-specific standards, ensuring reproducibility, ethical compliance, and alignment with the journal's scope. Avoid personal biases and focus on the work's scientific merit and impact.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Visual Timing For Sound Source Depth Estimation in the Wild\n\n# Abstract\nDepth estimation enables a wide variety of 3D applications, such as robotics and autonomous driving. Despite significant work on various depth sensors, it is challenging to develop an all-in-one method to meet multiple basic criteria. In this paper, we propose a novel audio-visual learning scheme by integrating semantic features with physical spatial cues to boost monocular depth with only one microphone. Inspired by the flash-to-bang theory, we develop FBDepth, the first passive audio-visual depth estimation framework. It is based on the difference between the time-of-flight (ToF) of the light and the sound. We formulate sound source depth estimation as an audio-visual event localization task for collision events. To approach decimeter-level depth accuracy, we design a coarse-to-fine pipeline to push the temporary localization accuracy from event-level to millisecond-level by aligning audio-visual correspondence and manipulating optical flow. FBDepth feeds the estimated visual timestamp together with the audio clip and object visual features to regress the source depth. We use a mobile phone to collect 3.6K+ video clips with 24 different objects at up to 60m. FBDepth shows superior performance especially at a long range compared to monocular and stereo methods.\n\n\n# 1 INTRODUCTION\n\n\nDepth estimation is the fundamental functionality to enable 3D perception and manipulation. Although there have been significant efforts on developing depth estimation methods with various sensors, current depth estimation schemes fail to achieve a good balance on multiple basic metrics including accuracy, range, angular resolution, cost, and power consumption.\n\n\nActive depth sensing methods actively emit signals, such as LiDAR (Caesar et al., 2020), structured-light (Zhang, 2012), mmWave (Barnes et al., 2020), ultrasound (Mao et al., 2016), WiFi (Vasisht et al., 2016). They compare the reflected signal with the reference signal to derive time-of-flight (ToF), phase change, or Doppler shift to estimate the depth. Active methods can achieve high accuracy because of the physical fundamental and well-designed modulated sensing signals. Lidar is the most attractive active senor due to its large sensing range and dense point cloud. However, the density is not sufficient enough to enable a small angular resolution. Therefore, the points are too sparse to be recognized at a long distance. Besides, the prohibitive cost and power consumption limit the availability of Lidar on general sensing devices.\n\n\nPassive depth sensing takes signals from the environment for sensing directly. It commonly uses RGB monocular camera (Bhoi, 2019; Laga et al., 2020), stereo camera (Cheng et al., 2020), thermal camera (Lu & Lu, 2021), or multi-view cameras (Long et al., 2021a). These sensors can achieve pixel-wise angular resolution and consume pretty less energy due to omitting the signal emitting. Among them, stereo matching can effectively estimate the disparity and infer a dense depth map since it transforms the spatial depth to the visual disparity based on the solid physical law. The baseline of the stereo camera determines the effective range and accuracy. Therefore, the dimension of the stereo camera is placed as the critical trade-off with sensing metrics. Thanks to the advance in deep learning, the cheap monocular depth estimation keeps on improving performance with new network structures and high-quality datasets. However, the accuracy is still not satisfactory especially at a long range because it can only regress depth based on the implicit visual cues. It is ill-posed without any physical formulation. Besides, it heavily relies on the dataset. It requires domain adaption and camera calibration for various camera intrinsics (Li et al., 2022).\n\n\nIn this paper, we propose to add only one microphone to enable explicit physical depth measurement and boost the performance of a single RGB camera. It does not rely on the intrinsic of cameras and implicit visual cues. We develop a novel passive depth estimation scheme with a solid physical formulation, called Flash-to-Bang Depth (FBDepth). Flash-to-Bang is used to estimate the distance to the lightning strike according to the difference between the arrival time of a lightning flash and a thunder crack. This works because light travels a million times faster than sound. When the sound source is several miles away, the delay is large enough to be perceptible. Applying it to our context, FBDepth can estimate the depth of a collision that triggers audio-visual events. The collision event has been explored for navigation and physical search in (Gan et al., 2022), but our work is the first that uses the collision for depth estimation. Collisions are common and can arise when a ball bounces on the ground, a person takes a step, or a musician hits a drum. We identify and exploit several unique properties related to various collisions in the wild. First, the duration of a collision is short and collision events are sparse. Thus, there are few overlapped collisions. Second, though the motion of objects changes dramatically after the collision, they are almost static at the collision moment. Third, the impact sound is loud enough to propagate to a long range.\n\n\nFlash-to-Bang is applied to the range of miles for human perception. Using it for general depth estimation poses several significant challenges: (i) It is inaccessible to ground truth collision time from video and audio. Video only offers up to 240 frames per second(fps), and may not capture the exact instance when the collision occurs. Audio has a high sampling rate but it is hard to detect the start of a collision solely based on the collision sound due to different sound patterns arising from collisions as well as ambient noise. (ii) We need highly accurate collision time. 1 ms error can result in a depth error of  $34~\\mathrm{cm}$ . (iii) Noise present in both audio and video further exacerbate the problem.\n\n\nTo realize our idea, we formulate the sound source depth estimation as the audio-visual localization task. Whereas existing work (Wu et al., 2019; Xia & Zhao, 2022) still focuses on 1-second-segment level localization. FBdepth performs event-level localization by aligning correspondence between the audio and the video. Apart from audio-visual semantic features as input in existing work (Tian et al., 2018; Chen et al., 2021a), we incorporate optical flow to exclude static objects with similar visual appearances. Furthermore, FBDepth applies the impulse change of optical flow to locate collision moments at the frame level. Finally, we formulate the ms-level estimation as an optimization problem of video interpolations. FBDepth succeeds to interpolate the best collision moment by maximizing the intersection between extrapolations of before-collision and after-collision flows.\n\n\nWith the estimated timestamp of visual collision, we regress the sound source depth with the audio clip and visual features. FBdepth avoids the requirement to know the timestamp of audio collision. Besides, different objects have subtle differences in audio-visual temporal alignment. For example, a rigid body generates the sound peak once it touches another body. But an elastic body produces little sound during the initial collision and takes several ms to produce the peak with the maximum deformation. We feed semantic features to enable the network aware of the material, size, etc.\n\n\nOur main contributions are as follows:\n\n- 1. To the best of our knowledge, FBDepth is the first passive audio-visual depth estimation. It brings the physical propagation property to audio-visual learning.\n\n- 2. We introduce the ms-level audio-visual localization task. We propose a novel coarse-to-fine method to improve temporal resolution by leveraging the unique properties of collisions.\n\n- 3. We collect  $3.6\\mathrm{K}+$  audio-visual samples across 24 different objects in the wild. Our extensive evaluation shows that FBDepth achieves  $0.64\\mathrm{m}$  absolute error(AbsErr) and  $2.98\\%$  AbsRel across a wide range from  $2\\mathrm{m}$  to  $60\\mathrm{m}$ . Especially, FBDepth shows more improvement in the longer range.\n\n\n# 2 RELATED WORK\n\n\nMulti-modality Depth estimation. Recent work on depth estimation has shown the benefits of fusing cameras and other active sensors. (Qiu et al., 2019; Imran et al., 2021) recover dense depth maps from sparse Lidar point clouds and a single image. (Long et al., 2021b) associates pixels with pretty sparse radar points to achieve superior accuracy. The effective range can be increased as well by Lidar-camera (Zhang et al., 2020) or Radar-camera (Zhang et al., 2021). However, these methods are still expensive in cost and power consumption.\n\n\n(Gao et al., 2020; Parida et al., 2021) emit audio chirps and learn the depth map implicitly with audio reflections and a single image. However, these methods require many nearby acoustic reflectors to produce effective echos so the setup is limited in rooms. Besides, they are evaluated in an audio-visual simulator. FBDepth only uses one extra microphone to perceive natural sounds directly. It keeps the passive design of the audio but applies the physical measurement explicitly. The one-path sound propagation has a longer effective range than echoes.\n\n\nSound source localization. Previous systems localize sound sources with microphone arrays (Valin et al., 2003; Rascon & Meza, 2017) or one microphone with a camera (Hershey & Movellan, 1999). They intend to estimate the direction of arrival(DOA) or the distance. The DOA is inferred by the subtle difference in arrival time from the sound source to each microphone(Mao et al., 2019; Sun et al., 2022) or by semantic matching with the visual appearance if given images(Tian et al., 2018; Arandjelovic & Zisserman, 2018). The distance can be estimated by triangulation methods with multiple DOAs and room structures(Wang et al., 2021; Shen et al., 2020). Many work study the room acoustic and the distance cues from the reverberation(Singh et al., 2021; Chen et al., 2021b) but (Zahorik, 2002) shows that the reverberation has a coarse coding with the distance. Compared to these methods, FBDepth directly estimates the distance by the ToF and achieves superior accuracy to indirect triangulation methods and implicitly depth learning networks on reverberation.\n\n\nAudio-visual event localization aims to detect and localize events in videos. (Tian et al., 2018) first propose the task and build up the audio-visual event(AVE) dataset. They apply an audio-guided visual attention mechanism to learn visual regions with the related sounding object or motions. Recent works develop dual-modality sequence-sequence framework (Lin et al., 2019) and dual attention matching mechanism (Wu et al., 2019) to leverage global features. However, the temporal event boundary is 1s-level in AVE dataset so it is split as 1s-long segments. We study the instant collision event and solve the coarse boundary problem as well.\n\n\n(Gan et al., 2022) has a similar setup to ours. They use an embodied robot agent to navigate to a dropped object in 3D virtual rooms. They integrate asynchronous vision and audition and navigate to the object. The asynchronism comes from the invisibility of the object. Even though their simulator has been pretty vivid enough for semantic tasks, it has a gap in the real-world collision for the m-level formulation. Falling objects dataset(Kotera et al., 2020), TbD dataset(Kotera et al., 2019) and TbD-3D dataset(Rozumnyi et al., 2020) explore falling motions and fast movements but they do not have audio and depth information.\n\n\nVideo frame interpolation aims to synthesize intermediate frames between existing ones of a video. Most state-of-the-art approaches explicitly or implicitly assume a simplistic linear motion. Warping-based methods (Baker et al., 2011; Park et al., 2020) apply optical flow and forward warping to shift pixels to intermediate frames linearly. Phase-based methods (Meyer et al., 2015; 2018) combine the phase information across different scales but the phase is modeled as a linear function of time. Recent methods are developed to approximate non-linear motion, such as kernel-based methods (Niklaus et al., 2017a;b), quadratic interpolation (Xu et al., 2019a), cubic motion modeling (Chi et al., 2020), etc. However, they still fail to complex non-linear motions because precise motion dynamics cannot be captured in the blind time between keyframes. Unfortunately, collisions are super non-linear and instant. Given two keyframes before and after the collision, it is ambiguous to decide whether there is a collision. Hence, these methods are not applicable. We analyze the motions before and after the collision and extrapolate optical flows to find the most potential collision position.\n\n\n# 3 PROBLEM FORMULATION\n\n\nWe formulate the depth estimation by the physical law of wave propagation. We have:\n\n\n$$\n\n$$\n\\frac {d}{v} - \\frac {d}{c} = T \\tag {1}\n$$\n\n$$\n\n\nwhere the depth of the sound source is  $d$  and the difference between the ToF of sound and light is  $T$ .  $c$  and  $v$  denote the propagation speeds of light and sound, respectively. We can estimate  $d$  based on  $d = \\frac{cvT}{c - v} \\approx vT$  since  $c \\gg v$ . We observe  $T = T_{audio} - T_{video} + T_{hardware}$ , where  $T_{audio}$  and  $T_{video}$  denote the event time in the audio and video recordings, respectively, and  $T_{hardware}$\n\n\ndenotes the start time difference in the audio and video recordings. It can be small as well as have a small variance with a well-designed media system such as the Apple AVFoundation framework. We regard it as a constant unknown bias to learn.\n\n\nIt is impossible to label the precise  $T_{video}$  and  $T_{audio}$  manually.  $T_{video}$  can be tagged at most frame-level. Even though many commercial cameras can support up to 240 FPS, it results in a 4-ms segment and 1.43m depth variation. Moreover, it is tough to determine the exact frame that is nearest to the collision in high FPS mode by a human being due to the constrained view of the camera.  $T_{audio}$  is challenging to recognize in the wild as well. Although the audio sampling rate is high enough, we can recognize the significant early peaks instead of the first sample triggered by the collision. The best effort of segmentation is 10-ms level based on real data.\n\n\nWe cannot learn the timestamp with supervision. We propose a 2-stage estimation framework. The goal of the first stage is to estimate the numerical  $T_{video}$ . As figure 1 shows, we localize the audio-visual event in the stream and then take advantage of the unique optical flow of the collision to estimate  $T_{video}$  at ms-level. In the second stage, we place the  $T_{video}$  as an anchor into the audio clip and direct regress the depth with depth supervision. We make the network optimize  $T_{audio}$  automatically with knowledge of the  $T_{video}$ , the audio waveform and visual features.\n\n\n# 4 APPROACH\n\n\n![Figure 1: Model architecture. Our audio-visual depth estimation uses the video, audio and optical flow to perform the event-level localization to retrieve the collision event. It analyzes the collision flow and estimate the collision timestamp in video. It uses multiple modalities including RGB, flow, audio and the timestamp to estimate the depth.](images/f9516a01b247146bba42363b6346fbc836500a6ee1429746ee21e85ad481c069.jpg)\n\n\nWe demonstrate a novel coarse-to-fine pipeline to localize the collision with a super temporal resolution in the video. This method does not require annotations on ms-level, which is at least two orders of magnitude finer than previous approaches. They rely on the supervision of segment annotations, such as AVE dataset with 1-second segments (Tian et al., 2018), Lip Reading Sentences 2 dataset with word-level segments (Chung & Zisserman, 2016), BOBSL with sentence-level alignments (Bull et al., 2021).\n\n\n# 4.1 EVENT-LEVEL LOCALIZATION\n\n\nAudio-visual modeling for collisions. In this step, our goal is to localize the audio-visual event for the region and the period of interest. It is similar to (Tian et al., 2018), but the unique properties of collisions bring new opportunities to learning strategy. Collisions have a significant motion than other sound sources. We can use the optical flow to inform the network of moving pixels. Besides, the impact sound is highly correlated to the rich information of objects (Gan et al., 2022), such as shape, materials, size, mass, etc. It makes audio-visual cross-matching easier than general audio-visual events so that we do not need to apply a complex scheme to learn. Another fact is that\n\n\ncollisions are pretty sparse temporally in the wild because the duration of collisions is extremely short. It is rare to come across overlapped collisions based on our empirical study on the basketball court. Only two frames have double collisions among all 1208 frames and a total of 203 collisions when 7 basketballs are played during a 40-s duration.\n\n\nWe propose a motion-guided audio-visual correspondence network (MAVNet). Similar to (Tian et al., 2018; Wu et al., 2019), MAVNet performs the cross-matching for the audio features and the RGB-F channels. Besides, it predicts audio-visual segmentation to capture whole pixels of the target object. It can achieve fine-grained audio-visual scene understanding (Zhou et al., 2022). We use the segmentation mask to filter flows of interest and perform high-resolution estimation in the next steps.\n\n\nMAVNet has two backbones to deal with RGB-F channels and audio clips respectively. A U-Net (Ronneberger et al., 2015) style encoder is applied to extract the frame features conditioned by optical flows. It uses a series of convolution layers to extract visual features. Another branch is the audio encoder which takes in the time-domain signal. It has a 1D convolution layer to learn an STFT-like representation and a stack of 2D convolution layers with batch normalization to learn the semantic audio features. We replicate the audio feature, tile them to match the visual feature dimension, and concatenate the audio and visual feature maps. MAVNet has two output heads as well. The U-Net decoder applies a series of up-convolutions and skip-connections from the RGB-F encoder to fused feature maps to learn the binary segmentation mask  $M$ . Meanwhile, the fused feature map is fed into a binary classification head consisting of convolution layers and linear layers to predict the audio-visual event relevance  $y \\in \\{0,1\\}$ .\n\n\nTraining We use the weighted sum Binary Cross Entropy (BCE) loss as the training objective for both segmentation and the cross matching. We train all components to jointly optimize the location predictions and energy reconstruction. We minimize the total loss  $\\mathcal{L}_{total} = BCE(M, \\hat{M}) + \\lambda * BCE(y, \\hat{y})$  where  $\\lambda$  is the hypermeter to set.\n\n\nInference We only use low FPS to perform MAVNet to avoid dense inference at this stage. Moreover, we do not need to activate the segmentation head until the audio clip and the frame are highly matched. Finally, MAVNet uses this audio clip to retrieve a sequence of frames including the full collision procedure.\n\n\n# 4.2 FRAME-LEVEL LOCALIZATION\n\n\nGiven a sequence of video frames, our goal is to split them into two sets: the frames before the collision  $\\mathbf{V_0}$  and the frames after the collision  $\\mathbf{V_1}$ . This essentially requires us to determine the last frame  $I_{e}$  in  $\\mathbf{V_0}$  before the collision and the first frame  $I_{s}$  in  $\\mathbf{V_1}$  after the collision. Thus, we locate the collision between the frame  $I_{e}$  and  $I_{s}$ .\n\n\nBased on the analysis of the physical motion, we make an important observation that can help determine  $I_{e}$  and  $I_{s}$ . The collision results in a significant acceleration change due to the strong impulse force. Let  $a_{t} = v_{t} - v_{t-1}$  and  $\\delta a_{t} = a_{t} - a_{t-1}$  denote the acceleration and acceleration change of frame  $I_{t}$ , respectively.  $\\delta a$  between  $I_{e}$  and  $I_{s}$  is large, while  $\\delta a$  between adjacent frames before or after the collision is small. If the object stops moving immediately after the collision, we take the static frame  $I_{e+1}$  as  $I_{s}$ . Finally, we select the frames before  $I_{e}$  to generate  $\\mathbf{V}_{0}$ , and select the frames after  $I_{s}$  to generate  $\\mathbf{V}_{1}$ .\n\n\nWe use the retrieved mask in the last stage to determine the object positions in the frames and calculate the velocity, acceleration, and acceleration change. We find the  $I_{e}$  and  $I_{s}$  at the low FPS and then replicate the procedure for frames between  $I_{e}$  and  $I_{s}$  at high FPS. Finally, we locate  $I_{e}$  and  $I_{s}$  in the high FPS mode efficiently.\n\n\n# 4.3 MS-LEVEL LOCALIZATION\n\n\nTo further locate the exact moment of the collision, we try to interpolate frames between  $I_{e}$  and  $I_{s}$  to recover the skipped frame. Unfortunately, the common assumption of frame-based interpolation is fully broken down.\n\n\nMotion consistency is fundamental for spatio-temporal video processing. If the motion of the object is temporally stable across several frames (e.g., due to a constant force), the position and pose can be predicted in the future frames as well as be interpolated between two frames. We denote it\n\n\nas motion first consistency. However, the impact sound is caused by an impulse force, which results in a rapid change of the motion status. It breaks the motion continuity and consistency. When we observe  $I_{e}$  and  $I_{s}$ , we cannot determine whether a collision happens or the object just flies in the air.\n\n\nLuckily, the collision moment retains a new form of motion consistency. We denote it as motion second consistency. It reveals that the motions before and after the collision share the same intersection position. Besides, they keep the motion first consistency separately. Therefore, we can extrapolate the motions based on the motion first consistency and search for the most similar motion extrapolations by leveraging motion second consistency. Note that our final goal is to find the timestamp of the collision instead of the motion status at the shared position. (Kotera et al., 2019; Rozumnyi et al., 2020) try to recover the sub-frame motions and trajectories as well but they require the high FPS ground truth to guide the training. In our context, we care more about when the collision happens than what it looks like.\n\n\nOptical flow extrapolation Optical flow is widely used for frame prediction () and interpolation (Baker et al., 2011) by warping the frame with the estimated optical flow. Because it can capture all motions of pixels and get a finer understanding of the object dynamics. The optical flow sequence is usually generated by adjacent video frames. However, it is not efficient for extrapolation. The drift of pixels in the flow requires extra iterative wrappings to align the corresponding pixels, which results in accumulation errors.\n\n\nTherefore, we compute the optical flows from an anchor frame  $I_{a}$  to the frame sequence  $\\mathcal{V}$  as  $\\{I_0,I_1,\\ldots I_n\\}$ . We can estimate the flow sequence  $\\mathcal{F}_{a\\to \\nu}$  as  $\\{f_{a\\to 0},f_{a\\to 1},\\dots f_{a\\to n}\\}$ . As  $f_{a\\to n}(x,y)$  represents the movement of the pixel  $I_{a}(x,y)$  to  $I_{n}$ ,  $\\mathcal{F}_{a\\to \\mathcal{V}}(x,y)$  describes how the pixel in  $I_{a}(x,y)$  moves across the frame sequence  $\\mathcal{V}$ . Hence,  $\\mathcal{F}_{a\\to \\mathcal{V}}$  tracks the global motion of each pixel without iterative warpings. With the historical positions of  $I_{a}(x,y)$  from frame  $I_0$  to  $I_{n}$ , we can regress the motion of this pixel and extrapolate the flow to  $f_{a\\to n + \\delta t}$ , which is the relative pixel position to  $I_{n + \\delta t}$  with an arbitrary  $\\delta t$ .\n\n\nIn our context, We pick  $k$  consecutive frames before the collision  $\\mathcal{V}_{pre}$  as  $\\{I_{e - k + 1}, I_{e - k + 2}, \\dots, I_e\\}$  and after the collision  $\\mathcal{V}_{post}$  as  $\\{I_{s + k - 1}, I_{s + k - 2}, \\dots, I_s\\}$ . We select the frame  $I_e$  as the anchor frame. It is near the collision moment, so its motion to other frames is not dramatic and easy to be estimated. Hence, we can estimate the optical flow sequences  $\\mathcal{F}_{e \\to \\mathcal{V}_{pre}}$  and  $\\mathcal{F}_{e \\to \\mathcal{V}_{post}}$ . Meanwhile, we apply the predicted segmentation mask of  $I_e$  to filter the pixels of the target object. In the last step, we build up regressors  $\\mathcal{R}$  for each pixel's motion individually and predict future locations in any sub-frame.\n\n\nOptical flow interpolation We have constructed pixel level regressors for  $\\mathcal{F}_{e\\rightarrow \\mathcal{V}_{pre}}$  and corresponding  $\\mathcal{F}_{e\\rightarrow \\mathcal{V}_{post}}$ . They can extrapolate the flow  $f_{e\\rightarrow e + \\delta t_0}$  and  $f_{a\\rightarrow s + \\delta t_1}$ , respectively.  $\\delta t_0, \\delta t_1$  are extrapolation steps. The optimization goal is to\n\n\n$$\n\n$$\n\\min  _ {e - s \\leq \\delta t _ {1} \\leq 0 \\leq \\delta t _ {0} \\leq s - e} | | f _ {e \\rightarrow e + \\delta t _ {0}}, f _ {a \\rightarrow s + \\delta t _ {1}} | | _ {2}, \\text {s . t .} e + \\delta t _ {0} <   s + \\delta t _ {1}\n$$\n\n$$\n\n\nThe collision duration is  $s + \\delta t_1 - (e + \\delta t_0)$ , which is always more than 0.  $e + \\delta t_0$  is the target m-level localization  $\\hat{T}_{video}$ . We can apply this interpolation methodology to search the intersection of the object's center trajectory or maximize the Intersection over Union (IoU) of the object's bounding box. However, both only use several key points so they cannot achieve a fine granularity since the optical flow takes advantage of thousands of pixels.\n\n\n# 4.4 DEPTH REGRESSION\n\n\nBased on the estimation  $\\hat{T}_{video}$ , we directly regress the depth to fit the  $T_{audio}$  and the bias  $T_{Hardware}$  with the supervision of ground truth depth. We observe that the sound generation procedure varies a lot across different objects, materials, shapes, and motions. On one hand, the diverse waveforms make it impractical to measure the exact  $T_{audio}$  manually. On the other hand, each specific waveform has significant implications on what is the best  $T_{audio}$  corresponding to  $\\hat{T}_{video}$ . To combat the background noise from other sources, we also feed the RGB-F crop of the target object from frame  $I_e$  to the depth predictor. It includes the semantic features of the object as well as the motion status just before the collision. These cues can guide the predictor to find the waveform pattern easily.\n\n\nWe select a sequence of audio samples starting from  $I_{e}$  and label some anchor samples as 1 at  $\\hat{T}_{video}$ . It informed the audio sequence about the timestamp of the visual collision directly. We feed the enriched sequence into the 1D convolution layer to extract a 2D representation. It is followed\n\n\nby two residual blocks to learn high-dimension features. Meanwhile, we use ResNet-18 (He et al., 2015) to extract the RGB-F features of the target object. We tile and concatenate the RGB-F features to the audio features along the channel dimension and append another two residual blocks to fuse the features. Finally, it is followed by a pooling layer and a fully connected layer to predict the depth. The output maps to depth by the 2D projection. We use Mean Square Error (MSE)  $\\mathcal{L}_{depth} = ||d,d||_2$  as the learning objective where  $d$  and  $\\hat{d}$  are the target depth and the predicted depth.\n\n\n# 5 EXPERIMENTS\n\n\n# 5.1 SETUP\n\n\nDataset platform and collection We use an iPhone XR with a 240-fps slow-motion mode to collect the video with audio. The audio sampling rate is 48Khz. We set a stereo camera and a Lidar together to collect ground truth. We include details of data collection in the Appendix B.\n\n\nAVD Dataset We collect  $3.6\\mathrm{K}+$  raw audio-visual sequences with a single collision event as the audio-visual depth(AVD) dataset. We randomly sample raw sequences to generate train/val/eval splits, which have 2600/500/522 sequences. We augment the raw sequences by cropping one moving object from a raw video sequence and inserting it into another raw sequence with a random temporal location. Besides, we augment the raw depth with a maximum  $3\\%$  random change to diversify the depth and shift audio samples accordingly to the video timestamp. More details are described in Appendix B.\n\n\nBaselines We include three types of baseline for comparison. We compare to a monocular depth estimation method NeWCRFs (Yuan et al., 2022), a state-of-the-art(SOTA) on multiple benchmarks. We also compare to stereo matching methods including the ZED built-in ultra depth estimation SDK and a SOTA method LEAStereo (Cheng et al., 2020). We use dense depth maps collected by the Lidar to finetune the NeWCRFs and LEAStereo on images collected by the stereo camera. Despite optical flow based interpolation, we compare to interpolation using key points such as the trajectories of center or bounding boxes.\n\n\nMetrics We use the mean absolute depth errors as  $AbsErr = \\frac{1}{n}\\sum_{i=1}^{n}|d - \\hat{d}|$ , root mean square absolute relative errors  $RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(d - \\hat{d})^2}$ ,  $AbsRel = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{|d - \\hat{d}|}{d}$  as the end-to-end performance metrics. FBDepth is a sparse depth estimation. We evaluate the depth of each target object. However, monocular and stereo baselines have dense depth estimations for all pixels of the object. We evaluate the median estimation depth with the median depth of the ground truth dense map. We provide the results over different distance ranges as close(\u2264 10m), mid(10m-30m), and far(\u2265 30m). Intuitively, there is an upper bound for the temporal resolution so  $AbsRel$  at close depths performs worse than at further distances.\n\n\n5.2 RESULTS\n\n\n<table><tr><td>Method</td><td>Input</td><td>FPS</td><td>AbsErr(m) close/mid/far/all</td><td>AbsRel(%) close/mid/far/all</td><td>RMSE(m) close/mid/far/all</td></tr><tr><td>NeWCRFs</td><td>V</td><td>-</td><td>0.553/1.09/3.27/1.68</td><td>11.1/6.74/8.64/9.48</td><td>0.895/1.51/5.82/3.49</td></tr><tr><td>ZED SDK</td><td>S</td><td>-</td><td>0.083/0.96/5.10/2.03</td><td>1.78/6.05/12.7/7.28</td><td>0.108/1.07/6.30/3.69</td></tr><tr><td>LEAStereo</td><td>S</td><td>-</td><td>0.067/0.66/2.47/0.88</td><td>1.48/4.24/5.98/4.09</td><td>0.083/0.76/5.08/2.95</td></tr><tr><td>FBDepth</td><td>A+V</td><td>30</td><td>0.485/0.83/1.33/0.95</td><td>10.9/5.20/3.32/4.26</td><td>0.731/1.01/2.29/1.51</td></tr><tr><td>FBDepth</td><td>A+V</td><td>60</td><td>0.418/0.70/1.11/0.72</td><td>8.94/4.33/2.79/3.34</td><td>0.597/0.83/1.86/1.27</td></tr><tr><td>FBDepth</td><td>A+V</td><td>120</td><td>0.392/0.61/0.98/0.67</td><td>8.42/3.79/2.49/3.11</td><td>0.534/0.75/1.68/1.09</td></tr><tr><td>FBDepth</td><td>A+V</td><td>240</td><td>0.337/0.58/0.95/0.64</td><td>7.25/3.55/2.41/2.98</td><td>0.476/0.69/1.61/1.03</td></tr></table>\n\n\nTable 1: A comprehensive comparison for different depth estimation approaches. V, S, A represent visual, stereo, audio respectively. We input video with different frame rates as well.\n\n\nTable 1 shows the results on the depth estimation. In all, FBDepth can achieve better performance on all metrics than baselines across different FPS. Several important trends can be observed. Stereo matching methods perform extraordinarily on close objects, where more clear view difference can be captured. The AbsErr and RMSE increase dramatically as the targets become further because\n\n\nthe limited baseline cannot resolve the view difference easily. In the other side, the AbsErr and RMSE of FBDepth grows slowly with the increasing distance while its AbsRel decreases gradually. Intuitively, there is a upper bound for the temporal resolution due to the limited FPS, the lack of the accurate timestamp and the small disturbance of audio-video software. Thus FBDepth may not achieve the centimeter level easily. A Further depth can break the assumption of stereo matching methods as well as monocular methods which has a fixed depth range of training data, but FBDepth still holds the physical propagation law in this condition.\n\n\nFBdepth also shows advantages on NeWCRFs. The monocular methods rely on the training set, which includes various scenarios and depths. Although we apply camera remapping with intrinsic matrix and finetuning, NeWCRFs still cannot achieve the best performance as the one in the pretrained dataset. The implicit depth regression has difficulty in domain adaption. In the contrast, stereo methods can be directly applied to the new scenario and achieve awesome estimation because its fundamental is the explicit spatial view difference on stereo images. FBDepth applies the explicit spatial measurement and does not reply on the camera and scenarios heavily. It requires several learning models but these model can be applied to common cameras and microphones. FBDepth can be more general with a more diverse dataset. We show some visual qualitative results in the Appendix B.3. Compared to other methods, the delay between audio and video can be visually recognized, which is similar to object detection. In another word, FBDepth transforms the tough depth estimation problem to a simple interpretable problem.\n\n\n# 5.3 ABLATION\n\n\nIn the ablation study, we show how each stage contributes to the final results.\n\n\n<table><tr><td>Method</td><td>AbsErr(m)</td><td>AbsRel(%)</td><td>RMSE(m)</td><td>Recall</td><td>Precision</td></tr><tr><td>event loc w/o flow</td><td>-</td><td>-</td><td>-</td><td>87.3</td><td>93.7</td></tr><tr><td>FBDepth w/o interp</td><td>1.95</td><td>9.16</td><td>4.07</td><td>-</td><td>-</td></tr><tr><td>FBDepth w/ center</td><td>1.39</td><td>6.57</td><td>2.31</td><td>-</td><td>-</td></tr><tr><td>FBDepth w/_bbox</td><td>1.23</td><td>5.65</td><td>2.06</td><td>-</td><td>-</td></tr><tr><td>FBDepth w/o RGB-F</td><td>0.92</td><td>4.25</td><td>1.42</td><td>-</td><td>-</td></tr><tr><td>FBDepth</td><td>0.64</td><td>2.98</td><td>1.03</td><td>94.5</td><td>98.7</td></tr></table>\n\n\nTable 2: Ablation study for FBDepth using different setups at each stage. The input is 240 FPS.\n\n\nEvent-level localization We invest how the optical flow can help detect the collision event as well as contour the object mask. We define recall and precision as the percentage of correct recognized audio-visual events in all audio-visual events and all recognized events with an IoU more than 0.5, respectively. With the flow, both recall and precision improve as the flow can work as a pre-mask to guide the network. The main failures in recall come from weak collision sounds or simultaneous collisions. The incorrect recognition is mainly due to similar objects in the frame.\n\n\n![(a) Temporal error of the estima- (b) Improvement ratio of tempo  \ntion of low FPS compared to 240 ral resolution FPS](images/b6344b31ea4e5e006eea0189956a758bb60c7e91ea6b33e750c46593450129e3.jpg)\n\n\n![Figure 2: Effectiveness the video event detection in the second stage; change figure ratio](images/64ac59a66a20880f4c1ef4240ed7444fad4900cd841c86af619a94e426b1ffb5.jpg)\n\n\nFrame-level localization Frame rate is most related to the frame-level stage. We observe that increasing the frame rate reduces the numerical error of FBDepth in Table 1. Especially, increasing 30 FPS to 60 FPS yields the largest improvement, and the benefit gradually tapers off with a further increase in the frame rate. We observe that 30 FPS is too slow to capture sudden movements and\n\n\nfast dynamics while 60 FPS is around the borderline. It is consistent with the trend to set 60 FPS as the default video recording and playing. The motion in 120FPS and 240 FPS is even slower so it is more difficult to distinguish the frame  $I_{e}$ . The frame error is no more than the one in the low FPS mode. Thus, 120 FPS and 240 FPS bring less improvement.\n\n\nMs-level localization We investigate our special interpolation in two perspectives. First, we need to verify whether this method works. However, there is no ground truth timestamp so we cannot directly quantify the accuracy. We set the estimation of 240 FPS as a baseline and compare it with the estimation of lower FPS. If it can get similar numerical results from independent input, which means the algorithm is reliable. In Figure refmicro2, the median temporal error for 30, 60, and 120 FPS is 2.3ms, 0.65ms, and 0.5ms respectively. Considering the frame resolution, we can compute the improvement ratio as frame_duration temporal_error. The 60 FPS has the largest 25x improvement over the frame duration. This is strong evidence that our ms-level localization is reasonable and robust.\n\n\nSecond, we compare the performance of depth estimation with different interpolation strategies in Table 2. We use the result from frame localization to predict the depth when there is no interpolation. The error is large since this timestamp is ambiguous for the depth prediction. Interpolation with the traces of centers or bounding boxes does not work well. A few key points cannot capture the dynamics in fine granularity.\n\n\nDepth regression Without the RGB-F channel of the target object in depth regression, the estimation will be less robust due to the ambient sound and the background noise as shown in the Table 2\n\n\n# 6 LIMITATION AND FUTURE WORKS\n\n\nWe classify audio-visual events into 3 categories by the quality and the quantity of visual cues during the sound production.\n\n\nObvious visual cues during sound production (e.g. collision) This is the main scenario we try to address in this paper. It requires both visible procedure and audible sound to estimate the depth. We can apply it to sports analytics, human stepping, etc. Moreover, it can collect the sparse depth point and accumulate depth points over time. According to existing work on depth completion (Long et al., 2021b; Xu et al., 2019b), adding some accurate depth points can boost the performance of monocular depth.\n\n\nIndirect visual cues during sound production (e.g.speech, playing the piano) This scenario is challenging but common every day. They do not show the vibration visually. Fortunately, there are still lots of visual cues. Existing work on speech synthesis with lip motion(Ephrat & Peleg, 2017), and music generation with pose(Gan et al., 2020) indicates the strong semantic relationship between video and audio. The spatial correlation still holds here. We propose to apply a high-resolution multi-frame alignment between the video and audio to find the accurate propagation delay.\n\n\nNo visual cues during sound production (e.g. car engines, mobile phone speaker) We admit that we have no idea to estimate the depth when these sound sources are static because we cannot see them at all. Luckily, we still have a chance when these sound sources move. We propose a Doppler-like formulation to associate visual cues and audio cues.\n\n\nAnother urgent problem is that the microphone is pretty challenging to synchronize with other sensors. Pushing the latency to the sub-ms level can boost many applications including FBDepth.\n\n\n# 7 CONCLUSION\n\n\nIn this paper, we develop a novel depth estimation method based on the \"Flash-to-Bang\". By aligning the video with the audio and detecting the events from both, we can estimate the depth in the wild without calibration or prior knowledge about the environment or target. Our extensive evaluation shows that our approach yields similar errors across varying distances. In comparison, the errors of several existing methods increase rapidly with distance. Therefore, our method is particularly attractive for large distances. As part of our future work, we are interested in further enhancing the accuracy of our method, generalizing to more contexts, and using the estimated depth to the collision to estimate the depth to other objects in the scene.\n\n\n# REFERENCES\n\n- Apple. Apple developer documentation., 2022. URL https://developer.apple.com/ documentation/avfoundation/additional_data Capture/capturing_ photos_with_depth.\n\n- Relja Arandjelovic and Andrew Zisserman. Objects that sound. In Proceedings of the European conference on computer vision (ECCV), pp. 435-451, 2018.\n\n- Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth, Michael J Black, and Richard Szeliski. A database and evaluation methodology for optical flow. International journal of computer vision, 92(1):1-31, 2011.\n\n- Dan Barnes, Matthew Gadd, Paul Murcutt, Paul Newman, and Ingmar Posner. The oxford radar robotcar dataset: A radar extension to the oxford robotcar dataset. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 6433-6438. IEEE, 2020.\n\n- Amlaan Bhoi. Monocular depth estimation: A survey. pre-print, 2019. https://arxiv.org/pdf/1901.09402.pdf.\n\n- Hannah Bull, Triantafyllos Afouras, G\u00fcl Varol, Samuel Albanie, Liliane Momeni, and Andrew Zisserman. Aligning subtitles in sign language videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11552-11561, 2021.\n\n- Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11621-11631, 2020.\n\n- Changan Chen, Ziad Al-Halah, and Kristen Grauman. Semantic audio-visual navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15516-15525, 2021a.\n\n- Changan Chen, Wei Sun, David Harwath, and Kristen Grauman. Learning audio-visual dereverberation. arXiv preprint arXiv:2106.07732, 2021b.\n\n- Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao Dai, Xiaojun Chang, Hongdong Li, Tom Drummond, and Zongyuan Ge. Hierarchical neural architecture search for deep stereo matching. Advances in Neural Information Processing Systems, 33, 2020.\n\n- Zhixiang Chi, Rasoul Mohammadi Nasiri, Zheng Liu, Juwei Lu, Jin Tang, and Konstantinos N Plataniotis. All at once: Temporally adaptive multi-frame interpolation with advanced motion modeling. In European Conference on Computer Vision, pp. 107-123. Springer, 2020.\n\n- Joon Son Chung and Andrew Zisserman. Lip reading in the wild. In *Asian conference on computer vision*, pp. 87-103. Springer, 2016.\n\n- Ariel Ephrat and Shmuel Peleg. Vid2speech: speech reconstruction from silent video. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5095-5099. IEEE, 2017.\n\n- Chuang Gan, Deng Huang, Peihao Chen, Joshua B Tenenbaum, and Antonio Torralba. Foley music: Learning to generate music from videos. In European Conference on Computer Vision, pp. 758-775. Springer, 2020.\n\n- Chuang Gan, Yi Gu, Siyuan Zhou, Jeremy Schwartz, Seth Alter, James Traer, Dan Gutfreund, Joshua B Tenenbaum, Josh H McDermott, and Antonio Torralba. Finding fallen objects via asynchronous audio-visual integration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10523\u201310533, 2022.\n\n- Ruohan Gao, Changan Chen, Ziad Al-Halah, Carl Schissler, and Kristen Grauman. Visualechoes: Spatial image representation learning through echolocation. In European Conference on Computer Vision, pp. 658-676. Springer, 2020.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.\n\n- John Hershey and Javier Movellan. Audio vision: Using audio-visual synchrony to locate sounds. Advances in neural information processing systems, 12, 1999.\n\n- Saif Imran, Xiaoming Liu, and Daniel Morris. Depth completion with twin surface extrapolation at occlusion boundaries. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2583-2592, 2021.\n\n- Jan Kotera, Denys Rozumnyi, Filip Sroubek, and Jiri Matas. Intra-frame object tracking by deblating. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pp. 0-0, 2019.\n\n- Jan Kotera, Ji\u0159\u00ed Matas, and Filip \u0160roubek. Restoration of fast moving objects. IEEE Transactions on Image Processing, 29:8577-8589, 2020.\n\n- Hamid Laga, Laurent Valentin Jospin, Farid Boussaid, and Mohammed Bennamoun. A survey on deep learning techniques for stereo-based depth estimation. pre-print, 2020. https://arxiv.org/pdf/2006.02535.pdf.\n\n- Zhenyu Li, Zehui Chen, Ang Li, Liangji Fang, Qinhong Jiang, Xianming Liu, and Junjun Jiang. Unsupervised domain adaptation for monocular 3d object detection via self-training. arXiv preprint arXiv:2204.11590, 2022.\n\n- Yan-Bo Lin, Yu-Jhe Li, and Yu-Chiang Frank Wang. Dual-modality seq2seq network for audiovisual event localization. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2002-2006. IEEE, 2019.\n\n- LIVOX. Livox mid-70 spec, 2021. URL https://www.livoxtech.com/mid-70/specs.\n\n- Xiaoxiao Long, Lingjie Liu, Wei Li, Christian Theobalt, and Wenping Wang. Multi-view depth estimation using epipolar spatio-temporal networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8258-8267, 2021a.\n\n- Yunfei Long, Daniel Morris, Xiaoming Liu, Marcos Castro, Punarjay Chakravarty, and Praveen Narayanan. Radar-camera pixel depth association for depth completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12507-12516, 2021b.\n\n- Yawen Lu and Guoyu Lu. An alternative of lidar in nighttime: Unsupervised depth estimation based on single thermal image. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3833-3843, 2021.\n\n- Wenguang Mao, Jian He, and Lili Qiu. CAT: high-precision acoustic motion tracking. In Proc. of ACM MobiCom, 2016.\n\n- Wenguang Mao, Mei Wang, Wei Sun, Lili Qiu, Swadhin Pradhan, and Yi-Chao Chen. Rnn-based room scale hand motion tracking. In The 25th Annual International Conference on Mobile Computing and Networking, pp. 1-16, 2019.\n\n- Simone Meyer, Oliver Wang, Henning Zimmer, Max Grosse, and Alexander Sorkine-Hornung. Phase-based frame interpolation for video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1410-1418, 2015.\n\n- Simone Meyer, Abdelaziz Djelouah, Brian McWilliams, Alexander Sorkine-Hornung, Markus Gross, and Christopher Schroers. Phasenet for video frame interpolation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 498-507, 2018.\n\n- Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable convolution. In Proceedings of the IEEE International Conference on Computer Vision, pp. 261-270, 2017a.\n\n- Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive convolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 670-679, 2017b.\n\n- Kranti Kumar Parida, Siddharth Srivastava, and Gaurav Sharma. Beyond image to depth: Improving depth prediction using echoes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8268-8277, 2021.\n\n- Junheum Park, Keunsoo Ko, Chul Lee, and Chang-Su Kim. Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation. In European Conference on Computer Vision, pp. 109-125. Springer, 2020.\n\n- Jiaxiong Qiu, Zhaopeng Cui, Yinda Zhang, Xingdi Zhang, Shuaicheng Liu, Bing Zeng, and Marc Pollefeys. Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3313-3322, 2019.\n\n- Caleb Rascon and Ivan Meza. Localization of sound sources in robotics: A review. Robotics and Autonomous Systems, 96:184-210, 2017.\n\n- Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. Springer, 2015.\n\n- Denys Rozumnyi, Jan Kotera, Filip Sroubek, and Jiri Matas. Sub-frame appearance and 6d pose estimation of fast moving objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6778-6786, 2020.\n\n- Sheng Shen, Daguan Chen, Yu-Lin Wei, Zhijian Yang, and Romit Roy Choudhury. Voice localization using nearby wall reflections. In Proc. of ACM MobiCom, 2020.\n\n- Nikhil Singh, Jeff Mentch, Jerry Ng, Matthew Beveridge, and Iddo Drori. Image2reverb: Cross-modal reverb impulse response synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 286-295, 2021.\n\n- StereoLab. Zed 2 camera dataset. https://www.stereolabs.com/assets/datasheets/zed2-camera-datasheet.pdf, 2021.\n\n- Wei Sun, Mei Wang, and Lili Qiu. Spatial aware multi-task learning based speech separation. arXiv preprint arXiv:2207.10229, 2022.\n\n- Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 247-263, 2018.\n\n- J-M Valin, Fran\u00e7ois Michaud, Jean Rouat, and Dominic L\u00e9tourneau. Robust sound source localization using a microphone array on a mobile robot. In Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003)(Cat. No. 03CH37453), volume 2, pp. 1228-1233. IEEE, 2003.\n\n- Deepak Vasisht, Swarun Kumar, and Dina Katabi. Decimeter-level localization with a single wifi access point. In 13th {USENIX} Symposium on Networked Systems Design and Implementation (\\{NSDI\\} 16), pp. 165-178, 2016.\n\n- Mei Wang, Wei Sun, and Lili Qiu. {MAVL}: Multiresolution analysis of voice localization. In 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21), pp. 845-858, 2021.\n\n- Gordon Wichern, Joe Antognini, Michael Flynn, Licheng Richard Zhu, Emmett McQuinn, Dwight Crow, Ethan Manilow, and Jonathan Le Roux. Wham!: Extending speech separation to noisy environments. arXiv preprint arXiv:1907.01160, 2019.\n\n- Yu Wu, Linchao Zhu, Yan Yan, and Yi Yang. Dual attention matching for audio-visual event localization. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6292\u20136300, 2019.\n\n- Yan Xia and Zhou Zhao. Cross-modal background suppression for audio-visual event localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19989-19998, 2022.\n\n- Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and Ming-Hsuan Yang. Quadratic video interpolation. Advances in Neural Information Processing Systems, 32, 2019a.\n\n- Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun Bao, and Hongsheng Li. Depth completion from sparse lidar data with depth-normal constraints. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2811-2820, 2019b.\n\n- Chongjian Yuan, Xiyuan Liu, Xiaoping Hong, and Fu Zhang. Pixel-level extrinsic self calibration of high resolution lidar and camera in targetless environments. IEEE Robotics and Automation Letters, 6(4):7517-7524, 2021.\n\n- Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Newcrfs: Neural window fully-connected crfs for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Pavel Zahorik. Direct-to-reverberant energy ratio sensitivity. The Journal of the Acoustical Society of America, 112(5):2110-2117, 2002.\n\n- Diana Zhang, Akarsh Prabhakara, Sirajum Munir, Aswin Sankaranarayanan, and Swarun Kumar. A hybrid mmwave and camera system for long-range depth imaging. arXiv preprint arXiv:2106.07856, 2021.\n\n- Kai Zhang, Jiaxin Xie, Noah Snavely, and Qifeng Chen. Depth sensing beyond lidar range. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1692-1700, 2020.\n\n- Zhengyou Zhang. Microsoft Kinect sensor and its effect. IEEE multimedia, 19(2):4-10, 2012.\n\n- Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. Audio-visual segmentation. arXiv preprint arXiv:2207.05042, 2022.\n\nResponse: ### \\boxed{Accept}\n\n#### Summary of Review:\nThe manuscript presents a novel approach to depth estimation using audio and visual cues, specifically leveraging the flash-to-bang phenomenon. The authors propose FBDepth, a novel audio-visual learning scheme that integrates semantic features with physical spatial cues to boost monocular depth estimation.\n\n**Strengths:**\n1. **Novelty and Conceptual Innovation**: The paper introduces a fundamentally new approach by combining audio and visual cues to estimate depth, which addresses limitations of existing monocular and stereo methods.\n2. **Methodological Rigor**: The authors provide a detailed formulation of the problem and a thorough explanation of the proposed method, including the audio-visual localization task, frame-level and ms-level localization, and depth regression.\n3. **Experiments and Results**: The experiments showcase significant improvements over state-of-the-art methods across various metrics, demonstrating the effectiveness of the proposed approach.\n4. **Datasets and Evaluation**: The paper provides a comprehensive AVD dataset and detailed evaluation results, which are crucial for validating the approach.\n\n**Weaknesses:**\n1. **Temporal Resolution Limitations**: The authors acknowledge that temporal resolution is limited due to the frame rate, which might affect the performance at very short distances. This could be addressed by future work on reducing latency.\n2. **Generalization**: While the results are promising, the evaluation primarily focuses on indoor scenes. Expanding the evaluation to more varied and dynamic environments would strengthen the claim of generality.\n3. **Noise Handling**: The paper mentions handling noise in the audio and video streams but does not delve deeply into the specifics of noise reduction techniques. Future work could include more robust noise handling mechanisms.\n4. **Detailed Discussion on Baseline Methods**: The baseline methods are mentioned briefly, and a more detailed discussion comparing their performance across different metrics would enhance the manuscript.\n\nOverall, the paper presents a well-thought-out and innovative approach with strong empirical validation. The authors should address the noted limitations to further solidify the contributions and ensure broader applicability of the method.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634882.7328095", "msecs": "732.0", "relativeCreated": "227556.28371238708", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research manuscripts for potential acceptance in a peer-reviewed journal. Assess the paper's originality, methodology, significance, and clarity, prioritizing evidence-based reasoning. Identify strengths (e.g., novel contributions, rigorous analysis) and weaknesses (e.g., methodological flaws, insufficient justification) while adhering to peer review principles. Provide balanced, constructive feedback that objectively weighs the paper's merits against field-specific standards, ensuring reproducibility, ethical compliance, and alignment with the journal's scope. Avoid personal biases and focus on the work's scientific merit and impact.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Data-centric Prediction Explanation via Kernelized Stein Discrepancy\n\n# Abstract\nExisting example-based prediction explanation methods often bridge test and training data points through the model\u2019s parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant computational overhead or producing coarse-grained explanations. This paper presents a Highly-precise and Data-centric Explanation (HD-Explain) prediction explanation method that exploits properties of Kernelized Stein Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized kernel function for a trained model that encodes model-dependent data correlation. By leveraging the kernel function, one can identify training samples that provide the best predictive support to a test point efficiently. We conducted thorough analyses and experiments across multiple classification domains, where we show that HD-Explain outperforms existing methods from various aspects, including 1) preciseness (fine-grained explanation), 2) consistency, and 3) computation efficiency, leading to a surprisingly simple, effective, and robust prediction explanation solution.\n\n\n# 1 INTRODUCTION\n\n\nAs one of the decisive factors affecting the performance of a Machine Learning (ML) model, training data points are of great value in promoting the model's transparency and trustworthiness, including explaining prediction results, tracing sources of errors, or summarizing the characteristics of the model (Cai et al., 2019; Anik & Bunt, 2021; Nam et al., 2022). The challenges of example-based prediction explanation mainly come from retrieving relevant data points from a vast pool of training samples or justifying the rationale of such explanations (Lim et al., 2019; Zhou et al., 2021).\n\n\nModern example-based prediction explanation methods commonly approach the above challenges by constructing an influence chain between training and test data points (Li et al., 2020; Nam et al., 2022; Tsai et al., 2023). The influence chain could be either data points' co-influence on model parameters or their similarity in terms of latent representations. In particular, Influence Function (Koh & Liang, 2017), one of the representative, model-aware explanation methods, looks for the shift of the model parameters (due to up-weighting each training sample) as the sample's influence score. Since computing the inverse Hessian matrix is challenging, the approach adapts Conjugate Gradients Stochastic Estimation and the Perlmutter trick to reduce its computation cost. Representer Point Selection (RPS) (Yeh et al., 2018), as another example, reproduces the representer theorem by refining the trained neural network model with  $L2$  regularization, such that the influence score of each training sample can be represented as the gradient of the predictive layer. While computationally efficient, RPS is criticized for producing coarse-grained explanations that are more class-level rather than instance-level explanations (Sui et al., 2021) (In this paper we use the instance level explanation and example-based explanation interchangeably.). Multiple later variants (Pruthi et al., 2020; Sui et al., 2021) attempted to mitigate the drawbacks above, but their improvements were often limited by the cause of their shared theoretical scalability bounds.\n\n\nThis paper presents Highly-precise and Data-centric Explanation (HD-Explain), a post-hoc, model-aware, example-based explanation solution for neural classifiers. Instead of relying on data co-influence on model parameters or feature representation similarity, HD-Explain retains the influence chain between training and test data points by exploiting the underrated properties of Kernelized Stein Discrepancy (KSD) (Liu et al., 2016) between the trained predictive model and its training\n\n\nTable 1: Summary of existing Post-hoc Example-based Prediction Explanation Methods that work with deep neural networks. Practicality of the whole model explanation is measured by the feasibility of explaining the prediction of ResNet-18 trained on CIFAR-10 with a single A100 GPU machine. CIFAR-10 is a small benchmark data with 50000 training samples.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Explanation of</td><td rowspan=\"2\">Need optimization as sub-routine</td><td colspan=\"2\">Whole model explanation</td><td rowspan=\"2\">Inference computation complexity bounded by</td><td rowspan=\"2\">Memory/cache (of each training sample) bounded by</td></tr><tr><td>Theoretical</td><td>Practical</td></tr><tr><td>Influence Function</td><td>Original Model</td><td>Yes (Iterative HVP approximation)</td><td>Yes</td><td>No</td><td>1.H\u03b8-1\u2207\u03b8L(xt,\u03b8) approximation 2. &lt; \u2207\u03b8L(x,\u03b8), H\u03b8-1\u2207\u03b8L(xt,\u03b8) &gt;</td><td>Size of model parameters</td></tr><tr><td>RPS</td><td>Fine-tuned Model</td><td>Yes (L2 regularized last layer retrain)</td><td>No</td><td>No</td><td>1.last layer representation ft 2.&lt; \u03b1tf, ft &gt;</td><td>Size of model parameters of the last layer</td></tr><tr><td>TracIn*</td><td>Original Model</td><td>No</td><td>Yes</td><td>No</td><td>1.\u2207\u03b8L(xt,\u03b8) approximation 2. &lt; \u2207\u03b8L(x,\u03b8), \u2207\u03b8L(xt,\u03b8) &gt;</td><td>Size of model parameters</td></tr><tr><td>HD-Explain</td><td>Original Model</td><td>No</td><td>Yes</td><td>Yes</td><td>1.\u2207xf(xt,\u03b8)yt 2. Closed-form k\u03b8(x,xt) defined by KSD</td><td>Size of data dimension</td></tr></table>\n\n\ndataset. Specifically, we note that the Stein operator augmented kernel uniquely defines a pairwise data correlation (in the context of a trained model) whose expectation on the training dataset results in the minimum KSD (as a discrete approximation) compared to that of the dataset sampled from different distributions. By exploiting this property, we can 1) reveal a subset of training data points that provides the best predictive support to the test point and 2) identify the potential distribution mismatch among training data points. Jointly leveraging these advantages, HD-Explain can produce explanations faithful to the original trained model.\n\n\nThe contributions of our work are summarized as follows: 1) We propose a novel example-based explanation method. 2) We propose several quantitative evaluation metrics to measure the correctness and effectiveness of generated explanations. 3) We perform a thorough evaluation comparing several existing explanation methods across a wide set of classification tasks.\n\n\nOur findings conclude that HD-Explain offers fine-grained, instance-level explanations with remarkable computational efficiency, compared to well-known example-based prediction explanation methods. Its algorithmic simplicity, effective performance and scalability enables its deployment to real world scenarios where transparency and trustworthiness is essential.\n\n\n# 2 PRELIMINARY AND RELATED WORK\n\n\n# 2.1 POST-HOC CLASSIFIER EXPLANATION BY EXAMPLES\n\n\nPost-hoc Classifier Explanation by Examples (a.k.a prototypes, representatives) refers to a category of classifier explanation approaches that pick a subset of training data points as prediction explanations without accessing the model training process. Its research history spans from the model-intrinsic approach (see survey Molnar (2020)) to the recent impact-based approach (Li et al., 2020).\n\n\nModel-inherent approaches (Molnar, 2020) refer to machine learning models that are considered interpretable such as  $k$ -nearest neighbor (Peterson, 2009) or decision tree; For a given test data point, similar data points on the raw feature space can be efficiently selected as explanations through the inherent decision making mechanism of the self-explanatory machine learning models. In fact, attracted by their inherent explanatory power, multiple well-known works attempted to compile complex black-box models into self-explanatory models for enabling prediction explanation (Frosst & Hinton, 2017), while computationally inefficient.\n\n\nTo unlock the general explanatory power applicable to black-box models, multiple later studies suggested to fall back to statistics-based solutions, looking for prototype samples that represent data points that either are common in the dataset or play critical roles in data distribution. MMD-critic (Kim et al., 2016) and Normative and Comparative explanations (Cai et al., 2019) are the well-known examples in this category. Unfortunately, those approaches are often with strong assumption of good prototypes (which often overlook the characteristics of trained models) (Li et al., 2020), making their prediction explanations general to the training dataset rather than a trained model instance.\n\n\nRecently, influence-based methods have emerged as the prevailing technique in model explanation (Li et al., 2020; Nam et al., 2022; Bae et al., 2022; Park et al., 2023a). Influence function (Koh & Liang, 2017), as one of the earliest influence-based solutions, bridges the outcome of a prediction task to training data points by, first, evaluating training data's influence on the model parameters and, then, estimating how model parameter changes affect prediction. Similarly, Representative Point Selection (RPS) (Yeh et al., 2018) builds such an influence chain by fitting the representation theorem, where the weighted product between the representations of test and training samples comes into play. Concerning the computational overhead of previous work, the later solution TracIn (Pruthi et al., 2020) proposed a simple approximation of the influence function via a first-order Taylor expansion (essentially Neural Tangent Kernel (Jacot et al., 2018)), successfully discarding the inverse Hessian matrix from the influence chain formulation. BoostIn (Brophy et al., 2023) further extends TracIn and is dedicated to interpreting the predictions of gradient-boosted decision trees. RPS-LJE (Sui et al., 2021), on the other hand, alleviated the inconsistent explanation problem of RPS through Local Jacobian Expansion. In the latest publication (Tsai et al., 2023), all of the above methods described in this paragraph are identified as special cases of Generalized Representatives but with different chosen kernels.\n\n\nOne limitation of the current influence-based methods is that they attribute the influence of each training data point to the parameters of the trained model as an essential intermediate step. Indeed, as the nature of stochastic gradient descent (the dominating training strategy of neural networks), isolating such contribution is barely possible without 1) relying on approximations or 2) accessing the training process. Unfortunately, either solution would result in performance degradation or heavy computational overhead (Schioppa et al., 2022). Hence, this work delves into the exploration of an alternative influence connection between training and test data points without exploiting the perturbation of model parameters.\n\n\n# 2.2 KERNELIZED STEIN DISCREPANCY\n\n\nThe idea of Kernelized Stein Discrepancy (KSD) (Liu et al., 2016) can be traced back to a theorem called Stein Identity (Kattumannil, 2009) that states, if a smooth distribution  $p(x)$  and a function  $\\phi(x)$  satisfy  $\\lim_{||x|| \\to \\infty} p(x)\\phi(x) = 0$ ,\n\n\n$$\n\n$$\n\\mathbb {E} _ {x \\sim p} [ \\phi (x) \\nabla_ {x} \\log p (x) + \\nabla_ {x} \\phi (x) ] = 0, \\quad \\forall \\phi .\n$$\n\n$$\n\n\nThe identity can characterize distribution  $p(x)$  such that it is often served to assess the goodness-of-fit (Kubokawa, 2024) of the model. The above expression could be further abstracted to use function operator  $\\mathcal{A}_p$  (a.k.a Stein operator) such that\n\n\n$$\n\n$$\n\\mathcal {A} _ {p} \\phi (x) = \\phi (x) \\nabla_ {x} \\log p (x) + \\nabla_ {x} \\phi (x),\n$$\n\n$$\n\n\nwhere the operator encodes distribution  $p(x)$  in the form of derivative to input (a.k.a score function).\n\n\nStein's identity offers a mechanism to measure the gap between two distributions by assuming the variable  $x$  is sampled from a different distribution  $q \\neq p$  such that\n\n\n$$\n\n$$\n\\sqrt {\\mathbb {S} (q , p)} = \\max _ {\\phi \\in \\mathcal {F}} \\mathbb {E} _ {x \\sim q} [ \\mathcal {A} _ {p} \\phi (x) ],\n$$\n\n$$\n\n\nwhere the expression takes the most discriminant  $\\phi$  that maximizes the violation of Stein's identity to quantify the distribution discrepancy. This discrepancy is, accordingly, referred as Stein Discrepancy.\n\n\nThe challenge of computing Stein Discrepancy comes from the selection of function set  $\\mathcal{F}$ , which motivates the later innovation of KSD that takes  $\\mathcal{F}$  to be the unit ball of a reproducing kernel Hilbert space (RKHS). By leveraging the reproducing property of RKHS, the KSD could be eventually transformed into\n\n\n$$\n\n$$\n\\mathbb {S} (q, p) = \\mathbb {E} _ {x, x ^ {\\prime} \\sim q} [ \\kappa_ {p} (x, x ^ {\\prime}) ]\n$$\n\n$$\n\n\nwhere  $\\kappa_{p}(x,x^{\\prime}) = \\mathcal{A}_{p}^{x}\\mathcal{A}_{p}^{x^{\\prime}}k(x,x^{\\prime})$  that can work with arbitrary kernel function  $k(x,x^{\\prime})$ . See Appendix C for expanded derivations.\n\n\nIn the literature, KSD has been adopted for tackling three types of application tasks - 1) parameter inference (Barp et al., 2019), 2) Goodness-of-fit tests (Chwialkowski et al., 2016; Liu et al., 2016; Yang et al., 2018), and 3) particle filtering (sampling) (Gorham et al., 2020; Korba et al., 2021). However, to the best of our knowledge, its innate property that uniquely defines model-dependent data correlation has never been exploited, which, we note, is valuable to interpret model behaviour from various aspects, including instance-level prediction explanation and global prototypical explanations.\n\n\n![Figure 1: Varying of Kernelized Stein Discrepancy given the shift of training data distribution on Two Moon dataset.](images/29fd56d5ca01c162d34e47e66ed4c4d2a26156520176e453f3284b16211e0f5c.jpg)\n\n\n# 3 HIGHLY-PRECISE AND DATA-CENTRIC EXPLANATION\n\n\nHD-Explain is an example-based prediction explanation method based on Kernelized Stein Discrepancy. Consider a trained classifier  $f_{\\theta}$  as the outcome of a training process with Maximum Likelihood Estimation (MLE)\n\n\n$$\n\n$$\n\\underset {\\theta} {\\operatorname {a r g m a x}} \\mathbb {E} _ {(\\mathbf {x}, y) \\sim P _ {D}} \\left[ \\log P _ {\\theta} (y | \\mathbf {x}) \\right].\n$$\n\n$$\n\n\nTheoretically, maximizing observation likelihood is equivalent to minimizing a KL divergence between data distribution  $P_{D}$  and the parameterized distribution  $P_{\\theta}$  such that\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathbb {D} _ {\\mathrm {K L}} \\left(P _ {D}, P _ {\\theta}\\right) = \\mathbb {E} _ {(\\mathbf {x}, y) \\sim P _ {D}} \\left[ \\log \\frac {P _ {D} (\\mathbf {x} , y)}{P _ {\\theta} (\\mathbf {x} , y)} \\right] \\\\ = - \\underbrace {\\mathbb {E} _ {(\\mathbf {x} , y) \\sim P _ {D}} \\left[ \\log P _ {\\theta} (y | \\mathbf {x}) \\right]} _ {\\text {L i k e l i h o o d}} + \\underbrace {\\mathbb {E} _ {(\\mathbf {x} , y) \\sim P _ {D}} \\left[ \\log P _ {D} (y | \\mathbf {x}) \\right]} _ {\\text {c o n s t a n t}} + \\underbrace {\\mathbb {E} _ {(\\mathbf {x} , y) \\sim P _ {D}} \\left[ \\log P _ {D} (\\mathbf {x} / P _ {\\theta} (\\mathbf {x}) \\right]} _ {\\text {c o n s t a n t a s \\theta d o e s n o t m o d e l i n p u t s}}, \\\\ \\end{array}\n$$\n\n$$\n\n\nwhich, in turn, is proven to align with minimizing KSD in the form of gradient descent (Liu & Wang, 2016)\n\n\n$$\n\n$$\n\\nabla_ {\\epsilon} \\mathbb {D} _ {\\mathrm {K L}} \\left(P _ {D}, P _ {\\theta}\\right) | _ {\\epsilon} = - \\mathbb {S} \\left(P _ {D}, P _ {\\theta}\\right),\n$$\n\n$$\n\n\nwhere  $\\epsilon$  is the step size of gradient decent.\n\n\nThe chain of reasoning above shows that a well-trained classifier  $f_{\\theta}$  through gradient-descent should lead to minimum discrepancy between the training dataset distribution and the model encoded distribution  $\\mathbb{S}(P_D, P_\\theta)^1$ . We empirically verify the connection through simple examples as shown in Figure 1, where the changes in training data distribution would result in larger KSD compared to that of the original training data distribution. Intuitively, the connection shows that there is a tie between a model and its training data points, encoded in the form of a Stein kernel function  $k_{\\theta}(\\cdot, \\cdot)$  defined on each pair of data points. As the kernel function is conditioned on model  $f_{\\theta}$ , we note it is an encoding of data correlation under the context of a trained model, which paves the foundation of the example-based prediction explanation.\n\n\n# 3.1 KSD BETWEEN MODEL AND TRAINING DATA\n\n\nRecall that KSD,  $\\mathbb{S}(P_D,P_\\theta)$ , defines the correlation between pairs of training samples through model  $\\theta$  dependent kernel function with closed-form decomposition\n\n\n$$\n\n$$\n\\begin{array}{l} \\kappa_ {\\theta} \\left(\\left(\\mathbf {x} _ {a}, y _ {a}\\right), \\left(\\mathbf {x} _ {b}, y _ {b}\\right)\\right) = \\mathcal {A} _ {\\theta} ^ {a} \\mathcal {A} _ {\\theta} ^ {b} k (a, b) \\\\ = \\nabla_ {a} \\nabla_ {b} k (a, b) + k (a, b) \\nabla_ {a} \\log P _ {\\theta} (a) \\nabla_ {b} \\log P _ {\\theta} (b) \\tag {1} \\\\ + \\nabla_ {a} k (a, b) \\nabla_ {b} \\log P _ {\\theta} (b) + \\nabla_ {b} k (a, b) \\nabla_ {a} \\log P _ {\\theta} (a), \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere we denote data point  $(\\mathbf{x}_a, y_a)$  with  $a$  for clean notation. The only model-dependent factor in the above decomposition is a derivative  $\\nabla_{\\mathbf{x},y} \\log P_\\theta(\\mathbf{x},y)$  (for both data  $a$  or  $b$ ).\n\n\nHowever, as the KSD only models the discrepancy between joint distributions rather than conditional distributions, it is challenging to estimate discrepancy between predictive models  $P_{\\theta}(y|\\mathbf{x})$  and its training set  $\\mathbf{z} = (\\mathbf{x}, y) \\sim P_D$  without including  $P_{\\theta}(\\mathbf{x})$  into consideration, even though the marginal distribution  $P_{\\theta}(\\mathbf{x})$  is not estimated by the predictive model at all.\n\n\n![Figure 2: Demonstration of HD-Explain on 2D Rectangular synthetic dataset. Left shows the training dataset with three classes. Middle figure shows the explanation support of training data points to a given test point (as black cross), where green shows a higher KSD kernel value. Right shows the distribution of KSD kernel values (over the training set) to the test point, where only a small number of training data points provide strong support to this prediction.](images/1774b418c6af2cab669415f058b4718e6857a15c519ccec6f693df2a0d666015.jpg)\n\n\nInspired by the previous study on Goodness of Fit (Jitkrittum et al., 2020), to unlock KSD support on predictive models, we propose to set  $P_{\\theta}(\\mathbf{x}) \\equiv P_D(\\mathbf{x})$  such that the identical marginal distribution would not contribute to the discrepancy between the joint distributions  $P_{D}(\\mathbf{x},y)$  and  $P_{\\theta}(\\mathbf{x},y)$ . In addition, while original distribution  $P(\\mathbf{x})$  that generates data could be arbitrary complex distribution (that is out of modelling scope of the predictive model), we may simply set data point distribution  $P_{D}$  (not original distribution  $P$ ) as an Uniform distribution over data points in the dataset under the particular context, given a data point is sampled from a generated dataset uniformly. Although the relaxation appears hasty, we believe that the relaxation is valid in the prediction explanation context (and probably only valid in this particular context) in terms of measuring correlation between a pair of data points, where all train/test data points' input are valid observation (e.g. images) rather than random continuous valued sample in the same space form an unknown distribution.\n\n\nWith the above relaxations, the score function  $\\nabla_{\\mathbf{x},y}\\log P_{\\theta}(\\mathbf{x},y)$  in the Stein operator  $\\mathcal{A}_{\\theta}$  could be derived as a concatenation of the gradient of model  $f_{\\theta}(\\mathbf{x})_y$  to its input  $\\mathbf{x}$  and its probabilistic prediction  $f_{\\theta}(\\mathbf{x})$  in logarithm form, since\n\n\n$$\n\n$$\n\\begin{array}{l} \\nabla_ {\\mathbf {x}, \\mathbf {y}} \\log P _ {\\theta} (\\mathbf {x}, \\mathbf {y}) = \\nabla_ {\\mathbf {x}, \\mathbf {y}} [ \\log P _ {\\theta} (\\mathbf {y} | \\mathbf {x}) + \\log P _ {D} (\\mathbf {x}) ] \\\\ = \\nabla_ {\\mathbf {x}, \\mathbf {y}} \\log P _ {\\theta} (\\mathbf {y} | \\mathbf {x}) + [ \\nabla_ {\\mathbf {x}} \\log P _ {D} (\\mathbf {x}) ] [ \\nabla_ {\\mathbf {y}} P _ {D} (\\mathbf {x}) ] \\\\ = \\left[ \\nabla_ {\\mathbf {x}} \\mathbf {y} ^ {\\top} \\log f _ {\\theta} (\\mathbf {x}) \\right\\rvert   \\left\\lvert   \\right. \\nabla_ {\\mathbf {y}} \\mathbf {y} ^ {\\top} \\log f _ {\\theta} (\\mathbf {x}) ] + [ \\mathbf {0} | | \\mathbf {0} ] \\tag {2} \\\\ = \\left[ \\nabla_ {\\mathbf {x}} \\log f _ {\\theta} (\\mathbf {x}) _ {y} \\right| | \\log f _ {\\theta} (\\mathbf {x}) ], \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere  $[\\cdot ||\\cdot ]$  denotes concatenation operation. We use one-hot vector representation  $\\mathbf{y}$  to represent data label  $y$  here. Since  $P_{D}(\\mathbf{x})$  follows uniform distribution, its gradient to the inputs is a 0 vector.\n\n\nIn the above derivation, we treat discrete label  $y$  as-is without specialized discrete distribution treatments (see (Yang et al., 2018)) to avoid significant computation overhead. In fact, data space in practice is unlikely dense even if a group of features are continuous (e.g. images). Treating the label as a sparse continuous feature can be viewed as an approximation.\n\n\nCombining Equation 1 and 2, we can estimate the correlation of any pairs of training data points conditioned on the trained machine learning model. Computationally, since a score function  $\\nabla_{\\mathbf{x},y}\\log P_{\\theta}(\\mathbf{x},y)$  depends on a single data point, its outputs of the training set could be pre-computed and cached to accelerate the kernel computation. In particular, the output dimension of the score function is simply  $m + k$  for data with  $m$  dimensional features and  $k$  class labels. Compared to the existing solutions, whose training data cache (or influence) are bounded by the dimension of model parameters (such as Influence function, TracIn, RPS, RPS-JLE), the explanation method built on KSD would come with a significant advantage in terms of scalability (see comparison in Table 1). This statement is generally true for neural network based classifiers, whenever the size of model parameters is far larger than the data dimension.\n\n\n# 3.2 PREDICTION EXPLANATION\n\n\nThe computation of kernel function in Equation 1 requires access to features and labels of a data point. While the ground-truth label information is available for the training set, it is inaccessible for a test data point. We consider the predicted class  $\\hat{y}_t$  of the test data point  $\\mathbf{x}_t$  as a label to construct a complete data point  $(\\mathbf{x}_t, \\hat{y}_t)$  and apply the KSD kernel function. For a test data point  $\\mathbf{x}_t$ , we search for top-k training data points that maximize the KSD defined kernel.\n\n\nFigure 2 demonstrates HD-Explain on a 2d synthetic dataset. The distribution of  $\\kappa_{\\theta}(\\mathbf{d},\\cdot)$  in the right most plot shows that only a small number of training data points have a strong influence on a particular prediction.\n\n\n# 4 EVALUATION AND ANALYSIS\n\n\nIn this section, we conduct several qualitative and quantitative experiments to demonstrate various properties of HD-Explain and compare it with the existing example-based solutions.\n\n\nDatasets: We consider multiple disease classification tasks where diagnosis explanation is highly desired. We also introduced synthetic and benchmark classification datasets to deliver the main idea without the need for medical background knowledge. Concretely, we use CIFAR-10  $(32 \\times 32 \\times 3)$ , Brain Tumor (Magnetic Resonance Imaging,  $128 \\times 128 \\times 3$ ), Ovarian Cancer (Histopathology Images,  $128 \\times 128 \\times 3$ ) datasets, and SVHN  $(32 \\times 32 \\times 3)$ . More details are listed in the Appendix F.\n\n\nBaselines: The baseline explainers used in our experiments include Influence Function, Representer Point Selection, and TracIn. While other variants of these baseline explainers exist (Barshan et al., 2020; Sui et al., 2021; Chen et al., 2021), we note they don't offer fundamental performance improvements over the classic ones. In addition, as Influence Function and TracIn face scalability issues, we limit the influence of parameters to the last layer of the model so that they can work with models that contain a large number of parameters. Our experiments use ResNet-18 as the backbone model architecture (with around 11 million trainable parameters) for all image datasets (see Appendix H for detail on our hardware setup).\n\n\nFinally, we also introduce an HD-Explain variant (HD-Explain*) to match the last layer setting of other baseline models, even though HD-Explain can scale up to the whole model without computation pressure. The HD-Explain* is a simple change of HD-Explain in terms of using data representations (the output of the last non-predictive layer of the neural classifier) rather than the raw features. Specifically, we assume a neural network model  $f_{\\theta}$  could be decomposed into two components  $f_{\\theta_2} \\cdot f_{\\theta_1}$ , where  $f_{\\theta_1}$  is a representation encoder and  $f_{\\theta_2}$  is a linear model for prediction. With this decomposition, we define the KSD kernel function for HD-Explain* as\n\n\n$$\n\n$$\n\\begin{array}{l} \\kappa_ {\\theta} \\big ((f _ {\\theta_ {1}} (\\mathbf {x} _ {a}), y _ {a}), (f _ {\\theta_ {1}} (\\mathbf {x} _ {b}), y _ {b}) \\big) = \\mathcal {A} _ {\\theta_ {2}} ^ {a} \\mathcal {A} _ {\\theta_ {2}} ^ {b} k (a, b) \\\\ = \\nabla_ {a} \\nabla_ {b} k (a, b) + k (a, b) \\nabla_ {a} \\log P _ {\\theta_ {2}} (a) \\nabla_ {b} \\log P _ {\\theta_ {2}} (b) \\\\ + \\nabla_ {a} k (a, b) \\nabla_ {b} \\log P _ {\\theta_ {2}} (b) + \\nabla_ {b} k (a, b) \\nabla_ {a} \\log P _ {\\theta_ {2}} (a), \\\\ \\end{array}\n$$\n\n$$\n\n\nwhere we define  $a = (f_{\\theta_1}(\\mathbf{x}_a),y_a)$  and  $b = (f_{\\theta_1}(\\mathbf{x}_b),y_b)$  for short. This setting reduces the prediction explanation to the last layer of the neural network in a similar fashion to RPS.\n\n\nMetrics: In existing example-based explanation works, the experimental results are often demonstrated qualitatively, as visualized explanation instances, without quantitative evaluation. This results in subjective evaluation. In this paper, we propose several quantitative evaluation metrics to measure the effectiveness of each method.\n\n- - Hit Rate: Hit rate measures how likely an explanation sample hits the desired example cases where the desired examples are guaranteed to be undisputed. Specifically, we modify a training data point with minor augmentations (adding noise or flipping horizontally) and use it as a test data point, such that the best explanation for the generated test data point should be the original data point in the training set.\n\n- - Coverage: Given n test data points, the metric measures the number of unique explanation samples an explanation method produces when configuring to return top-k training samples.\n\n\n![(a) High-Conf Correct Predict (b) Low-Conf Correct Predict (c) Low-Conf Incorrect Predict](images/7be398a60d0134ac83305630465254173348b2e52345f0daff375de7faf03248.jpg)\n\n\n![Figure 3: Qualitative evaluation of various example-based explanation methods using CIFAR10. We show three scenarios where the target model makes a) a highly-confident prediction that matches ground truth label, b) a low-confident prediction that matches ground truth label, c) low-confident prediction that does not match ground truth label (which is a bird). For each sub plot, we show top-3 influential training data points picked by the explanation methods for the test example. (a) High-Conf Correct Predict (b) High-Conf Correct Predict (c) Low-Conf Correct Predict Figure 4: Qualitative evaluation of various example-based explanation methods using SVHN. We show two scenarios where the target model makes a-b) a highly-confident prediction that matches ground truth label, c) a low-confident prediction that matches ground truth label. For each sub plot, we show top-3 influential training data points picked by the explanation methods for the test example. We include two samples of high-confidence correct predictions to show the overlap of explanations.](images/34183ff80692e226c3dfb205020e4773748da6ef97f062bf380b94c07eef9bbb.jpg)\n\n\nFormally,\n\n\n$$\n\n$$\n\\text {C o v e r a g e} = \\frac {\\left| \\cup_ {i = 1} ^ {n} e _ {i} \\right|}{n \\times k}\n$$\n\n$$\n\n\nwhere  $e_i$  is the set of top-k explanations for test data point  $i$ . Coverage is motivated to measure the diversity of explanations across a test set where a high value reflects higher granularity (per test point) of the explanation.\n\n\n- Run Time: It measures the run time of an explanation method in wall clock time.\n\n\n# 4.1 QUALITATIVE EVALUATION\n\n\nFigure 3 shows three test cases of the CIFAR10 classification task that cover different classification outcomes, including high-confidence correct prediction, low-confident correct prediction, and low-confident incorrect prediction. For both correct prediction cases, we are confident that HD-Explain provides a better explanation than others in terms of visually matching test data points e.g. brown\n\n\n![(a) Hit Rate](images/8cadb84a504c741375e1fdc483c1c68a10a3d63d2aa51908257fb18f3e3ed192.jpg)\n\n\n![(b) Coverage](images/3e4e1512a4885aa02f5ba409bc07f1f1558ef73aa5406a0ae5eb7a27038f855d.jpg)\n\n\n![(c) Execution Time](images/e775f7f9232d5e7d1c90b51dda90381e6bfd2f5b46a1c92026eac2d8654cd017.jpg)\n\n\n![Figure 5: Quantitative explanation comparison among candidate example-based explanation methods. Data augmentation strategy used is Noise Injection. Error bar shows  $95\\%$  confidence interval. (a) Hit Rate Figure 6: Quantitative explanation comparison among candidate example-based explanation methods. Data augmentation strategy used is Horizontal Flip. Error bar shows  $95\\%$  confidence interval. We reuse the legend of Figure 12.](images/2ecb5f2368d113728dc8312d3c711dbd95cc0a222d617925ce601ddac7e304e2.jpg)\n\n\n![(b) Coverage](images/ca9130d1ad190a2f5b4d36da28d3e958179cff9d938f146ea62adce40b4cc357.jpg)\n\n\n![(c) Execution Time](images/5a7d342ef28ff441af2698140a3172ac3a5c2e7fad5dcbed59f758b7425a22b6.jpg)\n\n\nfrogs in Figure 3 (a) and deer on the grass in Figure 3 (b). In contrast, for the misclassified prediction case (as shown in Figure 3 (c)), we note the HD-Explain produces an example that does not even belong to the same class as the predicted one. Specifically, the predicted class is cat (while the ground truth label is bird), and HD-Explain generates an explanation sample from the deer class. This reflects a low confidence in model's prediction for the particular test example and highlight a potential error in the prediction. RPS also shows such inconsistency in explanation, which aligns with its claim (Yeh et al., 2018). The other two baseline methods do not offer such properties and still produce explanations that match the predicted label well. It is hard to justify how those training samples support such prediction visually (since no clear shared pattern is obvious to us). In addition, it is interesting to see that Influence Function and TracIn produce near identical explanations, reflecting their similarity in leveraging the perturbation of model parameters.\n\n\nFigure 4 provides additional insights on SVHN dataset. HD-Explain again shows a better explanation for producing training samples that appear similar to the test samples. In addition, we notice that RPS produces the same set of explanations for different sample cases, as shown in Fig. 4 (a-b), which reveals its limitation in providing instance-level explanations. To verify this observation further, we conducted a quantitative evaluation as described in the next section.\n\n\nThe qualitative evaluation for OCH and MRI datasets are given in the appendix due to the page limit of the main paper. The overall observations remain consistent with CIFAR10 and SVHN.\n\n\n# 4.2 QUANTITATIVE EVALUATION\n\n\nIn order to perform quantitative evaluation, we limit our experiments to datasets where ground-truth explanation samples are available. Specifically, given a training data sample  $(\\mathbf{x}_i,y_i)$ , we generate a test point  $\\mathbf{x}_t$  by adopting two image data augmentation methods:\n\n- - Noise Injection:  $\\mathbf{x}_t = \\mathbf{x}_i + \\epsilon$  s.t.  $\\epsilon \\sim \\mathcal{N}(0,0.01\\sigma)$ , where  $\\sigma$  is the element-wise standard deviation of features in the entire training dataset.\n\n- - Horizontal Flip:  $\\mathbf{x}_t = \\mathrm{flip}(\\mathbf{x}_i)$ , where we flip images horizontally that do not compromise the semantic meaning of images.\n\n\n![(a) Hit Rate (Noise Injection) Figure 7: Quantitative explanation comparison among HD-Explainers with different kernel functions on all image classification datasets. Error bar shows  $95\\%$  confidence interval.](images/5168ad98de467946936e0b9ac5a8ce73274677e4f580df2c9629c6885ed6d858.jpg)\n\n\n![(b) Coverage both)](images/f21ba73d78b8cfdfd456203c1f5244ebf5c9c3ba3f75af154f8eb72b31190484.jpg)\n\n\n![(c) Hit Rate (Horizontal Flip)](images/18ea99add895f129d01ba446b5cbfe367659861f7e5a336568a39e4bef79b525.jpg)\n\n\n![(d) Execution Time both)](images/e332dde6889af94e070c313fd81915462c39d473764e075147d6fd320422f1c1.jpg)\n\n\nWe created 30 augmented test points for each training data point ( $>10,000$  data points) in each dataset, resulting in more than 300,000 independent runs. Since the data augmentation is guaranteed to maintain prediction consistency, the ideally best explanation for the generated test point is the original data point  $\\mathbf{x}_i$  itself. Hence, the quantitative evaluation could be a sample retrieval evaluation where Hit Rate measures the probability of successful retrieval.\n\n\nFigure 12(a) shows the hit rate comparison among candidate methods on the four image classification datasets under Noise Injection data augmentation. The existing methods face significant difficulty in retrieving the ideal explanatory sample ( $\\leq 10\\%$ ), even with such a simple problem setup; only HD-Explain (and its variant) produces a reasonable successful rate ( $>80\\%$ ). We further investigate the diversity of explanations across a testset using the Coverage metric. Here, diversity indirectly reflects the granularity of an explanation when accumulated over the test dataset. Figure 12(b) shows the Coverage score, the ratio of explanation samples that are unique over many test points. It turns out that existing solutions produce only  $10\\% - 50\\%$  coverage \u2013 many test points receive the same set of explanations, disregarding their unique characteristic. We further observed that the explanations of baselines are often dominated by the class labels; data points predicted as the same class would receive a similar set of explanations. In contrast, HD-Explain shows substantially higher coverage, generating explanations that considers the unique characteristics of each test point.\n\n\nRegarding computation efficiency, while we have summarized the scalability limitation of the candidate methods in Table 1, there was no computational efficiency evaluation conducted in previous works. We recorded the wall clock execution time of each experiments as shown in Figure 12(c). As expected, the Influence Function takes longer to return its explanation than other methods. HD-Explain*, TracIn* and RPS, all use the last layer to generate explanation. RPS showed the lowest compute time since it does not require auto-differentiation for computing the training data influence. HD-Explain* showed the second best compute time and is efficient than TracIn $^{*2}$  and IF. HD-Explain considers the whole model for explanation and its compute time is not directly comparable to others. However, it shows better efficiency than IF across all datasets and is better than TracIn* on CIFAR10.\n\n\nWe observe a similar trend in the other data augmentation scenario, Horizontal Flip, where computation time and coverage are roughly the same, as shown in Figure 6. However, we do notice that, as the outcome of image flipping, the raw feature (pixel) level similarity between  $\\mathbf{x}_t$  and  $\\mathbf{x}_i$  is destroyed. As an outcome, the HD-Explain that works on raw features suffers from performance deduction while other methods, including HD-Explain*, are less impacted. This observation suggests that choosing the layer of explanation might be considered in the practical usage of this approach.\n\n\n# 4.3 KERNEL OPTIONS\n\n\nWe use the Radial Basis Function (RBF) as our default choice of kernel. However, another kernel may better fit a particular application domain. In this experiment, we compare three well-known kernels i.e. Linear, RBF, and Inverse Multi-Quadric (IMQ) on the three image classification datasets.\n\n\nFigure 7 presents the results under both data augmentation scenarios. Overall, the IMQ kernel performs better than the RBF kernel regarding explanation quality (Hit Rate). The advantage is significant when the data augmentation scenario is Horizontal Flip (Figure 7c, which appears more challenging than Noise Injection. IMQ also showed better performance on Coverage (Figure 7b). The linear kernel performs worse compared to other kernels. However, it is substantially efficient than the others, as shown in Figure 7d, highlighting its utility on large datasets. Compared to the baselines presented in Figure 12, we note that the Linear kernel is sufficient for HD-Explain to stand out from other methods in both performance and efficiency.\n\n\n# 4.4 DISCUSSION: INTUITION ON WHY HD-EXPLAIN WORKS\n\n\nAfter showing HD-Explain's empirical performance, we now present our understanding of how HD-Explain finds the explanations for the prediction of a test data point. In particular, we want to understand why the approach is faithful to the pre-trained model.\n\n\nIn HD-Explain, the key metric on measuring the predictive supports of a test point  $\\mathbf{x}_t$  given a training data  $(\\mathbf{x}_i,\\mathbf{y}_i)$  is the KSD defined kernel  $\\kappa_{\\theta}([\\mathbf{x}_t||\\hat{\\mathbf{y}}_t],[\\mathbf{x}_i||\\mathbf{y}_i])$ , where  $\\hat{\\mathbf{y}}_t$  denotes the predicted class label by model  $f_{\\theta}$  in one-hot encoding. By definition, the kernel  $\\kappa_{\\theta}((\\mathbf{x}_a,y_a),(\\mathbf{x}_b,y_b)) = k_{\\theta}(a,b)$  between two data points can be decomposed into four terms\n\n\n$$\n\n$$\n\\begin{array}{l} \\underbrace {\\operatorname {t r a c e} (\\nabla_ {a} \\nabla_ {b} k (a , b))} _ {\u2460} + \\underbrace {k (a , b) \\nabla_ {a} \\log P _ {\\theta} (a) ^ {\\top} \\nabla_ {b} \\log P _ {\\theta} (b)} _ {\u2461} \\\\ \\qquad + \\underbrace {\\nabla_ {a} k (a , b) ^ {\\top} \\nabla_ {b} \\log P _ {\\theta} (b)} _ {\u2462} + \\underbrace {\\nabla_ {b} k (a , b) ^ {\\top} \\nabla_ {a} \\log P _ {\\theta} (a)} _ {\u2463}. \\end{array}\n$$\n\n$$\n\n\nWe examine the effect of each term as follows:\n\n- - \u2460: We note that the first term is often a similarity bias of raw data points given a specified kernel function. In particular, for the RBF kernel  $k(a,b) = \\exp (-\\gamma ||a - b||^2)$ , the first term is simply  $\\sum_{i}^{d + l}2\\gamma k(a,b)$ , where  $d + l$  refers to the sum of input and output (in one-hot) dimensions of a data point. Intuitively, the term shows how similar the two data points are given the RBF kernel. For linear kernel  $k(a,b) = a^\\top b$ , on another hand, the first term is simply  $d + l$  as a constant bias term, which does not deliver any similarity information between the two data points.\n\n- - \u2461: The second term reflects the similarity between two data points in the context of the trained model. In particular, considering the sub-term  $\\nabla_{a}\\log P_{\\theta}(a)^{\\top}\\nabla_{b}\\log P_{\\theta}(b)$ , based on our derivation in Equation 2 (in the main paper), we note it is equivalent to\n\n\n$$\n\n$$\n\\begin{array}{l} \\left[ \\nabla_ {\\mathbf {x} _ {a}} \\log f _ {\\theta} (\\mathbf {x} _ {a}) _ {y _ {a}} | | \\log f _ {\\theta} (\\mathbf {x} _ {a}) \\right] ^ {\\top} [ \\nabla_ {\\mathbf {x} _ {b}} \\log f _ {\\theta} (\\mathbf {x} _ {b}) _ {y _ {b}} | | \\log f _ {\\theta} (\\mathbf {x} _ {b}) ] \\\\ = \\underbrace {\\nabla_ {\\mathbf {x} _ {a}} \\log f _ {\\theta} (\\mathbf {x} _ {a}) _ {y _ {a}} ^ {\\top} \\nabla_ {\\mathbf {x} _ {b}} \\log f _ {\\theta} (\\mathbf {x} _ {b}) _ {y _ {b}}} _ {\\text {s i m i l a r i t y o f s c o r e s (i n p u t g r a d i e n t s)}} + \\underbrace {\\log f _ {\\theta} (\\mathbf {x} _ {a}) ^ {\\top} \\log f _ {\\theta} (\\mathbf {x} _ {b})} _ {\\text {s i m i l a r i t y o f p r e d i c t i o n s}}, \\end{array}\n$$\n\n$$\n\n\nwhere both terms could be viewed as similarity between data points in the context of trained model.\n\n\n- \u2462-\u2463: Both of the last two terms examine the alignment between the score of one data point and the kernel derivative of another data point. We conjecture that this alignment reflects how a test prediction would change if there is a training data point present closer to it than before.\n\n\n# 5 CONCLUSION\n\n\nThis paper presents HD-Explain, a Kernel Stein Discrepancy-driven example-based prediction explanation method. We performed comprehensive qualitative and quantitative evaluation comparing three baseline explanation methods using three datasets. The results demonstrated the efficacy of HD-Explain in generating explanations that are accurate and effective in terms of their granularity level. In addition, compared to other methods, HD-Explain is flexible to apply on any layer of interest and can be used to analyze the evolution of a prediction across layers. HD-Explain serves as an important contribution towards improving the transparency of machine learning models.\n\n\n# REFERENCES\n\n- Ariful Islam Anik and Andrea Bunt. Data-centric explanations: explaining training data of machine learning systems to promote transparency. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-13, 2021.\n\n- Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger B Grosse. If influence functions are the answer, then what is the question? Advances in Neural Information Processing Systems, 35:17953-17967, 2022.\n\n- Alessandro Barp, Francois-Xavier Briol, Andrew Duncan, Mark Girolami, and Lester Mackey. Minimum stein discrepancy estimators. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Elnaz Barshan, Marc-Etienne Brunet, and Gintare Karolina Dziugaite. Relatif: Identifying explanatory training samples via relative influence. In International Conference on Artificial Intelligence and Statistics, pp. 1899-1909. PMLR, 2020.\n\n- Jonathan Brophy, Zayd Hammoudeh, and Daniel Lowd. Adapting and evaluating influence-estimation methods for gradient-boosted decision trees. Journal of Machine Learning Research, 24(154): 1-48, 2023.\n\n- Carrie J Cai, Jonas Jongejan, and Jess Holbrook. The effects of example-based explanations in a machine learning interface. In Proceedings of the 24th international conference on intelligent user interfaces, pp. 258-262, 2019.\n\n- Yuanyuan Chen, Boyang Li, Han Yu, Pengcheng Wu, and Chunyan Miao. Hydra: Hypergradient data relevance analysis for interpreting deep neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 7081-7089, 2021.\n\n- Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. In International conference on machine learning, pp. 2606-2615. PMLR, 2016.\n\n- Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. arXiv preprint arXiv:1711.09784, 2017.\n\n- Jackson Gorham, Anant Raj, and Lester Mackey. Stochasticstein discrepancies. Advances in Neural Information Processing Systems, 33:17931-17942, 2020.\n\n- Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Understanding predictions with data and data with predictions. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 9525-9587. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/ilyas22a.html.\n\n- Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\n- Wittawat Jitkrittum, Heishiro Kanagawa, and Bernhard Scholkopf. Testing goodness of fit of conditional density models with kernels. In Conference on Uncertainty in Artificial Intelligence, pp. 221-230. PMLR, 2020.\n\n- Sudheesh Kumar Kattumannil. On stein's identity and its applications. Statistics & probability letters, 79(12):1444-1449, 2009.\n\n- Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. Examples are not enough, learn to criticize! criticism for interpretability. Advances in neural information processing systems, 29, 2016.\n\n- Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International conference on machine learning, pp. 1885-1894. PMLR, 2017.\n\n- Anna Korba, Pierre-Cyril Aubin-Frankowski, Szymon Majewski, and Pierre Ablin. Kernel stein discrepancy descent. In International Conference on Machine Learning, pp. 5719-5730. PMLR, 2021.\n\n- Tatsuya Kubokawa. Stein's identities and the related topics: an instructive explanation on shrinkage, characterization, normal approximation and goodness-of-fit. *Japanese Journal of Statistics and Data Science*, pp. 1-45, 2024.\n\n- Xiao-Hui Li, Caleb Chen Cao, Yuhan Shi, Wei Bai, Han Gao, Luyu Qiu, Cong Wang, Yuanyuan Gao, Shenjia Zhang, Xun Xue, et al. A survey of data-driven and knowledge-aware explainable ai. IEEE Transactions on Knowledge and Data Engineering, 34(1):29-49, 2020.\n\n- Brian Y Lim, Qian Yang, Ashraf M Abdul, and Danding Wang. Why these explanations? selecting intelligibility types for explanation goals. In IUI Workshops, 2019.\n\n- Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. Advances in neural information processing systems, 29, 2016.\n\n- Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-fit tests. In International conference on machine learning, pp. 276-284. PMLR, 2016.\n\n- Christoph Molnar. Interpretable machine learning. Lulu. com, 2020.\n\n- Chang S Nam, Jae-Yoon Jung, and Sangwon Lee. Human-Centered Artificial Intelligence: Research and Applications. Academic Press, 2022.\n\n- Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale. In International Conference on Machine Learning (ICML), 2023a.\n\n- Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. TRAK: Attributing model behavior at scale. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 27074-27113. PMLR, 23-29 Jul 2023b. URL https://proceedings.mlr.press/v202/park23c.html.\n\n- Leif E Peterson. K-nearest neighbor. *Scholarpedia*, 4(2):1883, 2009.\n\n- Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence by tracing gradient descent. Advances in Neural Information Processing Systems, 33: 19920-19930, 2020.\n\n- Andrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8179-8186, 2022.\n\n- Yi Sui, Ga Wu, and Scott Sanner. Representer point selection via local jacobian expansion for post-hoc classifier explanation of deep neural networks and ensemble models. Advances in neural information processing systems, 34:23347-23358, 2021.\n\n- Che-Ping Tsai, Chih-Kuan Yeh, and Pradeep Ravikumar. Sample based explanations via generalized representatives. Advances in Neural Information Processing Systems., 36, 2023.\n\n- Jiasen Yang, Qiang Liu, Vinayak Rao, and Jennifer Neville. Goodness-of-fit testing for discrete distributions via stein discrepancy. In International Conference on Machine Learning, pp. 5561-5570. PMLR, 2018.\n\n- Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection for explaining deep neural networks. Advances in neural information processing systems, 31, 2018.\n\n- Jianlong Zhou, Amir H Gandomi, Fang Chen, and Andreas Holzinger. Evaluating the quality of machine learning explanations: A survey on methods and metrics. *Electronics*, 10(5):593, 2021.\n\nResponse: Given the paper's focus on a novel and theoretically sound approach to explanation, and its detailed methodology and extensive experimental validation, I would lean towards **\\boxed{Accept}** for this manuscript. Here\u2019s a breakdown of the reasoning:\n\n### Strengths:\n1. **Novel Contribution**: HD-Explain introduces a new method leveraging Kernelized Stein Discrepancy (KSD) to provide precise and data-centric explanations. This is a unique approach that addresses the limitations of existing methods by focusing on the intrinsic data correlation under the trained model.\n   \n2. **Theoretical Foundation**: The paper establishes a solid theoretical basis for HD-Explain, connecting it to the minimization of KSD between the trained model and the training data. This theoretical grounding is crucial for the validity and reliability of the method.\n\n3. **Quantitative Evaluation**: The authors conduct a thorough evaluation across multiple datasets, demonstrating the method's superiority in various aspects (hit rate, coverage, and computation efficiency) compared to state-of-the-art methods. The experimental design is rigorous and includes controlled data augmentation strategies to validate the method's robustness.\n\n4. **Flexibility**: The method is flexible in terms of layer selection, allowing for deeper analysis of model behavior and the evolution of predictions across different layers of the neural network.\n\n### Weaknesses:\n1. **Scalability Limitations**: While the paper emphasizes scalability benefits over existing methods, some experiments (particularly on large datasets like CIFAR-10) suggest that the computation time for HD-Explain is not negligible. Further optimizations or parallelization techniques might be needed for broader applicability.\n\n2. **Kernel Choice**: The choice of kernel (Radial Basis Function, RBF) seems optimal for the datasets tested, but the flexibility of choosing other kernels might be an area for future work. \n\n3. **Limitations in Baseline Comparison**: Some baselines like Influence Function and TracIn are noted for their scalability issues. However, the authors limit these methods to their last layer, which might not fully capture the full potential of these methods, especially when dealing with larger datasets.\n\n### Overall Assessment:\nThe paper presents a compelling and innovative approach to explanation that stands out in terms of precision, scalability, and practical utility. The experimental validation is strong and convincing, supporting the claims made about the method's effectiveness. Despite some minor scalability concerns, the paper's contributions are substantial and timely, addressing key gaps in the existing literature on explanation methods for neural networks.\n\nTherefore, based on the outlined strengths and the absence of major weaknesses, I recommend **\\boxed{Accept}** for this paper.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768634890.1868718", "msecs": "186.0", "relativeCreated": "235010.34593582153", "thread": "140063876147008", "threadName": "MainThread", "processName": "MainProcess", "process": "849273", "text": "System:You are an expert academic reviewer tasked with evaluating research manuscripts for potential acceptance in a peer-reviewed journal. Assess the paper's originality, methodology, significance, and clarity, prioritizing evidence-based reasoning. Identify strengths (e.g., novel contributions, rigorous analysis) and weaknesses (e.g., methodological flaws, insufficient justification) while adhering to peer review principles. Provide balanced, constructive feedback that objectively weighs the paper's merits against field-specific standards, ensuring reproducibility, ethical compliance, and alignment with the journal's scope. Avoid personal biases and focus on the work's scientific merit and impact.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Pixab-CAM: Attend Pixel, not Channel\n\n# Abstract\nTo understand the internal behaviors of convolution neural networks (CNNs), many class activation mapping (CAM) based methods, which generate an explanation map by a linear combination of channels and corresponding weights, have been proposed. Previous CAM-based methods have tried to define a channel-wise weight that represents the importance of a channel for the target class. However, these methods have two common limitations. First, all pixels in the channel share a single scalar value. If the pixels are tied to a specific value, some of them are overestimated. Second, since the explanation map is the result of a linear combination of channels in the activation tensor, it is inevitably dependent on the activation tensor. To address these issues, we propose gradient-free Pixel-wise Ablation-CAM (Pixab-CAM), which utilizes pixel-wise weights rather than channel-wise weights to break the link between pixels in a channel. In addition, in order not to generate an explanation map dependent on the activation tensor, the explanation map is generated only with pixel-wise weights without linear combination with the activation tensor. In this paper, we also propose novel evaluation metrics to measure the quality of explanation maps using an adversarial attack. We demonstrate through experiments the qualitative and quantitative superiority of Pixab-CAM.\n\n\n# 1 INTRODUCTION\n\n\nAs the performance of convolution neural networks (CNNs) converges to some extent(Szegedy et al., 2015; Simonyan & Zisserman, 2014; Huang et al., 2017; Howard et al., 2017; Chollet, 2017), researchers have become interested in interpreting their predictions(Springenberg et al., 2014; Smilkov et al., 2017; Fong & Vedaldi, 2017; Bach et al., 2015; Lundberg & Lee, 2017; Ribeiro et al., 2016; Petsiuk et al., 2018; Shrikumar et al., 2017). One of them is class activation mapping (CAM) based approach(Zhou et al., 2016), which generates an explanation map by a linear combination of channels in the activation tensor and corresponding weights. The explanation map  $L_{ij}^{c}$  for the target class  $c$  is calculated as Eqn.1.  $A_{ij}^{k}$  represents activation of pixel at spatial location  $(i,j)$  for  $k$ -th channel of the activation tensor  $A$ , and  $w_{k}^{c}$  denotes the coefficient of the  $k$ -th channel for class  $c$ .\n\n\n$$\n\n$$\nL _ {i j} ^ {c} = \\sum_ {k} w _ {k} ^ {c} \\cdot A _ {i j} ^ {k} \\tag {1}\n$$\n\n$$\n\n\nThe activation tensor  $A$  is fixed since it is determined by the given input image and pretrained model(LeCun et al., 1998). Therefore,  $L^c$  depends on how  $w_k^c$  is defined. Thus, different  $w_k^c$  have been proposed in many CAM-based methods to generate high-quality explanation maps. The previous CAM-based methods can be divided into two categories, one is gradient-based CAMs(Selvaraju et al., 2017; Chattopadhyay et al., 2018; Fu et al., 2020), and the other is gradient-free CAMs(Wang et al., 2020; Ramaswamy et al., 2020). Gradient-based CAMs and gradient-free CAMs can be distinguished by whether or not gradients are used in the process of deriving weights.\n\n\n# 1.1 GRADIENT-BASED CAMS\n\n\nGrad-CAM(Selvaraju et al., 2017) obtains the weight of  $k$ -th channel by averaging their gradients. However, it has a problem in that the weight is crushed due to the offset of positive and negative gradients. ( $Z$  is the number of pixels in a channel and  $Y^{c}$  is the classification score for class  $c$ )\n\n\n$$\n\n$$\nw _ {k} ^ {c} = \\frac {1}{Z} \\sum_ {i} \\sum_ {j} \\frac {\\partial Y ^ {c}}{\\partial A _ {i j} ^ {k}} \\tag {2}\n$$\n\n$$\n\n\nGrad-CAM++(Chattopadhyay et al., 2018) introduced higher-order derivatives  $\\alpha_{ij}^{kc}$  in Grad-CAM equation to solve the problem that channels with fewer footprints fade away in the explanation map.\n\n\n$$\n\n$$\nw _ {k} ^ {c} = \\sum_ {i} \\sum_ {j} \\alpha_ {i j} ^ {k c} \\cdot R e L U \\left(\\frac {\\partial Y ^ {c}}{\\partial A _ {i j} ^ {k}}\\right) \\tag {3}\n$$\n\n$$\n\n\nXGrad-CAM(Fu et al., 2020) points out that previous CAM-based methods lack sufficient theoretical support and propose weight that satisfies two basic axioms: Sensitivity and Conservation.\n\n\n$$\n\n$$\nw _ {k} ^ {c} = \\sum_ {i} \\sum_ {j} \\left(\\frac {A _ {i j} ^ {k}}{\\sum_ {i} \\sum_ {j} A _ {i j} ^ {k}} \\frac {\\partial Y ^ {c}}{\\partial A _ {i j} ^ {k}}\\right) \\tag {4}\n$$\n\n$$\n\n\nHowever, Gradient-based CAMs suffer from gradient saturation problems, leading to localization failures for relevant regions.(Wang et al., 2020; Ramaswamy et al., 2020; Jung & Oh, 2021)\n\n\n# 1.2 GRADIENT-FREE CAMS\n\n\nScore-CAM(Wang et al., 2020) defines \"increase of confidence\" as the coefficient of the channel. For input  $X$  and baseline input  $X_{b}$ , the weight of the  $k$ -th channel for class  $c$  is defined as\n\n\n$$\n\n$$\nw _ {k} ^ {c} = f \\left(X \\circ s \\left(u \\left(A ^ {k}\\right)\\right)\\right) - f \\left(X _ {b}\\right) \\tag {5}\n$$\n\n$$\n\n\nwhere  $u(\\cdot)$  indicates the upsampling operation into the input size,  $s(\\cdot)$  is a function that normalizes all elements to [0, 1] and  $\\circ$  is the \"Hadamard product\".\n\n\nAblation-CAM(Ramaswamy et al., 2020), which inspired our research, defines the weight of a specific channel as the fraction of drop in activation score of target class when that channel is removed. In the equation below,  $Y_{k}^{c}$  is the score of class  $c$  when the  $k$ -th channel has been replaced by zero.\n\n\n$$\n\n$$\nw _ {k} ^ {c} = \\frac {Y ^ {c} - Y _ {k} ^ {c}}{Y ^ {c}} \\tag {6}\n$$\n\n$$\n\n\nGradient-free CAMs remove the dependence of gradients, but they are time-consuming because the prediction process must be repeated as many times as the number of channels to acquire the weights.\n\n\nRegardless of whether gradient is used or not, all CAM-based methods so far have two things in common. First, they all use channel-wise weights  $w_{k}^{c}$  (see Eqn.1). However, since the channel-wise weights allow all pixels in a specific channel to share a single weight, it is an unfair weighting method from the pixel perspective. (Although  $\\alpha_{ij}^{kc}$  in Grad-CAM++ (Eqn.3) modifies the gradients at the pixel level, but these gradients are eventually crushed into a single scalar value, so it doesn't solve the problem of pixels being tied together.) Furthermore, channel-wise weight-based methods tend to assign high weight if there are pixels that have a high contribution to the target class among the active pixels in the channel. Therefore, within a specific channel, other pixels that are active with the pixels related to the target class are also given a high weight. If channels containing these overestimated pixels are accumulated, overestimated pixels also occur in the explanation map.(see Sec.3.1) Second, when generating the explanation map  $L^{c}$ , all CAM-based methods follow Eqn.1, which contains the activation tensor  $A$ . Therefore, the explanation map has no choice but to depend on the activation tensor, and eventually may capture too much meaningless information since the channels in the activation tensor are not necessarily related to the target category(Zhang et al., 2021).(see Sec.3.2)\n\n\n# 2 PROPOSED EVALUATION METRICS OF THE EXPLANATION MAP\n\n\nSince it is hard to know where CNN looks and makes a specific prediction, the existing evaluation metrics of the explanation map have limitations. AD & IIC(Chattopadhy et al., 2018) and Deletion & Insertion(Petsiuk et al., 2018) are coarse proxy evaluations because removing pixels by zero-ing/graying is unnatural in real photos(Dabkowski & Gal, 2017). Also, in Localization Evaluation(Wang et al., 2020), there may not be a correlation between localization ability and the quality of the explanation map since the human-annotated bounding box may not match where CNN is actually focusing. However, if we could know where CNN is looking, we could make a more accurate assessment. We use an adversarial attack for this.(we use FGSM attack(Goodfellow et al., 2014).) If a targeted adversarial attack is made on a specific patch in the zero image, it is self-evident that CNN sees that patch and determines that it is a target, so the performance can be evaluated by whether the explanation map correctly captures that part. We propose 2 evaluation tasks (DAP1 and DAP2) from this idea. Before introducing them, we will explain 2 adversarial examples (AE1 and AE2) first.\n\n\n# 2.1 ADVERSARIAL EXAMPLE 1 (AE1)\n\n\n![(a) Zero Image](images/0547c4b6d71475e684ba360c9057bf44787776b0dee47b625c401f3d4c5367df.jpg)\n\n\n![(b) Adversarial Patch](images/34a56fbfedb3c4b9991f04d0c61ea56233537ae75579b678bb0ac765a04763ab.jpg)\n\n\n![(c) Input Image Figure 1: Adversarial patch in (b) was attacked to become a \"cucumber\".](images/9a7331d6091839304faeec8abe46dee286a15a2cf92c4c49952a596f20537928.jpg)\n\n\n![(d) Zero-ing Patch](images/14649134b4ef1119e9045e1c73419ffbbb385563b5a0972cfb9b3c25b6281eba.jpg)\n\n\n![(e) Adversarial Example](images/c63587c9b49760a6b673c20dadaa6363fa425de921dfb481f4401d08b2605e82.jpg)\n\n\nIt is simple to attach an adversarial patch classified as a given target class to an input image. First, we apply a targeted adversarial attack so that the patch at a specific location in the zero image is classified into the class we want. Since the attack is applied only to the patch, the rest of the area is still zero as in (b). Then, the input image with only the patch area of zero((d)) is added to the image (b). The finally obtained adversarial example AE1((e)) is used for our evaluation task DAP1.\n\n\n# 2.2 ADVERSARIAL EXAMPLE 2 (AE2)\n\n\n![(a) Adversarial Patch 1 Figure 2: (a): \"chihuahua\", (b): \"tusker\", (c): \"soccer ball\", (d): \"broccoli\"](images/f2a28c53596ea16de2afb64c3b9480e5d5bb35aa719fa5547c36c8906e00928c.jpg)\n\n\n![(b) Adversarial Patch 2](images/642343fe98276e9b444fd5628e09b117ff9420b4aa47f623a3fd62cec9b96e52.jpg)\n\n\n![(c) Adversarial Patch 3](images/ff17ede68541d2ad9a5475f96e0b226d05c938cd6abb5b7026687ea13feb162c.jpg)\n\n\n![(d) Adversarial Patch 4](images/1da15e2405c417e05a22ab7458c416c9ac0cec6dac44c513e04f0e39912eccb5.jpg)\n\n\n![(e) Adversarial Example](images/52e5849cb2139511002d1151d5f31b342fd142409fead83d77c5bd864af8737e.jpg)\n\n\nFor generating a second adversarial example AE2, only adversarial patches are used without input images. This can also be easily created. After generating four adversarial patches in the same way as in Sec.2.1 (b), combine them to make the final adversarial example AE2. (Each adversarial patch is attacked to become a different class.) AE2 created in this way is used for evaluation task DAP2.\n\n\n# 2.3 DETECTING ADVERSARIAL PATCH\n\n\n![Explanation Map  $^{AE1}$ Figure 3: Columns 1-3 and 4-6 correspond to the tasks \"Detecting Adversarial Patch 1 (DAP1)\" and \"Detecting Adversarial Patch 2 (DAP2)\". Also, each is an explanation of \"cucumber\" and \"tusker\".](images/5446c927929562c3b97e8ca2ed1add362c6afb3cbc92763f72bf113b2c4a0f8e.jpg)\n\n\n![Pointing Game  $^{AE1}$](images/824c7eeca3c53012f6feb03325c87ed1b2bf36627d0bde584addcafb5af08729.jpg)\n\n\n![Object Localization  $^{AE1}$](images/5daaf8ca105155ec514decb4d3640db30e92af7fe9a0f18f7faa3859c1e4f72a.jpg)\n\n\n![Explanation Map  $^{AE2}$](images/efa5d92f88fd86f70e1e715268823d8a5c9c5594269c7a5634414cf18db1c20e.jpg)\n\n\n![Pointing Game  $^{AE2}$  Object Localization  $^{AE2}$](images/789888ff9023bbd900f8400b1834ba0e460274d521ccb62c43af0863c09370fe.jpg)\n\n\n![](images/2f317a590398c539a2a3911200fc38a6a56d64d04f531dfe8e0b8b0e45890d62.jpg)\n\n\nIn this paper, \"Pointing Game\" and \"Object Localization\" are used as metrics in \"Detecting Adversarial Patch 1 (DAP1)\" and \"Detecting Adversarial Patch 2 (DAP2)\" tasks.\n\n\nPointing Game (PG) (Petsiuk et al., 2018): The Pointing Game (PG) is a metric that measures whether the maximum point of the explanation map is inside the human-annotated object bounding box. As in columns 2 and 5 of Figure 3, if the point lies inside the target bounding box, it is counted as a hit. Pointing Game accuracy is calculated as  $\\frac{\\#Hits}{\\#Hits + \\#Misses}$ .\n\n\nObject Localization (OL) (Wang et al., 2020): Object Localization (OL) is a metric that measures how much the energy of the explanation map falls into the target bounding box instead of using only the maximum point. This metric can be formulated as  $\\frac{\\sum L_{ij\\in bbox}^c}{\\sum L_{ij\\in bbox}^c + \\sum L_{ij\\notin bbox}^c}$ .\n\n\nHowever, in both DAP1 and DAP2, the ground truth bounding box of the target object is the patch, not the human-annotated bounding box. So, the ground truth bounding box in Sec.5.2.2 is the human-annotated bounding box, and the ground truth bounding box in Sec.5.2.3 is the patch.\n\n\n# 3 MOTIVATION\n\n\n# 3.1 CHANNELS WITH OVERESTIMATED PIXELS (COP)\n\n\n![(a) Input Image](images/5a4b9ce0b0acbcc8393e6ab13e5caf5a5759097b91e1c0dd1413871e5f908649.jpg)\n\n\n![(b) Target Only Figure 4: Target: \"chihuahua\", non-Target: \"tusker\", \"soccer ball\", \"broccoli\"](images/9affd75dfb6b0a9a21ce176d96cee64a5c71bf55cae4cf11b5a504642f8e9397.jpg)\n\n\n![(c) Target & non-Target](images/d31211c0f6aa2bad99e892e87e4f26bef8e284d39a7efdcec5065f0b032f2564.jpg)\n\n\n![(d) non-Target Only](images/409962fbaa30a2e416af83cf261d326801ec3703ad28211f959c87c727ff9306.jpg)\n\n\n<table><tr><td>Name</td><td>#COP</td></tr><tr><td>Grad-CAM</td><td>56</td></tr><tr><td>Grad-CAM++</td><td>39</td></tr><tr><td>Score-CAM</td><td>26</td></tr><tr><td>Ablation-CAM</td><td>53</td></tr><tr><td>XGrad-CAM</td><td>37</td></tr></table>\n\n\nTable 1: # COP\n\n\nIn Figure 4, (b)-(d) indicate which part of the input image the specific channels of the activation tensor focus on (Masking input image with upsampled channels.). Channel (b) is focusing only on the target part, channel (c) is focusing on both the target and non-target part and channel (d) is focusing only on the non-target part. We investigated the weights for channels (b)-(d), and it was confirmed that Grad-CAM, Grad-CAM++, Score-CAM, Ablation-CAM, and XGrad-CAM all gave high weights to channels (b) and (c) and low weights to channel (d). In other words, regardless of whether a specific channel contains a non-target part or not, as long as it contains a target part, that channel tends to be given a high weight. However, since channel-wise weight-based methods assign one weight per channel, all pixels in the channel share a single weight. As a result, for a channel like (c), pixels in the non-target part are also given a high weight. As shown in Appendix A, these channels are commonly found in the activation tensor. Since the explanation map of previous methods is generated by a linear combination of channels, if the number of channels with overestimated pixels(COP) increases, the overestimated pixels also appear in the explanation map. These overestimated pixels suppress other pixels, so the explanation map may highlight the wrong places or only part of the target. Table 1 shows the average number of channels focusing on both the target and non-target part among the channels with the top 100 weights(Inception V3 has 2048 channels). (If the channel contains more than  $80\\%$  of the patch, the channel is considered to be focusing on that patch.) The values in Table 1 are averaged for 500 randomly generated AE2s. Among the five methods, Score-CAM best distinguishes the channel focusing only on the target part and the channel focusing on both the target and non-target part. However, this is also not a satisfactory value.\n\n\n# 3.2 DEPENDENCY ON ACTIVATION TENSOR (DOA)\n\n\nReason for removing the dependency on activation tensor: To check the dependence between the explanation map and the activation tensor, we compare Pixab-CAM, which will be covered in Sec.4, with the previous methods for the \"Detecting Adversarial Patch 1 (DAP1)\" task presented in Sec.2. In addition, in order to check where the activation values are intensively distributed in the activation tensor, we defined Activation-CAM, which is expressed as  $\\sum_{k} 1 \\cdot A_{ij}^{k}$ . Since the channel-wise weights are fixed at 1, Activation-CAM can indicate where the activation tensor focuses.\n\n\nIf there is only an adversarial patch in the image, like the first row in Figure 5, all methods detect this patch well. However, if features are evenly distributed in the image, such as when a natural image and an adversarial patch are together (the result of Activation-CAM in the second row of Figure 5), the channel-wise weight-based methods cannot capture the patch properly even if the same patch is in the same location. Since the explanation map is generated from a linear weighted combination of channels in the activation tensor, it is difficult to properly focus on target if the target is located where the activation tensor is sparsely distributed (= the number of channels capturing the target is small), such as the result of Activation-CAM in the second row of the Figure 5 (In this case, the target is a patch.). This situation is common in the real world since channels in the activation tensor are not necessarily related to the target class(Zhang et al., 2021). On the other hand,\n\n\n![Input Image Activation-CAM Grad-CAM Grad-CAM++ Score-CAM Ablation-CAM XGrad-CAM Pixab-CAM Figure 5: Both the first row and the second row are explanations of \"keypad\".](images/118b443b8eb0c1ef7f4228984d9a891aac6e53bbadda14f1eccb6c3aa41fce7a.jpg)\n\n\nPixab-CAM can handle any situation flexibly regardless of how the activation tensor is distributed on the image because the activation tensor does not participate in the process of generating the explanation map (see Sec.4). Another finding is that the explanation maps generated by channel-wise weight-based methods except Grad-CAM and Ablation-CAM do not deviate much from the result of Activation-CAM, as shown in Figures 5, 6, and 8. In particular, the explanation map of Score-CAM is very similar to the results of Activation-CAM. From this, it can be inferred that the reason that all the channel-wise weight-based methods in the first row of Figure 5 produced the correct explanation map is that the features are distributed only in the adversarial patch, as shown in the result of Activation-CAM. In other words, since there were no features that prevented the detection of the target in the first place, all of them were able to provide a proper explanation.\n\n\nHow to measure dependence with activation tensor: To quantitatively check how dependent the explanation maps of CAM-based methods are on the activation tensor, we measured the cosine distance between Activation-CAM and CAM-based methods. However, for natural images like ImageNet, this cosine distance may not give reliable results as the activation tensor may be properly distributed on the target. As in the first row of Figure 5, if the activation tensor is distributed only on the target, it is impossible to distinguish a correct explanation map from an explanation map that depends on the activation tensor. To solve this, we use \"Adversarial Example 2 (AE2)\" introduced in Sec.2.2. We calculate the cosine distance after making the activation tensor evenly distributed over the four patches, such as the result of Activation-CAM in Figure 6. This approach ensures that the activation tensor is not distributed only to the target, so the dependency can be computed more accurately.(Generating an explanation map similar to Activation-CAM (= Low cosine distance) means it is not capturing the target properly.) We measured the cosine distances for 500 AE2s.\n\n\n![Input Image Activation-CAM Grad-CAM Grad-CAM++ Score-CAM Ablation-CAM XGrad-CAM Pixab-CAM Figure 6: 1st row: \"chihuahua\", 2nd row: \"tusker\", 3rd row: \"soccer ball\", 4th row: \"broccoli\".](images/a8e711d1b31271faae5fb60ec0d0a6ca4b3a3a1d164278e8044d4f14a90ad89b.jpg)\n\n\nThe result for \"Dependency on Activation Tensor (DOA)\" is in Table 2. Grad-CAM++, Score-CAM, and XGrad-CAM have very low DOA values. In particular, Score-CAM has a value close to 0. In\n\n\nother words, the explanation map they generate is highly dependent on the activation tensor. We also observed that the less the activation tensor is distributed in the target, the more they depend on the activation tensor. This result implies that in the case of a very weakly activated feature on the image, the explanation maps of these methods may not be able to capture that part even though it is a target. The explanation maps of Grad-CAM and Ablation-CAM do not depend on the activation tensor, but they focus on the wrong place or only part of the target, as shown in Figures 5, 6, and 8.\n\n\n# 4 PROPOSED METHODOLOGY\n\n\n![Figure 7: An overview of Pixab-CAM.](images/00de87240f72dd92d5b04179d9e5b3174b292419adad56889e0293b6b8156f9a.jpg)\n\n\nPixab-CAM has three main features. The first is that it doesn't use gradients to avoid problems with gradient saturation that negatively affect visualization quality (Wang et al., 2020; Ramaswamy et al., 2020). Second, we define a pixel-wise weight rather than a channel-wise weight to solve the problem that pixels within a specific channel are tied to a single scalar value. (First motivation in Sec.3.1.) Finally, in order to completely remove the dependency on the activation tensor, we do not include the activation tensor in generating the explanation map. (Second motivation in Sec.3.2.)\n\n\nAs in Eqn.7, we define the weight by using the idea of Ablation-CAM in terms of pixels. Although Ablation-CAM uses only the channel contribution from the deletion(=ablation) perspective when deriving the channel coefficient(Ramaswamy et al., 2020), we define pixel coefficient using both the pixel contribution from the deletion(first term) and preservation(second term) perspectives. If a specific pixel is associated with a target class, the class score will decrease when it is removed(first term) and increase when it appears(second term). These two terms are complementary to each other because they reflect the contribution of pixels from different perspectives.(It is a strict weight assignment method because a high coefficient can be obtained only by showing a high contribution from both perspectives.) As shown in Figure 7, Eqn.7 can be expressed as a tensor composed of  $0s$  (first term) and a tensor composed of  $1s$  (second term), each of size  $1 \\times 1 \\times$  channel_size, moving on the activation tensor in a sliding window manner. We'll call this tensor an ablation box, and use four ablation boxes-  $1 \\times 1$ ,  $1 \\times 2$ ,  $2 \\times 1$ ,  $2 \\times 2$ . (An evaluation of each design choice and why these four ablation boxes were used are covered in Appendix B.) The reason for using multiple scale boxes is to detect objects of various sizes and shapes as in Faster R-CNN(Ren et al., 2015). For reference, even if all boxes are used, the prediction process is only 1/4 of Ablation-CAM. Also, regardless of the size of the box, stride size is 1, and padding is used in all except the  $1 \\times 1$  box to reflect the pixel contribution fairly. Then, we average over all boxes of each perspective. Finally, the explanation map is obtained after applying ReLU and the normalization function as in Eqn.8. What should be noted here is that the activation tensor is not used in the process of generating the explanation map.\n\n\n$$\n\n$$\np _ {i j} ^ {c} = \\left(\\frac {Y ^ {c} - Y _ {i j} ^ {c}}{Y ^ {c}}\\right) \\cdot \\left(\\frac {\\widetilde {Y} _ {i j} ^ {c} - \\widetilde {Y} ^ {c}}{\\widetilde {Y} ^ {c}}\\right) \\tag {7}\n$$\n\n$$\n\n\n$$\n\n$$\nL _ {i j} ^ {c} = s \\left(R e L U \\left(p _ {i j} ^ {c}\\right)\\right) \\tag {8}\n$$\n\n$$\n\n\nwhere  $Y_{ij}^{c}$  is the score of class  $c$  when the pixel at spatial location  $(i,j)$  has been replaced by zero, conversely,  $\\widetilde{Y}_{ij}^{c}$  is the score of class  $c$  when the pixel at spatial location  $(i,j)$  is the only remaining pixel and  $\\widetilde{Y}^{c}$  is the score of class  $c$  when a tensor with all values of 0 is input to the model.\n\n\n# 5 EXPERIMENTS\n\n\nIn this section, we mainly compare Pixab-CAM with state-of-the-art CAM-based methods. Qualitative (see Sec.5.1) and quantitative evaluations (see Sec.5.2) are performed to evaluate the quality of the explanation map. In addition, we use Inception V3 network(Szegedy et al., 2016) pretrained on the ImageNet provided by TensorFlow as a black box model.\n\n\n# 5.1 QUALITATIVE EVALUATION (MORE EXAMPLES ARE PROVIDED IN APPENDIX C)\n\n\n![Input Image Activation-CAM Grad-CAM Grad-CAM++ Score-CAM Ablation-CAM XGrad-CAM Pixab-CAM Figure 8: Explanation Map Comparison. (Rows 1-2: Natural Image, Rows 3-4: AE1)](images/b395fe373047b3877dca1ad3073fdaf803426c85862f317959214f993e1a5255.jpg)\n\n\n![Input Image Guided-BP Grad-CAM Grad-CAM++ Score-CAM Ablation-CAM XGrad-CAM Pixab-CAM Figure 9: Guided CAM-based Explanation Comparison (Springenberg et al., 2014)](images/54ed4d761f3bc3d62ca5ef7065f84cfb2f61803dc57e653a11cc0d585292abfb.jpg)\n\n\nAs shown in Figures 8, 9, and 10, we qualitatively compare the explanation maps, guided CAM-based explanations, and weekly supervised localization results generated by Activation-CAM, five state-of-the-art CAM-based methods, and Pixab-CAM. Compared to previous CAM-based methods, Pixab-CAM produces higher quality explanation maps in all examples, whether or not the activation tensor is focusing on the target. On the other hand, the explanation maps of previous CAM-based methods tend not to focus properly on the target. First, Grad-CAM and Ablation-CAM tend to generate insufficient explanation maps, and even Ablation-CAM captures places that are not related to the target class at all. Second, if a part not related to the target class is emphasized in the result of Activation-CAM, that part is also highlighted in the explanation maps of Grad-CAM++, Score-CAM, and XGrad-CAM, which means that they are greatly affected by the activation tensor.\n\n\n# 5.2 QUANTITATIVE EVALUATION\n\n\nWe scraped 2,000 images with two or more objects with different labels (MOD) to evaluate the performance of the explanation map in situations where fine-grained weight adjustment is required. MOD images consist of objects with different labels but similar textures, such as cats and dogs,\n\n\n![Figure 10: Weakly Supervised Localization Comparison (Oquab et al., 2015)](images/d131efcba37813c40177e8174cbd8493af5e83db32d7f44c49e560f913cf795e.jpg)\n\n\nlions and tigers, trucks and cars, so the explanation map cannot properly capture the target unless the activation tensor is precisely adjusted by weight. In addition, ILSVRC 2012 validation set (IMGN)(Russakovsky et al., 2015), PASCAL VOC 2007 test set (VOC)(Everingham et al., 2010), and COCO 2014 validation set (COCO)(Lin et al., 2014) were used for evaluation.\n\n\n# 5.2.1 FAITHFULNESS EVALUATION\n\n\nTable 2: DOA\n\n\n<table><tr><td>Name</td><td>DOA</td></tr><tr><td>Grad-CAM</td><td>0.391</td></tr><tr><td>Grad-CAM++</td><td>0.028</td></tr><tr><td>Score-CAM</td><td>0.014</td></tr><tr><td>Ablation-CAM</td><td>0.223</td></tr><tr><td>XGrad-CAM</td><td>0.031</td></tr><tr><td>Pixab-CAM</td><td>0.402</td></tr></table>\n\n\nTable 3: AD, IIC, and Compactness\n\n\n<table><tr><td rowspan=\"2\">Name</td><td colspan=\"4\">AD (%)</td><td colspan=\"4\">IIC (%)</td><td colspan=\"4\">Compactness</td></tr><tr><td>IMGN</td><td>VOC</td><td>COCO</td><td>MOD</td><td>IMGN</td><td>VOC</td><td>COCO</td><td>MOD</td><td>IMGN</td><td>VOC</td><td>COCO</td><td>MOD</td></tr><tr><td>Grad-CAM</td><td>34.8</td><td>24.1</td><td>24.5</td><td>20.0</td><td>22.0</td><td>31.3</td><td>33.1</td><td>37.6</td><td>14.7</td><td>13.7</td><td>14.2</td><td>11.3</td></tr><tr><td>Grad-CAM++</td><td>33.7</td><td>22.4</td><td>22.8</td><td>21.3</td><td>22.1</td><td>33.3</td><td>31.0</td><td>35.2</td><td>18.4</td><td>19.8</td><td>20.4</td><td>16.9</td></tr><tr><td>Score-CAM</td><td>33.2</td><td>22.8</td><td>23.6</td><td>22.8</td><td>22.2</td><td>32.5</td><td>30.8</td><td>35.0</td><td>19.4</td><td>21.1</td><td>21.7</td><td>19.4</td></tr><tr><td>Ablation-CAM</td><td>32.7</td><td>21.2</td><td>22.4</td><td>20.1</td><td>24.5</td><td>33.3</td><td>32.9</td><td>37.6</td><td>16.3</td><td>17.9</td><td>18.3</td><td>14.5</td></tr><tr><td>XGrad-CAM</td><td>33.7</td><td>22.1</td><td>23.2</td><td>20.3</td><td>23.8</td><td>32.7</td><td>33.0</td><td>36.8</td><td>17.8</td><td>18.7</td><td>19.4</td><td>16.2</td></tr><tr><td>Pixab-CAM</td><td>30.2</td><td>19.4</td><td>19.1</td><td>18.9</td><td>24.2</td><td>36.2</td><td>35.9</td><td>39.7</td><td>15.4</td><td>16.1</td><td>16.6</td><td>12.6</td></tr></table>\n\n\nAD, IIC, and Compactness: Average Drop (AD)(Chattopadhy et al., 2018) is a metric that measures how much the explanation image, which is generated by point-wise multiplication of the upsampled explanation maps with the original image, maintains the part that plays essential roles in correct decision-making. Increase in Confidence (IIC)(Chattopadhy et al., 2018) measures whether the part that confuses correct decisions in the image has been sufficiently removed. Compactness is the metric proposed for the first time in this paper to measure how large the area the explanation map occupies in an image. An ideal explanation map should be of a suitable size to cover the target compactly. Compactness is expressed as  $\\frac{1}{N}\\sum_{i=1}^{N}(\\sum_{a}\\sum_{b}L_{i,(a,b)}^{c})$ , where  $L_{i,(a,b)}^{c}$  is the explanation map value at spatial location  $(a,b)$  for the  $i$ -th input  $X_{i}$  and  $N$  is the number of evaluation images. Results for 1,000 images randomly sampled for each dataset are in Table 3. From the results of IIC and AD, except for IIC in IMGN, Pixab-CAM outperforms other CAM-based methods by large scale on all metrics. This result reveals that Pixab-CAM can successfully leave only the essential parts for the correct decision while sufficiently removing the parts that interfere with decision making. In addition, Pixab-CAM has the second smallest Compactness value after Grad-CAM. It means that Grad-CAM generates an explanation map with a size less than necessary, and other previous CAM-based methods generate an explanation map with excessive size. Meanwhile, Pixab-CAM generates an explanation map of an appropriate size enough to cover the target compactly.\n\n\nDeletion and Insertion: The deletion metric(Petsiuk et al., 2018) measures the decrease in the probability of the predicted class as more and more important pixels are removed. The insertion metric(Petsiuk et al., 2018), on the other hand, measures the increase in probability as more and more pixels are introduced. Table 4 reports the average result of 1,000 images randomly selected from ImageNet. Pixab-CAM exhibited the highest insertion AUC and lowest deletion AUC. This experiment shows that Pixab-CAM successfully assigned coefficients in order of importance.\n\n\n# 5.2.2 OBJECT LOCALIZATION EVALUATION\n\n\nIn this section, we compare the localization ability of the generated explanation maps. The \"Object Localization (OL)\" metric was discussed in Sec.2.3 and is therefore omitted from this section. The values in Table 5 are averaged for 1,000 images randomly selected from ImageNet. Pixab-CAM has the highest proportion compared to other CAM-based methods. This means that the explanation maps generated by Pixab-CAM compactly focus the essential parts of the image.\n\n\n# 5.2.3 \u201cDETECTING ADVERSARIAL PATCH 1\u201d & \u201cDETECTING ADVERSARIAL PATCH 2\u201d\n\n\nWe also evaluate the performance of explanation maps for tasks \"Detecting Adversarial Patch 1 (DAP1)\" and \"Detecting Adversarial Patch 2 (DAP2)\", which are proposed for the first time in this paper (see Section 2). The number of patches in the image is randomly selected from 1-9, and the width and height of the patches are also randomly selected from  $15\\% - 20\\%$  of the input image size. DAP1 and DAP2 were tested on 500 AE1s and 500 AE2s, respectively, and the results are in Table 6. We observe that Pixab-CAM outperforms previous CAM-based methods in both DAP1 and DAP2.\n\n\nTable 4: Deletion & Insertion\n\n\n<table><tr><td>Name</td><td>Deletion</td><td>Insertion</td></tr><tr><td>Grad-CAM</td><td>0.090</td><td>0.433</td></tr><tr><td>Grad-CAM++</td><td>0.094</td><td>0.428</td></tr><tr><td>Score-CAM</td><td>0.101</td><td>0.425</td></tr><tr><td>Ablation-CAM</td><td>0.088</td><td>0.457</td></tr><tr><td>XGrad-CAM</td><td>0.093</td><td>0.434</td></tr><tr><td>Pixab-CAM</td><td>0.082</td><td>0.469</td></tr></table>\n\n\nTable 5: Object Localization\n\n\n<table><tr><td>Name</td><td>Proportion (%)</td></tr><tr><td>Grad-CAM</td><td>46.39</td></tr><tr><td>Grad-CAM++</td><td>48.17</td></tr><tr><td>Score-CAM</td><td>50.77</td></tr><tr><td>Ablation-CAM</td><td>51.32</td></tr><tr><td>XGrad-CAM</td><td>47.19</td></tr><tr><td>Pixab-CAM</td><td>53.22</td></tr></table>\n\n\nTable 6: DAP1 & DAP2\n\n\n<table><tr><td rowspan=\"2\">Name</td><td colspan=\"2\">DAP1</td><td colspan=\"2\">DAP2</td></tr><tr><td>PG</td><td>OL</td><td>PG</td><td>OL</td></tr><tr><td>Grad-CAM</td><td>46.5</td><td>42.7</td><td>45.3</td><td>43.1</td></tr><tr><td>Grad-CAM++</td><td>44.3</td><td>40.7</td><td>43.8</td><td>40.2</td></tr><tr><td>Score-CAM</td><td>43.2</td><td>40.2</td><td>42.9</td><td>39.5</td></tr><tr><td>Ablation-CAM</td><td>46.8</td><td>43.1</td><td>46.7</td><td>44.9</td></tr><tr><td>XGrad-CAM</td><td>44.0</td><td>41.9</td><td>43.1</td><td>40.6</td></tr><tr><td>Pixab-CAM</td><td>55.3</td><td>50.1</td><td>54.9</td><td>51.3</td></tr></table>\n\n\n# 5.3 SANITY CHECK (ADEBAYO ET AL., 2018)\n\n\nModel parameter randomization test was proposed to compare the explanation map on a trained model to the explanation map on a randomly initialized, untrained network of the same architecture. As in Figure 11, Pixab-CAM is sensitive to model parameters and can judge the quality of the model.\n\n\n![Figure 11: Cascading randomization from top to bottom layers on Inception v3 trained on ImageNet](images/469d837d051ab5454a5918fdc2814a56c822722304e16310173bc873d7ab9610.jpg)\n\n\n# 5.4 APPLICATIONS\n\n\nTo prove the applicability of Pixab-CAM in downstream tasks, we applied it to the Brain Tumor Detection task. Since the Brain Tumor dataset is small (N: 253), we used \"Transfer Learning\"(Pan & Yang, 2009). We downloaded VGG-16(Simonyan & Zisserman, 2014) pretrained on the ImageNet dataset, then trained only the final fully connected layer while keeping the weights of all ConvNets fixed. Figure 12 shows the explanation map on a trained VGG-16 for brain tumor images. Pixab-CAM detects tumor better than other CAM-based methods. This experiment demonstrates that Pixab-CAM can also be applied to fine-tuned models using small datasets.\n\n\n![Figure 12: Explanation Map of Brain Tumor images](images/4837cb96391c9039923cdce85b73937830b6acdc92cc4f2c065c9425dc1d28aa.jpg)\n\n\n# 6 CONCLUSION\n\n\nWe proposed Pixab-CAM, a novel CAM-based approach with three characteristics. First, we use pixel-wise weights rather than channel-wise weights so that the importance of the pixels is not distorted. Second, we exclude the activation tensor to create an explanation map independent of the activation tensor. Third, to avoid problems with gradient-based CAMs, gradients are not used when obtaining pixel-wise weights. In addition, we propose a new evaluation metric using adversarial attack and demonstrate the superiority of Pixab-CAM through qualitative and quantitative evaluation.\n\n\n# REFERENCES\n\n- Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. arXiv preprint arXiv:1810.03292, 2018.\n\n- Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. *PloS one*, 10(7):e0130140, 2015.\n\n- Aditya Chattopadhy, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Gradcam++: Generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 839-847. IEEE, 2018.\n\n- Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1251-1258, 2017.\n\n- Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. arXiv preprint arXiv:1705.07857, 2017.\n\n- Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The Pascal visual object classes (voc) challenge. International journal of computer vision, 88(2): 303-338, 2010.\n\n- Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3429-3437, 2017.\n\n- Ruigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, and Biao Li. Axiom-based grad-cam: Towards accurate visualization and explanation of cnns. arXiv preprint arXiv:2008.02312, 2020.\n\n- Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\n\n- Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n\n- Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708, 2017.\n\n- Hyungsik Jung and Youngrock Oh. Lift-cam: Towards better explanations for class activation mapping. arXiv preprint arXiv:2102.05228, 2021.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\n- Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr D\u00f3lar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740-755. Springer, 2014.\n\n- Scott Lundberg and Su-In Lee. A unified approach to interpreting model predictions. arXiv preprint arXiv:1705.07874, 2017.\n\n- Maxime Aquab, L\u00e9on Bottou, Ivan Laptev, and Josef Sivic. Is object localization for free?-weakly-supervised learning with convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 685-694, 2015.\n\n- Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345-1359, 2009.\n\n- Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of black-box models. arXiv preprint arXiv:1806.07421, 2018.\n\n- Harish Guruprasad Ramaswamy et al. Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 983-991, 2020.\n\n- Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015.\n\n- Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135-1144, 2016.\n\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211-252, 2015.\n\n- Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626, 2017.\n\n- Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In International Conference on Machine Learning, pp. 3145-3153. PMLR, 2017.\n\n- Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\n- Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda V\u00e9gas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.\n\n- Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.\n\n- Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.\n\n- Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826, 2016.\n\n- Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu. Score-cam: Score-weighted visual explanations for convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 24-25, 2020.\n\n- Qinglong Zhang, Lu Rao, and Yubin Yang. A novel visual interpretability for deep neural networks by optimizing activation maps with perturbation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3377-3384, 2021.\n\n- Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2921-2929, 2016.\n\nResponse: Given the comprehensive nature of the paper and the strong methodology described, I would assess this manuscript for publication in ICLR as:\n\n\\[\n\\boxed{Accept}\n\\]\n\n### Justification:\n\n#### Originality:\n- **Novel Contributions**: The paper introduces Pixab-CAM, which is a significant advancement over existing CAM methods by addressing the limitations of channel-wise weights and reliance on the activation tensor. This is a fresh approach that addresses practical challenges in visual interpretation of CNNs.\n- **Adversarial Evaluation Metrics**: The authors propose novel evaluation metrics using adversarial attacks, which provides a robust framework for assessing the quality of explanation maps. This adds substantial value to the field.\n\n#### Methodology:\n- **Proposed Approach**: The method proposed is clearly defined and justified. The use of pixel-wise weights instead of channel-wise weights is a principled approach to avoid overestimation of certain pixels within a channel.\n- **Implementation Details**: The paper provides detailed descriptions of the proposed method, including the mathematical formulation of the pixel-wise weights and the generation of the explanation maps. The implementation details are clear and well-documented.\n\n#### Significance:\n- **Relevance**: The paper addresses a critical issue in the field of explainable AI, specifically in the context of CNNs. Understanding how CNNs make predictions is crucial for improving transparency and accountability in machine learning systems.\n- **Impact**: By providing a method that avoids common pitfalls of previous CAM approaches, Pixab-CAM has the potential to significantly enhance the interpretability of deep neural networks.\n\n#### Clarity:\n- **Presentation**: The paper is well-written and structured. The motivation, methodology, experimental setup, and results are presented in a logical flow. The supplementary materials (e.g., figures, tables) complement the text effectively.\n- **Figures and Tables**: The inclusion of detailed visualizations and comparative results (e.g., Figures 8, 9, 10) helps in understanding the improvements over existing methods.\n\n### Areas for Improvement:\n- **Additional Controls**: While the authors mention using adversarial patches to evaluate the method, more extensive control experiments could strengthen the argument. For instance, testing on a broader range of datasets and varying conditions might provide additional insights.\n- **Discussion on Limitations**: Discussing potential limitations or future work directions would round out the paper. For example, discussing scalability to larger models or handling cases where the target is not well-separated from non-targets.\n- **Ethical Considerations**: Mentioning how the method can be ethically applied in real-world scenarios could add depth to the discussion.\n\nOverall, the paper presents a solid, innovative approach with strong empirical validation. With minor refinements, it is likely to meet the acceptance criteria of ICLR.", "message": "LLMCall function forward"}
