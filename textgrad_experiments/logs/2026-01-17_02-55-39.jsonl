{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636545.8609571", "msecs": "860.0", "relativeCreated": "6554.097652435303", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# TAFS: Task-aware Activation Function Search for Graph Neural Networks\n\n# Abstract\nSince the inception of Graph Neural Networks (GNNs), extensive research efforts have concentrated on enhancing graph convolution, refining pooling operations, devising robust training strategies, and advancing theoretical foundations. Notably, one critical facet of current GNN research remains conspicuously underexplored\u2014the design of activation functions. Activation functions serve as pivotal components, imbuing GNNs with the essential capacity for non-linearity. Yet, the ubiquitous adoption of Rectified Linear Units (ReLU) persists. In our study, we embark on a mission to craft task-aware activation functions tailored for diverse GNN applications. We introduce TAFS (Task-aware Activation Function Search), an adept and efficient framework for activation function design. TAFS leverages a streamlined parameterization and frames the problem as a bi-level stochastic optimization challenge. To enhance the search for smooth activation functions, we incorporate additional Lipschitz regularization. Our approach automates the discovery of the optimal activation patterns, customizing them to suit any downstream task seamlessly. Crucially, this entire process unfolds end-to-end without imposing significant computational or memory overhead. Comprehensive experimentation underscores the efficacy of our method. We consistently achieve substantial improvements across a spectrum of tasks, including node classification over diverse graph data. Moreover, our approach surpasses state-of-the-art results in the realm of link-level tasks, particularly in biomedical applications.\n\n\n# 1 INTRODUCTION\n\n\nGraph Neural Networks (GNN) have demonstrated their prowess in modeling relationships within graph-structured data, as evidenced by their superior performance in various domains (Kipf & Welling, 2017; Velickovic et al., 2017; Hu et al., 2020; Xu et al., 2019). They have excelled in applications spanning biomedicine (Wu et al., 2023; Jiang et al., 2021), physical simulation (Sanchez-Gonzalez et al., 2020), material design (Reiser et al., 2022), sustainability (Donon et al., 2020), social network (Fan et al., 2019), transportation (Li et al., 2018b), recommendation (Wu et al., 2019), and more. Consequently, GNN models continue to captivate the attention of researchers across diverse scientific communities (Shi et al., 2020; Wang et al., 2022; Seo et al., 2020).\n\n\nDespite the extensive body of literature, we must highlight a significant gap in current research, specifically the design of activation functions, a fundamental component used in nearly every GNN model. While Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) is a prevalent choice for activation, it often falls short, as illustrated in Figure 1. Regrettably, GNN studies have hardly explored alternative activation functions. This oversight is critical, as the activation function plays a pivotal role in introducing non-linearity to GNNs. Without it, GNNs merely perform linear transformations on raw graph features.\n\n\nIn contrast, the Computer Vision community has spent decades exploring a wide array of manually\n\n\n![Figure 1: Activation function makes a big difference in GNN.](images/66c16489c3adf48e54dabed9ea1bf174d62a12679ee3776f2e4cfe1a7cc4106c.jpg)\n\n\ndesigned activation functions such as Sigmoid (LeCun et al., 1998), Tanh, ReLU (Nair & Hinton, 2010), and improved variants of ReLU (He et al., 2015; Clevert et al., 2016; Maas et al., 2013). However, transferring these manually crafted functions to different tasks poses challenges, and customizing new ones is a labor-intensive process. Furthermore, the marginal performance gains from human-designed functions diminish rapidly. To address this, researchers have proposed automated methods to discover tailored activation functions, which have demonstrated notable improvements in other network architectures like Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs) (Ramachandran et al., 2018; Eger et al., 2018; Farzad et al., 2019).\n\n\nHence, our research question is how can we design GNN activation functions to adapt effectively to various graph-based tasks, creating task-aware activation functions?\n\n\nAddressing this question poses two primary challenges. Challenge #1: Existing search algorithms are inefficient. Current activation function search methods suffer from over-parameterization and heavy computation. For example, APL (Agostinelli et al., 2015) introduces additional parameters for each neuron, which usually leads to at least ten times more parameters upon any model, thus significantly increasing the model complexity. Swish (Ramachandran et al., 2018) requires training a full network until convergence for each iteration, making it computationally burdensome. These issues render current search algorithms inefficient and less effective. Challenge #2: Current search methods lack support for non-differentiable objectives. GNN methods have wide applications regarding tasks of different levels (node, link, graph), many of which are evaluated by non-differentiable metrics. In the case of drug interaction prediction, we would like to know a certain positive (synergy) or negative (confliction) interaction exist. In fact, most drug pairs does not have positive or negative interactions. Receiver Operating Characteristic curve (ROC) which is not differentiable, is what we should use instead of accuracy. Similar application cases can be found in hit ratio of recommendation, latency optimization, hardware resources constraint, etc. Supporting these non-differentiable objectives would broaden the applicability of activation function search in diverse GNN tasks.\n\n\nIn this study, we embark on a systematic exploration of GNN activation function search\u2014a first of its kind. We frame this search as a bi-level optimization problem, with the inner level optimizing GNN parameters and the outer level optimizing activation function parameters. We propose an efficient search algorithm that navigates through a compact search space. This space is characterized by universal approximators with additional smoothness constraints, facilitating the rapid discovery of high-quality functions, thereby addressing Challenge #1. Additionally, we tackle Challenge #2 by jointly considering non-differentiable objectives and potential activation function constraints. We incorporate these elements into a stochastic relaxation of the outer level optimization, removing the need to compute gradients for non-differentiable metrics used in GNN tasks. Our algorithm undergoes extensive experimentation across various GNN models, datasets, and objectives, consistently outperforming existing activation functions. By overcoming Challenges #1 and #2, our algorithm achieves task-awareness in GNN activation function design.\n\n\nOur contributions can be summarized in three key points:\n\n- 1. To the best of our knowledge, we are the first to propose activation function search in the context of Graph Neural Networks. Our work serves as a catalyst, drawing attention to this critical aspect of GNN model design and paving the way for future investigations.\n\n- 2. We propose TAFS (Task-aware Activation Function Search), a probabilistic search algorithm capable of efficiently exploring a regularized functional space to discover novel activation functions tailored for diverse downstream tasks.\n\n- 3. Through comprehensive evaluations spanning node and link level tasks, we demonstrate that our algorithm enhances activation function design without requiring extensive manual effort and excels in optimizing non-differentiable objectives. We also conduct ablation studies to examine the searched activation functions, the impact of design choices, and algorithm efficiency.\n\n\n# 2 RELATED WORKS\n\n\nGraph Neural Networks. GNN is a power model in capturing relational information including typically GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2017), GIN (Xu et al., 2019), etc. Mathematically, in the context of GNN with a given network  $\\mathcal{G} = \\{\\mathcal{V},\\mathcal{E}\\}$  containing node set  $\\mathcal{V}$  and edge set  $\\mathcal{E}$ , the problem is formulated below:\n\n\n$$\n\n$$\nz _ {u} ^ {(l + 1)} = \\operatorname {U P D A T E} ^ {(l + 1)} \\left(h _ {u} ^ {(l)}, \\operatorname {A G G R E G A T E} ^ {(l)} \\left(\\left\\{h _ {u} ^ {(l)}, \\forall v \\in \\mathcal {N} (u) \\right\\}\\right)\\right), \\tag {1}\n$$\n\n$$\n\n\nwhere  $h_u^{(l)}$  is the latent representation of node  $u$  at layer  $l$  and  $z_u$  is the pre-activation of node  $u$ . AGGREGATE (abbr. Agg) and UPDATE (abbr. Up) are core modules of GNN, denoting the different message passing operations used across the model for collecting and updating representations. The latent representation (respectively pre-activation) for all nodes constitute  $\\mathbf{H}$  (resp.  $\\mathbf{Z}$ ) and then we have the activation transformation:  $\\mathbf{H}^{(l + 1)} = \\sigma^{(l + 1)}(\\mathbf{Z}^{(l + 1)})$ , where  $\\mathbf{Z}$  is activated by function  $\\sigma$  at the  $(l + 1)$ th layer.\n\n\nNumerous studies have been devoted to ameliorate it through different aspects. For example, GCN (Kipf & Welling, 2017) simplifies graph convolution and derives performant GCN networks. GAT (Velickovic et al., 2017) proposes graph attention as a replacement to model global features. GNN-Pretrain (Hu et al., 2020) studies pretraining strategies at the node level and graph level to make GNN model work for transferable tasks. GIN (Xu et al., 2019) understands the fundamental question of graph expressiveness by discriminating Weisfeiler-Lehman graph isomorphism. GNN Co-training (Li et al., 2018a) connects Laplacian smoothing with graph convolution and studies the problem of oversmoothing. However, almost every GNN model uses ReLU as the activation function (Kipf & Welling, 2017; Velickovic et al., 2017; Xu et al., 2018; Huang et al., 2020; KC et al., 2022; Xu et al., 2019), leaving GNN activation function a missing research piece.\n\n\nActivation Function Design. Since the early application of Sigmoid in Le-Net, activation functions have been considered as an important component until today (Hayou et al., 2019). In 2012, ReLU (Nair & Hinton, 2010) was proposed to train Boltzman Machines and soon extensively adopted in every neural network models. The study of activation function design happens mostly in CNN community, where a couple of milestone works include Swish (Ramachandran et al., 2018; Eger et al., 2018) and APL (Agostinelli et al., 2015). Swish proposes a Reinforcement Learning (RL)-based search algorithm to find appropriate activation functions in a discrete space. APL (Adaptive Piecewise Linear) uses linear hinge functions to approximate target patterns in a differentiable way. Comprehensive surveys of manual designed and parametric activation functions can be found in (Apicella et al., 2021; Dubey et al., 2022). Another notable work related to our research question is GReLU (Zhang et al., 2022), which tries to make GNN activation function adaptive by including graph convolution into the activation function. However, such design is not a typical univariate activation function. As a result, no work yet has proposed novel activation functions designed under the context of GNN.\n\n\n# 3 PROBLEM FORMULATION AND CHALLENGES\n\n\nOur research problem requires to propose a systematic way of designing adaptive activation functions that can be effectively integrated into GNN for downstream applications. Similar to Neural Architecture Search (NAS) (Liu et al., 2019), the activation function design could be modeled into a bi-level optimization problem:\n\n\n$$\n\n$$\n\\min  _ {\\alpha} \\mathcal {M} \\left(w ^ {*} (\\alpha), \\alpha ; \\mathcal {D} _ {\\text {v a l}}\\right) \\quad \\text {s . t .} \\quad w ^ {*} (\\alpha) = \\arg \\min  _ {w} \\mathcal {L} (w, \\alpha ; \\mathcal {D} _ {\\text {t r a i n}}), \\tag {2}\n$$\n\n$$\n\n\nwhere the inner level optimization learns  $w$  weight of GNN and the outer level optimization learns  $\\alpha$  weights of activation function. Both levels may have different objective metrics  $\\mathcal{M}, \\mathcal{L}$  depending on downstream applications.\n\n\nPrevious activation function search methods suffer from low efficiency (Challenge #1) and poor support of non-differentiable metrics (Challenge #2). On one hand, the efficiency bottleneck lies in the search space choice and search strategy design. The search space is crucial for search efficiency and requires careful consideration. The space should be proper both in candidate function number and effectiveness, making it a trade-off between quantity and quality. Then, the search strategy should be able to discover as quickly as possible the most suitable function candidate in the space. On the other hand, diverse GNN applications require that the algorithm is able to tackle any downstream target metric, whether or not differentiable.\n\n\nAll these issues prevent us from using off-the-shelf algorithms for GNN activation function search. To this end, we need a novel parameterization of the search space, that is jointly designed with search strategy to allow efficient search, and we need to deal with differentiable and non-differentiable target metrics at the same time to enable general applications in all kinds of GNN tasks.\n\n\n![Figure 2: Algorithm framework. We replace the activation function with sampled weights from Gaussian distributions. The inner level optimization is the learning of distribution parameter  $\\theta^{(1)},\\ldots ,\\theta^{(L)}$  and the outer level optimization is the learning of GNN weights. Both levels are iterated for faster convergence.](images/82c6262858b1d88a3b812ade0f6dbea69a2a33caa85d9e70103e76290b62aedd.jpg)\n\n\n# 4 THE PROPOSED METHOD\n\n\n# 4.1 ALGORITHM FRAMEWORK\n\n\nAs illustrated in Figure 2 and Table 1, we propose TAFS to solve both challenges in a unified way. We follow the bi-level optimization formulation and search activation function represented by learnable parameters. Specifically, a typical GNN network of L layers can be represented as below:\n\n\n$$\n\n$$\n\\sigma^ {(L)} \\circ \\mathbf {G L} ^ {(L)} \\dots \\sigma^ {(2)} \\circ \\mathbf {G L} ^ {(2)} \\circ \\sigma^ {(1)} \\circ \\mathbf {G L} ^ {(1)} (\\mathbf {X}), \\tag {3}\n$$\n\n$$\n\n\nwhere Graph Layer (GL) denotes all the AGGREGATE (Agg) and UPDATE (Up) operations related to graph,  $\\mathbf{X}$  is the initial graph features. Note that different layers could use different activation functions whereas current GNN models tend to fix the same ReLU for every layer.\n\n\nDenote by  $w_{\\sigma}$  all the parameters of activation functions, i.e.  $\\sigma^{(1)}, \\ldots, \\sigma^{(L)}$ , and denote by  $\\overline{w}_{\\sigma}$  all the parameters of GNN, i.e., parameters of  $\\mathrm{GL}^{(1)}, \\ldots, \\mathrm{GL}^{(L)}$ . We propose a continuous implicit functional space to parameterize  $w_{\\sigma}$ . This search space is expressive yet compact, with smoothness regularization induced by human prior. The parameter update process is stochastic to deal with any downstream objective especially non-differentiable metrics. The search algorithm is bi-level and end-to-end trained. The optimization step of the outer level (learning  $w_{\\sigma}$ ) and the inner level optimization (learning  $\\overline{w}_{\\sigma}$ ) are iterated. In the following parts, we explain in sequence the design of search space, stochastic relaxation and search algorithm.\n\n\n# 4.2 IMPLICIT FUNCTIONAL SEARCH SPACE\n\n\nIn order to facilitate activation function search, we propose a continuous implicit functional space that parameterizes the search space by universal approximators. This implicit functional space could be implemented by Multi-Layer Perception (MLP) to approximate target function. As in Figure 2, activation function parameters is equivalent to the parameters of MLP, denoted by  $w_{\\sigma}$ .\n\n\nIt's worth noting that we employ MLP as a representative example of universal approximators, chosen for its simplicity, while retaining generality. However, it's crucial to emphasize that alternative implementations, such as Gaussian Mixtures or Radial Basis Functions (RBF), are entirely feasible.\n\n\nIn addition, we focus on the smooth functions such that the searched activation functions will not change dramatically if the pre-activation value  $Z$  is slightly perturbed. Smooth functions are bounded by Lipschitz constant  $c$ , i.e.  $|f(x) - f(y)| \\leq c|x - y|$ . As a result, the functional search space is\n\n\nregularized by smoothness constraint  $\\{w_{\\sigma}|w_{\\sigma}\\in R^{|w_{\\sigma}|},c < \\gamma \\}$ , where  $c$  is the Lipschitz constant of the function parameterized by  $w_{\\sigma}$  and  $\\gamma$  is the hard limit on  $c$ . Here, we model the constraint on Lipschitz constant as a regularization term denoted by  $R(w_{\\sigma})$ . Many references design Lipschitz constant as additional soft metric to be trained together with any loss (Hoffman et al., 2019; Weng et al., 2018; Liu et al., 2022). We use Jacobian regularization (Hoffman et al., 2019) without loss of generality  $R(w_{\\sigma}) = ||J(x)||_F = \\{\\Sigma_{i,j}[J_{i,j}(x)]^2\\}$ , where  $J_{i,j}(x) = \\frac{\\partial h_i}{\\partial x_j} (x)$  is the Jacobian matrix.\n\n\nThis design of our functional space encourages the discovery of smooth functions characterized by small Lipschitz constants. Notably, this characteristic aligns with existing manually designed functions, such as ReLU, Tanh, Sigmoid, and Swish, all of which exhibit 1-Lipschitz properties.\n\n\n# 4.3 STOCHASTIC RELAXATION\n\n\nGNN applications are diverse that in many cases, the preferred evaluation metrics may not be differentiable. As mentioned in Table 1 and related works, APL could not deal with non-differentiable metrics but Swish could due to the RL-based search algorithm. However, the search efficiency of Swish is far from satisfactory.\n\n\nWe propose to use stochastic relaxation that re-parameterizes the search space  $(w_{\\sigma})$  with a Gaussian distribution  $p_{\\theta_{\\sigma}}(w_{\\sigma})$ . The Gaussian distribution has its own parameters  $\\theta_{\\sigma}$  and we sample the parameters of activation function  $w_{\\sigma}$  from the probability  $p_{\\theta_{\\sigma}}(w_{\\sigma})$  and optimize the probability parameters  $\\theta_{\\sigma}$  instead of  $w_{\\sigma}$ .\n\n\nFollowing (4), we replace  $\\sigma$  by  $w_{\\sigma}$  to emphasize the parameters on activation function; we replace  $w$  by  $\\overline{w}_{\\sigma}$  to denote the rest parameters of GNN model. Task objective  $\\mathcal{M}$  is jointly integrated into stochastic relaxation with space regularization  $\\mathcal{R}$ . The ultimate problem is formulated as below:\n\n\n$$\n\n$$\n\\theta_ {\\sigma} ^ {*} = \\underset {\\theta_ {\\sigma}} {\\arg \\min } \\left\\{\\mathcal {J} \\left(\\theta_ {\\sigma}\\right) \\equiv \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\theta_ {\\sigma}} \\left(w _ {\\sigma}\\right)} \\left[ \\mathcal {M} \\left(w _ {\\sigma}, \\bar {w _ {\\sigma}} ^ {*}; \\mathcal {D} _ {\\text {v a l}}\\right) + \\eta R \\left(w _ {\\sigma}\\right) \\right] \\right\\}, \\tag {4}\n$$\n\n$$\n\n\n$$\n\n$$\n\\text {s . t .} \\quad \\overline {{w _ {\\sigma}}} ^ {*} = \\underset {\\overline {{w _ {\\sigma}}}} {\\arg \\min } \\mathcal {L} (\\overline {{w _ {\\sigma}}}, w _ {\\sigma}; \\mathcal {D} _ {\\text {t r a i n}}),\n$$\n\n$$\n\n\nwhere  $w_{\\sigma}$  denotes the parameters of activation functions  $\\sigma$  with a regularization term  $R$  weighted by  $\\eta$ ,  $\\overline{w_{\\sigma}}$  represents GNN parameters,  $\\mathcal{L}$  is downstream task criterion of interest,  $\\mathcal{M}$  is upstream task criterion of interest, probably non-differentiable,  $\\theta_{\\sigma}$  represents the re-parameterization of  $w_{\\sigma}$  through Gaussian distribution, then the whole learning problem is optimized in a stochastic way.\n\n\nTo compute the target loss gradient with respect to probability parameters  $\\nabla_{\\theta_{\\sigma}}\\mathcal{J}(\\theta_{\\sigma})$ , we have the following proposition. The proof is given in Appendix A.\n\n\nProposition 1 Let  $w_{\\sigma} \\sim p_{\\theta_{\\sigma}}(w_{\\sigma})$  represent that the weights of activation functions are sampled from  $p_{\\theta_{\\sigma}}$ . We have\n\n\n$$\n\n$$\n\\begin{array}{l} \\begin{array}{l} \\nabla_ {\\theta_ {\\sigma}} \\mathcal {J} (\\theta_ {\\sigma}) = \\nabla_ {\\theta_ {\\sigma}} \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\theta_ {\\sigma}} (w _ {\\sigma})} [ \\mathcal {M} (w _ {\\sigma}, \\bar {w} _ {\\sigma} ^ {*}; \\mathcal {D} _ {v a l}) + \\eta R (w _ {\\sigma}) ] \\\\ - \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\sigma}} \\left[ \\left[ \\mathcal {M} (w _ {\\sigma}, \\bar {w} _ {\\sigma} ^ {*}; \\mathcal {D} _ {v a l}) \\right] \\log p _ {\\sigma} (w _ {\\sigma}) \\right] \\end{array} \\tag {5} \\\\ = \\mathbb {E} _ {w _ {\\sigma} \\sim p _ {\\theta_ {\\sigma}} (w _ {\\sigma})} \\left[ \\left[ \\mathcal {M} \\left(w _ {\\sigma}, \\overline {{w _ {\\sigma}}} ^ {*}; \\mathcal {D} _ {v a l}\\right) + \\eta R \\left(w _ {\\sigma}\\right) \\right] \\nabla_ {\\theta_ {\\sigma}} \\log p _ {\\theta_ {\\sigma}} \\left(w _ {\\sigma}\\right) \\right] \\\\ \\end{array}\n$$\n\n$$\n\n\nWith the help of stochastic relaxation, the previously needed derivation of  $\\mathcal{M}$  is replaced by a multiplication between forward pass of  $\\mathcal{M}$  and a gradient of probability loss. In practice, this gradient expectation could be further approximated by Monte Carlo samplings, i.e.  $\\nabla_{\\theta_{\\sigma}}\\mathcal{J}(\\theta_{\\sigma})\\approx \\sum_{i = 1}^{K}\\nabla_{\\theta_{\\sigma}}\\log p_{\\theta_{\\sigma}}(w_{\\sigma}^{i})[\\mathcal{M}(w_{\\sigma}^{i},\\overline{w_{\\sigma}}^{*};\\mathcal{D}_{\\mathrm{val}}) + \\eta R(w_{\\sigma}^{i})]$ ,  $K$  is the sample number that we use to approximate the gradient. As a result, the differentiability requirement of  $\\mathcal{M}$  is removed.\n\n\n# 4.4 SEARCH STRATEGY\n\n\nAccording to (4), the learning is divided in two levels. The outer level optimizes probability parameters  $\\theta_{\\sigma}$  on validation dataset with (non-differentiable) metric  $\\mathcal{M}$ . Every time  $K$  number of samples are generated from the probability distribution (such as Gaussian). Each sample is forwarded and calculated according to (Prop. 1), whose average is an approximation of outer level loss gradient. The optimization of outer level parameters  $\\theta_{\\sigma}$  influences directly the value of activation function weights since the weights are sample from the updated probability every time a forward pass is needed. The inner level optimizes GNN parameters  $\\overline{w_{\\sigma}}$  on training dataset with metric  $\\mathcal{L}$ . It is similar to a normal training epoch of any network. The outer and inner levels are interplayed to accelerate convergence.\n\n\n```\n1: Initialize  $\\theta^0 = 1$ , initialize  $\\overline{w}_{\\sigma}$  by Xavier initialization and  $w_{\\sigma}$  randomly sampled from  $p_{\\theta^0}(w_\\sigma)$ .  \n2: for  $m = 0, \\dots, M - 1$  do  \n3: // Outer level optimization  \n4: Freeze GNN paramaters  $\\overline{w}_{\\sigma}$ ;  \n5: for  $k = 0, \\dots, K - 1$  do  \n6: Sample activation functions weights  $w_{\\sigma}^{k}$  from  $p_{\\theta^m}(w_\\sigma)$ ;  \n7: Forward inference of the whole network and accumulate stochastic loss  $\\mathcal{J}(\\theta_{\\sigma})$  as in Prop 1;  \n8: end for  \n9: Obtain  $\\nabla_{\\theta_{\\sigma}} \\mathcal{J}(\\theta_{\\sigma})$  by automatic differentiation and update  $\\theta^m$ ;  \n10: // Inner level optimization  \n11: Sample activation function parameters  $w_{\\sigma}$  from distribution  $p_{\\theta^m}(w_\\sigma)$  and freeze  $w_{\\sigma}$ ;  \n12: Forward inference of the whole network to obtain loss  $\\mathcal{L}$ ;  \n13: Update GNN parameters  $\\overline{w}_{\\sigma}$  by automatic differentiation;  \n14: end for  \n15: Training until convergence and obtain the final model parameter  $\\overline{w}_{\\sigma}^*$  and dist. parameter  $\\theta_{\\sigma}$ ;  \n16: return Final model parameter  $\\overline{w}_{\\sigma}^*$  and distribution parameter  $\\theta_{\\sigma}$ .\n```\n\n\nTable 1: Our proposed TAFS (Task-aware Activation Function Search) enables efficient differentiable search through a flexible and powerful MLP functional space. TAFS supports non-differentiable objective metrics in diverse GNN applications.\n\n\n<table><tr><td rowspan=\"2\">Search Method</td><td colspan=\"2\">Search Efficiency</td><td rowspan=\"2\">Non-Differentiable Metric</td></tr><tr><td>Search Space</td><td>Search Strategy</td></tr><tr><td>Swish</td><td>Discrete template choice</td><td>Reinforcement Learning</td><td>Applicable</td></tr><tr><td>APL</td><td>Explicit piecewise linear</td><td>Differentiable</td><td>Not applicable</td></tr><tr><td>TAFS (ours)</td><td>Continuous implicit MLP</td><td>Differentiable</td><td>Applicable</td></tr></table>\n\n\nThe complete TAFS algorithm is given in Algorithm 1. We also compare in Table 1 our proposed TAFS and literature methods. From the time efficiency perspective, Swish is the slowest in Table 1 because it optimizes a new network until convergence before the learning of RL controller. APL on the other hand, has a number of parameters dependent of base models due to its adaptability per neuron. As a result, TAFS enjoys a compact search space without over-parameterization and has superior efficiency in searching. Empirical results are given in Table 4.\n\n\n# 5 EXPERIMENTS\n\n\nIn this section, we experiment our methods on diverse GNN applications including node classification and link prediction, in order to fully evaluation the methods on differentiable and non-differentiable metrics. Later, we provide detailed analysis on search efficiency and hyperparameter impact. All our experiments are run on single NVIDIA RTX 3090.\n\n\n# 5.1 NODE CLASSIFICATION\n\n\nDatasets. We experiment on diverse graph datasets for node tasks, including Cora and DBLP for paper classification based on reference network, Cornell and Texas for webpage classification from university network, and Chameleon for wikipedia page classification based on hyperlink network. Statistics are in Appx. E. The task metric here is classification accuracy.\n\n\nBaselines. To fairly compare different activation functions, we compare our searchable activation functions with manually designed ones or previously searched function. Some of these activation functions are visualized in Figure 3(a). For each dataset and baseline chosen, we evaluate on two aggregation layers (GCN and GraphSage) and five network connection topologies (stack, residual, dense, jump knowledge, mixhop). Each model has four layers of aggregation layers. The model is trained for 400 epochs.\n\n\nTable 2: Overall node classification improvement of different models on different datasets. Metric is classification accuracy. Avg. Imp. is the improvement of TAFS with respect to the other choices averaged over all the datasets.\n\n\n<table><tr><td colspan=\"2\">Model</td><td>Activation</td><td>Cora</td><td>DBLP</td><td>Cornell</td><td>Texas</td><td>Chameleon</td><td>Avg. Imp.</td></tr><tr><td rowspan=\"40\">GCN</td><td rowspan=\"5\">Stack</td><td>ReLU</td><td>83.06\u00b10.66</td><td>84.63\u00b10.21</td><td>56.76\u00b15.92</td><td>60.54\u00b16.42</td><td>61.60\u00b11.75</td><td>\u2191 2.8%</td></tr><tr><td>Tanh</td><td>84.82\u00b10.51</td><td>85.58\u00b10.15</td><td>56.49\u00b15.19</td><td>57.84\u00b15.01</td><td>61.51\u00b11.88</td><td>\u2191 3.2%</td></tr><tr><td>L-ReLU</td><td>84.57\u00b10.93</td><td>84.50\u00b10.40</td><td>57.38\u00b12.16</td><td>60.54\u00b17.37</td><td>61.95\u00b12.18</td><td>\u2191 2.1%</td></tr><tr><td>Swish</td><td>83.88\u00b10.81</td><td>84.89\u00b10.34</td><td>57.30\u00b13.97</td><td>58.65\u00b15.55</td><td>58.33\u00b11.68</td><td>\u2191 4.1%</td></tr><tr><td>TAFS</td><td>89.08\u00b10.48</td><td>86.24\u00b10.17</td><td>57.37\u00b14.37</td><td>62.11\u00b15.48</td><td>62.31\u00b11.82</td><td>-</td></tr><tr><td rowspan=\"5\">Residual</td><td>ReLU</td><td>85.13\u00b10.95</td><td>84.45\u00b10.34</td><td>57.84\u00b15.43</td><td>57.84\u00b15.95</td><td>66.93\u00b12.17</td><td>\u2191 3.2%</td></tr><tr><td>Tanh</td><td>86.02\u00b10.55</td><td>85.63\u00b10.14</td><td>58.38\u00b14.39</td><td>57.57\u00b15.93</td><td>68.86\u00b11.84</td><td>\u2191 2.0%</td></tr><tr><td>L-ReLU</td><td>86.60\u00b10.72</td><td>84.97\u00b10.33</td><td>55.68\u00b18.30</td><td>57.84\u00b16.75</td><td>67.50\u00b11.48</td><td>\u2191 3.3%</td></tr><tr><td>Swish</td><td>85.86\u00b10.64</td><td>84.67\u00b10.19</td><td>56.22\u00b16.14</td><td>60.54\u00b17.66</td><td>66.29\u00b12.12</td><td>\u2191 2.8%</td></tr><tr><td>TAFS</td><td>88.16\u00b10.58</td><td>86.29\u00b10.18</td><td>58.20\u00b14.80</td><td>60.22\u00b15.51</td><td>70.49\u00b11.64</td><td>-</td></tr><tr><td rowspan=\"5\">JKNet</td><td>ReLU</td><td>86.86\u00b10.71</td><td>84.99\u00b10.25</td><td>76.49\u00b17.36</td><td>77.57\u00b17.36</td><td>58.18\u00b11.63</td><td>\u2191 3.8%</td></tr><tr><td>Tanh</td><td>86.41\u00b10.57</td><td>85.57\u00b10.20</td><td>68.92\u00b16.76</td><td>65.95\u00b19.22</td><td>60.20\u00b12.19</td><td>\u2191 9.1%</td></tr><tr><td>L-ReLU</td><td>87.45\u00b10.51</td><td>85.04\u00b10.15</td><td>74.05\u00b15.57</td><td>76.49\u00b18.80</td><td>57.98\u00b12.36</td><td>\u2191 4.1%</td></tr><tr><td>Swish</td><td>86.34\u00b10.92</td><td>84.95\u00b10.28</td><td>77.03\u00b15.57</td><td>78.11\u00b16.99</td><td>57.00\u00b12.54</td><td>\u2191 4.7%</td></tr><tr><td>TAFS</td><td>88.84\u00b10.56</td><td>87.07\u00b10.22</td><td>81.35\u00b16.40</td><td>81.08\u00b15.01</td><td>60.21\u00b12.04</td><td>-</td></tr><tr><td rowspan=\"5\">Mixhop</td><td>ReLU</td><td>85.31\u00b10.64</td><td>85.10\u00b10.18</td><td>73.78\u00b15.55</td><td>74.05\u00b19.53</td><td>51.64\u00b12.24</td><td>\u2191 2.7%</td></tr><tr><td>Tanh</td><td>85.15\u00b10.67</td><td>85.12\u00b10.30</td><td>72.97\u00b17.55</td><td>76.76\u00b16.86</td><td>50.59\u00b12.60</td><td>\u2191 2.6%</td></tr><tr><td>L-ReLU</td><td>86.38\u00b10.50</td><td>85.01\u00b10.17</td><td>72.43\u00b16.14</td><td>72.70\u00b15.05</td><td>51.36\u00b12.80</td><td>\u2191 3.4%</td></tr><tr><td>Swish</td><td>86.21\u00b11.03</td><td>85.43\u00b10.25</td><td>72.34\u00b18.18</td><td>74.86\u00b16.51</td><td>51.89\u00b12.10</td><td>\u2191 2.5%</td></tr><tr><td>TAFS</td><td>88.77\u00b10.57</td><td>86.18\u00b10.17</td><td>75.14\u00b15.38</td><td>78.43\u00b15.28</td><td>52.17\u00b11.97</td><td>-</td></tr><tr><td rowspan=\"5\">Stack</td><td>ReLU</td><td>83.06\u00b10.66</td><td>83.67\u00b10.41</td><td>58.11\u00b16.19</td><td>70.00\u00b16.78</td><td>47.02\u00b14.20</td><td>\u2191 12.1%</td></tr><tr><td>Tanh</td><td>84.82\u00b10.51</td><td>84.90\u00b10.19</td><td>68.65\u00b16.75</td><td>71.89\u00b17.85</td><td>53.50\u00b11.68</td><td>\u2191 4.3%</td></tr><tr><td>L-ReLU</td><td>84.57\u00b10.65</td><td>84.16\u00b10.23</td><td>62.16\u00b15.92</td><td>68.11\u00b17.23</td><td>49.21\u00b13.02</td><td>\u2191 9.8%</td></tr><tr><td>Swish</td><td>81.53\u00b10.74</td><td>83.62\u00b10.50</td><td>57.03\u00b16.45</td><td>68.65\u00b16.19</td><td>48.42\u00b12.17</td><td>\u2191 13.0%</td></tr><tr><td>TAFS</td><td>87.08\u00b10.48</td><td>85.22\u00b10.30</td><td>72.43\u00b17.23</td><td>74.51\u00b16.92</td><td>58.57\u00b11.20</td><td>-</td></tr><tr><td rowspan=\"5\">Residual</td><td>ReLU</td><td>84.11\u00b10.82</td><td>83.05\u00b10.33</td><td>65.95\u00b16.64</td><td>73.51\u00b16.71</td><td>55.02\u00b12.73</td><td>\u2191 6.1%</td></tr><tr><td>Tanh</td><td>85.62\u00b10.52</td><td>85.22\u00b10.17</td><td>72.43\u00b13.97</td><td>78.11\u00b19.00</td><td>59.17\u00b11.80</td><td>\u2191 0.6%</td></tr><tr><td>L-ReLU</td><td>85.63\u00b10.42</td><td>84.05\u00b10.21</td><td>71.89\u00b13.67</td><td>74.86\u00b15.80</td><td>55.86\u00b11.83</td><td>\u2191 3.1%</td></tr><tr><td>Swish</td><td>84.97\u00b10.79</td><td>84.17\u00b10.43</td><td>71.08\u00b14.02</td><td>75.41\u00b17.09</td><td>54.17\u00b11.44</td><td>\u2191 3.9%</td></tr><tr><td>TAFS</td><td>89.10\u00b10.385</td><td>85.22\u00b10.17</td><td>73.38\u00b13.63</td><td>77.03\u00b16.86</td><td>58.62\u00b12.08</td><td>-</td></tr><tr><td rowspan=\"5\">JKNet</td><td>ReLU</td><td>85.29\u00b10.56</td><td>83.97\u00b10.15</td><td>80.00\u00b16.07</td><td>81.62\u00b15.10</td><td>56.78\u00b11.62</td><td>\u2191 3.0%</td></tr><tr><td>Tanh</td><td>86.01\u00b10.51</td><td>85.25\u00b10.18</td><td>77.03\u00b15.57</td><td>78.92\u00b16.14</td><td>57.68\u00b11.92</td><td>\u2191 3.8%</td></tr><tr><td>L-ReLU</td><td>85.90\u00b10.42</td><td>85.01\u00b10.25</td><td>80.27\u00b16.84</td><td>81.35\u00b14.75</td><td>57.41\u00b12.01</td><td>\u2191 2.5%</td></tr><tr><td>Swish</td><td>85.56\u00b10.61</td><td>84.71\u00b10.22</td><td>77.13\u00b15.30</td><td>81.06\u00b15.43</td><td>55.00\u00b11.93</td><td>\u2191 4.5%</td></tr><tr><td>TAFS</td><td>89.51\u00b10.66</td><td>86.73\u00b10.20</td><td>81.79\u00b15.08</td><td>82.10\u00b15.16</td><td>59.37\u00b11.53</td><td>-</td></tr><tr><td rowspan=\"5\">Mixhop</td><td>ReLU</td><td>84.55\u00b11.07</td><td>84.23\u00b10.21</td><td>75.95\u00b19.59</td><td>81.63\u00b14.80</td><td>54.19\u00b12.15</td><td>\u2191 2.3%</td></tr><tr><td>Tanh</td><td>84.82\u00b10.65</td><td>84.21\u00b10.38</td><td>78.11\u00b16.99</td><td>81.89\u00b15.80</td><td>53.20\u00b11.21</td><td>\u2191 2.0%</td></tr><tr><td>L-ReLU</td><td>84.80\u00b11.10</td><td>84.34\u00b10.28</td><td>76.22\u00b18.36</td><td>77.30\u00b14.71</td><td>53.14\u00b11.72</td><td>\u2191 3.7%</td></tr><tr><td>Swish</td><td>84.18\u00b10.55</td><td>84.69\u00b10.30</td><td>75.95\u00b18.15</td><td>80.27\u00b16.74</td><td>53.20\u00b11.86</td><td>\u2191 3.0%</td></tr><tr><td>TAFS</td><td>87.77\u00b11.40</td><td>85.30\u00b10.24</td><td>77.77\u00b14.39</td><td>83.70\u00b14.05</td><td>55.07\u00b10.57</td><td>-</td></tr></table>\n\n\nResults. We provide in Table 2 the results of node classification tasks. The improvement of TAFS with respect to the other function choices is significant. Note that the improvements are observable across different graph data and GNN models, showing that TAFS is task-aware to graphs in citation, university webpage, wikipedia link graph, etc.\n\n\n# 5.2 MOLECULE AND PROTEIN INTERACTION PREDICTION\n\n\nDatasets. Biomedical graph are one of the most active and effective application areas of GNN. Biomedical GNN has accelerated important studies in protein prediction, molecule generation, gene\n\n\nexpression, etc. We consider the link prediction that is a typical task in molecule and protein interaction prediction. Specifically, we consider Drug-Drug Interaction (DDI), Drug-Target Interaction (DTI), Protein-Protein Interaction (PPI) and Disea-Gene Association (DGA). The statistics of the four datasets are provided in Appx. Table 6.\n\n\nBaselines. We adopt two biomedical graph baselines SkipGNN (Huang et al., 2020) and HOGCN (KC et al., 2022). SkipGNN proposes a general GNN architecture to model molecular interactions and works well on all these biomedical tasks. We use both as base model and experiment TAFS to replace the activation functions. The training hyperparameters are the same as in the original work.\n\n\nTable 3: Drug and protein interaction predictions.\n\n\n<table><tr><td>Task</td><td>Model</td><td>Activation</td><td>ROCAUC</td><td>PRAUC</td></tr><tr><td rowspan=\"6\">Drug-Target Interaction</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.922\u00b10.004</td><td>0.928\u00b10.006</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.933\u00b10.002</td><td>0.934\u00b10.001</td></tr><tr><td>TAFS</td><td>0.952\u00b10.001</td><td>0.954\u00b10.001</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.927\u00b10.001</td><td>0.929\u00b10.001</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.923\u00b10.002</td><td>0.922\u00b10.001</td></tr><tr><td>TAFS</td><td>0.943\u00b10.002</td><td>0.940\u00b10.001</td></tr><tr><td rowspan=\"6\">Drug-Drug Interaction</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.886\u00b10.003</td><td>0.866\u00b10.006</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.890\u00b10.002</td><td>0.874\u00b10.001</td></tr><tr><td>TAFS</td><td>0.911\u00b10.002</td><td>0.898\u00b10.003</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.898\u00b10.002</td><td>0.881\u00b10.003</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.897\u00b10.002</td><td>0.901\u00b10.002</td></tr><tr><td>TAFS</td><td>0.917\u00b10.002</td><td>0.901\u00b10.001</td></tr><tr><td rowspan=\"6\">Protein-Protein Interaction</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.917\u00b10.004</td><td>0.921\u00b10.003</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.920\u00b10.001</td><td>0.922\u00b10.002</td></tr><tr><td>TAFS</td><td>0.927\u00b10.001</td><td>0.937\u00b10.002</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.919\u00b10.001</td><td>0.922\u00b10.002</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.919\u00b10.002</td><td>0.924\u00b10.001</td></tr><tr><td>TAFS</td><td>0.923\u00b10.003</td><td>0.929\u00b10.002</td></tr><tr><td rowspan=\"6\">Disease-Gene Association</td><td rowspan=\"3\">SkipGNN</td><td>ReLU</td><td>0.912\u00b10.004</td><td>0.915\u00b10.003</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.916\u00b10.001</td><td>0.920\u00b10.001</td></tr><tr><td>TAFS</td><td>0.930\u00b10.001</td><td>0.940\u00b10.001</td></tr><tr><td rowspan=\"3\">HOGCN</td><td>ReLU</td><td>0.927\u00b10.001</td><td>0.934\u00b10.001</td></tr><tr><td>TAFS w.o. relaxation</td><td>0.929\u00b10.002</td><td>0.933\u00b10.001</td></tr><tr><td>TAFS</td><td>0.933\u00b10.001</td><td>0.942\u00b10.002</td></tr></table>\n\n\nResults. We provide in Table 3 the results of four link prediction tasks. Again, both SkipGNN and HOGCN use ReLU by default. With TAFS, SkipGNN and HOGCN has gained significant performance evaluated in ROCAUC and PRAUC, two non-differentiable metrics. Furthermore, when TAFS is integrated with SkipGNN, a model from 2020, it outperforms HOGCN, the state-of-the-art model from 2022. This underscores the significance of activation function search, which has hitherto been overlooked in the GNN community.\n\n\n# 5.3 ABLATION STUDY\n\n\nTo further analyze our proposed TAFS algorithm, we provide additional experiments to illustrate the search results, search efficiency and hyperparameter impact.\n\n\nVisualization of activation function search. We show in Figure 3 the searched activation functions from the literature methods and TAFS. It can be observed that TAFS could find diverse activation functions different than manually design ones or the searched ones by Swish and APL. Moreover, TAFS learns layer-wise activation function, leading to different behaviours of functions in different layers as in Figure 3(b)(c). Deeper layers' activation functions are smoother than shallow layers.\n\n\nSearch efficiency The modeling differences between literature search methods and TAFS are given in Table 1. In this part, we provide more empirical details of the search efficiency comparison in Table 4. TAFS has a significantly smaller consumption of extra memory and shorter running time. This huge efficiency improvement is credited to TAFS' compact MLP functional search space and differentiable search strategy, making TAFS' extra parameters independent of base models, whichever dataset or\n\n\n![(a) Literature search algorithms.](images/af3d2dc16af32136e39f596dae27e88d65ea40aedbff3b999636f48c35dcb38d.jpg)\n\n\n![(b) Results on dataset Cora Figure 3: Literature activation functions and searched activation results. (b)(c) include searched results on different layers across two datasets.](images/b11507cfba3258b75da195ae2cbcfaf11e5b524c873c56d4279762d8fdcc6044.jpg)\n\n\n![(c) Results on dataset DBLP](images/7e13fc6da84ed4db609d8a276c20829e94faac99bcbe6f07484b3c5b685003e0.jpg)\n\n\nGNN model (as long as the model has the same number of activation functions), while APL models each neuron with a piecewise linear unit, leading to over 2000 times more parameters than TAFS.\n\n\nHyperparameters impact. The choice of hyperparameters significantly affects performance. TAFS introduces two sets of hyperparameters: the number of samples (K) and the selection of the MLP architecture. We present their effects in Figure 4. The number of samples (K) in stochastic optimization exhibits a consistent increasing trend, representing a trade-off between accuracy and computational time. Regarding MLP hyperparameters, we analyze their impact on two node tasks, DBLP and Cornell, using nine different configurations: depths ranging from two to four layers and widths spanning from 10 to 1000 neurons. It is evident that a very small MLP (e.g., two layers with 10 neurons) is inadequate for\n\n\nmodeling adaptive activation functions. However, the distinctions between other choices are negligible. Given that deeper and wider MLPs require significantly more parameters, we opt for a two-layer MLP with 100 hidden units in all other experiments.\n\n\nTable 4: Search efficiency comparison.\n\n\n<table><tr><td>Dataset</td><td>Model</td><td>Parameters</td><td>Time(min)</td></tr><tr><td rowspan=\"4\">DBLP</td><td>Base</td><td>420K</td><td>0.15</td></tr><tr><td>Swish</td><td>+340K (+82%)</td><td>350</td></tr><tr><td>APL</td><td>+2400K (+575%)</td><td>4</td></tr><tr><td>TAFS</td><td>+1.3K (+0.3%)</td><td>1</td></tr><tr><td rowspan=\"4\">Chameleon</td><td>Base</td><td>315K</td><td>1.2</td></tr><tr><td>Swish</td><td>+420K (+108%)</td><td>2990</td></tr><tr><td>APL</td><td>+1760K (+558%)</td><td>33</td></tr><tr><td>TAFS</td><td>+1.3K (+0.4%)</td><td>11</td></tr><tr><td rowspan=\"4\">Ogbg-Molhiv</td><td>Base</td><td>27M</td><td>1020</td></tr><tr><td>Swish</td><td>-</td><td>&gt;70 days</td></tr><tr><td>APL</td><td>OOM (+150M)</td><td>-</td></tr><tr><td>TAFS</td><td>+12K</td><td>1380</td></tr></table>\n\n\n![(a) Number of samples in stochastic optimization. Larger K is lawways better but slower.](images/1b4fb305a40747e0fdddae0ea5b9af8243af20e143813ef2b16d61f37abbd836.jpg)\n\n\n![(b) MLP dimension impacts on DBLP dataset. Figure 4: Hyperparameter impact of number of samples in stochastic relaxation and the impact of MLP dimensions. In (b)(c), deeper color means better performance.](images/ec9501441f0a88918d11f9983df6b4dce431e54dd07be173b545a963595dad4d.jpg)\n\n\n![(c) MLP dimension impacts on Cornell dataset.](images/18218eff81e36c61f38c170a3e6f2b484b0ed369de546c8895363ea2e4c9c7b6.jpg)\n\n\n# 6 CONCLUSION\n\n\nIn a word, we achieve a task-aware activation function search in GNN through an expressive and compact representation of search space, stochastic relaxation with reparameterization, which are carefully co-designed with search strategy. Our search space is inclusive and parameter efficient, including appropriate number of high-quality functions. The search strategy is end-to-end trained and every operation of the framework is differentiable. Finally, the stochastic relaxation is capable of dealing with any metric of interest, closing the optimization gap.\n\n\n# REFERENCES\n\n- Forest Agostinelli, Matthew D. Hoffman, Peter J. Sadowski, and Pierre Baldi. Learning activation functions to improve deep neural networks. In International Conference on Learning Representations, ICLR Workshop, 2015.\n\n- Andrea Apicella, Francesco Donnarumma, Francesco Isgro, and Roberto Prevete. A survey on modern trainable activation functions. Neural Networks, 138:14-32, 2021.\n\n- Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). In International Conference on Learning Representations, ICLR, 2016.\n\n- Balthazar Donon, Zhengying Liu, Wenzhuo Liu, Isabelle Guyon, Antoine Marot, and Marc Schoenauer. Deep statistical solvers. In Neural Information Processing Systems, NeurIPS, 2020.\n\n- Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Activation functions in deep learning: A comprehensive survey and benchmark. Neurocomputing, 503:92-108, 2022.\n\n- Steffen Eger, Paul Youssef, and Iryna Gurevych. Is it time to swish? comparing deep learning activation functions across NLP tasks. In Empirical Methods in Natural Language Processing, Brussels, EMNLP, 2018.\n\n- Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In World Wide Web Conference, WWW, pp. 417-426. ACM, 2019.\n\n- Amir Farzad, Hoda Mashayekhi, and Hamid Hassanpour. A comparative performance analysis of different activation functions in LSTM networks for classification. Neural Computing and Applications, 31:2507-2521, 2019.\n\n- Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function on deep neural networks training. In International Conference on Machine Learning, ICML, volume 97, pp. 2672-2680, 2019.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In International Conference on Computer Vision, ICCV, 2015.\n\n- Judy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust learning with jacobian regularization. CoRR, abs/1908.02729, 2019.\n\n- Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In International Conference on Learning Representations, ICLR, 2020.\n\n- Kexin Huang, Cao Xiao, Lucas M Glass, Marinka Zitnik, and Jimeng Sun. Skipgnn: predicting molecular interactions with skip-graph networks. *Scientific reports*, 10(1):1-16, 2020.\n\n- Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang, Chao Shen, Dongsheng Cao, Jian Wu, and Tingjun Hou. Could graph neural networks learn better molecular representation for drug discovery? a comparison study of descriptor-based and graph-based models. Journal of Cheminformatics, 2021.\n\n- K. KC, R. Li, F. Cui, and A. R. Haake. Predicting biomedical interactions with higher-order graph convolutional networks. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(02), mar 2022. ISSN 1557-9964. doi: 10.1109/TCBB.2021.3059415.\n\n- Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulouse, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 1998.\n\n- Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), AAAI, 2018a.\n\n- Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. In International Conference on Learning Representations, ICLR, 2018b.\n\n- Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.\n\n- Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In International Conference on Learning Representations, ICLR, 2019.\n\n- Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, and Or Litany. Learning smooth neural functions via lipschitz regularization. In SIGGRAPH. ACM, 2022.\n\n- Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rectifier nonlinearities improve neural network acoustic models. In ICML. Atlanta, GA, 2013.\n\n- Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Johannes Furnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 807-814. Omnipress, 2010.\n\n- Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. In International Conference on Learning Representations, ICLR, 2018.\n\n- Patrick Reiser, Marlen Neubert, Andr\u00e9 Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer, and Pascal Friederich. Graph neural networks for materials science and chemistry. Communications Materials, 3(1), 2022. doi: 10.1038/s43246-022-00315-6. URL https://doi.org/10.1038/s43246-022-00315-6.\n\n- Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W. Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, ICML, 2020.\n\n- Sungyong Seo, Chuizheng Meng, and Yan Liu. Physics-aware difference graph networks for sparsely-observed dynamics. In International Conference on Learning Representations, ICLR, 2020.\n\n- Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. In International Conference on Learning Representations, ICLR, 2020.\n\n- Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. CoRR, abs/1710.10903, 2017.\n\n- Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. Comenet: Towards complete and efficient message passing for 3d molecular graphs. In NeurIPS, 2022.\n\n- Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. In International Conference on Learning Representations, ICLR, 2018.\n\n- Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-based recommendation with graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, 2019.\n\n- Zhenxing Wu, Jike Wang, Hongyan Du, Dejun Jiang, Yu Kang, Dan Li, Peichen Pan, Yafeng Deng, Dongsheng Cao, Chang-Yu Hsieh, and Tingjun Hou. Chemistry-intuitive explanation of graph neural networks for molecular property prediction with substructure masking. Nature Communications, 2023.\n\n- Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, ICML, volume 80 of Proceedings of Machine Learning Research, pp. 5449-5458. PMLR, 2018.\n\n- Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, ICLR, 2019.\n\n- Yifei Zhang, Hao Zhu, Ziqiao Meng, Piotr Koniusz, and Irwin King. Graph-adaptive rectified linear unit for graph neural networks. In The ACM Web Conference, WWW, 2022.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636546.9258914", "msecs": "925.0", "relativeCreated": "7619.03190612793", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Numerical Pitfalls in Policy Gradient Updates\n\n# Abstract\nNumerical instability, such as gradient explosion, is a fundamental problem in practical deep reinforcement learning (DRL) algorithms. Beyond anecdotal debugging heuristics, there is a lack of systematic understanding of the causes for numerical sensitivity that leads to exploding gradient failures in practice. In this work, we demonstrate that the issue arises from the ill-conditioned density ratio in the surrogate objective that comes from importance sampling, which can take excessively large values during training. Perhaps surprisingly, while various policy optimization methods such as TRPO and PPO prevent excessively large policy updates, their optimization constraints on KL divergence and probability ratio cannot guarantee numerical stability. This also explains why gradient explosion often occurs during DRL training, even with code-level optimizations. To address this issue, we propose the Vanilla Policy Gradient with Clipping algorithm, which replaces the importance sampling ratio with its logarithm. This approach effectively prevents gradient explosion while achieving performance comparable to PPO.\n\n\n# 1 INTRODUCTION\n\n\nDeep reinforcement learning (DRL) has demonstrated its effectiveness in various domains (Mnih et al., 2015; Silver et al., 2017; Vinyals et al., 2019; Ouyang et al., 2022). Despite these successes, the reliability and numerical stability of DRL algorithms remain fundamental limitations that hinder their application in real-world environments. In particular, DRL algorithms are often observed to have numerical instability issues, such as exhibiting the major problem of gradient explosion that destroys the learning progress. Existing work on the brittleness of DRL methods attributes this to either the high variance in actions (Fujita & Maeda, 2018), the gradient estimator (Liu et al., 2020), or the sensitivity to hyperparameters (Henderson et al., 2018).\n\n\nIn this paper, we perform a rigorous analysis of the root cause of numerical instability in policy optimization algorithms such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) (Schulman et al., 2015a; 2017). Gradient explosion is a consequence of overflows caused by computations involving excessively large numbers that exceed the limits of floating-point arithmetic. We show that the long horizons and large value variance are not the core reason for overflow, especially under action clipping as commonly used in standard DRL implementations (Duan et al., 2016; Raffin et al., 2021; Bou et al., 2024). Another popular explanation is that the covariance matrix of the stochastic policy can become singular during training, leading to gradient explosion. However, our experiments in Appendix B show that arithmetic overflows can occur even when the standard deviations are lower-bounded. Instead, our analysis indicates that the root cause is the importance sampling steps in TRPO and PPO (Schulman et al., 2015a; 2017), where the probability ratio can take exponentially large values, especially in the case of Gaussian distributions. Specifically, we analyze the condition number of the probability ratio, and demonstrate that the probability ratio is exponential with respect to a line integral in the policy space, which can lead to arithmetic overflows and cause gradient explosion.\n\n\nIn particular, we demonstrate that optimization constraints proposed in TRPO/PPO, such as limiting the KL-divergence and clipping, do not prevent numerical instability. We show that the probability ratio in importance sampling can grow exponentially fast, ultimately causing arithmetic overflows, even while the KL divergence remains small. This indicates that various policy optimization methods that prevent excessively large policy updates do not prevent numerical problems. Indeed, the probability ratios are not guaranteed to stay small, especially when the policy network's output is\n\n\nfurther clipped in the action space, leading to excessively small values in the probability density functions and large condition numbers in the density ratio.\n\n\nGiven that importance sampling is the root cause of the exploding gradient issue, we propose an algorithm called Vanilla Policy Gradient with Clipping, a modified version of the PPO algorithm in which the importance sampling ratio is replaced with its logarithm. We conduct experiments on continuous-control benchmarks to evaluate its effectiveness. Specifically, we find that this algorithm performs comparably to PPO on small policy networks where no gradient explosion occurs and maintains good performance on large policy networks where PPO encounters numerical issues. Additionally, the influence of other code-level techniques, such as reward scaling and learning rates, is discussed in Appendix B.\n\n\nOverall, our findings suggest that the TRPO/PPO loss is inherently ill-conditioned due to importance sampling and action clipping, which directly contribute to the numerical instability of deep policy gradient methods. This issue should not be viewed merely as a hyperparameter tuning problem but as a fundamental limitation of DRL algorithms that requires more attention.\n\n\n# 2 RELATED WORK\n\n\nDeep policy gradients in practice. It has been reported that implementation details and code-level optimizations fundamentally impact the performance of deep policy gradient algorithms such as TRPO and PPO (Henderson et al., 2018; Engstrom et al., 2020; Andrychowicz et al., 2021). In particular, empirical studies have shown that the most significant discrepancies between theory and implementation arise in gradient estimation, value prediction, and optimization landscapes (Ilyas et al., 2020). These findings were later supported by theoretical studies, which indicated that they stem from the fractal structures present in both value and policy landscapes (Wang et al., 2023). Our work contributes to this line of research by providing an in-depth study on the numerical stability of deep policy gradient methods.\n\n\nExploding gradients in deep learning. Many neural network architectures have been reported to suffer from the gradient explosion issue, including deep multi-layer networks (Bengio et al., 1994; Glorot & Bengio, 2010) and residual neural networks (RNNs) (Pascanu et al., 2013). This problem is closely linked to the depth of the network architecture (Philipp et al., 2017; Schoenholz et al., 2017), which can result in a long chain of Jacobian multiplications. In the context of reinforcement learning (RL), Liu et al. (2018) points out that the product of density ratios over a long horizon can grow exponentially, leading to exploding variance in off-policy estimation. Also, Khorasani et al. (2023) proposes to use second-order information in the form of Hessian vector products to bypass importance sampling weights in policy gradient. In this work, we demonstrate that even a single density ratio can take excessively large values and cause numerical overflow, and propose an algorithm that drops the importance sampling from PPO objective while achieving comparable performance. Wang et al. (2024) investigates gradient explosion in the context of generative adversarial imitation learning (GAIL), suggesting that a large reward function can cause exploding gradients. However, our empirical results in Section B indicate that this does not fully explain gradient explosion in policy gradient algorithms, as scaling down the reward does not completely overcome the issue.\n\n\n# 3 NUMERICAL INSTABILITY CAUSED BY IMPORTANCE SAMPLING\n\n\nTRPO and PPO objective. The original policy gradient estimator has the form\n\n\n$$\n\n$$\n\\nabla \\mathcal {J} (\\theta) = \\mathbb {E} _ {\\left(s _ {t}, a _ {t}\\right) \\sim \\pi_ {\\theta}} \\left[ \\nabla_ {\\theta} \\log \\pi_ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) Q ^ {\\pi} \\left(s _ {t}, a _ {t}\\right) \\right], \\tag {1}\n$$\n\n$$\n\n\nwhere  $Q^{\\pi}$  is the  $Q$ -function of the current policy  $\\pi_{\\theta}$  (Sutton et al., 1999). One difficulty of directly optimizing equation 1 is due to the complex dependency of sampled data on  $\\pi_{\\theta}$  (Schulman et al., 2015a). To address it, TRPO proposes a surrogate objective that incorporates importance sampling:\n\n\n$$\n\n$$\n\\max  _ {\\theta} \\hat {\\mathbb {E}} _ {(s _ {t}, a _ {t}) \\sim \\pi} \\left[ \\frac {\\pi_ {\\theta} (a _ {t} | s _ {t})}{\\pi (a _ {t} | s _ {t})} \\hat {A} ^ {\\pi} (s _ {t}, a _ {t}) \\right], \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\hat{\\mathbb{E}}_{(s_t,a_t)\\sim \\pi}[\\cdot ]$  denotes the estimated expectation over sampled trajectories and  $\\hat{A}^{\\pi}(s_t,a_t)$  the estimated advantage function at  $(s_t,a_t)$  which is usually obtained through Generalized Advantage\n\n\nEstimation (GAE, Schulman et al. (2015b)). Let  $L(\\theta)$  denote the surrogate objective in equation 2. It has been proved that the gradients of equation 1 and equation 2 coincide at  $\\pi = \\pi_{\\theta}$ . Additionally, both TRPO and PPO suggest that the current policy  $\\pi_{\\theta}$  should not deviate too much from the old policy  $\\pi$ , leading to various optimization constraints which will be discussed in Section 5.\n\n\nCondition number and numerical stability. We now briefly introduce the concept of the condition number from numerical analysis, which measures how much the output of an algorithm changes in response to a small change in its input:\n\n\nDefinition 3.1. (Condition number) Given a function  $f$  and an input  $x$ , let  $\\delta x$  be the error in  $x$  and  $\\delta f(x) = f(x + \\delta x) - f(x)$  be the corresponding error resulted in the output. We define the absolute condition number of  $f$  at  $x$  as\n\n\n$$\n\n$$\n\\kappa = \\lim  _ {\\delta \\rightarrow 0} \\sup  _ {| \\delta x | \\leq \\delta} \\frac {| \\delta f (x) |}{| \\delta x |}. \\tag {3}\n$$\n\n$$\n\n\nIn particular, when the function  $f$  is differentiable, it further has\n\n\n$$\n\n$$\n\\kappa = \\| J _ {f} (x) \\|,\n$$\n\n$$\n\n\nwhere  $J_{f}(x)$  is the Jacobian of  $f$  at  $x$ .\n\n\nA detailed study can be found in Trefethen & Bau (1997). The following concept of machine precision defines the threshold of numerical overflows:\n\n\nDefinition 3.2. (Machine precision) Let  $e > 0$  denote the machine precision throughout this paper, any quantity with its absolute value smaller than  $e$  will be considered zero.\n\n\nImportance sampling with clipped actions. Consider the probability ratio  $p_{\\theta}(a|s) = \\frac{\\pi_{\\theta}(a|s)}{\\pi(a|s)}$  in the loss function. This ratio might become ill-conditioned when  $\\pi(a|s)$  is very small. However, it is rarely reported in practice that importance sampling itself causes any numerical issues. This is because, when importance sampling is applied to modify a probabilistic distribution, the probability ratio  $p_{\\theta}(a|s)$  is only evaluated at data points sampled from the distribution  $\\pi(\\cdot|s)$ . This means that the probability of obtaining an action  $a$  where  $\\pi(a|s) \\ll 1$  is also very small, thereby rarely causing numerical instability. For example, let  $a \\sim \\mathcal{N}(\\mu_0(s), \\Sigma_0)$  be  $m$ -dimensional Gaussian random variable with probability density function  $\\pi(a|s)$ , then we have\n\n\n$$\n\n$$\nP (\\pi (a | s) <   e) \\leq 2 \\sqrt {2 \\pi} m \\left(e \\sqrt {\\det \\Sigma_ {0}}\\right) ^ {\\frac {1}{m}}, \\tag {4}\n$$\n\n$$\n\n\nwhere  $e \\ll 1$  is the machine precision.\n\n\nAlthough it is unlikely for  $\\pi(a|s)$  to take extremely small values if  $a$  is directly sampled from  $\\pi$  itself, as shown in equation 4, it is important to note that the actual action space usually has a bounded range due to physical or environmental constraints. Therefore, a common practice in DRL is to clip the sampled actions before feeding them into the simulator. Without loss of generality, we assume that the action space is of the box form  $\\mathcal{A} = [-\\beta, \\beta]^m$  for some positive  $\\beta$ , and write the action-clipping operation as  $\\phi: \\mathbb{R}^m \\to [-\\beta, \\beta]^m$ . Namely, for any  $a = (a_1, \\dots, a_m) \\in \\mathbb{R}^m$ ,  $\\phi(a) = (a_1', \\dots, a_m')$  where  $a_i' = \\max(-\\beta, \\min(a_i, \\beta))$ .\n\n\nNote that the probability ratio is evaluated after action clipping (Raffin et al., 2021). That is,  $p_{\\theta}(\\phi(a)|s)$  is used instead of  $p_{\\theta}(a|s)$  when computing the objective in equation 2. Consequently, the distance between the mean  $\\mu(s)$  and the clipped action  $\\phi(a)$  is no longer guaranteed to be small with high probability as suggested in equation 4, especially when  $\\mu(s)$  lies outside the action space  $A$ :\n\n\nTheorem 3.1. Let  $a \\sim \\mathcal{N}(\\mu(s), \\Sigma)$  be  $m$ -dimensional Gaussian random variable with probability density function  $\\pi(\\cdot|s)$ , and  $\\mathcal{A} = [-\\beta, \\beta]^m$  with  $\\beta > 0$ . Let  $\\phi: \\mathbb{R}^m \\to \\mathcal{A}$  be the action-clipping transformation, then\n\n\n1. Suppose that  $\\mu(s) \\in \\mathcal{A}$ , then for any  $a \\in \\mathbb{R}^m$ ,\n\n\n$$\n\n$$\nP (\\pi (\\phi (a) | s) <   e) \\leq 2 \\sqrt {2 \\pi} m (e \\sqrt {\\det \\Sigma_ {0}}) ^ {\\frac {1}{m}},\n$$\n\n$$\n\n\nwhere  $e$  is the machine precision;\n\n\n2. If  $\\mu(s) \\notin \\mathcal{A}$ , then for any  $a \\in \\mathbb{R}^m$  it has\n\n\n$$\n\n$$\n\\pi (\\phi (a) | s) \\leq \\frac {1}{(2 \\pi) ^ {\\frac {m}{2}} \\sqrt {\\det \\Sigma}} \\exp \\left(- \\frac {d ^ {2}}{2 \\lambda_ {m a x}}\\right),\n$$\n\n$$\n\n\nwhere  $d = \\mathrm{dist}(\\mu (s),\\mathcal{A})$  be the distance between  $\\mu (s)$  and the action space  $\\mathcal{A}$ ,  $\\lambda_{max}$  is the largest eigenvalue of  $\\Sigma$ .\n\n\nTherefore,  $\\pi (\\phi (a)|s)$  may fall below machine precision  $e$  when  $\\mu (s)$  is far from the action space  $\\mathcal{A}$  or when the standard deviations of  $\\pi (\\cdot |s)$  are sufficiently small. In either case,  $\\pi (\\phi (a)|s)$  becomes equal to 0 on the machine, and dividing by it can cause numerical issues.\n\n\nTanh-Gaussian transformation. It is worth mentioning that we can also bound the action by applying an invertible squashing function, such as tanh, to the Gaussian samples (Haarnoja et al., 2018). However, this approach still suffers from the ill-conditioned nature of importance sampling. For instance, consider the modified probability density function in the one-dimensional case, given by\n\n\n$$\n\n$$\n\\pi^ {\\prime} (a | s) = \\pi (u | s) \\left(1 - \\tanh  ^ {2} (u)\\right) ^ {- 1} \\tag {5}\n$$\n\n$$\n\n\nwhere  $u \\sim \\pi$  is a Gaussian random variable and  $a = \\tanh(u)$ . Note that when the mean of  $\\pi$  is too large, it is likely that the random variable  $u$  will take on extreme values, resulting in an excessively small value for the term  $1 - \\tanh^2(u)$  and causing numerical overflow in subsequent steps.\n\n\nIll-conditioned probability ratios. We have seen that the action-clipping transformation in deep policy gradients can significantly affect the numerical stability of importance sampling. However, one might argue that small values in  $\\pi(a|s)$  can be counterbalanced by small values in  $\\pi_{\\theta}(a|s)$  assuming that the current policy  $\\pi_{\\theta}$  is constrained to remain within a neighborhood of  $\\pi$ . This is particularly true in the first optimization step of each epoch, where  $\\pi_{\\theta}$  is reset to  $\\pi$  and all probability ratios are equal to 1. We will show that when action clipping is applied, the condition number of the probability ratio can become very large, causing  $p_{\\theta}(\\phi(a)|s)$  to grow exponentially fast and leading to instability as  $\\pi_{\\theta}$  deviates from  $\\pi$ . Let us again consider a Gaussian policy  $\\pi_{\\theta}(\\cdot|s) = \\mathcal{N}(\\mu(s;\\theta), \\Sigma(\\theta))$ , where  $\\mu(s)$  is typically parameterized by a neural network and  $\\Sigma(\\theta)$  is positive-definite and state-independent. Let  $\\pi(\\cdot|s) = \\mathcal{N}(\\mu(s;\\theta_0), \\Sigma(\\theta_0)))$ , the probability ratio is calculated as\n\n\n$$\n\n$$\n\\begin{array}{l} p _ {\\theta} (\\phi (a) | s) = \\frac {\\pi_ {\\theta} (\\phi (a) | s)}{\\pi (\\phi (a) | s)} \\\\ = \\sqrt {\\frac {\\det \\Sigma (\\theta_ {0})}{\\det \\Sigma (\\theta)}} \\frac {\\exp \\Big (- \\frac {1}{2} (\\phi (a) - \\mu (s ; \\theta)) ^ {T} \\Sigma (\\theta) ^ {- 1} (\\phi (a) - \\mu (s ; \\theta)) \\Big)}{\\exp \\Big (- \\frac {1}{2} (\\phi (a) - \\mu (s ; \\theta_ {0})) ^ {T} \\Sigma (\\theta_ {0}) ^ {- 1} (\\phi (a) - \\mu (s ; \\theta_ {0}))) \\Big)}. \\\\ \\end{array}\n$$\n\n$$\n\n\nTo better illustrate how the action-clipping transformation  $\\phi$  affects the condition number of  $p_{\\theta}$ , we further assume that  $\\Sigma (\\theta)\\equiv constant$ . The general case with parameterized standard deviations is discussed in Appendix C. Then, the gradient of the probability ratio is given by\n\n\n$$\n\n$$\n\\nabla_ {\\theta} p _ {\\theta} (\\phi (a) | s) = p _ {\\theta} (\\phi (a) | s) \\left(\\phi (a) - \\mu (s; \\theta)\\right) ^ {T} \\Sigma (\\theta) ^ {- 1} \\frac {\\partial \\mu (s ; \\theta)}{\\partial \\theta}, \\tag {6}\n$$\n\n$$\n\n\nwhere  $\\frac{\\partial\\mu(s;\\theta)}{\\partial\\theta}$  is the Jacobian of  $\\mu$  with respect to its parameters. Note that equation 6 is equivalent to\n\n\n$$\n\n$$\n\\nabla_ {\\theta} \\log \\Big (p _ {\\theta} (\\phi (a) | s) \\Big) = (\\phi (a) - \\mu (s; \\theta)) ^ {T} \\Sigma (\\theta) ^ {- 1} \\frac {\\partial \\mu (s ; \\theta)}{\\partial \\theta}, \\quad p _ {\\theta_ {0}} (\\phi (a) | s) = 1,\n$$\n\n$$\n\n\nwhich describes the gradient flow of  $p_{\\theta}$  in the policy space. Let  $\\mathcal{C}$  be the trajectory of policy parameters generated by PPO in the policy space, starting at  $\\theta_0$  and ending at  $\\theta_{1}$ . Applying the gradient theorem yields\n\n\n$$\n\n$$\np _ {\\theta_ {1}} (\\phi (a) | s) = \\exp \\left(\\int_ {\\mathcal {C}} (\\phi (a) - \\mu (s; \\theta)) ^ {T} \\Sigma (\\theta) ^ {- 1} \\frac {\\partial \\mu (s ; \\theta)}{\\partial \\theta} \\cdot d \\mathbf {r}\\right) \\tag {7}\n$$\n\n$$\n\n\nwhere the term in the exponent is the line integral along  $\\mathcal{C}$ . Therefore, the condition number  $\\kappa_{\\theta} = \\| \\nabla_{\\theta}p_{\\theta}(\\phi (a)|s)\\|$  can be extremely large as illustrated in Figure 1, implying that the probability ratio may overflow when\n\n- - The standard deviations of  $\\pi$  are small so that the matrix  $\\Sigma (\\theta)^{-1}$  has large eigenvalues;\n\n- - The action is clipped and the mean  $\\mu(s)$  lies outside  $\\mathcal{A}$  so that  $|\\phi(a) - \\mu(s; \\theta)|$  is large (illustrated in Figure 1(c));\n\n- - The policy parameterization  $\\mu (\\cdot ;\\theta)$  is complex (e.g. large policy networks) so that a small change in the parameter can lead to a dramatic difference in the output.\n\n\nSome DRL libraries further transform the standard deviations to prevent low variances. However, gradient explosion still occurs with those code-level techniques, indicating that the ill-conditioned probability ratio is the primary cause of the issue.\n\n\n![(a)](images/80e070eac0dce631c8c57b7069542977cfdf8fe6e6fa78a9d2282a2d8c8aa950.jpg)\n\n\n![(b) Figure 1: (a) When  $\\mu_0$  (the mean of  $\\pi_0$ ) is large, the condition number  $\\kappa_{\\theta}$  of the probability ratio  $p_{\\theta}$  changes exponentially when  $\\mu$  (the mean of  $\\pi$ ) deviates from  $\\mu_0$ ; (b)  $\\kappa_{\\theta}$  can become extremely large when the variance  $\\Sigma$  is small, particularly in cases of large means; (c) The action with the largest norm in each mini-batch grows outside the boundary during PPO training.](images/0309c81b33a71dcb866c9f40639ec3d55517aac930344bf0ca4c4c170bf15fa4.jpg)\n\n\n![(c)](images/1a4cc773ec14b85865c68182f6670597de906ac29e3e22ded4b878126cd4339e.jpg)\n\n\n# 4 EMPIRICAL ANALYSIS OF GRADIENT EXPLOSION\n\n\nIn the previous section, we theoretically demonstrated the ill-conditioned nature of importance sampling in TRPO/PPO methods. In this section, we present experimental evidence to support the theory. The experimental setup follows that described in Appendix A.\n\n\nGradient explosion is always accompanied by excessively large probability ratios. We run the Humanoid-v4 environment on five random seeds, all of which fail due to arithmetic overflow. In Figure 2, we observe that the maximum of probability ratio  $\\max_t \\log_{10}\\left(p_\\theta (a_t|s_t)\\right)$  takes excessively large values across all five random seeds. A widely accepted explanation for gradient explosion is that the standard deviations of the Gaussian policy can become very small, leading to potential singularity issues in the covariance matrix  $\\Sigma$ . While this can indeed cause numerical instability, it is not the only reason for gradient explosion. In Figure 6, we observe that the gradient explodes even before the standard deviations become small.\n\n\nLarge mean causes gradient explosion. We have mentioned that large outputs generated by the policy network are one of the primary causes of gradient explosion. To verify this analysis, we apply an additional transformation  $g(\\cdot)$  to the mean  $\\mu(s)$  to prevent it from leaving the action space  $\\mathcal{A}$ . The new Gaussian policy is defined as  $a \\sim \\mathcal{N}(g(\\mu(s)), \\Sigma(s))$ , where  $g(\\mu(s)) \\in \\mathcal{A}$ . In Table 1, we consider two transformations: the action-clipping function  $g = \\phi$ , defined in Section 3, and the hyperbolic tangent activation function  $g = \\tanh$ . We observe that both transformations effectively prevent the exploding gradient issue across three MuJoCo environments, indicating that having deviated means is indeed one of the main causes of numerical instability. Meanwhile, we also observe that the final returns are significantly lower than the benchmark (Raffin et al., 2021). This may be attributed to the fact that when the mean  $\\mu(s_t)$  goes outside  $\\mathcal{A}$  for some state  $s_t$ , the gradient norm\n\n\n$$\n\n$$\n\\nabla_ {\\theta} g (\\mu (s _ {t}; \\theta)) = \\frac {\\mathrm {d} g (\\mu)}{\\mathrm {d} \\mu} \\bigg | _ {\\mu = \\mu (s _ {t})} \\frac {\\partial \\mu (s _ {t} ; \\theta)}{\\partial \\theta},\n$$\n\n$$\n\n\n![(a)](images/6e586a0a0b9dad307be69807809d17c9b76159a59df20576ae5b0abe1ca28606.jpg)\n\n\n![(b)](images/fbf567908cdc2f0f8098d3a0b76f90126e7440fcba6657c57c14c59a02c002e0.jpg)\n\n\n![(c)](images/7c9d3df1f2e3fe85d813ad2b309b6dc51f5255ce87f855e7b105150b55975bb2.jpg)\n\n\n![(d)](images/5baf450fbdd89610e6212211c80f5148a02368bcbe392ea6839f0aece29e6dd0.jpg)\n\n\n![(e) Figure 2: The maximum of probability ratio after clipping takes exponentially large values when the gradient explodes in five individual runs.](images/adebdbcffc751f4bfeae8641e8a001fbcc2c91135a5bb55d531b5087c2251a8c.jpg)\n\n\n<table><tr><td>ENVIRONMENT</td><td>TRANSFORMATION</td><td>FINAL RETURN</td><td>EXPLOSION RATE</td></tr><tr><td>HOPPER-V4</td><td>CLIPPING</td><td>332 \u00b1 129</td><td>0%</td></tr><tr><td>HOPPER-V4</td><td>tanh</td><td>234 \u00b1 106</td><td>0%</td></tr><tr><td>HOPPER-V4</td><td>NONE</td><td>N/A</td><td>100%</td></tr><tr><td>WALKER2D-V4</td><td>CLIPPING</td><td>692 \u00b1 346</td><td>0%</td></tr><tr><td>WALKER2D-V4</td><td>tanh</td><td>434 \u00b1 60</td><td>0%</td></tr><tr><td>WALKER2D-V4</td><td>NONE</td><td>N/A</td><td>100%</td></tr><tr><td>HUMANOID-V4</td><td>CLIPPING</td><td>586 \u00b1 102</td><td>0%</td></tr><tr><td>HUMANOID-V4</td><td>tanh</td><td>530 \u00b1 72</td><td>0%</td></tr><tr><td>HUMANOID-V4</td><td>NONE</td><td>N/A</td><td>100%</td></tr></table>\n\n\nTable 1: Performance of mean transformation via clipping function or tanh function in three MuJoCo environments. For Hopper-v4 and Walker2d-v4, we directly apply tanh as their action input range is between -1 and 1. For the Humanoid-v4 task, we rescale the mapping to  $g = 0.4 * \\tanh$ , as the action inputs are bounded between -0.4 and 0.4.\n\n\nbecomes exponentially small or even equal to zero, depending on the transformation  $g$ . Notably, we may have  $|\\frac{\\mathrm{d}g(\\mu)}{\\mathrm{d}\\mu}|_2 \\simeq 0$  due to the vanishing gradient property of the transformation  $g$  when  $\\mu(s_t) \\notin \\mathcal{A}$ . As a result, policy gradient algorithms are more likely to converge towards sub-optimal regions in this case.\n\n\n# 5 OPTIMIZATION CONSTRAINTS CANNOT GUARANTEE NUMERICAL STABILITY\n\n\nIn previous sections, we demonstrated that importance sampling can be numerically unstable when the probability ratio is evaluated at transformed sample points. This is particularly problematic in deep policy gradient methods like TRPO and PPO, which incorporate the probability ratio into their surrogate objective functions. This integration is the fundamental reason for the numerical instability of these algorithms. However, it should be noted that both TRPO and PPO also apply certain constraints to the original optimization problem, such as KL divergence and probability ratio clipping. In this section, we will explain why these optimization constraints are insufficient to resolve the numerical issues as they are supposed to.\n\n\nKL divergence. In TRPO algorithm (Schulman et al., 2015a), it proposes to use the KL divergence between the current and old policy as a constraint to prevent large updates during policy improvement. Specifically, the KL divergence of two continuous probability distributions  $P$  and  $Q$  is defined as\n\n\n$$\n\n$$\nD _ {K L} (P \\| Q) = \\int_ {\\Omega} p (x) \\log \\frac {p (x)}{q (x)} \\mathrm {d} x, \\tag {8}\n$$\n\n$$\n\n\nwhere  $p$  and  $q$  are the probability density functions of  $P$  and  $Q$ , respectively. When optimizing the objective equation 2, TRPO applies the following hard constraint to the KL distance between successive policies\n\n\n$$\n\n$$\n\\hat {\\mathbb {E}} _ {s \\sim \\pi} \\left[ D _ {K L} \\left(\\pi_ {\\theta} (\\cdot | s) \\| \\pi (\\cdot | s)\\right) \\right] \\leq \\delta , \\tag {9}\n$$\n\n$$\n\n\nwhere  $\\delta > 0$  is a small quantity called the KL stepsize. Note that for many commonly used distributions, including Gaussian, the KL divergence between  $\\pi_{\\theta}$  and  $\\pi$  can be analytically calculated via a closed-form formula. For instance, consider two multivariate Gaussian policies  $\\pi_{\\theta}(\\cdot |s) \\sim \\mathcal{N}(\\mu(s), \\Sigma_0)$  and  $\\pi(\\cdot |s) \\sim \\mathcal{N}(\\mu_0(s), \\Sigma_0)$  with identical covariance matrices. The KL divergence is\n\n\n$$\n\n$$\nD _ {K L} \\left(\\pi_ {\\theta} (\\cdot | s) \\| \\pi (\\cdot | s)\\right) = \\frac {1}{2} \\left(\\mu (s) - \\mu_ {0} (s)\\right) ^ {T} \\Sigma_ {0} ^ {- 1} \\left(\\mu (s) - \\mu_ {0} (s)\\right). \\tag {10}\n$$\n\n$$\n\n\nWhile the above constraint can effectively prohibit  $\\pi_{\\theta}$  from making large updates steps in the distribution space, it may not be able to prevent the probability ratio  $p_{\\theta}(a|s)$  from taking extremely large values at some state-action pair  $(s,a)$  for two reasons: First, the KL divergence  $D_{KL}(\\pi_{\\theta}(\\cdot |s)\\parallel \\pi (\\cdot |s))$  measures the averaged distance between two distributions over the entire space  $\\mathbb{R}^m$ , while  $p_{\\theta}(a|s)$  is determined at a specific point  $a\\in \\mathbb{R}^m$ , which means that we can find some  $a\\in \\mathbb{R}^m$  such that the probability ratio  $p_{\\theta}(a|s)$  takes a large value even if the KL distance  $D_{KL}(\\pi_{\\theta}(\\cdot |s))$  is small. Second, the absolute value of KL divergence  $|D_{KL}(\\pi_{\\theta}(\\cdot |s)\\parallel \\pi (\\cdot |s))|$  is quadratic with respect to the policy update  $|\\mu (s) - \\mu_0(s)|$ , while the probability ratio  $\\pi_{\\theta}(a|s)$  grows exponentially with  $|\\mu (s) - \\mu_0(s)|$  especially for excessively small  $\\pi (a|s)$ .\n\n\nProbability ratio clipping. While TRPO involves second-order computations that are usually expensive in practice, PPO (Schulman et al., 2017) proposes a clipped surrogate objective\n\n\n$$\n\n$$\nL ^ {C L I P} (\\theta) = \\hat {\\mathbb {E}} _ {(s _ {t}, a _ {t}) \\sim \\pi} \\left[ \\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} (s _ {t}, a _ {t}), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right)\\right) \\right] \\tag {11}\n$$\n\n$$\n\n\nwhere  $\\epsilon \\in (0,1)$  is the clipping parameter. The clipped loss equation 11 is expected to block large updates in the policy that can improve the objective (i.e.,  $\\hat{A}^{\\pi}(s_t,a_t) > 0$ ), and allow them if they make the objective worse ( $\\hat{A}^{\\pi}(s_t,a_t) < 0$ ). This property, however, makes it possible that some ill-conditioned probability ratios are evaluated when computing  $L^{CLIP}(\\theta)$ .\n\n\nIn particular, consider a given state-action pair  $(s,a)$  and the estimated advantage  $\\hat{A}^{\\pi}(s,a)$ :\n\n\n- If  $\\hat{A}^{\\pi}(s,a)\\geq 0$  , it has\n\n\n$$\n\n$$\n\\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} (s, a), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} (s, a)\\right) = (1 + \\epsilon) \\hat {A} ^ {\\pi} (s, a),\n$$\n\n$$\n\n\nwhenever  $p_{\\theta} \\geq 1 + \\epsilon$ . Thus, it automatically blocks extremely large values in  $p_{\\theta}$  when evaluating  $L^{CLIP}(\\theta)$ ;\n\n\n- If  $\\hat{A}^{\\pi}(s,a) < 0$ , it is possible that the machine directly evaluates large probability ratios, since\n\n\n$$\n\n$$\n\\begin{array}{l} \\text {I n t e g r a n d i n e q u a t i o n 1 1} = \\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} (s, a), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} (s, a)\\right) \\\\ = \\hat {A} ^ {\\pi} (s, a) \\max  \\left(p _ {\\theta}, \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right)\\right) \\\\ = \\hat {A} ^ {\\pi} (s, a) \\max  \\left(p _ {\\theta}, 1 - \\epsilon\\right) \\\\ = p _ {\\theta} \\hat {A} ^ {\\pi} (s, a) \\\\ \\end{array}\n$$\n\n$$\n\n\nwhen  $p_{\\theta} \\geq 1 - \\epsilon$ , which then may lead to numerical overflows.\n\n\nIn practice, the expectation in equation 11 is estimated by the average over sampled trajectories\n\n\n$$\n\n$$\nL ^ {C L I P} (\\theta) \\simeq \\frac {1}{T} \\sum_ {t = 0} ^ {T - 1} \\min  \\left(p _ {\\theta} \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right), \\operatorname {c l i p} \\left(p _ {\\theta}, 1 - \\epsilon , 1 + \\epsilon\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right)\\right).\n$$\n\n$$\n\n\nTherefore, numerical errors may occur if there exists any pair  $(s_t, a_t)$  with negative advantage  $\\hat{A}^{\\pi}(s_t, a_t) < 0$  and large probability ratio  $p_{\\theta}(a_t | s_t) \\gg 1$ .\n\n\nEmpirical results. We perform experiments to validate aforementioned theoretical analysis. In Figure 3 (a), we see that the KL divergence  $D_{KL}(\\pi_{\\theta}(\\cdot |s) \\parallel \\pi (\\cdot |s))$  remains small even when the probability ratio  $p_{\\theta}(a|s)$  takes very large values, indicating that  $D_{KL}(\\pi_{\\theta}(\\cdot |s) \\parallel \\pi (\\cdot |s))$  cannot ensure numerical stability. We also test the following KL-penalized objective\n\n\n$$\n\n$$\nL ^ {K L P E N} (\\theta) = \\hat {\\mathbb {E}} _ {t} \\left[ p _ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right) - b D _ {K L} \\left(\\pi_ {\\theta} (\\cdot \\mid s _ {t}) \\| \\pi (\\cdot \\mid s _ {t})\\right) \\right] \\tag {12}\n$$\n\n$$\n\n\nwhich is also proposed in Schulman et al. (2017). In Figure 3 (b), we can observe that optimizing equation 12 still fails to prevent the probability ratio growing exponentially. While probability ratio clipping has proven insufficient to overcome numerical instability, it is worth noting that the PPO algorithm becomes even more unstable without clipping, as shown in Figure 3 (c) where the gradient explodes within 4 steps. When plotting  $\\max_t\\log_{10}\\left(p_\\theta (a_t|s_t)\\right)$ , it is calculated after clipping. For example, a large  $p_\\theta (a_t|s_t)$  with positive advantage  $\\hat{A}^{\\pi}(s_t,a_t)$  is clipped and equal to  $1 + \\epsilon$ .\n\n\n![(a)](images/11471b6d0fad3487752b972cc65877b4b0ef8018fe8c68e7bfebf09eb7c48397.jpg)\n\n\n![(b) Figure 3: The logarithm of KL divergence  $\\log_{10}\\left(D_{KL}(\\pi_{\\theta}(\\cdot |s)\\parallel \\pi (\\cdot |s))\\right)$  and the maximum of probability ratio  $\\max_t\\log_{10}\\left(p_\\theta (a_t|s_t)\\right)$  at each optimization step is plotted with different optimization constraints, where environment is MuJoCo Humanoid-v4. In each plot, we apply (a) probability ratio clipping; (b) KL-penalty; (c) no constraints.](images/82524e3bd5e898124a4561cc6f1647abfacfbb1fe725f5673d8caad7f0b45a1f.jpg)\n\n\n![(c)](images/81539e00d6b66443c13f8c6da010e300f757475bd52664bc667f30ebea601d94.jpg)\n\n\n# 6 BYPASSING IMPORTANCE SAMPLING IN PPO\n\n\nIn previous sections, we demonstrated that the TRPO/PPO objective is potentially ill-conditioned and that the built-in optimization constraints cannot effectively alleviate its numerical instability. In this section, we examine the numerical stability of the vanilla policy gradient algorithm and propose a new algorithm based on it.\n\n\nNumerical stability of vanilla policy gradient. We have discussed why importance sampling in TRPO/PPO is the primary cause of gradient explosion. It is also worth stepping back to revisit the vanilla policy gradient, whose objective does not involve any density ratio. In this case, we optimize the following objective\n\n\n$$\n\n$$\nL ^ {V A N I L L A} (\\theta) = \\hat {\\mathbb {E}} _ {\\left(s _ {t}, a _ {t}\\right) \\sim \\pi_ {\\theta}} \\left[ \\log \\pi_ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) \\hat {A} ^ {\\pi} \\left(s _ {t}, a _ {t}\\right) \\right], \\tag {13}\n$$\n\n$$\n\n\nwhere we replace  $Q^{\\pi}(s_t, a_t)$  with  $\\hat{A}^{\\pi}(s_t, a_t)$  to reduce the variance. The logarithm of the Gaussian probability density function is\n\n\n<table><tr><td>ENVIRONMENT</td><td>FINAL RETURN</td><td>EXPLOSION RATE</td></tr><tr><td>HOPPER-V4</td><td>536 \u00b1 254</td><td>0%</td></tr><tr><td>WALKER2D-V4</td><td>195 \u00b1 135</td><td>0%</td></tr><tr><td>HUMANOID-V4</td><td>355 \u00b1 191</td><td>0%</td></tr></table>\n\n\nTable 2: Empirical results of vanilla policy gradient in three MuJoCo environments. Despite its poor performance, vanilla policy gradient does not suffer from the exploding gradient issue. Hyperparameters are specified in Appendix A.\n\n\n$$\n\n$$\n\\log \\pi_ {\\theta} \\left(a _ {t} \\mid s _ {t}\\right) = - \\frac {1}{2} \\sum_ {i = 1} ^ {m} \\frac {\\left(a _ {t} - \\mu\\right) _ {i} ^ {2}}{\\sigma_ {i} ^ {2}} - \\frac {m}{2} \\log (2 \\pi) - \\sum_ {i = 1} ^ {m} \\log \\sigma_ {i}, \\tag {14}\n$$\n\n$$\n\n\nwhich is less likely to take extreme values where  $(a_{t} - \\mu)_{i}$  denotes the  $i$ -th element in  $(a_{t} - \\mu)$  and  $\\Sigma = \\mathrm{diag}(\\sigma_1^2,\\dots,\\sigma_m^2)$ . While the only problematic thing is that  $\\Sigma$  might have small eigenvalues, some DRL libraries employ a lower bound to the standard deviation of the policy to avoid singularity issues as shown in Table 5. Furthermore, the numerical stability of vanilla policy gradient is not very sensitive to the dimension of the action space: in equation 14, the magnitude of  $|\\log \\pi_{\\theta}(a_t|s_t)|$  is roughly linear to the dimension  $m$ . Similarly, the magnitude of the integrand  $\\mathcal{L}$  in equation 17 is also approximately linear to the dimension of  $\\mathcal{A}$ , implying that the probability ratio  $p_{\\theta}$  is approximately exponential to the dimension of  $\\mathcal{A}$  and thus much easier to explode as  $m$  increases.\n\n\nIn Table 2, we observe that the vanilla policy gradient does not exhibit any numerical instability when the standard deviations are lower-bounded, further supporting the claim that importance sampling is the primary cause of gradient explosion in deep policy gradient methods. It should be noted that while extremely small standard deviations can still affect numerical stability, this issue can be effectively mitigated by setting a lower bound on the standard deviations, as is done in PPO (though it is not entirely effective).\n\n\nVanilla Policy Gradient with Clipping. Based on the previous analysis, the vanilla policy gradient algorithm is more stable than TRPO/PPO. Motivated by this observation, we propose an objective that combines the strengths of both approaches. This modification results in the following objective:\n\n\n$$\n\n$$\nL ^ {C P G} (\\theta) = \\hat {\\mathbb {E}} _ {\\left(s _ {t}, a _ {t}\\right) \\sim \\pi} \\left[ \\min  \\left(\\log p _ {\\theta} \\hat {A} _ {t} ^ {\\pi}, \\operatorname {c l i p} \\left(\\log p _ {\\theta}, \\log (1 - \\epsilon), \\log (1 + \\epsilon)\\right) \\hat {A} _ {t} ^ {\\pi}\\right)\\right) \\tag {15}\n$$\n\n$$\n\n\nwhere  $\\log p_{\\theta} = \\log \\pi_{\\theta} - \\log \\pi$ . Notably, the new objective can also be interpreted as a logarithmic variant of the PPO loss in equation 11. As shown in Figure 5 and Table 2, this method achieves performance comparable to PPO and outperforms the vanilla policy gradient. Also, when there is no exploding gradient issue, our algorithm can achieve similar performance to PPO. To examine the performance in the general case, we use smaller policy networks with width 64 which are less affected by numerical issues based on our analysis in Section 3, and run both algorithms in three benchmarks. As shown in Figure 4, both achieve similar performance across all three environments, indicating that our algorithm serves as a numerically robust alternative to PPO.\n\n\n![(a) Hopper-v4](images/21ca62d2c9ca779e8122f1ddea3aacf9d1b8c513193f498927b375332d66233e.jpg)\n\n\n![(b) Walker2d-v4 Figure 4: For smaller policy networks that are less affected by gradient explosion, the proposed algorithm achieves performance comparable to PPO on three MuJoCo benchmarks.](images/be65ccf84f4c529bcdbccda3361e9b7ec3d02c740c77b6d1a464bf473d37e5f3.jpg)\n\n\n![(c) Humanoid-v4](images/9d28cdc1d2a8bb33a1e10bf9add76a5aaecff99fca1d40d441b06634b3485de1.jpg)\n\n\n![(a) Hopper-v4](images/d8a4a9efc920bce77427c71b9dfad3a453f9e3aa418c9b71399d2242921753a5.jpg)\n\n\n![(b) Walker2d-v4 Figure 5: For larger policy networks, where PPO completely fails due to gradient explosion, our algorithm avoids numerical issues and continues to produce reasonable results.](images/086c6077f5f1bfb37dcd8997c151d319fc5bcd33f58557aaa06ff698ddd1456a.jpg)\n\n\n![(c) Humanoid-v4](images/8809f256f79cd3b66f7b5d10d68356cff5782c114d6cc72ac3ed996063f584d0.jpg)\n\n\n# 7 CONCLUDING REMARKS\n\n\nIn this work, we demonstrate that ill-conditioned density ratios in importance sampling are the fundamental cause of numerical instability in deep policy gradient methods. While this issue cannot be effectively addressed through optimization constraints alone, code-level techniques are necessary to fully eliminate or mitigate its effects. To address this issue, we find that directly dropping the importance sampling term yields the best performance, and we therefore advocate for further investigation in this direction in future studies.\n\n\nAccording to our analysis, larger neural networks make bigger updates in each optimization step, resulting in larger density ratios in importance sampling and thus increasing the risk of gradient explosion. Therefore, we question whether deep policy gradient methods are numerically stable and robust when optimizing complex policy networks. This is particularly important for the scalability of deep policy gradient methods. Although the experiments in this work focus on MuJoCo continuous-control environments, the limitations discussed are inherent to the algorithms and may apply to other scenarios as well. We believe that a better understanding of these issues can undoubtedly enhance the effectiveness of deep reinforcement learning in real-world applications.\n\n\n# REFERENCES\n\n- A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98): 1-76, 2021.\n\n- M. Andrychowicz, A. Raichuk, P. Stanczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski, S. Gelly, and O. Bachem. What matters for on-policy deep actor-critic methods? A large-scale study. In *ICLR*, 2021.\n\n- Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, 1994.\n\n- A. Bou, M. Bettini, S. Dittert, V. Kumar, S. Sodhani, X. Yang, G. De Fabritiis, and V. Moens. TorchRL: A data-driven decision-making library for pytorch. In ICLR, 2024.\n\n- Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In ICML, 2016.\n\n- L. Engstrom, A. Ilyas, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph, and A. Madry. Implementation matters in deep RL: A case study on PPO and TRPO. In ICLR, 2020.\n\n- Y. Fujita and S. Maeda. Clipped action policy gradient. In ICML, 2018.\n\n- X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, pp. 249-256, 2010.\n\n- T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel, and S. Levine. Soft actor-critic algorithms and applications. In NeurIPS, 2018.\n\n- P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning that matters. In AAAI, pp. 3207-3214, 2018.\n\n- A. Ilyas, L. Engstrom, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph, and A. Madry. A closer look at deep policy gradients. In *ICLR*, 2020.\n\n- S. Khorasani, S. Salehkaleybar, N. Kiyavash, N. He, and M. Grossglauer. Efficiently escaping saddle points for non-convex policy optimization. arXiv preprint arXiv:2311.08914, 2023.\n\n- D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\n- T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n\n- Q. Liu, L. Li, Z. Tang, and D. Zhou. Breaking the curse of horizon: Infinite-horizon off-policy estimation. In NeurIPS, 2018.\n\n- Y. Liu, K. Zhang, Tamer Ba,sar, and W. Yin. An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods. In NeurIPS, pp. 7624-7636, 2020.\n\n- E.N.Lorenz.The Essence of Chaos.University of Washington Press,1995.\n\n- V. Mnih, . Kavukcuoglu, D. Silver, A. A Rusu, J. Veness, M. G Bellemare, A. Graves, M. Riedmiller, A. K Fidjeland, G. Ostrovski, and et al. Human-level control through deep reinforcement learning. Nature, 518:529-533, 2015.\n\n- L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In NeurIPS, pp. 27730-27744, 2022.\n\n- R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In ICML, pp. 1310-1318, 2013.\n\n- G. Philipp, D. Song, and J. G. Carbonell. The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions. arXiv preprint arXiv:1712.05577, 2017.\n\n- A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann. Stable-Baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22 (268):1-8, 2021.\n\n- S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. In ICLR, 2017.\n\n- J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In ICML, pp. 1889-1897, 2015a.\n\n- J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.\n\n- J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\n- D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In ICML, pp. 387-395, 2014.\n\n- D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. Driessche, T. Graepel, and D. Hassabis. Mastering the game of Go without human knowledge. Nature, 550:354-359, 2017.\n\n- R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, pp. 1057-1063, 1999.\n\n- L.N.Trefethen and D.Bau.Numerical Linear Algebra.SIAM,1997.\n\n- O. Vinyals, I. Babuschkin, W.M. Czarnecki, and et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575:350-354, 2019.\n\n- T. Wang, S. Herbert, and S. Gao. Fractal landscapes in policy optimization. In NeurIPS, pp. 4277-4294, 2023.\n\n- W. Wang, Y. Zhu, Y. Zhou, C. Shen, J. Tang, Z. Xu, Y. Peng, and Y. Zhang. Exploring gradient explosion in generative adversarial imitation learning: A probabilistic perspective. In AAAI, 2024.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636547.9952374", "msecs": "995.0", "relativeCreated": "8688.377857208252", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Improving Neural Language Generation with Spectrum Control\n\n# Abstract\nRecent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an anisotropic cone in the vector space, which is called the representation degeneration problem. In this paper, we propose a novel spectrum control approach to address this degeneration problem. The core idea of our method is to directly guide the spectra training of the output embedding matrix with a slow-decaying singular value prior distribution through a reparameterization framework. We show that our proposed method encourages isotropy of the learned word representations while maintains the modeling power of these contextual neural models. We further provide a theoretical analysis and insight on the benefit of modeling singular value distribution. We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model, and various Transformer-based models for machine translation, on common benchmark datasets for these tasks.\n\n\n# 1 INTRODUCTION\n\n\nNeural language generation (NLG) is an important task with many practical applications, such as automatic speech recognition (Graves et al., 2013; Toshniwal et al., 2018), text generation (Bowman et al., 2016; Radford et al., 2019; Keskar et al., 2019), machine translation (Bahdanau et al., 2015; Vaswani et al., 2017) and dialog systems (Gao et al., 2019a; Tang et al., 2019). Most NLG models utilize a complex encoding model to map a given context into a hidden state vector, and then predict the next word distribution by multiplying the encoded vector with the output embedding layer, followed by a softmax layer. In the past few years, it has witnessed a significant progress in NLG by improving the encoding model, from the recurrent neural network (RNN) (Bahdanau et al., 2015; Jozefowicz et al., 2016; Merity et al., 2018a) based models to the current Transformer-based models (Vaswani et al., 2017; Devlin et al., 2019; Dai et al., 2019; Radford et al., 2019). However, embeddings in the softmax output layer have been shown not capable enough to model the conditional probability (Yang et al., 2018).\n\n\nRecently, Gao et al. (2019b) pointed out another limitation of the output embeddings: the representation degeneration problem. They showed that the singular value distribution of the output embedding matrix tends to decay very fast, and the embedding space is squeezed into a narrow cone (as shown in Figure 1(a) and 1(c) in 2-D plots). Such anisotropic shape (Ethayarajh, 2019) is very different from what one would expect from an expressive word embedding space (Arora et al., 2016a; Mu & Viswanath, 2018). Therefore, several efforts (Gao et al., 2019b; Wang et al., 2019a) have been made to address the degeneration problem.\n\n\nUnlike previous approaches that applied implicit regularization to singular values of the output embedding matrix, we propose a novel spectrum control (SC) approach to explicitly control the singular value distribution: we first reparameterize the output embedding matrix  $\\mathbf{W}$  by its singular value decomposition (SVD):  $\\mathbf{W} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ , where  $\\mathbf{U},\\mathbf{V}$  are column orthonormal matrices, and  $\\boldsymbol{\\Sigma}$  is a diagonal matrix of singular values. Then we guide the training of  $\\boldsymbol{\\Sigma}$  by a predefined slow-decaying prior distribution, such as a polynomial decay distribution, or an exponential decay distribution. At the end of training, the distribution of singular values of the embedding matrix gets close to the prior distribution. Our spectrum control approach alleviates the representation degeneration problem by encouraging the diversity of word representations and improving isotropic property of these representations (see Figure 2), even on top of the powerful Transformer-based models.\n\n\n![(a) Transformer-XL](images/ac64bc4b862a767a1e12e3ad5746e66c95ffe8652ed3d69a7f21501bf8762236.jpg)\n\n\n![(b) Singular Values Figure 1: Projected word embeddings $^{1}$  and singular value distributions. (a) and (c): 2-D visualization of word embedding matrices of Transformer-XL for language modeling and Transformer for machine translation; (b) and (d): Normalized singular value distributions of embedding matrices.](images/054f52f50c2204308cc2c44275b5dee1b043c433a0593b2ed58961c49eb9568c.jpg)\n\n\n![(c) Transformer](images/4c00db17561ecfe9b8b65479b68cdec2c81f73634844c320f958ca51e18b6f5f.jpg)\n\n\n![(d) Singular Values](images/3434a9626338ddec8ced32870afed94c7432f7e4160af3ae12fffb7a3862d74a.jpg)\n\n\n![(a) Transformer-XL+SC](images/03ccd35f568b9f17f48bfdee7f08254a9eb1104fe9950abe8f641f5a0982581e.jpg)\n\n\n![(b) Singular Values Figure 2: Projected word embeddings and singular value distributions using our spectrum control method. (a) and (c): 2-D visualization of word embedding matrices of Transformer-XL for language modeling and Transformer for machine translation; (b) and (d): Normalized singular value distributions of embedding matrices.](images/1e1b6053028e7dd8f15fe323190ea4e0a0845bb141a5794ae5073a94b5959b17.jpg)\n\n\n![(c) Transformer+SC](images/b915bb7d43461a73390a90e5c0c8d7d92e47eb906abfc74f049f5fdc332891a7.jpg)\n\n\n![(d) Singular Values](images/eba354725e722319aaf51957ed7361487e9e5d0fb18968870672d78337a59a6d.jpg)\n\n\nWe further present a theoretical analysis to justify our method. The results suggest that it is beneficial to directly guide the singular value distribution of the embedding matrix throughout the training process and control the decay rate of these singular values. We demonstrate the effectiveness of our training framework with extensive experimental results on two tasks: language modeling and machine translation. Our spectrum control method outperforms the latest state-of-the-art TransformerXL model on WikiText-103 dataset for language modeling; and obtains close to 1.5 BLEU improvement on IWSLT 2014 German-English translation task compared to the Transformer baseline model.\n\n\n# 2 RELATED WORK\n\n\nIn this paper, we mainly focus on the output embedding matrix that is used in the softmax output layer for language generation tasks. This embedding matrix is also used as input word embeddings, which is known as weight tying trick. The weight tying not only reduces the number of parameters but also enjoys theoretical benefits (Inan et al., 2017). Thus the weight tying has been successfully applied to many state-of-the-art models for language modeling and machine translation (Merit et al., 2018a; Vaswani et al., 2017; Yang et al., 2018).\n\n\nHowever, there are certain issues with the softmax output layer. Yang et al. (2018) first identified a problem called \"softmax bottleneck\" that the softmax output layer does not have enough capacity to model natural language due to its connection with the rank bottleneck in matrix factorization. A simple and effective method called Mixture of Softmaxes (MoS) was proposed to deal with this issue. Follow-up work in Kanai et al. (2018); Ganea et al. (2019) tried to replace softmax with alternative activation functions. Kanai et al. (2018) proposed to use sigsoftmax, which is composed of a multiplication of an exponential function and sigmoid function. Ganea et al. (2019) proposed a Linear-Monotonic-Softmax (LMS) model that generalized the approach in Kanai et al. (2018) by learning parametric point-wise increasing functions to optimally distort the logits before feeding them into the softmax layer. Pappas & Henderson (2019) instead resorted to a powerful deep residual nonlinear output mapping while using a single softmax function without modifying its dimensionality or rank.\n\n\nAnother line of work focuses on increasing the expressive power of the output embedding matrix by adding a cosine similarity regularization term (Gao et al., 2019b), or adversarial noise (Wang et al., 2019a). Gao et al. (2019b) analyzed the representation degeneration problem that the out-\n\n\nput embeddings tend to degenerate and be distributed into a narrow cone. A regularization term based on the summation of pairwise cosine similarity among all words was proposed to increase the representation power of word embeddings. Wang et al. (2019a) pointed out the computation of regularization term in (Gao et al., 2019b) depends on the size of the vocabulary and hence is costly. Instead, they proposed a simple yet highly effective adversarial training method that adds adversarial noises to the output embedding layer when training the models. They proved in theory that this adversarial training increases the distances between two different words, and thus encourages the diversity of the embedding vectors. Our work follows this line of thought, but takes a different approach: inspired by the spectrum control that encourages slow singular value decay in Generative Adversarial Network (GAN) training (Jiang et al., 2019), we propose a novel method that directly guides the singular values by a slow-decaying prior distribution during the model training process. We show that the anisotropic behavior of the contextualized word representations from powerful Transformer-based models (Ethayarajh, 2019) are alleviated by our spectrum control approach.\n\n\n# 3 PROBLEM SETUP\n\n\nIn this section, we briefly introduce the neural models for language generation, and illustrate the singular value decay phenomena of existing neural language models. We first introduce some notations used in the rest of the paper.\n\n\nNotation: For a  $d$ -dimensional vector  $\\mathbf{x} \\in \\mathbb{R}^d$ , we use  $\\| \\mathbf{x} \\|_q = (\\sum_{i=1}^d |x_i|^q)^{1/q}$ , where  $0 < q < \\infty$  to denote its  $\\ell_q$ -norm, and  $\\| \\mathbf{x} \\|_\\infty = \\max_i |x_i|$  to be its infinity norm. For a matrix  $\\mathbf{A} \\in \\mathbb{R}^{d_1 \\times d_2}$ , let  $\\mathbf{A}_{i*}$  be the  $i$ -th row of  $\\mathbf{A}$ , and we use  $\\| \\mathbf{A} \\|_2$ ,  $\\| \\mathbf{A} \\|_F$ ,  $\\| \\mathbf{A} \\|_1$  to denote its spectral norm, Frobenius norm, and matrix 1-norm. Given two sequences  $\\{a_n\\}$  and  $\\{b_n\\}$ , if there exists a constant  $0 < C < \\infty$  such that  $a_n \\leq C b_n$ , we write  $a_n = O(b_n)$ , and we use  $\\widetilde{O}(\\cdot)$  to hide the logarithmic factors.\n\n\n# 3.1 NEURAL LANGUAGE GENERATION\n\n\nWe first briefly review the softmax output layer typically used in neural language generation models. We define the joint probability of a given length- $n$  sentence of words (tokens)  $\\mathbf{s}_n = (\\mathbf{y}_1, \\dots, \\mathbf{y}_n)$  as the following product of conditional probabilities\n\n\n$$\n\n$$\n\\mathbb {P} \\left(\\mathbf {s} _ {n}\\right) = \\prod_ {t = 1} ^ {n} \\mathbb {P} \\left(\\mathbf {y} _ {t} \\mid \\mathbf {c} _ {t}\\right), \\tag {3.1}\n$$\n\n$$\n\n\nwhere  $\\mathbf{y}_t\\in \\mathcal{V}$  is the  $t$  -th word in sentence  $\\mathbf{s}_n$ , and  $\\mathcal{V}$  represents the word vocabulary,  $\\mathbf{c}_t = \\mathbf{y}_{1:t - 1} = (\\mathbf{y}_1,\\dots ,\\mathbf{y}_{t - 1})$  is referred to as the context of word  $\\mathbf{y}_t$ . In addition, the context  $\\mathbf{c}_t$  is usually modeled by a fixed size vector  $\\mathbf{h}_t\\in \\mathbb{R}^d$ , which is referred to as the hidden state, using some neural networks such as LSTM (Hochreiter & Schmidhuber, 1997) and Transformer (Vaswani et al., 2017). Then, the probability distribution of the output word  $\\mathbf{y}_t$  given the context  $\\mathbf{c}_t$ , i.e.,  $\\mathbb{P}(\\mathbf{y}_t|\\mathbf{c}_t)$  in (3.1), is parameterized as the following softmax function:\n\n\n$$\n\n$$\n\\mathbb {P} \\left(\\mathbf {Y} _ {t} = \\mathbf {y} _ {t} \\mid \\mathbf {c} _ {t}\\right) = \\mathbb {P} \\left(\\mathbf {Y} _ {t} = \\mathbf {y} _ {t} \\mid \\mathbf {h} _ {t}\\right) = \\frac {\\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\mathbf {W} _ {\\mathcal {I} (\\mathbf {y} _ {t}) *}\\right)}{\\sum_ {i = 1} ^ {N} \\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\mathbf {W} _ {i *}\\right)}, \\tag {3.2}\n$$\n\n$$\n\n\nwhere  $\\mathbf{W} \\in \\mathbb{R}^{N \\times d}$  is the weight matrix and is usually tied with the input word embedding matrix (Press & Wolf, 2017; Inan et al., 2017),  $N = |\\mathcal{V}|$  is the vocabulary size,  $d$  is the embedding dimension,  $\\mathcal{I}(\\mathbf{y}_t)$  represents the index of word  $\\mathbf{y}_t$  in vocabulary  $\\mathcal{V}$ . In the following discussion, we call output weight matrix  $\\mathbf{W}$  as the word embedding matrix of the neural language model since it is tied with the input word embedding matrix.\n\n\nIn this paper we focus on the output layer of the neural model for language generation, i.e., the softmax layer in (3.2), as in (Yang et al., 2018; Kanai et al., 2018; Ganea et al., 2019; Gao et al., 2019b). We will examine the singular value distribution of  $\\mathbf{W}$  and propose a different approach to increase the expressive power of the word embedding.\n\n\n# 3.2 FAST SINGULAR VALUE DECAY\n\n\nAs we mentioned before, the singular values of  $\\mathbf{W}$  tend to drop very fast and it may interact with the cone-shaped embedding space. More specifically, Figure 1(b) and 1(d) illustrate the distributions of the normalized singular values of the Transformer-XL based language model $^2$  (Dai et al., 2019)\n\n\nand the Transformer-based machine translation model (Vaswani et al., 2017) trained on WikiText-103 (Merit et al., 2018a) and IWSLT 2014 De-En (Cettolo et al., 2014) datasets, respectively. The plots show a fast singular value decay phenomenon, i.e., there is a huge drop between the first and remaining singular values. Such a phenomenon has also been observed in some previous work (Gao et al., 2019b; Wang et al., 2019a).\n\n\nFigure 1(a) and 1(c) present the distributions of the projected word embeddings of the aforementioned two models. We can see from the plots that the projected word embeddings are distributed into some narrow cone shapes, which implies an anisotropic property of the learned word representations, i.e., the embedding vectors are not uniformly distributed in the space. The detailed analysis in Ethayarajh (2019) also confirms that contextualized word embeddings learnt from ELMo (Peters et al., 2018) (LSTM-based model), BERT and GPT-2 (Devlin et al., 2019; Radford et al., 2019) (Transformer-based models) indeed tend to be anisotropic.\n\n\nThese contextualized word embeddings have shown great success on many NLP tasks. However, static word embeddings, such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) have been shown (Arora et al., 2016b; Mu & Viswanath, 2018) to be isotropic with great expressive power. Hence it may be also beneficial to increase the expressive power of the contextualized word embedding by increasing its isotropy. This motivates us to alleviate the fast singular value decay phenomenon to increase the isotropy of the learned word representations.\n\n\n# 4 PROPOSED METHOD\n\n\nTo alleviate the fast singular value decay phenomenon, we propose to guide the singular value distribution of the contextualized word embedding throughout the training. As a result, we can achieve a trade-off between modeling contextual information, that tends to make word representations anisotropic, and the expressive power of word representations, that tends to be isotropic.\n\n\n# 4.1 SVD REPARAMETERIZATION\n\n\nWe propose to apply singular value decomposition (SVD) based reparameterization to the embedding matrix  $\\mathbf{W}$ , i.e.,  $\\mathbf{W} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ , where  $\\mathbf{U} \\in \\mathbb{R}^{N \\times d}$ ,  $\\mathbf{V} \\in \\mathbb{R}^{d \\times d}$  are column orthonormal matrices, and  $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}$  is a diagonal matrix with  $\\Sigma_{kk} = \\sigma_k$  being the  $k$ -th largest singular value of  $\\mathbf{W}$ . Note that SVD reparameterization is standard and has been widely used in the literature such as model compression (Chen et al., 2018), training DNNs (Zhang et al., 2018), and analyzing word embeddings (Arora et al., 2016c). Given the SVD reparameterization, we can control the singular values of the embedding matrix  $\\mathbf{W}$  by constraining the matrix  $\\mathbf{E}$ , and the conditional distribution in (3.2) can be rewritten as follows:\n\n\n$$\n\n$$\n\\mathbb {P} \\left(\\mathbf {Y} _ {t} = \\mathbf {y} _ {t} \\mid \\mathbf {y} _ {<   t}\\right) = \\frac {\\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\left(\\mathbf {U} \\boldsymbol {\\Sigma} \\mathbf {V} ^ {\\top}\\right) _ {\\mathcal {I} \\left(\\mathbf {y} _ {t}\\right) *}\\right)}{\\sum_ {i = 1} ^ {N} \\exp \\left(\\mathbf {h} _ {t} ^ {\\top} \\left(\\mathbf {U} \\boldsymbol {\\Sigma} \\mathbf {V} ^ {\\top}\\right) _ {i *}\\right)} \\text {s u b j e c t t o} \\mathbf {U} ^ {\\top} \\mathbf {U} = \\mathbf {I}, \\mathbf {V} ^ {\\top} \\mathbf {V} = \\mathbf {I}, \\boldsymbol {\\Sigma} \\in \\mathcal {P}, \\tag {4.1}\n$$\n\n$$\n\n\nwhere  $\\mathcal{P}$  is a feasible set to represent the singular value distribution of the embedding matrix  $\\mathbf{W}$ .\n\n\nTo ensure the orthogonal constraints in (4.1), we propose to use the orthogonal regularization for  $\\mathbf{U},\\mathbf{V}$  during the training process. In particular, we use the following regularization, which is a linear combination of Frobenius norm and spectral norm errors:  $\\lambda_1\\| \\mathbf{U}^\\top \\mathbf{U} - \\mathbf{I}\\| _F^2 +\\lambda_2\\| \\mathbf{V}^\\top \\mathbf{V} - \\mathbf{I}\\| _F^2 +\\lambda_3\\| \\mathbf{U}^\\top \\mathbf{U} - \\mathbf{I}\\| _2^2 +\\lambda_4\\| \\mathbf{V}^\\top \\mathbf{V} - \\mathbf{I}\\| _2^2$  where  $\\{\\lambda_i\\}_{i = 1}^4$  are positive regularization parameters.\n\n\n# 4.2 SPECTRUM CONTROL\n\n\nRecall the SVD of  $\\mathbf{W} = \\mathbf{U}\\pmb{\\Sigma}\\mathbf{V}^{\\top}$ , and we consider the following two types of the singular value distribution for  $\\pmb{\\Sigma}$ , which is inspired by the eigenvalue distribution of kernel methods (Wei et al., 2017; Pacchiano et al., 2019):\n\n- - Exponential decay: we say the singular values  $\\{\\sigma_k\\}_{k=1}^d$  of  $\\mathbf{W}$  satisfy the exponential decay if  $\\mathbf{W} \\in \\mathcal{P}_e(\\gamma) = \\{\\mathbf{W} \\in \\mathbb{R}^{N \\times d} \\mid \\sigma_k \\leq c_1 \\exp(-c_2 k^\\gamma), k = 1, \\dots, d, \\gamma > 0, c_1, c_2 > 0$  are universal constants\\}.\n\n- - Polynomial decay: we say the singular values  $\\{\\sigma_k\\}_{k=1}^d$  of  $\\mathbf{W}$  satisfy the polynomial decay if  $\\mathbf{W} \\in \\mathcal{P}_p(\\gamma) = \\{\\mathbf{W} \\in \\mathbb{R}^{N \\times d} \\mid \\sigma_k \\leq c_1 k^{-\\gamma}, k = 1, \\dots, d, \\gamma > 0, c_1 > 0$  is a universal constant\\}.\n\n\nFor  $\\mathcal{P}_e(\\gamma)$  and  $\\mathcal{P}_p(\\gamma)$ , the parameter  $\\gamma$  controls the rate of singular value decay: the larger  $\\gamma$  is, the faster singular value decay will be. To ensure the learned word embedding matrix  $\\mathbf{W}$  have the desired singular value distributions, we propose to add the following regularizations to\n\n\nour training objective:  $\\mathcal{R}_e(\\Sigma) = \\lambda_e\\sum_{k = 1}^d\\left(\\sigma_k - c_1\\exp (-c_2k^\\gamma)\\right)^2$  for exponential decay and  $\\mathcal{R}_p(\\Sigma) = \\lambda_p\\sum_{k = 1}^d\\left(\\sigma_k - c_1k^{-\\gamma}\\right)^2$  for polynomial decay, where  $\\lambda_{e},\\lambda_{p}$  are positive regularization parameters. Although the concept of \"spectrum control\" was previously used in Jiang et al. (2019) to improve the training of GANs, our spectrum control method is trying to solve a totally different problem, i.e., neural language generation, and its motivation is coming from a very different perspective, i.e., the representation degeneration of the word representations. In addition, our method of controlling the singular value with prior distributions is significantly different from the penalty function used in their method. Finally, our method is essential to improve the performance of the neural language generation, while the penalty function proposed in Jiang et al. (2019) can deteriorate the training of neural language models, as we illustrated in Appendix C.\n\n\n# 4.3 THEORETICAL ANALYSIS\n\n\nIn this subsection, we show some theoretical insights on why our proposed method can improve the performance of the NLG model. In particular, we follow the similar setup as considered in (Gao et al., 2019b), i.e., focusing on the optimization of the embedding matrix  $\\mathbf{W} \\in \\mathbb{R}^{N \\times d}$  and assume all the other parameters are fixed and well-optimized. In practice, we use our method to train the models from scratch. Therefore, according to the output layer in (3.2), we consider the following empirical risk minimization problem: given a training dataset  $S = \\{(\\mathbf{h}_i, y_i)\\}_{i=1}^n$  with each example drawn i.i.d. from some unknown but fixed distribution  $\\mathcal{D}$ , and  $\\mathbf{h}_i \\in \\mathbb{R}^d$  as a hidden state,  $y_i \\in \\{1, \\dots, N\\}$  as its associated label, our goal is to minimize the training loss as follows:\n\n\n$$\n\n$$\n\\min  _ {\\mathbf {W} \\in \\mathbb {R} ^ {N \\times d}} L _ {S} (\\mathbf {W}) = \\frac {1}{n} \\sum_ {i = 1} ^ {n} \\ell \\left(\\mathbf {h} _ {i} ^ {\\top} \\mathbf {W}, y _ {i}\\right), \\tag {4.2}\n$$\n\n$$\n\n\nwhere  $\\ell(\\mathbf{h}_i^\\top \\mathbf{W}, y_i)$  is the cross-entropy loss with respect to  $\\mathbf{h}_i^\\top \\mathbf{W}$  and  $y_i$ . The cross-entropy loss defined above is widely used to train NLG models, and is also used to compute the perplexity of the trained model, which is the benchmark criterion to evaluate the performance of language models.\n\n\nIn addition, we define the expected loss as follows  $L_{\\mathcal{D}}(\\mathbf{W}) = \\mathbb{E}[\\ell (\\mathbf{h}^{\\top}\\mathbf{W},y)]$ , where the expectation is taken over the distribution  $\\mathcal{D}$  of the training data.\n\n\nLet  $\\widehat{\\mathbf{W}} = \\arg \\min_{\\mathbf{W} \\in \\mathcal{P}(\\gamma)} L_S(\\mathbf{W})$ , where  $\\mathcal{P}(\\gamma) = \\mathcal{P}_e(\\gamma)$  for exponential decay and  $\\mathcal{P}(\\gamma) = \\mathcal{P}_p(\\gamma)$  for polynomial decay. We assume the right singular vector matrix satisfies  $\\| \\mathbf{V} \\|_1 \\leq V$  for all  $\\mathbf{W} \\in \\mathcal{P}(\\gamma)$ . Now, we are ready to provide the main theory of our method (The proof can be found in Appendix A).\n\n\nTheorem 4.1. Under previously stated conditions, suppose that  $|\\ell(\\cdot)| \\leq B$ ,  $\\ell$  is  $G$ -Lipschitz continuous, and  $\\| \\mathbf{h}_i \\|_{\\infty} \\leq H$  for all  $i = 1, \\ldots, n$ . If we choose  $\\gamma > 1/2$ , then with probability at least  $1 - \\delta$ , we have\n\n\n$$\n\n$$\nL _ {\\mathcal {D}} (\\widehat {\\mathbf {W}}) \\leq \\min  _ {\\mathbf {W} \\in \\mathcal {P} (\\gamma)} L _ {S} (\\mathbf {W}) + \\frac {C _ {1} A N \\left(\\sqrt {\\sum_ {j = 1} ^ {m - 1} \\sigma_ {j} ^ {2}} + \\sqrt {m ^ {1 - 2 \\gamma} / (2 \\gamma - 1)}\\right) + C _ {2} B \\sqrt {\\log (1 / \\delta)}}{\\sqrt {n}}, \\tag {4.3}\n$$\n\n$$\n\n\nwhere  $C_1, C_2$  are absolute constants,  $m \\in [2, d]$ ,  $A = GVH\\sqrt{\\log d}$ .\n\n\nRemark 4.2. According to Theorem 4.1, the expected loss of the learned embedding matrix  $\\widehat{\\mathbf{W}}$  consists of two terms. The first term represents the training loss, the second term is the generalization error gap. Specifically, the smaller the  $\\gamma$ , the larger the feasible set  $\\mathcal{P}(\\gamma)$ , thus the smaller the training loss  $\\min_{\\mathbf{W} \\in \\mathcal{P}(\\gamma)} L_S(\\mathbf{W})$ . On the other hand, the larger the  $\\gamma$ , the faster the singular value decays, and the smaller the generalization error gap. Therefore, our generalization error bound demonstrates an appealing property of our proposed method: by directly controlling the singular value distribution of the learned word embedding, we are able to achieve a trade-off between the training loss and generalization error.\n\n\nRemark 4.3. For the generalization error bound in (4.3), penalizing the largest singular value could reduce this upper bound. It validates the method proposed in Gao et al. (2019b), which implicitly penalizes the largest singular value (c.f. Section 5 in Gao et al. (2019b)). Compared with their method, the error term  $\\widetilde{O}\\left(N\\sqrt{m^{1 - 2\\gamma} / (2\\gamma - 1)} /\\sqrt{n}\\right)$  suggests that by explicitly manipulating the singular value distribution, our method has a better control of the tail sum of the singular values.\n\n\n# 5 EXPERIMENTS\n\n\nWe demonstrate the effectiveness of our proposed spectrum control algorithm on two tasks: language modeling and machine translation. We compare our results with the state-of-the-art models. In our experiments, we try both exponential and polynomial singular value decays, and present the one with the better result. The performances of exponential decay and polynomial decay are very close (See Appendix E). In practice we found that for large scale dataset its better to use polynomial decay and for small scale dataset its better to use exponential decay.\n\n\n# 5.1 LANGUAGE MODELING\n\n\nDatasets We consider two benchmark datasets for language modeling: WikiText-2 and WikiText-103, which consist of pre-processed Wikipedia articles and were introduced by Merity et al. (2018a). WikiText-2 is a small dataset with around 2 million words and 30K vocabulary size, while WikiText-103 is a significantly large dataset with around 103 million words and 260K vocabulary size.\n\n\nModel Configuration On the small WikiText-2 dataset, we implement our method based on the state-of-the-art AWD-LSTM model (Merit et al., 2018a). It is a 3-layer LSTM model with 1150 dimensional hidden states and 400 dimensional embeddings. We also follow the same regularization and optimization procedures introduced in (Merit et al., 2018a). The implementation of our method is based on the open-source code<sup>3</sup> for AWD-LSTM. On the large WikiText-103 dataset, we implement our method based on the state-of-the-art Transformer-XL based models (Dai et al., 2019). We follow the same settings reported in (Dai et al., 2019), and our implementation is based on the official code<sup>4</sup> for Transformer-XL. To evaluate the performance of our method more thoroughly, we consider two Transformer-XL models with different number of layers. The first is the standard Transformer-XL model with 16 layers used in (Dai et al., 2019). For the second, we consider a smaller Transformer-XL model with just 4 layers and other configurations unchanged.\n\n\nParameters For the parameters  $\\{\\lambda_i\\}_{i=1}^4$  of the orthogonal regularizations, we tune them by grid search over  $\\{0.01, 0.1, 1, 10\\}$ . For the parameters  $\\lambda_e, \\lambda_p$  of the spectrum control, we tune them over the grid  $\\{0.1, 1, 10, 100\\}$ . We try different singular value distributions, and the best distributions for AWD-LSTM and Transformer-XL are exponential and polynomial, respectively.\n\n\nResults of the LSTM Model on WikiText-2 We first present the results of language modeling on the small dataset WikiText-2 using LSTM models. In Table 1 we compare the validation/test perplexity<sup>5</sup> of the baseline AWD-LSTM model (Merit et al., 2018a), the cosine similarity regularization (MLE-CosReg) model (Gao et al., 2019b), and the models trained using our method under three different settings (Merit et al., 2018a): without finetune, with finetune and with further continuous cache pointer. Compared with the baselines, our method achieves  $2.3 / 2.9 / 2.3$  test perplexity reduction under all three settings; compared with the MLE-CosReg method, our method achieves  $1.5 / 1.2 / 0.3$  test perplexity reduction under all three settings.\n\n\nResults of the Transformer-XL Model on WikiText-103 We next show the results of language modeling on the large dataset WikiText-103 using Transformer-XL models. Table 2 compares the validation/test perplexity of Transformer-XL based models (Dai et al., 2019) and the models trained by our method on WikiText-103 dataset. The results demonstrate that our method consistently improves upon the small Transformer-XL model (0.9 test perplexity reduction) and the standard Transformer-XL model (0.8 test perplexity reduction).\n\n\nAnalysis We study the output embedding matrix of Transformer-XL trained on WikiText-103 using our method. In particular, we want to evaluate the isotropy of the learned word representations. We consider the partition function  $Z(\\mathbf{a}) = \\sum_{i=1}^{N} \\exp(\\langle \\mathbf{w}_i, \\mathbf{a} \\rangle)$  introduced in (Arora et al., 2016b), where  $\\mathbf{w}_i$  is the  $i$ -th row of the embedding matrix  $\\mathbf{W} \\in \\mathbb{R}^{N \\times d}$  and  $\\mathbf{a} \\in S^{d-1}$  is a unit vector. According to Lemma 2.1 in (Arora et al., 2016b), if the word representation vectors are isotropic,  $Z(\\mathbf{a})$  is close to some constant with high probability for all unit vectors. Thus to empirically measure\n\n\nTable 1: Comparison of different methods in terms of perplexity on WikiText-2 dataset for the task of language modeling.\n\n\n<table><tr><td>Method</td><td>Parameters</td><td>Validation</td><td>Test</td></tr><tr><td colspan=\"4\">Existing results</td></tr><tr><td>Variational LSTM (Inan et al., 2017)</td><td>51M</td><td>91.5</td><td>87.0</td></tr><tr><td>2-layer skip connection LSTM (Mandt et al., 2017)</td><td>24M</td><td>69.1</td><td>65.9</td></tr><tr><td colspan=\"4\">w/o finetune</td></tr><tr><td>AWD-LSTM (Merit et al., 2018a)</td><td>33M</td><td>69.1</td><td>66.0</td></tr><tr><td>MLE-CosReg (Gao et al., 2019b)</td><td>33M</td><td>68.2</td><td>65.2</td></tr><tr><td>Ours</td><td>33M</td><td>66.3</td><td>63.7</td></tr><tr><td colspan=\"4\">+ finetune</td></tr><tr><td>AWD-LSTM (Merit et al., 2018a)</td><td>33M</td><td>68.6</td><td>65.8</td></tr><tr><td>MLE-CosReg (Gao et al., 2019b)</td><td>33M</td><td>67.1</td><td>64.1</td></tr><tr><td>Ours</td><td>33M</td><td>65.3</td><td>62.9</td></tr><tr><td colspan=\"4\">+ continuous cache pointer</td></tr><tr><td>AWD-LSTM (Merit et al., 2018a)</td><td>33M</td><td>53.8</td><td>52.0</td></tr><tr><td>MLE-CosReg (Gao et al., 2019b)</td><td>33M</td><td>51.7</td><td>50.0</td></tr><tr><td>Ours</td><td>33M</td><td>51.1</td><td>49.7</td></tr></table>\n\n\nTable 2: Comparison of different methods in terms of perplexity on WikiText-103 dataset for the task of language modeling.\n\n\n<table><tr><td>Method</td><td>Parameters</td><td>Validation</td><td>Test</td></tr><tr><td colspan=\"4\">Existing results</td></tr><tr><td>4 layer QRNN (Merit et al., 2018b)</td><td>151M</td><td>32.0</td><td>33.0</td></tr><tr><td>Hebbian + Cache (Rae et al., 2018)</td><td>-</td><td>29.7</td><td>29.9</td></tr><tr><td>Small Transformer-XL (Dai et al., 2019)</td><td>120M</td><td>29.6</td><td>30.4</td></tr><tr><td>Ours</td><td>120M</td><td>29.0</td><td>29.5</td></tr><tr><td>Standard Transformer-XL (Dai et al., 2019)</td><td>151M</td><td>23.1</td><td>24.0</td></tr><tr><td>Ours</td><td>151M</td><td>22.9</td><td>23.2</td></tr></table>\n\n\nthe isotropy of the learned word representations, we consider two criteria based on  $Z(\\mathbf{a})$ :\n\n\n$$\n\n$$\nI _ {1} (\\mathbf {W}) = \\frac {\\operatorname* {m i n} _ {\\mathbf {a} \\in \\mathcal {E}} Z (\\mathbf {a})}{\\operatorname* {m a x} _ {\\mathbf {a} \\in \\mathcal {E}} Z (\\mathbf {a})} \\quad \\text {a n d} \\quad I _ {2} (\\mathbf {W}) = \\sqrt {\\frac {\\sum_ {\\mathbf {a} \\in \\mathcal {E}} (Z (\\mathbf {a}) - \\bar {Z} (\\mathbf {a})) ^ {2}}{| \\mathcal {E} | \\bar {Z} (\\mathbf {a}) ^ {2}}},\n$$\n\n$$\n\n\nwhere  $\\mathcal{E}$  is the set of eigenvectors of  $\\mathbf{W}^{\\top}\\mathbf{W}$ , as suggested by Mu & Viswanath (2018). We also propose to check sampled standard deviation measure  $I_{2}(\\mathbf{W})$  (normalized by its average, i.e.,  $\\bar{Z}(\\mathbf{a})$ ). We have  $I_{1}(\\mathbf{W}) \\in [0,1]$  and  $I_{2}(\\mathbf{W}) \\geq 0$ . Larger  $I_{1}(\\mathbf{W})$  and smaller  $I_{2}(\\mathbf{W})$  indicate more isotropic for word embeddings. We uniformly sample 40K words from the vocabulary (around 260K) of WikiText-103 to compute these two criteria. The left half of Table 3 summarizes the values of  $I_{1}(\\mathbf{W})$  and  $I_{2}(\\mathbf{W})$ , which are averaged over 10 runs, for the baseline method and our method. We can see from the results that our method significantly improves the isotropy of the learned word representations in terms of both criteria.\n\n\nTable 3: Comparison of different methods in terms of isotropy for the tasks of language modeling and machine translation. (For perfect isotropy,  $I_{1}(\\mathbf{W}) = 1$ ,  $I_{2}(\\mathbf{W}) = 0$ .)\n\n\n<table><tr><td colspan=\"3\">Language Modeling</td><td colspan=\"3\">Machine Translation</td></tr><tr><td>Method</td><td>I1(W)</td><td>I2(W)</td><td>Method</td><td>I1(W)</td><td>I2(W)</td></tr><tr><td>Standard Transformer-XL</td><td>0.24</td><td>0.037</td><td>Transformer-Base</td><td>0.31</td><td>0.031</td></tr><tr><td>Ours</td><td>0.63</td><td>0.022</td><td>Ours</td><td>0.88</td><td>0.005</td></tr></table>\n\n\nTable 4: Comparison of different methods in terms of BLEU scores on the task of De\u2192En machine translation, trained on IWSLT 2014 dataset.\n\n\n<table><tr><td rowspan=\"2\">IWSLT 2014 De\u2192En</td><td colspan=\"4\">Method</td></tr><tr><td>Adversarial (Wang et al., 2019a)</td><td>Dual-learning (Wang et al., 2019b)</td><td>Transformer-Base (Wang et al., 2019b)</td><td>Ours</td></tr><tr><td>BLEU</td><td>35.18</td><td>35.44</td><td>34.01</td><td>35.50</td></tr></table>\n\n\n# 5.2 MACHINE TRANSLATION\n\n\nWe also apply our spectrum control method to machine translation tasks. Given a source sentence  $s$ , the decoder of an neural machine translation (NMT) model is to predict the next word in the target sentence  $t$  and the previous decoded words in  $t$ . In the following we use the state-of-the-art Transformer-based NMT model as our baseline.\n\n\nDatasets We compare various NMT models on the IWSLT 2014 German\u2192English (De-En) dataset (Cettolo et al., 2014). We follow the same setup as in (Gehring et al., 2017). More specifically, we have 160K sentence pairs as the training data, 7K sentence pairs as the validation data, and we combine tst2010, tst2011, tst2012, dev2010 and dev2012 datasets to form our test data.\n\n\nModel Configuration We implement our method on top of Transformer model (Vaswani et al., 2017). In particular, we consider the Transformer-Base architecture (Vaswani et al., 2017), which has a 6-layer encoder and 6-layer decoder with 512 dimensional hidden states and embeddings, except that we choose the dimension of the inner feed-forward layer as 1024 instead of 2048 and the number of attention heads is set to be 4 rather than 8. Our implementation is based on the open-sourced code provided by Ott et al. (2018). We follow the same procedures as in the language modeling task to choose the regularization parameters. In addition, the best results for this task are from the models where the singular values are controlled by exponential distribution.\n\n\nResults Comparisons of different methods in terms of BLEU scores are summarized in Table 4. Compared with the baseline models, our method improves the BLEU score from 34.01 to 35.50 on the German\u2192English task, close to 1.5 gain on BLEU score; our method is also better than 35.18 reported in (Wang et al., 2019a), and 35.44 reported in (Wang et al., 2019b).\n\n\nAnalysis We also study the learned word embedding matrix of the Transformer trained on IWSLT 2014 De-En using our method in terms of isotropy. The values of  $I_{1}(\\mathbf{W})$  and  $I_{2}(\\mathbf{W})$  are reported in the right half of Table 3, we compute these two values based on all the tokens. It shows that the isotropy of the learned word representations using our method increases significantly in terms of these criteria, from very anisotropic to nearly isotropic. We also demonstrate the projected word embedding matrices and the singular value distributions of different methods in Figure 3. The plots show that the learned word representations from our method are distributed isotropically in the space, which is in contrast to the narrow cone distribution from the baseline method.\n\n\n![(a) Transformer for NMT](images/a3a3e55cd643e2fae8cd9afe73fdf5428e123204a82941a8f82c90127deb9751.jpg)\n\n\n![(b) Transformer for NMT+SC](images/cb27d7792c80b28b706465fbd93e32f3a9c044bdbe576634467d8646fafb3202.jpg)\n\n\n![(c) Singular Value Decay Figure 3: (a) Word embedding for the vanilla Transformer, which has a narrow cone distribution; (b) Word embedding for Transformer using spectrum control, which has a uniform distribution; (c) Normalized singular value for different methods, which shows a slow decay of our method.](images/818c0e945de1b87307fd1f72b37ccc2013c067d4c85430209bc7cff16049e95d.jpg)\n\n\n# 6 CONCLUSIONS AND FUTURE WORK\n\n\nIn this paper, we tackle the degeneration problem that occurs at the output embeddings in the softmax layer used in neural language generation models. We develop a novel spectrum control method to explicitly guide the spectra training of the output embedding matrix with some slow-decaying singular value prior distributions. Our proposed method is shown to alleviate the degeneration problem and improve isotropy of the learned contextualized word representations. Thorough experimental results demonstrate the advantage of our method over the state-of-the-art neural models for language model and machine translation. Since our work is orthogonal to Wang et al. (2019a), it would be interesting to combine the adversarial softmax training with our spectrum control method, and investigate its performance. For the future work, we would also like to investigate how the frequency-agnostic word representations in (Gong et al., 2018) relate to the single value distribution of the output embedding matrix.\n\n\n# REFERENCES\n\n- Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. NeurIPS, 2019.\n\n- Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach to pmi-based word embeddings. TACL, 4:385-399, 2016a.\n\n- Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach to pmi-based word embeddings. TACL, 4:385-399, 2016b.\n\n- Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. 2016c.\n\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICML, 2015.\n\n- Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. JMLR, 2002.\n\n- Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. In Proceedings of the Twentieth Conference on Computational Natural Language Learning, 2016.\n\n- Mauro Cettolo, Jan Niehues, Sebastian St\u00fcker, Luisa Bentivogli, and Marcello Federico. Report on the 11th iwslt evaluation campaign, iwslt 2014. In SLT, 2014.\n\n- Patrick Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-Jui Hsieh. Groupreduce: Block-wise low-rank approximation for neural language model shrinking. In Advances in Neural Information Processing Systems, pp. 10988-10998, 2018.\n\n- Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. ACL, 2019.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In *NAACL*, 2019.\n\n- Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. EMNLP, 2019.\n\n- Octavian-Eugen Ganea, Sylvain Gelly, Gary B\u00e9cigneul, and Aliaksei Severyn. Breaking the softmax bottleneck via learnable monotonic pointwise non-linearities. ICML, 2019.\n\n- Jianfeng Gao, Michel Galley, Lihong Li, et al. Neural approaches to conversational ai. Foundations and Trends\u00ae in Information Retrieval, 13(2-3):127-298, 2019a.\n\n- Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tieyan Liu. Representation degeneration problem in training natural language generation models. In ICLR, 2019b.\n\n- Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In ICML, pp. 1243-1252, 2017.\n\n- Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Frage: frequency-agnostic word representation. In NIPS, 2018.\n\n- A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, 2013.\n\n- Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.\n\n- Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. In ICLR, 2017.\n\n- Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo Zhao. On computation and generalization of generative adversarial networks under spectrum control. In ICLR, 2019.\n\n- Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n\n- Sekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka, and Shuichi Adachi. Sigsoftmax: Reanalysis of the softmax bottleneck. In NIPS, 2018.\n\n- Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional transformer language model for controllable generation. arXiv:1909.05858, 2019.\n\n- Percy Liang. Cs229t/stat231: Statistical learning theory (winter 2016), 2014.\n\n- Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian inference. JMLR, 2017.\n\n- Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. In ICLR, 2018a.\n\n- Stephen Merity, Nitish Shirish Keskar, and Richard Socher. An analysis of neural language modeling at multiple scales. arXiv preprint arXiv:1803.08240, 2018b.\n\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\n- Jiaqi Mu and Pramod Viswanath. All-but-the-top: Simple and effective postprocessing for word representations. In ICLR, 2018.\n\n- Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation, 2018.\n\n- Aldo Pacchiano, Niladri S Chatterji, and Peter L Bartlett. Online learning with kernel losses. ICML, 2019.\n\n- Nikolaos Pappas and James Henderson. Deep residual output layers for neural language generation. In ICML, 2019.\n\n- Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In EMNLP, 2014.\n\n- Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In *NAACL*, 2018.\n\n- Ofir Press and Lior Wolf. Using the output embedding to improve language models. In EACL, 2017.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\n\n- Jack Rae, Chris Dyer, Peter Dayan, and Timothy Lillicrap. Fast parametric learning with activation memorization. In ICML, 2018.\n\n- Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric P. Xing, and Zhiting Hu. Target-guided open-domain conversation. In ACL, 2019.\n\n- Shubham Toshniwal, Anjuli Kannan, Chung-Cheng Chiu, Yonghui Wu, Tara Sainath, and Karen Livescu. A comparison of techniques for language model integration in encoder-decoder speech recognition. In IEEE Workshop on Spoken Language Technology, 2018.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n\n- Dilin Wang, Chengyue Gong, and Qiang Liu. Improving neural language modeling via adversarial training. In ICML, 2019a.\n\n- Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multiagent dual learning. In ICLR, 2019b.\n\n- Yuting Wei, Fanny Yang, and Martin J Wainwright. Early stopping for kernel boosting algorithms: A general analysis with localized complexities. In NIPS, 2017.\n\n- Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. Breaking the softmax bottleneck: A high-rank RNN language model. In *ICLR*, 2018.\n\n- Jiong Zhang, Qi Lei, and Inderjit Dhillon. Stabilizing gradients for deep neural networks via efficient svd parameterization. In International Conference on Machine Learning, pp. 5801-5809, 2018.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636549.1557243", "msecs": "155.0", "relativeCreated": "9848.864793777466", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Multi-Label Knowledge Distillation\n\n# Abstract\nExisting knowledge distillation methods typically work by enforcing the consistency of output logits or intermediate feature maps between the teacher network and student network. Unfortunately, these methods can hardly be extended to the multi-label learning scenario. Because each instance is associated with multiple semantic labels, neither the prediction logits nor the feature maps obtained from the whole example can accurately transfer knowledge for each label. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by label decoupling with the one-versus-all reduction strategy; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the proposed method can avoid knowledge counteraction among labels, and achieve superior performance against diverse comparing methods.\n\n\n# 1 INTRODUCTION\n\n\nDespite the remarkable success in training deep neural networks (DNNs) (Krizhevsky et al., 2012), it is hard to deploy these large neural networks on lightweight terminals, e.g., mobile phones, under the constraint of computational resource or requirement of short inference time. To mitigate this issue, knowledge distillation (Hinton et al., 2015) aims to improve the performance of a small network (also known as the student) by requiring the knowledge from a large network (also known as the teacher) to guide the training of the student network. Existing knowledge distillation methods can be roughly divided into two categories: logits-based methods and feature-based methods. The former minimizes the difference between logits of teacher model and student model (Hinton et al., 2015; Zhao et al., 2022), while the latter distills knowledge from feature maps of intermediate layers (Park et al., 2019; Tian et al., 2019; Chen et al., 2021).\n\n\nTypical knowledge distillation methods focus on the multi-class classification task, where each instance is associated with only one class label. However, in many real-world scenarios, an instance inherently contains complex semantics and can be simultaneously assigned with multiple class labels. For example, an image of street scene may be annotated with labels building, car and person. To learn the complex object-label mapping, there is always necessity of training large models to obtain desirable performance in multi-label classification. Unfortunately, due to computational resource constraints, it cannot be allowed to adopt large neural networks in many practical applications, leading to noticeable decrease in model performance (Gou et al., 2021). To alleviate the performance degradation, it is necessary to design specific knowledge distillation methods for multi-label learning. We formalize such a problem as a new learning framework called Multi-Label Knowledge Distillation (MLKD).\n\n\nAlthough knowledge distillation has been proven to be effective for improving the performance of the student network in single-label classification, it is still a challenging problem to directly extend existing KD methods to solve MLKD problems. Specifically, logits-based methods often obtain the predicted probabilities based on the softmax function, which is unavailable for MLKD, since the sum of predicted probabilities may not equal to one in Multi-Label Learning (MLL). Feature-based methods often perform knowledge distillation based on feature maps of the whole image with multiple semantics, which makes the model focus on the major objects while neglect the minor objects. This would lead the model to obtain sub-optimal even undesirable distillation performance. Figure 1 provides empirical validations for these observations.\n\n\n![(a) Models with the same architecture](images/f19190e708504d97ed3128099f7c12e9c7ec22ab86ab96476cbbcf643736c555.jpg)\n\n\n![(b) Models with different architectures Figure 1: Comparison results between our proposed L2D method and conventional KD methods on MS-COCO. We compare our method with the following three baselines: 1) Vanilla: student trained without distillation; 2) Softmax (Hinton et al., 2015): a representative logits-based method by measuring KL divergence on softmax score after logits; 3) ReviewKD (Chen et al., 2021): a feature-based method that achieves sota performance. The red dashed lines mark the performance of teachers. Conventional KD methods achieve unfavorable performance similar to the student, while our method significantly outperforms these methods and achieves comparable performance to the teacher. Details about the implementation of this experiment can be found in Appendix A.](images/8eea145e1dc78c1c6a262d374b1f7762f2d7f4f3908017810b858a491913b47f.jpg)\n\n\nIn this paper, to perform multi-label knowledge distillation, we propose a new method consisting of multi-label logits distillation and label-wise embedding distillation (L2D for short). Specifically, to exploit informative semantic knowledge compressed in the logits, L2D employs the one-versus-all reduction strategy to obtain a series of binary classification problems and perform logits distillation for each one. To enhance the distinctiveness of learned feature representations, L2D encourages the student model to maintain a consistent structure of intra-class and intra-instance (inter-class) label-wise embeddings with the teacher model. By leveraging the structural information of the teacher model, these two structural consistencies respectively enhance the compactness of intra-class embeddings and dispersion of inter-class embeddings for the student model.\n\n\nOur main contributions can be summarized as follows:\n\n- - A general learning framework called MLKD is proposed. To our best knowledge, the framework is the first study specially designed for knowledge distillation in the multi-label learning scenario.\n\n- - A new approach called L2D is proposed. It performs multi-label logits distillation and label-wise embedding distillation simultaneously. The former provides informative semantic knowledge while the latter encourages the student model to learn more distinctive feature representations.\n\n- - Extensive experimental results on benchmark datasets demonstrate the effectiveness of our proposed method.\n\n\n# 2 RELATED WORK\n\n\nThe concept of knowledge distillation (KD) proposed by Hinton et al. (2015) defines a learning framework that transfers knowledge from a large teacher network to a small student network. Existing works can be roughly divided into two groups, i.e., logits-based methods and feature-based methods. Logits-based methods mainly focus on designing effective distillation losses to distill knowledge from logits and softmax scores after logits. DML (Zhang et al., 2018) introduces a mutual learning method to train both teachers and students simultaneously. TAKD (Mirzadeh et al., 2020) proposes a new architecture called \"teacher assistant\", which is an intermediate-sized network bridging the gap between teachers and students. Besides, a recent study (Zhao et al., 2022) proposes a novel logits-based method to reformulate the classical KD loss into two parts and achieves the state-of-the-art performance by adjusting weights for these two parts. Some other methods focus on distilling knowledge from intermediate feature layers. FitNet (Romero et al., 2014) is the first approach to distill knowledge from intermediate features by measuring the distance between feature maps. Attention transfer (Zagoruyko & Komodakis, 2016a) achieves better performance than FitNet\n\n\nby distilling knowledge from the attention maps. PKT (Passalis & Tefas, 2018) measures the KL divergence between features by treating them as probability distributions. RKD (Park et al., 2019) utilizes the relations among instances to guide the training process of the student model. CRD (Tian et al., 2019) incorporates contrastive learning into knowledge distillation. ReviewKD (Chen et al., 2021) proposes a review mechanism which uses multiple layers in the teacher to supervise one layer in the student. ITRD (Miles et al., 2021) aims to maximize the correlation and mutual information between the students' and teachers' representations.\n\n\nMulti-label learning has increasingly attracted a lot of interest recently. Existing solutions for solving MLL problems can be categorized into three directions. The first type attempts to design novel loss functions for tackling the intrinsic positive-negative imbalance issue in multi-label classification tasks. For example, ASL (Ridnik et al., 2021a) uses different weights to re-weight positive and negative examples for the balanced training. The second type focuses on modeling the label correlations, which provides prior knowledge for multi-label classification. Among them, MLGCN (Chen et al., 2019b) is a representative method that employs a graph convolutional network to model correlation matrix. CADM (Chen et al., 2019a) constructs a similar graph based on class-aware maps. To handle the multiple semantic objects contained in an image, the last type of methods aims to locate areas of interest related to semantic labels by using attention techniques. Among them, C-Tran (Lanchantin et al., 2021) first utilizes the transformer to retrieve embeddings from visual features for each label. Query2Label (Liu et al., 2021a) uses several stacked transformer encoders to identify interesting areas. ML-Decoder (Ridnik et al., 2021b) simplifies transformer encoders in Query2Label. ADDS (Xu et al., 2022b) introduces encoders from CLIP (Radford et al., 2021) in order to get better textual and visual embedding inputs to the classification head. In addition, ADDS adds a multi-head cross-attention layer and a skipping connection from the query input to the query output based on ML-Decoder.\n\n\nSeveral previous studies have applied KD techniques to improve the performance of MLL. For example, Liu et al. (2018) and Xu et al. (2022a) simply minimized mean squared error (MSE) loss between teacher logits and student logits. Song et al. (2021) designed a partial softmax function by combining a positive label with all other negative labels. Then, the conventional KD loss can be computed for each positive label. The main difference between our work and previous works is that we aim to improve the distillation performance instead of the performance of MLL. This can be reflected in experiments in Section 4 and Appendix B, where the proposed method mainly compares with KD methods while the previous method mainly compares with MLL methods.\n\n\n# 3 THE PROPOSED APPROACH\n\n\nLet  $\\pmb{x} \\in \\mathcal{X}$  be an instance and  $\\pmb{y} \\in \\mathcal{Y}$  be its corresponding label vector, where  $\\mathcal{X} \\subset \\mathbb{R}^d$  is the input space with  $d$  dimensions and  $\\mathcal{Y} \\subset \\{0,1\\}^q$  is the target space with  $q$  class labels. We further use  $y_j$  to denote the  $j$ -th component of  $\\pmb{y}$ . For a given instance  $\\pmb{x}$ ,  $y_j = 1$  indicates the  $j$ -th label is relevant to the instance;  $y_j = 0$ , otherwise. In multi-label learning, each instance may be assigned with more than one label, which means  $\\sum_{j=1}^{q} \\mathbb{I}(y_j = 1) \\geq 1$ , where  $\\mathbb{I}(\\cdot)$  is the indicator function. We also denote by  $[q]$  the integer set  $\\{1,2,\\dots,q\\}$ .\n\n\nIn this paper, we use a classification model consisting of three components, i.e., a visual backbone  $f$ , which extracts a feature map  $f(\\pmb{x})$  for the input  $\\pmb{x}$ , a label-wise embedding encoder  $g$  (Lanchantin et al., 2021; Liu et al., 2021a), which produces a label-wise embedding  $e_k = g_k(f(\\pmb{x}))$  with respect to the  $k$ -th class based on the feature map  $f(\\pmb{x})$ , and a multi-label classifier  $h$ , which predicts multi-label probabilities  $\\hat{\\pmb{y}} = [\\sigma(h_1(e_1)), \\sigma(h_2(e_2)), \\dots, \\sigma(h_q(e_q))]$ , where  $\\sigma(\\cdot)$  denotes the sigmoid function. It is noteworthy that the used model is very general, which can be built by equipping commonly used backbones, e.g., ResNet (He et al., 2016), with a label-wise embedding encoder  $g$ . For the notations mentioned above, we use the superscripts  $\\mathcal{T}$  (or  $S$ ) to denote the teacher (or student) model. For example, we use  $e_k^T$  and  $e_k^S$  to denote the label-wise embeddings for teacher and student models.\n\n\nIn multi-label learning, a popular method is to employ the one-versus-all reduction strategy to transform the original task into multiple binary problems. Among the various loss functions, the most commonly used one is the binary cross entropy (BCE) loss. Specifically, given a batch of examples\n\n\n![Figure 2: An illustration of the L2D framework. The framework simultaneously performs multi-label logits distillation and label-wise embedding distillation to improve the performance of the student model.](images/3aac4bb2349429772ac122d178545b82a5989e70f1e03e86fe96473b48c9979a.jpg)\n\n\n$\\{(\\pmb{x}_i,\\pmb{y}_i)\\}_{i = 1}^b$  and the predicted probabilities  $\\hat{\\pmb{y}}$ , the BCE loss can be defined as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {B C E}} = - \\frac {1}{b} \\sum_ {i = 1} ^ {b} \\sum_ {k = 1} ^ {q} y _ {i k} \\log \\left(\\hat {y} _ {i k}\\right) + (1 - y _ {i k}) \\log \\left(1 - \\hat {y} _ {i k}\\right). \\tag {1}\n$$\n\n$$\n\n\nFigure 2 illustrates the distillation process of the proposed L2D framework. For a batch of training examples, we feed them into the teacher/student model to obtain the label-wise embeddings and predicted probabilities. In order to train the student model, besides the BCE loss, we design the following two distillation losses: 1) multi-label logits distillation loss  $\\mathcal{L}_{\\mathrm{MLD}}$  to exploit informative semantic knowledge compressed in the logits, 2) label-wise embedding distillation loss  $\\mathcal{L}_{\\mathrm{LED}}$  to leverage structural information for enhancing the distinctiveness of learned feature representations. The overall objective function can be presented as\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {L 2 D}} = \\mathcal {L} _ {\\mathrm {B C E}} + \\lambda_ {\\mathrm {M L D}} \\mathcal {L} _ {\\mathrm {M L D}} + \\lambda_ {\\mathrm {L E D}} \\mathcal {L} _ {\\mathrm {L E D}}, \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\lambda_{\\mathrm{MLD}}$  and  $\\lambda_{\\mathrm{LED}}$  are two balancing parameters.\n\n\n# 3.1 MULTI-LABEL LOGITS DISTILLATION\n\n\nTraditional logits-based distillation normally minimizes the Kullback-Leibler (KL) divergence between the predicted probabilities, i.e., the logits after the softmax function, of teacher and student model. However, the method cannot be directly applied to the MLL scenario, since it depends on a basic assumption that the predicted probabilities of all classes should sum to one, which hardly holds for MLL examples.\n\n\nTo mitigate this issue, inspired by the idea of one-versus-all reduction, we propose a multi-label logits distillation (MLD) loss, which decomposes the original multi-label task into multiple binary classification problems and minimizes the divergence between the binary predicted probabilities of two models. Formally, the MLD loss can be formulated as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {M L D}} = \\frac {1}{b} \\sum_ {i = 1} ^ {b} \\sum_ {k = 1} ^ {q} \\mathcal {D} \\left(\\left[ \\hat {y} _ {i k} ^ {\\mathcal {T}}, 1 - \\hat {y} _ {i k} ^ {\\mathcal {T}} \\right] \\mid \\mid \\left[ \\hat {y} _ {i k} ^ {\\mathcal {S}}, 1 - \\hat {y} _ {i k} ^ {\\mathcal {S}} \\right]\\right), \\tag {3}\n$$\n\n$$\n\n\nwhere  $[\\cdot, \\cdot]$  is an operator used to concatenate two scalars into a vector, and  $\\mathcal{D}$  is a divergence function. The most common choice is the KL divergence  $\\mathcal{D}_{\\mathrm{KL}}(P\\|Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\left(\\frac{P(x)}{Q(x)}\\right)$ , where  $P$  and  $Q$  are two different probability distributions. The MLD loss aims to improve the performance of student model by sufficiently exploiting informative knowledge from logits.\n\n\n# 3.2 LABEL-WISE EMBEDDING DISTILLATION\n\n\nThe MLD loss performs distillation on the predicted probabilities that can be regarded as a high-level representation, i.e., the final outputs of model. The knowledge distilled from the teacher model\n\n\n![Figure 3: An illustration of class/instance-aware label-wise embedding distillation. Class-aware label-wise embedding distillation (CD) captures structural relations among intra-class label-wise embeddings from different examples, while instance-aware label-wise embedding distillation (ID) explores structural relations among intra-instance (inter-class) label-wise embeddings.](images/fc91b8a5684cb5236c7dad483996d6e68e9365330e16f8de842aedff6512faab.jpg)\n\n\nby only using the MLD loss would be insufficient to train a student model with desirable performance due to the limited information carried by the logits. To further strengthen the effectiveness of distillation, we design the label-wise embedding distillation (LED) loss, which aims to explore the structural knowledge from label-wise embeddings. The main idea is to capture two types of structural relations among label-wise embeddings: 1) class-aware label-wise embedding distillation (CD) loss  $\\mathcal{L}_{\\mathrm{LED - CD}}$ , which captures the structural relation between any two intra-class label-wise embeddings from different examples; 2) instance-aware label-wise embedding distillation (ID) loss  $\\mathcal{L}_{\\mathrm{LED - ID}}$ , which models the structural relation between any two inter-class label-wise embeddings from the same example. In the following content, we introduce these two distillation losses in detail.\n\n\n# 3.2.1 CLASS-AWARE LABEL-WISE EMBEDDING DISTILLATION\n\n\nClass-aware label-wise embedding distillation aims to improve the distillation performance by exploiting the structural relations among intra-class label-wise embeddings. Generally, the same semantic objects from two different images often differ from each other by their individual characteristics, such as two cars with different colors and diverse styles (see the left side in Figure 3). Since our goal is to distinguish between car and other semantic classes instead of identifying different cars, these distinctiveness would be confusing information for the corresponding classification task. Due to the powerful learning capacity, the large model is able to capture the highly abstract semantic representations for each class label by neglecting the useless individual information. From the perspective of learned feature representations, as shown in the left side of Figure 3, the teacher model tends to obtain a more compact structure of intra-class label-wise embedding, which often leads to better classification performance. By transferring the structural knowledge from the teacher model to the student model, CD encourages the student model to enhance the compactness of intra-class label-wise embeddings, which can improve its classification performance.\n\n\nFor a batch of examples, let  $\\{e_{ik}^{\\mathcal{T}}\\}_{i=1}^{b}$  and  $\\{e_{ik}^{\\mathcal{S}}\\}_{i=1}^{b}$  respectively denote the intra-class label-wise embeddings with respect to class  $k \\in [q]$  generated by the teacher and student models. Then, we can capture the structural relation between any two intra-class label-wise embeddings  $e_{ik}^{\\mathcal{T}}$  and  $e_{jk}^{\\mathcal{T}}$\n\n\nby measuring their distance in the embedding space:\n\n\n$$\n\n$$\n\\phi_ {\\mathrm {C D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {j k} ^ {\\mathcal {T}}\\right) = \\left\\{ \\begin{array}{l l} \\| \\boldsymbol {e} _ {i k} ^ {\\mathcal {T}} - \\boldsymbol {e} _ {j k} ^ {\\mathcal {T}} \\| _ {2} & y _ {i k} = 1, y _ {j k} = 1, \\\\ 0 & \\text {o t h e r w i s e .} \\end{array} \\right. \\tag {4}\n$$\n\n$$\n\n\nIt is worth to note that we only consider the structural relation between any two valid label-wise embeddings, i.e., the embeddings with respect to positive labels. Similar to Eq.(4), for any two intra-class label-wise embeddings  $e_{ik}^{S}$  and  $e_{jk}^{S}$ , we can obtain the structural relation  $\\phi_{\\mathrm{CD}}(e_{ik}^{S}, e_{jk}^{S})$  for the student model.\n\n\nBy enforcing the teacher and student structural relations to maintain the consistency for each pair of intra-class label-wise embeddings, we can achieve the class-aware structural consistency as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {L E D - C D}} = \\sum_ {k = 1} ^ {q} \\sum_ {i, j \\in [ b ]} \\ell \\left(\\phi_ {\\mathrm {C D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {j k} ^ {\\mathcal {T}}\\right), \\phi_ {\\mathrm {C D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {S}}, \\boldsymbol {e} _ {j k} ^ {\\mathcal {S}}\\right)\\right), \\tag {5}\n$$\n\n$$\n\n\nwhere  $\\ell$  is a function to measure the consistency between the teacher and student structural relations. In experiments, we use the following Huber loss function as a measurement:\n\n\n$$\n\n$$\n\\ell (a, b) = \\left\\{ \\begin{array}{l l} \\frac {1}{2} (a - b) ^ {2} & | a - b | \\leq 1, \\\\ | a - b | - \\frac {1}{2} & \\text {o t h e r w i s e .} \\end{array} \\right. \\tag {6}\n$$\n\n$$\n\n\nwhere  $a$  and  $b$  are two different structural relations.\n\n\n# 3.2.2INSTANCE-AWARE LABEL-WISE EMBEDDING DISTILLATION\n\n\nInstance-aware label-wise embedding distillation (ID) aims to improve the distillation performance by exploring the structural relations among inter-class label-wise embeddings from the same image. Generally, one can hardly distinguish between two different semantic objects occurring in an image due to the high similarities they share. For example, see the right side of Figure 3, for an image annotated with sky, sea and beach, one can hardly distinguish between the semantic objects sky and sea, since they share the same color and similar texture. A feasible solution is to exploit other useful information, such as their spatial relation, i.e., sky is always above sea. Due to the powerful learning capacity, the large model is able to distinguish between the similar semantic objects by exploiting such implicit supervised information. From the perspective of learned feature representations, as shown in the right side of 3, the teacher model tends to learn a dispersed structure of inter-class label-wise embedding, which is beneficial for improving its discrimination ability. By distilling the structural knowledge from the teacher model, ID enforces the student model to enhance the dispersion of inter-class label-wise embeddings, which can improve its discrimination ability.\n\n\nFor a given instance  $\\boldsymbol{x}_i$ , let  $\\{e_{ik}^{\\mathcal{T}}\\}_{k=1}^{q}$  and  $\\{e_{ik}^S\\}_{k=1}^{q}$  respectively denote the label-wise embeddings generated by teacher and student models. Then, we can capture the structural relation between any two inter-class label-wise embeddings  $e_{ik}^{\\mathcal{T}}$  and  $e_{il}^{\\mathcal{T}}$  by measuring their distance in the embedding space:\n\n\n$$\n\n$$\n\\phi_ {\\mathrm {I D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {i l} ^ {\\mathcal {T}}\\right) = \\left\\{ \\begin{array}{l l} \\| \\boldsymbol {e} _ {i k} ^ {\\mathcal {T}} - \\boldsymbol {e} _ {i l} ^ {\\mathcal {T}} \\| _ {2} & y _ {i k} = 1, y _ {i l} = 1, \\\\ 0 & \\text {o t h e r w i s e .} \\end{array} \\right. \\tag {7}\n$$\n\n$$\n\n\nNote that in Eq.(7), we only consider the structural relation between any two valid label-wise embeddings, i.e., the embedding with respect to positive labels. Similar to Eq.(7), for any two inter-class label-wise embeddings  $e_{ik}^S$  and  $e_{il}^S$ , we can obtain the structural relation  $\\phi_{\\mathrm{ID}}(e_{ik}^S, e_{il}^S)$  for the student model.\n\n\nBy encouraging the teacher and student model to maintain the consistent structure of intra-instance label-wise embeddings, we can minimize the following  $\\mathcal{L}_{\\mathrm{LED - ID}}$  loss to achieve the instance-aware structural consistency where  $\\ell (.)$  is Huber loss as defined in Eq.(6):\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {L E D - I D}} = \\sum_ {i = 1} ^ {b} \\sum_ {k, l \\in [ q ]} \\ell \\left(\\phi_ {\\mathrm {I D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {T}}, \\boldsymbol {e} _ {i l} ^ {\\mathcal {T}}\\right), \\phi_ {\\mathrm {I D}} \\left(\\boldsymbol {e} _ {i k} ^ {\\mathcal {S}}, \\boldsymbol {e} _ {i l} ^ {\\mathcal {S}}\\right)\\right). \\tag {8}\n$$\n\n$$\n\n\nFinally, the overall objective function of L2D (Eq.(2)) can be re-written as follows:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {L 2 D}} = \\mathcal {L} _ {\\mathrm {B C E}} + \\lambda_ {\\mathrm {M L D}} \\mathcal {L} _ {\\mathrm {M L D}} + \\lambda_ {\\mathrm {L E D - C D}} \\mathcal {L} _ {\\mathrm {L E D - C D}} + \\lambda_ {\\mathrm {L E D - I D}} \\mathcal {L} _ {\\mathrm {L E D - I D}}, \\tag {9}\n$$\n\n$$\n\n\nwhere  $\\lambda_{\\mathrm{MLD}}$ ,  $\\lambda_{\\mathrm{LED - CD}}$  and  $\\lambda_{\\mathrm{LED - ID}}$  are all balancing parameters.\n\n\nTable 1: Results on MS-COCO where teacher and student models are in the same architectures.\n\n\n<table><tr><td>Teacher</td><td colspan=\"3\">RepVGG-A2</td><td colspan=\"3\">ResNet-101</td><td colspan=\"3\">WRN-101</td><td colspan=\"3\">Swin-S</td></tr><tr><td>Student</td><td colspan=\"3\">RepVGG-A0</td><td colspan=\"3\">ResNet-34</td><td colspan=\"3\">WRN-50</td><td colspan=\"3\">Swin-T</td></tr><tr><td>Metrics</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td></tr><tr><td>Teacher</td><td>72.71</td><td>74.11</td><td>68.63</td><td>73.62</td><td>73.89</td><td>68.61</td><td>74.70</td><td>75.56</td><td>70.73</td><td>81.70</td><td>80.48</td><td>77.12</td></tr><tr><td>Student</td><td>70.02</td><td>72.49</td><td>66.77</td><td>70.31</td><td>72.49</td><td>66.82</td><td>74.45</td><td>75.43</td><td>70.61</td><td>79.59</td><td>79.18</td><td>75.42</td></tr><tr><td>RKD</td><td>70.08</td><td>72.39</td><td>66.73</td><td>70.13</td><td>72.44</td><td>66.78</td><td>74.70</td><td>75.71</td><td>70.84</td><td>79.63</td><td>79.19</td><td>75.57</td></tr><tr><td>PKT</td><td>70.11</td><td>72.47</td><td>66.80</td><td>70.43</td><td>72.64</td><td>66.68</td><td>74.54</td><td>75.47</td><td>70.58</td><td>79.64</td><td>79.09</td><td>75.39</td></tr><tr><td>ReviewKD</td><td>70.00</td><td>72.35</td><td>66.82</td><td>70.39</td><td>72.62</td><td>66.76</td><td>74.03</td><td>75.29</td><td>70.36</td><td>79.81</td><td>79.18</td><td>75.55</td></tr><tr><td>MSE</td><td>70.26</td><td>72.54</td><td>66.99</td><td>70.54</td><td>72.75</td><td>66.85</td><td>74.53</td><td>75.60</td><td>70.71</td><td>79.67</td><td>79.20</td><td>75.52</td></tr><tr><td>PS</td><td>70.65</td><td>72.89</td><td>67.60</td><td>70.86</td><td>72.66</td><td>67.12</td><td>75.12</td><td>76.05</td><td>71.63</td><td>79.96</td><td>79.64</td><td>76.20</td></tr><tr><td>MLD</td><td>70.74</td><td>72.81</td><td>67.46</td><td>70.68</td><td>72.69</td><td>67.19</td><td>74.92</td><td>75.75</td><td>71.21</td><td>80.11</td><td>79.68</td><td>76.44</td></tr><tr><td>L2D</td><td>72.81</td><td>74.59</td><td>69.49</td><td>72.87</td><td>74.45</td><td>69.43</td><td>76.61</td><td>77.08</td><td>72.79</td><td>81.59</td><td>81.03</td><td>77.86</td></tr></table>\n\n\nTable 2: Results on MS-COCO where teacher and student models are in the different architectures.\n\n\n<table><tr><td>Teacher</td><td colspan=\"3\">ResNet-101</td><td colspan=\"3\">Swin-T</td><td colspan=\"3\">ResNet-101</td><td colspan=\"3\">Swin-T</td></tr><tr><td>Student</td><td colspan=\"3\">RepVGG-A0</td><td colspan=\"3\">ResNet-34</td><td colspan=\"3\">MobileNet v2</td><td colspan=\"3\">MobileNet v2</td></tr><tr><td>Metrics</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td></tr><tr><td>Teacher</td><td>73.62</td><td>73.89</td><td>68.61</td><td>79.43</td><td>78.77</td><td>75.07</td><td>73.62</td><td>73.89</td><td>68.61</td><td>79.43</td><td>78.77</td><td>75.07</td></tr><tr><td>Student</td><td>70.02</td><td>72.49</td><td>66.77</td><td>70.31</td><td>72.49</td><td>66.82</td><td>71.85</td><td>73.59</td><td>68.26</td><td>71.85</td><td>73.59</td><td>68.26</td></tr><tr><td>RKD</td><td>70.08</td><td>72.35</td><td>66.72</td><td>70.00</td><td>72.34</td><td>66.64</td><td>71.76</td><td>73.68</td><td>68.40</td><td>71.74</td><td>73.68</td><td>68.37</td></tr><tr><td>PKT</td><td>69.99</td><td>72.35</td><td>66.56</td><td>70.26</td><td>72.39</td><td>66.82</td><td>71.88</td><td>73.60</td><td>68.35</td><td>71.84</td><td>73.76</td><td>68.37</td></tr><tr><td>ReviewKD</td><td>70.00</td><td>72.33</td><td>66.62</td><td>70.29</td><td>72.39</td><td>66.58</td><td>71.92</td><td>73.73</td><td>68.48</td><td>71.73</td><td>73.71</td><td>68.36</td></tr><tr><td>MSE</td><td>70.07</td><td>72.50</td><td>66.85</td><td>70.33</td><td>72.57</td><td>66.72</td><td>71.91</td><td>73.68</td><td>68.28</td><td>71.80</td><td>73.74</td><td>68.38</td></tr><tr><td>PS</td><td>70.30</td><td>72.61</td><td>67.10</td><td>70.94</td><td>72.93</td><td>67.57</td><td>72.11</td><td>73.89</td><td>68.42</td><td>72.42</td><td>74.14</td><td>68.94</td></tr><tr><td>MLD</td><td>70.48</td><td>72.77</td><td>67.10</td><td>71.14</td><td>72.99</td><td>67.63</td><td>72.17</td><td>73.84</td><td>68.52</td><td>72.35</td><td>74.10</td><td>68.91</td></tr><tr><td>L2D</td><td>72.14</td><td>74.08</td><td>68.78</td><td>73.42</td><td>74.97</td><td>70.20</td><td>73.24</td><td>74.85</td><td>69.72</td><td>74.21</td><td>75.72</td><td>70.87</td></tr></table>\n\n\n# 4 EXPERIMENTS\n\n\nDatasets. We perform experiments on three benchmark datasets Pascal VOC2007 (Everingham et al., 2015) (VOC for short), MS-COCO2014 (Lin et al., 2014) (MS-COCO for short) and NUS-WIDE (Chua et al., 2009). VOC contains 5,011 images in the train-val set, and 4,952 images in the test set. It covers 20 common objects, with an average of 1.6 labels per image. MS-COCO contains 82,081 training images and 40,137 test images. It covers 80 common objects, with an average of 2.9 labels per image. NUS-WIDE contains 161,789 training images and 107,859 test images. It covers 81 visual concepts, with an average of 1.9 labels per image.\n\n\nMetrics. Following existing works (Zhang & Zhou, 2013; Liu et al., 2021a; Sun et al., 2022), we adopt the mean average precision (mAP) over all classes, overall F1-score (OF1) and average per-class F1-score (CF1) to evaluate the performance. We choose OF1 and CF1, since they consider both recall and precision and thus are more comprehensive.\n\n\nMethods. To validate the proposed method, we compare it with the following KD methods: RKD (Park et al., 2019), which captures the relations among instances to guide the training of the student model; PKT (Passalis & Tefas, 2018), which measures KL divergence between features by treating them as probability distributions; ReviewKD (Chen et al., 2021), which transfers knowledge across different stages instead of just focusing on features in the same levels; MSE (Xu et al., 2022a), which minimizes the MSE loss between logits of teacher and student model; PS (Song et al., 2021), which minimizes KL divergence of logits after a partial softmax function.\n\n\nImplementation Details. We use the models pretrained on ImageNet (Deng et al., 2009) as the backbones. We resize the resolution of all images to  $224 \\times 224$  and set the batch size as 64. For each training image, we adopt a weak augmentation consisting of random horizontal flipping and a strong augmentation consisting of Cutout (Devries & Taylor, 2017) and RandAugment Cubuk et al. (2020). We use the Adam optimization method (Kingma & Ba, 2015) to train the model for 80 epochs. The one-cycle policy is used with a maximal learning rate of 0.0001 and the weight decay (Loshchilov & Hutter, 2018) of 0.0001. For all experiments, we set  $\\lambda_{\\mathrm{MLD}} = 10$ ,  $\\lambda_{\\mathrm{LED-CD}} = 100$ , and  $\\lambda_{\\mathrm{LED-ID}} = 1000$ . Parameter sensitivity analysis in Appendix D shows that the performance of\n\n\n![Figure 4: Comparison results of the comparing methods on VOC in terms of AP and mAP  $(\\%)$  where the backbones of teacher and student model are respectively ResNet-50 and ResNet-18.](images/815ee5fde3e1930a3dc20789856e9d2a4070e1872c8094a7427757a554e80b2a.jpg)\n\n\nL2D are insensitive to all of our balancing parameters. For the comparing methods, we set their parameters as suggested in the original papers. Especially, for all feature-based methods, we just deploy them on the feature maps which is output from the visual backbone  $f$ . All the experiments are conducted on GeForce RTX 2080 GPUs. More details about the used models and implementation of the label-wise embedding encoder are attached in Appendix A.\n\n\n# 4.1 COMPARISON RESULTS\n\n\nTable 1 and Table 2 report comparison results on MS-COCO with the same and different architectures of student and teacher models. From Table 1, it can be observed that: 1) Conventional feature-based distillation methods only achieve minor improvements in performance when compared with the student model (without distillation). This indicates these methods do not work in multi-label scenarios due to their disability to capture multiple semantics occurred in feature maps. 2) MLD can outperform conventional feature-based distillation methods in most cases, which indicates by performing one-versus-all reduction, the logits-based distillation can be adapted into multi-label knowledge distillation. 3) The proposed L2D significantly outperforms all other methods and achieves comparable performance with the teacher model, which convincingly validates the effectiveness of the proposed label-wise embedding distillation. From Table 2, we can see: 1) Compared with the same architecture, the performance gap between teacher and student model is larger for different architectures, which indicates the corresponding distillation task is harder. 2) Our method significantly outperforms all the comparing methods by a significant margin in all cases. 3) L2D achieves the best performance in all cases and significantly outperforms MLD. These results provide a strong empirical validation for the effectiveness of the proposed method.\n\n\nFigure 4 illustrates the performance of the proposed methods and other comparing methods on VOC in terms of AP and mAP. It is noteworthy that the performance is ranked in descending according to the performance of student model. From the figure, it can be observed that: 1) Our proposed L2D achieves the best performance and significantly outperforms the comparing methods in terms of mAP. 2) L2D consistently achieves superior performance to the comparing methods in most classes and the performance gap is large especially for classes that the student model achieves poor performance. This observation discloses that L2D improves the performance of hard classes by enhancing the distinctiveness of feature representations. These experimental results demonstrate the practical usefulness of the proposed method. More results on VOC and NUS-WIDE can be found in Appendix B and Appendix C.\n\n\n# 4.2 ABLATION STUDY\n\n\nIn this section, to further analyze how the proposed method improves distillation performance, Table 3 reports results of ablation studies on MS-COCO (teacher: ResNet-101, student: ResNet-34) and VOC (teacher: ResNet-50, student: ResNet-18). The first line in Table 3 reports the performance of student model without knowledge distillation. It can be observed that the distillation performance is improved slightly by only conducting multi-label logits distillation. Compared with MLD, label-wise embeddings distillation achieves the major improvement for the proposed method. It can\n\n\nTable 3: Ablation studies on MS-COCO and VOC.\n\n\n<table><tr><td colspan=\"3\"></td><td colspan=\"3\">MS-COCO</td><td colspan=\"3\">Pascal VOC 2007</td></tr><tr><td>MLD</td><td>CD</td><td>ID</td><td>mAP</td><td>OF1</td><td>CF1</td><td>mAP</td><td>OF1</td><td>CF1</td></tr><tr><td></td><td></td><td></td><td>70.31</td><td>72.49</td><td>66.82</td><td>84.01</td><td>83.60</td><td>79.42</td></tr><tr><td>\u2713</td><td></td><td></td><td>70.68</td><td>72.69</td><td>67.19</td><td>84.48</td><td>84.07</td><td>80.29</td></tr><tr><td>\u2713</td><td>\u2713</td><td></td><td>71.91</td><td>73.74</td><td>68.76</td><td>85.65</td><td>85.64</td><td>81.94</td></tr><tr><td>\u2713</td><td></td><td>\u2713</td><td>71.79</td><td>73.62</td><td>68.43</td><td>84.59</td><td>84.19</td><td>80.24</td></tr><tr><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>72.87</td><td>74.45</td><td>69.43</td><td>85.71</td><td>85.70</td><td>82.11</td></tr></table>\n\n\n![(a) Vanilla](images/d1f0114043661588281e56adabc3a7f07e5c07ecd238fe740dd2877829ba3029.jpg)\n\n\n![(b) PKT Figure 5: The differences between correlation matrices of student and teacher predicted probabilities on MS-COCO. The student distilled by our method shows significant matching between student's and teacher's correlations compared to others.](images/6270e730612b0212bd711e9f655031c90ed12f996ed053b587e1b58e4052ef8e.jpg)\n\n\n![(c)MLD](images/7810745b4c3ccc7a22fd8ee4f3532aeb7c16bf2e7fb0a55c4a65ebed7ec91a5c.jpg)\n\n\n![(d) L2D](images/f332ad196cfd7cef24131b81982c14cc1022b94d9004b8bdd121d8b918934cd7.jpg)\n\n\nbe observed that by performing CD and ID, the mAP performance achieves  $1.6\\%$  and  $1.48\\%$  increments, respectively. Finally, we also examine the combination of these techniques. By incorporating these components together, the fusing method achieves the best performance and significantly outperforms each other method. These results demonstrate that all of three components are of great importance to the performance of the proposed L2D.\n\n\n# 4.3 DISTILLING INTER-CLASS CORRELATIONS\n\n\nAs discussed in the previous works (Hinton et al., 2015), the conventional supervised learning losses, e.g., BCE loss, often neglects the correlations among class predictions. However, the label correlation is a foundational element for multi-label classification. Distillation losses utilize \"soft targets\", which can effectively distill such correlation information from the teacher model, leading to a desirable distillation performance. To validate whether the correlations can be captured by L2D effectively, Figure 5 illustrates the differences between correlation matrices of student and teacher predicted probabilities on MS-COCO. From the figure, it can be observed that: 1) Without knowledge distillation, the teacher and student correlations are very different, which indicates teacher model tends to capture a more precise correlation than student model and thus achieves better performance. 2) The representative comparing method PKT shows a large difference, which discloses that the conventional KD methods are ineffective to capture the correlations in multi-label scenarios. 3) MLD shows a reduced difference, which indicates that the rich informativeness of logits is beneficial for capturing precise correlations. 4) L2D shows significant matching between the teacher and student correlations. By enhancing the distinctiveness of label-wise embeddings, L2D can obtain more correct predicted probabilities, leading to a more precise correlation estimation.\n\n\n# 5 CONCLUSION\n\n\nThe paper studies the problem of multi-label knowledge distillation. In the proposed method, the multi-label logits distillation explores the informative semantic knowledge compressed in the teacher logits to obtain more semantic supervision. Furthermore, the label-wise embedding distillation exploits the structural knowledge from label-wise embeddings to learn more distinctive feature representations. Experimental results on benchmark datasets validate the effectiveness of the proposed method. In future, we plan to improve the performance of MLKD by exploiting other abundant structural information.\n\n\n# REFERENCES\n\n- Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge review. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5008-5017, 2021.\n\n- Zhao-Min Chen, Xiu-Shen Wei, Xin Jin, and Yanwen Guo. Multi-label image recognition with joint class-aware map disentangling and label correlation embedding. In Proceedings of the IEEE International Conference on Multimedia and Expo, pp. 622-627, 2019a.\n\n- Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen Guo. Multi-label image recognition with graph convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5177-5186, 2019b.\n\n- Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yan-Tao Zheng. Nus-wide: A real-world web image database from national university ofsingapore. In Proceedings of ACM Conference on Image and Video Retrieval, 2009.\n\n- Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In Advances in Neural Information Processing Systems, 2020.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2009.\n\n- Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.\n\n- Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pp. 13733-13742, 2021.\n\n- Mark Everingham, SM Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The Pascal visual object classes challenge: A retrospective. International Journal of Computer Vision, pp. 98-136, 2015.\n\n- Fran\u00e7ois-Guillaume Fernandez. Torchcam: class activation explorer. https://github.com/frgfmd/torch-cam, 2020.\n\n- Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. International Journal of Computer Vision, pp. 1789-1819, 2021.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.\n\n- Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\n- Peng-Tao Jiang, Chang-Bin Zhang, Qibin Hou, Ming-Ming Cheng, and Yunchao Wei. Layercam: Exploring hierarchical class activation maps for localization. IEEE Transactions on Image Processing, pp. 5875-5888, 2021.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.\n\n- Jack Lanchantin, Tianlu Wang, Vicente Ordonez, and Yanjun Qi. General multi-label image classification with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16478-16488, 2021.\n\n- Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr D\u00f3lar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision, pp. 740-755, 2014.\n\n- Shilong Liu, Lei Zhang, Xiao Yang, Hang Su, and Jun Zhu. Query2label: A simple transformer way to multi-label classification. arXiv preprint arXiv:2107.10834, 2021a.\n\n- Yongcheng Liu, Lu Sheng, Jing Shao, Junjie Yan, Shiming Xiang, and Chunhong Pan. Multi-label image classification via knowledge distillation from weakly-supervised detection. In Proceedings of the 26th ACM International Conference on Multimedia, pp. 700-708, 2018.\n\n- Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012-10022, 2021b.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.\n\n- Roy Miles, Adri\u00e1n L\u00f3pez Rodr\u00edguez, and Krystian Mikolajczyk. Information theoretic representation distillation. arXiv preprint arXiv:2112.00459, 2021.\n\n- Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 5191-5198, 2020.\n\n- Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3967-3976, 2019.\n\n- Nikolaos Passalis and Anastasios Tefas. Learning deep representations with probabilistic knowledge transfer. In Proceedings of the European Conference on Computer Vision, pp. 268-284, 2018.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning, pp. 8748-8763, 2021.\n\n- Tal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi Zelnik-Manor. Asymmetric loss for multi-label classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 82-91, 2021a.\n\n- Tal Ridnik, Gilad Sharir, Avi Ben-Cohen, Emanuel Ben-Baruch, and Asaf Noy. MI-decoder: Scalable and versatile classification head. arXiv preprint arXiv:2111.12933, 2021b.\n\n- Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.\n\n- Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. *Mobilenetv2: Inverted residuals and linear bottlenecks*. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 4510-4520, 2018.\n\n- Liangchen Song, Jialian Wu, Ming Yang, Qian Zhang, Yuan Li, and Junsong Yuan. Handling difficult labels for multi-label image classification via uncertainty distillation. In Proceedings of the 29th ACM International Conference on Multimedia, pp. 2410-2419, 2021.\n\n- Feng Sun, Ming-Kun Xie, and Sheng-Jun Huang. A deep model for partial multi-label image classification with curriculum based disambiguation. arXiv preprint arXiv:2207.02410, 2022.\n\n- Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In International Conference on Learning Representations, 2019.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.\n\n- Jiazhi Xu, Sheng Huang, Fengtao Zhou, Luwen Huangfu, Daniel Zeng, and Bo Liu. Boosting multi-label image classification with complementary parallel self-distillation. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, 2022a.\n\n- Shichao Xu, Yikang Li, Jenhao Hsiao, Chiuman Ho, and Zhu Qi. A dual modality approach for (zero-shot) multi-label classification. arXiv preprint arXiv:2208.09562, 2022b.\n\n- Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016a.\n\n- Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British Machine Vision Conference, 2016b.\n\n- Min-Ling Zhang and Zhi-Hua Zhou. A review on multi-label learning algorithms. IEEE Transactions on Knowledge and Data Engineering, pp. 1819-1837, 2013.\n\n- Ying Zhang, Tao Xiang, Timothy M Hospedales, and Hutchuan Lu. Deep mutual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4320-4328, 2018.\n\n- Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. Decoupled knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11953-11962, 2022.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636550.9775736", "msecs": "977.0", "relativeCreated": "11670.714139938354", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Learning Graph Invariance by Harnessing Spuriosity\n\n# Abstract\nRecently, graph invariant learning has become the _de facto_ approach to tackle the Out-of-Distribution (OOD) generalization failure in graph representation learning. They generically follow the framework of invariant risk minimization to capture the invariance of graph data from different environments. Despite some success, it remains unclear to what extent existing approaches have captured invariant features for OOD generalization on graphs. In this work, we find that representative OOD methods such as IRM and VRex, and their variants on graph invariant learning may have captured a limited set of invariant features. To tackle this challenge, we propose $\\texttt{LIRS}$, a novel learning framework designed to **L**earn graph **I**nvariance by **R**emoving **S**purious features. Different from most existing approaches that _directly_ learn the invariant features, $\\texttt{LIRS}$ takes an _indirect_ approach by first learning the spurious features and then removing them from the ERM-learned features, which contains both spurious and invariant features. We demonstrate that learning the invariant graph features in an _indirect_ way can learn a more comprehensive set of invariant features. Moreover, our proposed method outperforms the second-best method by as much as 25.50% across all competitive baseline methods, highlighting its effectiveness in learning graph invariant features.\n\n\n# 1 INTRODUCTION\n\n\nGraph representation learning with graph neural networks (GNNs) (Kipf & Welling, 2017; Xu et al., 2019; Velickovi\u0107 et al., 2017) has proven to be highly successful in tasks involving relational information (Qiu et al., 2018; Wu et al., 2022b; Yu et al., 2018; Zhang et al., 2022c). Despite their achievements, GNNs often assume that the training and test data share the same distribution. This assumption often does not hold in real-world applications (Hu et al., 2020; Huang et al., 2021; Ji et al., 2022; Koh et al., 2021) due to changes in the underlying data generation process, leading to distribution shifts. Such phenomenon can severely degrade the performance of GNN models, which presents a critical challenge for their deployment in practical scenarios (DeGrave et al., 2020).\n\n\nTo address the OOD generalization challenge, graph invariant learning has become the de facto approach. Inspired by the success of invariance principle (Arjovsky et al., 2020; Kreuzer et al., 2021), graph invariant learning aims to identify invariant subgraphs that are causally related with the targets (Yang et al., 2022; Li et al., 2022b; Liu et al., 2022; Zhuang et al., 2023; Chen et al., 2022; Li et al., 2022a). Typically, they follow the framework of Invariant Risk Minimization (IRM) (Arjovsky et al., 2020) or Variance-Risk Minimization (VRex) (Krueger et al., 2021), which aim to capture invariant features by learning an equipredictive classifier across different environments. Despite some success, many graph invariant learning methods only perform similarly compared to the traditional Empirical Risk Minimization (ERM) (Vapnik, 1995) in graph OOD benchmarks (Gui et al., 2022), it raises a critical yet overlooked question:\n\n\nTo what extent do the graph invariant learning algorithms capture the graph invariance?\n\n\nIn this work, we address this question by investigating two representative OOD algorithms, namely IRMv1 (Arjovsky et al., 2020) and VRex (Kreuzer et al., 2021), in graph-level OOD classification scenarios. These algorithms are widely used, and many graph invariance learning algorithms are based on their variants or extensions (Li et al., 2022b; Liu et al., 2022; Zhuang et al., 2023; Wu et al., 2022c). Surprisingly, we find that IRM, VRex and their variants on graph invariant learning\n\n\nmay only learn a subset of invariant features (Sec. 3). In contrast, the learning paradigm that learns invariant features indirectly by removing spurious features from the ERM-learned features may\n\n\nrender learning a more comprehensive set of invariant features (Figure 1 and Sec. 3). Motivated by this observation, we propose a novel learning framework LIRS: Learning graph Invariance by Removing Spuriosity. Different from most existing approaches that directly learn the invariant features, LIRS adopts the indirect learning paradigm. Specifically, LIRS first leverages the biased info-max principle (Def. 3) to effectively learn graph spuriosity (Theorem 4.1), then employees class-conditioned cross entropy loss to effectively learn graph invariance (Theorem 4.2). Extensive experiments on synthetic datasets demonstrate that LIRS is able to learn more invariant features compared to state-of-the-art graph invariant learning methods that adopt the direct invariant learning paradigm. Furthermore, LIRS shows superior OOD performance on real-world datasets with various types of distribution shifts, highlighting its effectiveness in learning graph invariant features. Our contributions can be summarized as follows:\n\n\n![Figure 1: Illustration of the two graph invariant learning paradigms. Compared with the invariant learning methods  $\\mathcal{A}_{Inv}$  that adopt the direct learning paradigm, our proposed method first adopts a spuriosity learner  $\\mathcal{A}_{Spu}$  to learn spurious features, then learns invariant features by removing the spurious features using the feature disentanglement module  $\\mathcal{A}_{Disent}$ .](images/20bd0e2cd67019623f594f852f2e9acd438e2ced314646b25e099ac300624c64.jpg)\n\n- - We reveal that learning invariant features indirectly i.e., by first learning and then removing spurious features, can be more effective in capturing invariant features than existing (graph) invariant learning algorithms that learn invariant features directly.\n\n- - We propose LIRS, a novel framework that adopts the indirect learning paradigm for learning invariant features, which consists of: a) The biased infomax principle, a contrastive learning algorithm that provably learns graph spuriosity; b) class-conditioned cross-entropy loss, a learning objective that effectively learns graph invariance by removing spurious features from ERM-learned features.\n\n- - Extensive experiments demonstrate that LIRS outperforms second-best baseline methods by up to  $25.50\\%$  across 17 competitive baselines on both synthetic and real-world datasets with various distribution shifts.\n\n\n# 2 PRELIMINARY\n\n\nNotations. An undirected graph  $G$  with  $n$  nodes and  $m$  edges is denoted by  $G := \\{\\mathcal{V}, \\mathcal{E}\\}$ , where  $\\mathcal{V}$  is the node set and  $\\mathcal{E}$  is the edge set.  $G$  is also represented by the adjacency matrix  $\\mathbf{A}$  and the node feature matrix  $\\mathbf{X} \\in \\mathbb{R}^{n \\times D}$  with  $D$  feature dimensions. We use  $G_{c}$  and  $G_{s}$  to denote invariant and spurious subgraphs for graph  $G$  respectively, and  $\\mathbf{h}_{c}$  and  $\\mathbf{h}_{s}$  are invariant and spurious features in the latent space.  $\\widehat{\\mathbf{h}}_{i}$  and  $\\widehat{\\mathbf{h}}_{G}$  denote the estimated representations for node  $v_{i}$  and graph  $G$  respectively. Finally, we use  $[K] := \\{1, 2, \\dots, K\\}$  to denote an index set,  $w$  to denote a scalar value,  $\\mathbf{w}$  to denote a vector,  $\\mathbf{W}$  to denote a matrix,  $W$  to denote a random variable, and  $\\mathcal{W}$  to denote a set. A more complete set of notations is presented in Appendix A.\n\n\nProblem Definition. We focus on OOD generalization in graph classification in hidden environments. Given a set of graph datasets  $\\mathcal{D} = \\{\\mathcal{G}^e\\}_{e\\in \\mathcal{E}_{\\mathrm{tr}}\\subseteq \\mathcal{E}_{\\mathrm{all}}}$ , a GNN model  $f$ , denoted as  $\\rho \\circ h$ , comprises an encoder  $h:\\mathbb{G}\\to \\mathbb{R}^{F}$  that learns a representation  $\\mathbf{h}_G$  for each graph  $G$ , followed by a downstream classifier  $\\rho :\\mathbb{R}^{F}\\to \\mathbb{Y}$  to predict the label  $\\widehat{Y}_G = \\rho (\\widehat{\\mathbf{h}}_G)$ . The objective of OOD generalization on graphs is to learn an optimal GNN model  $f^{*}(\\cdot):\\mathbb{G}\\rightarrow \\mathbb{Y}$  using data from training environments  $\\mathcal{D}_{\\mathrm{tr}} = \\{\\mathcal{G}^e\\}_{e\\in \\mathcal{E}_{\\mathrm{tr}}}$  that effectively generalizes across all (unseen) environments:  $f^{*}(\\cdot) = \\arg \\min_{f}\\sup_{e\\in \\mathcal{E}_{\\mathrm{all}}}\\mathcal{R}(f|e)$ , where  $\\mathcal{R}(f|e) = \\mathbb{E}_{G,Y}^{e}[l(f(G),Y)]$  is the risk of the predictor  $f$  on the environment  $e$ , and  $l(\\cdot ,\\cdot):\\mathbb{Y}\\times \\mathbb{Y}\\to \\mathbb{R}_{+}$  denotes a loss function.\n\n\nAssumption 1. (Stable and predictive subgraphs) Given a graph  $G \\in \\mathcal{D}$ , there exists a set of stable (causal) substructure patterns  $\\mathcal{G}_c$  for every class label  $Y = y$ , satisfying: a)  $\\forall e, e' \\in \\mathcal{E}_{tr}, G_c \\in$\n\n\n$\\mathcal{G}_c, \\mathbb{P}^e(Y \\mid G_c) = \\mathbb{P}^{e'}(Y \\mid G_c)$ ; and b) The target  $Y$  can be expressed as  $Y = f^*(G_c) + \\epsilon$ , where  $\\epsilon \\perp G$  represents random noise, and  $\\perp$  indicates statistical independence.\n\n\nAssumption 1 posits that one or more substructure patterns in  $\\mathcal{G}_c$  are not only stably associated with the target label  $Y$  across different environments but also possess sufficient predictive power to accurately determine  $Y$ . This assumption is well-aligned with real-world scenarios. For instance, in the GOODHIV dataset (Gui et al., 2022; Hu et al., 2020; Wu et al., 2018), a molecule's ability to inhibit HIV may depend on the presence of several functional groups interacting with various parts of the virus. Moreover, recent study also provides empirical evidence for graph applications, suggesting that multiple substructures remain stable and predictive of the targets (see Appendix E.2 in Bui et al. (2024)), thereby supporting the validity of Assumption 1. Therefore, when the OOD algorithms are able to learn a broader set of invariant substructures (features), they will generalize more effectively across different environments.\n\n\n# 3 WHY LEARNING INVARIANT FEATURES INDIRECTLY?\n\n\nIn this section, we motivate our study by addressing the question: Why might learning invariant features indirectly be more effective than learning them directly? We begin by presenting an intuitive hypothesis related to our research question, followed by an empirical study on synthetic datasets to support this hypothesis. Consider a dataset  $\\mathcal{D}$  where each sample  $G^{(i)}$  com\n\n\n![Figure 2: Comparison of the distribution  $\\mathbb{P}_{\\Delta_{prob}}$  across different OOD algorithms.](images/a4bc8b56c142669b20aeb036887a2d3a63ed256ab156d39bd1704e38c1c09ec1.jpg)\n\n\n![](images/8ada362b52dede972e473ab00104cae1f0532f0706e48035dcb9d80f94679fc8.jpg)\n\n\nprises invariant features  $\\{\\mathbf{h}_{c,j}^{(i)}\\}_{j = 1}^{n_c}$  and spurious features  $\\{\\mathbf{h}_{s,j}^{(i)}\\}_{j = 1}^{n_s}$ , with  $n_c$  and  $n_s$  denoting the number of invariant and spurious features respectively. Any subset of the invariant features  $\\{\\mathbf{h}_{c,j}^{(i)}\\}_{j = 1}^{n_c}$  maintains stable relationships with  $Y$ , consequently OOD objectives that aim to directly learn invariant features (e.g., IRMv1 and VREx) may only capture a subset of these invariant features if a subset of invariant features can already achieve accurate predictions in the training set. In contrast, an OOD objective that seeks to remove the spurious features  $\\{\\mathbf{h}_{s,j}^{(i)}\\}_{j = 1}^{n_s}$  from the ERM-learned features may result in learning a more complete set of invariant features from  $\\{\\mathbf{h}_{c,j}^{(i)}\\}_{j = 1}^{n_c}$ , thus facilitating OOD generalization in unseen environments.\n\n\nEmpirical Study. To validate our hypothesis that learning invariant features by removing spurious features may facilitate learning more invariant features, we perform experiments using a variant of SPMotif datasets (Wu et al., 2022c; Ying et al., 2019). Specifically, for each sample three invariant subgraphs are attached to the base spurious subgraph in the training and validation sets. In the test dataset, we perform model inference and record the estimated probability for correctly predicted samples, denoted as  $\\hat{y}_j^{(i)}$ . Subsequently, we randomly removed two invariant subgraphs from these samples and compute the new estimated probability score, denoted as  $\\tilde{y}_j^{(i)}$ . We then fit the distribution  $\\mathbb{P}_{\\Delta_{prob}}$  using Kernel Density Estimation (KDE) (Terrell & Scott, 1992), based on the changes in probability  $\\Delta_{prob} \\coloneqq \\hat{y}_j^{(i)} - \\tilde{y}_j^{(i)}, \\forall i,j$ , as illustrated in Figure 2. If the encoder  $h(\\cdot)$  can indeed learn all invariant features in the test set, then removing two invariant subgraphs should not change  $\\hat{y}_j^{(i)}$ . Therefore, the closer  $\\mathbb{P}_{\\Delta_{prob}}$  is to 0, the more it indicates that the encoder has learned the full set of invariant features. In addition to the density function of  $\\Delta_{prob}$ , we also calculate the Wasserstein distance of each algorithm to a null distribution, where the probability mass is entirely centered at 0. As demonstrated in Figure 2, IRMv1 (Arjovsky et al., 2020) and VRex (Krueger et al., 2021) exhibit significant changes in  $\\Delta_{prob}$ , implying that for some samples, IRMv1 and VRex learn only a subset of the invariant patterns. GIL (Li et al., 2022b) and GREA (Liu et al., 2022) are two variants of VRex and IRM that learn invariant features directly by performing environment inference and environment augmentation respectively. As shown in Figure 2, the Wasserstein distances for GIL (Li et al., 2022b) and GREA (Liu et al., 2022) are also greater than that of our proposed\n\n\nmethod. This highlights the advantages of the learning paradigm that indirectly learns invariant features by first learning and then removing spuriosity. However, one may raise concerns that while\n\n\nlearning invariant features by first removing spurious features may indeed lead to capturing a more complete set of invariant features for the correctly predicted samples, it may also introduce spurious patterns if the spurious features are not effectively removed, which could render such a learning paradigm suboptimal in practice. To address this concern, we report test accuracy under varying strengths of spurious correlations, following previous experiments. As the strength of the spurious correlation increases, it becomes more challenging to eliminate the spurious features. From Figure 3, we have three observations: a) Our method consistently achieves superior OOD performance across different correlation strengths; b) Our method is less sensitive to the increased spurious correlations compared to other representative OOD methods, which is evident by the flatter curve in Figure 3; c) The higher test accuracy, combined with the distribution of\n\n\nprobability changes from the previous experiment, suggests that our method enables a larger number of test samples to learn a more complete set of invariant features. Based on these observations and analysis, we conclude that indirectly learning invariant features is more effective than learning them directly.\n\n\n![Figure 3: Test accuracy under varying spurious correlation strengths of the variant of SPMtoif datasets.](images/9e168d893f9aee8ed0839b19a8ec58acc698df5b966b09eb71bf26d4e9cff799.jpg)\n\n\n# 4 METHODOLOGY\n\n\nMotivated by the observations in Sec. 3, we now introduce our proposed LIRS framework, which learns invariant features on graphs indirectly.\n\n\n# 4.1 LEARNING GRAPH SPURIOSITY VIA BIASED INFOMAX\n\n\nThe first step of learning invariant features in the LIRS framework is to effectively learn graph spuriosity. In this section, we first propose a variant of the infomax principle (Hjelm et al., 2019; Velickovic et al., 2019; Linsker, 1988), namely biased infomax, which encourages spuriosity learning by suppressing invariant signals. We then present a more practical version of biased infomax, suitable for the real-world datasets. We first formally define spuriosity learning in below:\n\n\nDefinition 1. (Spuriosity Learning) Assuming each data instance  $G^{(i)}$  in a dataset  $\\mathcal{D}$  is composed of two parts  $\\mathbf{h}_c^{(i)}$  and  $\\mathbf{h}_s^{(i)}$ , an algorithm  $\\mathcal{A}$  is said to be a spuriosity learner if  $\\mathbf{h}_s^{(i)} = \\mathcal{A}(G^{(i)})$ ,  $\\forall i$ , i.e.,  $\\mathcal{A}$  learns only the spurious features for all instances in  $\\mathcal{D}$ .\n\n\nOne such algorithm to encourage spuriosity learning is the global-local infomax principle (Hjelm et al., 2019; Velickovi\u0107 et al., 2019; Linsker, 1988), which maximizes the mutual information (MI) between global representation and local node representations, as defined in the following:\n\n\nDefinition 2. (The Infomax Principle) The infomax principle optimizes the following optimization objective in Eqn. 1 w.r.t the GNN encoder  $h_{\\theta}(\\cdot)$ .\n\n\n$$\n\n$$\n\\max  _ {\\theta} \\mathbb {E} _ {G \\sim \\mathcal {G}} \\frac {1}{| G |} \\sum_ {v _ {i} \\in G} I (\\widehat {\\mathbf {h}} _ {i}, \\widehat {\\mathbf {h}} _ {G}), \\text {s . t .} \\widehat {\\mathbf {h}} _ {i} = h _ {\\theta} (G), \\widehat {\\mathbf {h}} _ {G} = R E A D O U T (\\widehat {\\mathbf {h}} _ {i}). \\tag {1}\n$$\n\n$$\n\n\nThe encoder  $h_{\\theta}(\\cdot)$  will primarily learn spurious features (Yao et al., 2024). However, it will still be constrained by  $|G_{c}|$ , the size of the invariant subgraph. As  $|G_{c}|$  increases, the learned global representation  $\\widehat{\\mathbf{h}}_G$  will increasingly capture invariant representations (Yao et al., 2024). To mitigate this constraint and further facilitate spuriosity learning, we propose the biased infomax principle.\n\n\nDefinition 3. (The Biased Infomax Principle) The biased infomax principle optimizes the following optimization objective in Eqn. 2 w.r.t the GNN encoder  $h_{\\theta}(\\cdot)$ .\n\n\n$$\n\n$$\n\\max  _ {\\theta} \\mathbb {E} _ {G \\sim \\mathcal {G}} \\frac {1}{| G |} \\left(\\sum_ {v _ {i} \\in G _ {s}} I (\\widehat {\\mathbf {h}} _ {i}, \\widehat {\\mathbf {h}} _ {G}) - \\sum_ {v _ {i} \\in G _ {c}} I (\\widehat {\\mathbf {h}} _ {i}, \\widehat {\\mathbf {h}} _ {G})\\right), \\text {s . t .} \\widehat {\\mathbf {h}} _ {i} = h _ {\\theta} (G), \\widehat {\\mathbf {h}} _ {G} = \\operatorname {R E A D O U T} (\\widehat {\\mathbf {h}} _ {i}). \\tag {2}\n$$\n\n$$\n\n\nBy maximizing MI across nodes  $v_{i} \\in G_{s}$  and minimizing MI across nodes  $v_{i} \\in G_{c}$ , the biased infomax principle yields an encoder  $h_{\\theta^{*}}(\\cdot)$  that encourages the learning of spurious patterns while suppressing invariant ones. Formally, we can demonstrate that the biased infomax principle achieves spuriosity learning.\n\n\nTheorem 4.1. Given that the invariant subgraph  $G_{c}$  contains invariant patterns that are causally related to the target labels, and  $G_{s}$  contains only spurious patterns, the biased infomax principle achieves spuriosity learning, i.e., the encoder  $h_{\\theta^{*}}(\\cdot)$  learns solely the spurious features for each data sample  $G$  in  $\\mathcal{D}$ .\n\n\nTo achieve invariant learning by removing spuriosity, it is crucial to effectively learn the spurious features first. Theorem 4.1 demonstrates that the biased infomax principle can learn graph spuriosity more effectively than vanilla infomax, as defined in Def. 2. However, in real-world scenarios,  $G_{c}$  is often unobservable, which hinders the practicality of the biased infomax principle. To make the proposed algorithm more practical, we propose an instance-level adaptive biased infomax, which incorporates a GNN explainer to identify the important nodes first, and then performs instance-wise counterfactual inference to determine whether the selected nodes should be biased or not.\n\n\nInstance-level adaptive biased infomax. At first glance, the biased infomax principle appears impractical due to the necessity of knowing  $G_{c}$  and  $G_{s}$ . To address this limitation, we employ an approximation algorithm to identify  $\\widehat{G}_c$ , which serves as an estimate for  $G_{c}$ . Subsequently, we treat nodes  $v_{i}\\in \\widehat{G}_{c}$  to realize the biased infomax principle. In practice, we utilize GSAT (Miao et al., 2022), a GNN explainer  $e(\\cdot)$  that is robust under distribution shifts, to identify  $\\widehat{G}_c$  and  $\\widehat{G}_s = G\\backslash \\widehat{G}_c$ . Nevertheless, the approximation error may inadvertently bias the algorithm by amplifying invariant signals and suppressing spurious ones, as demonstrated in the following proposition.\n\n\nProposition 1. Given an error rate  $p\\%$  in the approximation algorithm for  $G_{c}$ , i.e.,  $\\widehat{G}_c$  contains  $1 - p\\%$  nodes from  $G_{c}$  and  $p\\%$  nodes from  $G_{s}$ , let the learning objectives for the biased infomax with the ground-truth subgraphs  $G_{c}$  and  $G_{s}$  and with the approximated subgraphs  $\\widehat{G}_{c}$  and  $\\widehat{G}_{s}$  be denoted as  $\\mathcal{L}(\\theta^{*};\\mathcal{D})$  and  $\\mathcal{L}(\\theta^{\\prime};\\mathcal{D})$  respectively. The difference between  $\\mathcal{L}(\\theta^{*};\\mathcal{D})$  and  $\\mathcal{L}(\\theta^{\\prime};\\mathcal{D})$  can be expressed as:\n\n\n$$\n\n$$\n\\mathbb {E} _ {G \\sim \\mathcal {G}} \\left[ \\frac {2}{| G |} \\left(\\sum_ {v _ {i} \\in p G _ {s}} I (\\widehat {\\mathbf {h}} _ {i}; \\widehat {\\mathbf {h}} _ {G}) - \\sum_ {v _ {i} \\in p G _ {c}} I (\\widehat {\\mathbf {h}} _ {i}; \\widehat {\\mathbf {h}} _ {G})\\right) \\right].\n$$\n\n$$\n\n\nHere  $pG$  denotes  $p\\%$  nodes in graph  $G$ . When  $\\widehat{\\mathbf{h}}_G$  learns primarily spurious features,  $I(\\widehat{\\mathbf{h}}_i; \\widehat{\\mathbf{h}}_G) > I(\\widehat{\\mathbf{h}}_j; \\widehat{\\mathbf{h}}_G), \\forall v_i \\in G_s, v_j \\in G_c$ , then Prop. 1 implies that as  $p\\%$  increases, the gap between the ground-truth learning objective and the learning objective with approximation will also increase, thus leading to inaccurate graph representations. To reduce the approximation error and mitigate this issue, we conduct counterfactual inference for a graph  $G$  by evaluating the changes in predicted probability with and without  $\\widehat{G}_c$  derived from the GNN explainer. Assuming  $\\widehat{G}_c = e(G, Y)$  closely approximates the ground-truth  $G_c$ , the removal of  $\\widehat{G}_c$  should result in a significant change in the predicted probability score. Under this condition, for graph  $G$ , we bias  $v_i \\in \\widehat{G}_c$ . Otherwise, we employ vanilla infomax without the bias operation. Specifically, we use a pre-defined threshold  $\\tau$ . For each graph data instance  $G^{(i)}$  with target label  $j$ , let  $\\widehat{y}_j^{(i)}$  and  $\\tilde{y}_j^{(i)}$  denote the predicted probability scores before and after the removal of  $\\widehat{G}_c$ , respectively. We then bias the infomax for graph  $G^{(i)}$  if  $\\widehat{y}_j^{(i)} - \\tilde{y}_j^{(i)} > \\tau$  using Eqn. 2; otherwise, we use Eqn. 1 to learn the graph representation.\n\n\n# 4.2 LEARNING GRAPH INVARIANCE VIA CLASS-CONDITIONED CROSS-ENTROPY LOSS\n\n\nClass-conditioned cross-entropy loss. So far we have obtained the spurious graph representations  $\\mathbf{h}_s$  for every instance  $G$  in the dataset. Next we consider how to learn graph invariance by re\n\n\nmoving the spurious features from ERM-learned representations. To solve this challenge, previous study (Yao et al., 2024) takes spurious features  $\\mathbf{h}_s$  and target label  $Y$  as inputs to train a linear classifier using cross-entropy loss to obtain the logits, which capture spurious correlation for each sample  $G$ , and use these (spurious) logits for subsequent spurious feature removing. Although the input only contains spurious features, we argue that: The logits generated may be inaccurate to capture the spurious correlation strengths due to the overlapping of spurious patterns across different classes. To see this, we propose Prop. 2 to demonstrate this issue:\n\n\nProposition 2. Given a linear regression model with parameters  $\\{\\theta_1, \\theta_2\\}$  and spurious features  $\\{x_1, x_2\\}$ , the correlation strength for feature  $x_1$  is  $p$  and for  $x_2$  is  $1 - p$  when  $Y = 0$ . Similarly, the correlation strength for feature  $x_1$  is  $q$  and for  $x_2$  is  $1 - q$  when  $Y = 1$ . Assuming the spurious features  $x_1$  and  $x_2$  can each take values in  $\\{0, 1\\}$ , we obtain the following parameter estimates using Mean Squared Error (MSE) loss:  $\\theta_1 = \\frac{q}{p + q}, \\theta_2 = \\frac{1 - q}{2 - p - q}$ .\n\n\nProp. 2 demonstrates the issue brought by the interference of overlapping spurious features across different classes. For instance, when  $p = q$ ,  $\\theta_{1} = \\theta_{2} = \\frac{1}{2}$ , the generated logits fail to distinguish different spurious patterns. To mitigate this issue, we propose class-conditioned cross-entropy loss. Specifically, we first perform clustering within each class based on the spurious features  $\\{\\mathbf{h}_s^{(i)}\\}_{i=1}^n$ . The resulted clusters will reflect different spurious patterns and environments accurately given the spurious features learned by the biased infomax objective. However, employing hard labels can result in a loss of information regarding the relative positions of samples within a cluster. To address this, we refit a linear classifier using the cluster labels as targets. This approach ensures that samples near the cluster boundaries exhibit higher entropy in their spurious logits, thereby preserving the information about their position within the cluster. Given the estimated spurious logits  $\\mathbf{s}^{(i)} \\in \\mathbb{R}^K$  with  $K$  clusters in each class, we propose the following learning objective for spurious feature removing:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {i n v}} := \\mathbb {E} _ {y \\sim \\mathcal {Y}} \\mathbb {E} _ {G ^ {(i)} | Y = y} - \\sum_ {j = 1} ^ {K} w ^ {(i)} \\mathbf {s} _ {j} ^ {(i)} \\log \\left(\\sigma \\left(\\widehat {\\mathbf {s}} _ {j} ^ {(i)}\\right)\\right), s. t., \\widehat {\\mathbf {s}} ^ {(i)} = \\rho^ {\\prime} \\left(\\widehat {\\mathbf {h}} _ {G} ^ {(i)}\\right) \\in \\mathbb {R} ^ {K}. \\tag {3}\n$$\n\n$$\n\n\nHere,  $\\sigma(\\cdot)$  denotes the softmax function,  $\\rho'(\\cdot): \\mathbb{R}^F \\to \\mathbb{R}^C$  represents the clustering classification head, which takes  $\\widehat{\\mathbf{h}}_G$  from the GNN encoder  $h(\\cdot)$  as input. The reweighting coefficient for each sample  $G^{(i)}$  is denoted as  $w^{(i)}$ , which adjusts the weight for samples from the majority group by reducing it and increases the weight for samples from the minority group. The generalized cross-entropy (GCE) method (Zhang & Sabuncu, 2018) is employed to calculate  $w^{(i)}$ , i.e.,  $w^{(i)} = \\frac{1 - (\\mathbf{s}_j^{(i)})^\\gamma}{\\gamma}$ , where  $j$  is the ground-truth clustering label for sample  $G^{(i)}$ , and  $\\gamma$  is a hyperparameter. Finally, the loss objective for feature disentanglement is:\n\n\n$$\n\n$$\n\\mathcal {L} = \\mathcal {L} _ {G T} + \\lambda \\mathcal {L} _ {i n v}, \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\mathcal{L}_{GT}$  denotes the ERM loss.  $\\mathcal{L}_{inv}$ , serving as a regularization term, will guide the learning process to learn invariant features by removing spurious features from the ERM-learned features. Formally, we present the following theorem.\n\n\nTheorem 4.2. There exists a suitable  $\\gamma$  and clustering number  $K$ , such that minimizing the loss objective  $\\mathcal{L} = \\mathcal{L}_{GT} + \\lambda \\mathcal{L}_{inv}$  will lead to the optimal encoder  $h^{*}(\\cdot)$  which elicits invariant features for any graph  $G$ , i.e.,  $\\widehat{\\mathbf{h}}_G = h^* (G) = \\mathbf{h}_c$ .\n\n\nThe proof is included in Appendix D. In the proof of Theorem 4.2, we show that  $\\mathcal{L}_{inv}$  will guide the ERM-learned features to focus on invariant features by removing or unlearning the spurious ones. More concretely, given ERM learns both invariant and spurious features (Kirichenko et al., 2023; Chen et al., 2023b), i.e.,  $\\widehat{\\mathbf{h}}_G = \\kappa (\\mathbf{h}_c,\\mathbf{h}_s)$ , where  $\\kappa (\\cdot)$  is a functional mapping to graph representation  $\\widehat{\\mathbf{h}}_G$  given a set of invariant features  $\\mathbf{h}_c$  and a set of spurious features  $\\mathbf{h}_s$ ,  $\\mathcal{L}_{inv}$  regularizes the learning process to optimize  $\\kappa^{*}(\\cdot)$  such that  $\\widehat{\\mathbf{h}}_G$  contains only  $\\mathbf{h}_c$ . While previous OOD methods achieve a similar goal (to learn only invariant features), our approach differs by first learning the spurious features and then leveraging them to identify the causal features from the ERM-learned features, a process we refer to as spurious feature removing or unlearning.\n\n\nTable 1: Performance on synthetic and real-world datasets. Numbers in bold indicate the best performance, while the underlined numbers indicate the second best performance.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GOOD-Motif</td><td colspan=\"2\">GOOD-HIV</td><td colspan=\"2\">OGBG-Molbace</td><td colspan=\"2\">OGBG-Molbbp</td></tr><tr><td>base</td><td>size</td><td>scaffold</td><td>size</td><td>scaffold</td><td>size</td><td>scaffold</td><td>size</td></tr><tr><td>ERM</td><td>68.66\u00b14.25</td><td>51.74\u00b12.88</td><td>69.58\u00b12.51</td><td>59.94\u00b12.37</td><td>75.11\u00b13.03</td><td>83.60\u00b13.47</td><td>68.10\u00b11.68</td><td>78.29\u00b13.76</td></tr><tr><td>IRM</td><td>70.65\u00b14.17</td><td>51.41\u00b13.78</td><td>67.97\u00b11.84</td><td>59.00\u00b12.92</td><td>75.47\u00b12.22</td><td>83.12\u00b12.58</td><td>67.22\u00b11.15</td><td>77.56\u00b12.48</td></tr><tr><td>GroupDRO</td><td>68.24\u00b18.92</td><td>51.95\u00b15.86</td><td>70.64\u00b12.57</td><td>58.98\u00b12.16</td><td>-</td><td>-</td><td>66.47\u00b12.39</td><td>79.27\u00b12.43</td></tr><tr><td>VREx</td><td>71.47\u00b16.69</td><td>52.67\u00b15.54</td><td>70.77\u00b12.84</td><td>58.53\u00b12.88</td><td>72.81\u00b14.29</td><td>82.55\u00b12.51</td><td>68.74\u00b11.03</td><td>78.76\u00b12.37</td></tr><tr><td>RSC</td><td>46.12\u00b13.76</td><td>51.70\u00b15.47</td><td>69.16\u00b13.23</td><td>61.17\u00b10.74</td><td>74.59\u00b13.65</td><td>84.34\u00b12.65</td><td>69.01\u00b12.84</td><td>78.07\u00b13.89</td></tr><tr><td>DiverseModel</td><td>54.24\u00b18.22</td><td>41.01\u00b11.98</td><td>69.17\u00b13.62</td><td>61.59\u00b12.23</td><td>73.48\u00b13.56</td><td>79.40\u00b11.70</td><td>68.04\u00b13.27</td><td>77.62\u00b11.90</td></tr><tr><td>DropEdge</td><td>45.08\u00b14.46</td><td>45.63\u00b14.61</td><td>70.78\u00b11.38</td><td>58.53\u00b11.26</td><td>70.81\u00b12.12</td><td>76.39\u00b12.29</td><td>66.49\u00b11.55</td><td>78.32\u00b13.44</td></tr><tr><td>FLAG</td><td>61.12\u00b15.39</td><td>51.66\u00b14.14</td><td>68.45\u00b12.30</td><td>60.59\u00b12.95</td><td>80.37\u00b11.58</td><td>84.72\u00b10.88</td><td>67.69\u00b12.36</td><td>79.26\u00b12.26</td></tr><tr><td>LiSA</td><td>54.59\u00b14.81</td><td>53.46\u00b13.41</td><td>70.38\u00b11.45</td><td>52.36\u00b13.73</td><td>78.05\u00b15.01</td><td>83.92\u00b12.52</td><td>68.11\u00b10.52</td><td>78.62\u00b13.74</td></tr><tr><td>DIR</td><td>62.07\u00b18.75</td><td>52.27\u00b14.56</td><td>68.07\u00b12.29</td><td>58.08\u00b12.31</td><td>75.49\u00b12.80</td><td>77.42\u00b17.43</td><td>66.86\u00b12.25</td><td>76.40\u00b14.43</td></tr><tr><td>DisC</td><td>51.08\u00b13.08</td><td>50.39\u00b11.15</td><td>68.07\u00b11.75</td><td>58.76\u00b10.91</td><td>57.78\u00b13.60</td><td>71.13\u00b18.86</td><td>67.12\u00b12.11</td><td>56.59\u00b110.09</td></tr><tr><td>CAL</td><td>65.63\u00b14.29</td><td>51.18\u00b15.60</td><td>67.37\u00b13.61</td><td>57.95\u00b12.24</td><td>76.29\u00b11.60</td><td>79.68\u00b14.06</td><td>68.06\u00b12.60</td><td>79.50\u00b14.81</td></tr><tr><td>GREA</td><td>56.74\u00b19.23</td><td>54.13\u00b110.02</td><td>67.79\u00b12.56</td><td>60.71\u00b12.20</td><td>77.16\u00b11.37</td><td>83.15\u00b19.07</td><td>69.72\u00b11.66</td><td>77.34\u00b13.52</td></tr><tr><td>GSAT</td><td>62.80\u00b111.41</td><td>53.20\u00b18.35</td><td>68.66\u00b11.35</td><td>58.06\u00b11.98</td><td>72.32\u00b15.66</td><td>82.45\u00b12.73</td><td>66.78\u00b11.45</td><td>75.63\u00b13.83</td></tr><tr><td>CIGA</td><td>66.43\u00b111.31</td><td>49.14\u00b18.34</td><td>69.40\u00b12.39</td><td>59.55\u00b12.56</td><td>76.44\u00b11.72</td><td>83.95\u00b12.75</td><td>64.92\u00b12.09</td><td>65.98\u00b13.31</td></tr><tr><td>AIA</td><td>73.64\u00b15.15</td><td>55.85\u00b17.98</td><td>71.15\u00b11.81</td><td>61.64\u00b13.37</td><td>79.42\u00b12.01</td><td>85.11\u00b10.74</td><td>70.79\u00b11.53</td><td>81.03\u00b15.15</td></tr><tr><td>OOD-GCL</td><td>56.46\u00b14.61</td><td>60.23\u00b18.49</td><td>70.85\u00b12.07</td><td>58.48\u00b12.94</td><td>75.96\u00b12.21</td><td>85.34\u00b11.77</td><td>67.28\u00b13.09</td><td>78.11\u00b13.32</td></tr><tr><td>EQuAD</td><td>67.11\u00b110.11</td><td>59.72\u00b13.69</td><td>72.24\u00b10.64</td><td>64.19\u00b10.56</td><td>79.15\u00b12.32</td><td>86.41\u00b15.63</td><td>70.22\u00b12.36</td><td>80.82\u00b15.28</td></tr><tr><td>LIRS</td><td>75.51\u00b12.19</td><td>74.95\u00b17.69</td><td>72.82\u00b11.61</td><td>66.64\u00b11.44</td><td>81.91\u00b11.98</td><td>88.77\u00b11.64</td><td>71.04\u00b10.76</td><td>82.19\u00b11.57</td></tr></table>\n\n\n# 5 EXPERIMENTS\n\n\nIn this section, we perform empirical study on both synthetic and real-world datasets. More details about the datasets and experiment setup are included in Appendix H.\n\n\n# 5.1 EXPERIMENTAL SETUP\n\n\nDatasets. We adopt GOOMotif and GOODHIV datasets (Gui et al., 2022), OGBG-Molbace and OGBG-Molbbp datasets (Hu et al., 2020; Wu et al., 2018) to comprehensively evaluate the OOD generalization performance of our proposed framework. For GOOMotif datasets, we adopt base shift and size shift, for OBGG datasets, in addition to scaffold shift, we also create size shift, following previous studies (Gui et al., 2022; Sui et al., 2023). More details on the datasets used in our work are included in Appendix H.\n\n\nBaselines. Besides ERM (Vapnik, 1995), we compare our method against three lines of OOD baselines: (1) OOD algorithms on Euclidean data, including IRM (Arjovsky et al., 2020), VREx (Krueger et al., 2021), and GroupDRO (Sagawa et al., 2019); (2) Diverse feature learning methods, including RSC (Huang et al., 2020), and DiverseModel (Teney et al., 2022). (3) graph-specific OOD and data augmentation algorithms without requiring environment labels, including DIR (Wu et al., 2022c), GSAT (Miao et al., 2022), GREA (Liu et al., 2022), DisC (Fan et al., 2022), CIGA (Chen et al., 2022), AIA (Sui et al., 2023), OOD-GCL (Li et al., 2024a), EQuAD (Yao et al., 2024), DropEdge (Rong et al., 2019), FLAG (Kong et al., 2022), and LiSA (Yu et al., 2023). Details of the baselines and their implementation are provided in Appendix H.\n\n\nEvaluation. We report the ROC-AUC score for GOOD-HIV, OGBG-Molbbbp, and OGBG-Molbace datasets, where the tasks are binary classification. For GOOD-Motif and SPMotif datasets, we use accuracy as the evaluation metric. We run experiments 4 times with different random seeds, select models based on the validation performance, and report the mean and standard deviations on the test set.\n\n\n# 5.2 EXPERIMENTAL RESULTS\n\n\nIn this section, we report the main results on both synthetic and real-world datasets.\n\n\nSynthetic datasets. For the GOOD-Motif datasets, LIRS demonstrates its superiority by outperforming the second-best method by  $12.51\\%$  and  $25.50\\%$  in the base and size split respectively. This substantial margin highlights the effectiveness of LIRS in capturing domain-invariant features across different types of distribution shifts. Although EQuAD adopts a similar learning paradigm\n\n\nby attempting to remove spurious features, LIRS outperforms it by  $25.50\\%$  in the size split of the GOOD-Motif datasets. The underlying reason for this performance gap is that in the training set of GOOD-Motif, the spurious subgraph  $G_{s}$  occupies a significant portion of the graph. The infomax principle adopted in EQuAD can inadvertently affect the learning of spurious features and hinder the subsequent invariant learning process. In contrast, LIRS adopts biased infomax, which facilitates the suppression of invariant signals, thereby allowing it to learn spurious features more effectively. OOD-GCL (Li et al., 2024a) also utilizes contrastive learning to learn invariant features, however, it aims to learn invariant features without labeled data, which is inherently more challenging. As illustrated in Table 1, LIRS outperforms OOD-GCL across all the datasets, likely due to that LIRS has access to labeled data during training, while OOD-GCL uses labeled data only to fine-tune the linear layer. We also compare LIRS with RSC Huang et al. (2020) and DiverseModel Teney et al. (2022), which aim to learn diverse features via ERM. LIRS can be viewed as a diverse feature learning method, as it seeks to learn more invariant features by removing spurious ones. The key distinction between LIRS and these two methods lies in that: while RSC and DiverseModel propose different regularization terms to ERM to capture diverse features, LIRS focuses on removing spurious features from ERM-learned features. As illustrated in Table 1, although RSC and DiverseModel perform well on real-world datasets, they achieve suboptimal results on the GOOD-Motif datasets. This discrepancy may arise from the fact that these methods attempt to capture diverse features, while in the GOOD-Motif datasets, only one invariant subgraph is causally related to the target. As a result, these methods may mistakenly capture non-generalizable patterns in  $G_{s}$ . In contrast, LIRS leverages biased infomax to effectively learn spurious patterns and subsequently identify the invariant subgraph by unlearning the spurious patterns. Lastly, LIRS significantly outperforms other OOD methods that aim to directly learn invariant features, showcasing the efficacy of the proposed learning paradigm.\n\n\nReal-world datasets. In real-world datasets, which present more complex and realistic distribution shifts, many graph OOD algorithms exhibit instability, occasionally underperforming ERM. In contrast, LIRS adopts an indirect learning paradigm, which may enable the learning of more invariant features compared to previous graph invariant learning methods, leading to more effective OOD generalization. Similarly to the results on synthetic datasets, LIRS also demonstrates superior OOD performance under both scaffold shift and size shift compared to other methods. EQuAD, which adopts a similar learning paradigm as LIRS, shows a comparable trend, with a more significant advantage under the size shift relative to other methods. Furthermore, the improvements of LIRS over EQuAD across various datasets highlights the effectiveness of the biased infomax principle and the intra-class cross-entropy loss for learning spurious features and invariant features respectively. Compared to other baselines that learns invariant features directly, LIRS outperforms the best method (AIA) by  $4.62\\%$  in size shift and  $1.95\\%$  in scaffold shift respectively.\n\n\n# 5.3 IN-DEPTH ANALYSIS\n\n\nCan LIRS learn more invariant features? To further validate the quality of invariant features learned by LIRS, we curated a dataset derived from the SPMotif dataset. Specifically, we constructed a binary classification dataset where the motifs House and Crane corresponds to label 0, and Diamond and Cycle corresponds to label 1. During the construction of each class's samples, we attached both invariant subgraphs to the base subgraph with  $50\\%$  chance, while in the remaining  $50\\%$ , we randomly attached one invariant subgraph to the base spurious subgraph. For the test set, we randomly attached a single invariant subgraph to the base subgraph. Similar to the SPMotif dataset, the base spurious subgraph was correlated with the target la\n\n\nTable 2: Performance on the SPMotif-binary datasets under varying spurious correlations.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">SPMotif-binary</td></tr><tr><td>b=0.40</td><td>b=0.60</td><td>b=0.90</td></tr><tr><td>ERM</td><td>74.93\u00b13.94</td><td>72.78\u00b13.15</td><td>63.78\u00b14.18</td></tr><tr><td>IRM</td><td>76.01\u00b14.12</td><td>70.85\u00b14.73</td><td>66.55\u00b14.80</td></tr><tr><td>VREx</td><td>79.03\u00b11.02</td><td>73.78\u00b11.75</td><td>65.27\u00b16.78</td></tr><tr><td>GIL</td><td>77.15\u00b13.18</td><td>73.85\u00b12.76</td><td>68.90\u00b17.28</td></tr><tr><td>GREA</td><td>79.65\u00b16.36</td><td>73.01\u00b17.99</td><td>69.85\u00b10.35</td></tr><tr><td>CIGA</td><td>76.93\u00b13.94</td><td>71.70\u00b11.55</td><td>66.80\u00b15.35</td></tr><tr><td>AIA</td><td>78.46\u00b13.19</td><td>71.83\u00b10.69</td><td>64.37\u00b14.14</td></tr><tr><td>EQuAD</td><td>80.82\u00b10.65</td><td>74.20\u00b14.10</td><td>69.79\u00b17.81</td></tr><tr><td>LIRS</td><td>82.17\u00b10.91</td><td>75.32\u00b11.65</td><td>71.29\u00b12.12</td></tr></table>\n\n\nbels, maintaining an equal correlation strength with the labels in the test set. In addition to evaluating LIRS against traditional methods such as IRM, VRex and their variants, we also incorporated strong graph-specific OOD baselines, including CIGA, AIA, and EQuAD for comparisons. The ex\n\n\nExperimental results in Table 2 demonstrate that LIRS consistently outperforms other baselines across varying spurious correlation strengths, indicating its superior ability to learn more invariant features. Similarly, EQuAD, adopting a similar learning paradigm, also achieves competitive performance in SPMotif-binary datasets. This demonstrates that learning invariant features indirectly can lead to learning a more comprehensive set of invariant features and achieving better OOD generalization ability.\n\n\n![(a) Ablation study on biased infomax and intra-class cross-entropy loss. Figure 4: Ablation and hyperparameter sensitivity studies for LIRS.](images/52c4afbb88b891862759bcd6da85f0b71a7dad421c9d229e0a3bd4571a991717.jpg)\n\n\n![(b) Hyperparameter sensitivity analysis on GOODHIV datasets.](images/6380874ca18b68e126048cd9d1e76f038db57e853e70256eadb6f1e8b7d4bbda.jpg)\n\n\nAblation study. We conducted an ablation study on biased infomax and class-conditioned (intra-class) cross-entropy loss to evaluate their effectiveness in LIRS. As illustrated in Figure 4a, replacing biased infomax with vanilla infomax to generate spurious features leads to performance degradation across all datasets. This decline is primarily due to the enhanced capability of biased infomax in learning spurious features, which is crucial for subsequent steps for learning graph invariance. Furthermore, replacing class-conditioned cross-entropy loss with standard cross-entropy loss also led to negative effects, which is mainly due to the interference of spurious patterns across different classes, as discussed in Sec. 4. One typical empirical support arises from the Motif-size dataset, where each spurious pattern is correlated with different target labels with nearly equal strength. As a result, even though biased infomax can generate accurate spurious features, the logits generated using the standard cross-entropy loss are not sufficiently precise, which aligns with Prop. 2. This limitation significantly constrains the test performance, as illustrated in Figure 4a. In contrast, the class-conditioned cross-entropy loss is able to generate more accurate spurious logits, thus enhancing the OOD performance.\n\n\nHyperparameter Sensitivity. We investigate the impact of hyperparameters in LIRS, including the reweighting coefficient  $\\gamma$ , the penalty weights  $\\lambda$ , and hyperparameters related to biased infomax. Specifically, we analyze the effect of the epoch  $E$  at which embeddings are derived from biased infomax and the threshold used to determine whether a sample should be considered biased or not. As illustrated in Figure 4b, for both the GOODHIV-Sca and GOODHIV-Size datasets, LIRS demonstrates stable performance, highlighting its robustness to variations in hyperparameter settings. This consistency suggests that LIRS is agnostic to hyperparameter tuning, which further emphasizes its applicability in real-world scenarios.\n\n\nHow do different size split methods affect the OOD performance? For the real-world datasets, the size split was performed in descending order, where the graphs in the training set have a larger number of nodes compared\n\n\nto those in the test set. To evaluate the impact of varying graph sizes on generalization ability of various OOD methods, we also performed size splits based on ascending order for the OGBG-\n\n\nTable 3: Performance on OGBG datasets using a different size split procedure, where the graphs in the training set have a smaller number of nodes compared to those in the test set.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">OGBG Datasets</td></tr><tr><td>OGBG-Molbace</td><td>OGBG-Molbbbp</td></tr><tr><td>ERM</td><td>76.31\u00b10.58</td><td>87.31\u00b12.37</td></tr><tr><td>IRM</td><td>79.44\u00b13.05</td><td>88.77\u00b12.45</td></tr><tr><td>VREx</td><td>76.38\u00b11.75</td><td>84.20\u00b12.80</td></tr><tr><td>GREA</td><td>77.46\u00b15.57</td><td>86.18\u00b12.54</td></tr><tr><td>GSAT</td><td>72.29\u00b14.45</td><td>87.46\u00b12.67</td></tr><tr><td>CIGA</td><td>77.89\u00b13.68</td><td>87.94\u00b10.86</td></tr><tr><td>AIA</td><td>78.62\u00b12.88</td><td>89.18\u00b11.77</td></tr><tr><td>EQuAD</td><td>82.34\u00b13.42</td><td>88.65\u00b13.83</td></tr><tr><td>LIRS</td><td>84.62\u00b10.84</td><td>90.24\u00b14.14</td></tr></table>\n\n\nMolbace and OGBG-Molbbb datasets. The results of these experiments are presented in Table 3. As shown, LIRS continues to outperform all competitive baselines under this alternative size split. Specifically, LIRS achieves the highest ROC-AUC scores on both OGBG-Molbace and OGBG-Molbbb datasets. This highlights the superiority of LIRS in generalizing to larger graphs when trained on smaller ones, further confirming its potential for real-world applications.\n\n\n# 6 RELATED WORK\n\n\nLearning invariant features. OOD generalization is a critical challenge in machine learning, where models trained on a specific data distribution often fail to generalize well to unseen distributions. Recently invariance learning has been proposed to tackle this issue, which builds upon the theory of causality (Peters et al., 2016; Pearl, 2009) to learn causally-related representation that remain stable across different environments (Arjovsky et al., 2020; Parascandolo et al., 2020; Mahajan et al., 2021; Wald et al., 2021; Ahuja et al., 2020; 2021). Inspired from IRM (Arjovsky et al., 2020), several invariant learning methods on graphs are proposed (Yang et al., 2022; Li et al., 2022b; Fan et al., 2022; Liu et al., 2022; Wu et al., 2022c; Chen et al., 2022; 2023a; Gui et al., 2023; Sui et al., 2023; Li et al., 2024b) to learn graph-level representations that are robust to distribution shifts. Most of these methods aim to learn invariant features directly by training an equipredictive classifier. Recent study (Yao et al., 2024) has proposed a new learning paradigm to learn graph invariance indirectly by learning spurious features first, then disentangle them from the ERM-learned representations. In this work, we adopt this learning paradigm for learning graph invariance.\n\n\nSelf-supervised learning induces spuriosity. Self-supervised learning (SSL) has emerged as a potent paradigm for learning representations from unlabeled datasets. The fundamental concept involves devising pretext tasks that maximize similarity between two augmented views of the same data point, often utilizing the InfoNCE loss (Oord et al., 2018). This approach has been widely applied in domains such as images (Chen et al., 2020a;b) and graphs (Velickovi\u0107 et al., 2019; Sun et al., 2019; Zhu et al., 2020; You et al., 2020). Recent work have shown that SSL may tend to learn spurious features in both image domain (Hamidieh et al., 2024; Meehan et al., 2023) and graph domain (Yao et al., 2024). Hamidieh et al. (2024) attempted to enhance SSL model performance by addressing this issue and promoting the learning of invariant features. In contrast, our goal in this work is to improve SSL model ability to capture spuriosity, thereby facilitating better feature disentanglement for invariant learning.\n\n\nLearning diverse features. Recent studies have shown that ERM tends to encourage models to learn the simplest predictive features (Hermann & Lampinen, 2020; Kalimeris et al., 2019; Neyshabur et al., 2014; Pezeshki et al., 2021). This simplicity bias causes the models to rely on simple (spurious) but non-causal features, ignoring more complex patterns that might be equally predictive. To address this challenge, Huang et al. (2020) employs a self-challenging mechanism to force the model to learn diverse patterns by discarding dominant features, while Teney et al. (2022) constructs diverse features by training a collection of classifiers with diversity regularization. Additionally, Zhang et al. (2022a); Chen et al. (2023b) adopt DRO to iteratively explore new features. While previous works aim to encourage models to learn a diverse set of predictive patterns, our method takes a different approach. Specifically, we aim to remove or unlearn spurious features from ERM-learned features, thereby allowing the model to capture more invariant features.\n\n\n# 7 CONCLUSIONS\n\n\nIn this study, we identified a critical limitation in IRM, VRex and their variants on graph invariant learning, i.e., these methods may only capture a subset of the invariant features, thereby limiting their OOD generalization performance. To address this issue, we investigate the effectiveness of learning invariant features indirectly by first learning and then removing spurious features. Our theoretical and empirical analyses demonstrate that this approach facilitates the learning of a more comprehensive set of invariant features than traditional (graph) invariant learning methods, thereby improving generalization to unseen environments. We then propose LIRS that adopts this learning paradigm, which consists of: a) The biased infomax principle, and b) The class-conditioned cross-entropy loss, which elicit effective spuriosity learning and invariant learning respectively, aiming to learn more invariant features. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of our proposed method over existing state-of-the-art OOD algorithms.\n\n\n# REFERENCES\n\n- Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. In International Conference on Machine Learning, pp. 145-155. PMLR, 2020.\n\n- Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. Advances in Neural Information Processing Systems, 34:3438-3450, 2021.\n\n- Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization, 2020.\n\n- Steve Azzolin, Antonio Longa, Pietro Barbiero, Pietro Li\u00f2, and Andrea Passerini. Global explainability of gnns via logic combination of learned concepts. arXiv preprint arXiv:2210.07147, 2022.\n\n- Guy W Bemis and Mark A Murcko. The properties of known drugs. 1. molecular frameworks. Journal of medicinal chemistry, 39(15):2887-2893, 1996.\n\n- Ngoc Bui, Hieu Trung Nguyen, Viet Anh Nguyen, and Rex Ying. Explaining graph neural networks via structure-aware interaction index. arXiv preprint arXiv:2405.14352, 2024.\n\n- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. PMLR, 2020a.\n\n- Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.\n\n- Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, MA Kaili, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-distribution generalization on graphs. Advances in Neural Information Processing Systems, 35:22131-22148, 2022.\n\n- Yongqiang Chen, Yatao Bian, Kaiwen Zhou, Binghui Xie, Bo Han, and James Cheng. Does invariant graph learning via environment augmentation learn invariance? In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openreview.net/forum?id=EqpR9Vtt13.\n\n- Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, and James Cheng. Understanding and improving feature learning for out-of-distribution generalization. Advances in Neural Information Processing Systems, 36, 2023b.\n\n- Alex J. DeGrave, Joseph D. Janizek, and Su-In Lee. Ai for radiographic covid-19 detection selects shortcuts over signal. medRxiv, 2020. URL https://api(semanticscholar.org/ CorpusID:221653623.\n\n- Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. Advances in neural information processing systems, 32, 2019.\n\n- Cian Eastwood, Shashank Singh, Andrei L Nicolicioiu, Marin Vlastelica Pogan\u010di\u0107, Julius von K\u00fcgelgen, and Bernhard Sch\u00f6lkopf. Spuriosity didn't kill the classifier: Using invariant predictions to harness spurious features. Advances in Neural Information Processing Systems, 36, 2023.\n\n- Shaohua Fan, Xiao Wang, Yanhu Mo, Chuan Shi, and Jian Tang. Debiasing graph neural networks via learning disentangled causal substructure. Advances in Neural Information Processing Systems, 35:24934-24946, 2022.\n\n- Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric, 2019.\n\n- Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of machine learning research, 17(59):1-35, 2016.\n\n- Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=8hHg-zs_p-h.\n\n- Shurui Gui, Meng Liu, Xiner Li, Youzhi Luo, and Shuiwang Ji. Joint learning of label and environment causal independence for graph out-of-distribution generalization. Advances in Neural Information Processing Systems, 36, 2023.\n\n- Kimia Hamidieh, Haoran Zhang, Swami Sankaranarayanan, and Marzyeh Ghassemi. Views can be deceiving: Improved SSL through feature space augmentation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=mutJBk3ILg.\n\n- Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In International conference on machine learning, pp. 4116-4126. PMLR, 2020.\n\n- Katherine Hermann and Andrew Lampinen. What shapes feature representations? exploring datasets, architectures, and training. Advances in Neural Information Processing Systems, 33: 9995-10006, 2020.\n\n- R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization, 2019.\n\n- Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give robust classifiers? In International Conference on Machine Learning, pp. 2029-2037. PMLR, 2018.\n\n- Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118-22133, 2020.\n\n- Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. arXiv preprint arXiv:2102.09548, 2021.\n\n- Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In Computer vision-ECCV 2020: 16th European conference, Glasgow, UK, August 23-28, 2020, proceedings, part II 16, pp. 124-140. Springer, 2020.\n\n- Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Long-Kai Huang, Tingyang Xu, Yu Rong, Lanqing Li, Jie Ren, Ding Xue, et al. Drugood: Out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discovery-a focus on affinity prediction problems with noise annotations. arXiv preprint arXiv:2201.09637, 2022.\n\n- Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in neural information processing systems, 32, 2019.\n\n- Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:18661-18673, 2020.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\n- Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=SJU4ayYgl.\n\n- Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations, 2023.\n\n- Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal-subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International conference on machine learning, pp. 5637-5664. PMLR, 2021.\n\n- Kezhi Kong, Guohao Li, Muong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. Robust optimization as data augmentation for large-scale graphs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 60-69, 2022.\n\n- Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L\u00e9tourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. Advances in Neural Information Processing Systems, 34:21618-21629, 2021.\n\n- David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In International Conference on Machine Learning, pp. 5815-5826. PMLR, 2021.\n\n- Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Ood-gnn: Out-of-distribution generalized graph neural network. IEEE Transactions on Knowledge and Data Engineering, 2022a.\n\n- Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning invariant graph representations for out-of-distribution generalization. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022b. URL https://openreview.net/forum?id=acKK8MQe2xc.\n\n- Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Invariant node representation learning under distribution shifts with multiple latent environments. ACM Transactions on Information Systems, 42(1):1-30, 2023a.\n\n- Haoyang Li, Xin Wang, Zeyang Zhang, Haibo Chen, Ziwei Zhang, and Wenwu Zhu. Disentangled graph self-supervised learning for out-of-distribution generalization. In *Forty-first International Conference on Machine Learning*, 2024a.\n\n- Xiner Li, Shurui Gui, Youzhi Luo, and Shuiwang Ji. Graph structure and feature extrapolation for out-of-distribution generalization. arXiv preprint arXiv:2306.08076, 2023b.\n\n- Xiner Li, Shurui Gui, Youzhi Luo, and Shuiwang Ji. Graph structure extrapolation for out-of-distribution generalization. In *Forty-first International Conference on Machine Learning*, 2024b. URL https://openreview.net/forum?id=Xgrey8uQhr.\n\n- Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European conference on computer vision (ECCV), pp. 624-639, 2018.\n\n- R. Linsker. Self-organization in a perceptual network. Computer, 21(3):105-117, 1988. doi: 10. 1109/2.36.\n\n- Gang Liu, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Graph rationalization with environment-based augmentations. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '22. ACM, August 2022. doi: 10.1145/3534678.3539347. URL http://dx.doi.org/10.1145/3534678.3539347.\n\n- Yang Liu, Xiang Ao, Fuli Feng, Yunshan Ma, Kuan Li, Tat-Seng Chua, and Qing He. Flood: A flexible invariant learning framework for out-of-distribution generalization on graphs. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1548-1558, 2023.\n\n- Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Parameterized explainer for graph neural network. Advances in neural information processing systems, 33:19620-19631, 2020.\n\n- Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In International conference on machine learning, pp. 7313-7324. PMLR, 2021.\n\n- Casey Meehan, Florian Bordes, Pascal Vincent, Kamalika Chaudhuri, and Chuan Guo. Do ssl models have d\u00e9j\u00e0 vu? a case of unintended memorization in self-supervised learning. Advances in Neural Information Processing Systems, 36, 2023.\n\n- Siqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. In International Conference on Machine Learning, pp. 15524-15543. PMLR, 2022.\n\n- Hongseok Namkoong and John C. Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS'16, pp. 2216-2224, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819.\n\n- Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.\n\n- Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n\n- Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, and Bernhard Scholkopf. Learning explanations that are hard to vary. arXiv preprint arXiv:2009.00329, 2020.\n\n- Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.\n\n- Judea Pearl. Causality. Cambridge University Press, Cambridge, UK, 2 edition, 2009. ISBN 978-0-521-89560-6. doi: 10.1017/CBO9780511803161.\n\n- F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.\n\n- Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(5):947-1012, 2016.\n\n- Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. Advances in Neural Information Processing Systems, 34:1256-1272, 2021.\n\n- Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainability methods for graph convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10772-10781, 2019.\n\n- Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. Deepinf: Social influence prediction with deep learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery amp; Data Mining, KDD '18. ACM, July 2018. doi: 10.1145/3219819.3220077. URL http://dx.doi.org/10.1145/3219819.3220077.\n\n- Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Droppedge: Towards deep graph convolutional networks on node classification. arXiv preprint arXiv:1907.10903, 2019.\n\n- Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.\n\n- Thomas Schnake, Oliver Eberle, Jonas Lederer, Shinichi Nakajima, Kristof T Sch\u00fctt, Klaus-Robert M\u00fcller, and Gr\u00e9goire Montavon. Higher-order explanations of graph neural networks via relevant walks. IEEE transactions on pattern analysis and machine intelligence, 44(11):7581-7596, 2021.\n\n- Yongduo Sui, Qitian Wu, Jiancan Wu, Qing Cui, Longfei Li, Jun Zhou, Xiang Wang, and Xiangnan He. Unleashing the power of graph data augmentation on covariate distribution shift. Advances in Neural Information Processing Systems, 36, 2023.\n\n- Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation, 2016.\n\n- Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. arXiv preprint arXiv:1908.01000, 2019.\n\n- Damien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton Van den Hengel. Evading the simplicity bias: Training a diverse set of models discovers solutions with superior ood generalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16761-16772, 2022.\n\n- George R Terrell and David W Scott. Variable kernel density estimation. The Annals of Statistics, pp. 1236-1265, 1992.\n\n- Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle, 2015.\n\n- Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\n\n- Vladimir N. Vapnik. The nature of statistical learning theory. Springer-Verlag New York, Inc., 1995. ISBN 0-387-94559-8.\n\n- Petar Velickovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\n- Petar Velickovi\u0107, William Fedus, William L Hamilton, Pietro Li\u00f2, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.\n\n- Petar Velickovi\u0107, William Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rklz9iAcKQ.\n\n- Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain generalization. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=XWYJ25-yTRS.\n\n- Xiaoqi Wang and Han-Wei Shen. Gnninterpreter: A probabilistic generative model-level explanation for graph neural networks. arXiv preprint arXiv:2209.07924, 2022.\n\n- Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. Handling distribution shifts on graphs: An invariance perspective, 2022a.\n\n- Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in recommender systems: A survey. ACM Comput. Surv., 55(5), dec 2022b. ISSN 0360-0300. doi: 10.1145/3535101. URL https://doi.org/10.1145/3535101.\n\n- Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. In International Conference on Learning Representations, 2022c. URL https://openreview.net/forum?id=hGXij5rfiHw.\n\n- Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Genisses, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. Moleculenet: A benchmark for molecular machine learning, 2018.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636552.3882835", "msecs": "388.0", "relativeCreated": "13081.423997879028", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws\n\n# Abstract\nThe composition of pretraining data is a key determinant of foundation models' performance, but there is no standard guideline for allocating a limited computational budget across different data sources. Most current approaches either rely on extensive experiments with smaller models or dynamic data adjustments that also require proxy models, both of which significantly increase the workflow complexity and computational overhead. In this paper, we introduce Adaptive Data Optimization (ADO), an algorithm that optimizes data distributions in an online fashion, concurrent with model training. Unlike existing techniques, ADO does not require external knowledge, proxy models, or modifications to the model update. Instead, ADO uses per-domain scaling laws to estimate the learning potential of each domain during training and adjusts the data mixture accordingly, making it more scalable and easier to integrate. Experiments demonstrate that ADO can achieve comparable or better performance than prior methods while maintaining computational efficiency across different computation scales, offering a practical solution for dynamically adjusting data distribution without sacrificing flexibility or increasing costs. Beyond its practical benefits, ADO also provides a new perspective on data collection strategies via scaling laws.\n\n\n# 1 INTRODUCTION\n\n\nFoundation models, large neural networks pre-trained on vast amounts of data, are the backbone for a wide range of today's machine learning workload (Bommasani et al., 2021). It is well established that the composition of the pretraining data plays a crucial role in these models' final performance (Gadre et al., 2024); however, there are currently no standard guidelines for selecting good pretraining data. Most existing datasets are filtered and curated snapshots of the web data (Gao et al., 2020; Penedo et al., 2024), often categorized into distinct heterogeneous domains (e.g., CommonCrawl and Github). Even after filtering, a challenge remains: deciding how to allocate computational resources across these different domains. Given the ever increasing cost of pretraining large foundation models (Sharir et al., 2020), optimizing the data composition is a potential avenue for improving performance without increasing computational cost.\n\n\nThere are two popular approaches to adjusting the data distribution. A straightforward approach to this problem is to experiment with smaller models, test different data policies, and then apply the best-performing policy to the larger model (Ye et al., 2024; Ge et al., 2024). However, this strategy has several drawbacks. First, as the number of domains increases, the cost of training on all possible policies can grow linearly or even exponentially (if the level of discretization remains the same), making even small-scale experiments costly. Second, the optimal data policy for a smaller model does not necessarily generalize to larger models (Ye et al., 2024; Albalak et al., 2023). Another approach involves dynamically adjusting the data policy during training based on various statistics (Xie et al., 2024; Chen et al., 2024; Qin et al., 2024; Fan et al., 2023). However, these methods often require training a smaller proxy model to guide the data policy, whose cost is still non-negligible.\n\n\nA significant limitation of almost all existing methods (with the exception of Albalak et al. (2023)) is their requirement for additional computational steps beyond standard training. This extra complexity entails substantial modifications to the training pipeline, making it challenging to integrate these methods when other components are varied (e.g., the architecture, tokenizer, or optimizer). The consequence of this usage barrier is that these methods have not been tested widely, making them less\n\n\n![Figure 1: ADO is a cheap online technique for adjusting the data distribution while training large models. In contrast to prior methods, ADO tailors its data distribution to the model as it is being trained, and does not require training smaller proxy models in advance.](images/9697384cb73119500dd1ce99a205ab4ad357ee227801b9967a8dda180e28b851.jpg)\n\n\n![Multi-staged data selection with proxy models Adaptive Data Optimization (ADO)](images/9c989d2e48a57b7462591bc2ea0bf1644160cd19b1802b08a81a57fe0c12afcc.jpg)\n\n\npopular for those training a new model. To enable broader adoption of data mixture optimization, we argue that it must be implemented in an online fashion, concurrent with model training, without disrupting the standard training process, which means that the data mixture must adapt itself based on the feedback from the model. This is closely related to curriculum learning (Bengio et al., 2009), a training strategy where models are progressively exposed to domains in a curated order.\n\n\nAt a fundamental level, this work develops and empirically investigates questions about the existence of good pretraining curricula and the feasibility of cheaply identifying them. We first demonstrate in controlled experiments that good curricula can generally be found with more computation and that it is hard to accurately predict larger models' performance with only a few small models. This leads to our main contribution, Adaptive Data Optimization (ADO): an algorithm for adaptively adjusting the data mixture online during training (Figure 1). In experiments on language models up to 1.3B parameters trained on the Pile (Gao et al., 2020), we find that ADO improves performance across a variety of common benchmarks and improves validation loss on SlimPajama (Soboleva et al., 2023) and FineWeb (Penedo et al., 2023), both of which are considered to be higher-quality datasets. Most importantly, ADO achieves these without requiring significant additional computation (less than  $0.4\\%$  wallclock time for 1.3B), proxy models, or extensive modification to the training pipelines.\n\n\n# 2 MOTIVATIONS\n\n\nThe drawbacks of small proxy models. At first glance, transferring data selection strategies from smaller to larger models seems plausible, with positive results in specific cases, such as in Mindermann et al. (2022) and Xia et al. (2024b). However, as shown in Figure 2, using a small number of models to predict the behavior of larger models can exhibit high variance on the Pythia model family (Biderman et al., 2023), indicating that they cannot reliably forecast larger models' performance. At a per-domain level (Figure 7), the variation becomes even more pronounced. While this does not fully rule out the possibility of transfer, it raises doubts about whether small models can reliably select data mixtures for larger models. Another issue is that the efficacy of distributions generated by proxy models is highly sensitive to factors like the tokenizer and other hyperparameters (Albalak et al., 2023). Our findings also confirm this brittleness (Section 4). This means that the optimized distributions from one experiment might not transfer, and they need to be retrained for a new model. This is undesirable because proxy models add non-trivial overhead \u2014 Ge et al. (2024)\n\n\nestimated that a single round of DoReMi proxy training requires 760 GPU hours on 8 Nvidia A100 GPUs, and several proxy training rounds are needed before training the full model.\n\n\nThe existence of good curricula. Curriculum learning (Bengio et al., 2009) is a conceptually appealing idea that has been regularly studied over the years, but these methods have not been widely\n\n\n![Figure 2: Extrapolating larger models' loss using scaling laws up to a threshold model size, indicated by color. Blue dots correspond to different models. Extrapolations become more accurate with more models.](images/0083870cabf99e4a3f0bdcc0ec6148faadaa488f30861ef6c96b61f101729e4a.jpg)\n\n\n![Figure 3: The average learning curves of SGD and the meta-optimized curricula for a logistic regression (Left) and an MLP (Middle). The meta-optimized curricula consistently outperform SGD. Further, the meta loss for curricula is still decreasing after 1000 steps (Right). The first two figures are averaged over 50 runs and the shaded region corresponds to 1 standard deviation.](images/5d32c8b54d184ada09e4785f7ed86414ff312fba0a725c31b5d3435c348b718d.jpg)\n\n\n![](images/ceb85badf6f0996317ed829beb993b51b646dba869bc0370d5284bbf763ee42e.jpg)\n\n\n![](images/0266dccf9d95578c9841e3671e42df4ffa39289eabaeb0aaf9929caefa948c04.jpg)\n\n\nadopted in deep learning outside of a few relatively niche areas (Graves et al., 2017; Florensa et al., 2017; Jesson et al., 2018; Tang et al., 2018). To the best of our knowledge, there is no consensus on why curriculum learning has not had wide success in deep learning. Some hypothesize that deep learning is different from human learning or that overparameterization makes curriculum less effective (Wu et al., 2021; Mannelli et al., 2024). On the other hand, some experimental evidence suggests that deep neural networks implicitly learn functions of increasing complexity from easy to hard (Kalimeris et al., 2019; Baldock et al., 2021), loosely resembling some aspects of human learning (Lawrence, 1952; Baker & Osgood, 1954; Skinner, 1958). For overparameterization, it is not clear if modern foundation model pretraining is in the overparameterized regime since dataset size generally scales with the number of model parameters (Hoffmann et al., 2022).\n\n\nTo motivate our later work, we propose an alternative hypothesis: good curricula exist but they are computationally difficult to find. Let's start with an instructive experiment of finding good curricula with more computation. Suppose we have a fixed dataset of size  $N$  and want to train SGD at a minibatch size of 1 for 1 epoch (i.e., each data point is used once, which is common for language model pretraining). We are interested in finding a data ordering that is better than a random shuffle when training a model initialized at random. Brute force search is intractable even for a small dataset. Unlike most existing curricula that rely on some prior notions of difficulty, we will find a good curriculum with only training data by optimizing over the space of permutations with meta learning. Due to space constraints, we defer the details of this meta learning procedure to Appendix B.\n\n\nWe apply this procedure to a logistic regression problem as well as a multi-layer perceptron where the goal is to imitate a teacher. We randomly sample the initialization  $\\theta_0$ , ground truth  $\\theta^{\\star}$  and the data  $\\mathcal{Z} = \\{(x_i, f(x_i; \\theta^{\\star}))\\}_{i=1}^{N}$ . For a fixed set of data points, we run the meta optimization for some steps (200 for logistic regression and 1000 for MLP) and evaluate the performance of learned ordering on a new initialization. We repeat this experiment 50 times and report the results in Figure 3. As can be seen from the results, the performance of meta-optimized data ordering is significantly better than that of random order even though during evaluation the models are initialized at random. In a concurrent work, Gu et al. (2024) conducted a similar experiment but their meta objective uses additional validation data and optimizes a distribution over all training data points rather than SGD.\n\n\nThis shows that given a dataset, it is possible to find an ordering that greatly outperforms random ordering without committing to any prior notion of difficulty. This procedure does not leverage additional data, but it requires significantly more computational resources than random ordering, so it is neither practical nor scalable. Moreover, given our limited understanding of the landscape of curriculum optimization, it is highly unlikely that we are identifying the globally optimal curriculum. As shown in Figure 3 (right), even this meta-optimization has not fully converged after 1000 steps. Nonetheless, it demonstrates definitively the existence of a good curriculum, which raises the question of how we can find a better curriculum in a computationally efficient manner<sup>1</sup>.\n\n\n# 3 ADAPTIVE DATA OPTIMIZATION\n\n\nWe demonstrated above a general, albeit costly, strategy for finding good curricula but it is too expensive to be practically useful. We believe that an ideal online data selection strategy for pretraining foundation models should incur a minimal computational cost.\n\n\nFurthermore, computational efficiency alone is not sufficient. Perhaps an equally important consideration is that such methods should avoid explicit dependence on any particular specification of downstream tasks. The fundamental premise of a foundation model is to serve as the basis for all reasonable downstream tasks; explicitly relying on a fixed set of downstream tasks to pick pretraining data distribution risks unintentionally overfitting to the downstream tasks (Goodhart, 1985). Indeed, the evaluation of current language models has become increasingly difficult as performance on existing benchmarks becomes saturated. While evaluating downstream tasks is undoubtedly important because they are representative of the models' main use cases, we believe that it is valuable to define an objective for online data selection that is agnostic to the downstream tasks.\n\n\nPrevious works like DoReMi (Xie et al., 2024), and DoGE (Fan et al., 2023) share this task-agnostic philosophy of data selection. However, these methods require training proxy models which makes them inconvenient and expensive to use. Lifting the requirement for a proxy model would make data selection methods more accessible and practical.\n\n\nThe goal of this work is to advocate for data selection methods that satisfy the following desiderata:\n\n\n# ADAPTIVE DATA OPTIMIZATION\n\n- (i) Does not leverage extra information such as additional data or existing models.\n\n- (ii) Does not depend on multi-staged training with different proxy models.\n\n- (iii) Does not require significant computational resources and can be run online.\n\n\nBased on these criteria, we introduce our method of online data selection, Adaptive Data Optimization (ADO). In the following sections, we present the algorithmic components of ADO. First, we discuss how we fit a scaling law for each domain, allowing the data mixture to account for variations in the natural diversity across different domains. Next, we describe how the data mixture is dynamically adjusted based on these scaling laws during model training. Each domain is characterized by two key quantities: 1. the domain's learning potential, indicating the potential improvement from further optimization in that domain (Section 3.1), and 2. a credit assignment score that quantifies the domain's contribution to the reduction of training loss (Section 3.2). To address potential noise from simultaneously updating the data mixture and the model, we employ several time-averaging techniques to smooth these quantities (Section 3.3).\n\n\n# 3.1 CONSTRUCTION NEURAL SCALING LAWS FOR EACH DOMAIN\n\n\nTo achieve our desiderata, it is crucial to be able to predict the evolution of our model's training online with cheap computational routines that do not scale with the size of the model. To accomplish this, we use neural scaling laws (Kaplan et al., 2020) to extrapolate the loss as a function of the amount of training data observed so far, which we denote  $n$ . It is important to highlight that this scaling law is for predicting the future training loss within a single training run. In contrast, typical neural scaling laws (Hoffmann et al., 2022) use the results of completed training runs to extrapolate the loss of future training runs, typically with larger models or datasets.\n\n\nFor simplicity and interpretability, we use a standard power law:  $\\widehat{\\mathcal{L}}(n; \\alpha, \\beta, \\varepsilon) = \\varepsilon + \\beta n^{-\\alpha}$ , where  $\\alpha, \\beta, \\varepsilon$  are the parameters to be fitted, and  $\\widehat{\\mathcal{L}}$  is a surrogate loss used to model the behavior of the true loss. The parameter  $\\varepsilon$  can be seen as the irreducible loss or the \"entropy\" of the data. Data with high diversity (e.g., CommonCrawl) would naturally have a higher irreducible loss and data that are more predictable (e.g., code and math) would tend to have a lower irreducible loss. The parameter  $\\alpha$  measures how quickly the loss decreases with more training data. Although such scaling laws may be inaccurate at extrapolating training loss far into the future (e.g., due to the learning rate schedule),\n\n\nwe can cheaply refit the parameters on the fly (Appendix A.1). Since the parameters from each fit are only used for a short period before being updated and the learning curves usually do not change abruptly, our scaling law need only be locally accurate.\n\n\nInstead of fitting a single scaling law for the overall loss (Kaplan et al., 2020; Hoffmann et al., 2022), we will fit a separate scaling law for the loss on each domain, each of which will guide ADO to adjust the weights of each domain.\n\n\nDefinition 1. A domain scaling law is a training sample scaling law,  $\\widehat{\\mathcal{L}}_k(n) = \\widehat{\\mathcal{L}}(n; \\alpha_k, \\beta_k, \\varepsilon_k) = \\varepsilon_k + \\beta_k n^{-\\alpha_k}$ , that predicts the  $k^{th}$  domain's training loss after training on  $n$  samples.  $\\varepsilon_k, \\beta_k, \\alpha_k$  are functions of the architecture, training algorithms, and the overall data distribution.\n\n\nThe domain scaling law serves as a tractable middle ground between modeling the overall loss and modeling how every domain interacts with each other. Modeling the full dependencies faithfully may be challenging while training online because we cannot modify the data distribution drastically. As the first step towards this goal, our method will only model how each domain interacts with itself and leave more complex modeling to future works. Nonetheless, this scaling law reveals important information about each domain. For example,  $\\varepsilon_{k}$  corresponds to the estimated irreducible loss of a domain, which could be useful for various applications (Xia et al., 2024a). More importantly, we can interpret the derivative of the loss with respect to the number of samples as the following:\n\n\n$$\n\n$$\n\\frac {d \\widehat {\\mathcal {L}} _ {k} (n)}{d n} = \\frac {- \\alpha_ {k} \\beta_ {k} n ^ {- \\alpha_ {k}}}{n} = - \\frac {1}{n} \\begin{array}{c c} \\alpha_ {k} & (\\widehat {\\mathcal {L}} _ {k} (n) - \\varepsilon_ {k}) \\\\ \\text {L e a n i n g s p e e d} & \\text {R e d u c i b l e l o s s} \\end{array} . \\tag {1}\n$$\n\n$$\n\n\nThe quantity  $\\widehat{\\mathcal{L}}_k(n) - \\varepsilon_k$  informs us about how far away a particular domain is from the estimated minimum loss, which coincides with the notion of population-level reducible loss (Mindermann et al., 2022), and  $\\alpha_{k}$  indicates how fast the loss of a particular domain is changing with new data. This derivative informs us about how much loss decrease we can expect per data point locally. At a high level, the cross entropy loss has an information theoretic interpretation, so this quantity can be understood as a form of information density, or, the amount of information gain per unit of computation. Intuitively, prioritizing data with high information density is a likely better use of computation and could lead to richer representation as it implies more information is \"stored in\" the model weights. We leave the formalization of this statement to future works. It may be useful to contrast this quantity with Albalak et al. (2023, ODM) which only prioritizes domains with a high loss  $\\mathcal{L}_k(n)$  without considering the irreducible loss. This objective could lead to the underrepresentation of domains with inherently lower entropy such as code or math<sup>3</sup>.\n\n\n# 3.2 THE CONTRIBUTION OF THE DATA FROM EACH DOMAIN\n\n\nIn contrast to prior works that train multiple models to find optimal data policies offline (Ye et al., 2024; Ge et al., 2024; Liu et al., 2024), our online approach adaptively prioritizes selecting data so that the model learns quickly, as predicted by scaling laws that we fit on the fly. It consists of a data policy  $\\pi(t) \\in \\Delta^K$  that specifies a sampling distribution over the  $K$  domains. An intuitive approach would be to prioritize sampling domains where the model will learn quickly: e.g., we could sample from domain  $k$  in proportion to  $-\\frac{d\\widehat{\\mathcal{L}}_k}{dn}$  (Equation 1).\n\n\nHowever, independent domain scaling laws do not account for how samples from one domain help learning on a different domain. Consider a thought experiment where the loss  $\\mathcal{L}_k$  is decreasing rapidly, but we have sampled very little data from domain  $k$ . Intuitively, data from domain  $k$  should not get much credit for this loss decrease, which can instead be attributed to transfer learning from other domains. This calls for some form of credit assignment based on whether a domain has actually been sampled recently.\n\n\nDefinition 2. A credit assignment score,  $\\lambda_{k}(t)$ , is a real positive-valued function that indicates how much data from the  $k^{th}$  domain contributed to recent changes in the loss  $\\mathcal{L}_k$ .\n\n\nIdeally, we would like to understand how each domain affects every other domain. Unfortunately, this interaction might be highly complex and not local (i.e., the best solution locally at a given time\n\n\n```\n1: Input: prior  $\\mu \\in \\Delta^K$ , update interval  $t_{\\mathrm{update}}$ , warmup duration  $t_{\\mathrm{warmup}}$ ,  $\\gamma_1, \\gamma_2, s, \\delta_{\\mathrm{min}}$   \n2: Initialize  $h \\gets \\mu$ ,  $\\bar{\\pi} \\gets \\mu$ ,  $\\{\\mathrm{loss\\_k} \\gets []\\}_{k \\in [K]}$   \n3: Train with  $\\mu$  for  $t_{\\mathrm{warmup}}$  steps to initialize each loss_k  \n4: Initialize domain  $k$ 's scaling law with loss_k, for  $k \\in [K]$   \n5: for  $t = 0 \\to T$  do  \n6:  $\\rho(t) \\gets$  compute the preference distribution according to Equation 3  \n7:  $\\pi(t) \\gets$  clip  $(\\gamma_2 \\rho(t) + (1 - \\gamma_2) \\bar{\\pi}(t - 1), \\delta_{\\mathrm{min}})$   \n8:  $\\{\\ell_k\\}_{k \\in [K]} \\gets$  train the model according to  $\\pi$  and add domain  $k$  loss to loss_k  \n9:  $h(t) \\gets \\gamma_1 \\pi(t) + (1 - \\gamma_1) h(t - 1)$   \n10:  $\\bar{\\pi}(t) \\gets \\frac{1}{t + 1} \\rho(t) + (1 - \\frac{1}{t + 1}) \\bar{\\pi}(t - 1)$   \n11: if  $t \\mod t_{\\mathrm{update}} = 0$  then  \n12: Update domain  $k$  scaling law with loss_k, for  $k \\in [K]$\n```\n\n\nstep may not be optimal in the long run). As mentioned earlier, we focus on modeling the domain's contribution to itself for now. A reasonable assumption is that the contribution a domain makes to its own learning progress is positively correlated with what proportion of the recent data came from this domain. In other words, we assume that if a domain was useful for itself in recent history, it will likely continue to be useful. Based on this assumption, we keep a history,  $h(t)$ , of the exponential moving average (EMA) of the recent data policy (with a coefficient of  $\\gamma_{1} < 1$ ) and apply a power transformation with a smoothing parameter  $s < 1$  to smooth the distribution since the assignment can be inaccurate:\n\n\n$$\n\n$$\nh _ {k} (t) = \\gamma_ {1} \\pi_ {k} (t) + (1 - \\gamma_ {1}) h _ {k} (t - 1), \\quad \\lambda_ {k} (t) \\propto h _ {k} (t) ^ {s}. \\tag {2}\n$$\n\n$$\n\n\nHere a smaller  $\\gamma_{1}$  puts more weighting on policy history in the EMA, and decreasing  $s$  leads to stronger smoothing of the credit assignment across tasks. We find that  $\\gamma_{1} = 0.1$  and  $s = 0.5$  work well consistently for all scales we tested. We emphasize that this design choice is a simple heuristic that can be easily computed and other choices are possible.\n\n\n# 3.3 CONSTRUCTING THE DATA POLICY UPDATE\n\n\nTo define our data policy, we combine the two principles we have seen thus far: prioritize sampling from a domain  $k$  if the model is decreasing its loss  $\\mathcal{L}_k$  quickly, but only if that decrease can be attributed to data from domain  $k$ . Additionally, it is common to have a natural prior distribution  $\\mu \\in \\Delta^K$  over the domains (e.g., number of tokens in each domain), which we may also incorporate:\n\n\nDefinition 3. Given the learning speed forecast by a scaling law,  $\\frac{\\partial}{\\partial n}\\widehat{\\mathcal{L}}_k(n)$ , credit assignment score  $\\lambda_{k}(t)$ , and a prior domain weight  $\\mu_{k}$ , the preference distribution is\n\n\n$$\n\n$$\n\\rho_ {k} (t) \\propto - \\mu_ {k} \\frac {\\partial}{\\partial n} \\widehat {\\mathcal {L}} _ {k} (n) \\lambda_ {k} (t) = \\frac {1}{n} \\mu_ {k} \\lambda_ {k} (t) \\alpha_ {k} (\\widehat {\\mathcal {L}} _ {k} (n) - \\varepsilon_ {k}). \\tag {3}\n$$\n\n$$\n\n\nNote that the preference distribution is not a perfect indication of the best possible distribution to learn from because we have no access to the true gradient of the data policy. As shown in the definition of the credit assignment score, we have to resort to various approximations.\n\n\nTemporal average. This local estimate of a data policy may not be optimal globally. Both DoReMi (Xie et al., 2024) and DoGE (Fan et al., 2023) found superior performance by averaging the proxy model's training distribution over all time steps, to obtain a stationary distribution for the actual training run. Since we need to select a data policy online, we instead take inspiration from Defazio et al. (2024) and use an online analogy to post-hoc averaging:\n\n\n$$\n\n$$\n\\bar {\\pi} _ {k} (t) = \\frac {1}{t + 1} \\rho_ {k} (t) + \\left(1 - \\frac {1}{t + 1}\\right) \\bar {\\pi} _ {k} (t - 1), \\tag {4}\n$$\n\n$$\n\n\n$$\n\n$$\n\\pi_ {k} (t) = \\gamma_ {2} \\rho_ {k} (t) + (1 - \\gamma_ {2}) \\bar {\\pi} _ {k} (t - 1). \\tag {5}\n$$\n\n$$\n\n\nHere  $\\bar{\\pi}_k(t)$  is a temporal moving average of the preference policy at every step and  $\\pi_k(t)$  is a linear combination of the moving average and the chosen policy according to which the data are sampled\n\n\nTable 1: Zero-shot performance of different methods across different downstream evaluations. Highlighted cells indicate top-performing methods (deeper color indicates better performance).\n\n\n<table><tr><td></td><td>HellaSwag</td><td>WinoGrande</td><td>PIQA</td><td>ARC-E</td><td>SciQ</td><td>LogiQA2</td><td>LAMBADA</td><td>Average</td></tr><tr><td>124M-Pile</td><td>0.279</td><td>0.515</td><td>0.615</td><td>0.435</td><td>0.747</td><td>0.228</td><td>0.357</td><td>0.454</td></tr><tr><td>124M-DoReMi</td><td>0.279</td><td>0.520</td><td>0.609</td><td>0.429</td><td>0.741</td><td>0.237</td><td>0.368</td><td>0.455</td></tr><tr><td>124M-ODM</td><td>0.285</td><td>0.514</td><td>0.603</td><td>0.453</td><td>0.764</td><td>0.230</td><td>0.374</td><td>0.461</td></tr><tr><td>124M-Balanced</td><td>0.280</td><td>0.511</td><td>0.610</td><td>0.447</td><td>0.762</td><td>0.227</td><td>0.362</td><td>0.457</td></tr><tr><td>124M-Natural</td><td>0.290</td><td>0.503</td><td>0.624</td><td>0.435</td><td>0.755</td><td>0.234</td><td>0.401</td><td>0.463</td></tr><tr><td>124M-ADO</td><td>0.290</td><td>0.520</td><td>0.635</td><td>0.456</td><td>0.771</td><td>0.244</td><td>0.371</td><td>0.470</td></tr><tr><td>1.3B-DoReMi</td><td>0.416</td><td>0.582</td><td>0.707</td><td>0.609</td><td>0.870</td><td>0.228</td><td>0.615</td><td>0.575</td></tr><tr><td>1.3B-Balanced</td><td>0.382</td><td>0.546</td><td>0.689</td><td>0.580</td><td>0.862</td><td>0.225</td><td>0.613</td><td>0.557</td></tr><tr><td>1.3B-Natural</td><td>0.424</td><td>0.584</td><td>0.718</td><td>0.627</td><td>0.886</td><td>0.232</td><td>0.624</td><td>0.585</td></tr><tr><td>1.3B-ADO</td><td>0.442</td><td>0.590</td><td>0.730</td><td>0.625</td><td>0.875</td><td>0.228</td><td>0.638</td><td>0.590</td></tr></table>\n\n\nfrom, which combines both the current preference distribution and the time-averaged distribution. Using the coefficient of  $\\frac{1}{t}$  ensures that all the preference distributions from all time steps contribute equally to the final average, similar to the effect of post-hoc averaging. Intuitively, a larger value of  $\\gamma_{2}$  makes the algorithm more responsive to the current loss dynamics, while a smaller value makes it smoother and less responsive. We find that  $\\gamma_{2} = 0.1$  works well consistently. Additionally, we enforce the smallest domain probability to be no smaller than  $\\delta_{\\mathrm{min}} = 0.01$  to avoid domains being unsampled (Appendix C). Algorithm 1 shows the pseudocode for the entire algorithm.\n\n\n# 4 EXPERIMENTS\n\n\nEvaluation. We conduct all our experiments on the Pile dataset (Gao et al., 2020) with decoder-only transformer language models (Vaswani et al., 2017) of varying sizes. We consider two types of metrics: 1. validation loss on the Pile, an unweighted version of the pile validation set where each domain receives equal probability, SlimPajama (Soboleva et al., 2023), and a 1 billion token subset of FineWeb (Penedo et al., 2024), 2. zero-shot downstream performance on 6 common-sense reasoning domains from the language model evaluation harness (Gao et al., 2024): HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019), PIQA (Bisk et al., 2019), ARC-E (Clark et al., 2018), SciQ (Welbl et al., 2017), LogiQA2 (Liu et al., 2023) and LAMBADA (Paperno et al., 2016).\n\n\nMethods. We conduct experiments at 2 computation scales: 124 million parameters (124M) and 1.3 billion parameters (1 . 3B). For 124M models, we use a batch size of 256 with a context length of 1024 trained for 60,000 steps (approximately 15 billion tokens), and for 1 . 3B, we use a batch size of 2048 with a context length of 1024 trained for 60000 steps (approximately 125 billion tokens). Both scales use the natural distribution for  $\\mu$  and the same hyperparameters for ADO. Training details can be found in Appendix A.\n\n\nComparisons. For comparison, we use the original Pile weights (Gao et al., 2020, Pile) and DoReMi weights (Xie et al., 2024, DoReMi) $^{4}$  as the baselines. We also consider ODM (Albalak et al., 2023), another online method. In addition, we introduce a simple baseline that is surprisingly missing from the literature: weighing each domain by the number of tokens in it. This mixture naturally takes into account the tokenizer being used and training on this policy essentially corresponds to training on the natural distribution of the dataset (i.e., corresponds to empirical risk minimization where train, validation, and test sets all follow the same distribution defined by the tokenizer). We estimate the tokens per domain by randomly sampling 1000 documents from each domain to estimate each domain's average tokens per document, and then multiply it by the total number of documents (Appendix A). We will refer to this policy as Natural. We also use a second baseline, Balanced, which considers the unweighted mixture of all the domains.\n\n\nObservations. The results for downstream performance are shown in Table 3. We will highlight some key observations here. Observation 1: at  $124\\mathrm{M}$  scale, ADO achieves the highest average accuracy; it also outperforms all the baselines on 6 out of 7 downstream tasks and is competitive on\n\n\n![Figure 4: Perplexity of 1.3B models on different domains, trained on the Pile using either DoReMi, Natural, or ADO. On the left side of the red line, we have the validation perplexity on the Pile validation set, the unweighted Pile validation set, the SlimPajama validation set, and a random subset of FineWeb. On the right side of the red line, we show the validation perplexity of each domain of the Pile.](images/7d3b145e813659831860c0bba6a2dc7d26bcb9691f2f5bd1e1080370093e56e6.jpg)\n\n\nLAMBADA. At 1.3B scale, ADO also achieves the highest average accuracy and outperforms the baselines on 4 out of 7 downstream tasks. Observation 2: interestingly, Natural turns out to be a very competitive baseline. It achieves the second-best average downstream performance at both scales and performs well on individual domains, too. To the best of our knowledge, very few works on data selection have benchmarked against using the empirical distribution of tokens given by the dataset and tokenizer. As such, we recommend future works on data selection to compare against this simple baseline. Balanced also turns out to be a quite competitive baseline at 124M scale likely because smaller domains are not extensively repeated given the low total number of tokens processed at this scale. This observation is in line with the findings of Goyal et al. (2024) which suggest that good data policies are different for different compute scales.\n\n\nConventionally, validation loss is considered a good indicator of the models' performance if they are trained on the same data, but it is less clear whether it remains a good indicator when the data policy is dynamically changing throughout the training. Empirically (Figure 4), we found that it is difficult to outperform Natural on the Pile validation loss (at least within the same training setup and without spending much more computation), even though we do outperform this mixture on the downstream tasks as shown above. Observation 3: for validation loss, ADO slightly underperforms Natural on the Pile validation set, but on the validation set of SlimPajama and a subset of FineWeb, ADO outperforms Natural. Since SlimPajama and FineWeb are heavily filtered to be \"higher quality\" data compared to the Pile, we speculate that ADO may be able to implicitly select data aligned with some notion of high quality throughout the training process (e.g., by prioritizing learnability). Nonetheless, since it can be challenging to quantify or precisely define \"high quality\" data, we will leave this investigation to future work. We include more analysis in Appendix D.1.\n\n\n# 4.1 ANALYSIS\n\n\nMixture over the training. A natural question about data selection methods is what does the distribution look like. We now show that the curricula learned by ADO differ for models of different scales in Figure 5. We observe that CommonCrawl (File-CC) receives the most probability mass in both 124M and 1.3B. This observation is consistent with some previous works that put most of the probability mass on CommonCrawl (Xie et al., 2024). OpenText2 also receives a higher proportion compared to the empirical mixture. Github receives a higher probability in 124M initially but eventually decays whereas in 1.3B it first decreases rapidly and gradually increases\n\n\n![](images/76cba97914be8647da4b3061246500ea27c1360fe8415b5718079811e21a9c5e.jpg)\n\n\n![](images/73924ae91cd1a32fd8918e283d5e8ba80802561b08eda33f2f8d83b06c3c33d0.jpg)\n\n\n![Figure 5: The sampling distribution produced by our data policy during training on The Pile. For ease of visualization, we only highlight the top 6 largest domains. ADO produces qualitatively different strategies at different model scales and adaptively changes its weightings over time.](images/65bbf58386697c109a20e08de3bf087dc52fd269fa03a77bb3091b29125c9df6.jpg)\n\n\n![Figure 6: The forecasts from our scaling laws (fit online throughout training) versus the actual training loss on different domains in the Pile. The forecasts shown were fit at 10,000, 30,000, and 50,000 steps (time of fit is shown in dashed lines). We observe that the scaling laws: (1) become more accurate over the course of training, and (2) can be surprisingly accurate at forecasting the final loss even very early into training on some domains, such as Pile-CC. Forecasts from all domains are shown in the Appendix, Figure 8.](images/c79642bea79e475d4c4b0f14a8633c2c9c9c3e23e7fa2cab13ff5bbccc618ef1.jpg)\n\n\ntowards the end of the training. This is consistent with the intuition that code data naturally have much lower entropy so it is \"easier\" to learn in some sense. ADO does not account for the fact that code could be desirable for general capabilities such as reasoning (Ma et al., 2024) since it is agnostic to the downstream tasks. This issue can be partially resolved by assigning a higher prior to code but further research on data selection targeting reasoning is likely needed.\n\n\nAccuracy of the scaling law throughout the training. Unlike most applications of scaling laws, we refit the scaling laws on the fly as the models train and the data mixtures are dynamically changing. This introduces new challenges because the forecasts from scaling laws fit early on can be inaccurate for steps far in the future. In Figure 6, we show the predictions of the scaling laws for 3 domains at various points during training (all domains are shown in the Appendix, Figure 8). It can be seen that while the predictions of our scaling laws eventually deviate from the true learning curves, they are accurate locally for much of the training and thus can act as a learning signal for the data policy. More specifically, we can see that the scaling laws consistently overestimate the final loss for all domains, likely due to the presence of learning rate schedules. Empirically, when the learning rate decays towards the end of the training, the training loss tends to decrease more rapidly, until a certain point when the learning rate is too small (H\u00e4gele et al., 2024). We believe that incorporating recent techniques for incorporating the effect of learning rate schedules into the scaling laws (Tissue et al., 2024) could further improve ADO.\n\n\n# 5 RELATED WORKS\n\n\nData curation and selection. For current large language models, compute often poses a greater constraint than data availability, making data selection crucial (Albalak et al., 2024). A widely used approach is data filtering (Soboleva et al., 2023; Penedo et al., 2023; 2024), where undesirable data points are removed based on heuristics like perplexity, repetition (Lee et al., 2021), or semantic similarity (Abbas et al., 2023). This filtering process is foundational for constructing most large-scale datasets today. After filtering, data is often categorized into subsets or domains (e.g., code, books), and deciding how much data to use from each domain becomes an important step.\n\n\nTwo primary strategies for data selection are prevalent: one focuses on deciding whether individual data points should be included based on various criteria (Mindermann et al., 2022; Engstrom et al., 2024), and the other uses all available data but samples from different domains with varying probabilities (Xie et al., 2024; Fan et al., 2023; Albalak et al., 2023). While data selection aims to enhance training efficiency, these methods may introduce considerable computational overhead (Xie et al., 2024; Chen et al., 2024; Fan et al., 2023). Moreover, Kaddour et al. (2023) show that under the same computational budget, these methods often fail to surpass standard training, and Wang et al. (2024) proves that the data selection's efficacy depends on the user's utility function.\n\n\nAnother line of work focuses on selecting pre-training data that aligns more closely with downstream tasks (Kang et al., 2024). Data selection has also been explored for computer vision. For example, Evans et al. (2023) use a small reference model to select data for CLIP, while others propose pruning batches based on diversity criteria (Qin et al., 2024; Hong et al., 2024) to improve training efficiency.\n\n\nNeural scaling laws. Studies have found that various quantities of interest for large pretrained models (e.g., validation loss) can be reliably predicted with simple statistics such as the model size, dataset size, or the amount of computation (Kaplan et al., 2020; Hoffmann et al., 2022). These findings have been central to the design of training protocols for large language models where trial and error are expensive. More recent works have studied the relationship between data repetition and the performance of the models (Hernandez et al., 2022; Muennighoff et al., 2024; Goyal et al., 2024). Data curation, in particular pruning, has also been shown to have significant effects to achieve better scaling laws (Sorscher et al., 2022). Studying the loss curve via scaling law (Hutter, 2021) is relatively less well-explored because the loss curves do not follow a power law exactly due to the learning rate schedule; however, we found that a power law is still a decent model for learning curves with cosine decay for language models. Recently, Tissue et al. (2024) showed that learning rate schedules can be incorporated into scaling laws to make even more accurate predictions, though we do not explore this direction in this work.\n\n\n# 6 CONCLUSION\n\n\nDynamic data selection has the potential to improve the pretraining efficiency of foundation models, but most existing approaches incur additional computation costs. We introduce ADO, a cheap online data selection method that dynamically adjusts data distribution over different domains based on domain scaling laws. The scaling laws forecast the model's loss on different data domains and automatically adjust the training data distribution based on each domain's learning potential. ADO does not require a proxy model so it naturally takes into account the architecture, the optimizer, and other hyperparameters of the training. In our experiments, with a single set of hyperparameters, ADO performs comparably to or better than existing methods on scales from 124M to 1.3B, with much less additional computation. Our implementation only added 20 minutes of additional wall-clock time to a 3.5-day training run ( $\\sim 0.4\\%$ ), and can be optimized further. The relative cost would decrease more at larger scales since ADO's cost does not scale with the model size. Overall, we believe ADO represents an important step towards accessible and automated online data selection.\n\n\nLimitations and future directions. Given further computational resources, it would be interesting to scale ADO up to larger models (such as 8B models in DoReMi (Xie et al., 2024)) and datasets and study its behaviors more thoroughly. On the technical side, we believe there are several promising directions: 1. how to create better and more fine-grained domains, 2. how to design more realistic scaling laws that can model inter-domain interactions and learning rate schedules, 3. whether insights of ADO can be applied to other training settings such as continued pretraining or fine-tuning, and 4. why do these domain scaling laws arise from naturally occurring data distributions.\n\n\n# REFERENCES\n\n- Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic dedduplication. arXiv preprint arXiv:2303.09540, 2023.\n\n- Ryan Prescott Adams and Richard S Zemel. Ranking via sinkhorn propagation. arXiv preprint arXiv:1106.1925, 2011.\n\n- Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for language model pre-training. In *R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models*, 2023.\n\n- Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024.\n\n- Robert A Baker and Stanley W Osgood. Discrimination transfer along a pitch continuum. Journal of Experimental Psychology, 48(4):241, 1954.\n\n- Robert Baldock, Hartmut Maennel, and Behnam Neyshabur. Deep learning through the lens of example difficulty. Advances in Neural Information Processing Systems, 34:10876-10889, 2021.\n\n- Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41-48, 2009.\n\n- Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. PMLR, 2023.\n\n- Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence, 2019. URL https://api-semanticscholar.org/CorpusID:208290939.\n\n- Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022.\n\n- Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258.\n\n- James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n\n- Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher R\u00e9. Skill-it! a data-driven skills framework for understanding and training language models. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. URL https://api_semanticscholar.org/ CorpusID:3922816.\n\n- DeepMind, Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci, Jonathan Godwin, Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza Merzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Laurent Sartran, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, Milo\u0161 Stanojevic, Wojciech Stokowiec, Luyu Wang, Guangyao Zhou, and Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL http://github.com/google-deepmind.\n\n- Aaron Defazio, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, Ashok Cutkosky, et al. The road less scheduled. arXiv preprint arXiv:2405.15682, 2024.\n\n- Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels. arXiv preprint arXiv:2401.12926, 2024.\n\n- Talfan Evans, Shreya Pathak, Hamza Merzic, Jonathan Schwarz, Ryutaro Tanno, and Olivier J Henaff. Bad students make great teachers: Active learning accelerates large-scale visual understanding. arXiv preprint arXiv:2312.05328, 2023.\n\n- Simin Fan, Matteo Pagliardini, and Martin Jaggi. Doge: Domain reweighting with generalization estimation. ArXiv, abs/2310.15393, 2023. URL https://apisemantic scholar.org/ CorpusID:264439382.\n\n- Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and P. Abbeel. Reverse curriculum generation for reinforcement learning. ArXiv, abs/1707.05300, 2017. URL https://api-semanticscholar.org/CorpusID:19181872.\n\n- Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\n- Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602.\n\n- Ce Ge, Zhijian Ma, Daoyuan Chen, Yaliang Li, and Bolin Ding. Data mixing made efficient: A bivariate scaling law for language model pretraining. arXiv preprint arXiv:2405.14908, 2024.\n\n- Charles Albert Eric Goodhart. Monetary control\u2014the British experience. In Monetary Conditions for Economic Recovery, pp. 59-84. Springer, 1985.\n\n- Sachin Goyal, Pratyush Maini, Zachary C Lipton, Aditi Raghunathan, and J Zico Kolter. Scaling laws for data filtering-data curation cannot be compute agnostic. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22702-22711, 2024.\n\n- Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. In international conference on machine learning, pp. 1311-1320. Pmlr, 2017.\n\n- Yuxian Gu, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, and Furu Wei. Towards optimal learning of language models. arXiv preprint arXiv:2402.17759, 2024.\n\n- Alexander Hagele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro Von Werra, and Martin Jaggi. Scaling laws and compute-optimal training beyond fixed training durations. arXiv preprint arXiv:2405.18392, 2024.\n\n- Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487, 2022.\n\n- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL https://api-semanticscholar.org/CorpusID:247778764.\n\n- Feng Hong, Yueming Lyu, Jiangchao Yao, Ya Zhang, Ivor W Tsang, and Yanfeng Wang. Diversified batch selection for training acceleration. arXiv preprint arXiv:2406.04872, 2024.\n\n- Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021.\n\n- Andrew Jesson, Nicolas Guizard, Sina Hamidi Ghalehjegh, Damien Goblot, Florian Soudan, and Nicolas Chapados. CASED: curriculum adaptive sampling for extreme data imbalance. CoRR, abs/1807.10819, 2018. URL http://arxiv.org/abs/1807.10819.\n\n- Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt J. Kusner. No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models, July 2023. URL http://arxiv.org/abs/2307.06440. arXiv:2307.06440 [cs].\n\n- Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in neural information processing systems, 32, 2019.\n\n- Feiyang Kang, Hoang Anh Just, Yifan Sun, Himanshu Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit Kumar Sahu, and Ruoxi Jia. Get more for less: Principled data selection for warming up fine-tuning in llms. In ICLR, 2024. URL https://openreview.net/forum?id=QmYNBVukex.\n\n- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\n- Patrick Kidger and Cristian Garcia. Equinox: neural networks in JAX via callable PyTrees and filtered transformations. Differentiable Programming workshop at Neural Information Processing Systems 2021, 2021.\n\n- Douglas H Lawrence. The transfer of a discrimination along a continuum. Journal of Comparative and Physiological Psychology, 45(6):511, 1952.\n\n- Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021.\n\n- Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue Zhang. Logiqa 2.0\u2014an improved dataset for logical reasoning in natural language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:2947-2962, 2023. URL https://api-semanticscholar.org/CorpusID:259515154.\n\n- Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024.\n\n- I Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\n- Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. At which training stage does code data help LLMs reasoning? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=KIPJKST4gw.\n\n- Stefano Sarao Mannelli, Yaraslau Ivashinka, Andrew Saxe, and Luca Saglietti. Tilting the odds at the lottery: the interplay of overparameterisation and curricula in neural networks. arXiv preprint arXiv:2406.01589, 2024.\n\n- Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning latent permutations with gumbel-sinkhorn networks. arXiv preprint arXiv:1802.08665, 2018.\n\n- S\u00f6ren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H\u00f6ltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In International Conference on Machine Learning, pp. 15630-15649. PMLR, 2022.\n\n- Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and R. Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. ArXiv, abs/1606.06031, 2016. URL https://api-semanticscholar.org/CorpusID:2381275.\n\n- Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refined web dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\n\n- Guilherme Penedo, Hynek Kydl\u00ed\u010dek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fine web datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557.\n\n- Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, xu Zhao Pan, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, and Yang You. Infobatch: Lossless training speed up by unbiased dynamic data pruning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=C61sk5LsK6.\n\n- Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. An adversarial winograd schema challenge at scale. 2019. URL https://apisemantic scholar.org/ CorpusID:199370376.\n\n- Or Sharir, Barak Peleg, and Yoav Shoham. The cost of training NLP models: A concise overview. CoRR, abs/2004.08900, 2020. URL https://arxiv.org/abs/2004.08900.\n\n- Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\n\n- Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. The annals of mathematical statistics, 35(2):876-879, 1964.\n\n- Burrhus F Skinner. Reinforcement today. American Psychologist, 13(3):94, 1958.\n\n- Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B.\n\n- Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523-19536, 2022.\n\n- Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. corr abs/2104.09864 (2021). arXiv preprint arXiv:2104.09864, 2021.\n\n- Yuxing Tang, Xiaosong Wang, Adam P. Harrison, Le Lu, Jing Xiao, and Ronald M. Summers. Attention-guided curriculum learning for weakly supervised classification and localization of thoracic diseases on chest radiographs. ArXiv, abs/1807.07532, 2018. URL https://api.sementicscholar.org/CorpusID:49882848.\n\n- Howe Tissue, Venus Wang, and Lu Wang. Scaling law with learning rate annealing, 2024. URL https://arxiv.org/abs/2408.11029.\n\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Jasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\n- Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. URL https://apisemantic scholar.org/CorpusID:13756489.\n\n- Jiachen T Wang, Tianji Yang, James Zou, Yongchan Kwon, and Ruoxi Jia. Rethinking data shapley for data selection tasks: Misleads and merits. arXiv preprint arXiv:2405.03875, 2024.\n\n- Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. ArXiv, abs/1707.06209, 2017. URL https://api_semanticscholar.org/CorpusID:1553193.\n\n- Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. When do curricula work? In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=tW4QEInpni.\n\n- Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared LLaMA: Accelerating language model pre-training via structured pruning. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=09iOdaeOzp.\n\n- Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. LESS: Selecting influential data for targeted instruction tuning. In *Forty-first International Conference on Machine Learning*, 2024b. URL https://openreview.net/forum?id=PG5fvV50maR.\n\n- Sang Michael Xie and Stefano Ermon. Reparameterizable subset sampling via continuous relaxations, 2021. URL https://arxiv.org/abs/1901.10517.\n\n- Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. arXiv preprint arXiv:2403.16952, 2024.\n\n- Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Annual Meeting of the Association for Computational Linguistics, 2019. URL https://api-semanticscholar.org/CorpusID:159041722.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636553.4100416", "msecs": "410.0", "relativeCreated": "14103.182077407837", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations\n\n# Abstract\nReconstructing visual images from fMRI data presents a challenging task, particularly when dealing with limited data and compute availability. This work introduces a novel approach to fMRI-based visual image reconstruction using a subject-agnostic common representation space. We show that subjects' brain signals naturally align in this common space during training, without the need for explicit alignment. This is leveraged to demonstrate that aligning subject-specific adapters to a reference subject is significantly more efficient than traditional end-to-end training methods. Our approach excels in low-data scenarios, where training the adapter with limited data achieves faster and better performance. We also introduce a novel method to select the most representative subset of images for a new subject, allowing for fine-tuning with 40\\% less data while maintaining performance. These advancements make fMRI data collection more efficient and practical, reducing the burden on subjects and improving the generalization of fMRI reconstruction models.\n\n\n# 1 INTRODUCTION\n\n\nOver the past several decades, the use of machine learning techniques has enabled decoding and/or reconstruction of information represented in neural activation patterns measured in awake, behaving humans. Because the neural representations supporting behavior likely entail patterns of activity across large swaths of tissue, these approaches are thought to offer a key step towards characterizing and quantifying the computational principles brains use to support cognition (Naselaris et al., 2011).\n\n\nEarly approaches, including self-supervised learning frameworks (Beliy et al., 2019; Gaziv et al., 2022; Mozafari et al., 2020; Shen et al., 2019; Seeliger et al., 2018; Ren et al., 2021) struggled to generate semantically accurate image reconstructions, often resulting in images with limited resemblance to the original stimuli. Furthermore, these methods required large amounts of data for training, and their generalization to a wider population of subjects was severely limited. Significant advancements were achieved by incorporating multimodal data, such as text (Lin et al., 2022), and later by leveraging the generative power of latent diffusion models (LDMs) (Takagi & Nishimoto, 2023; Scotti et al., 2023; Lu et al., 2023; Xia et al., 2024). These models introduced novel mapping techniques to project fMRI signals into the input space of LDMs, yielding higher-quality natural images. However, despite the improved fidelity, the issue of generalization to new participants persisted. In response, Scotti et al. (2024) proposed projecting fMRI data into a shared representation space, demonstrating significant improvements in both performance and generalizability.\n\n\nBeyond being able to use decoding and reconstruction approaches to provide evidence of the existence of information represented in a particular brain activation pattern, an additional goal is to compare decoding or reconstruction performance across experimental manipulations, such as cued visual attention (Serences & Boynton, 2007; Kamitani & Tong, 2005; Scolari et al., 2012; Sprague & Serences, 2013; Itthipuripat et al., 2019; Sprague et al., 2018) and visual working memory (Serences et al., 2009; Harrison & Tong, 2009; Ester et al., 2013; Sprague et al., 2014; Christophel et al., 2012; Li et al., 2021) However, for these approaches to be useful for a cognitive neuroscience lab, the lab would need to acquire sufficient data to train a decoding or reconstruction model for each individual participant (often  $>20$  per study to attain statistical power). The state-of-the-art methods described above use extremely massive datasets with dozens of hours of fMRI data per participant, which is not feasible in almost all cases. Thus, there is a clear need to maximize the efficiency and generalizability\n\n\nof image reconstruction methods such that a model for a new participant can be trained using as little data as possible.\n\n\nIn this work, we introduce an efficient and generalizable approach to fMRI-based visual reconstruction that leverages a subject-agnostic common representation space. Unlike previous approaches that perform end-to-end training of fMRI signals to mapped contextual embeddings, we propose aligning brain activity from new subjects to a pre-trained reference subject in a shared representation space using Adapter Alignment (AA) training. Throughout this work, we refer to lightweight subject-specific modules designed to map input fMRI signals to a shared representation space as \"adapters\"(Liu et al., 2024). These modules are implemented as single-layer neural networks. In their linear form, adapters consist of a single linear transformation layer, while in the non-linear form, they include a linear layer followed by a non-linear activation function such as GELU (Hendrycks & Gimpel, 2023). Using AA training, we significantly reduce the amount of fMRI data required to achieve accurate reconstructions and improve the reconstruction quality. Our method achieves better performance to existing state-of-the-art models, but with far fewer training hours, making it feasible for cognitive neuroscience labs with limited data collection resources. We build on this approach by also proposing an adaptive image selection strategy, allowing for optimal fine-tuning of new subjects with minimal data, further improving the practicality and accessibility of fMRI-based image reconstruction.\n\n\nOur contributions to this work are threefold:\n\n- - We provide strong evidence for the existence of a common visual representation space across subjects, where brain activity from different individuals can be aligned in a shared space. By extracting and analyzing embeddings from a pre-trained model, we show that even without explicit alignment, subject-specific fMRI signals, when mapped to a common space display similar behavior and representation structures. Furthermore, we demonstrate that applying a simple orthogonal transformation improves the alignment of embeddings between subjects, and using non-linear adapters with explicit mapping leads to almost perfect alignment across subjects in the common space. (Section 2)\n\n- - We introduce Adapter Alignment (AA), a novel approach to facilitate efficient subject-specific alignment in fMRI-based visual reconstruction. Our training pipeline first pre-trains on a reference subject, then aligns new subjects' brain data using lightweight adapters that map them to the shared common space. This process, which is more efficient than traditional end-to-end training, enables faster convergence and better performance, particularly in low-data scenarios. By incorporating MSE losses at the adapter level for common images, we show that our method not only reduces training time but also improves generalization and reconstruction quality. (Section 3)\n\n- - We develop a greedy image selection algorithm that identifies the most representative subset of images for training new subjects. By efficiently selecting fMRI signals that capture the most informative aspects of the common space, this strategy significantly reduces the amount of data required for fine-tuning without sacrificing performance (from a baseline of 250 images to 150 images). This makes our method more practical for real-world applications, where acquiring large fMRI datasets is often prohibitively expensive. (Section 4)\n\n\n# 2 A COMMON VISUAL REPRESENTATION SPACE\n\n\nCentral to our approach is the hypothesis that each individual's brain (or at least the tissue involved in processing static visual images) can be considered an instantiation of a 'common' brain, with activity patterns varying along similar dimension in a common shared representational space. Aspects of this hypothesis are already strongly supported: most human brains have localized tissue responsive to specific types of stimuli, like faces and scenes. Additionally, several continuous dimensions of high-level visual selectivity have been identified across the ventral temporal cortex (Huth et al., 2012), including dimensions like animate/inanimate, large/small (based on real-world size) (Long et al., 2018), and spiky/round (Bao et al., 2020). Retinotopic organization is another example of a shared representational space, as each individual brain instantiates a unique anatomical mapping from retinal coordinates to cortical representations, and adaptive models can be estimated to transform activation patterns from individual brains to this common shared retinotopic coordinate system (Sprague & Serences, 2013; Wandell & Winawer, 2015). We analyze the similarity of representation spaces\n\n\n<table><tr><td>Dimension 1</td><td>Animals (giraffes, zebras, and elephants), nature, and outdoor scenes</td></tr><tr><td>Dimension 2</td><td>Cats and food</td></tr><tr><td>Dimension 3</td><td>Kite flying, airplanes, and flying objects</td></tr><tr><td>Dimension 4</td><td>Surfing, water sports, beach, and ocean scenes</td></tr><tr><td>Dimension 5</td><td>Airplanes, flying objects, outdoor activities and events, transportation scenes</td></tr></table>\n\n\nTable 1: Concept analysis of images along principal dimensions.\n\n\nfrom different perspectives of common concepts, visual analysis, and eigenvectors in the following subsections. Finally, we reflect on the different ways of aligning the common representations.\n\n\n# 2.1 EMERGENCE OF COMMON CONCEPTS\n\n\nThe existence of a semantic common representation space is suggested from the emergence of semantic categories in this space. For this, we extracted the most significant left singular vectors from the ridge regression weight matrix. This matrix was extracted from the pre-trained MindEye2 (Scotti et al., 2024) model which was trained using the 1024-dimensional common space. Subsequently, we projected the 1024-dimensional embeddings of each image onto these vectors and sampled from the two extremes (set 1 and set 3) and the middle (set 2). In order to obtain an objective classification, we presented these sets of images to GPT-4o (Achiam et al., 2023) and asked it to find the semantic categories that are well represented in set 1, somewhat represented in set 2 and not represented in set 3. Results for the first five singular vectors are shown in Table 1. As is evident, images in the principal dimensions of the representation space have a common theme.\n\n\nPrevious studies, such as those by Huth et al. (2012), have indicated that animal categories, including humans, are represented distinctly from non-animal categories within the brain. In our study, we observed that the common representational space also preserves these semantic properties. As demonstrated in Figure 1, after projecting the fMRI signals of Subject 2 into the common space and onto the first two singular vectors, we can see the animal vs. non-animal distinction, consistent with previous hypotheses. Extending this analysis across additional dimensions, we observed similar separations between other hypothesized semantic categories of \"Social interaction,\" \"man-made vs nature,\" and \"biological vs non-biological.\"\n\n\n![Figure 1: Visualization of theAnimate/Inanimate semantic category on the two principal singular vectors. Each point (embedding) is colored based on whether its corresponding image contains 'objects' that fall into theAnimate category (red), Inanimate category (blue) or both categories (green).](images/aceb003e4bbbb907ac8ac8753fba27d87d056a52418260fb80892ff654a4dcee.jpg)\n\n\nWe also analyze the images in the representation space visually. We present our results and analysis in the Appendix Section D.\n\n\n# 2.2 SIMILARITY IN REPRESENTATION STRUCTURE ACROSS SUBJECTS\n\n\nExisting works, such as MindEye2 (Scotti et al., 2024), employ a simple linear adapter network to project subject-specific fMRI signals into a common representation space. A key indication of the existence of such a shared space is the structural similarity of embeddings across subjects. To explore this, we first use a pre-trained MindEye2 model to extract common-space embeddings for the shared images (the test set in MindEye2) across all subjects. Initial cosine similarity between the embeddings from different subjects is low. However, after applying an orthogonal transformation using Procrustes analysis (Sch\u00f6nemann, 1966), the cosine similarity improves, suggesting that the embeddings can be structurally aligned. Despite this improvement, the alignment is not perfect, likely due to the limitations of linear transformations.\n\n\n![Figure 2: Cosine Similarity of shared image embeddings in common space. (a) The alignment is weak before an aligning orthogonal transformation is applied. (b) The alignment improves after an orthogonal transformation. For each row in this heatmap, the embedding spaces of subjects are transformed orthogonally to align with the corresponding row's subject. (c) Near perfect alignment with nonlinear adapters and explicit alignment. We only consider subjects 1,2,5 and 7 in this plot as we worked with 4 subjects only when training our pipeline.](images/fbfa5b3b456d0a58e1f258d0b4a107407b4d58aebc6911a4adf44fa7bf9b5c06.jpg)\n\n\n![](images/2aa41a62aa36f445bd016bce3b6d5b52973989cf17f4f3f61bfeb25b88d404eb.jpg)\n\n\n![](images/5dd64f82c2ee5f4f979c99cd98bfea0f58569d09e09d9e0e2568119890c1fdcf.jpg)\n\n\nAs a preliminary experiment, we pre-trained a single subject model with a non-linear adapter and explicitly mapped subsequent subjects to this reference subject using subject-specific non-linear adapters. This approach results in near-perfect alignment in the common space. Although existing methods do not explicitly align subject-specific representations, we observed significant semantic and structural similarities across subjects' representations. Furthermore, it is possible to map one subject's representations to another's using non-linear adapters. While this alignment in the common space is nearly perfect, it does not directly translate to better reconstruction performance.\n\n\nThis evidence supports the core hypothesis of our work: it is possible to construct an aligned, subject-agnostic common space. In Section 3, we empirically test this hypothesis by mapping a new subject to a reference subject during training and evaluating its reconstruction performance. Additional analysis is provided in Appendix C.\n\n\n# 3 FMRI RECONSTRUCTION\n\n\nBuilding a common brain representation requires constructing an efficient fMRI reconstruction pipeline. Our approach is guided by two primary objectives: 1) achieving competitive reconstruction performance and 2) prioritizing efficiency over marginal performance gains. We hypothesize that it is possible to pre-train a single reference brain and align subsequent subjects to this reference using subject-specific adapters, significantly reducing the computational cost while maintaining high reconstruction quality.\n\n\n# 3.1 DATASET\n\n\nWe use the Natural Scenes Dataset (NSD) (Allen et al., 2022) but apply a different split compared to the traditional setup. Typically, common images across subjects are assigned to the test set, while the\n\n\ntraining set consists of unique images viewed by each subject. However, for effective Common Space Alignment, it is critical to have a one-to-one mapping of images across subjects. To achieve this, we incorporate the common images into the training set and swap an equal amount of training images to the test sets. This means that every subject now has a unique randomly sampled set of images on which they are tested. We present the performance of the original training method and the proposed technique on the new split to ensure a fair comparison across subjects and alignment techniques.\n\n\nOur initial experiments revealed that models performed slightly worse on the unique image test set than on the common image test set, suggesting that the unique image test set presents a more challenging benchmark. Similar to prior work, we focus on subjects 1, 2, 5, and 7, as they have complete 40-hour fMRI datasets available in NSD, enabling better reconstruction performance. These subjects provide robust data for training and evaluation, allowing us to validate our alignment and reconstruction methods comprehensively.\n\n\n# 3.2 ADAPTER ALIGNMENT\n\n\n![Figure 3: Adapter Alignment Procedure. The Reference Subject is trained end-to-end first. The common space embeddings of the shared images from the reference subject are used as ground truth to train the adapters of subsequent subjects.](images/67929ce7549af003d6115c3b4ffe0f12751cbb0bf3a91632f8698599708534a6.jpg)\n\n\nCurrent approaches for training multi-subject pipelines with a common representation space typically follow an end-to-end strategy, where fMRI inputs are processed by the model and the loss is computed at the final layer to minimize the discrepancy between the ground-truth and predicted embeddings. In this setup, the common space emerges organically during training without explicit constraints. However, this approach has two key drawbacks: (1) it requires extended training to form a satisfactory common space and achieve convergence, and (2) the subject-specific embeddings in the common space are not strongly aligned, making it harder for the residual MLP to map multiple subject inputs to a unified output.\n\n\nTo address these challenges, we propose Adapter Alignment (AA) training. In AA, the pipeline is first pre-trained end-to-end on a single reference subject, chosen based on reconstruction performance and data availability. Among NSD subjects, Subject 1 consistently exhibited the best reconstruction performance, making it the ideal candidate for constructing the reference space. After pre-training, the output embeddings for the subject-specific adapter, particularly those corresponding to the shared images across subjects, are extracted.\n\n\nWhen fine-tuning a new subject, we adopt a two-step process. First, we train the new subject's adapter using the shared images, with the objective of aligning its output to the embeddings of Subject 1 for the same images. The adapter is allowed to overfit on these shared images until the loss is minimized. In the second step, we unfreeze the entire pipeline and resume training, applying the usual losses at the final output. Additionally, we introduce an MSE loss at the adapter level specifically for the common images, which allows the adapter to restructure itself to account for subject-specific discrepancies and adapt to the unique images encountered by the new subject. We call this process of overfitting the adapter and letting it fine-tune end to end \"AAMax\".\n\n\nThis training method offers improvements in both high-data and low-data scenarios. When all training data is available, AAMax converges faster while achieving performance similar to traditional end-to-end training. However, in limited data settings, AAMax proves significantly more efficient, yielding better reconstructions and higher accuracy with fewer training samples.\n\n\n# 3.3 RECONSTRUCTION PIPELINE\n\n\n![Figure 4: fMRI Reconstruction Pipeline. The high level pipeline maps data to CLIP ViT-L-14 when using Versatile Diffusion and OpenCLIP ViT/bigG-14 when using Stable Diffusion XL. The Adapter Alignment Procedure is explained in more detail in Section 3.2](images/25b3519a15c4de70a40254ad25e705ba28462f8b8f54c25d5cefb160b7f730fd.jpg)\n\n\nOur reconstruction pipeline follows a similar structure to previous works, dividing the reconstruction process into two components: a high-level semantic pipeline that maps to CLIP embeddings and a low-level visual pipeline that maps to visual autoencoder embeddings. Both pipelines receive the same fMRI data as input but are trained separately. The outputs from both pipelines are then integrated into a pre-trained diffusion model to generate the final reconstruction.\n\n\nFlattened fMRI voxels from the nsdgeneral region are fed into a subject-specific adapter, consisting of a single linear layer with a GELU non-linearity. This step maps the fMRI data into a subject-agnostic shared space. The shared space representation is then processed by a Residual Multi-Layer Perceptron (MLP). For mappings into the CLIP (Radford et al., 2021) embedding space, we further incorporate a pre-trained diffusion prior based on DALL-E 2 (Ramesh et al., 2022) to bridge the modality gap between fMRI data and CLIP embeddings.\n\n\nOur high-level pipeline is based on the architecture used in MindEye1 (Scotti et al., 2023) and MindEye2 (Scotti et al., 2024), where the fMRI data is mapped to CLIP image embeddings of the corresponding source images. The pretraining phase leverages a combination of diffusion prior loss and CLIP loss. During fine-tuning, we add an additional MSE loss for the common images to further refine the mappings. The overall loss function in this stage is:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\mathrm {n e w}} = \\left\\{ \\begin{array}{l l} \\lambda_ {1} \\mathcal {L} _ {\\mathrm {P r i o r}} + \\lambda_ {2} \\mathcal {L} _ {\\mathrm {C L I P}}, & \\text {f o r n o n - c o m m o n i m a g e s} \\\\ \\lambda_ {1} \\mathcal {L} _ {\\mathrm {P r i o r}} + \\lambda_ {2} \\mathcal {L} _ {\\mathrm {C L I P}} + \\lambda_ {3} \\mathcal {L} _ {\\mathrm {M S E}} (\\mathbf {z} _ {\\mathrm {n e w}}, \\mathbf {z} _ {\\mathrm {r e f}}), & \\text {f o r c o m m o n i m a g e s} \\end{array} \\right.\n$$\n\n$$\n\n\nwhere  $\\mathbf{z}_{\\mathrm{adapter, new}}$  and  $\\mathbf{z}_{\\mathrm{adapter, ref}}$  are the adapter-level embeddings for the new and reference subjects, respectively, on the shared common images. We present a breakdown of the high level pipeline loss functions in Appendix H.\n\n\nThe low-level pipeline also follows a similar structure to the high-level one but maps the fMRI data to latent embeddings derived from a custom autoencoder. This autoencoder is trained on downscaled images from the NSD dataset (64x64x3) and reduces them to a latent space of size 1024. The low-level pipeline learns to map to this latent space, and passing these embeddings through a pre-trained decoder produces blurry reconstructions. The loss function for this stage is:\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {n e w}} = \\left\\{ \\begin{array}{l l} \\mathcal {L} _ {\\text {M S E}} (\\mathbf {z} _ {\\text {f M R I , n e w}}, \\mathbf {z} _ {\\text {A E}}), & \\text {f o r n o n - c o m m o n i m a g e s} \\\\ \\mathcal {L} _ {\\text {M S E}} (\\mathbf {z} _ {\\text {f M R I , n e w}}, \\mathbf {z} _ {\\text {A E}}) + \\mathcal {L} _ {\\text {M S E}} (\\mathbf {z} _ {\\text {a d a p t e r , n e w}}, \\mathbf {z} _ {\\text {a d a p t e r , r e f}}), & \\text {f o r c o m m o n i m a g e s} \\end{array} \\right.\n$$\n\n$$\n\n\nwhere  $\\mathbf{z}_{\\mathrm{fMRI, new}}$  is the output embedding for the new subject,  $\\mathbf{z}_{\\mathrm{AE}}$  is the AE latent space representation of the image and  $\\mathbf{z}_{\\mathrm{adapter, new}}$  and  $\\mathbf{z}_{\\mathrm{adapter, ref}}$  are the adapter-level embeddings for the new and reference subjects, respectively. We present a breakdown of the low level pipeline loss functions in Appendix I.\n\n\nFor the final step, the mapped CLIP embedding is used as a conditioning input to the pre-trained diffusion prior model. The low-level blurry reconstructions serve as an initialization for the diffusion model, and the final, refined reconstructions are then produced. For more architectural details, we refer readers to the MindEye1 (Scotti et al., 2023) and MindEye2 (Scotti et al., 2024)papers.\n\n\n# 3.4 TESTS ON COMPLETE DATA\n\n\nWe first validate our training method using the full dataset. Subject 1 is used as the reference subject, while fine-tuning is performed on Subjects 2, 5, and 7. The results are presented in Table 2. We use the same set of metrics as Scotti et al. (2024) to evaluate our reconstructions. Pixel Correlation, SSIM and 2-way percent correct for the 2nd and 5th layer of AlexNet are considered low level visual metrics. 2-way percent correct for Inception(Szegedy et al., 2016), CLIP, EfficientNet(Tan, 2019) and SwAV(Caron et al., 2020) score are considered high level metrics. When trained to completion, both methods\u2014end-to-end training and adapter alignment\u2014show similar overall performance. However, the MSE loss at the adapter level is significantly lower (by an order of magnitude as shown in Appendix C.2) when using adapter alignment. This allows for more precise post-hoc analysis after extracting embeddings from the common space.\n\n\nAdditionally, we observe that adapter alignment yields better results than regular end-to-end training when the model is trained for fewer epochs, with faster convergence. Both approaches eventually reach similar performance levels, as shown in Figure 5.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">Low-Level</td><td colspan=\"4\">High-Level</td></tr><tr><td>PixCorr \u2191</td><td>SSIM \u2191</td><td>Alex(2) \u2191</td><td>Alex(5) \u2191</td><td>Incep \u2191</td><td>CLIP \u2191</td><td>Eff \u2193</td><td>SwAV \u2193</td></tr><tr><td>Subj1 Pretraining</td><td>0.345</td><td>0.346</td><td>92.80%</td><td>96.88%</td><td>94.40%</td><td>90.02%</td><td>0.692</td><td>0.399</td></tr><tr><td>Normal FineTune Avg</td><td>0.258</td><td>0.339</td><td>89.64%</td><td>95.05%</td><td>92.63%</td><td>89.64%</td><td>0.717</td><td>0.427</td></tr><tr><td>AAMax FineTune Avg</td><td>0.259</td><td>0.337</td><td>89.20%</td><td>95.09%</td><td>92.68%</td><td>89.49%</td><td>0.713</td><td>0.423</td></tr><tr><td>Subj2 FineTune Normal</td><td>0.294</td><td>0.345</td><td>91.42%</td><td>95.92%</td><td>93.00%</td><td>88.68%</td><td>0.721</td><td>0.425</td></tr><tr><td>Subj2 FineTune AAMax</td><td>0.295</td><td>0.340</td><td>91.52%</td><td>96.20%</td><td>92.96%</td><td>89.87%</td><td>0.707</td><td>0.409</td></tr><tr><td>Subj5 FineTune Normal</td><td>0.236</td><td>0.335</td><td>89.45%</td><td>95.32%</td><td>93.98%</td><td>91.63%</td><td>0.701</td><td>0.419</td></tr><tr><td>Subj5 FineTune AAMax</td><td>0.240</td><td>0.332</td><td>88.60%</td><td>95.16%</td><td>94.04%</td><td>90.71%</td><td>0.698</td><td>0.413</td></tr><tr><td>Subj7 FineTune Normal</td><td>0.244</td><td>0.339</td><td>88.06%</td><td>93.91%</td><td>90.92%</td><td>88.61%</td><td>0.731</td><td>0.438</td></tr><tr><td>Subj7 FineTune AAMax</td><td>0.242</td><td>0.340</td><td>87.48%</td><td>93.91%</td><td>91.05%</td><td>87.89%</td><td>0.734</td><td>0.448</td></tr></table>\n\n\nTable 2: Quantitative results on training the reconstruction models on all 40 hours of data. Both methods perform similarly across all metrics.\n\n\n# 3.5 TESTS ON LIMITED DATA\n\n\nA key objective of multi-subject pipelines is to achieve satisfactory models of new subjects with limited data. Acquiring fMRI data is costly, and reducing the data requirements for modeling a subject's brain makes fMRI reconstruction more accessible. MindEye2 demonstrated that fine-tuning a new subject with as little as 1 hour of data is possible, though the resulting reconstructions leave room for improvement.\n\n\n![Figure 5: Comparing finetuning reconstruction performance over time with all the data. From left to right: (a) High level metrics over time for Subject 1. Both methods perform similarly but the normal method takes about 20 epochs to match AAmax's numbers (b) Example reconstruction over time. The top row is the normal pipeline and the bottom row is the Adapter alignment pipeline.](images/90d94af57a79add0e1e469d8c07e585d9c2a6a8c50f67c82d6e320ee10d1493b.jpg)\n\n\nIn our experiments, we evaluate the fine-tuning of a new subject in three scenarios: 1 hour, 2 hours, and 4 hours of training data. The NSD dataset provides 40 hours of data per subject, all of which is used for the full data tests in Section 3.4. For this experiment, we designate Subject 5 as the reference subject and fine-tune Subject 1 using the shared images. We compare the performance of normal end-to-end training and Adapter Alignment (AA), with results presented in Table 3.\n\n\nAdapter Alignment consistently outperforms the end-to-end approach, showing significant improvements even after just one epoch of training. The performance gap widens as the amount of fine-tuning data is reduced. Due to the lightweight nature of Adapter Alignment, which involves training on just 250 images using a single layer, the time required to align the adapter is minimal compared to full end-to-end training. In fact, even after 160 epochs of normal training, the model's performance fails to match the Epoch 1 results of Adapter Alignment. Figure 6 illustrates the reconstruction performance across epochs for both methods.\n\n\n![Figure 6: Comparing finetuning reconstruction performance over time with limited data. From left to right: (a) Finetuning Subject 1 with 250 images or 1 hour of data (b) Finetuning Subject 1 with 500 images or 2 hours of data (c) Finetuning Subject 1 with 1000 images or 3 hours of data. We observe that adapter alignment (AAMax) not only outperforms normal end-to-end training, but the performance at Epoch 1 is better than the normal method's performance at Epoch 160.](images/cac86a5a9787d88217707f484de2bacaed4234f1d0bd83105c80194b3c1daef7.jpg)\n\n\n![](images/6cf19ed9819f1168942a5a0ef9a90c26cee26284aa0d91abfb0228502d83ea25.jpg)\n\n\n![](images/c7718cf32635aac70c5984e796f5c7478ae5cf015315a018f4ae570bff91c35f.jpg)\n\n\n# 4 BEST CANDIDATE IMAGE SELECTION\n\n\nHere, we present a selection algorithm that significantly reduces the amount of data required to train a model for a new subject. Later, we carry out an empirical analysis of the method.\n\n\n# 4.1 GREEDY ALGORITHM\n\n\nIn the context of fMRI-based visual reconstruction, we introduce a greedy algorithm that selects the most representative subset of images from the common space. We begin with an embedding of the common set of images into a representation space and choose the first  $d$  dimensions corresponding to the principal singular vectors. For each dimension  $j$ , we partition the range into  $B_{j}$  bins that is determined by the ratio of singular value  $\\lambda_{j}$  to the largest singular value  $\\lambda_{1}$ :\n\n\n$$\n\n$$\nB _ {j} = \\left\\lfloor w \\cdot \\frac {\\lambda_ {j}}{\\lambda_ {1}} \\right\\rfloor , \\quad j = 1, 2, \\dots , d\n$$\n\n$$\n\n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">Low-Level</td><td colspan=\"4\">High-Level</td></tr><tr><td>PixCorr \u2191</td><td>SSIM \u2191</td><td>Alex(2) \u2191</td><td>Alex(5) \u2191</td><td>Incep \u2191</td><td>CLIP \u2191</td><td>Eff \u2193</td><td>SwAV \u2193</td></tr><tr><td>Subj1 FT NS 250 Normal E1</td><td>0.031</td><td>0.234</td><td>58.31%</td><td>62.56%</td><td>58.51%</td><td>58.71%</td><td>0.947</td><td>0.611</td></tr><tr><td>Subj1 FT NS 250 AAMAX E1</td><td>0.095</td><td>0.254</td><td>77.20%</td><td>85.98%</td><td>78.80%</td><td>79.39%</td><td>0.833</td><td>0.503</td></tr><tr><td>Subj1 FT NS 250 Normal E160</td><td>0.094</td><td>0.239</td><td>71.54%</td><td>80.10%</td><td>73.11%</td><td>74.02%</td><td>0.875</td><td>0.537</td></tr><tr><td>Subj1 FT NS 250 AAMAX E160</td><td>0.105</td><td>0.259</td><td>76.77%</td><td>86.20%</td><td>80.08%</td><td>80.14%</td><td>0.828</td><td>0.496</td></tr><tr><td>Subj1 FT NS 500 Normal E1</td><td>0.049</td><td>0.242</td><td>62.41%</td><td>68.06%</td><td>62.64%</td><td>64.80%</td><td>0.925</td><td>0.583</td></tr><tr><td>Subj1 FT NS 500 AAMAX E1</td><td>0.112</td><td>0.275</td><td>79.46%</td><td>88.29%</td><td>83.28%</td><td>82.61%</td><td>0.803</td><td>0.485</td></tr><tr><td>Subj1 FT NS 500 Normal E160</td><td>0.110</td><td>0.270</td><td>75.32%</td><td>84.62%</td><td>76.66%</td><td>78.55%</td><td>0.845</td><td>0.516</td></tr><tr><td>Subj1 FT NS 500 AAMAX E160</td><td>0.114</td><td>0.285</td><td>80.00%</td><td>88.50%</td><td>83.19%</td><td>83.05%</td><td>0.799</td><td>0.480</td></tr><tr><td>Subj1 FT NS 1000 Normal E1</td><td>0.079</td><td>0.241</td><td>65.84%</td><td>73.04%</td><td>64.19%</td><td>68.10%</td><td>0.914</td><td>0.574</td></tr><tr><td>Subj1 FT NS 1000 AAMAX E1</td><td>0.129</td><td>0.265</td><td>80.83%</td><td>90.93%</td><td>86.66%</td><td>86.17%</td><td>0.767</td><td>0.460</td></tr><tr><td>Subj1 FT NS 1000 Normal E160</td><td>0.124</td><td>0.253</td><td>77.86%</td><td>89.18%</td><td>83.10%</td><td>83.26%</td><td>0.801</td><td>0.480</td></tr><tr><td>Subj1 FT NS 1000 AAMAX E160</td><td>0.127</td><td>0.273</td><td>81.73%</td><td>91.29%</td><td>86.23%</td><td>86.26%</td><td>0.763</td><td>0.453</td></tr></table>\n\n\nTable 3: Quantitative results on fine tuning a new subject with limited data. FT=Fine tuning. The first section shows the result of fine tuning with 250 images after 1 epoch and after 160 epochs using normal end-to-end training vs AAMax. The second section compares performance on 500 images and the third section compares performance on 1000 images. In all cases AAMax, significantly outperforms normal training. The greyed out low level metrics are not optimized for in these experiments.\n\n\nwhere  $w$  is a predefined scaling parameter. Dimensions with larger singular values are partitioned into more bins, reflecting their greater variance and importance in capturing meaningful data features.\n\n\nEach image (and its fMRI) maps to exactly one bin in each dimension. Our goal is to find the smallest subset of images that covers all bins in each dimension. This subset is critical in selecting the most representative images for a new subject, ensuring that the selected images capture the key activity patterns that generalize across subjects. The problem as stated is NP-hard. However, it is submodular and a greedy heuristic such as given below achieves an  $(1 - 1 / e)$  approximation ratio.\n\n\nGiven a subset  $S$  of images, let  $Gap(S, j)$  be the number of empty bins in dimension  $j$ , and let  $Gap(S) = \\Sigma_{j} Gap(S, j)$  be the total number of images across all dimensions. At every step, the greedy algorithm chooses image  $i \\notin S$  that minimizes  $Gap$  when added to  $S$ .\n\n\n# 4.2 RESULTS\n\n\nWe achieve better results on high-level metrics compared to state-of-the-art methods while reducing the required data by  $40\\%$ . Figure 7 (a) illustrates how Inception and CLIP distances fluctuate as a function of the amount of training data. Figure 7 (b) presents the model's performance when 150 images are selected for the training set, while varying the number of principal eigenvectors onto which the embeddings are projected. A notable improvement is observed when the number of eigenvectors reaches 40, likely due to the nature of the binning process, as the number of bins created along the eigenvectors provides better coverage. Further analysis of this behavior presents an interesting direction for future work. Our method highlights the critical role of strategic data selection in achieving superior performance. For additional details on the experimental performance, please refer to Tables 4 and 5 in the Appendix Section A.1.\n\n\n# 5 RELATED WORK\n\n\nRecent works have achieved impressive results by mapping fMRI data to latent diffusion model (LDM) spaces (Takagi & Nishimoto, 2023; Scotti et al., 2023; Lu et al., 2023; Xia et al., 2024), while simultaneously integrating multiple modalities. Despite this progress, these methods have not been thoroughly tested for their generalization performance across a larger population of subjects. In other words, constructing a model for a completely new subject still requires retraining the model from scratch to ensure optimal performance.\n\n\n![Figure 7: Comparison of the performance of the image selection strategy. Both figures display the Inception and CLIP distance metrics. (a) Performance with varying training set sizes, where the red and black lines (labeled as Inception ME2 and CLIP ME2) at the 250-point mark indicate the model's performance without a data selection strategy. (b) Performance with varying numbers of principal eigenvectors.](images/32e9efa66924c8c8f80e5cbf62a93339df7ebb8ae9ae230f5f90637d24d2cc03.jpg)\n\n\n![](images/1b2a391b6e0692c0aafecf19ee4de43fdcc826f8ecaca5bee44bfcee48fa7980.jpg)\n\n\nApproaches like Ferrante et al. (2024) have demonstrated cross-subject brain decoding using limited data samples by aligning multiple subjects. Although the authors, like us, leveraged visual stimuli commonly used for multiple subjects, the fidelity of their reconstructed images lags behind current advancements. Scotti et al. (2024), the foundation of our method, produced significant results in image reconstruction under limited data conditions but did not fully exploit the properties of the common representation space. In contrast, our novel approach enables faster convergence and improved performance in a limited data setting by effectively leveraging the structure of the common space.\n\n\n# 6 CONCLUSIONS & DISCUSSION\n\n\nIn this work, we present compelling evidence for the existence of a shared visual representation space across subjects, allowing brain activity from different individuals to be aligned in a common space. Building on this foundation, we introduce Adapter Alignment (AA), a novel and efficient approach for subject-specific alignment in fMRI-based visual reconstruction. Our method, which is more efficient than traditional end-to-end training pipelines, achieves faster convergence and superior performance, particularly in low-data scenarios. Additionally, we propose a greedy image selection algorithm that identifies the most representative subset of images for training new subjects. This approach reduces the amount of data required for fine-tuning by up to  $40\\%$  without sacrificing performance, addressing the challenge of minimizing data requirements in fMRI-based image reconstruction.\n\n\nBy leveraging the structure of the common representation space, we demonstrate the feasibility of producing high-quality visual reconstructions with a significantly reduced amount of subject-specific data. Future work could explore the integration of non-linear adaptors to enhance concept extraction, potentially improving the performance and flexibility of the model. Another promising direction is to extend our methodology to different pipelines, ensuring its generalizability across various architectures, thereby making the approach architecture-agnostic. Furthermore, validating the results using different datasets (Contier et al., 2021) would offer valuable insights into the robustness of the method across diverse data settings.\n\n\n# REFERENCES\n\n- Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\n- Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. A massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, 25(1):116-126, 2022.\n\n- Pinglei Bao, Liang She, Mason McGill, and Doris Y Tsao. A map of object space in primate inferotemporal cortex. Nature, 583(7814):103-108, 2020.\n\n- Roman Belyi, Guy Gaziv, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal Irani. From voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912-9924, 2020.\n\n- Thomas B Christophel, Martin N Hebart, and John-Dylan Haynes. Decoding the contents of visual short-term memory from human visual and parietal cortex. Journal of Neuroscience, 32(38): 12983-12989, 2012.\n\n- Oliver Contier, Martin N Hebart, Adam H Dickter, Lina Teichmann, Alexis Kidder, Anna Corriveau, Charles Zheng, Maryam Vaziri-Pashkam, and Charles Baker. Things-fmri/meg: A large-scale multimodal neuroimaging dataset of responses to natural object images. Journal of Vision, 21(9): 2633-2633, 2021.\n\n- Edward F Ester, David E Anderson, John T Serences, and Edward Awh. A neural measure of precision in visual working memory. Journal of cognitive neuroscience, 25(5):754-761, 2013.\n\n- Matteo Ferrante, Tommaso Boccato, Furkan Ozcelik, Rufin VanRullen, and Nicola Toschi. Through their eyes: multi-subject brain decoding with simple alignment techniques. Imaging Neuroscience, 2:1-21, 2024.\n\n- Marshall L Fisher, George L Nemhauser, and Laurence A Wolsey. An analysis of approximations for maximizing submodular set functions-II. Springer, 1978.\n\n- Guy Gaziv, Roman Belyi, Niv Granot, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal Irani. Self-supervised natural image reconstruction and large-scale semantic classification from brain activity. NeuroImage, 254:119121, 2022.\n\n- Stephenie A Harrison and Frank Tong. Decoding reveals the contents of visual working memory in early visual areas. Nature, 458(7238):632-635, 2009.\n\n- Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023. URL https://arxiv.org/abs/1606.08415.\n\n- Alexander G Huth, Shinji Nishimoto, An T Vu, and Jack L Gallant. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. Neuron, 76(6):1210-1224, 2012.\n\n- Sirawaj Itthipuripat, Thomas C Sprague, and John T Serences. Functional mri and eeg index complementary attentional modulations. Journal of Neuroscience, 39(31):6162-6179, 2019.\n\n- Yukiyasu Kamitani and Frank Tong. Decoding the visual and subjective contents of the human brain. Nature neuroscience, 8(5):679-685, 2005.\n\n- Hsin-Hung Li, Thomas C Sprague, Aspen H Yoo, Wei Ji Ma, and Clayton E Curtis. Joint representation of working memory and uncertainty in human cortex. Neuron, 109(22):3699-3712, 2021.\n\n- Sikun Lin, Thomas Sprague, and Ambuj K Singh. Mind reader: Reconstructing complex images from brain activities. Advances in Neural Information Processing Systems, 35:29624-29636, 2022.\n\n- Yulong Liu, Yongqiang Ma, Guibo Zhu, Haodong Jing, and Nanning Zheng. See through their minds: Learning transferable neural representation from cross-subject fmri, 2024. URL https://arxiv.org/abs/2403.06361.\n\n- Bria Long, Chen-Ping Yu, and Talia Konkle. Mid-level visual features underlie the high-level categorical organization of the ventral stream. Proceedings of the National Academy of Sciences, 115(38):E9015-E9024, 2018.\n\n- Yizhuo Lu, Changde Du, Qiongyi Zhou, Dianpeng Wang, and Huiguang He. Minddiffuser: Controlled image reconstruction from human brain activity with semantic and structural diffusion. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 5899-5908, 2023.\n\n- Milad Mozafari, Leila Reddy, and Rufin VanRullen. Reconstructing natural scenes from fmri patterns using bigbigan. In 2020 International joint conference on neural networks (IJCNN), pp. 1-8. IEEE, 2020.\n\n- Thomas Naselaris, Kendrick N Kay, Shinji Nishimoto, and Jack L Gallant. Encoding and decoding in fmri. Neuroimage, 56(2):400-410, 2011.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021.\n\n- Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n\n- Ziqi Ren, Jie Li, Xuetong Xue, Xin Li, Fan Yang, Zhicheng Jiao, and Xinbo Gao. Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning. NeuroImage, 228:117602, 2021.\n\n- Peter H. Schonemann. A generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):1-10, 1966.\n\n- Alexander Schrijver et al. Combinatorial optimization: polyhedra and efficiency, volume 24. Springer, 2003.\n\n- Miranda Scolari, Anna Byers, and John T Serences. Optimal deployment of attentional gain during fine discriminations. Journal of Neuroscience, 32(22):7723-7733, 2012.\n\n- Paul Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Aidan Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth Norman, et al. Reconstructing the mind's eye: fmri-to-image with contrastive learning and diffusion priors. Advances in Neural Information Processing Systems, 37, 2023.\n\n- Paul S Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth A Norman, et al. Mindeye2: Shared-subject models enable fmri-to-image with 1 hour of data. arXiv preprint arXiv:2403.11207, 2024.\n\n- Katja Seeliger, Umut G\u00fc\u00e7l\u00fc, Luca Ambrogioni, Yagmur G\u00fc\u00e7l\u00fct\u00fcrk, and Marcel AJ Van Gerven. Generative adversarial networks for reconstructing natural images from brain activity. NeuroImage, 181:775-785, 2018.\n\n- John T Serences and Geoffrey M Boynton. Feature-based attentional modulations in the absence of direct visual stimulation. Neuron, 55(2):301-312, 2007.\n\n- John T Serences, Edward F Ester, Edward K Vogel, and Edward Awh. Stimulus-specific delay activity in human primary visual cortex. Psychological science, 20(2):207-214, 2009.\n\n- Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu Horikawa, and Yukiyasu Kamitani. End-to-end deep image reconstruction from human brain activity. Frontiers in computational neuroscience, 13:432276, 2019.\n\n- Thomas C Sprague and John T Serences. Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices. Nature neuroscience, 16(12):1879-1887, 2013.\n\n- Thomas C Sprague, Edward F Ester, and John T Serences. Reconstructions of information in visual spatial working memory degrade with memory load. Current Biology, 24(18):2174-2180, 2014.\n\n- Thomas C Sprague, Sirawaj Itthipuripat, Vy A Vo, and John T Serences. Dissociable signatures of visual salience and behavioral relevance across attentional priority maps in human cortex. Journal of neurophysiology, 119(6):2153-2165, 2018.\n\n- Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826, 2016.\n\n- Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from human brain activity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14453-14463, 2023.\n\n- Mingxing Tan. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.\n\n- Brian A Wandell and Jonathan Winawer. Computational neuroimaging and population receptive fields. Trends in cognitive sciences, 19(6):349-357, 2015.\n\n- Weihao Xia, Raoul de Charette, Cengiz Oztireli, and Jing-Hao Xue. Dream: Visual decoding from reversing human visual system. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 8226-8235, 2024.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636554.3360798", "msecs": "336.0", "relativeCreated": "15029.220342636108", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Language-Informed Visual Concept Learning\n\n# Abstract\nOur understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training.\n\n\n# 1 INTRODUCTION\n\n\nIn order to make sense of the myriad visual entities in the world, humans develop an abstracted generative model of them and organize the underlying sources of variation into visual concepts, such as different colors or different types of objects. Designing systems that can recognize visual concepts within images as humans do has been a longstanding goal in the fields of computer vision and artificial intelligence (Russakovsky et al., 2015; Krizhevsky et al., 2012; Girshick et al., 2014).\n\n\nTo facilitate efficient reasoning and communication of these concepts, humans created symbolic depictions that have evolved into natural language. Such natural language grounding of visual data has been instrumental in the recent proliferation of powerful large vision-language models that are capable of semantically identifying objects in images (Radford et al., 2021; Kirillov et al., 2023) or generating photo-realistic images from arbitrary text prompts (Ramesh et al., 2021; Rombach et al., 2022; Sahara et al., 2022; Yu et al., 2022). While different concept axes can be easily specified by words, such as category and style, it is much less intuitive to delineate the subtleties of low-level visual nuances along each axis using language, such as one particular style of a painting.\n\n\nIn this work, our goal is to distill from large pre-trained vision-language models a function that extracts visual concepts along a set of language-specified concept axes from images. As illustrated in Figure 1, once these concepts are extracted, we can recompose them across different image instances at inference time to produce new images with novel concept combinations. To learn this function, rather than collecting a large-scale dataset of human annotations for each specific visual concept, we design a language-informed visual concept representation, and simply distill from a pre-trained Text-to-Image (T2I) generation model. There are three fundamental properties we seek in this visual concept representation.\n\n\nFirst, unlike T2I generation, which relies on generic words as visual concept descriptors, we would like to capture fine-grained visual nuances using continuous concept embeddings. One common technique is to invert the text-to-image generation process by optimizing an embedding with the objective of reproducing a given input image using a pre-trained T2I model, often referred to as Textual Inversion (Gal et al., 2022). However, most existing Textual Inversion methods (Gal et al.,\n\n\n![Figure 1: Language-Informed Visual Concept Learning. Our goal is to learn a visual concept representation grounded on a set of language-informed concept axes, e.g., category, color, and material, by simply distilling from pre-trained text-to-image generation models without manual annotations. After training, the concept encoders extract disentangled axis-specific embeddings from an image, which can be remixed to generate new images with novel concept compositions.](images/6cbedad65b640ee4643ec354e3272add961f72badef713adc46c3340efa57a0f.jpg)\n\n\n![Figure 2: Learned Disentangled Concept Embeddings Improve Compositionality. Left: Vanilla text-to-image model may fail to adhere to text prompts of uncommon combinations of concepts even with prompt engineering, e.g. \"red banana\". Right: With the same backbone T2I generator, our learned disentangled concept embeddings greatly enhance concept compositionality.](images/6265050443a8dd5d6436db8a5a10b895d5a05dfd528dd484535bac88388debcf.jpg)\n\n\n2022) optimize embeddings for individual image instances independently, overlooking the shared nature of visual concepts across instances. For instance, the concept of \"red\" is shared between a \"red apple\" and a \"red dress\". Moreover, the concepts of \"red\" and \"yellow\" also are instances of the property of color.\n\n\nHence, the second desired property of the visual concept representation is to preserve such common concept structures among various visual instances. Instead of optimizing on individual image instances independently, we design a set of concept encoders, where each encoder learns to encode the visual characteristics of an input image pertaining to one concept axis specified by language. This ensures that the inverted concept embeddings can be shared across different instances and remixed to generate new images.\n\n\nThe third crucial aspect of this representation is to ascertain that different concept axes are disentangled, allowing for changes to be made specifically on single concept axis without modifying other axes. To do so, we reuse the disentangled nature of linguistic concepts and ground the predictions to a set of discrete text anchors in the concept embeddings space, which can be obtained by querying a pre-trained generic Visual Question Answering (VQA) model, e.g., BLIP-2 (Li et al., 2023b). This soft anchoring constraint significantly improves the disentanglement of concept embeddings across different axes while still retaining sufficient leeway to capture nuanced visual variations that BLIP-2 struggles to discern, e.g., the style of an art piece in Figure 6.\n\n\nPutting these ideas together, we design a generic framework for learning disentangled and compositional visual concepts grounded to linguistic structures by exploiting pre-trained text-to-image generation and visual question answering models. We show that these concept encoders can be trained purely on synthetic images generated by a pre-trained T2I model, and extract concept embeddings from real images at test time, which capture the fine-grained visual nuances.\n\n\nOur contributions can be summarized as follows:\n\n- 1. We propose a generic framework for learning language-informed visual concepts by simply distilling pretrained vision-language models.\n\n- 2. At inference time, the trained concept encoders extract concept embeddings from a test image, which can be remixed to generate images with novel compositions of concepts.\n\n- 3. Using a light-weight test-time finetuning procedure, these encoders can also be quickly adapted to extract novel concepts unseen during training.\n\n\n4. Experiments show that this visual concept representation achieves better disentanglement and compositionality, compared to text-based prompting baselines, as shown in Figures 2 and 6.\n\n\n# 2 RELATED WORK\n\n\n# 2.1 VISUAL CONCEPT LEARNING\n\n\nDesigning learning-based systems to discover various visual concepts in natural images has been a long-standing goal in machine perception and intelligence. Early attempts typically rely on extensive semantic annotations done by humans, such as object classification (Barnard et al., 2003; Fei-Fei et al., 2006; Fergus et al., 2005), which were later epitomized by the effort of ImageNet (Russakovsky et al., 2015). Visual concepts are intrinsically linked to concepts in language, and such end-to-end supervised learning paradigms can be seen as learning a direct mapping between visual concepts and discrete linguistic concepts. Other approaches attempt to better exploit this inherent structure in language by constructing a structured representation of visual concepts such as scene graphs (Zhong et al., 2021) and symbolic programs (Mao et al., 2019; Han et al., 2019).\n\n\nMore recently, the success of natural language modeling (Devlin et al., 2018; Brown et al., 2020; Raffel et al., 2020) has paved the way for grounding visual concepts to open vocabularies, unlike category labels or fixed symbolic programs, by training large Vision-Language Models (VLMs) on massive image captioning datasets (Schuhmann et al., 2022). This has powered recent Text-to-Image (T2I) generation models to turn linguistic concepts from free-form text prompts into photo-realistic images (Rombach et al., 2022; Sahara et al., 2022). These T2I models have been leveraged by Personalization methods for extracting individual visual concepts from one or a few images. This is done by either by optimizing token embeddings (Gal et al., 2022; Vinker et al., 2023; Avrahami et al., 2023; Chefer et al., 2023b; Liu et al., 2023), finetuning the backbone denoiser (Ruiz et al., 2023), or training additional encoders for amortized optimization (Gal et al., 2023; Arar et al., 2023; Li et al., 2023a). We also distill visual concepts from a pre-trained T2I model, but unlike existing works, we train encoders to adhere to a set of language-specified concept axes, preserving the disentangled and compositional nature of language. Ranasinghe & Ryoo (2023) also explores language-defined concepts but focuses on video action recognition tasks while we focus on image generation.\n\n\nA separate line of work focuses on unsupervised visual concept disentanglement without explicitly leveraging language, typically by simply imposing information constraints in the latent space of a generative model, like VAEs and GANs (Higgins et al., 2017; Chen et al., 2016; Hsu et al., 2023). Here, we are interested in learning visual concepts that are explicitly grounded to language.\n\n\n# 2.2 CONTROLLABLE IMAGE GENERATION\n\n\nThe success of GAN-based image generation (Goodfellow et al., 2014; Brock et al., 2018; Karras et al., 2019) has spawned a series of works that discover controllable directions in the GAN latent space (Voynov & Babenko, 2020; Harkonen et al., 2020). More recently, the advancements of diffusion-based T2I models have unlocked new possibilities for controllable image generation, where photo-realistic images can be generated from free-form text prompts. Recent works proposed to further improve the alignment of image samples and input text conditions by manipulating attention maps within T2I models (Chefer et al., 2023a; Epstein et al., 2023). Another form of controllable image generation is compositional generation. Liu et al. (2022) proposes to improve the quality of T2I diffusion models for composing multiple pre-given concepts, specified via text prompts, by modifying the inference procedure. In this work, instead of assuming that concepts are given and are in a text format, we tackle the task of identifying disentangled concepts which can be used for composition.\n\n\nImage generation can also be controlled with image analogies (\u0160ubtrova et al., 2023; Hertzmann et al., 2001), a form of visual prompting. These works typically do not explicitly extract visual concepts from inputs unlike ours. In this work, we amalgamate both visual prompts and text queries, employing them as the editing interface.\n\n\n# 3 METHOD\n\n\nFig. 3 gives an overview of our proposed learning framework. Our goal in this work is to extract visual concepts from images along a number of concept axes specified by language, such as\n\n\n![Figure 3: Training Pipeline. During training, an input image is processed by a set of concept encoders that predict concept embeddings specific to given concept axes. These embeddings are trained to (1) retain information in order to reproduce visual inputs via a pre-trained Text-to-Image model given an axis-informed text template, and (2) ensure disentanglement across different axes by anchoring to text embeddings obtained from a pre-trained Visual Question Answering model.](images/b89b484cc861cafea48ff730bbca55210c3cfe07e46b038f971fe09bd2439a2b.jpg)\n\n\ncategory, color, and material, so as to enable the flexible composition of concepts into high-quality image generations.\n\n\nTo achieve this, we train a set of visual concept encoders by distilling concept guidance from pretrained vision-language models. Specifically, the encoders are trained to extract concept embeddings from an image in order to fulfill two objectives. First, they should be recomposed to explain the input image through a pretrained text-to-image (T2I) generation model, given a concept-axis-informed text prompt. Second, these visual concept embeddings should be anchored to the corresponding text embeddings obtained from a pre-trained visual question answering (VQA) model, further exploiting the disentangled nature of linguistic concepts for better disentanglement of visual concepts.\n\n\n# 3.1 VISUAL CONCEPT ENCODING BY INVERTING TEXT-TO-IMAGE GENERATION\n\n\nOur understanding of the visual world is centered around various concept axes, to which we have often assigned words due to their significance in communication and reasoning. This vision-language grounding has fueled recent explosion of text-to-image generation models (Rombach et al., 2022; Sahara et al., 2022; Ramesh et al., 2022), allowing them to generate photo-realistic images with various combinations of concepts defined by words.\n\n\nHere, we are interested in the reverse direction of text-to-image generation, where the goal is to extract language-grounded visual concepts present in natural images. Specifically, given  $K$  concept axes of interest defined by language, we would like to learn  $K$  concept encoders  $\\{f_k(\\cdot)\\}_{k=1}^K$ , each of which extracts a concept representation  $\\mathbf{e}_k = f_k(\\mathbf{x})$  along a concept axis from an input image  $\\mathbf{x}$ .\n\n\nIn order to train these concept encoders  $\\{f_k(\\cdot)\\}$ , instead of relying on extensive human labeling, we opt to exploit the vision-language grounding embedded within large pre-trained T2I generation models. Using the technique of Textual Inversion (Gal et al., 2022), one can optimize a token embedding  $<\\star>$  to capture a visual entity in a given image, through the objective of regenerating the image with the T2I model from a text template, such as \"a photo of  $<\\star>$ \". Here, we adopt a similar objective, but instead of inverting a specific embedding capturing the overall \"identity\" of an individual image instance, we would like to predict embeddings  $\\mathbf{e}_k$  that are grounded to a number of meaningful concept axes, using an axis-informed text template, such as \"a photo of  $<\\mathbf{e}_1>$  with  $<\\mathbf{e}_2>$  color and  $<\\mathbf{e}_3>$  material\". This allows the extracted concept embeddings to be shared across different images, encapsulating the common visual characteristics pertinent to one concept axis.\n\n\nSpecifically, given an image  $\\mathbf{x}$ , the concept encoders  $\\{f_k(\\cdot)\\}$  extract a set of concept embeddings  $\\{\\mathbf{e}_k\\in \\mathbb{R}^D\\}$ , which have the same dimension  $D$  as the text embeddings so that they can be directly inserted into the text embeddings of the axis-informed text template. To simplify the notations, let  $f_{\\gamma}(\\cdot)$  denote the function that takes in the image and produces the final sequence of embeddings\n\n\nof the template and the predicted concept embeddings, and  $\\gamma$  be the parameters of all the encoders which will be optimized during training. Let  $\\mathbf{c}_{\\theta}$  be the part of the T2I model's text encoder that takes in a sequence of text embeddings and outputs a conditioning vector for the T2I model's denoising network  $\\hat{\\epsilon}_{\\theta}$ , where  $\\theta$  denotes network parameters. We use DeepFloyd (StabilityAI; Sahara et al., 2022) as the backbone T2I model, which utilizes a pre-trained T5 model (Raffel et al., 2020) as the text encoder, and keep the parameters  $\\theta$  frozen in all experiments. To train the encoders, we reuse the training objective for the backbone diffusion model:\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {r e c o n}} (\\mathbf {x}; \\gamma) = \\mathbb {E} _ {\\boldsymbol {\\epsilon} \\sim \\mathcal {N} (\\mathbf {0}, I), t \\sim \\mathcal {U} ([ 0, 1 ])} \\left[ \\| \\hat {\\epsilon} _ {\\theta} (\\mathbf {x}, t, \\mathbf {c} _ {\\theta} (f _ {\\gamma} (\\mathbf {x}))) - \\boldsymbol {\\epsilon} \\| _ {2} ^ {2} \\right], \\tag {1}\n$$\n\n$$\n\n\nwhere the noise  $\\epsilon$  is sampled from a standardmultivariate Gaussian distribution and the timestep  $t$  is sampled from a uniform distribution in [0, 1]. Minimizing  $\\mathcal{L}^{\\mathrm{recon}}$  amounts to finding concept embeddings within the space of the pre-trained T2I model that can best reproduce the input image  $\\mathbf{x}$ , resembling a \"reconstruction\" objective.\n\n\nCompared to per-instance token optimization in vanilla Textual Inversion, the advantages of training these concept encoders are two-fold. First, the concept embedding space is naturally shared across different image instances, encapsulating the common understanding of the corresponding concept axes. Second, it makes training more efficient by amortizing the optimization across all instances, and more crucially, it allows for test-time inference in a feed-forward pass.\n\n\n# 3.2 CONCEPT DISENTANGLEMENT USING TEXT ANCHORS\n\n\nThe objective of  $\\mathcal{L}^{\\mathrm{recon}}$  ensures that the extracted concept embeddings can sufficiently reconstruct the concept of a given image through a pre-trained text-to-image generation model. However, with this loss alone, there is little guarantee that each embedding encodes only the information pertinent to a particular concept axis. In practice, we found that this baseline results in poor disentanglement of different concept axes when remixing the concept embeddings to generate new images, potentially due to the imprecise vision-language grounding in the pre-trained T2I model. For instance, as shown in Figure 8, the extracted category embedding  $<\\mathbf{e}_1>$  for \"red berries\" cannot be remixed with various color embeddings  $<\\mathbf{e}_2>$  e.g., \"orange\", as  $<\\mathbf{e}_1>$  is highly entangled with the concept of a \"red\" color due to the bias in natural images.\n\n\nTo encourage better disentanglement of different concept axes, we further incorporate a sparse set of text anchors into the concept embedding space. Along each concept axis like color, we have often named some prominent modes, such as \"red\" or \"yellow\", and these text labels entail clearly disentangled concepts. Therefore, we would like to reuse this disentangled nature of linguistic concepts to improve the disentanglement of visual concepts. To this end, we make use of the text predictions from a pre-trained Visual Question Answering (VQA) model, BLIP-2 (Li et al., 2023b), as pseudo ground-truth anchors for the concept embeddings.\n\n\nSpecifically, for each training image  $\\mathbf{x}$  and for each concept axis of interest (e.g., color) indexed by  $k$ , we query the BLIP-2 model  $\\Psi$  with the image  $\\mathbf{x}$  and a question  $q_{k}$  in natural language that is specific to this concept axis, e.g., \"what is the color of the object in the image\". Denote the answer from BLIP-2, also in the form of natural language, as  $\\Psi(\\mathbf{x}, q_{k})$ . We encode this answer with the pre-trained text encoder  $c_{\\theta}$  to obtain a text embedding  $\\tilde{\\mathbf{e}}_{k} = c_{\\theta}(\\Psi(\\mathbf{x}, q_{k}))$ . The prediction of our concept encoders  $f_{k,\\gamma}$  is encouraged to stay close to this anchor text embedding:\n\n\n$$\n\n$$\n\\mathcal {L} _ {k} ^ {\\text {a n c h o r}} (\\mathbf {x}; \\gamma) = \\| f _ {k, \\gamma} (\\mathbf {x}) - \\tilde {\\mathbf {e}} _ {k} \\| _ {2} ^ {2}, \\quad \\text {w h e r e} \\quad \\tilde {\\mathbf {e}} _ {k} = c _ {\\theta} \\left(\\Psi (\\mathbf {x}, q _ {k})\\right). \\tag {2}\n$$\n\n$$\n\n\nIt is crucial to highlight that we use these BLIP-2 predictions only as anchors by assigning a small weight to this anchor loss  $\\mathcal{L}_k^{\\mathrm{anchor}}$  during training. Otherwise, the embeddings predicted by the concept encoders could easily collapse to a set of discrete text embeddings and fail to capture the visual nuances in images.\n\n\n# 3.3 TRAINING AND INFERENCE\n\n\nTraining. Given a collection of training images  $\\mathcal{D}$  containing various combinations of concepts along each axis, the final objective to train the concept encoders consists of the two parts:\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {t o t a l}} (\\gamma) = \\mathbb {E} _ {\\mathbf {x} \\sim \\mathcal {D}} \\left[ \\mathcal {L} ^ {\\text {r e c o n}} (\\mathbf {x}; \\gamma) + \\sum_ {k = 1} ^ {K} \\lambda_ {k} \\mathcal {L} _ {k} ^ {\\text {a n c h o r}} (\\mathbf {x}; \\gamma) \\right]. \\tag {3}\n$$\n\n$$\n\n\nInference. At inference time, given a new test image, the concept encoders extract embeddings  $\\{\\mathbf{e}_k\\}$  capturing its characteristics along each concept axis of interest. These embeddings can be remixed across different images, or be replaced by embeddings converted from explicit words, to produce images with new compositions of visual concepts through the backbone T2I generator.\n\n\nGeneralization to Unseen Concepts via Test-Time Finetuning. While the encoders can precisely extract an axis-specific concept that has been seen during training from a new test image, they tend to be less robust to concepts unseen at training. However, with a lightweight test-time optimization procedure, where we use only the reconstruction objective  $\\mathcal{L}^{\\mathrm{recon}}$  to update the parameters for all encoders,  $\\gamma$ , these encoders can generalize to novel concepts unseen during training. Note that  $\\mathcal{L}^{\\mathrm{anchor}}$  is omitted here in order to capture the visual nuances without over-committing to the coarse text anchors. After training, the encoders have learned to generate outputs within a relatively narrow region of the embedding space, which allows the model to adapt to the test images shown in Figure 5 within around 600 iterations while maintaining disentanglement and compositional capability.\n\n\n# 4 EXPERIMENTS\n\n\n# 4.1 EXPERIMENT SETUP\n\n\nTraining Data Generation. We train the concept encoders only using synthetic images generated by DeepFloyd from 5 different domains, including fruits, figurines, furniture, art, and clothing. More details of our dataset can be found in A.2. For each dataset, we consider 2-3 concept axes, such as category, color, material, style, and season. For example, considering category and color for the fruits dataset, we generate training images by prompting DeepFloyd with text prompts describing varying combinations of categories and colors, e.g. \"a photo of an apple which is red in color\". Note that these text prompts are used only for data generation and not for training, as they may not be reliable (Figure 2). On average, we obtain 669 training images for each dataset.\n\n\nImplementation Details. Inspired by Gal et al. (2023), we leverage a pre-trained CLIP ViT/L-14 model for image encoding (Radford et al., 2021; Dosovitskiy et al., 2021), which was trained with a contrastive objective aligning image and text features, and hence well-suited for our task. We extract image features from CLIP ViT and train  $K$  separate concept encoders  $f_{k}$  on top of the features, which share the same architecture but maintains separate weights. Specifically, we take the [CLS] tokens from each CLIP layer and process each token with a distinct linear layer. This is different from Gal et al. (2023), which extracts [CLS] tokens from every other layer and uses a single shared linear layer for all token features. The transformed features are then aggregated with average pooling followed by a LeakyReLU (Xu et al., 2015) activation, and passed into another linear layer that produces the final predicted concept embeddings.\n\n\nTo ground the concept embeddings to the concept axes, we adapt the text templates from CLIP (Radford et al., 2021), which were originally used to assemble captions with class categories from ImageNet (Russakovsky et al., 2015). For training, we use AdamW (Loshchilov & Hutter, 2017) optimizer with learning rate 0.02, and randomly flip the images horizontally. For test-time finetuning, we use the AdamW optimizer with learning rate 0.001. We set  $\\lambda_{k} = 0.0001$  (Equation (3)) for the category axis and  $\\lambda = 0.001$  for others. We use IF-I-XL from DeepFloyd as the backbone model, with training resolution  $64 \\times 64$ . Training on one dataset takes approximately 12 hours on one NVIDIA GeForce RTX 3090 GPU. Generated images are upsampled  $256 \\times 256$  using IF-II-L for visualization purpose.\n\n\n# 4.2 QUALITATIVE RESULTS\n\n\nVisual Concept Extraction, Decomposition and Extrapolation. Once trained, the concept encoders can extract disentangled concept embeddings specific to each concept axis from different test images, which can be recomposed to generate new images with various concept compositions. As shown in Figure 4, across various datasets, our method is able to recompose axis-specific visual concepts from different images and consistently generate new images depicting the recomposed concepts. More examples can be found in Appendix A.1, where images generated from individual decomposed concept embeddings are also presented.\n\n\nThis disentangled concept representation also allows us to extrapolate along a particular concept axis for visual exploration, as shown in Figure 7. For instance, we can ask BLIP-2 \"what is the style\n\n\n![(c) Furniture](images/9ca0e6d02b392d03e980ae8e71551e4b0df2fbd1553f75e8a3e9ddd236c3c193.jpg)\n\n\n![Figure 4: Concept Decomposition. At test time, our model extracts visual concepts along various axes from different images and recompose them to generate new images. We show decomposition results across different pairs of concept axes in 3 datasets: (a) Fruits, (b) Paintings, and (c) Furniture. Figure 5: Generalization to Unseen Concepts via Finetuning. After test-time fine-tuning on a single test-time image, encoders can adapt to novel concept. Visual details from the input images are preserved as can be seen from images visualizing embedding predictions. Importantly, these embeddings do not overfit to the input images and maintain a good disentanglement, such that they can be freely recomposed to create new concepts. More results can be found in Figures 9 to 13. More real-world results can be found in Figures 20 to 24.](images/85d5816954f586fcaa9669d8c3122a783a6ffb288a6c17a0a841c707cb4048aa.jpg)\n\n\nof the painting?\" in an image, and prompt GPT-4 (OpenAI, 2023) to name a few alternatives. We can then recompose the text embeddings of these alternative styles with our concept embeddings and generate images to visualize these variants. Representing concepts as continuous embeddings further enables concept interpolations. Details and results are shown in Appendix A.6.\n\n\nGeneralization to Unseen Concepts via Test-Time Finetuning. Although the encoders have only seen a limited range of concepts during training due to the small size of the training dataset, it can be quickly adapted to unseen concepts with the lightweight test-time finetuning procedure in Section 3.3, as shown in Figure 5. For instance, after 600 finetuning iterations, the model can extract the specific style of the dog painting unseen at training and compose it with the content from other images. It can also capture nuanced colors e.g. yellow-ish-orange and transfer them to other objects.\n\n\n![Figure 6: Baselines Comparisons. Compared the text-based image editing methods, our method achieves significantly better compositionality due to the disentangled concept representation, and captures fine-grained color variations, which the baseline struggles to encode with language.](images/186927e07b61069a975658bfbcb53a7f42b9171212357179fb24ca686d9ddadd.jpg)\n\n\n<table><tr><td rowspan=\"3\"></td><td colspan=\"6\">CLIP-Score \u2191</td><td colspan=\"2\">Human Evaluation \u2191</td></tr><tr><td colspan=\"3\">Edit Category</td><td colspan=\"3\">Edit Color</td><td>Edit Cat.</td><td>Edit Clr.</td></tr><tr><td>Cat.&amp;Clr.</td><td>Cat.</td><td>Clr.</td><td>Cat.&amp;Clr.</td><td>Cat.</td><td>Clr.</td><td>Score</td><td>Score</td></tr><tr><td>Null-text Inversion</td><td>0.258</td><td>0.249</td><td>0.249</td><td>0.259</td><td>0.265</td><td>0.223</td><td>0.287</td><td>0.316</td></tr><tr><td>InstructPix2Pix</td><td>0.267</td><td>0.277</td><td>0.226</td><td>0.270</td><td>0.245</td><td>0.268</td><td>0.233</td><td>0.648</td></tr><tr><td>Baseline w/ BLIP-2</td><td>0.313</td><td>0.294</td><td>0.248</td><td>0.287</td><td>0.271</td><td>0.237</td><td>0.448</td><td>0.379</td></tr><tr><td>Ours</td><td>0.308</td><td>0.297</td><td>0.238</td><td>0.302</td><td>0.287</td><td>0.236</td><td>0.968</td><td>0.840</td></tr><tr><td>w/o Lkanchor</td><td>0.268</td><td>0.276</td><td>0.219</td><td>0.263</td><td>0.257</td><td>0.236</td><td>-</td><td>-</td></tr><tr><td>w/o Encoder &amp; Lkanchor</td><td>0.288</td><td>0.300</td><td>0.214</td><td>0.242</td><td>0.213</td><td>0.265</td><td>-</td><td>-</td></tr></table>\n\n\nTable 1: Quantitative Comparisons on Visual Concept Editing. Compared to existing image editing baselines (Brooks et al., 2023; Mokady et al., 2022), our method achieves better overall CLIP score when editing either axis, and is particularly effective at retaining category-related concepts as reflected in human evaluation. 'Cat' denotes Category and 'Crl' denotes Color.\n\n\n# 4.3 COMPARISON WITH PRIOR WORKS\n\n\nQualitative Comparisons While this task of image generation with disentangled visual concepts is new, we identified prior work that is capable of text-based image editing and generation, and establish a side-by-side comparison on the task of visual concept editing. Specifically, given an image  $\\mathbf{x}$ , for example, of a teal-colored apple in Figure 6, the task is to generate a new image  $\\hat{\\mathbf{x}}_{\\mathbf{e}_k \\to \\mathbf{e}_k'}$  with one concept axis  $k$  (e.g. category) modified by a text prompt, from  $\\langle \\mathbf{e}_k \\rangle$  to  $\\langle \\mathbf{e}_k' \\rangle$ , while preserving other axes  $\\{i | i \\neq k\\}$  from the input.\n\n\nWe compare to two existing methods. Null-text Inversion (Mokady et al., 2022) performs concept editing by first inverting the input image to token embeddings of Stable-Diffusion (Saharia et al., 2022) and then applying Prompt-to-Prompt (Hertz et al., 2022), which modifies cross attention maps for editing, a process that leads to pixel-aligned editing results. InstructPix2Pix (Brooks et al., 2023) is a conditional diffusion model for text-based image editing. Since it is trained on data curated with Prompt-to-Prompt for supervision, it also tends to generate pixel-aligned results. We also design a naive baseline for this task, where we query BLIP-2 for a text description of the attributes to be retained from the test image, and combine the answer with a target concept to generate the final recomposed image. As shown in Figure 6, our method achieves better decomposition results, whereas baseline methods fail to disentangle the desired axis-specific concepts from input images, and therefore struggle to faithfully reproduce the desired concepts from the image and the text.\n\n\nQuantitative Comparisons. We also conduct quantitative evaluations on the task of text-based visual concept editing and compare with prior work. Specifically, we record the ground-truth text\n\n\n![Figure 7: Visual Concept Extrapolation. Given an input image, we can extrapolate along a concept axis by querying BLIP-2 and GPT-4 to name a few alternatives to the concept in the input. Our model can then generate new variants of the input for visual exploration, by mixing them with the extracted concept embeddings.](images/156d041247697f43fbfafd963eacdff341bf090a710ea3b3147443cebac4467d.jpg)\n\n\n![Figure 8: Ablations. Both the text anchor loss and the encoder design reduce overfitting to input images and improve concept disentanglement. Removing these components significantly deteriorates the visual concept editing results. Additionally, we observe that the BLIP-2 baseline has difficulties transferring unnatural colors.](images/09e741c32b44cf2ae1e99561bd91ca98f1f15a6ac745de310932013bcab3c389.jpg)\n\n\nprompts  $y$  that we used to generate each training image  $\\mathbf{x}$ , and manually filter out the inconsistent pairs. For each concept axis  $k$ , we randomly select a target label  $\\langle \\mathbf{e}_k' \\rangle$  from a set of candidates that differ from the one in the original prompt  $\\langle \\mathbf{e}_k \\rangle$ . We then measure the alignment score between the modified image  $\\hat{\\mathbf{x}}_{\\mathbf{e}_k \\rightarrow \\mathbf{e}_k'}$  with the modified text prompt  $y_{\\mathbf{e}_k \\rightarrow \\mathbf{e}_k'}$  using CLIP (Radford et al., 2021), following Gal et al. (2022). We also break down the score to each individual axis, by computing the alignment of the image to a prompt specifying only one axis, e.g. \"a photo of  $\\langle \\mathbf{e}_k' \\rangle$ \". All results are summarized in Table 1. Our method captures the characteristics of category particularly well and outperforms others in changing category or preserving category while changing color, which highlights its ability to extract disentangled concepts allowing for flexible compositions. InstructPix2Pix is more effective in changing color, but tends to do poorly in preserving the category, as seen in Figure 6. More details on our quantitative results can be found in A.4.\n\n\nWe further conduct a human evaluation. Each participant is presented the results from all three methods in a random order together with the editing instruction, and asked to rank them considering both realism and faithfulness to the instruction (see Appendix A.3 for details). We aggregate the responses from 20 people and report the average average score normalized to 0-1 in Table 1. More details of our setup can be found in A.3.\n\n\n# 4.4 ABLATIONS\n\n\nWe conduct an ablation study to understand the effects of the proposed encoder (as opposed per instance optimization) and the text anchoring loss  $\\mathcal{L}_k^{\\mathrm{anchor}}$ . We use the same evaluation dataset and the CLIP-alignment metric as described in Section 4.3. As shown in Fig. 8 and Table 1, removing  $\\mathcal{L}_k^{\\mathrm{anchor}}$  and the encoders deteriorates disentanglement of the different concept axes due to severe overfitting, resulting in poor recomposition results. More results are included in the Appendix.\n\n\n# 5 CONCLUSION\n\n\nIn this paper, we have presented a framework for learning language-informed visual concepts from images, by simply distilling from pre-trained vision-language models. After training, the concept encoders extract disentangled concept embeddings along various concept axes specified by language, which can be remixed or edited to generate images with novel concept compositions. We conduct thorough evaluations both quantitatively and qualitatively, and demonstrate that our approach yields superior results in visual concept editing compared to prior work.\n\n\n# REFERENCES\n\n- Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H Bermano. Domain-agnostic tuning-encoder for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06925, 2023.\n\n- Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from a single image. SIGGRAPH Asia, 2023.\n\n- Kobus Barnard, Pinar Duygulu, David Forsyth, Nando de Freitas, David M. Blei, and Michael I. Jordan. Matching words and pictures. JMLR, 2003.\n\n- Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\n\n- Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023.\n\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020.\n\n- Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models, 2023a.\n\n- Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Michal Irani, Inbar Mosseri, and Lior Wolf. The hidden language of diffusion models. arXiv preprint arXiv:2306.00966, 2023b.\n\n- Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-gan: Interpretable representation learning by information maximizing generative adversarial nets. NeurIPS, 29, 2016.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.\n\n- Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. NeurIPS, 2023.\n\n- Li Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE TPAMI, 2006.\n\n- R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. Learning object categories from google's image search. In ICCV, 2005.\n\n- Bruce Fraser, Fred Bunting, and Chris Murphy. Real world color management, 2004.\n\n- Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.\n\n- Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Trans. Graph., 2023.\n\n- Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.\n\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014.\n\n- Chi Han, Jiayuan Mao, Chuang Gan, Josh Tenenbaum, and Jiajun Wu. Visual concept-metaconcept learning. In NeurIPS, 2019.\n\n- Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering interpretable gan controls. NeurIPS, 2020.\n\n- Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.\n\n- Aaron Hertzmann, Charles E Jacobs, Nuria Oliver, Brian Curless, and David H Salesin. Image analogies. In SIGGRAPH, 2001.\n\n- Irina Higgins, Loic Matthew, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017.\n\n- Kyle Hsu, Will Dorrell, James C. R. Whittington, Jiajun Wu, and Chelsea Finn. Disentanglement via latent quantization. In NeurIPS, 2023.\n\n- Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019.\n\n- Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.\n\n- Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023a.\n\n- Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023b.\n\n- Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. In ECCV, 2022.\n\n- Nan Liu, Yilun Du, Shuang Li, Joshua B Tenenbaum, and Antonio Torralba. Unsupervised compositional concepts discovery with text-to-image generative models. ICCV, 2023.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\n- Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In ICLR, 2019.\n\n- Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.\n\n- OpenAI. Gpt-4 technical report, 2023.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.\n\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n\n- Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\n- Kanchana Ranasinghe and Michael Ryoo. Language-based action concept spaces improve video self-supervised learning. NeurIPS, 2023.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pp. 10684-10695, 2022.\n\n- Nataniel Ruiz, Yanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023.\n\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015.\n\n- Donald G Saari. Selecting a voting method: the case for the borda count. Constitutional Political Economy, 34(3):357-366, 2023.\n\n- Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022.\n\n- Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022.\n\n- StabilityAI. DeepFloyd IF. https://github.com/deep-floyd/IF.\n\n- Ad\u00e9la \u0160ubrtov\u00e1, Michal Luk\u00e1\u010d, Jan \u010cech, David Futschik, Eli Shechtman, and Daniel Sykora. Diffusion image analogies. In SIGGRAPH, pp. 1-10, 2023.\n\n- Yael Vinker, Andrey Voynov, Daniel Cohen-Or, and Ariel Shamir. Concept decomposition for visual exploration and inspiration. arXiv preprint arXiv:2305.18203, 2023.\n\n- Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan latent space. In ICML, 2020.\n\n- Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.\n\n- Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\n\n- Yiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu, and Yin Li. Learning to generate scene graph from natural language supervision. In ICCV, 2021.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636555.6349335", "msecs": "634.0", "relativeCreated": "16328.073978424072", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph\n\n# Abstract\nLarge Language Models (LLMs) excel in code generation yet struggle with modern AI software engineering tasks. Unlike traditional function-level or file-level coding tasks, AI software engineering requires not only basic coding proficiency but also advanced skills in managing and interacting with code repositories. However, existing methods often overlook the need for repository-level code understanding, which is crucial for accurately grasping the broader context and developing effective solutions. On this basis, we present RepoGraph, a plug-in module that manages a repository-level structure for modern AI software engineering solutions. RepoGraph offers the desired guidance and serves as a repository-wide navigation for AI software engineers. We evaluate RepoGraph on the SWE-bench by plugging it into four different methods of two lines of approaches, where RepoGraph substantially boosts the performance of all systems, leading to a new state-of-the-art among open-source frameworks. Our analyses also demonstrate the extensibility and flexibility of RepoGraph by testing on another repo-level coding benchmark, CrossCodeEval.\n\n\n# 1 INTRODUCTION\n\n\nRecent advancements in large language models (LLMs) have showcased their powerful capabilities across various natural language processing tasks (OpenAI, 2023; Anil et al., 2023; Dubey et al., 2024), and now, coding-specific LLMs are emerging to tackle complex software engineering challenges (Hou et al., 2023; Fan et al., 2023), such as Code-Llama (Rozi\u00e8re et al., 2023) and StarCoder (Li et al., 2023a). These coding-specific LLMs are capable of assisting users with various software engineering tasks, even achieving human-level performance in many function-level coding tasks, such as program synthesis (Chen et al., 2021; Austin et al., 2021), code annotation (Yao et al., 2019), bug fixing (Tufano et al., 2019), and code translation (Rozi\u00e8re et al., 2020).\n\n\nReal-world software engineering often extends beyond single function or self-contained code files. Applications are typically built as repositories containing multiple interdependent files, modules, and libraries (Bairi et al., 2024). These complex structures require a holistic understanding of the entire codebase to perform tasks such as code completion (Shrivastava et al., 2023; Ding et al., 2023), feature addition (Liang et al., 2024), or issue resolving (Jimenez et al., 2024). Recent benchmarks like SWE-Bench (Jimenez et al., 2024) have been proposed to evaluate LLMs on real-world GitHub issues. It requires LLMs to modify the repository to resolve the issue, either by fixing a bug or introducing a new feature. This task is particularly challenging because it requires navigating complex code bases, understanding intricate dependencies between code files, and ensuring that changes integrate seamlessly without introducing new issues, which highlights the difficulties in scaling from function-level to repository-level understanding, as expounded in Figure 1.\n\n\nA key step in addressing repository-level tasks is to understand the structure of a repository and identify related code. To achieve this, retrieval-augmented generation (RAG) and its variants (Zhang et al., 2023; Phan et al., 2024; Wu et al., 2024) have been leveraged, in a procedural manner, to retrieve relevant code files across the repository first, providing context for LLMs for further edition.\n\n\n(a) Function-level Coding Problem\n\n\nInput text: Write a python function to find the first repeated character in a given string.\n\n\n![](images/9883c20b91c9601cb769ab3107450abf2647de1e7bf46fa8a1a880fdccf8bb42.jpg)\n\n\n1. def first_repeated_char(str1):\n\n- 2. for index,c in enumerate(str1):\n\n- 3. if str1[:index+1].count(c) > 1:return c\n\n- 4. return \"None\"\n\n\n(b) Repository-level Coding Problem\n\n\nIssue: modeling's \"separability_matrix\" does not compute separability correctly for CompoundModels...\n\n\n![Figure 1: The illustration of (a) a function-level coding problem from HumanEval (Chen et al., 2021) and (b) a repository-level coding problem from SWE-Bench (Jimenez et al., 2024).](images/4211fe98f1c4cec12d3fb9abb19390e3a7b311d09f6bab3ef01c17d82545fe3a.jpg)\n\n\n(i) Understand intricate dependencies\n\n\n![](images/f1d49d72c356a5a629a423aa534583b49eaa79e7dd4dc8dd5b51e970f43c3a9e.jpg)\n\n\nGenerated patch:\n\n- diff --git a/astropy/modeling/separable.py\n\n- --- a/astropy/modeling/separable.py\n\n- $+ + +$  b/astropy/modeling/separable.py @@-242,7  $+ 2 4 2 , 7 \\text{\u5df2}$  def cstack(left, right):...\n\n\n![](images/e71c4face282d548b2b6dd6d7f72227649ae3e018e4af3e9fb3eedb3a11ef9b3.jpg)\n\n\nUnit test:\n\n\n<table><tr><td>test_coord_matrix</td><td>X</td></tr><tr><td>test_cdot</td><td>\u2713</td></tr><tr><td>test_arith_opera</td><td>X</td></tr></table>\n\n\n(iii) Resolve without introducing new issues\n\n\nHowever, indexing at file-level can only identify semantically similar but not genuinely related code snippets. Instead of using RAG, recent approaches like Agentless (Xia et al., 2024) construct a skeletal format for each file, and directly prompt LLMs to identify relevant files and code lines. However, this method still treats code repositories as flat documents (Zhang et al., 2024), which suffers from limitations of repository structure such as the intricate inter-dependencies cross files. An alternative approach is to design agent frameworks (Yang et al., 2024; Wang et al., 2024), which enables LLMs to interact with repositories using actions. While LLM agents can freely determine the next action based on current observations, without the grasp of global repository structures, they tend to focus narrowly on specific files, resulting in local optimums. Addressing these limitations requires going beyond semantic matching and developing techniques that enable a deeper understanding of the codebase structure. This will allow LLMs to leverage fine-grained context across multiple files and function calls, facilitating more informed, repository-wide decision-making for coding tasks.\n\n\nMotivated by this, we propose REPOGRAPH, a plug-in module designed to help LLMs-based AI programmers leverage the code structure of an entire repository. REPOGRAPH is a graph structure and operates at the line level, offering a more fine-grained approach compared to previous file-level browsing methods. Each node in the graph represents a line of code, and edges represent the dependencies of code definitions and references. REPOGRAPH is constructed via code line parsing and encodes the structured representation of the entire repository. Sub-graph retrieval algorithms are then used to extract ego-graphs, which represent the relationships of a central node (in our case, specific keywords). These ego-graphs can be smoothly integrated with both procedural and agent frameworks, offering key clues that provide a more comprehensive context for LLMs to solve real-world software engineering problems.\n\n\nTo assess REPOGRAPH's effectiveness and versatility as a plug-in module, we integrate it with four existing software engineering frameworks and evaluate its performance using SWE-bench, a recent benchmark for AI software engineering. Experiment results show that REPOGRAPH boosts the success rate of existing methods for both agent and procedural frameworks by achieving an average relative improvement of  $32.8\\%$ . We also test REPOGRAPH on CrossCodeEval to verify its transferability to general coding tasks that require repository-level code understanding. Additionally, we systematically analyze different sub-graph retrieval algorithms and integration methods. Together with error analyses, we hope to shed light on future works targeting modern AI software engineering.\n\n\n# 2 RELATED WORKS\n\n\n# 2.1 LLM-BASED METHODS FOR AI SOFTWARE ENGINEERING\n\n\nRecently, there has been a significant increase in research focused on AI-driven software engineering, which can be broadly categorized into two primary approaches: (i) LLM agent-based frameworks and (ii) SWE-featured procedural frameworks. While this field has advanced rapidly, with most methods being released as proprietary solutions for industry applications (Cognition, 2024), our related work section will concentrate specifically on open-source frameworks.\n\n\nLLM agent-based framework equips large language models (LLMs) with a set of predefined tools, allowing agents to iteratively and autonomously perform actions, observe feedback, and plan future steps (Yang et al., 2024; Zhang et al., 2024; Wang et al., 2024; Cognition, 2024). While the exact\n\n\nset of tools may vary across different agent frameworks, they typically include capabilities such as opening, writing, or creating files, searching for code lines, running tests, and executing shell commands. To solve a problem, agent-based approaches involve multiple actions, with each subsequent turn depending on the actions taken in previous ones and the feedback received from the environment. For example, SWE-agent (Yang et al., 2024) facilitates interactions with the execution environment by designing a special agent-computer interface (ACI). There are various actions, including \"search and navigation\", \"file viewer and editor\", and \"context management\". Another work, AutoCodeRover (Zhang et al., 2024), further offers fine-grained searching methods for LLM agents in better contexts without an execution process. Specifically, it supports class and function-level code search. OpenDevin (Wang et al., 2024), initiated after Devin (Cognition, 2024), is a community-driven platform that integrates widely used agent systems and benchmarks. The action space design of OpenDevin is highly flexible, requiring LLM agents to generate code on the fly.\n\n\nSWE-featured procedural frameworks typically follow a two-step Localize/Search-Edit approach, as seen in existing literature (Zhang et al., 2023; Wu et al., 2024; Liang et al., 2024; Xia et al., 2024). The localize step focuses on identifying relevant code snippets, while the edit step involves completing or revising the code. Some works introduce additional steps to further enhance performance, such as the Search-Expand-Edit approach (Phan et al., 2024). Retrieval (Lewis et al., 2020) is a popular technique used for localization, allowing models to search for relevant code snippets from large repositories by treating issue descriptions as queries and code snippets as indexed data. Some approaches use a sliding window to ensure completeness (Zhang et al., 2023). Besides, Agentless (Xia et al., 2024) is a recently developed method that uses LLMs to directly identify relevant elements for editing within code repositories. It first recursively traverses the repository structure to generate a format that aligns files and folders vertically, with indices for subdirectories. This structure and the issue description are then input into the LLM, which performs a hierarchical search to identify the top-ranked suspicious files requiring further inspection or modification.\n\n\n# 2.2 REPOSITORY-LEVEL CODING CAPABILITY\n\n\nThe evaluation of coding capabilities in AI systems has traditionally focused on function-level or line-level assessments (Lu et al., 2021; Chen et al., 2021; Austin et al., 2021), where individual code snippets or isolated functions are the primary units of analysis. Unlike previous studies, SWE-bench (Jimenez et al., 2024) highlights the trend of repository-level coding, driven by recent advances of coding-specific LLMs (Guo et al., 2024; Li et al., 2023b). It reflects the growing user demand to understand and contribute to entire projects rather than isolated functions (Ouyang et al., 2023), as well as\n\n\nsolving real-world problems in an end-to-end and automatic manner.\n\n\nTable 1: Comparison between our approach REPOGRAPH and existing methods for representing the repository on various aspects. *RepoUnderstander (Ma et al., 2024) and CodexGraph (Liu et al., 2024) are concurrent works to ours.\n\n\n<table><tr><td>Model</td><td>Line-level</td><td>File-level</td><td>Repo-level</td></tr><tr><td>DraCo</td><td>X</td><td>\u2713</td><td>X</td></tr><tr><td>Aider</td><td>\u2713</td><td>X</td><td>X</td></tr><tr><td>RepoUnderstander*</td><td>X</td><td>\u2713</td><td>\u2713</td></tr><tr><td>CodexGraph*</td><td>X</td><td>\u2713</td><td>\u2713</td></tr><tr><td>REPOGRAPH</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></table>\n\n\nLong before the LLM era, the software engineering community has been studying the navigation of code repositories, including using eye-tracking data (Busjahn et al., 2015) and exploring interconnections (Begel et al., 2010). Then pre-trained code LLMs incorporate repository-level information such as file dependencies. But tasks at the repository level often involve more intricate call relationships within their context. Recent works like RepoCoder (Zhang et al., 2023) and RepoFuse (Liang et al., 2024) have started integrating RAG modules to harness additional information from repositories. Building on this, subsequent research has focused on embedding repository-level context into their methodologies. For instance, DraCo (Cheng et al., 2024) introduces importing relationships between files, CodePlan maintains the code changes by LLMs with incremental dependency analysis, while Aider (Gauthier, 2024) employs PageRank (Page, 1999) to identify the most significant contextual elements. RepoUnderstander (Ma et al., 2024) and CodexGraph (Liu et al., 2024) model code files as a knowledge graph. Despite similarities in representation, methods vary in how they retrieve information from these structures and utilize it for downstream tasks. Table 1 summarizes the differences between these methods and REPOGRAPH. REPOGRAPH surpasses previous approaches by effectively integrating context at the line, file, and repository levels.\n\n\n![(a) RepoGraph Construction](images/2d58b3de9c36ef24e6e7b44b1bb321b3caa41bb05fa7cd8ef79b2a34d51bf7ac.jpg)\n\n\n![(c) Integration with Agent Frameworks Figure 2: An in-depth illustration of (a) the construction, (b) the integration with procedural frameworks, and (c) the integration with agent frameworks of REPOGRAPH. Given a code repository, we first utilize AST to construct  $\\mathcal{G} = \\{\\mathcal{V},\\mathcal{E}\\}$ , where  $\\mathcal{G}$  consists of \"reference\" and \"definition\" node,  $\\mathcal{E}$  includes \"invoke\" and \"contain\" relations (files and code lines shown in corresponding color). The constructed REPOGRAPH are then used in procedural frameworks by adding sub-retrieval results into each step, and agent frameworks by adding graph retrieval as an additional action \"search_repograph\". A simplified version can be found in Figure 10.](images/126123f3edba1131af28c26f023bf724a182b63f285c8a33a79fc30aa0bf46e2.jpg)\n\n\n# 3 REPOGRAPH\n\n\nThis section introduces REPOGRAPH, a novel plug-in module that can be seamlessly integrated into existing research workflows for both agent-based and procedural frameworks. The primary goal of REPOGRAPH is to provide a structured way to analyze and interact with complex codebases, enabling detailed tracing of code dependencies, execution flow, and structural relationships across the repository. In the following sections, we will provide a detailed description of REPOGRAPH's construction, its underlying representation, and its utility across various scenarios. The overall architecture is depicted in Figure 2, highlighting its key components and operational flow.\n\n\n# 3.1 CONSTRUCTION\n\n\nGiven a repository-level coding task, the first step is to carefully examine the repository structure so that the necessary information can be collected. The input for REPOGRAPH construction is a repository, i.e., a collection of its folders and files, while the output is a structured graph, where each node is a code line, and each edge represents the dependencies in between. REPOGRAPH enables tracing back to the root cause of the current issue and gathering dependent code context to help solve the problem. The construction process of REPOGRAPH could be divided into three key steps.\n\n\nStep 1: Code line parsing. We first traverse the entire repository using a top-down approach to identify all code files as candidates for next-step parsing. This is accomplished by filtering based on file extensions, retaining only those with relevant code file suffixes (e.g., .py) while excluding other file types (e.g., .git or requirements.txt), which might be noisy and irrelevant for coding tasks. For each code file, we utilize tree-sitter  ${}^{2}$  to parse the code,leveraging its Abstract Syntax Tree (AST) framework. The AST provides a tree-based representation of the abstract syntactic structure of the source code, enabling the identification of key elements such as functions, classes, variables, types, and other definitions. While recognizing these definitions is crucial, tracing their usage and references throughout the code is equally important. Tree-sitter facilitates this by capturing the definitions and tracking where they are utilized or referenced within the codebase. For example, in figure 2, we not only identify definitions like class Model and its inherent methods but also references like self._validate_input.units(). After processing each line of code with a tree-sitter, we selectively retain lines that involve function calls and dependency relations, discarding extraneous information. Our focus is primarily on the functions and classes, as these represent the core structural components of\n\n\nthe code. By concentrating on these elements and their interrelationships, REPOGRAPH optimizes the analysis process by excluding less significant details, such as individual variables, which tend to be redundant and less relevant for further processing.\n\n\nStep 2: Project-dependent relation filtering. After the previous parsing step, we obtain code lines with calling and dependency relations. However, not all relations are useful for fixing issues. Specifically, many default and built-in function/class calls could distract from the project-related ones. Therefore, we additionally introduce a filtering process that excludes the repository-independent relations. Two types of such relations exist: (i) global relation refers to Python standard and built-in functions and classes. (ii) local relation are introduced by third-party libraries, which are specific to the current code file. For global relations, we maintain a comprehensive list of methods from standard and built-in libraries, excluding any identified relations from this list. The list is empirically constructed by gathering methods of the builtins library and default methods such as \"list\" and \"tuple\". For example, in figure 2, line inputs = len(input) is excluded since \"len\" is a default method. For local relations, we parse import statements in the code to identify third-party methods that are included, and exclude them accordingly. A detailed illustration of this step can be found in Figure 11.\n\n\nStep 3: Graph organization. At this stage, we construct REPOGRAPH using code lines as the fundamental building units. The graph can be represented as  $\\mathcal{G} = \\{\\mathcal{V},\\mathcal{E}\\}$ , where  $\\mathcal{V}$  represents the set of nodes, with each node corresponding to a line of code, and  $\\mathcal{E}$  represents the set of edges, capturing the relationships between these code lines. Each node in  $\\mathcal{V}$  contains attributes to represent its meta-information, such as line_number, file_name, directory, etc. Additionally, we classify each code line as either a \"definition\" (def) or a \"reference\" (ref) to a particular module. A \"def\" node corresponds to the line where a function, class, or variable is initially defined, while a \"ref\" node indicates a code line where this entity is referenced or invoked elsewhere in the code. Similar to soft links to \"def\" nodes, \"ref\" nodes also represent other variations of invoking methods. For example, in Figure 2, the class definition \"class Model\" would be a \"def\" node, while any subsequent usages of Model would be \"ref\" nodes. Each \"def\" node may have multiple \"ref\" nodes associated with it, as a single function or class can be referenced in various places throughout the code. We define two types of edges:  $\\mathcal{E}_{\\text{invoke}}$  and  $\\mathcal{E}_{\\text{contain}}$ . The triple  $(\\mathcal{V}_1,\\mathcal{E}_{\\text{contain}},\\mathcal{V}_2)$  denotes that  $\\mathcal{V}_1$  (e.g., a function definition) contains another module  $\\mathcal{V}_2$  (e.g., an internal function or class). The edge  $\\mathcal{E}_{\\text{contain}}$  typically connects a \"def\" node to its internal components. In contrast,  $\\mathcal{E}_{\\text{invoke}}$  represents an invocation relationship, usually connecting a \"def\" node to a \"ref\" node, where the reference node includes a dependency on the definition.\n\n\n# 3.2 UTILITY\n\n\nThe constructed REPOGRAPH serves as a structured representation of the current repository and facilitates better-related information collection and aggregation. For information collection based on REPOGRAPH, specifically, we use one search term each time for subgraph retrieval. Search terms are the key functions or classes that are determined by current states. For example, \"separability_matrix\" is the initial search term in Figure 2(c). We retrieve the  $k$ -hop ego-graphs (Hu et al., 2024) with the search term in the centric. The ego-graph is crucial for solving the problem because it focuses on the immediate relationships (Jin et al., 2024) around the search term, capturing the relevant dependencies and interactions within the repository, which is key to understanding the functional context. Additionally, the retrieved content explicitly contains information at both the method and line levels and implicitly expresses the grouping at the file level.\n\n\nThis process is abstracted via search_repograph() as illustrated in the middle of Figure 2. The retrieved  $k$ -hop ego-graph will be flattened for further processing. We also tried other variants for integration later in Section 5.2 and their performance in Table 4. We narrate how REPOGRAPH could be plugged in with existing representative research lines in the following.\n\n\nIntegration with procedural framework. In a procedural framework, LLMs are usually prompted in \"localization\" and \"editing\" stages with the given repository context and issue description. In this case, we use search_repograph() before both stages, leveraging our REPOGRAPH to assist in making more informed decisions at each step. For example, in Figure 2, we first include the subgraph of \"separability_matrix\" for localization, and then use the localized result \"Model\" to search in the\n\n\nedition stage. To implement the strategy, we flatten the context of retrieved ego-graphs and append it as part of the prompt. As a result, the LLM generation is conditioned on both retrieved ego-graphs and the context provided by baseline methods, helping the model preserve nuances.\n\n\nIntegration with agent framework. A significant difference in existing agent frameworks is the action space design, as expounded in Section 2. To leverage the power of REPOGRAPH, we put search_repograph() as an additional action in the action space. The agent decides when and where to use this action. The search term is also determined by the agent accordingly. The returned subgraph will be flattened and used as an observation for the next state.\n\n\n# 4 EXPERIMENTS\n\n\n# 4.1 SETUP\n\n\nWe evaluated REPOGRAPH as a plug-in component, i.e., integrated into existing baseline models of the two aforementioned research lines to assess its performance. We use the same baseline settings and configurations when incorporating REPOGRAPH to ensure a fair comparison.\n\n\nDataset. We test REPOGRAPH in SWE-bench-Lite<sup>3</sup>. Each problem in the dataset requires submitting a patch to solve the underlying issue described in the input issue description. The goal is to generate a patch that accurately revises the relevant portions of the codebase within the repository, ensuring that all test scripts included in the dataset are successfully executed.\n\n\nBaselines. We integrate REPOGRAPH with representative methods from both aforementioned research lines. (i) For procedural frameworks, we evaluate the widely used traditional method, RAG (Lewis et al., 2020), as well as Agentless (Xia et al., 2024), an open-source state-of-the-art approach in this direction. For RAG, we follow its initial setting and use BM25 for file-level retrieval. After that, we append the context of REPOGRAPH after the code files as part of the prompt. Agentless first performs a hierarchical localization in terms of \"file-class/function- edits\" and then conducts repair based on localization. The context of REPOGRAPH is inserted in every step of Agentless. (ii) For agent frameworks, we consider SWE-agent (Yang et al., 2024) and AutoCodeRover (Zhang et al., 2024). For both frameworks, we add an additional action \"search_repograph\" for the LLM agent as described in Section 3. All the choices in the two research lines incorporate GPT-4 and GPT-4o based methods to ensure the generalization. Detailed implementations and prompts used can be found in Appendix A.\n\n\nEvaluation metrics. We evaluate all methods across two key dimensions: Accuracy and Average Cost. (i) For accuracy, we report the resolve rate and patch application rate. The resolve rate represents the percentage of issues successfully resolved across all data points. An issue is considered resolved if the submitted patch passes all test scripts. To assess the patch application rate, we attempt to apply the generated patches to the repository using the patch program, counting only successful applications toward this metric. (ii) To evaluate cost efficiency, we report two metrics: average cost and average tokens, which refer to the inference cost and the number of input/output tokens used when querying the LLMs, respectively.\n\n\nConfigurations. We use the same GPT version as in the baselines in the experiments. We used GPT-4o (2024-05-13) and GPT-4-Turbo (gpt-4-1106-preview) from OpenAI for evaluation and analyses in our experiments. All evaluation processes are performed in a containerized Docker environment  $^{4}$ , ensuring stability and reproducibility, made possible through contributions from the open-source community. For plug-ins with procedural frameworks, it usually takes around 2-3 hours to finish. For agent frameworks, the inference time is larger, up to around 10 hours.\n\n\n# 4.2 EXPERIMENT RESULTS\n\n\nTable 2 presents the main evaluation results of all baseline methods and the corresponding performance with REPOGRAPH (+REPOGRAPH) as a plug-in in the SWE-bench-Lite test\n\n\nTable 2: Results of REPOGRAPH with open-source baselines in two research lines, including procedural and agent frameworks. Numbers of accuracy-related metrics are directly taken from the leaderboard, while the cost-related ones are computed from the corresponding trajectories<sup>6</sup>.\n\n\n<table><tr><td>Methods</td><td>LLM</td><td>resolve</td><td>Accuracy # samples</td><td>patch apply</td><td>Avg. Cost $ cost</td><td># tokens</td></tr><tr><td colspan=\"7\">Procedural frameworks</td></tr><tr><td>RAG</td><td>GPT-4</td><td>2.67</td><td>8</td><td>29.33</td><td>$0.13</td><td>11,736</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>5.33\u21912.66</td><td>16\u21918</td><td>47.67\u219118.34</td><td>$0.17</td><td>15,439</td></tr><tr><td>Agentless</td><td>GPT-4o</td><td>27.33</td><td>82</td><td>97.33</td><td>$0.34</td><td>42,376</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>29.67\u21912.34</td><td>89\u21917</td><td>98.00\u21910.67</td><td>$0.39</td><td>47,323</td></tr><tr><td colspan=\"7\">Agent frameworks</td></tr><tr><td>AutoCodeRover</td><td>GPT-4</td><td>19.00</td><td>57</td><td>83.00</td><td>$0.45</td><td>38,663</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>21.33\u21912.33</td><td>64\u21917</td><td>86.67\u21913.67</td><td>$0.58</td><td>45,112</td></tr><tr><td>SWE-agent</td><td>GPT-4o</td><td>18.33</td><td>55</td><td>87.00</td><td>$2.53</td><td>498,346</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>20.33\u21912.00</td><td>61\u21916</td><td>90.33\u21913.33</td><td>$2.69</td><td>518,792</td></tr></table>\n\n\nset. We also report the number of correct samples for each method. The performance increase is marked by  $\\uparrow$ num. We also tested other (open-source) LLMs using Claude-3.5-Sonnet (claude-3-5-sonnet-20240620) (Anthropic, 2024) from Anthropic in Appendix C. Based on the results, we have the following key observations:\n\n- (i) REPOGRAPH brings consistent performance gain for all combinations of frameworks and LLM model bases. Specifically, REPOGRAPH achieves an absolute improvement of  $+2.66$  and  $+2.34$  in terms of the resolve rate for RAG and Agentless, respectively, which is  $99.63\\%$  and  $8.56\\%$  of relative improvement. The notable improvement demonstrates the effectiveness of our REPOGRAPH in adapting to various scenarios by inducing relevant code context and performing precise code editorings. Additionally, our best performance so far by plugging in Agentless, 29.67, achieves the state-of-the-art performance on the benchmark among all open-source methods.\n\n- (ii) Performance gain brought by REPOGRAPH is slightly larger on procedural frameworks than agent ones. With procedural frameworks, REPOGRAPH correctly fixes more issues than agent ones. This could be due to two primary reasons. Firstly, we observed that mature procedural frameworks tend to achieve better baseline performance than agent-based frameworks on SWE-bench. The initial definitive nature of procedural frameworks, with their well-defined running flow and structure, allows them to leverage plug-ins more effectively. Another reason is that this deterministic behavior reduces the complexity that arises from dynamic decision-making, a key characteristic of agent-based systems, thereby enabling a smoother integration of performance improvements.\n\n- (iii) Performance gain brought by REPOGRAPH does not rely on more costs. We also compute and report each method's average cost and token consumption. By introducing REPOGRAPH, we manage to reduce the costs associated with managing the entire repository while achieving comparable or even superior performance. As shown in Table 2, the additional token cost introduced by RepoGraph is justified given the significant performance improvement it enables, particularly when compared to the substantially higher costs incurred by previous methods like SWE-agent. This indicates REPOGRAPH's performance gains are not mainly due to increased token usage.\n\n- (iv) Average costs are generally larger on agent frameworks with REPOGRAPH. This phenomenon is especially obvious with SWE-agent, as it allows the agent to freely determine the next action based on the current observation. We also found that the integration with agent frameworks usually leads to larger cost increases, as exemplified by  $+0.13$  and  $+0.18$  with AutoCodeRover and SWE-agent, respectively. The reason lies in the large exploration space in agent frameworks. The agents might call the search_repograph() action many times, which leads to the explosion of prompt contexts. We encourage users to be mindful of cost and to adopt a more granular approach to cost control when integrating REPOGRAPH into the agent framework in the future.\n\n\n# 5 ANALYSIS\n\n\nThis section presents a detailed analysis to demonstrate that the additional context provided by REPOGRAPH is beneficial for the task. We begin by analyzing localization accuracy in comparison to the gold-standard patch. Next, we explore various REPOGRAPH configurations, focusing on how the additional context can be effectively integrated into the existing system. Finally, we perform an in-depth error analysis, highlighting aspects where REPOGRAPH can be further improved. For more analyses including resolve rate in various aspects and action distributions of agent frameworks, please refer to Appendices C.\n\n\n# 5.1 LOCALIZATION COVERAGE\n\n\nA crucial step in issue resolution is accurately identifying the correct locations within the code that require modification. Proper localization is essential, as it forms the foundation for generating an effective and accurate patch. Without this step, the quality of the fix may be compromised, leading to incomplete or incorrect solutions. In three granularity, we compute the percentage of problems where the edit locations match the ground truth patch. Namely, file-level, function-level, and line-level. We report that a patch con\n\n\nTable 3: Percentage of problems for accurate edition localizations with respect to file, function, and line levels. All the numbers are computed from the corresponding generated patches.\n\n\n<table><tr><td>Methods</td><td>LLM</td><td>file</td><td>function</td><td>line</td></tr><tr><td>RAG</td><td>GPT-4</td><td>47.3</td><td>23.3</td><td>12.7</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>51.7\u21914.4</td><td>25.3\u21912.0</td><td>14.3\u21911.6</td></tr><tr><td>Agentless</td><td>GPT-4o</td><td>68.7</td><td>51.0</td><td>34.3</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>74.3\u21915.6</td><td>54.0\u21913.0</td><td>36.7\u21912.4</td></tr><tr><td colspan=\"5\">Agent frameworks</td></tr><tr><td>AutoCodeRover</td><td>GPT-4</td><td>62.3</td><td>42.3</td><td>29.0</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4</td><td>69.0\u21914.7</td><td>45.7\u21913.4</td><td>31.7\u21912.7</td></tr><tr><td>SWE-agent</td><td>GPT-4o</td><td>61.7</td><td>46.3</td><td>32.3</td></tr><tr><td>+REPOGRAPH</td><td>GPT-4o</td><td>67.3\u21915.6</td><td>49.3\u21913.0</td><td>35.0\u21912.7</td></tr></table>\n\n\ntains the correct location if it edits a superset of all locations in the ground truth patch.\n\n\nTable 3 presents the results of our analysis. We observed that integrating REPOGRAPH with all baseline methods significantly improves file-level accuracy, whereas the enhancement of accuracy in line-level is comparatively modest. This result aligns with our expectations, as file-level localization is the most coarse-grained, making it inherently easier to improve. In contrast, line-level localization, being the most fine-grained, poses a greater challenge due to its need for more precise identification of code segments. Additionally, we found that although line-level accuracy improvements are more pronounced for agent frameworks, their overall resolve rate is lower than that of procedural frameworks, as shown in Table 2. This discrepancy can be attributed to the fact that localization, while necessary for generating a final patch, is insufficient. The success of the final revision still heavily relies on the underlying capabilities of LLMs. Agent frameworks, designed to operate in a trial-and-error fashion, are particularly susceptible to error accumulation. As these frameworks iteratively refine their patches, small inaccuracies in earlier localization steps can propagate and magnify throughout the process, ultimately reducing the overall resolve rate. Procedural frameworks, on the other hand, follow a more structured and deterministic approach to localization and patch generation. They typically localize and fix issues in a single, more direct step, which can help mitigate the compounding of errors.\n\n\n# 5.2 INVESTIGATION OF REPOGRAPH VARIANTS\n\n\nIn this section, we investigate the efficacy of various combinations of sub-graph retrieval and integration techniques as outlined in Section 3. We explore two sub-graph retrieval variants and two integration methods. Specifically, for sub-graph retrieval, we index  $k$ -hop ego-graphs where  $k$  is set to 1 and 2. We limit our exploration to  $k$  values up to 2 due to the extensive context required for integration and the potential introduction of noise or irrelevant nodes for  $k \\geq 3$ . For the integration methods, we employ two distinct approaches: (i) directly flattening the textual sub-graph by explicitly detailing the relationships between the search term and its neighboring nodes, and (ii) leveraging an LLM first to summarize the sub-graph in terms of the core modules and salient dependencies, before proceeding with further processing. Detailed implementations of these variants and prompts used can be found in Appendix B.1.\n\n\nTable 4: The number of nodes, edges, and tokens of REPOGRAPH and its variants. For different retrieval and integration variants, we report the average number on the test set. \"summ.\" refers to the summarized version by LLMs of the retrieved ego-graph.\n\n\n<table><tr><td>Metrics</td><td>REPOGRAPH</td><td>1-hop + flatten</td><td>1-hop + summ.</td><td>2-hop + flatten</td><td>2-hop + summ.</td></tr><tr><td># Nodes</td><td>1419.3</td><td>11.6</td><td>11.6</td><td>54.5</td><td>54.5</td></tr><tr><td># Edges</td><td>26392.1</td><td>37.1</td><td>37.1</td><td>89.9</td><td>89.9</td></tr><tr><td># tokens</td><td>-</td><td>2310.7</td><td>717.5</td><td>10505.3</td><td>1229.2</td></tr><tr><td>resolve rate</td><td>-</td><td>29.67</td><td>28.33</td><td>26.00</td><td>28.67</td></tr></table>\n\n\nWe begin by presenting some statistics for REPOGRAPH and its various configurations. Performance evaluations are conducted on Agentless with REPOGRAPH integrated as a plug-in module. Table 4 reports the number of nodes and edges within the (sub)-graphs. Notably, the average number of nodes and edges for REPOGRAPH across the SWE-bench dataset is quite substantial, featuring over 1,000 nodes and 25,000 edges. This highlights the comprehensive nature of the constructed structure. For the different variants, when  $k = 1$ , the information within REPOGRAPH is concentrated around the search term, resulting in an average of 11.6 nodes and 37.1 edges. As  $k$  increases to 2, the retrieved ego-graph expands exponentially, reaching an average of 54.5 nodes and 89.9 edges. Moreover, directly flattening the retrieved ego-graph often significantly increases the token count, frequently reaching several thousand tokens. However, utilizing an LLM as an additional summarizer greatly reduces the token count, typically around a thousand.\n\n\nTable 4 presents the resolve rates for four variants of our method. Notably, while the 2-hop variant incorporates additional information, directly flattening this information results in the poorest performance, with a resolve rate of  $26.00\\%$ , even lower than the original baseline. In contrast, incorporating summarization via the LLM significantly alleviates context length constraints and enhances information organization, thereby improving performance. For the 1-hop variant, however, we observed that adding summarization actually degrades performance, reducing the resolve rate to  $28.33\\%$ . We hypothesize that this occurs because the flat 1-hop ego-graph already contains comprehensive information that fits within the LLM's context window; thus, summarization may introduce inevitable information loss.\n\n\n# 5.3 TRANSFERIBILITY TEST\n\n\nTo demonstrate the representational power of REPOGRAPH for repositories and its transferability to tasks requiring an understanding of repository structures, we conducted experiments using the CrossCodeEval benchmark (Ding et al., 2023). CrossCodeEval is a multilingual code completion benchmark that is derived from real-world repositories. It is designed to emphasize numerous cross-file dependencies. We focus on problems using Python as the evaluation programming lan\n\n\nTable 5: Results on the subset of CrossCodeEval with GPT-4o and Deepseek-Coder-V2-LiteInstruct as the backbone LLMs.\n\n\n<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">Code Match</td><td colspan=\"2\">Identifier Match</td></tr><tr><td>EM</td><td>ES</td><td>EM</td><td>F1</td></tr><tr><td>Deepseek-Coder</td><td>10.2</td><td>57.3</td><td>16.6</td><td>49.1</td></tr><tr><td>+REPOGRAPH</td><td>19.7</td><td>67.8</td><td>29.3</td><td>58.9</td></tr><tr><td>GPT-4o</td><td>10.5</td><td>59.6</td><td>16.8</td><td>47.9</td></tr><tr><td>+REPOGRAPH</td><td>28.7</td><td>68.9</td><td>36.0</td><td>61.3</td></tr></table>\n\n\nguage, leading to 2,665 samples. For evaluation metrics, we follow the settings in the original paper and measure performance with code match and identifier match metrics, assessing accuracy with exact match (EM), edit similarity (ES), and F1 scores. In our experiments, the search terms are determined to be the function within which the current line is to be completed.\n\n\nTable 5 demonstrates the results on CrossCodeEval using GPT-4o and Deepseek-Coder as the backbone language model. The original LLMs struggles with repository-level tasks, evidenced by an EM score of only 10.8 for code matching and 16.7 for identifier matching. These results indicate significant limitations in handling code structure and variable usage in a broader repository context. However, with the integration of the REPOGRAPH method, there is a substantial improvement across all metrics with both LLMs. Particularly, we find that the improvement brought by REPOGRAPH is more pronounced when integrated with GPT-4o. A potential reason is that GPT-4o is better at inter\n\n\n![Figure 3: Venn diagram of REPOGRAPH and baselines on (a) procedural framework and (b) agent framework on SWE-Bench-Lite. We also plot the error distribution of failing cases against counterparts, e.g., detailed error distribution of 12 cases REPOGRAPH succeeds while Agentless fails.](images/8593889f7523e5c7cb665d49567ce8b44e18922f502c7e8a49cd0937fbf90ccd.jpg)\n\n\npreting the additional context information provided for better code reasoning. These improvements suggest that incorporating repository-level knowledge, as facilitated by REPOGRAPH, greatly enhances the model's ability to understand and generate more contextually accurate and semantically consistent code.\n\n\n# 5.4 ERROR ANALYSIS\n\n\nWe want to compare REPOGRAPH with the corresponding baselines to see the distribution of resolved cases and analyze the error reasons for unresolved cases. We plot a Venn diagram for representative methods in both procedural and agent frameworks in Figure 3, respectively. We manually examined all the unique error cases and defined three error categories. (i) Incorrect localization refers to the failure in accurately identifying code snippets, (ii) contextual misalignment happens when the generated patch fails to align with the broader context of the codebase, and (iii) regressive fix introduces new issues in resolving the original issues. More examples are in Appendix D.\n\n\nWe found that the improvement in agent frameworks is more complementary than procedural frameworks, with larger uniquely resolved cases of 22 compared with 12 in procedural frameworks. Together, they make even larger performance of  $31.33\\%$  and  $22.33\\%$ . The reason could also be attributed to the determinism of procedural frameworks. As a plug-in module, REPOGRAPH tends to make modifications on existing deterministic processes, resulting in larger overlaps in resolved issues compared with baselines. Agent frameworks, on the other hand, have quite different action distributions with REPOGRAPH as a plug-in (please refer to Appendix A.2). Therefore, the uniquely resolved cases are more compared with procedural frameworks. For error distributions, contextual misalignment is the most prevalent error type, followed by incorrect localization and regressive fixes for all methods, suggesting that while localization is often correct, the applied solutions may regress or fail to integrate contextually. The phenomenon also echoes with conclusion obtained in Section 5.1. This is intuitive as all the existing methods focus on providing a more comprehensive and desired context for LLMs to solve the task, which fundamentally depends on the power of backbone LLMs. We also found that when integrated with REPOGRAPH, the proportion of all three error types decreases (specifically for \"incorrect localization\"), indicating that REPOGRAPH is specifically useful in aggregating the related contexts of the current to-be-fixed issue.\n\n\n# 6 CONCLUSION AND DISCUSSION\n\n\nThis paper introduces REPOGRAPH, a plug-in module for modern AI software engineering. REPOGRAPH operates at the code-line level and offers desired navigation through code bases by incorporating and aggregating information at the line level, file level, and repository level through subgraph retrieval of ego-graphs. Extensive experiments on real-world SWE tasks show that REPOGRAPH significantly boosts the performance of both procedural and agent frameworks. In addition, REPOGRAPH is proved to be smoothly and effectively transferred to other tasks that require repository-level understanding. Future work could further investigate the generalizability of REPOGRAPH across different programming languages, frameworks, and developer environments. Real-time execution and feedback could also be explored to further enhance REPOGRAPH towards a dynamic version for better AI software engineering abilities.\n\n\n# REFERENCES\n\n- Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pittler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan First, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv.2312.11805.\n\n- Anthropic. Introducing claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. Accessed: 2024-10-01.\n\n- Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n\n- Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675-698, 2024.\n\n- Andrew Begel, Yit Phang Khoo, and Thomas Zimmermann. Codebook: discovering and exploiting relationships in software repositories. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1, ICSE '10, pp. 125-134, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781605587196. doi: 10.1145/1806799.1806821. URL https://doi.org/10.1145/1806799.1806821.\n\n- Teresa Busjahn, Roman Bednarik, Andrew Begel, Martha E. Crosby, James H. Paterson, Carsten Schulte, Bonita Sharif, and Sascha Tamm. Eye movements in code reading: relaxing the linear order. In Andrea De Lucia, Christian Bird, and Rocco Oliveto (eds.), Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension, ICPC 2015, Florence/Firenze, Italy, May 16-24, 2015, pp. 255-265. IEEE Computer Society, 2015. doi: 10.1109/ICPC.2015.36. URL https://doi.org/10.1109/ICPC.2015.36.\n\n- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fiotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.\n\n- Wei Cheng, Yuhan Wu, and Wei Hu. Dataflow-guided retrieval augmentation for repository-level code completion. In ACL, 2024.\n\n- Cognition. Devin, 2024. URL https://www.cognition.ai/blog/introducing-devin.\n\n- Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang. Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=wgDcbBMSfh.\n\n- Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aur\u00e9lien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi\u00e8re, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gr\u00e9goire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Ko-revaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783.\n\n- Angela Fan, Belize Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M Zhang. Large language models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FOSE), pp. 31-53. IEEE, 2023.\n\n- Paul Gauthier. Aider is ai pair programming in your terminal. https://aider chat/, 2024.\n\n- Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming - the rise of code intelligence, 2024. URL https://arxiv.org/abs/2401.14196.\n\n- Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: A systematic literature review. arXiv preprint arXiv:2308.10620, 2023.\n\n- Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao. GRAG: graph retrieval-augmented generation. CoRR, abs/2405.16506, 2024. doi: 10.48550/ARXIV.2405.16506. URL https://doi.org/10.48550/arXiv.2405.16506.\n\n- Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66.\n\n- Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, and Jiawei Han. Graph chain-of-thought: Augmenting large language models by reasoning on graphs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 163-184, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024-findings-acl.11. URL https://aclanthology.org/2024.findings-acl.11.\n\n- Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 9459-9474, 2020.\n\n- Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee,\n\n- Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason T. Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! Trans. Mach. Learn. Res., 2023, 2023a. URL https://openreview.net/forum?id=KoFOg41haE.\n\n- Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023b.\n\n- Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng Di, Hongwei Chen, Chengpeng Wang, Gang Fan, et al. Repofuse: Repository-level code completion with fused dual context. arXiv preprint arXiv:2402.14323, 2024.\n\n- Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Wenmeng Zhou, Fei Wang, and Michael Shieh. Codexgraph: Bridging large language models and code repositories via code graph databases. arXiv preprint arXiv:2408.03910, 2024.\n\n- Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code understanding and generation. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/ hash/c16a5320fa475530d9583c34fd356ef5-Abstraction-round1.html.\n\n- Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. How to understand whole software repository? arXiv preprint arXiv:2406.01422, 2024.\n\n- OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.\n\n- Siru Ouyang, Shuohang Wang, Yang Liu, Ming Zhong, Yizhu Jiao, Dan Iter, Reid Pryzant, Chenguang Zhu, Heng Ji, and Jiawei Han. The shifted and the overlooked: A task-oriented investigation of user-GPT interactions. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 2375-2393, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.146. URL https://aclanthology.org/2023.emnlp-main.146.\n\n- L Page. The page-rank citation ranking: Bringing order to the web, 1999.\n\n- Huy N Phan, Hoang N Phan, Tien N Nguyen, and Nghi DQ Bui. Repohyper: Better context retrieval is all you need for repository-level code completion. arXiv preprint arXiv:2403.06095, 2024.\n\n- Baptiste Rozi\u00e8re, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised translation of programming languages. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/black/ed23fbf18c2cd35f8c7f8de44f85c08d-AAbstract.html.\n\n- Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and\n\n- Gabriel Synnaeve. Code llama: Open foundation models for code. CoRR, abs/2308.12950, 2023. doi: 10.48550/ARXIV.2308.12950. URL https://doi.org/10.48550/arXiv.2308.12950.\n\n- Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. Repository-level prompt generation for large language models of code. In International Conference on Machine Learning, pp. 31693-31715. PMLR, 2023.\n\n- Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. An empirical study on learning bug-fixing patches in the wild via neural machine translation. ACM Transactions on Software Engineering and Methodology (TOSEM), 28(4):1-29, 2019.\n\n- Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024.\n\n- Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, and Xiaofei Ma. Repoformer: Selective retrieval for repository-level code completion. arXiv preprint arXiv:2403.10059, 2024.\n\n- Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024.\n\n- John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024.\n\n- Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. Coacor: Code annotation for code retrieval with reinforcement learning. In The world wide web conference, pp. 2203-2214, 2019.\n\n- Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repeditor: Repository-level code completion through iterative retrieval and generation. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=q09vTY1Cqh.\n\n- Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement, 2024.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636557.2651527", "msecs": "265.0", "relativeCreated": "17958.293199539185", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# ZipIt! Merging Models from Different Tasks without Training\n\n# Abstract\nTypical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then averages them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce \"ZipIt!\", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to allow for merging features within each model by defining a general \"zip\" operation. Second, we add support for partially zipping the models up until a specified layer, naturally creating a multi-head model. We find that these two changes combined account for 20-60% improvement over prior work, making it more feasible to merge models trained on disjoint tasks without retraining.\n\n\n# 1 INTRODUCTION\n\n\nEver since AlexNet (Krizhevsky et al., 2017) popularized deep learning in computer vision, the field has thrived under the reign of massive models with an ever increasing number of parameters. Many vision problems once considered difficult or impossible are now benchmark tasks: classification with tens of thousands of classes (Deng et al., 2009; Zhou et al., 2017; Gemmeke et al., 2017), fast instance segmentation (He et al., 2017; Bolya et al., 2019), realistic image generation (Karras et al., 2018; Ho et al., 2020; Rombach et al., 2022), and more.\n\n\nThere are an abundance of independent, carefully tuned models out there for many tasks. However, if we want to expand an existing model's capabilities, we run into many potential issues. If we try training the model on an additional task, we face catastrophic forgetting (Kirkpatrick et al., 2017; Li & Hoiem, 2017; De Lange et al., 2021). If we evaluate the same model on different data without adaptation, we often find it doesn't generalize to out of domain samples (Blanchard et al., 2011; Muandet et al., 2013; Wang et al., 2022). We can try so called \"intervention\" strategies (Wang et al., 2022; De Lange et al., 2021) to mitigate these effects, but these often require further training which can be expensive (Dosovitskiy et al., 2020; Zhai et al., 2022; Dehghani et al., 2023). Instead, it would be nice if we could expand a model's capacity to solve new tasks by simply \"zipping\" it with other models trained on those tasks without additional training.\n\n\nCombining multiple models into one has recently started to gain traction in the vision community. Model Soups (Wortsman et al., 2022a) can add multiple models finetuned from the same pretrained initialization to improve accuracy and robustness. Git Re-Basin (Ainsworth et al., 2022) generalizes further to models trained on the same data but with different initializations, though with a significant accuracy drop. REPAIR (Jordan et al., 2022) improves on Git Re-Basin by adding new parameters and adjusting model batch norms where applicable. However, all of these methods only combine models trained on the same task. In this paper, we take this line of work to a logical extreme: merging differently initialized models trained on completely separate tasks (see Fig. 1ab). We show that this is an incredibly difficult problem for prior work and employ two simple strategies to make it feasible.\n\n\n![Figure 1: Setting and ZipIt! (a) Prior work merges differently initialized models from the same dataset with the same label sets: e.g., merging two models both trained to classify dog breeds. (b) Our setting expands this to merging models from different datasets with different label sets: e.g., merging a model that classifies dog breeds with one that classifies bird species. (c) ZipIt! merges these models without retraining by identifying shared features.](images/6e7a6073a90b331ae60be0db208311215cf0b4c1495e1345205151b5a84237ac.jpg)\n\n\n![](images/f47160305d6d399085e0d2019bdb58d4549ce0632ac6011aca456837e3d9780f.jpg)\n\n\n![](images/84d3f8e963bef0f726c3823d5cf2aa41f2c25da9c5dadea3cb66554433037972.jpg)\n\n\nFirst, we note that prior work focuses on permuting one model to the other when merging them. This creates a 1-1 mapping between the two models, inherently assuming that most features across them are correlated. Since this isn't necessarily the case for models trained on different tasks, we cannot rely on permutation alone. Instead, we generalize model merging to support \"zipping\" any combination of correlated features within and across each model. We find that on some tasks, this alone improves accuracy by up to  $20\\%$  vs. permutation-based approaches. Moreover, we prove that merging within models can yield a better result in the theoretical setting of Entezari et al. (2021).\n\n\nSecond, existing methods merge the entire network. While this might work for extremely similar models trained in the same setting, the features of models trained on disjoint tasks become less correlated over the course of the network (Kornblith et al., 2019). To solve this, we introduce partial zipping, where we only \"zip\" up to a specified layer. Afterward, we feed the merged model's outputs to the remaining unmerged layers of the original networks, creating a multi-head model. Depending on task difficulty, this can improve accuracy by over  $15\\%$  while still keeping most layers merged.\n\n\nIncorporating both of these strategies, we introduce ZipIt! (Fig. 1c), a general method for \"zipping\" any number of models trained on different tasks into a single multitask model without retraining. By deriving a general graph-based algorithm for merging and unmerging (Sec. 4), we can zip models of the same architecture together, merge features within each model, and partially zip them to create a multi-task model. We validate our approach by merging models trained on entirely disjoint sets of CIFAR (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009) categories, as well as merging several models trained on completely independent datasets into one, significantly outperforming prior work (Sec. 5). Finally, we ablate and analyze our method's capabilities on these scenarios (Sec. 6).\n\n\n# 2 RELATED WORK\n\n\nModel merging combines the weights of two or more models into a one. Our work differs from prior work in that we adapt mode connectivity techniques to target models trained on disjoint tasks (Fig. 1).\n\n\nMerging Finetuned Models. If two models are finetuned from the same pretrained checkpoint, they often lie in the same error basin (Neyshabur et al., 2020). Several works (Huang et al., 2017; Izmailov et al., 2018; Von Oswald et al., 2020; Wortzman et al., 2022b; Ilharco et al., 2022b; Don-Yehiya et al., 2023) have exploited this property to average together the weights of a model at different stages of training. Tarvainen & Valpola (2017); Cai et al. (2021); Grill et al. (2020); Caron et al. (2021); Baevski et al. (2022) use an \"exponential moving average\" of training checkpoints as a teacher for self-supervised learning. Other works merge models initialized from the same pretrained base, but that were finetuned independently, either by simply averaging their weights (McMahan et al., 2017; Wortzman et al., 2022a; Choshen et al., 2022; Ram\u00e9 et al., 2022), permuting one model to the other (Ashmore & Gashler, 2015; Yurochkin et al., 2019; Wang et al., 2020), combining meaningful weight regions (Ilharco et al., 2022a; Gueta et al., 2023; Yadav et al., 2023; Sung et al., 2023), or maximizing an objective (Matena & Raffel, 2021). Our setting differs, as we do not assume the same initialization.\n\n\nMerging Differently Initialized Models. Merging models with different initializations is a much more challenging problem. Works in this space often rely on mode connectivity (Freeman & Bruna, 2016; Garipov et al., 2018; Draxler et al., 2018; Frankle et al., 2020), attempting to interpolate between models along a low loss path (e.g., Tatro et al. (2020); Singh & Jaggi (2020); Liu et al. (2022)). Most recent work follows the intuition, later formalized by Entezari et al. (2021), that\n\n\n![Figure 2: Task Loss Landscapes for models in Tab. 1b. Model A and Model B lie in low loss basins for their own tasks, but not for the other task. Thus, any interpolation between Model A and a permuted Model B (e.g., Git Re-basin) lies outside the minima for both tasks and thus performs poorly. In contrast, ZipIt! improves the merge by finding a model that lies in a low loss basin for both.](images/6b1b9d301e6c25aa303a00d56e5748de1b45d6f70552d2505002eb9e741e4121.jpg)\n\n\n![](images/491cce3b1f925a5cef5b4f1c0ae5be6ed78c26db5a2c330f9a56fbd09ab26557.jpg)\n\n\n![](images/04a96a7630097237363b4d28764689540ca9ba488e3d72fffd669b6459f40124.jpg)\n\n\nmodels permuted to the same loss basin can be merged by averaging their weights. Most notably, Git Re-Basin (Ainsworth et al., 2022) permutes models by comparing the similarity between their weights. REPAIR (Jordan et al., 2022) improves the accuracy of Git Re-Basin by instead computing the correlation between their intermediate layer feature activations, and adding several batch norms to the network. Pe\u00f1a et al. (2022) find permutations using global rather than local optimization, though they don't support skip connections. Some of these works (e.g., Singh & Jaggi (2020); Ainsworth et al. (2022)) evaluate on on a setting where each model sees varying numbers of instances per class. And Pe\u00f1a et al. (2022) evaluates on a continual learning setting with disjoint categories, but their method requires training optimization. Similarly, He et al. (2018) merges models of different tasks, but requires jointly finetuning after each layer merge. As far as we are aware, we present the first general method to successfully merge models trained on disjoint tasks without additional training.\n\n\n# 3 BACKGROUND AND MOTIVATION\n\n\nModel merging stems from mode connectivity (Garipov et al., 2018), where it is conjectured that models trained with SGD on the same dataset lying in the same loss basin (i.e., region or mode of low loss) can be combined into a single model that's just as performant as the original models. If these models can be combined well by linearly interpolating their weights, they are said to be linearly mode connected (LMC) (Garipov et al., 2018; Entezari et al., 2021). Our work is similar to finding LMC, but across different datasets with disjoint label sets (i.e., separate tasks as in De Lange et al. (2021)).\n\n\nConsider a model  $\\mathcal{L}$  as a collection of layers  $L_{i} \\in \\mathcal{L}$ , each of which may have some parameters (e.g.,  $W_{i}, b_{i}$  for a linear layer). If  $\\mathcal{L}^A$  and  $\\mathcal{L}^B$  are finetuned from the same checkpoint, several works (e.g., Izmailov et al. (2018); Wortsman et al. (2022a)) find merging them is as easy as linearly interpolating their weights (i.e., they are LMC). E.g., if  $L_{i}$  is a linear layer, the new weight matrix  $W_{i}^{*}$  is simply\n\n\n$$\n\n$$\nW _ {i} ^ {*} = \\gamma W _ {i} ^ {A} + (1 - \\gamma) W _ {i} ^ {B} \\tag {1}\n$$\n\n$$\n\n\nwith an interpolation constant  $\\gamma \\in [0,1]$ , usually set to  $^{1 / 2}$ . However, if  $\\mathcal{L}^A$  and  $\\mathcal{L}^B$  were not finetuned from the same checkpoint, they often do not lie in the same mode (Entezari et al., 2021; Ainsworth et al., 2022) and cannot be interpolated. Indeed, Eq. 1 typically results in random accuracy.\n\n\nTo fix this, Entezari et al. (2021) conjecture that large enough models are likely LMC modulo permutation. This is because (1) many neural networks can be permuted internally without affecting their outputs and (2) permutation can move two models into the same basin, allowing much of the lost accuracy to be recovered. More concretely, let  $P_{i}$  be a permutation matrix that permutes outputs of layer  $L_{i}^{B}$  to the space of  $L_{i}^{A}$ . Then for each layer, permutation works apply\n\n\n$$\n\n$$\nW _ {i} ^ {*} = \\gamma W _ {i} ^ {A} + (1 - \\gamma) P _ {i} W _ {i} ^ {B} P _ {i - 1} ^ {T} \\tag {2}\n$$\n\n$$\n\n\nNote that here we permute the output space of  $W_{i}^{B}$ , but we also need to permute its input space to undo the permutation from the previous layer (hence the use of  $P_{i - 1}^{T}$ ).\n\n\nProblems with Permutation. Eq. 2 relies on the likelihood of model B lying in the same basin as model A after permutation being high. However, this is far less likely when the models are trained\n\n\non different tasks, as each model optimizes for basins containing distinct task-specific information. In this case the optimal permutation of model B to model A still lies in a strong basin on task B but doesn't lie in a basin on task A, as shown in Figure 2. This causes the interpolated model to perform worse than either the two original models. Thus, we explore alternative merging methods.\n\n\n# 4 ZIPIT!\n\n\nIn this work, we treat model merging as combining the checkpoints (i.e., collection of weights) of multiple models into a single checkpoint that can perform all the tasks of its constituents. We do this by merging the layers of the models together. For instance, suppose  $L_{i} \\in \\mathcal{L}$  is a linear layer with parameters  $W_{i} \\in \\mathbb{R}^{n_{i} \\times m_{i}}$ ,  $b_{i} \\in \\mathbb{R}^{n_{i}}$  with input features  $x \\in \\mathbb{R}^{m_{i}}$  and outputs features  $f_{i} \\in \\mathbb{R}^{n_{i}}$ :\n\n\n$$\n\n$$\nf _ {i} = L _ {i} (x) = W _ {i} x + b _ {i} \\tag {3}\n$$\n\n$$\n\n\nOur goal is to take  $L_{i}^{A} \\in \\mathcal{L}^{A}$  from model A and  $L_{i}^{B} \\in \\mathcal{L}^{B}$  from model B and merge them into a layer  $L_{i}^{*}$  that combines their feature spaces such that information from both  $f_{i}^{A}$  and  $f_{i}^{B}$  is retained in  $f_{i}^{*}$ . We accomplish this by merging each layer of one model with the corresponding layer in the other, both merging features in one across both layers or within the same layer. This is in contrast to permutation-based merging method, which only combine features across layers.\n\n\nWhy should we merge within? Features of models trained on different tasks may be dissimilar, as the models solve different problems. Forcibly combining these dissimilar features can yield merges that don't perform well on either original task (Fig 2). Instead, those features may be more compatible with others within the same model, which would better retain performance when combined.\n\n\nIn fact, we can prove that methods which allow merges within each model (as well as across both) perform equal to or better than those which only merge across models (e.g., permutation-reliant approaches) in a limited but prevalent setting. Specifically, we obtain a tighter bound over Theorem 3.1 from Entezari et al. (2021) when redundancy exists within a model and is leveraged. Both Theorem 3.1 and our Theorem 1 (see Appendix G for formalization and proof) bound the degree to which the loss of a merged two-layer model with  $d$ -input dimensions and  $h$ -intermediate dimensions increases compared to the losses of the original models. Theorem 3.1 bounds this increase to  $\\tilde{O}(h^{-1/(2d+4)})$ . However, if features within a model are redundant, then we reduce the bound to\n\n\n$$\n\n$$\n\\text {L o s s I n c r e a s e o f M e r g e d M o d e l} \\leq \\left\\{ \\begin{array}{l l} \\tilde {O} \\left(\\left(\\frac {h}{1 - 2 \\Gamma}\\right) ^ {- \\frac {1}{2 d + 4}}\\right) & \\Gamma <   0. 5 \\\\ 0 & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {4}\n$$\n\n$$\n\n\nwith  $\\Gamma \\in [0,1]$  measuring what portion of features are redundant. This bound is  $2^{d + 4}\\sqrt{1 - 2\\Gamma} \\leq 1$  times that of Theorem 3.1 when  $\\Gamma < 0.5$  (equal only when  $\\Gamma = 0$ ) and explicitly zero when  $\\Gamma \\geq 0.5$ .\n\n\nHow do we merge features? In prior work, each of the merged features  $f_{i}^{*}$  is the result of combining one feature from  $f_{i}^{A}$  and one from  $f_{i}^{B}$ . However in our case, we can also merge features by combining two from just  $f_{i}^{A}$  or two from just  $f_{i}^{B}$ . To account for this, we concatenate  $f_{i}^{A}$  and  $f_{i}^{B}$  into a single feature vector:  $f_{i}^{A}\\|f_{i}^{B}\\in\\mathbb{R}^{2n_{i}}$ . Then, like prior work (e.g. Li et al. (2015); Jordan et al. (2022)), we define feature similarity as the pairwise correlation between between neuron activations over a small set of images (without labels). However, unlike those works, we compute correlations between every activation in the full concatenated space  $f_{i}^{A}\\|f_{i}^{B}$ . Our approach thus measures the similarity of every feature  $f_{i}^{A}$  and  $f_{i}^{B}$  to all features in both models, rather than solely between  $f_{i}^{A}$  and  $f_{i}^{B}$ .\n\n\nNext, if two features are well correlated, we can average them without losing much information. Thus, we can construct  $f_{i}^{*}$  by finding  $n_i$  pairs of similar features in  $f_{i}^{A} \\| f_{i}^{B}$  and averaging them together. By default, we do this greedily: i.e., iteratively match the features with the highest correlation without replacement; though we explore extensions to this in Sec. 4.3 and test other methods in Tab. 4. Then we can use these matches to construct  $f_{i}^{*}$ . Formally, we define a \"merge matrix\"  $M_{i} \\in \\mathbb{R}^{n_{i} \\times 2n_{i}}$  s.t.\n\n\n$$\n\n$$\nf _ {i} ^ {*} = M _ {i} \\left(f _ {i} ^ {A} \\| f _ {i} ^ {B}\\right) \\tag {5}\n$$\n\n$$\n\n\n$M_{i}$  averages the matched features, with each match corresponding to one output feature in  $f_{i}^{*}$ . For instance, if  $u$ th match is between indices  $s, t \\in \\{1, \\dots, 2n_{i}\\}$  of  $f_{i}^{A} \\| f_{i}^{B}$ , then the  $u$ th row of  $M_{i}$  would be  $1/2$  at columns  $s$  and  $t$  and  $0$  elsewhere. This results in  $f_{i}^{*}[u] = \\frac{1}{2}(f_{i}^{A} \\| f_{i}^{B})[s] + \\frac{1}{2}(f_{i}^{A} \\| f_{i}^{B})[t]$ .\n\n\n![The Zip Operation (a) Two Disjoint Layers](images/db0e3414e86afd5bf89943298f95523230c31eacdb4e14557c1fd8a039d96b9a.jpg)\n\n\n![](images/29a35c847f61bf6c02e33e5aa9edac28d3a7cfdc22abce2d8ff9763ae61ab8f9.jpg)\n\n\n![(c) Merge and Unmerge Figure 3: ZipIt! merges models layer-wise by exploiting redundancy in their features. (a) Output features  $f^A$  and  $f^B$  from two disjoint layers are (b) paired with other features based on the similarity of their activations. (c) We produce a merge matrix  $M$  to combine the pairs into a single shared output feature space, and a corresponding unmerge matrix  $U$  that undoes this operation. (d) We then propagate  $U$  up the network to align the next layer's input space, and simultaneously receive the previous layer's  $U$  to align our input space. (e) We apply Eq. 8 to \"zip\" the layers together using the  $M$  for the output and  $U$  for the input, producing a single layer (f). We then repeat (a) on the next layer.](images/07b291970f89cf53e1105c4e713ed9a3e3f7c747d646d30a14c1debf6372ce72.jpg)\n\n\n![(d) Propagate M and U](images/18b1097526d090664980cff0c57f742f5eb573d5a04ec61f6cd42b40b9cf6bf1.jpg)\n\n\n![(e) Apply M and U to Weights](images/d1f94863a899cda90bc182ec1f7be82e9a6add699beecb317dc0a5fc7c0a49c6.jpg)\n\n\n![(f) Result: One Zipped Layer](images/b003cb8c8136ad8dd0742c08fb5f322abb51185059da82b397c962c3ef515271.jpg)\n\n\nThus, applying  $M_{i}$  has the effect of interpolating with  $\\gamma = 1 / 2$  but is more general (e.g., allows for merging more than 2 models at once, see Sec. 4.3).\n\n\nWhat about the next layer? After merging features in one layer, we now have the problem that the next layers,  $L_{i+1}^{A}, L_{i+1}^{B}$ , are incompatible with  $f_{i}^{*}$ . Instead, we need to undo the merge operation before passing the features to the next layer. Thus, we define an \"unmerge\" matrix  $U_{i} \\in \\mathbb{R}^{2n_{i} \\times n_{i}}$  s.t.\n\n\n$$\n\n$$\nU _ {i} f _ {i} ^ {*} \\approx f _ {i} ^ {A} \\| f _ {i} ^ {B} \\tag {6}\n$$\n\n$$\n\n\n$U_{i}$  is the pseudoinverse of  $M_{i}$  and in the case of the matching from earlier is simply  $2M_{i}^{T}$ . Note that strict equality is unlikely here. Like in prior work, merging models is a lossy operation.\n\n\nWe split this unmerge matrix in half along its rows into  $U_{i}^{A}, U_{i}^{B} \\in \\mathbb{R}^{n_{i} \\times n_{i}}$  that act individually to produce  $f_{i}^{A}$  and  $f_{i}^{B}$ . With this, we can evaluate the next layers using the merged features:\n\n\n$$\n\n$$\nf _ {i + 1} ^ {A} \\approx L _ {i + 1} ^ {A} \\left(U _ {i} ^ {A} f _ {i} ^ {*}\\right) \\quad f _ {i + 1} ^ {B} \\approx L _ {i + 1} ^ {B} \\left(U _ {i} ^ {B} f _ {i} ^ {*}\\right) \\tag {7}\n$$\n\n$$\n\n\n# 4.1 THE \"ZIP\" OPERATION\n\n\nWe now have all the necessary pieces, and can derive a general operation to merge  $L_{i}^{A}$  and  $L_{i}^{B}$  at an arbitrary point in the network (Fig. 3). First, we compute  $M_{i}$  and  $U_{i}$  by matching features between  $f_{i}^{A}$  and  $f_{i}^{B}$ . We then pass  $U_{i}$  to the next layer and receive  $U_{i-1}$  from the previous layer. Using  $M_{i}$  and  $U_{i-1}$ , we \"fuse\" the merge and unmerge operations into the layer's parameters. For a linear layer:\n\n\n$$\n\n$$\nW _ {i} ^ {*} = M _ {i} ^ {A} W _ {i} ^ {A} U _ {i - 1} ^ {A} + M _ {i} ^ {B} W _ {i} ^ {B} U _ {i - 1} ^ {B} \\tag {8}\n$$\n\n$$\n\n\nwhere  $M_i^A$  and  $M_i^B$  are  $M_i$  split along its columns.  $b_i^*$  has the same equation but without unmerging.\n\n\nNote the similarity between Eq. 8 and Eq. 2. This isn't a coincidence: if we only allowed merging across models and not within models, our \"zip\" operation would be identical to Git Re-Basin's permute-then-interpolate approach. Thus, Eq. 8 can be thought of as a generalization of prior work.\n\n\n# 4.2 ZIP PROPAGATION\n\n\nHowever, most modern neural networks are not simply collections of linear layers stacked on top of each other. In practice, we cannot combine merge and unmerge matrices into every layer of the network, as a local zip (Eq. 8) expects the layer to have a weight matrix\u2014i.e., the layer has to have separate input and output spaces so that we can unmerge the input space and merge the output space. Other layers (e.g., BatchNorm, ReLU) don't have such a weight matrix.\n\n\n![Figure 4: Zip Propagation. We propagate  $M_{i}$  backward until we hit a layer with weights, merging merging element-wise layers (e.g., BatchNorm) along the way.](images/923810c928ff8b8b0525cdf727993a0ba94df427adbddba39c7ef71e7df6d55e.jpg)\n\n\n![](images/14a289c2d198501c220c735b7cf0cc6cbf8da424a0032968902e674897243bb6.jpg)\n\n\n![](images/53500d042eb1f88fdf720a81f1cce7c12b125e19b16ff7be8da55090199dbf3b.jpg)\n\n\n![](images/1043733cc3faed0660df8e79ece58f75251ca6ae83ebbeefdeecc8ce6eabf31e.jpg)\n\n\n![(a) Stop zipping (b) Unmerge Figure 5: Partial Zip. (a) If we stop zipping early and (b) apply the latest U from the zip propagation to the inputs of the first unmerged layer in each model, (c) we get a multi-head model with a head for each task.](images/95eee3f2f24e55c211388c8c3ff2283c7bd8f08e6b62d02dc600561691d8657d.jpg)\n\n\n![](images/272fe708035c93c549911d362aadf7a2a7940f647384d915c946895b4d0a12e4.jpg)\n\n\n![(c) Result is Multi-Head](images/50bfab26a0b5e0cae33761c04a8f47717ca3301ca8a637b5ded64cf9b14a59f3.jpg)\n\n\nThus, we \"propagate\"  $M_{i}$  and  $U_{i}$  through these layers. For instance, in Fig. 4, we show a common stack of layers found in a typical ConvNet. Following Jordan et al. (2022), we compute  $M_{i}$  and  $U_{i}$  using the activations of the network (i.e., after each ReLU). We can't fuse  $M_{i}$  with the ReLU layer, as it doesn't have any parameters. Similarly, we can merge the parameters of the preceding BatchNorm layer (i.e., in the same way as bias). But it doesn't have a weight matrix, so we also can't fuse  $M_{i}$  into it. Only once we've reached the Conv layer can we fuse  $M_{i}$  and  $U_{i}$  into it using Eq. 8 (in this case, treating each kernel element as independent).\n\n\nSimilar care needs to be taken with skip connections, as every layer that takes input from or outputs to a skip connection shares the same feature space. However, this too can be dealt with during propagation\u2014we just need to propagate  $M_{i}$  backward and  $U_{i}$  forward to each layer connected by the same skip connection. In general, we can define propagation rules to handle many different types of network modules (see Appendix C).\n\n\n# 4.3 EXTENSIONS\n\n\nPartial Zip. We don't always want to zip every layer of the two networks, especially if their output spaces are incompatible, or if doing so would lose too much accuracy. Instead, we can perform a partial zip. That is, we zip most of the layers together, but leave the later ones unzipped (Fig. 5).\n\n\nImplementing this operation is simple in our framework: zip as normal until the specified layer  $i$ , then the remaining unzipped layers will receive  $U_{i}$  through zip propagation. If we apply  $U_{i}^{A}$  to  $L_{i+1}^{A}$  and  $U_{i}^{B}$  to  $L_{i+1}^{B}$ , the remaining unzipped layers will form \"heads\" that expect merged features as input. We can then ensemble the heads or choose one to evaluate at runtime.\n\n\nRepeated Matching  $(\\alpha)$ . In some cases, we'd like to merge more than two models together. To do this, we allow \"repeated matches\". That is, when two features are matched in our greedy algorithm, they are removed and replaced with the resulting merged feature. To ensure that one feature doesn't get merged endlessly, we set the correlations of the new feature to be the minimum of the old features' similarities weighted by  $\\alpha \\in (0,1]$ . We find a small value of  $\\alpha$  typically works best.\n\n\nSame-model Budget  $(\\beta)$ . To demonstrate the effectiveness of same-model merges, we introduce a \"budget\" parameter  $\\beta \\in [0,1]$  that denotes what percent of total merged features can come from models merging within themselves, with each model receiving an equal portion of this budget. Note that a budget of 0 results in Eq. 2, as in that case no features can be merged within models.\n\n\n# 5 RESULTS\n\n\nThere is no standard benchmark to evaluate merging approaches on models from distinct tasks, so we construct our own. We evaluate our approach in two different settings. (1) A versatile test-bed: disjoint category splits of the same dataset (i.e., same dataset and different label sets). (2) A very challenging setting: completely different datasets and tasks (i.e., different datasets and label sets).\n\n\nExperimental Details. For each experiment where we sample multiple disjoint splits of categories, we hold one split out for hyperparameter search and report mean and standard deviation on the rest. For experiments with models trained on different datasets, we subsample the validation set into a validation and test set to use for the same purpose. To compute correlations, we use a portion of the\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">FLOPs (G)</td><td colspan=\"4\">Accuracies (%)</td></tr><tr><td>Joint</td><td>Task A</td><td>Task B</td><td>Avg</td></tr><tr><td>Model A</td><td>0.68</td><td>48.2\u00b11.0</td><td>97.0\u00b10.6</td><td>45.1\u00b18.6</td><td>71.0\u00b14.4</td></tr><tr><td>Model B</td><td>0.68</td><td>48.4\u00b13.8</td><td>49.1\u00b19.3</td><td>96.1\u00b11.1</td><td>72.6\u00b14.9</td></tr><tr><td>W. Avg (Eq. 1)</td><td>0.68</td><td>43.0\u00b11.6</td><td>54.1\u00b11.4</td><td>67.5\u00b11.2</td><td>60.8\u00b14.5</td></tr><tr><td>Git Re-Basin\u2021</td><td>0.68</td><td>46.2\u00b10.8</td><td>76.8\u00b18.9</td><td>82.7\u00b15.1</td><td>79.8\u00b16.5</td></tr><tr><td>Permute (Eq. 2)</td><td>0.68</td><td>58.4\u00b16.8</td><td>86.6\u00b12.1</td><td>87.4\u00b11.1</td><td>87.4\u00b11.4</td></tr><tr><td>ZipIt&#x27;20/20</td><td>0.68</td><td>79.1\u00b11.1</td><td>92.9\u00b11.1</td><td>91.2\u00b11.4</td><td>92.1\u00b11.0</td></tr><tr><td>Ensemble</td><td>1.37</td><td>87.4\u00b12.6</td><td>97.0\u00b10.6</td><td>96.1\u00b11.1</td><td>96.6\u00b10.4</td></tr><tr><td>ZipIt&#x27;13/20</td><td>0.91</td><td>83.8\u00b13.1</td><td>95.1\u00b10.7</td><td>94.1\u00b11.5</td><td>94.6\u00b10.6</td></tr></table>\n\n\n(a) CIFAR-10  $(5 + 5)$ . ResNet-20  $(4 \\times$  width).\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">FLOPs (G)</td><td colspan=\"4\">Accuracies (%)</td></tr><tr><td>Joint</td><td>Task A</td><td>Task B</td><td>Avg</td></tr><tr><td>Model A</td><td>2.72</td><td>41.6\u00b10.3</td><td>82.9\u00b10.7</td><td>24.8\u00b10.4</td><td>53.9\u00b10.5</td></tr><tr><td>Model B</td><td>2.72</td><td>41.6\u00b10.2</td><td>25.1\u00b11.2</td><td>82.8\u00b10.2</td><td>54.0\u00b10.6</td></tr><tr><td>W. Avg (Eq. 1)</td><td>2.72</td><td>17.0\u00b11.7</td><td>23.8\u00b16.9</td><td>24.8\u00b15.9</td><td>24.3\u00b11.9</td></tr><tr><td>Git Re-Basin\u2021</td><td>2.72</td><td>40.9\u00b10.2</td><td>57.3\u00b11.5</td><td>56.7\u00b10.7</td><td>57.0\u00b10.8</td></tr><tr><td>Permute (Eq. 2)</td><td>2.72</td><td>42.8\u00b10.7</td><td>61.6\u00b11.4</td><td>60.5\u00b10.5</td><td>61.0\u00b10.8</td></tr><tr><td>Ziplt&#x27;20/20</td><td>2.72</td><td>54.9\u00b10.8</td><td>68.2\u00b10.8</td><td>67.9\u00b10.6</td><td>68.0\u00b10.4</td></tr><tr><td>Ensemble</td><td>5.45</td><td>73.5\u00b10.4</td><td>82.9\u00b10.7</td><td>82.8\u00b10.2</td><td>82.8\u00b10.4</td></tr><tr><td>Ziplt&#x27;13/20</td><td>3.63</td><td>70.2\u00b10.4</td><td>80.3\u00b10.8</td><td>80.1\u00b10.7</td><td>80.2\u00b10.6</td></tr></table>\n\n\n(b) CIFAR-100  $(50 + 50)$ . ResNet-20  $(8 \\times$  width).\n\n\nTable 1: CIFAR Results. ZipIt! vs. baselines on combining a model trained on half the classes (Task A) with one trained on the other half (Task B) without extra training. We report both joint (10/100-way) and per-task (5/50-way) accuracy. ZipIt! significantly outperforms its baseline and closes in on the upper bound (ensemble accuracy).  $\\ddagger$  refers to Ainsworth et al. (2022).\n\n\ntraining set for each dataset as in Li et al. (2015) (see Appendix B). For a fair comparison, we reset the batch norms for all methods (including the original models) using the training data (following the recommendation in Jordan et al. (2022)). For our method,  $\\text{ZipIt!}_{\\mathrm{n/m}}$  indicates that  $n$  out of the  $m$  layers in the network have been zipped (Sec. 4.3). Note, all our models have different initializations.\n\n\nEvaluation. For the setting with disjoint class splits of the same dataset, we evaluate performance in two ways: joint accuracy and per task accuracy. For joint accuracy, we evaluate each model over all classes in the combined dataset. For per task accuracy, we compute the accuracy of each task individually (i.e., supposing we had task labels at runtime) and then report the average. The former is similar to a continual learning setting where we want to augment the knowledge of the model, while the latter is akin to a multi-task setting where we know which task we're using at test time. For the scenario where we merge models trained on different datasets, we use the per task accuracy metric, as the label spaces are not comparable.\n\n\nBaselines. In addition to the default Weight Matching version of Git Re-Basin (Ainsworth et al., 2022), we compare to two baselines: Weight Averaging (Eq. 1) and Permute (Eq. 2) with  $\\gamma = 1/2$  using our framework (i.e., we set  $M_i$  and  $U_i$  such that Eq. 8 is equivalent). For Permute, we use linear sum assignment to find optimal permutations (following Li et al. (2015)). Note that our Permute is a strong baseline we create using our framework and is more accurate than Git Re-Basin in our settings. It's also similar to REPAIR (Jordan et al., 2022), but without adding extra parameters to the model. Finally, with perfect merging, the merged model's outputs would be identical to the originals. Thus we include Ensemble as an upper bound (executing and concatenating the results of both models).\n\n\n# 5.1 CIFAR-10 AND CIFAR-100\n\n\nWe train 5 pairs of ResNet-20 (He et al., 2016) from scratch with different initializations on disjoint halves of the CIFAR-10 and CIFAR-100 classes (Krizhevsky et al., 2009). While ZipIt! supports \"partial zipping\" to merge models with different outputs (in this case, disjoint label sets), prior methods without retraining do not. To make a fair comparison, we train these CIFAR models with a CLIP-style loss (Radford et al., 2021) using CLIP text encodings of the class names as targets. This way, both models output into the same CLIP-space regardless of the category. Note, this means the models are capable of some amount of zero-shot classification on the tasks they were not trained on.\n\n\nCIFAR-10 (5+5). In Tab. 1a, we merge models trained on disjoint 5 class subsets of CIFAR-10 using ResNet-20 with a  $4 \\times$  width multiplier (denoted as ResNet-20\u00d74). In joint classification (i.e., 10-way), Git Re-Basin is unable to perform better than using either of the original models alone, while our Permute baseline performs slightly better. In stark contrast, our ZipIt! performs a staggering  $32.9\\%$  better than Git Re-Basin and  $20.7\\%$  better than our baseline. If allow the last stage of the network to remain unzipped (i.e., zip up to 13 layers), our method obtains  $83.8\\%$ , which is only  $3.6\\%$  behind an ensemble of model A and model B (which is practically the upper bound for this setting). We also achieve similar results when merging VGG11 models in this setting (Appendix D).\n\n\nCIFAR-100 (50+50). We find similar results on disjoint 50 class splits of CIFAR-100 in Tab. 1b, this time using an  $8 \\times$  width multiplier instead. Like with CIFAR-10, Git Re-Basin fails to outperform even the unmerged models themselves in joint classification (i.e., 100-way), and this time Permute is only  $1.2\\%$  ahead. ZipIt! again significantly outperforms prior work with  $+14\\%$  accuracy over Git Re-Basin for all layers zipped, and a substantial  $+29.2\\%$  if zipping 13/20 layers. At this accuracy,\n\n\nZipIt!13/20 is again only  $3.3\\%$  behind the ensemble for joint accuracy and  $2.6\\%$  behind for average per task accuracy, landing itself in an entirely different performance tier compared to prior work.\n\n\n# 5.2 IMAGENET-1K (200+200)\n\n\nTo test our method on the much harder setting of large-scale data, we train 5 differently initialized ResNet-50 models with cross entropy loss on disjoint 200 class subsets of ImageNet-1k (Deng et al., 2009). To compare to prior work that doesn't support partial zipping, we initialize the models with capacity for all 1k classes, but only train each on their subset.\n\n\nIn Tab. 2 we show results on exhaustively merging pairs from the 5 models. To compute joint (i.e., 400-way) accuracy, we softmax over each task's classes individually (like in Ahn et al. (2021)), and take the argmax over the combined\n\n\n400 class vector. On this extremely difficult task, Git Re-Basin only obtains  $3.1\\%$  for joint accuracy (with random accuracy being  $0.25\\%$ ). Both the Permute baseline and ZipIt! with all layers zipped perform better, but with each at  $8.6\\%$ , are still clearly lacking. Note that we find the same-model merging budget  $\\beta$  to not matter for this set of models (see Fig. 6), which suggests that there's not a lot of redundant information within each model in this setting. Thus, ZipIt! chooses to merge mostly across models instead, performing similarly to the permute baseline. We find this same trend in CIFAR with smaller models (see Fig. 6), and may be an artifact of model capacity. The story changes when we increase the capacity of the merged model by partial zipping:  $\\mathrm{ZipIt!}_{10/50}$  reaches close to upper bound ensemble accuracy on this extremely difficult task, while saving on FLOPs.\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">FLOPs (G)</td><td colspan=\"4\">Accuracies (%)</td></tr><tr><td>Joint</td><td>Task A</td><td>Task B</td><td>Avg</td></tr><tr><td>Model A</td><td>4.11</td><td>37.2\u00b12.0</td><td>74.3\u00b14.0</td><td>0.5\u00b10.1</td><td>37.4\u00b12.0</td></tr><tr><td>Model B</td><td>4.11</td><td>35.3\u00b11.6</td><td>0.5\u00b10.1</td><td>70.5\u00b13.2</td><td>35.5\u00b11.6</td></tr><tr><td>W. Avg (Eq.1)</td><td>4.11</td><td>0.3\u00b10.1</td><td>0.6\u00b10.1</td><td>0.7\u00b10.1</td><td>0.6\u00b10.1</td></tr><tr><td>Git Re-Basin\u2020</td><td>4.11</td><td>3.1\u00b11.2</td><td>5.3\u00b12.6</td><td>5.7\u00b12.4</td><td>5.5\u00b11.7</td></tr><tr><td>Permute (Eq. 2)</td><td>4.11</td><td>8.6\u00b15.8</td><td>10.1\u00b14.4</td><td>15.3\u00b111.1</td><td>12.7\u00b17.7</td></tr><tr><td>ZipIt!50/50</td><td>4.11</td><td>8.6\u00b14.7</td><td>12.4\u00b15.9</td><td>14.7\u00b17.8</td><td>13.5\u00b16.6</td></tr><tr><td>Ensemble</td><td>8.22</td><td>63.3\u00b14.9</td><td>74.3\u00b14.0</td><td>70.5\u00b13.2</td><td>72.4\u00b12.5</td></tr><tr><td>ZipIt!22/50</td><td>6.39</td><td>55.8\u00b14.1</td><td>65.9\u00b12.5</td><td>64.1\u00b13.0</td><td>65.0\u00b12.3</td></tr><tr><td>ZipIt!10/50</td><td>7.43</td><td>60.9\u00b14.1</td><td>70.7\u00b13.0</td><td>69.0\u00b12.9</td><td>69.9\u00b11.9</td></tr></table>\n\n\n# 5.3 MULTI-DATASET MERGING\n\n\nWe now take our model merging framework one step further by merging differently initialized models trained on completely separate datasets and tasks. We present two settings: merging multiple classification datasets and merging semantic segmentation with image classification.\n\n\nImage Classification Datasets. Merging ResNet-50 models trained on: Stanford Dogs (Khosla et al., 2011), Oxford Pets (Parkhi et al., 2012), CUB200 (Welinder et al., 2010), and NABirds (Van Horn et al., 2015). In Tab. 3, we show the average per task accuracy from exhaustively merging each pair and the much more difficult setting of merging all four at once. We report the accuracy of our baselines by applying them up until the last layer, but we can't compare to prior work as they don't support this setting. As in all our previous experiment we merge without retraining.\n\n\nFor pairs of models, ZipIt! slightly outperforms our permute baseline across all tasks and performs similarly when merging all 4 models at once. However, if we add capacity to the merged model through partial zipping, we perform up to  $33\\%$\n\n\nbetter on merging pairs and  $50\\%$  better on merging all four models than the permute baseline. Partial zipping is a significant factor to obtain strong performance, especially with more than 2 models.\n\n\nTable 2: ImageNet-1k (200+200) Results. Merging ResNet-50 models trained from scratch on disjoint 200 category subsets (Task A and B) of ImageNet-1k. Prior work performs poorly, but ZipIt! makes this task feasible.  ${}^{ \\ddagger  }$  Ainsworth et al. (2022).\n\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">FLOPs (G)</td><td colspan=\"5\">Per-Task Accuracies (%)</td></tr><tr><td>SD</td><td>OP</td><td>CUB</td><td>NAB</td><td>Avg</td></tr><tr><td colspan=\"7\">Merging Pairs</td></tr><tr><td>W. Avg (Eq. 1)</td><td>4.11</td><td>12.9</td><td>18.2</td><td>13.9</td><td>0.2</td><td>11.3</td></tr><tr><td>Permute (Eq. 2)</td><td>4.11</td><td>46.2</td><td>47.6</td><td>35.6</td><td>13.5</td><td>35.7</td></tr><tr><td>ZipIt!49/50</td><td>4.11</td><td>46.9</td><td>50.7</td><td>38.0</td><td>12.7</td><td>37.1</td></tr><tr><td>Ensemble</td><td>8.22</td><td>72.7</td><td>81.1</td><td>71.0</td><td>77.2</td><td>75.5</td></tr><tr><td>ZipIt!22/50</td><td>6.39</td><td>62.6</td><td>71.2</td><td>62.8</td><td>53.0</td><td>62.4</td></tr><tr><td>ZipIt!10/50</td><td>7.42</td><td>66.5</td><td>75.8</td><td>65.6</td><td>66.8</td><td>68.7</td></tr><tr><td colspan=\"7\">Merging All 4</td></tr><tr><td>W. Avg (Eq. 1)</td><td>4.12</td><td>0.8</td><td>3.0</td><td>0.6</td><td>0.3</td><td>1.2</td></tr><tr><td>Permute (Eq. 2)</td><td>4.12</td><td>15.7</td><td>26.1</td><td>14.0</td><td>5.3</td><td>15.3</td></tr><tr><td>ZipIt!49/50</td><td>4.12</td><td>21.1</td><td>33.3</td><td>8.6</td><td>3.9</td><td>16.8</td></tr><tr><td>Ensemble</td><td>16.4</td><td>72.7</td><td>81.2</td><td>71.0</td><td>77.2</td><td>75.5</td></tr><tr><td>ZipIt!22/50</td><td>11.0</td><td>50.2</td><td>55.9</td><td>44.0</td><td>32.0</td><td>45.5</td></tr><tr><td>ZipIt!10/50</td><td>14.1</td><td>63.5</td><td>70.8</td><td>63.7</td><td>63.1</td><td>65.3</td></tr></table>\n\n\nTable 3: Multi-Dataset Results. Merging ResNet-50 models trained on completely different datasets: Stanford Dogs (SD), Oxford Pets (OP), CUB200 (CUB), and NABirds (NAB). We report average per-task accuracy over merging model pairs, and all four.\n\n\nMultiple Output Modalities. In Appendix F, we combine across modalities by merging the ResNet-50 backbone of a DeeplabV3 (Chen et al., 2017) segmentation model with an ImageNet-1k classification model. The resulting combined model can perform both semantic segmentation and image classification. Even with half of layers merged, ZipIt! retains good performance on both tasks.\n\n\n![(a) Ablation: Varying  $\\beta$](images/75ed538048be079e8872350e230041163fbe61f22c7797600a0eb65421f3fc11.jpg)\n\n\n![Figure 6: Varying  $\\beta$  and Model Scale. Left: We find when the model has enough capacity for the task, a high budget (Sec. 4.3) improves performance. Right: ZipIt! makes effective use of extra model capacity to quickly reach the ensemble on CIFAR-100 (50+50) when we increase the width of ResNet-20 models. In contrast, our baselines only slightly benefit from the extra scale.](images/cc00d41500bd1efc41b879a07dc05932ef9df0c58dcbc02d39b923b6b2e72944.jpg)\n\n\n![(b) Ablation: Model Scale](images/f103d1d45e90c5d5e82deeb4a0d07e77b5e38b3517cd0143b776855bcc2be31a.jpg)\n\n\n# 6 ANALYSIS\n\n\nMerging within Models. A critical piece of ZipIt! compared to prior work is the ability to merge within models, not just across models. In Sec. 4.3, we introduce a budget parameter  $\\beta$  to limit the number of same-model merges, and here use CIFAR-100 (50+50) and ImageNet-1k (200+200) to illustrate its effectiveness (Fig. 6a). On CIFAR, same-model merges are very important, with the optimal budget being above 0.8, meaning  $80\\%$  of merges are allowed to be within the same model. This is not the case, however, on ImageNet, where the difficulty of the task means there likely are much fewer redundant features within each model.\n\n\nModel Scale. In Fig. 6b, we test the effect of model scale directly by evaluating joint accuracy on our CIFAR-100 (50+50) setting with ResNet-20 models of increasing width. Here, we explicitly see that when the width of the models are too small for the task (e.g.,  $< 4 \\times$ ), ZipIt! and the Permute baseline perform identically (though both much better than Git Re-Basin). However, when the scale increases, ZipIt! trends toward the ensemble upper bound of  $75\\%$ , while both the Permute baseline and Git Re-Basin plateau at around  $45\\%$ . This corroborates Eq. 4 and indicates our method uses the extra model capacity effectively, much better than prior work.\n\n\nMatching Algorithm. In Tab. 4, we compare matching algorithms used to compute  $M_{i}$  in Eq. 8. Using either the identity (weight averaging) or a permutation (as in prior work) underperforms on CIFAR-10 (5+5) joint 10-way classification. In contrast, we obtain up to  $21.2\\%$  higher accuracy if we allow both permutations and merging within models. However, doing this optimally is difficult, as the standard linear sum assignment algorithm assumes bipartite matches. We could use a optimal graph-based solver (e.g., Hagberg et al. (2008)) instead, but doing so is prohibitively slow (11 minutes to transform a ResNet-20\u00d74 model). Thus, we find matches greedily by repeatedly taking the most corre\n\n\nlated pair of features without replacement. This performs almost as well, and is multiple orders of magnitude faster. If we allow repeated matches (Sec. 4.3), we obtain a slightly better result. Like Bolya et al. (2023), we find that matching is better for merging features than clustering (K-Means).\n\n\n<table><tr><td>Algorithm</td><td>A\u2194A/B\u2194B?</td><td>Acc</td><td>Time</td></tr><tr><td>Identity (Eq. 1)</td><td>X</td><td>43.0\u00b13.1</td><td>1.8 ms</td></tr><tr><td>Permute (Eq. 2)</td><td>X</td><td>58.4\u00b11.3</td><td>28 ms</td></tr><tr><td>K-Means</td><td>\u2713</td><td>29.1\u00b15.5</td><td>19 sec</td></tr><tr><td></td><td>Zip (Eq. 8)</td><td></td><td></td></tr><tr><td>Optimal Match</td><td>\u2713</td><td>79.6\u00b11.7</td><td>11 min</td></tr><tr><td>Greedy Match</td><td>\u2713</td><td>79.0\u00b11.8</td><td>1.1 sec</td></tr><tr><td>Greedy, \u03b1=0.1</td><td>\u2713</td><td>79.1\u00b12.1</td><td>1.2 sec</td></tr></table>\n\n\nTable 4: Comparing Matching Algorithms to use for  $M_{i}$  on CIFAR-10  $(5 + 5)$  joint 10-way accuracy. Permuting  $\\mathrm{B}\\rightarrow \\mathrm{A}$  as in prior work (Eq. 2) performs poorly. We significantly improve by merging features within each model (Eq. 8). Our greedy approach is nearly as accurate as the optimal algorithm while being two orders of magnitude faster.\n\n\n# 7 CONCLUSION\n\n\nIn this paper, we tackle the extremely difficult task of merging models trained on completely disjoint tasks without additional training. We find that prior work underperforms in this setting and posit that they neither fully (1) exploit model similarities nor (2) account for model dissimilarities. We introduce ZipIt!, a general framework for merging models that addresses these issues, and show it to significantly outperform prior work across several difficult settings, comprehensively analyzing each.\n\n- Reproducibility Statement. To ensure reproducibility, we will release code for our algorithm, experiments, and baselines. We also include algorithm details in Section 4 and further in Appendix C, experimental details in Section 5 and Appendix B, and a proof of our Theorem 1 in Appendix G.\n\n- Acknowledgements. This work was supported in part by funding from NSF CAREER #2144194, ARL, Google, and NSF GRFP. All views and conclusions expressed in this work are those of the authors and not a reflection of these sources.\n\n\n# REFERENCES\n\n- Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax for incremental learning. In ICCV, 2021.\n\n- Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. arXiv:2209.04836, 2022.\n\n- Stephen Ashmore and Michael Gashler. A method for finding similarity between multi-layer perceptrons by forward bipartite alignment. In IJCNN, 2015.\n\n- Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. In ICML, 2022.\n\n- Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011.\n\n- Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In ICCV, 2019.\n\n- Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. ICLR, 2023.\n\n- Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes, Zhuowen Tu, and Stefano Soatto. Exponential moving average normalization for self-supervised and semi-supervised learning. In CVPR, 2021.\n\n- Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.\n\n- Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\n\n- Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. Fusing finetuned models for better pretraining. arXiv:2204.03044, 2022.\n\n- Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. TPAMI, 2021.\n\n- Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. arXiv:2302.05442, 2023.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\n\n- Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, and Leshem Choshen. ColD fusion: Collaborative descent for distributed multitask finetuning. Association for Computational Linguistics, 2023.\n\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv:2010.11929, 2020.\n\n- Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In ICML, 2018.\n\n- Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. arXiv:2110.06296, 2021.\n\n- M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303-338, June 2010.\n\n- Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In ICML, 2020.\n\n- C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. arXiv:1611.01540, 2016.\n\n- Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. NeurIPS, 2018.\n\n- Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In ICASSP, 2017.\n\n- Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. NeurIPS, 2020.\n\n- Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. Knowledge is a region in weight space for fine-tuned language models. arXiv:2302.04863, 2023.\n\n- Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. Exploring network structure, dynamics, and function using networkx. In Proceedings of the 7th Python in Science Conference, 2008.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016.\n\n- Kaiming He, Georgia Gkioxari, Piotr D\u00f3lar, and Ross Girshick. Mask r-cnn. In ICCV, 2017.\n\n- Xiaoxi He, Zimu Zhou, and Lothar Thiele. Multi-task zipping via layer-wise neuron sharing. NeurIPS, 2018.\n\n- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. *NeurIPS*, 2020.\n\n- Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. arXiv:1704.00109, 2017.\n\n- Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv:2212.04089, 2022a.\n\n- Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. NeurIPS, 2022b.\n\n- Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. UAI, 2018.\n\n- Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur. Repair: Renormalizing permuted activations for interpolation repair. arXiv:2211.08403, 2022.\n\n- Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2018.\n\n- Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for fine-grained image categorization: Stanford dogs. In CVPR Workshop on Fine-Grained Visual Categorization (FGVC), 2011.\n\n- James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 2017.\n\n- Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In ICML, 2019.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 2017.\n\n- Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent Learning: Do different neural networks learn the same representations? arXiv:1511.07543, 2015.\n\n- Zhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 2017.\n\n- Chang Liu, Chenfei Lou, Runzhong Wang, Alan Yuhan Xi, Li Shen, and Junchi Yan. Deep neural network fusion via graph matching with applications to model ensemble and federated learning. In ICML, 2022.\n\n- Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. arXiv:2111.09832, 2021.\n\n- Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics. PMLR, 2017.\n\n- Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant feature representation. In ICML, 2013.\n\n- Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? NeurIPS, 2020.\n\n- Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012.\n\n- Fidel A Guerrero Pena, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric Granger, and Marco Pedersoli. Re-basin via implicit sinkhorn differentiation. arXiv:2212.12042, 2022.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.\n\n- Alexandre Ram\u00e9, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, L\u00e9on Bottou, and David Lopez-Paz. Recycling diverse models for out-of-distribution generalization. arXiv:2212.10445, 2022.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n\n- Berfin Simsek, Fran\u00e7ois Ged, Arthur Jacot, Francesco Spadaro, Clement Hongler, Wulfram Gerstner, and Johann Brea. Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances. In Proceedings of the 38th International Conference on Machine Learning. PMLR, 2021.\n\n- Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. NeurIPS, 2020.\n\n- Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, and Lijuan Wang. An empirical study of multimodal model merging, 2023.\n\n- Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. NeurIPS, 2017.\n\n- Norman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, and Rongjie Lai. Optimizing mode connectivity via neuron alignment. NeurIPS, 2020.\n\n- Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In CVPR, 2015.\n\n- Johannes Von Oswald, Seijin Kobayashi, Joao Sacramento, Alexander Meulemans, Christian Henning, and Benjamin F Grewe. Neural networks with late-phase weights. arXiv:2007.12927, 2020.\n\n- Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. arXiv:2002.06440, 2020.\n\n- Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. TKDE, 2022.\n\n- P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.\n\n- Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In ICML, 2022a.\n\n- Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In CVPR, 2022b.\n\n- Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Resolving interference when merging models. arXiv:2306.01708, 2023.\n\n- Masanori Yamada, Tomoya Yamashita, Shin'ya Yamaguchi, and Daiki Chijiwa. Revisiting permutation symmetry for merging models between different datasets, 2023.\n\n- Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In ICML, 2019.\n\n- Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In CVPR, 2022.\n\n- Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636558.4528906", "msecs": "452.0", "relativeCreated": "19146.031141281128", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Inductive Linguistic Reasoning with Large Language Models\n\n# Abstract\nEvaluating large language models (LLMs) on their linguistic reasoning capabilities is an important task to understand the gaps in their skills that may surface during large-scale adoption. In this work, we investigate the abilities of such models to perform abstract multilingual reasoning through the lens of linguistic puzzles on extremely low-resource languages. As these translation tasks involve inductive and deductive reasoning from reference instances, we examine whether diverse auxiliary demonstrations can be automatically induced from seed exemplars, through analogical prompting. We employ a two-stage procedure, first generating analogical exemplars with a language model, and then applying them in-context along with provided target language exemplars. We explore various combinations of language models as analogical generators and reasoning agents, testing different model sizes and specialized multilingual LLMs. Our results on the modeLing dataset show that analogical prompting is effective in eliciting models' knowledge of language grammar similarities, boosting the performance of GPT-4o by as much as 8.1\\% and Llama-3.1-405B by 5.9\\% over chain-of-thought approaches. These gains are realized with self-generated analogical demonstrations as well as those generated by weaker multilingual models. We also report several findings about interesting phenomena which drive linguistic reasoning performance, suggesting that such puzzles are a valuable benchmark for new reasoning methods.\n\n\n# 1 INTRODUCTION\n\n\nAs the capabilities of large language models (LLMs) continue to grow, it is necessary to develop ways of testing the boundaries of their ability to reason over a wide range of languages. In particular, adapting language models to low-resource languages is challenging due to a lack of high-quality annotated data in the target language for supervised fine-tuning. This has led to zero-shot and few-shot transfer learning approaches being more commonly employed (Zoph et al., 2016; Nguyen & Chiang, 2017; Lin et al., 2019). However, given the emergence of the in-context learning phenomenon in LLMs, we hypothesize that this behavior can be used to enable few-shot generalization to new languages at inference time.\n\n\nIn this work, we explore the task of linguistic reasoning, using linguistics puzzles akin to the International Linguistics Olympiad (IOL). Notably, in these puzzles, the target language is often extremely low-resource or functionally extinct (Bean et al., 2024). While prior work has largely examined the effect of vanilla in-context learning with English-target and target-English exemplars, chain-of-thought prompting, and traditional neural machine translation methods (Chi et al., 2024; \u015eahin et al., 2020), we believe that generating auxiliary exemplars which supplement the target language demonstrations can guide the model to more effectively learn grammar similarities over a language family. As such, we introduce an approach based on analogical prompting (Yasunaga et al., 2024), which uses strong language models to self-generate exemplars of relevant problems given the test instance and performs in-context learning conditioned on those demonstrations. In our setting, the knowledge retrieval-like nature of analogical prompting allows us to test models' parametric understanding of language families, performing inference with both the provided and induced demonstrations.\n\n\nWe evaluate our approach on the modeLing (Chi et al., 2024) dataset, consisting of unseen IOL-style problems. We find that strong models such as GPT-4o and Llama-3.1-405B-Instruct can identify the language family, similar languages within said family, generate exemplars in those similar\n\n\n![Figure 1: An illustration of our 2-stage analogical prompting approach, translating a phrase in Montenegrin to English. While prior works would solely provide exemplars translating between the source language and English and perform in-context learning, our method seeks diverse exemplars. Model  $M_{1}$  first identifies the language family (Slavic) and higher-resource languages in the family which the model has knowledge of (Croatian), then produces exemplars in those languages. Finally, both the original and generated set of exemplars are passed with the test puzzle to model  $M_{2}$  to perform the translation.  $M_{1} = M_{2}$  yields the self-generated analogical reasoning setting.](images/631acd980f3520aa3eca145aea5e5a331a5358622aac1bf7a29675297520d3dc.jpg)\n\n\nlanguages, and apply them to solve the test puzzle. Furthermore, while weak models do not benefit significantly from using strong model-generated exemplars, strong models improve from using exemplars produced by weaker yet specialized multilingual models (e.g. Aya-35B). Our findings show that the ability of the model to deduce and apply rules, following inductive learning from the exemplars, largely influences performance; where there is still much to be desired relative to the reasoning of human experts on this task. From our results, we suggest that the linguistic reasoning task presents a fertile ground for research on new language model reasoning methods, to uncover how the skills which drive logical thinking may be imbued to models.\n\n\n# 2 ANALOGICAL PROMPTING FOR LINGUISTIC REASONING\n\n\nAnalogical prompting (Yasunaga et al., 2024) avoids the need for annotated exemplars by relying on a strong model to generate exemplars which are related to the test instance, but are sufficiently diverse relative to one another and the test sample. Our approach of applying analogical prompts follows the human system 2 thinking framework of slow, deliberate reasoning (Kahneman, 2011). In chain-of-thought prompting for these puzzles, the model performs in-context learning with the given exemplars, learning the rules governing the language by induction, including the meaning of particular words, and using deduction to apply these rules to the test sample. This approach is supported by prior works demonstrating the ability of LMs to learn rules and attempt to apply them (Qiu et al., 2024; Zhu et al., 2024). Furthermore, we do not have access to complete grounding sources of human-written rules governing these low-resource languages, so we must rely on the LM to identify and generate these rules itself. However, as we expect the model to have little to no prior knowledge about the target language<sup>1</sup>, we seek to leverage other languages with similar grammar structure which the model has learned in order to guide the language model's reasoning process.\n\n\nWe use language families as a taxonomically-grounded means of identifying similar languages as the target. The generated exemplars provide a source of reasoning support to the model, enabling it to perform inductive reasoning first in a cross-lingual manner over the diverse exemplars, and then deduce from its shared understanding. For instance, in Figure 1, given the test instance and the provided exemplars in Montenegrin, we leverage the model to (i.) identify the family of Montenegrin (Slavic Language Family), (ii.) select a few languages in the Slavic Language Family, and (iii.) generate example puzzles with their solutions in those selected languages, e.g. Croatian. Then, the provided exemplars and the generated cross-lingual demonstrations are provided to the model, to solve the given test puzzle.\n\n\nIn line with the view of analogical prompting as a knowledge retrieval procedure in accordance with the LM's pre-training distribution, we desire for the model to produce exemplars from languages it has learned, while probing its' understanding of language families. In (Qiu et al., 2024), models have been demonstrated to improve with more familiar exemplars (based on inclusion in the pretraining data). Furthermore, their work suggested that noisy demonstrations hurt performance; as the provided examples from the unseen target language could be considered as \"noisy\", we hypothesize that the generated exemplars can help to compensate.\n\n\nExemplar Correctness. While one would ideally prefer to have a validator which, given a set of rules for a language, can determine if they are being appropriately applied for each of the analogical exemplars, this is very challenging at scale. In the context of Linguistics Olympiad problems, only a small fraction of the population who are experts in such tasks (equivalent to achieving a high score on these contests) would be able to reliably annotate solution rationales for these extremely low-resource translation puzzles. Furthermore, the notion of correctness is ambiguous \u2013 we rely on exact match relative to an annotated \"correct\" response, but it is unclear if there could be more than one \"correct\" response which is context-specific, or if partial credit assignment could be possible. Given the models' lack of zero-shot knowledge of these languages (else, there would likely be leakage), we also cannot reliably use another language model as a validator. As a result, we leverage all generated exemplars by the model for inference, and assume each problem has one correct solution.\n\n\n# 2.1 LINGUISTICS PUZZLES\n\n\nAs noted before, the focus of this work is on linguistics puzzles \u2013 in particular, translation problems from English to a low-resource language and vice versa. Such problems are also referred to in the literature as Rosetta Stone puzzles, and constitute one of the most frequent types of problems that appear in Linguistics Olympiad competitions (\u015eahin et al., 2020; Chi et al., 2024; Bean et al., 2024). These problems typically consist of a test phrase in language A along with 5-10 exemplars<sup>2</sup> of translation from language A to language B and vice versa, and the task is to translate the given phrase into language B. We include an example of such a problem below, from Chi et al. (2024).\n\n\n# EXAMPLE TRANSLATIONS FROM ENGLISH TO RAPA NUI\n\n\nEnglish: We see you.  $\\rightarrow$  Rapa Nui: tike'a t\u0101tou koe  \nEnglish: I hear you.  $\\rightarrow$  Rapa Nui: aro'a au koe  \nEnglish: I see you.  $\\rightarrow$  Rapa Nui: tike'a au koe\n\n\nEnglish: We hear you.  $\\rightarrow$  Rapa Nui: aro'a tatou koe English: We bite the bone.  $\\rightarrow$  Rapa Nui: au tatou ivi\n\n\nEnglish: We hit the bird.  $\\rightarrow$  Rapa Nui: pu'a t\u0101tou manu\n\n\n# TRANSLATE TEST PHRASE\n\n\nEnglish: The bird bites you.  $\\rightarrow$  Rapa Nui: au manu koe\n\n\n# 3 METHODS\n\n\nWe explore a number of sampling methods across various language models to assess their performance on reasoning over unknown languages.\n\n\n# 3.1 BASELINES\n\n\nWe include the following methods as baselines for robust comparison to our method, reflecting prior work examined in linguistic reasoning (Chi et al., 2024). We explore their results in Section 4.1.\n\n\nZero-Shot Prompting. Given the low-resource nature of the languages that we examine, we expect zero-shot performance to be poor, or even zero, on the exact match metric. However, we include this setting for two reasons: (1.) a model which gets multiple questions correct for a given language with zero-shot prompting may be an indication of leakage, and (2.) this serves as a robust check on any additional metrics examined aside from exact match.\n\n\nFew-Shot Prompting / In-Context Learning. As in the Linguistic Olympiad competitions, demonstrations of translation to and from the low-resource language are provided to the model, with the intention for inductive reasoning to guide the model towards identifying the set of grammar rules the language follows.\n\n\nFew-Shot Chain-of-Thought Reasoning. Given the efficacy of chain-of-thought prompting (Wei et al., 2024; Kojima et al., 2022), we extend the few-shot evaluation setting by including prompts for the model to \"think step-by-step\" (Kojima et al., 2022; Yang et al., 2024a). We also include a chain-of-thought rationale exemplar for English-Spanish translation from (Chi et al., 2024), to demonstrate how step-by-step reasoning rationales should be produced, and are denoted in Section 4 as \"w/ rationale\".\n\n\n# 3.2 ANALOGICAL PROMPTING VARIATIONS\n\n\nWe describe the various analogical prompting methods explored in the experiments; their results are in Section 4.2.\n\n\nAnalogical Prompting on Language Families. As noted in Section 2, we seek to use language families as a means to identify similar, auxiliary languages whose exemplars can boost the model's cross-lingual understanding. In a similar environment to the Linguistics Olympiad competition, where one does not have access to any external resources, we test the model on its latent understanding of language families and regional associations to generate further exemplars and puzzles in another language within the same language subgroup as the target language. For a target language  $L$ , we prompt the model to identify a few other languages (denote this list  $L_{Aux}$ ) in the same family as  $L$ ; then, for each language in  $L_{Aux}$ , generate a puzzle translating from  $l$  to English, and a puzzle in the reverse direction. Then, we apply these exemplars along with the given ones for  $L$  in a new instruction to the model. We term this 2-stage analogical reasoning.\n\n\nSeparating the two stages of analogical prompting (generation and application) yields an opportunity to explore how different combinations of models for this approach might perform. While the above entails using the same model for both steps, we look to contrast the strength of the models used, to attempt to boost the performance of both frontier and small models.\n\n\nInference-time Exemplar Distillation. In our work, inference-time distillation refers to generating analogical exemplars with a strong model (e.g. GPT-4o) and applying them to a weak model (e.g. models with roughly 7-8B parameters). Our hypothesis driving this setting is: can higher quality exemplars produced by strong models enable better deductive abilities with weak models?\n\n\nWeak-to-Strong Cross-Linguual Analogies. Specialized multilingual models such as the Aya-23 models hold promise for our linguistic reasoning analysis, as they have been fine-tuned for instruction-following across a wide range of languages. We propose using such models for generating analogical demonstrations, as they may have a stronger understanding of language families and can produce diverse exemplars, which we believe strong models may be able to deduce from.\n\n\n# 3.3 EXPERIMENTAL SETUP\n\n\nDatasets. We primarily evaluate our approaches on the modeLing dataset (Chi et al., 2024). This dataset consists of problems written by the authors and hence uninvolved in prior Linguistics\n\n\nOlympiads. This benchmark was released in 2024, and we rely on its recency to be more assured that leakage is not a factor driving performance. We note that all problems examined are purely text-based; while there exist linguistics puzzles that require deduction from images, filling in diagrams, etc., the benchmark we evaluate on does not include such problems. This suggests that future work could study the performance of multimodal models on these problem types. We also evaluate on the LINGOLY dataset (Bean et al., 2024), which features 1,133 problems and expands beyond \"Rosetta Stone\" translation problems to include grammatical pattern-based translation, matching translation pairs, text purely in an unknown language, identifying errors in machine translation, and longer text in multiple languages. The results are included in Appendix D.\n\n\nModels. We evaluate with the following models:\n\n- - OpenAI models: GPT-4o, GPT-4, and GPT-3.5-turbo\n\n- - Open-weight models: Llama 3.1 8B-Instruct, Llama 3.1 70B-Instruct, Llama 3.1 405B-Instruct (Dubey et al., 2024), Mixtral 8x7B-Instruct-v0.1 (Jiang et al., 2024), and Mixtral 8x22B-Instruct-v0.1\n\n- - Multilingual Instruction-tuned Models: Aya-23 8B and Aya-23 35B (henceforth referred to as Aya-8B and Aya-35B) (Aryabumi et al., 2024)\n\n\nOpenAI models are inferred with the OpenAI API, while the open-weight and multilingual instruction-tuned models are queried with the Together AI API and Apple MLX, respectively.\n\n\n# 4 RESULTS\n\n\nWe report exact match (EM) scores for all experiments performed. ChrF2 Popovic (2015), a character n-gram F-score measure, and corpus-level BLEU scores (Papineni et al., 2002) are recorded in Appendix A. We do not treat these as primary metrics as BLEU ignores word ordering nuances amidst short responses in machine translation, which is integral to measuring correctness in the puzzles we explore (Callison-Burch et al., 2006; Chi et al., 2024), and we find the ChrF scores to be noisy relative to EM scores. Smaller models with weaker instruction-following capabilities often failed to produce their output in the exact desired format specified in the prompts. To ensure that reliable exact match scores are reported while some responses may have parsing issues relative to the expected format, the authors of this work manually examined each response to confirm whether the output generated contains the target response. To enforce standardization across our evaluation procedure, this was performed for all experiments; this was not applicable for stronger models whose responses exactly followed the desired output format.\n\n\n# 4.1 CHAIN-OF-THOUGHT LINGUISTIC REASONING\n\n\nThe results of baseline methods are in Table 1. The prompts for all experiments are included in Appendix F, and all experiments are averaged over 3 runs. For the \"CoT with rationale experiment\", we take the best of using 512 and 4096 max tokens (see Appendix B). For the \"few shot\" results, we take the best out of two different prompt settings, ablated on in Appendix C.\n\n\nOur strongest baseline result is achieved with Llama-3.1-405B-Instruct producing CoT rationales, at  $65.81\\%$ ; in fact, this model produces the best results across all 4 baseline settings. GPT-4o remains in the high 50s, but does not exceed  $60\\%$  on any single run. Among smaller models, Llama-3.1-8B-Instruct performs comparably to Aya-35B and Mixtral-8x7B-Instruct, outperforming it on some baselines, which may be attributable to a stronger and more recent base model. We also observe that GPT-4o and Llama-3.1-405B-Instruct do indeed solve a few puzzles (2 and 4 samples, respectively) in the zero-shot setting. Given the former was released before the modeLing dataset, and the latter was released just shortly after, we do not believe this to be a sign of leakage; furthermore, each correct question was from a different language.\n\n\nWe report a few key observations below:\n\n\nStrong models produce rationales without being instructed to. We find that strong models such as GPT-4o and GPT-4 produce chain-of-thought stepwise rationales for responses, even in the zero-shot and few-shot settings, without including a chain-of-thought prompt or including rationales in\n\n\nTable 1: Baseline experimental results using chain-of-thought methods, reporting exact match. The models have been split into three groups, corresponding to the models noted in Section 3.3. All results reported are average of 3 runs at a temperature of 0.3, to address sampling variance.\n\n\n<table><tr><td>Model</td><td>Zero-Shot</td><td>Few-Shot w/o CoT</td><td>Few-Shot w/ CoT</td><td>Few-Shot CoT w/ Rationales</td></tr><tr><td>GPT 3.5-Turbo</td><td>0%</td><td>25.74%</td><td>26.10%</td><td>38.6%</td></tr><tr><td>GPT-4</td><td>0%</td><td>56.25%</td><td>45.22%</td><td>45.59%</td></tr><tr><td>GPT-4o</td><td>1.10%</td><td>59.19%</td><td>58.82%</td><td>55.88%</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>0%</td><td>22.79%</td><td>16.91%</td><td>23.16%</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>0%</td><td>45.22%</td><td>44.49%</td><td>42.28%</td></tr><tr><td>Llama-3.1-405B-Instruct</td><td>1.47%</td><td>61.76%</td><td>59.19%</td><td>65.81%</td></tr><tr><td>Mixtral-8x7B-Instruct</td><td>0%</td><td>22.43%</td><td>22.06%</td><td>16.18%</td></tr><tr><td>Mixtral 8x22B-Instruct</td><td>0%</td><td>45.59%</td><td>43.38%</td><td>39.71%</td></tr><tr><td>Aya-23-8B</td><td>0%</td><td>9.93%</td><td>7.35%</td><td>5.88%</td></tr><tr><td>Aya-23-35B</td><td>0%</td><td>23.53%</td><td>20.59%</td><td>14.34%</td></tr></table>\n\n\nthe exemplars. This is a key reason why the few-shot without chain-of-thought setting performs the highest for both models. Furthermore, when prompted with rationale-inducing exemplars (see Appendix F), these strong models produce rule libraries from the exemplars, akin to (Zhu et al., 2024), leading to very lengthy responses; some models such as Llama-3.1 70B fall into loops of repeating the same rule many times. This necessitates the use of a higher number of max tokens to be generated, to ensure that the final answer is indeed outputted, although this makes human verification of response correctness harder due to their length; we report ablations on this in Appendix B.\n\n\nCertain models perform uncertainty-based refusal. Some models, such as Mixtral-8x7B-Instruct and Mixtral-8x22B-Instruct respond to test instances by stating an inability to perform the desired task. This behavior especially appears in CoT with rationale exemplars; interestingly, this occurs after the implicit induction stage has been performed. For instance, with Mixtral-8x22B-Instruct, the model enumerates a set of word-level translations between the target language and English, respectively, then upon recognizing ambiguity in one of the word-level translations, it claims that solving the problem is impossible without additional information. By contrast, models such as GPT-4o instead output multiple candidate answers when it is not entirely certain. We include qualitative examples of this behavior in Appendix E. This appears to reinforce the findings of the Qiu et al. (2024) in that models are unable to reliably apply their inductively learned rules.\n\n\nOur analogical reasoning method introduces an inference-time approach to boost deductive reasoning, by deliberately using their learned multilingual knowledge to guide puzzle solving.\n\n\n# 4.2 TWO-STAGE ANALOGICAL REASONING\n\n\nTo critically explore the evaluation settings introduced in Section 3.2, we select 2 frontier models \u2013 GPT-4o and Llama-3.1-405B-Instruct \u2013 which were the strongest performers in our baselines. We select 2 weaker models \u2013 Aya-35B and Llama-3.1-8B-Instruct \u2013 for the inference-time distillation and weak-to-strong prompting experiments. These models performed comparably to one another in the baselines, and allow us to contrast multilingual specialization against a generalist model with multilingual support. The experiments with Llama-3.1-8B-Instruct are included in Appendix I.\n\n\nWe also establish an upper bound on the performance we can attain with our approach, by a psuedo-open-book method with oracle language families. That is, for each language in the evaluation set, rather than prompting the model to implicitly infer the language family and other languages which are a member of it, we abstract away the former by providing the language family in the prompt. We suggest that a human expert with strong cross-lingual reasoning abilities would be able to deduce such relationships with similar languages, so providing language family labels eliminates one uncertainty source in the model's generations. The results of this are included in Figure 2b.\n\n\nAnalogical reasoning boosts frontier models. We find that 2-stage analogical reasoning pushes the boundaries of the performance of frontier models, relative to their best baseline results. Solely considering the self-generation setting (where the same model both generates analogical exemplars\n\n\nTable 2: Pairwise results with our 2-stage analogical prompting method. The left column denotes the model generating the analogical exemplars, and the top row denotes the model applying both the generated and provided exemplars to answer the test puzzle. Note that these results address the self-generated analogical reasoning, inference-time distillation, and weak-to-strong prompting settings posed in Section 3.2.\n\n\n<table><tr><td>Deducer\nGenerator</td><td>GPT-4o</td><td>Llama-3.1-405B-Instruct</td><td>Aya-23-35B</td></tr><tr><td>GPT-4o</td><td>66.91%</td><td>71.69%</td><td>21.32%</td></tr><tr><td>Llama-3.1-405B-Instruct</td><td>67.28%</td><td>67.65%</td><td>20.22%</td></tr><tr><td>Aya-23-35B</td><td>65.44%</td><td>71.32%</td><td>15.44%</td></tr></table>\n\n\nand applies them), GPT-4o improves  $7.2\\%$  ( $59.19\\% \\rightarrow 66.91\\%$ ), and Llama-3.1-405B-Instruct improves  $1.8\\%$  ( $65.81\\% \\rightarrow 67.65\\%$ ). We subsequently observe even stronger gains for both models as the deducer, when selecting different models as the analogical exemplar generator. In the first stage, both of these frontier models correctly identify the language family at a fairly high rate (see Appendix H), select a few languages from said family, and generate analogical puzzles for those auxiliary languages, as intended. Then, in the second stage, the model considers the tokens in the test phrase, and analyzes how each is to be translated to the target language, and combines them together in the appropriate order following the syntactical patterns observed from the given exemplars. Thus, it appears that the model uses the analogical exemplars to better induce the mappings of words in the target language to words in the source language, which it then applies to the target phrase.\n\n\nWeak analogical \"supervision\" improves performance. We find that generating the analogical exemplars with Aya-35B and applying them to the test sample with Llama-3.1-405B-Instruct yields  $71.32\\%$ , averaged over 3 runs; a  $5.5\\%$  improvement over the best baseline for Llama-3.1-405B-Instruct  $(65.81\\% \\rightarrow 71.32\\%)$ . We similarly find that leveraging Aya-35B-generated exemplars and applying them with GPT-4o yields a  $6.2\\%$  improvement over the best GPT-4o baseline setting  $(59.19\\% \\rightarrow 65.44\\%)$ . In the case of Llama-3.1-405B-Instruct, using Aya-generated exemplars outperforms using self-generated exemplars, by  $3.7\\%$ $(67.65\\% \\rightarrow 71.32\\%)$ . Our findings suggest that when equipped with the right tools (analogical demonstrations) from effective multilingual reasoners, strong deducers can thrive.\n\n\nThis claim is further reinforced by the inference-time distillation results: smaller models such as Aya-35B do not benefit from the analogical exemplars, regardless of the analogical generator. At the same time, using the GPT-4o exemplars applied by Llama-3.1-405B-Instruct yields  $71.69\\%$ , our strongest result across all evaluation settings. Moreover, the reverse direction (Llama-3.1-405B-Instruct exemplars applied by GPT-4o) yields an  $8.09\\%$  improvement over the best GPT-4o baseline result. From these findings, we conclude that analogical exemplars generated by good multilingual reasoners do not \"unlock\" deductive reasoning abilities for models without them (Aya-35B); however, for strong baseline reasoners (Llama-3.1-405B, GPT-4o), better exemplars help performance. $^{3}$\n\n\nFrontier models understand language families. We compare model performance with and without oracle language families, finding that prompting models to infer the language family is superior. We observe that frontier models such as GPT-4o and Llama-3.1-405B, as well as specialized multilingual models like Aya-35B, have a strong parametric knowledge of language families, and do not need to rely on language family labels to identify similar languages. Furthermore, the model performing retrieval of the language family helps it to identify a few languages within the family that will help it, bootstrapping from the provided exemplars, whereas providing the language family often leads to the model listing many languages in the family and attempting to produce exemplars for all of them. We hypothesize that this is a source of noise; demonstrations beyond a certain number yield diminishing returns in performance. That is, the oracle language families setting stimulates inductive cross-lingual reasoning, but makes deductive reasoning more challenging due to having\n\n\n![(a) Best baseline vs. best analogical](images/48245bc3b4d3c14218c341f144d4ec1edb2d635b531c78a09df87e2dbe38caf8.jpg)\n\n\n![(b) Inferred families vs. oracle families Figure 2: Figure (a) contains a comparison of the best baseline (in Table 1) with the best 2-stage analogical reasoning result (in Table 2), for our two frontier models as the deducer. We find analogical to improve GPT-4o by  $8.1\\%$  and Llama-3.1-405B-Instruct by  $5.9\\%$ . Figure (b) compares self-generated analogical reasoning methods, with prompt-determined language families (\"inferred families\") and human-annotated language family labels (\"oracle families\").](images/40673e39e356c374031350dd339d76203c7ff32b0726b4f620fc6b512f46cf29.jpg)\n\n\nmany exemplars. Specific examples of this behavior are included in Appendix E, and the language families table is in Appendix G. We also include further discussion on the language families identified by GPT-4o and Llama-3.1-405B-Instruct in the inferred families setting, in Appendix H, and find that they achieve a high correctness rate relative to the oracle labels.\n\n\nLanguage Isolates and Proxy Languages. Language isolates would appear to pose particular difficulty to our models, as by definition, they do not belong to any well-defined language family. As a result, we rely solely on the models' ability to trace grammatical correspondences based on the languages it has seen in pre-training, even for our experiments with oracle language family labels. While in the baseline experiments, our models often believed that the target language is imaginary, prompting for language families leads models to note that the language is isolate. They then attempt to either follow syntactic or morphological patterns to induce a new fictitious language which is similar to the target, or select learned geographically-proximate languages. For the language of Bangime, spoken in Mali, the model either retrieves languages from families in the same geographical region, such as Dogon, or creates a new language (e.g. \"Xangime\") for which it generates analogical exemplars (see Appendix E). Analyzing at the instance level, this improves the correctness on the Bangime puzzles from  $27.8\\%$  to  $50\\%$  for GPT-4o in the self-generated setting.\n\n\nIn summary, our results suggest that the ability of the model to deduce by leveraging the given and analogically-generated exemplars is the key performance driver. This is lent credence by the efficacy of weak-to-strong prompting (i.e. relying on the exemplars of Aya-35b), while the performance of inference-time distillation remains roughly similar. Thus, we posit that the \"strength\" of a linguistic reasoning agent can be decomposed along two lenses, corresponding to our two stages: (1.) generating analogical exemplars by language identification and multilingual reasoning, and (2.) deducing from hypotheses in complex evaluation settings.\n\n\n# 5 RELATED WORK\n\n\n# 5.1 LARGE LANGUAGE MODEL REASONING.\n\n\nFew-shot Chain-of-Thought Reasoning. In-context learning has emerged as an exciting phenomenon in language models, enabling them to learn from few-shot demonstrations at inference-time to generalize to various tasks (Brown et al., 2020; Wei et al., 2022). At the same time, the chain-of-thought (CoT) reasoning method guides language models to think systematically through a problem, in a step-by-step manner (Wei et al., 2024; Kojima et al., 2022). In particular, applying chain-of-thought prompting (zero-shot or few-shot) with the goal to induce rationales yields explanations of why each step was performed, along with performance and faithfulness improvements (Nye et al., 2021; Lampinen et al., 2022). Various similar approaches (Yao et al., 2023; Wang et al., 2023; Besta et al., 2024) have been proposed to sample more diverse generations from models, leveraging test-time compute to improve reasoning performance; we believe such methods make for interesting lines of future research for the linguistic reasoning task.\n\n\nInductive Reasoning in LLMs. Inductive and deductive reasoning skills in language models have often been studied in the context of logical or abstract reasoning problems. Much of this prior work on inductive reasoning with language models studies evaluation settings with more clearly defined rules to be inductively learned and then applied; these works suggest gaps relative to the human intelligence in performing both inductive and deductive reasoning (Xu et al., 2024; Gendron et al., 2024; Yang et al., 2024c). In particular, Yang et al. (2024c) notes the need for more challenging tasks in inductive reasoning to better assess the boundaries of LM capabilities, such as hypothesis generation and pattern induction. Works such as Tang et al. (2023) demonstrate that models struggle to create rules by induction when the semantics of the exemplars do not follow in a commonsense manner \u2013 in our work, generating analogical exemplars similar to the models pre-training data may steer the model towards a relative \"commonsense\" representation of the rules underlying the exemplars.\n\n\nSeveral works dive into the realm of hypothesis search, determining the ability of LMs to pose hypotheses about the problem (e.g. rules which exemplars follow) before seeking to deductively apply them (Zhu et al., 2024; Qiu et al., 2024; Wang et al., 2024). Zhu et al. (2024) propose hypotheses-to-theories (HtT), which learns a rule library from an induction stage, and then applies it by a deduction stage; this multi-stage method is similar to our analogical approach, although we still perform both induction and deduction together after analogical generation. Furthermore, their rule library depends on verification \u2013 this is not possible in the linguistic reasoning task due to the lack of a reliable feedback source to judge responses, aside from expert humans. As discussed earlier in our work, Qiu et al. (2024) demonstrates that models can propose rules well, but cannot consistently apply them. Wang et al. (2024) proposes Hypothesis Search, a method which proposes hypotheses, implements a subset of them as Python programs, and applies them to training samples to verify their correctness.\n\n\nExemplar Generation and Automated Reasoning. Analogical prompting (Yasunaga et al., 2024) has been demonstrated to be an effective inference-time method to produce diverse, task-conditioned exemplars, improving in-context learning. As noted above, this effectively serves as a knowledge retrieval method which retrieves exemplars similar to (or directly from) the pre-training distribution which the model has seen; RECITE (Sun et al., 2023) similarly retrieves passages directly from the model's memory. Methods such as SG-ICL (Kim et al., 2022) and Auto-ICL (Yang et al., 2024b) also self-generate in-context exemplars in a similar manner as analogical prompting.\n\n\n# 5.2 MULTILINGUAL REASONING.\n\n\nMultilingual reasoning in LMs for low-resource languages poses a unique challenge, as the pretraining corpora and supervised fine-tuning datasets for many models are largely concentrated on a few high-resource languages. XLT (Huang et al., 2023) introduces a prompt template which translates problems in other languages to English and solves the problem with chain-of-thought in English. Qin et al. (2023) aligns each step in the chain-of-thought between the source language and English explanations, then solves the problem given this alignment; they also apply self-consistency with cross-lingual alignments with a set of pre-specified target languages. Li et al. (2024) trains on code data with multilingual comments, while using multilingual code prompts at inference time with symbolic function API calls as a structured way to solve the reasoning problem.\n\n\nLinguistic Reasoning Benchmarks. The PuzzLing Machines dataset (Sahin et al., 2020) first introduced a set of Linguistics Olympiad problems to study the ability of language models to learn from a small amount of data; they apply RoBERTa-based neural machine translation methods, but demonstrate a vast gap (attaining less than  $4\\%$  exact match performance). With concerns of potential leakage given the vast web scraping performed in procuring pre-training tokens for language model training, modeLing (Chi et al., 2024) introduced a new set of hand-written Linguistics Olympiad problems, demonstrating the performance of current models with CoT methods. The LINGOLY (Bean et al., 2024) dataset presents problems from the UK Linguistics Olympiad competition, and studies zero-shot and few-shot performance of current models categorized by question type.\n\n\n# 6 DISCUSSION\n\n\nWe propose applying analogical prompting as a test of inductive reasoning from diverse exemplars for challenging linguistic puzzles. Our results encouragingly suggest that despite frontier models struggling with deductive reasoning, in line with the findings of (Qiu et al., 2024), they can indeed\n\n\nfollow grammar rule similarities to generate analogical demonstrations and attempt to apply them adeptly. This yields improved performance in self-generated analogical prompting with GPT-4o and Llama-3.1-405B, as well as weak-to-strong prompting for those models employing Aya-35B-generated demonstrations. We also show that Llama-3.1-405B-Instruct is the best model for linguistic reasoning at present, becoming the first model to achieve over  $70\\%$  on the modeLing benchmark by way of our 2-stage analogical reasoning approach. This could be attributed to the vast scale of multilingual pre-training data (15T tokens), as well as multilingual supervised fine-tuning and dialogue data seen in adaptation. The multilingual prowess of the Llama-3.1 models is evidenced by its strong performance on the MGSM and multilingual MMLU benchmarks (Dubey et al., 2024).\n\n\nFurthermore, the ability of smaller and specialized multilingual models (Aya) to generate coherent analogical exemplars, which improve frontier models over their own self-generated exemplars, is promising towards developing widely-available multilingual reasoners. We find that the improvements observed can be attributed to the auxiliary exemplars generated, which are in turn due to the model's understanding of language families and grammar rules from the pre-training data or its multilingual adaptation. The errors made by current models due to an inability to apply diverse and complex exemplars suggest that the linguistic reasoning task is an exciting and challenging evaluation setting for LM reasoning at large. That is, seeking to emulate human reasoning, where deduction involves a clear application of recognized patterns, provides a ripe space for future work.\n\n\nThe interesting phenomenon examined with language isolates also provides a glimpse of model capabilities to follow grammatical similarities, rather than relying on knowledge retrieval of language families. That is, the multilingual language understanding abilities of frontier models expand beyond typological knowledge, going so far as to create proxy fictitious languages which enable it to solve the problem correctly. We suggest that future efforts in multilingual adaptation be placed in identifying techniques to guide languages models to support typologically unique languages.\n\n\nResearch at the intersection of machine translation and reasoning in the development of the latest foundation models is important from a societal perspective. With large language models being adopted widely, the need for multilingual capabilities and rapid adaptation grows, and our work proposes an effective method by which this can be performed at test-time. Notably, we have demonstrated evidence that models follow language similarities \u2013 given the massive number of languages and dialects present worldwide, this could help guide humans to learn dying languages, thus keeping their tradition alive, while doing so in a scalable manner. We hope that these findings can inspire future models releases to include evaluation on challenging multilingual tasks such as these puzzles, and research on reasoning can explore the multilingual setting further in depth.\n\n\n**Limitations** We note that the reliance on exact match scoring as our primary signal of performance is not ideal, as it is a binary indicator. We have sought to examine other metrics which correspond to \"partial credit\" such as ChrF2 and BLEU; however, there are flaws in these methods as well. A stronger human understanding of the rules which these extremely low-resource languages follow could guide us to better metrics, especially capturing semantic meaning and word ordering inversions, where appropriate. For instance, some languages might retain the same meaning while inverting the word order \u2013 exact match is sensitive to this, and while ChrF2 and BLEU are not, we should only be insensitive to ordering for languages which follow this property. We also recognized that the IOL 2024 problems could not be used as a benchmark with our method, as they require multimodality \u2013 our method only analyzes unimodal text problems. Another limitation of our work is that we do not have a reliable means of verifying the correctness of analogical exemplars, nor contrasting the quality of exemplars generated across models to determine the best analogical generator model. An expert annotator who could identify where a mistake was made in the model's reasoning process also would have been helpful to yield further insights into the fallacies of current models' linguistic reasoning. Nonetheless, our most effective deducer models are able to leverage exemplars generated by models of various sizes for improved linguistic reasoning.\n\n\nReproducibility Statement. We include all prompts used for generating our baseline experimental results, and all analogical prompting methods, in Appendix F. We have also broken down the two stages of our analogical reasoning method for clarity on how the method should be applied with two separator models (e.g. weak-to-strong prompting, inference-time exemplar distillation). We evaluate our work on the modeLing dataset, which is publicly available. We have included details of the platforms through which the models we evaluate have been queried (OpenAI API, TogetherAI API, Apple MLX), along with the list of models studied. Lastly, we will release our code publicly.\n\n\n# REFERENCES\n\n- Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet \u00dcst\u00fcn, and Sara Hooker. Aya 23: Open weight releases to further multilingual progress, 2024. URL https://arxiv.org/abs/2405.15032.\n\n- Andrew M. Bean, Simi Hellsten, Harry Mayne, Jabez Magomere, Ethan A. Chi, Ryan Chi, Scott A. Hale, and Hannah Rose Kirk. Lingoly: A benchmark of olympiad-level linguistic reasoning puzzles in low-resource and extinct languages, 2024. URL https://arxiv.org/abs/2406.06196.\n\n- Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefer. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17682-17690, March 2024. ISSN 2159-5399. doi: 10.1609/aaai.v38i16.29720. URL http://dx.doi.org/10.1609/aaaai.v38i16.29720.\n\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.\n\n- Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Diana McCarthy and Shuly Wintner (eds.), 11th Conference of the European Chapter of the Association for Computational Linguistics, pp. 249-256, Trento, Italy, April 2006. Association for Computational Linguistics. URL https://aclanthology.org/E06-1032.\n\n- Nathan Chi, Teodor Malchev, Riley Kong, Ryan Chi, Lucas Huang, Ethan Chi, R. McCoy, and Dragomir Radev. ModeLing: A novel dataset for testing linguistic reasoning in language models. In Michael Hahn, Alexey Sorokin, Ritesh Kumar, Andreas Shcherbakov, Yulia Otmakhova, Jinrui Yang, Oleg Serikov, Priya Rani, Edoardo M. Ponti, Saliha Murado\u011flu, Rena Gao, Ryan Coterell, and Ekaterina Vylomova (eds.), Proceedings of the 6th Workshop on Research in Computational Linguistic Typology and Multilingual NLP, pp. 113-119, St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.sigtyp-1.14.\n\n- Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\n- Gael Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. Large language models are not strong abstract reasoners, 2024. URL https://arxiv.org/abs/2305.19555.\n\n- Haoyang Huang, Tianyi Tang, Dongdong Zhang, Xin Zhao, Ting Song, Yan Xia, and Furu Wei. Not all languages are created equal in LLMs: Improving multilingual capability by cross-lingual-thought prompting. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 12365\u201312394, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023-findings-emnlp.826. URL https://aclanthology.org/2023-findings-emnlp.826.\n\n- Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of experts, 2024. URL https://arxiv.org/abs/2401.04088.\n\n- Daniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, New York, 2011. ISBN 9780374275631 0374275637. URL https://www.amazon.de/ Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=wl_it_dp_o_pdT1_nS_nC?ie  $\\equiv$  UTF8&colid  $=$  151193SNGKJT9&coliid  $=$  I3OCESLZCVDFL7.\n\n- Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang goo Lee. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator, 2022. URL https://arxiv.org/abs/2206.08082.\n\n- Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 22199-22213. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf.\n\n- Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from explanations in context? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 537-563, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022-findings-emnlp.38. URL https://aclanthology.org/2022-findings-emnlp.38.\n\n- Bryan Li, Tamer Alkhouli, Daniele Bonadiman, Nikolaos Pappas, and Saab Mansour. Eliciting better multilingual structured reasoning from LLMs through code. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5154\u20135169, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.281. URL https://aclanthology.org/2024.acl-long.281.\n\n- Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios Anastasopoulos, Patrick Littell, and Graham Neubig. Choosing transfer languages for cross-lingual learning. In Anna Korhonen, David Traum, and Lluis M\u00e1rquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3125\u20133135, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1301. URL https://aclanthology.org/P19-1301.\n\n- Toan Q. Nguyen and David Chiang. Transfer learning across low-resource, related languages for neural machine translation. In Greg Kondrak and Taro Watanabe (eds.), Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 296-301, Taipei, Taiwan, November 2017. Asian Federation of Natural Language Processing. URL https://aclanthology.org/I17-2050.\n\n- Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114.\n\n- Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin (eds.), Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.\n\n- Maja Popovic. chrF: character n-gram F-score for automatic MT evaluation. In Ondrej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina (eds.), Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 392\u2013395, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https://aclanthology.org/W15-3049.\n\n- Matt Post. A call for clarity in reporting BLEU scores. In Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9veol, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186-191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https://aclanthology.org/W18-6319.\n\n- Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 2695-2709, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.163. URL https://aclanthology.org/2023.emnlp-main.163.\n\n- Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=bNt7oajl2a.\n\n- G\u00f6zde G\u00fcl \u015eahin, Yova Kementchedjhieva, Phillip Rust, and Iryna Gurevych. PuzzLing Machines: A Challenge on Learning From Small Data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 1241-1254, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.115. URL https://aclanthology.org/2020.acl-main.115.\n\n- Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=-cqvvvb-NkI.\n\n- Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. Large language models are in-context semantic reasoners rather than symbolic reasoners, 2023. URL https://arxiv.org/abs/2305.14825.\n\n- Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman. Hypothesis search: Inductive reasoning with language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=G7UtIGQmjm.\n\n- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. URL https://arxiv.org/abs/2203.11171.\n\n- Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. URL https://arxiv.org/abs/2206.07682.\n\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088.\n\n- Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. Are large language models really good logical reasoners? a comprehensive evaluation and beyond, 2024. URL https://arxiv.org/abs/2306.09841.\n\n- Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers, 2024a. URL https://arxiv.org/abs/2309.03409.\n\n- Jinghan Yang, Shuming Ma, and Furu Wei. Auto-icl: In-context learning without human supervision, 2024b. URL https://arxiv.org/abs/2311.09263.\n\n- Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. Language models as inductive reasoners. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 209-225, St. Julian's, Malta, March 2024c. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-long.13.\n\n- Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 11809-11822. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf.\n\n- Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. Large language models as analogical reasoners. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=AgDICX1h50.\n\n- Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. Large language models can learn rules, 2024. URL https://arxiv.org/abs/2310.07064.\n\n- Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning for low-resource neural machine translation, 2016. URL https://arxiv.org/abs/1604.02201.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636559.4384828", "msecs": "438.0", "relativeCreated": "20131.62326812744", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability\n\n# Abstract\nPrecipitation nowcasting, a short-term (up to six hours) rain prediction, is arguably one of the most demanding weather forecasting tasks. To achieve accurate predictions, a forecasting model should consider miscellaneous meteorological and geographical data sources. Currently available datasets provide information only about precipitation intensity, vertically integrated liquid (VIL), or maximum reflectivity on the vertical section. Such single-level or aggregated data lacks description of the reflectivity change in vertical dimension, simplifying or distorting the corresponding models. To fill this gap, we introduce an additional dimension of the precipitation measurements in the RuDar dataset that incorporates 3D radar echo observations. Measurements are collected from 30 weather radars located mostly in the European part of Russia, covering multiple climate zones. Radar product updates every 10 minutes with a 2 km spatial resolution. The measurements include precipitation intensity (mm/h) at an altitude of 600 m, reflectivity (dBZ) and radial velocity (m/s) at 10 altitude levels from 1 km to 10 km with 1 km step. We also add the orography information as it affects the intensity and distribution of precipitation. The dataset includes over 50 000 timestamps over a two-year period from 2019 to 2021, totalling in roughly 100 GB of data. We evaluate several baselines, including optical flow and neural network models, for precipitation nowcasting on the proposed data. We also evaluate the uncertainty quantification for the ensemble scenario and show that the corresponding estimates do correlate with the ensemble errors on different sections of data. We believe that RuDar dataset will become a reliable benchmark for precipitation nowcasting models and also will be used in other machine learning tasks, e.g., in data shift studying, anomaly detection, or uncertainty estimation. Both dataset and code for data processing and model preparation are publicly available.\n\n\n# 1 INTRODUCTION\n\n\nPrecipitation nowcasting is the task of forecasting a rainfall situation (precipitation location and strength) for a short period of time, usually up to six hours. Due to climate change the frequency and magnitude of extreme weather events, e.g. sudden downpours, increase, and the techniques for forecasting such events are needed. Precipitation nowcasting can provide information about such events with a high spatiotemporal resolution. Such kind of weather forecasting plays an essential role in resource planning in the agricultural industry, aviation, sailing, etc. as well as in daily life.\n\n\nIncorrect precipitation forecasting could have a negative impact on human life activity, and data with diverse meteorological and geographical characteristics are needed for improving precipitation nowcasting models. The different benchmark dataset usage could improve the quality of precipitation nowcasting models to minimize the risk of forecasting error.\n\n\nPreviously published benchmarks Holleman (2007); Shi et al. (2017); Ansari et al. (2018); Ramsauer et al. (2018); Franch et al. (2020); Veillette et al. (2020) provide data collected with one or several\n\n\n![Figure 1: The geographical area covered by the proposed weather radar dataset (light areas). The covered area has a variety of geographic and climatic characteristics. The color indicates the height above sea level.](images/6b7a11ab3bcee6fb552fe032af459bc0a020f72b692afc4bead480af66c7a554.jpg)\n\n\n![](images/d32d32fc7354fd0c0181ce79e635e681ff299e6043a1604598286919cdb9bef2.jpg)\n\n\nweather radars. Some of those datasets only contain information about precipitation intensity, others provide vertically integrated liquid value (VIL) or maximum reflectivity on the vertical section.\n\n\nHowever, a single measurement type is often not enough for extreme weather events forecasting. Thereby, we propose a RuDar dataset that contains several measurement products: reflectivity (dBZ) and radial velocity  $(\\mathrm{m / s})$  on ten altitude levels from  $1\\mathrm{km}$  to  $10\\mathrm{km}$  with  $1\\mathrm{km}$  step and intensity  $(\\mathrm{mm / h})$  on a  $600\\mathrm{m}$  altitude level. Each measurement was carried out with a  $2\\mathrm{km}$  spatial resolution and a 10 minute temporal resolution. The dataset from 30 dual-pol Doppler weather radars were collected and processed at the Radar Center of the Central Aerological Observatory (CAO) of the Russian Federal Service for Hydrometeorology and Environmental Monitoring (ROSGIDROMET) and is used by our team within the conditions of commercial contract. For each radar, we additionally provide information about the surrounding orography Becker et al. (2009). The radars are located mostly in the European part of Russia as shown in Figure 1, therefore, a wide range of geographical and climatic conditions is considered. The proposed dataset includes more than 50 000 timestamps over a two year period from 2019 to 2021, allowing to investigate the effect of seasonality on rainfall forecast.\n\n\nWe illustrate the applicability of our dataset to the nowcasting task by benchmarking the current state-of-the-art optical flow approach Ayzel et al. (2019) and neural network models Shi et al. (2015); Veillette et al. (2020); Ravuri et al. (2021) on it. Experiments show a seasonal dependence having effect on the algorithm performance due to different precipitation intensity rates and differences between adjacent timestamps in different months.\n\n\nThe main paper contributions are (i) published weather radar dataset with different geographical and climatic conditions (provided under the CC BY NC SA 4.0 license) together with the accompanying exploratory data analysis, (ii) evaluations of common simple precipitation nowcasting models and its extension to support additional data, (iii) uncertainty estimation and its connection to the error for the nowcasting ensemble case, and (iv) accompanying source code for data processing and experiments.\n\n\nThe structure of the paper is as follows: Section 2 covers previously published datasets for the precipitation nowcasting task, Section 3 describes the proposed dataset, Section 4 introduces evaluated nowcasting benchmarks, Section 5 explores the uncertainty estimation scenario for the ensemble of models, and Section 6 concludes the paper.\n\n\n# 2 RELATED WORK\n\n\nDoppler weather radar is the most effective tool for detecting precipitation. The radar measures reflectivity of radio waves from precipitation drops, which can then be converted into precipitation intensity using Z-R relation Marshall & Palmer (1948). Standard ways of obtaining a single reflectivity measure from the different heights is either taking measurements from only the lower level (base reflectivity), or aggregating these measurements by the maximum value (composite reflectivity). In addition, a Doppler radar can detect movement towards or away from itself, which allows measuring the speed of precipitation movement along or against the direction of the radar. The latter type of measurement is called radial velocity.\n\n\nIn the public domain, one can find quite a variety of weather radar datasets collected and maintained by international agencies. We summarized the information about some of them in Table 1, where we\n\n\nprovided a comparison by spatial, temporal and pixel resolution, time periods, geographic coverage and number of radars from which measurements were taken.\n\n\nThe first three data sources in Table 1 are more large databases than ready-made ML benchmarks:\n\n- - NEXRAD Ansari et al. (2018) by US National Oceanic and Atmospheric Service (NOAA), which has been collected since 1994 and contains reflectivity data, radial velocity, and derivative products.\n\n- - MRMS Zhang et al. (2016) by NOAA National Severe Storms Laboratory combines the previous source together with various sources to provide severe weather, transportation, and precipitation products with 2-minute updates. This dataset is used in a number of nowcasting studies S\u00f8nderby et al. (2020), Klocek et al. (2021).\n\n- - Radarnet Fairman et al. (2017) by UK Met Office, which has been collected since 1970 and contains composite precipitation data derived from reflectivity data.\n\n\nThe ready-to-go datasets, suitable for exploration by ML practitioners and researchers include:\n\n- - HKO-7 Shi et al. (2017) by Hong Kong Observatory (HKO) contains a single-height reflectivity level from a single radar at the center of Hong Kong for a six years period.\n\n- - KNMI Overeem & Imhoff (2020) by the Royal Netherlands Meteorological Institute (KNMI) contains ten-year single-height composite reflectivity data from two radars.\n\n- - TAASRAD19 Franch et al. (2020) by Meteotrentino contains nine-year aggregated reflectivity data from a single radar located in the Italian Alps. It is interesting due to geographical specifics and a large amount of extreme phenomena, such as snowstorms, hails, downpours, etc.\n\n- - SEVIR Veillette et al. (2020) by NOAA and the Geostationary Environmental Satellite System (GOES) contains US radar and satellite data for a two year-period, sampled either randomly or on an event basis. It is focused on the detection of storm events.\n\n- - RADOLAN Ramsauer et al. (2018) by the German Weather Service contains three-year reflectivity and precipitation data collected by 18 radars in Germany.\n\n\nOur dataset was collected by 30 radars of the Central Aerological Observatory (CAO) and contains two years of observations (spring 2019 - spring 2021). Measurements were carried out mainly in the European part of Russia, but some areas of the Siberian and Far Eastern regions were also captured.\n\n\nThe main advantages of our dataset relative to the abovementioned are as follows:\n\n- 1. A large area is covered with different climatic conditions - from the extreme north (Arkhangelsk,  $64.62^{\\circ}N$ $40.51^{\\circ}E$ ) to the southern regions (Krasnodar,  $45.04^{\\circ}N$ $39.15^{\\circ}E$ ).\n\n- 2. Dataset introduces continuous measurements over a year period, which allows taking into account the influence of seasonal dependence on strength and distribution of precipitation.\n\n- 3. The data presents several radar products at once: reflectivity and radial velocity at 10 altitude levels  $(1 - 10\\mathrm{km})$  and precipitation  $(600\\mathrm{m})$ . We do not specifically aggregate the reflectivity and radial velocity data, as we believe that it can be useful for forecasting to know how moisture and its velocity are distributed depending on the height. There are situations when precipitation has not yet appeared at low levels, but at the same time, at high levels, there is information about future precipitation (see an example in Figure 6 in Supp.Mat.).\n\n- 4. In addition to the radar data itself, we provide information about the orography in the area around each radar. These data allow one to investigate the influence of the surrounding orography in predicting precipitation.\n\n\n# 3 DATASET DESCRIPTION AND PROCESSING\n\n\nThe proposed RuDar dataset contains measurements from 30 weather radars mainly located in the European part of Russia (Figure 1). Radars scan the area around with a radius of 250 kilometers.\n\n\nTable 1: Comparative table of various international open sources of radar data. The data were compared by spatio-temporal (km per pixel / minutes) and pixel resolution, as well as by time periods, geographical coverage and the number of radars involved. The main products contained in the datasets are base and composite reflectivity, as well as derived products: precipitation, maximum reflectivity and vertically integrated liquid (VIL). If there is more than one radar, all except NEXRAD and RuDar combine measurements from different radars into one frame.\n\n\n(*) For the 2020.03\u20132021.02 period, we publish every fifth day to be used for validation and testing.  \n(**) Feature codes: (1) precipitation rate, (2) radial velocity, (3) reflectivity by altitude layers, (4) integrated or one layer reflectivity, (5) satellites\n\n\n<table><tr><td>Dataset</td><td>Time periods</td><td>Spatio-temporal resolution</td><td>Pixel resolution per frame</td><td>Geography</td><td>No. of radars</td><td>Features</td><td>Ready to use</td></tr><tr><td>NEXRAD Ansari et al. (2018)</td><td>1994\u2013</td><td>1 km / 5 min</td><td>460\u00d7460</td><td>United States</td><td>160</td><td>1, 2, 3, 4</td><td>-</td></tr><tr><td>MRMS Zhang et al. (2016)</td><td>2014\u2013</td><td>1 km / 2 min</td><td>3500\u00d77000</td><td>United States</td><td>160</td><td>1, 3, 4</td><td>-</td></tr><tr><td>Radarnet Fairman et al. (2017)</td><td>1970\u2013</td><td>1 km / 5 min</td><td>1536\u00d71280</td><td>United Kingdom</td><td>15</td><td>1, 3</td><td>-</td></tr><tr><td>HKO-7 Shi et al. (2017)</td><td>2009\u20132015</td><td>1.06 km / 6 min</td><td>480\u00d7480</td><td>Hong-Kong</td><td>1</td><td>4</td><td>+</td></tr><tr><td>KNMI Overeem &amp; Imhoff (2020)</td><td>2008\u20132018</td><td>1 km / 5 min</td><td>400\u00d7400</td><td>Netherlands</td><td>2</td><td>4</td><td>+</td></tr><tr><td>TAASRAD19 Franch et al. (2020)</td><td>2010\u20132019</td><td>0.5 km / 5 min</td><td>480\u00d7480</td><td>Italian Alps</td><td>1</td><td>4</td><td>+</td></tr><tr><td>SEVIR Veillette et al. (2020)</td><td>2017\u20132019</td><td>1 km / 5 min</td><td>384\u00d7384</td><td>United States</td><td>160</td><td>4, 5</td><td>+</td></tr><tr><td>RADOLAN Ramsauer et al. (2018)</td><td>2014.12\u20132017.11</td><td>1 km / 5 min</td><td>900\u00d7900</td><td>Germany</td><td>18</td><td>1, 4</td><td>+</td></tr><tr><td>RuDar (Ours)</td><td>2019.03\u20132021.02*</td><td>2 km / 10 min</td><td>252\u00d7252</td><td>Russia</td><td>30</td><td>1, 2, 3</td><td>+</td></tr></table>\n\n\nMeasurements come from radars every 10 minutes and have a spatial resolution of  $2 \\times 2$  kilometers. The dataset covers two years from 2019 to 2021 and has over 50 000 unique timestamps.\n\n\nEach data sample is a three-dimensional tensor that contains the result of a 10 minute scan of the atmosphere with a single radar. The center of the frame corresponds to the location of the radar in the scanned area. Tensors have 21 channels with a spatial resolution of  $252 \\times 252$  pixels. The first channel contains information about precipitation intensity (mm/h) on a  $600\\mathrm{m}$  altitude level. The channels 2-11 contain reflectivity measurements (dBZ) performed at 10 altitude levels from  $1\\mathrm{km}$  to  $10\\mathrm{km}$  with  $1\\mathrm{km}$  step, and the channels 12-21 contain 10 radial velocity (m/s) measurements from the same  $1 - 10\\mathrm{km}$  altitudes. We also provide data on orography Becker et al. (2009) and latitude-longitude coordinates of the territory surrounding the area of the radar measurements. Coordinates were initially set on a kilometer grid with a step of 2 kilometers with zero at the radar location.\n\n\nThe data example is partially shown in Figure 2a. Please refer to the Section D in Supplementary for the comprehensive documentation<sup>2</sup>.\n\n\n# 3.1 PRECIPIATION INTENSITY\n\n\nPrecipitation intensity is measured at an altitude of 600 meters in mm/h and is represented by a single channel in the data. It is not a direct measurement: the precipitation rate is calculated from reflectivity values with Marshal-Palmer type of Z-R relationship Marshall & Palmer (1948). In addition to mm/h values, two special values are presented in data:  $-2\\in 6$  value marks areas where measurements are not available,  $-1\\in 6$  value marks areas where no precipitation events were detected. Both values can be reduced to  $0\\mathrm{mm / h}$ , however, when feeding a radar frame to the model, it is better to mask  $-2\\in 6$  values so that the model can distinguish the absence of precipitation from the blind zone of the radar.\n\n\nFigure 2d shows the distribution of precipitation by seasons. The data for each season is taken from the entire period 2019-2021. As can be seen in Figure 2d, the intensity has a seasonal dependence: the highest values of precipitation amount are reached in summer and the lowest in winter.\n\n\nWe investigated the distribution of precipitation intensity difference between two adjacent radar images. As Figure 2e shows, in the winter season the precipitation rate difference between two adjacent radar images is much lower than in the summer season. This means that in winter precipitation changes much less in the short-term interval than in summer, and therefore it is more difficult to solve the problem of nowcasting in warmer seasons.\n\n\nPeaks which are visible in Figure 2e are the result of specificity of the combination of radar-side filtration algorithms and Marshall-Palmer type of Z-R relationship.\n\n\n![(a)](images/9b019653c88aedfdbe7bacb10fa13d7823ae78b484546b301878fbd9ecf78d23.jpg)\n\n\n![](images/3486f0e198b84c80380ff99289f74bd14821c375782aafb4f6b5b213b73b4dfa.jpg)\n\n\n![(c)](images/5ebc53b64e0c891478909b3555467b551a789aef25589281138221548d2c26c9.jpg)\n\n\n![Figure 2: Featuring RuDar dataset. (a) The sky above the Moscow Region, Vnukovo Radar; axes ticks are given in kilometers. Ten levels of the precipitation intensity are shown (for each height level from  $1\\mathrm{km}$  to  $10\\mathrm{km}$ ), together with the orography (magnified for visualization purposes) on the bottom. The lined cuts on the left part of the plot demonstrate a specific case of urban obstruction in the radar vision: skyscrapers are not shown in the orography map yet they affect the scan result. (b) Tensor representation for a single radar at the given timestamp: each tensor has a shape of (252, 252, 21). Note that orography information is stored separately to avoid duplication. (c) Natural obstructions near Mineralnye Vody city: the mountain chain on the south blocks the radar, reducing the receptive field down to the area denoted by the white dotted line. (d) Seasonal dependence of the precipitation intensity distribution (mm/h per pixel). During the winter season events with high precipitation rates are much less represented than in the summer season. We clipped high precipitation rates ( $>140\\mathrm{mm/h}$ ). (e) The distribution of precipitation intensity difference between two adjacent radar images (mm/h per pixel) in RuDar dataset. In the winter season, precipitation events change slower than in the summer season. The periodical peaks are the result of a peculiarity of the combination of radar-side filtration algorithms and Marshall-Palmer type of Z-R relationship. Best viewed in color.](images/d2934a1d463d295dfe3e2859113d7f160bd1f4ab76986fddf40e17961fd467e8.jpg)\n\n\n![](images/9ea25e5b2e4e517987a3062f06cf5818c6a11e0965866b90b6b3925511bb3703.jpg)\n\n\n# 3.2 RADAR REFLECTIVITY\n\n\nReflectivity values represent direct radar measurements in dBZ units. The radar measures the amount of energy reflected from droplets distributed in the atmosphere, and this amount of energy turns out to be proportional to the amount of moisture in the air. All reflectivity measurements were carried out at heights of  $1 - 10\\mathrm{km}$  with  $1\\mathrm{km}$  step. We provide reflectivity data at several altitude levels, since we believe that not only information about precipitation changes in time, but also in space, is important for nowcasting, see Fig. 6 in Supp. Mat. for an example.\n\n\nTo demonstrate the geospatial diversity, we compare the mean reflectivity for two radars: Arkhangelsk (in the north) and Krasnodar (in the south) in Figure 3. It can be seen that both seasonal and altitudinal dependencies are different for these radars, which shows the wide range of weather conditions and data variability.\n\n\n# 3.3 RADIAL VELOCITY\n\n\nThe radial velocity is the wind velocity projected onto the ray starting in the radar. Negative values correspond to the movement towards the radar, and positive values correspond to movement away from the radar. Due to the peculiarities of this type of measurement, the values are available only at points where the radar has registered moisture droplets. The data is measured in m/s and provided\n\n\n![Figure 3: Comparison of two different regions: Krasnodar (on the south) and Arkhangelsk (on the north) in terms of mean reflectivity (left) and mean radial velocity (right) on different height levels. Only non-zero values are taken into account. Regions demonstrate different patterns in the altitude intensity and radial velocity distribution as well as the seasonal changes.](images/c2b85ab3f45c626550491af9d844518a1151ba94bb544d6d2517cd9a5379902d.jpg)\n\n\n![](images/6d8daf18a6f880a9e8c0b2e308697602233e380d75eca29dc8e115ce07296cb9.jpg)\n\n\nfor ten altitude levels from  $1\\mathrm{km}$  to  $10\\mathrm{km}$  with  $1\\mathrm{km}$  step similar to reflectivity data. According to Figure 3, radial velocity values are seasonally dependent.\n\n\n# 3.4 GEOGRAPHICAL INFORMATION\n\n\nIn addition to atmosphere scans, the RuDar dataset contains orography and geographical coordinates associated with each radar. Geographical coordinates (latitudes and longitudes) are provided for each point where measurements are presented. Orography is a two-dimensional tensor in which positive values correspond to elevations above sea level and negative values correspond to elevations below sea level. We resampled an available information Becker et al. (2009) to the used coordinate grid. An example of orography is shown in Figure 2c where the case of natural radar vision obstruction is demonstrated. The radar's visibility area in the figure is greatly reduced by high mountains located in the immediate vicinity of Mineralnye Vody. Note that the orography measurements do not include the urban areas, and Figure 2a shows that skyscrapers in Moscow may also pose a problem.\n\n\n# 4 NOWCASTING BASELINES\n\n\n# 4.1 PROBLEM STATEMENT\n\n\nThe nowcasting problem can be formulated in terms of sequence prediction task (precisely video prediction task) - the goal is to predict a sequence of future observations from an input sequence of historical measurements. More formally, let us have an input sequence of measurements  $X = (X_{t-M+1}, \\ldots, X_t)$  of length  $M$  for time step  $t$ , and a sequence of future measurements  $Y = (Y_{t+1}, \\ldots, Y_{t+K})$  of length  $K$ . Then our task is to construct a model  $f$  that predicts  $Y$  by  $X$ :\n\n\n$$\n\n$$\nY _ {t + 1}, Y _ {t + 2}, \\dots , Y _ {t + K} = f \\left(X _ {t - M + 1}, \\dots , X _ {t - 1}, X _ {t}\\right). \\tag {1}\n$$\n\n$$\n\n\nEach element  $X_{i}$  is a tensor of shape  $H \\times W \\times C_{input}$ , where  $H$  and  $W$  are the height and width of one frame, and  $C_{input}$  is the number of measurements used for prediction. An element  $Y_{i}$  of the output sequence is a tensor of shape  $H \\times W \\times C_{output}$ , where  $C_{output}$  is the number of predicted measurements. The numbers  $C_{input}$  and  $C_{output}$  may differ, since in the input sequence one can possibly use all the available measurements (intensity, reflectivity, radial velocity, orography), and the output sequence will contain only the target values. In our case, the target is future precipitation intensity, which implies  $C_{output} = 1$ .\n\n\nThe quality of the model can be measured by how well it predicts the strength of precipitation directly, as well as how well it copes with detecting precipitation. In the first case, we use pixelwise mean squared error (MSE) to measure quality, in the second we use  $F_{1}$ -score, see details of calculation in\n\n\nSection I of Supplementary Material. In meteorological articles Espeholt et al. (2021); Ravuri et al. (2021); Bouget et al. (2021), metrics such as Probability of Detection (POD), Success Ratio (SUCR), Critical Success Index (CSI) are sometimes used, but they are actually the equivalents of recall, precision and intersection over union (IOU), - standard metrics in machine learning. We chose  $F_{1}$  because it correlates with IOU and depends on both precision and recall.\n\n\n# 4.2 EXPERIMENTAL SETUP\n\n\n# 4.2.1 DATA SETUP\n\n\nWe are focusing on a year scale setup, which uses 2019 data for training, and every fifth day of 2020-2021 data for validation and testing.\n\n\nIn our experiments, we use 4 input tensors with historical observations to predict 12 future intensity frames ( $M = 4$ ,  $K = 12$ ,  $C_{output} = 1$ ). The spatial size of the input and output tensors is  $H = W = 252$ . We clip precipitation intensity values to  $50~\\mathrm{mm / h}$ , since high intensity rates are rare in the given geographical area. As we use an MSE loss to train our models, we use a binary mask for the special value  $-2 \\in 6$  to prevent penalizing our models for errors outside the radar visibility range.\n\n\nWhen only intensity is used as an input observation,  $C_{input} = 1$ . Using reflectivity levels increases  $C_{input}$  to 11. All values are clipped in the interval between  $-32$  dBZ and  $65$  dBZ, and then reduced to the interval from 0 to 1 by subtracting  $-32$  dBZ and dividing by  $65 - (-32) = 87$  dBZ. The intensity values for this setup are divided by  $50\\mathrm{mm / h}$  (for both input and target frames) to be also in the unit range (0, 1). Radial velocity also adds 10 input channels ( $C_{input} = 11$ ). All radial velocity values are clipped between  $-63\\mathrm{m / s}$  and  $63\\mathrm{m / s}$  and then divided by  $63\\mathrm{m / s}$  to be in the interval  $(-1, 1)$ . Orography adds one additional channel ( $C_{input} = 2$ ). Negative values of heights are reduced to zero, and positive values are divided by the maximum height equal to  $5336\\mathrm{m}$ .\n\n\nFor both radial velocity and orography, the intensity preprocessing is the same as in the absence of additional features.\n\n\n# 4.2.2 MODELS AND SETUPS\n\n\nWe represented  $f(\\cdot)$  with a persistent model as a weak baseline and with a state-of-the-art optical flow approach Ayzel et al. (2019) as a strong baseline. Also we trained UNet-like Veillette et al. (2020) and ConvLSTM-like Shi et al. (2015) neural network models (including Extended ConvLSTM architecture shown in Fig.4b) and tested a pretrained GAN-like model Ravuri et al. (2021). Each of the above models (excepting the pretrained GAN model) was trained on a single Tesla A100 80GB for approximately 70 hours. Details about model architectures are provided in Section E of Supplementary Materials.\n\n\nAlso we evaluated Earthformer Gao et al. (2022) on the proposed dataset using official implementation. Other strong neural network baselines S\u00f8nderby et al. (2020); Klocek et al. (2021); Espeholt et al. (2021) will be considered in future research as they require additional input data that is not presented in the proposed dataset. Satellite information is used in S\u00f8nderby et al. (2020), numerical weather forecast is used in Klocek et al. (2021), and Espeholt et al. (2021) requires both.\n\n\n# 4.3 RESULTS\n\n\nWe provide the results on  $F_{1}$  and  $MSE$  for each of the above models, as well as the results of Extended ConvLSTM using each of the additional features. We evaluate the baselines on seasonal test splits of the RuDar dataset. The results are shown in Table 2.\n\n\nAll baselines work better than Persistent, and among baselines without additional features, Extended ConvLSTM shows the best result in all seasons except summer: there it is inferior to Optical Flow in  $F_{1}$ .\n\n\nAdding reflectivity shows the best result for all metrics and seasons, except for  $F_{1}$  in summer (where it is inferior to Optical Flow) and in autumn (where it is inferior to Extended ConvLSTM with orography). Adding radial speed improves both  $MSE$  and  $F_{1}$  and adding orography improves  $F_{1}$  relative to Extended ConvLSTM without additional features.\n\n\nTable 2: Metrics of baseline models calculated separately for each season. The first value in the cell is  $MSE$ , the second value is  $F_{1}$ . Metrics are average across prediction horizons. Red highlights the maximum in the entire column, blue highlights the maximum among models without additional features.\n\n\n<table><tr><td></td><td>Spring 2020</td><td>Summer 2020</td><td>Autumn 2020</td><td>Winter 2020-2021</td></tr><tr><td>Persistent</td><td>0.2691 / 0.5177</td><td>0.6281 / 0.4975</td><td>0.0579 / 0.5233</td><td>0.0260 / 0.6425</td></tr><tr><td>Optical Flow</td><td>0.1936 / 0.6297</td><td>0.4768 / 0.6435</td><td>0.0384 / 0.6351</td><td>0.0190 / 0.6884</td></tr><tr><td>GAN</td><td>0.1885 / 0.6368</td><td>0.5820 / 0.6140</td><td>0.0352 / 0.6507</td><td>0.0218 / 0.6580</td></tr><tr><td>U-Net</td><td>0.1420 / 0.5649</td><td>0.3215 / 0.5406</td><td>0.0312 / 0.5730</td><td>0.0160 / 0.6232</td></tr><tr><td>ConvLSTM</td><td>0.1217 / 0.6222</td><td>0.3244 / 0.5580</td><td>0.0242 / 0.6650</td><td>0.0156 / 0.6799</td></tr><tr><td>Extended ConvLSTM</td><td>0.1192 / 0.6436</td><td>0.3193 / 0.5877</td><td>0.0233 / 0.6814</td><td>0.0150 / 0.6947</td></tr><tr><td>Extended ConvLSTM + reflectivity</td><td>0.1133 / 0.6597</td><td>0.3080 / 0.6170</td><td>0.0219 / 0.6936</td><td>0.0140 / 0.7014</td></tr><tr><td>Extended ConvLSTM + radial velocity</td><td>0.1170 / 0.6516</td><td>0.3153 / 0.5970</td><td>0.0228 / 0.6885</td><td>0.0144 / 0.7038</td></tr><tr><td>Extended ConvLSTM + orography</td><td>0.1193 / 0.6526</td><td>0.3197 / 0.5968</td><td>0.0234 / 0.6876</td><td>0.0148 / 0.7016</td></tr><tr><td>Earthformer</td><td>0.0124 / NA</td><td>0.0153 / NA</td><td>0.0047 / NA</td><td>0.0048 / NA</td></tr></table>\n\n\n![](images/9b975d14807bf789ca0c22282d655937c76750cb1835c552c8b7fba8f622c081.jpg)\n\n\n![](images/798e72c213a50109713fae9f8d01e9cbea2fe4c074f2769ad13e50e7f2280480.jpg)\n\n\n![Figure 4: (Left) Algorithm performance (left column:  $F_{1}$ , right column: MSE) as a function of a prediction horizon on the spring season of the test set. Top row: baseline algorithms, bottom row: Extended ConvLSTM with the various additional data. While optical flow may outperform other models on the initial time steps, NN-based and trained baselines easily outperform baselines. Usage of the radar reflectivity improves the performance of the ConvLSTM model. (Right) An extension of the architecture from Shi et al. (2015). The first ConvLSTM layer receives the output of a convolutional encoder as input, and the output of the last ConvLSTM layer is upsampled to the final prediction with a convolutional decoder. To output the next frame, the predictor uses the outputs of the last ConvLSTM layer from the previous step.](images/5a26ecde1fcf3943043f623642b88b80559d4db4f25fb822c4dfca64ef21b3a1.jpg)\n\n\n![](images/91639229c0a577590ed803505e8ed6a79303da312ba46cae8f54c98f30fc8f0c.jpg)\n\n\n![](images/5a55689205526b516be921d61af12155f78d142e91285d65c312219527c895e4.jpg)\n\n\nWe also show the relationship between algorithm performance and forecast horizon for the spring season in Figure 4a.\n\n\n# 5 UNCERTAINTY ESTIMATION FOR THE ENSEMBLE CASE\n\n\nWeather forecasting task, given its properties such as noisy data, probabilistic predictions, and general weather variability, may organically benefit from the uncertainty quantification. Uncertainty estimates (UEs) are helpful in a number of cases, including \"failing to predict\" (when the model understands that its prediction is not reliable enough, and gets the results from the basic robust model), active learning, data shift reaction, as well as the error analysis and model selection.\n\n\nWe focus on the last scenario and analyze the relationship between the error and UE for the ensemble case. Given an ensemble of five ConvLSTM models, trained on the same data but different seeds, we treat the mean and standard deviation of predictions as the output and UE, correspondingly. An ensemble error and UE on the test set are correlated with a large Spearman coefficient  $(>0.8)$ ; this relationship holds even when both metrics are averaged over the various dimensions: time, location, and forecast horizon, see Fig. 7, 8 in Section H of Supp.Mat. We also report that we did not find a relationship between the UE and orography (Pearson  $|r| < 0.1$ ).\n\n\nThis analysis suggests that UE metrics can be used to indicate large errors, which, in turn, can be used to deliver better predictions or uncertainty of the forecast, as well as assist the model selection and improvement.\n\n\n# 6 DISCUSSION\n\n\n# 6.1 SUMMARY\n\n\nIn this paper, we propose a weather radar dataset with a wide variety of geographical and climatic conditions and show in contrast to previously published works that a precipitation nowcasting task can be a seasonal dependent problem. This encourages the usage of the seasonal models and separate models for precipitation rate forecasting and precipitation events existence.\n\n\nWe also evaluated an ensemble of nowcasting models, analyzing the uncertainty estimates and showing its deep relation to the ensemble error on various scales.\n\n\nWe believe that our proposed dataset will become a reliable benchmark for precipitation nowcasting models. The code for data preprocessing and model preparation is publicly available  $^{3}$ . This dataset is provided under the CC BY NC SA 4.0 license. The data is maintained on the cloud service, and is maintained by authors team $^{4}$ .\n\n\n# 6.2 LIMITATIONS\n\n\nWe would like to point out a few limitations of our work:\n\n- - Dataset size. We included the two-year period only in order for the dataset to be comprehensible and processible by the community, as its unpacked size already exceeds 100 GB. This dataset is not intended for a study of long-term climatic changes.\n\n- - Data shift. Like any other piece of real-world data, our dataset may contain several peculiarities. In the text, we discuss the seasonal dependence and geographical shifts, however, it should be noted that the measurements within the sample are also not simultaneous as they were obtained by combining the measurements from the narrow beam that scans the atmosphere.\n\n- - Baselines. We provide the readers with simple baselines supporting the dataset. However, we plan to benchmark a number of state-of-the-art neural network-based baselines S\u00f8nderby et al. (2020); Espeholt et al. (2021); Klocek et al. (2021), in future studies.\n\n\n# 6.3 POSSIBLE PROSPECTS\n\n\nA two-year dataset may be applicable not for the nowcasting task only but in a number of contemporary ML problems, including:\n\n\nRare event detection. Various storms and rare weather conditions are of special interest both to the researchers and end-users. Some works even emphasize this area of research Veillette et al. (2020). We expect various anomaly detection algorithms to be of great use here.\n\n\nData shift and uncertainty estimation. The variety of both geographical and temporal conditions makes this dataset a good candidate for the modeling of distribution shift scenarios (see, e.g. Malinin et al. (2021)), where test data may naturally vary from the training data dramatically. In this case, various uncertainty estimation approaches, like in Gronquist et al. (2019) may be helpful.\n\n\nActive learning. One of the most important scenarios for the day-to-day forecasting systems is active learning since continuous data flow allows for the production model to learn from its own mistakes, correcting previous predictions. However, processing and retraining on huge amounts of data poses a challenge, and one may use smarter ways of data sampling (of both past archives and daily chunks of data) in order to reduce the data processing and model training times.\n\n\n# REFERENCES\n\n- Steve Ansari, Stephen Del Greco, Edward Kearns, Otis Brown, Scott Wilkins, Mohan Ramamurthy, Jeff Weber, Ryan May, Jed Sundwall, Jeff Layton, Ariel Gold, Adam Pasch, and Valliappa Lakshmanan. Unlocking the Potential of NEXRAD Data through NOAA's Big Data Partnership. Bulletin of the American Meteorological Society, 99(1):189 - 204, 2018. doi: 10.1175/BAMS-D-16-0021.1. URL https://journals.ametsoc.org/view/journals/bams/99/1/bams-d-16-0021.1.xml.\n\n- G. Ayzel, M. Heistermann, and T. Wintersmith. Optical flow models as an open benchmark for radar-based precipitation nowcasting (rainymotion v0.1). Geoscientific Model Development, 12 (4):1387-1402, 2019. doi: 10.5194/gmd-12-1387-2019. URL https://gmd.copernicus.org/articles/12/1387/2019/.\n\n- J. J. Becker, D. T. Sandwell, W. H. F. Smith, J. Braud, B. Binder, J. Depner, D. Fabre, J. Factor, S. Ingalls, S-H. Kim, R. Ladner, K. Marks, S. Nelson, A. Pharaoh, R. Trimmer, J. Von Rosenberg, G. Wallace, and P. Weatherall. Global bathymetry and elevation data at 30 arc seconds resolution: SRTM30_plus. Marine Geodesy, 32(4):355-371, 2009. doi: 10.1080/01490410903297766. URL https://doi.org/10.1080/01490410903297766.\n\n- Vincent Bouget, Dominique B\u00e9r\u00e9ziat, Julien Brajard, Anastase Charantonis, and Arthur Filoche. Fusion of rain radar images and wind forecasts in a deep learning model applied to rain nowcasting. Remote Sensing, 13(2), 2021. ISSN 2072-4292. doi: 10.3390/rs13020246. URL https://www.mdpi.com/2072-4292/13/2/246.\n\n- Lasse Espeholt, Shreya Agrawal, Casper S\u00f8nderby, Manoj Kumar, Jonathan Heek, Carla Bromberg, Cenk Gazen, Jason Hickey, Aaron Bell, and Nal Kalchbrenner. Skillful Twelve Hour Precipitation Forecasts using Large Context Neural Networks. arXiv preprint arXiv:2111.07470, 2021.\n\n- Jonathan G Fairman, David M Schultz, Daniel J Kirshbaum, Suzanne L Gray, and Andrew I Barrett. Climatology of Size, Shape, and Intensity of Precipitation Features over Great Britain and Ireland. Journal of Hydrometeorology, 18(6):1595-1615, 2017. doi: 10.1175/JHM-D-16-0222.1. URL https://journals.ametsoc.org/view/journals/hydr/18/6/jhm-d-16-0222{\\_}1.xml.\n\n- Gabriele Franch, Valerio Maggio, Luca Coviello, Marta Pendesini, Giuseppe Jurman, and Cesare Furlanello. TAASRAD19, a high-resolution weather radar reflectivity dataset for precipitation nowcasting. Scientific Data, 7(1):1-13, 2020.\n\n- Zhihan Gao, Xingjian Shi, Hao Wang, Yi Zhu, Bernie Wang, Mu Li, and Dit-Yan Yeung. Earthformer: Exploring space-time transformers for earth system forecasting. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=1zZstLVGVGW.\n\n- Peter Gronquist, Tal Ben-Nun, Nikoli Dryden, Peter Dueben, Luca Lavarini, Shigang Li, and Torsten Hoefler. Predicting weather uncertainty with deep convnets. arXiv preprint arXiv:1911.00630, 2019.\n\n- Iwan Holleman. Bias adjustment and long-term verification of radar-based precipitation estimates. Meteorological Applications, 14(2):195-203, 2007. doi: https://doi.org/10.1002/met.22. URL https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/met.22.\n\n- Sylwester Klocek, Haiyu Dong, Matthew Dixon, Panashe Kanengoni, Najeeb Kazmi, Pete Luferenko, Zhongjian Lv, Shikhar Sharma, Jonathan Weyn, and Siqi Xiang. MS-nowcasting: Operational Precipitation Nowcasting with Convolutional LSTMs at Microsoft Weather. arXiv preprint arXiv:2111.09954, 2021.\n\n- Andrey Malinin, Neil Band, German Chesnokov, Yarin Gal, Mark JF Gales, Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal Raina, et al. Shifts: A dataset of real distributional shift across multiple large-scale tasks. arXiv preprint arXiv:2107.07455, 2021.\n\n- J. S. Marshall and W. Mc K. Palmer. The distribution of raindrops with size. Journal of Atmospheric Sciences, 5(4):165 - 166, 1948. doi: 10.1175/1520-0469(1948)005<0165:TDORWS>2.0.CO;2. URL https://journals.ametsoc.org/view/journals/atsc/5/4/1520-0469_1948_005_0165_tdorws_2_0_co_2.xml.\n\n- Aart Overeem and Ruben Imhoff. *Archived 5-min rainfall accumulations from a radar dataset for the Netherlands*, Mar 2020. URL https://data.4tu.nl/articles/dataset/Archived_5-min_rainfall_accumulations_from_a_radar_dataset_for_the_Netherlands/12675278/1.\n\n- Thomas Ramsauer, Thomas Wei\u00df, and Philip Marzahn. Comparison of the GPM IMERG final precipitation product to RADOLAN weather radar data over the topographically and climatically diverse germany. Remote Sensing, 10(12), 2018. ISSN 2072-4292. doi: 10.3390/rs10122029. URL https://www.mdpi.com/2072-4292/10/12/2029.\n\n- Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Piotr Mirowski, Megan Fitzsimons, Maria Athanassiadou, Sheleem Kashem, Sam Madge, et al. Skilful precipitation nowcasting using deep generative models of radar. Nature, 597(7878):672-677, 2021.\n\n- Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. Springer, 2015.\n\n- Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun Woo. Convolutional LSTM network: a machine learning approach for precipitation nowcasting. In Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1, pp. 802-810, 2015.\n\n- Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun WOO. Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/a6db4ed04f1621a119799fd3d7545d3d-Paper.pdf.\n\n- Mennatullah Siam, Sepehr Valipour, Martin Jagersand, and Nilanjan Ray. Convolutional gated recurrent networks for video segmentation. In 2017 IEEE International Conference on Image Processing (ICIP), pp. 3090-3094, 2017. doi: 10.1109/ICIP.2017.8296851.\n\n- Casper Kaae S\u00f8nderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, and Nal Kalchbrenner. Metnet: A neural weather model for precipitation forecasting. arXiv preprint arXiv:2003.12140, 2020.\n\n- Mark Veillette, Siddharth Samsi, and Chris Mattioli. SEVIR: A storm event imagery dataset for deep learning applications in radar and satellite meteorology. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 22009-22019. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/fa78a16157fed00d7a80515818432169-Paper.pdf.\n\n- Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and Philip S Yu. PredRNN++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5123-5132. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/wang18b.html.\n\n- Jian Zhang, Kenneth Howard, Carrie Langston, Brian Kaney, Youcun Qi, Lin Tang, Heather Grams, Yadong Wang, Stephen Cocks, Steven Martinaitis, et al. Multi-Radar Multi-Sensor (MRMS) quantitative precipitation estimation: Initial operating capabilities. Bulletin of the American Meteorological Society, 97(4):621-638, 2016.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636561.23816", "msecs": "238.0", "relativeCreated": "21931.300401687622", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Test Time Learning for Time Series Forecasting\n\n# Abstract\nWe propose the use of Test-Time Training (TTT) modules in a cascade architecture to enhance performance in long-term time series forecasting. Through extensive experiments on standard benchmark datasets, we demonstrate that TTT modules consistently outperform state-of-the-art models, including Mamba-based TimeMachine, particularly in scenarios involving extended sequence and prediction lengths. Our results show significant improvements, especially on larger datasets such as Electricity, Traffic, and Weather, underscoring the effectiveness of TTT in capturing long-range dependencies. Additionally, we explore various convolutional architectures within the TTT framework, showing that convolutional blocks as hidden layer architectures can achieve competitive results.\n\n\n# 1 INTRODUCTION\n\n\nLong Time Series Forecasting (LTSF) is a crucial task in various fields, including energy Doe & Smith (2023), industry Doe & Smith (2024), defense Bakdash et al. (2017), and atmospheric sciences Lim & Zohren (2020). LTSF uses a historical sequence of observations, known as the look-back window, to predict future values through a learned or mathematically induced model. However, the stochastic nature of real-world events makes LTSF challenging. Deep learning models, including time series forecasting, have been widely adopted in engineering and scientific fields. Early approaches employed Recurrent Neural Networks (RNNs) to capture long-range dependencies in sequential data like time series. However, recurrent architectures like RNNs have limited memory retention, are difficult to parallelize, and have constrained expressive capacity. Transformers (Vaswani et al., 2017), with ability to efficiently process sequential data in parallel and capture contextual information, have significantly improved performance on time series prediction task (Wen et al., 2023; Liu et al., 2022b; Ni et al., 2024; Chen et al., 2024). Yet, due to the quadratic complexity of attention mechanisms with respect to the context window (or look-back window in LTSF), Transformers are limited in their ability to capture very long dependencies.\n\n\nIn recent years, State Space Models (SSMs) such as Mamba (Gu & Dao, 2024), a gated linear RNN variant, have revitalized the use of RNNs for LTSF. These models efficiently capture much longer dependencies while reducing computational costs and enhancing expressive power and memory retention. A new class of Linear RNNs, known as Test Time Training (TTT) modules (Sun et al., 2024), has emerged. These modules use expressive hidden states and provide theoretical guarantees for capturing long-range dependencies, positioning them as one of the most promising architectures for LTSF and due to their weight adaptation during test time are very effective on non-stationary data. We provide more motivation on TTT for non-stationary data in Appendix A.\n\n\n# KEY INSIGHTS AND RESULTS\n\n\nThrough our experiments, several key insights emerged regarding the performance of TTT modules when compared to existing SOTA models:\n\n- - Superior Performance with Longer Sequence and Prediction Lengths: TTT modules consistently outperformed the SOTA TimeMachine model, particularly as sequence and prediction lengths increased. Architectures such as Conv Stack 5 demonstrated their ability to capture long-range dependencies more effectively than Mamba-based models, resulting in noticeable improvements in Mean Squared Error (MSE) and Mean Absolute Error (MAE) across various benchmark datasets.\n\n- - **Strong Improvement on Larger Datasets:** On larger datasets, such as Electricity, Traffic, and Weather, the TTT-based models excelled, showing superior performance compared to both Transformer- and Mamba-based models. These results underscore the ability of TTT to handle larger temporal windows and more complex data, making it especially effective in high-dimensional, multivariate datasets.\n\n- - Hidden Layer Architectures: The ablation studies revealed that while convolutional architectures added to the TTT modules provided some improvements, Conv Stack 5 consistently delivered the best results among the convolutional variants. However, simpler architectures like Conv 3 often performed comparably, showing that increased architectural complexity did not always lead to significantly better performance. Very complex architectures like the modern convolutional block from Donghao & Xue (2024) showed competitive performance when used as TTT hidden layer architectures compared to the simpler single architectures proposed, hinting on the potential of more complex architectures in capturing more long term dependencies.\n\n- - Adaptability to Long-Term Predictions: The TTT-based models excelled in long-term prediction tasks, especially for really high prediction lengths like 2880. TTT-based models also excelled on increased sequence lengths as high as 5760 which is the maximum sequence length allowed by the benchmark datasets. This verified the theoretically expected superiority of TTT based models relative to the mamba/transformer based SOTA models.\n\n\n# MOTIVATION AND CONTRIBUTIONS\n\n\nIn this work, we explore the potential of TTT modules in Long-Term Series Forecasting (LTSF) by integrating them into novel model configurations to surpass the current state-of-the-art (SOTA) models. Our key contributions are as follows:\n\n- - We propose a new model architecture utilizing quadruple TTT modules, inspired by the TimeMachine model (Ahamed & Cheng, 2024), which currently holds SOTA performance. By replacing the Mamba modules with TTT modules, our model effectively captures longer dependencies and predicts larger sequences.\n\n- - We evaluate the model on benchmark datasets, exploring the original look-back window and prediction lengths to identify the limitations of the SOTA architecture. We demonstrate that the SOTA model achieves its performance primarily by constraining look-back windows and prediction lengths, thereby not fully leveraging the potential of LTSF.\n\n- - We extend our evaluations to significantly larger sequence and prediction lengths, showing that our TTT-based model consistently outperforms the SOTA model using Mamba modules, particularly in scenarios involving extended look-back windows and long-range predictions.\n\n- - We conduct an ablation study to assess the performance of various hidden layer architectures within our model. By testing six different convolutional configurations, one of which being ModernTCN by Donghao & Xue (2024), we quantify their impact on model performance and provide insights into how they compare with the SOTA model.\n\n\n# 2 RELATED WORK\n\n\nTransformers for LTSF Several Transformer-based models have advanced long-term time series forecasting (LTSF), with notable examples like iTransformer (Liu et al., 2024) and PatchTST\n\n\n(Nie et al., 2023). iTransformer employs multimodal interactive attention to capture both temporal and inter-modal dependencies, suitable for multivariate time series, though it incurs high computational costs when multimodal data interactions are minimal. PatchTST, inspired by Vision Transformers (Dosovitskiy et al., 2021), splits input sequences into patches to capture dependencies effectively, but its performance hinges on selecting the appropriate patch size and may reduce model interpretability. Other influential models include Informer (Zhou et al., 2021), which uses sparse self-attention to reduce complexity but may overlook finer details in multivariate data; Autoformer (Wu et al., 2022), which excels in periodic data but struggles with non-periodic patterns; Pyraformer (Liu et al., 2022b), which captures multi-scale dependencies through a hierarchical structure but at the cost of increased computational requirements; and Fedformer (Zhou et al., 2022), which combines time- and frequency-domain representations for efficiency but may underperform on noisy time series. While each model advances LTSF in unique ways, they also introduce specific trade-offs and limitations.\n\n\nState Space Models for LTSF S4 models (Gu et al., 2022a;b; Gupta et al., 2023) are efficient sequence models for long-term time series forecasting (LTSF), leveraging linear complexity through four key components:  $\\Delta$  (discretization step size),  $A$  (state update matrix),  $B$  (input matrix), and  $C$  (output matrix). They operate in linear recurrence for autoregressive inference and global convolution for parallel training, efficiently transforming recurrences into convolutions. However, S4 struggles with time-invariance issues, limiting selective memory. Mamba (Gu & Dao, 2024) addresses this by making  $B$ ,  $C$ , and  $\\Delta$  dynamic, creating adaptable parameters that improve noise filtering and maintain Transformer-level performance with linear complexity. SIMBA (Patro & Agneeswaran, 2024) enhances S4 by integrating block-sparse attention, blending state space and attention to efficiently capture long-range dependencies while reducing computational overhead, ideal for large-scale, noisy data. TimeMachine (Ahamed & Cheng, 2024) builds on these advances by employing a quadruple Mamba setup, managing both channel mixing and independence while avoiding Transformers' quadratic complexity through multi-scale context generation, thereby maintaining high performance in long-term forecasting tasks.\n\n\nLinear RNNs for LTSF RWKV-TS (Hou & Yu, 2024) is a novel linear RNN architecture designed for time series tasks, achieving O(L) time and memory complexity with improved long-range information capture, making it more efficient and scalable compared to traditional RNNs like LSTM and GRU. Orvieto et al. (2023) introduced the Linear Recurrent Unit (LRU), an RNN block matching the performance of S4 models on long-range reasoning tasks while maintaining computational efficiency. TTT (Sun et al., 2024) layers take a novel approach by treating the hidden state as a trainable model, learning during both training and test time with dynamically updated weights. This allows TTT to capture long-term relationships more effectively through real-time updates, providing an efficient, parallelizable alternative to self-attention with linear complexity. TTT's adaptability and efficiency make it a strong candidate for processing longer contexts, addressing the scalability challenges of RNNs and outperforming Transformer-based architectures in this regard.\n\n\nMLPs and CNNs for LTSF Recent advancements in long-term time series forecasting (LTSF) have introduced efficient architectures that avoid the complexity of attention mechanisms and recurrence. TSMixer (Chen et al., 2023b), an MLP-based model, achieves competitive performance by separating temporal and feature interactions through time- and channel-mixing, enabling linear scaling with sequence length. However, MLPs may struggle with long-range dependencies and require careful hyperparameter tuning, especially for smaller datasets. Convolutional neural networks (CNNs) have also proven effective for LTSF, particularly in capturing local temporal patterns. ModernTCN (Donghao & Xue, 2024) improves temporal convolution networks (TCNs) using dilated convolutions and a hierarchical structure to efficiently capture both short- and long-range dependencies, making it well-suited for multi-scale time series data.\n\n\nBuilding on these developments, we improve the original TimeMachine model by replacing its Mamba blocks with Test-Time Training (TTT) blocks to enhance long-context prediction capabilities. We also explore CNN configurations, such as convolutional stacks, to enrich local temporal feature extraction. This hybrid approach combines the efficiency of MLPs, the local pattern recognition of CNNs, and the global context modeling of TTT, leading to a more robust architecture for LTSF tasks that balances both short- and long-term forecasting needs.\n\n\n# 3 MODEL ARCHITECTURE\n\n\nThe task of Time Series Forecasting can be defined as follows: Given a multivariate time series dataset with a window of past observations (look-back window)  $L$ :  $(\\mathbf{x}_1, \\ldots, \\mathbf{x}_L)$ , where each  $\\mathbf{x}t$  is a vector of dimension  $M$  (the number of channels at time  $t$ ), the goal is to predict the next  $T$  future values  $(\\mathbf{x}_{L+1}, \\ldots, \\mathbf{x}_{L+T})$ .\n\n\nThe TimeMachine (Ahamed & Cheng, 2024) architecture, which we used as the backbone, is designed to capture long-term dependencies in multivariate time series data, offering linear scalability and a small memory footprint. It integrates four Mamba (Gu & Dao, 2024) modules as sequence modeling blocks to selectively memorize or forget historical data, and employs two levels of downsampling to generate contextual cues at both high and low resolutions.\n\n\nHowever, Mamba's approach still relies on fixed-size hidden states to compress historical information over time, often leading to the model forgetting earlier information in long sequences. TTT (Sun et al., 2024) uses dynamically updated weights (in the form of matrices inside linear or MLP layers) to compress and store historical data. This dynamic adjustment during test time allows TTT to better capture long-term relationships by continuously incorporating new information. Its Hidden State Updating Rule is defined as:\n\n\n$$\n\n$$\nW _ {t} = W _ {t - 1} - \\eta \\nabla \\ell (W _ {t - 1}; x _ {t}) = W _ {t - 1} - \\eta \\nabla \\| f (\\tilde {x} _ {t}; W) - x _ {t} \\| ^ {2}\n$$\n\n$$\n\n\nWe incorporated TTT into the TimeMachine model, replacing the original Mamba block. We evaluated our approach with various setups, including different backbones and TTT layer configurations. Additionally, we introduced convolutional layers before the sequence modeling block and conducted experiments with different context lengths and prediction lengths. We provide mathematical foundations as to why TTT is able to perform test-time adaptation without catastrophic forgetting and how the module adapts to distribution shifts in Appendix A. In the same Appendix we quantify the computational overhead introduced by test-time updates and provide empirical validation, published by the authors who proposed TTT in Sun et al. (2020), on how it performs on real corrupted data and provide some intuition on the parameter initialization in TTT as discussed in the same reference.\n\n\nOur goal is to improve upon the performance of the state-of-the-art (SOTA) models in LTSF using the latest advancements in sequential modeling. Specifically, we integrate Test-Time Training (TTT) modules into our model for two key reasons, TTT is theoretically proven to have an extremely long context window, being a form of linear RNN (Orvieto et al., 2023), capable of capturing long-range dependencies efficiently. Secondly, the expressive hidden states of TTT allow the model to capture a diverse set of features without being constrained by the architecture, including the depth of the hidden layers, their size, or the types of blocks used.\n\n\n# 3.1 GENERAL ARCHITECTURE\n\n\nOur model architecture builds upon the TimeMachine model (Ahamed & Cheng, 2024), introducing key modifications, as shown in Figure 1a, 1b and 1c. Specifically, we replace the Mamba modules in TimeMachine with TTT (Test-Time Training) modules (Sun et al., 2024), which retain compatibility since both are linear RNNs (Orvieto et al., 2023). However, TTT offers superior long-range dependency modeling due to its adaptive nature and theoretically infinite context window. A detailed visualization of the TTT block and the different proposed architectures can be found in Appendix D\n\n\nOur model features a two-level hierarchical architecture that captures both high-resolution and low-resolution context, as illustrated in Figure 1a. To adapt to the specific characteristics of the dataset, the architecture handles two scenarios\u2014Channel Mixing and Channel Independence\u2014illustrated in Figure 1b and 1c respectively. A more detailed and mathematical description of the normalization and prediction procedures can be found in Appendix D. We provide a computational complexity analysis of the TTT, Transformer, Mamba and ModernTCN modules in Appendix F and we also provide a computational complexity analysis for our model, TimeMachine, iTransformer, PatchTST, TSMixer and ModernTCN in Appendix G. We also included a mathematical comparison between the Mamba and TTT modules in Appendix B as well as theoretical comparison between the TTT module and models handling noise or temporal regularization in Appendix C.\n\n\n![(a) TimeMachine incorporating TTT-Blocks](images/2803ba6504911df49205efad04e246c5be49517aae973ab5a9064c1d7fc38fb5.jpg)\n\n\n![(b) Channel Mixing Mode](images/de31e3200758d072f5599ab7b1d2132912e301c346df5e7ab0078eb3c99d0836.jpg)\n\n\n![(c) Channel Independence Mode Figure 1: Our model architecture. (a) We replace the four Mamba Block in TimeMachine with four TTT(Test-Time Training) Block. (b) There are two modes of TimeMachine, the channel mixing mode for capturing strong between-channel correlations, and the channel independence mode for modeling within-channel dynamics. Recent works such as PatchTST (Nie et al., 2023) and TiDE (Das et al., 2024) have shown channel independence can achieve SOTA accuracy. For the channel independence scenario, the inputs are first transposed, and then we integrate two linear layers  $(1 \\times 16$  and  $16 \\times 1)$  to provide the TTT Block with a sufficiently large hidden size.](images/70339a4d854407c29e9bfa85c6eb9383adfea74fd276c2191e572a0700e76555.jpg)\n\n\n# 3.2 HIERARCHICAL EMBEDDING\n\n\nThe input sequence  $BML$  (Batch, Channel, Length) is first passed through Reversible Instance Normalization (Kim et al., 2021) (RevIN), which stabilizes the model by normalizing the input data and helps mitigate distribution shifts. This operation is essential for improving generalization across datasets.\n\n\nAfter normalization, the sequence passes through two linear embedding layers. Linear E1 and Linear E2 are used to map the input sequence into two embedding levels: higher resolution and lower resolution. The embedding operations  $E_1:\\mathbb{R}^{M\\times L}\\to \\mathbb{R}^{M\\times n_1}$  and  $E_{2}:\\mathbb{R}^{M\\times n_{1}}\\to \\mathbb{R}^{M\\times n_{2}}$  are achieved through MLP.  $n_1$  and  $n_2$  are configurations that take values from  $\\{512,256,128,64,32\\}$ , satisfying  $n_1 > n_2$ . Dropout layers are applied after each embedding layer to prevent overfitting, especially for long-term time series data. As shown in Figure 1a.\n\n\nWe provide more intuition on the effectiveness of hierarchical modeling in Appendix E.\n\n\n# 3.3 TWO LEVEL CONTEXTUAL CUE MODELING\n\n\nAt each of the two embedding levels, a contextual cues modeling block processes the output from the Dropout layer following E1 and E2. This hierarchical architecture captures both fine-grained and broad temporal patterns, leading to improved forecasting accuracy for long-term time series data.\n\n\nIn Level 1, High-Resolution Contextual Cues Modeling is responsible for modeling high-resolution contextual cues. TTT Block 3 and TTT Block 4 process the input tensor, focusing on capturing fine-grained temporal dependencies. The TTT Block3 operates directly on the input, and transposition may be applied before TTT Block4 if necessary. The outputs are summed, then concatenated with the Level 2 output. There is no residual connection summing in Level 1 modeling.\n\n\nIn Level 2, Low-Resolution Contextual Cues Modeling handles broader temporal patterns, functioning similarly to Level 1. TTT Block 1 and TTT Block 2 process the input tensor to capture low-resolution temporal cues and add them together. A linear projection layer (P-1) is then applied to maps the output (with dimension  $\\mathrm{RM}\\times \\mathrm{n}2$ ) to a higher dimension  $\\mathrm{RM}\\times \\mathrm{n}1$ , preparing it for concate\n\n\nnation. Additionally, the Level 1 and Level 2 Residual Connections ensure that information from previous layers is effectively preserved and passed on.\n\n\n# 3.4 FINAL PREDICTION\n\n\nAfter processing both high-resolution and low-resolution cues, the model concatenates the outputs from both levels. A final linear projection layer (P-2) is then applied to generate the output predictions. The output is subsequently passed through RevIN Denormalization, which reverses the initial normalization and maps the output back to its original scale for interpretability. For more detailed explanations and mathematical descriptions refer to Appendix D.\n\n\n# 3.5 CHANNEL MIXING AND INDEPENDENCE MODES\n\n\nThe Channel Mixing Mode (Figure 1a and 1b) processes all channels of a multivariate time series together, allowing the model to capture potential correlations between different channels and understand their interactions over longer time. Figure 1a illustrates an example of the channel mixing case, but there is also a channel independence case corresponding to Figure 1a, which we have not shown here. Figures 1b and 1c demonstrate the channel mixing and independence modes of the Level 1 High-Resolution Contextual Cues Modeling part with TTT Block 3 and TTT Block 4. Similar versions of the two channel modes for Level 2 Low-Resolution Contextual Cues Modeling are quite similar to those in Level 1, which we have also omitted here.\n\n\nThe Channel Independence Mode (Figure 1c) treats each channel of a multivariate time series as an independent sequence, enabling the model to analyze individual time series more accurately. This mode focuses on learning patterns within each channel without considering potential correlations between them.\n\n\nThe main difference between these two modes is that the Channel Independence Mode always uses transposition before and after one of the TTT blocks (in Figure 1c, it's TTT Block 4). This allows the block to capture contextual cues from local perspectives, while the other block focuses on modeling the global context. However, in the Channel Mixing Mode, both TTT Block 3 and TTT Block 4 model the global context.\n\n\nThe hidden size value for TTT Blocks in global context modeling is set to  $n_1$  since the input shape is  $BMn_1$  for Channel Mixing and  $(B \\times M)1n_1$  for Channel Independence. To make the TTT Block compatible with the local context modeling scenario\u2014where the input becomes  $(B \\times M)n_1 \\gets \\mathrm{Transpose}((B \\times M)1n_1)$  after transposition\u2014we add two linear layers: one for upsampling to  $(B \\times M)n_116$  and another for downsampling back. In this case, the hidden size of TTT Block 4 is set to 16.\n\n\n# 4 EXPERIMENTS AND EVALUATION\n\n\n# 4.1 ORIGINAL EXPERIMENTAL SETUP\n\n\nWe evaluate our model on seven benchmark datasets that are commonly used for LTSF, namely: Traffic, Weather, Electricity, ETTh1, ETTh2, ETTm1, and ETTm2 from Wu et al. (2022) and Zhou et al. (2021). Among these, the Traffic and Electricity datasets are significantly larger, with 862 and 321 channels, respectively, and each containing tens of thousands of temporal points. Table 6 summarizes the dataset details in Appendix I.\n\n\nFor all experiments, we adopted the same setup as in Liu et al. (2024), fixing the look-back window  $L = 96$  and testing four different prediction lengths  $T = 96, 192, 336, 720$ . We compared our TimeMachine-TTT model against 12 state-of-the-art (SOTA) models, including TimeMachine (Ahamed & Cheng, 2024), iTransformer (Liu et al., 2024), PatchTST (Nie et al., 2023), DLinear (Zeng et al., 2022), RLinear (Li et al., 2023), Autoformer (Wu et al., 2022), Crossformer Zhang & Yan (2023), TiDE (Das et al., 2024), Scinet (Liu et al., 2022a), TimesNet (Wu et al., 2023), FEDformer (Zhou et al., 2022), and Stationary (Liu et al., 2023b). All experiments were conducted with both MLP and Linear architectures using the original Mamba backbone, and we confirmed the results from the TimeMachine paper. We include calculations on the resource utilization of the model in Appendix G and quantify the impact of test-time updates on memory and latency in Appendix A.\n\n\nTable 1: Results in MSE and MAE (the lower the better) for the long-term forecasting task (averaged over 5 runs). We compare extensively with baselines under different prediction lengths,  $T = \\{96, 192, 336, 720\\}$  following the setting of iTransformer (Liu et al., 2023a). The length of the input sequence  $(L)$  is set to 96 for all baselines. TTT (ours) is our TTT block with the Conv Stack 5 architecture. The best results are in bold and the second best are underlined.\n\n\n<table><tr><td colspan=\"2\">Methods\u2192</td><td colspan=\"2\">TTT(ours)</td><td colspan=\"2\">TimeMachine</td><td colspan=\"2\">iTransformer</td><td colspan=\"2\">RLinear</td><td colspan=\"2\">PatchTST</td><td colspan=\"2\">Crossformer</td><td colspan=\"2\">TiDE</td><td colspan=\"2\">TimesNet</td><td colspan=\"2\">DLinear</td><td colspan=\"2\">SCIFNet</td><td colspan=\"2\">FEDformer</td><td colspan=\"2\">Stationary</td></tr><tr><td>D</td><td>T</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.165</td><td>0.214</td><td>0.164</td><td>0.208</td><td>0.174</td><td>0.214</td><td>0.192</td><td>0.232</td><td>0.177</td><td>0.218</td><td>0.158</td><td>0.230</td><td>0.202</td><td>0.261</td><td>0.172</td><td>0.220</td><td>0.196</td><td>0.255</td><td>0.221</td><td>0.306</td><td>0.217</td><td>0.296</td><td>0.173</td><td>0.223</td></tr><tr><td>192</td><td>0.225</td><td>0.263</td><td>0.211</td><td>0.250</td><td>0.221</td><td>0.254</td><td>0.240</td><td>0.271</td><td>0.225</td><td>0.259</td><td>0.206</td><td>0.277</td><td>0.242</td><td>0.298</td><td>0.219</td><td>0.261</td><td>0.237</td><td>0.296</td><td>0.261</td><td>0.340</td><td>0.276</td><td>0.336</td><td>0.245</td><td>0.285</td></tr><tr><td>336</td><td>0.246</td><td>0.275</td><td>0.256</td><td>0.290</td><td>0.278</td><td>0.296</td><td>0.292</td><td>0.307</td><td>0.278</td><td>0.297</td><td>0.272</td><td>0.335</td><td>0.287</td><td>0.335</td><td>0.280</td><td>0.306</td><td>0.283</td><td>0.335</td><td>0.309</td><td>0.378</td><td>0.339</td><td>0.380</td><td>0.321</td><td>0.338</td></tr><tr><td>720</td><td>0.339</td><td>0.343</td><td>0.342</td><td>0.343</td><td>0.358</td><td>0.349</td><td>0.364</td><td>0.353</td><td>0.354</td><td>0.348</td><td>0.398</td><td>0.418</td><td>0.351</td><td>0.386</td><td>0.365</td><td>0.359</td><td>0.345</td><td>0.381</td><td>0.377</td><td>0.427</td><td>0.403</td><td>0.428</td><td>0.414</td><td>0.410</td></tr><tr><td rowspan=\"4\">Traffic</td><td>96</td><td>0.397</td><td>0.268</td><td>0.397</td><td>0.268</td><td>0.395</td><td>0.268</td><td>0.649</td><td>0.389</td><td>0.544</td><td>0.359</td><td>0.522</td><td>0.290</td><td>0.805</td><td>0.493</td><td>0.593</td><td>0.321</td><td>0.650</td><td>0.396</td><td>0.788</td><td>0.499</td><td>0.587</td><td>0.366</td><td>0.612</td><td>0.338</td></tr><tr><td>192</td><td>0.434</td><td>0.287</td><td>0.417</td><td>0.274</td><td>0.417</td><td>0.276</td><td>0.601</td><td>0.366</td><td>0.540</td><td>0.354</td><td>0.530</td><td>0.293</td><td>0.756</td><td>0.474</td><td>0.617</td><td>0.336</td><td>0.598</td><td>0.370</td><td>0.789</td><td>0.505</td><td>0.604</td><td>0.373</td><td>0.613</td><td>0.340</td></tr><tr><td>336</td><td>0.430</td><td>0.283</td><td>0.433</td><td>0.281</td><td>0.433</td><td>0.283</td><td>0.609</td><td>0.369</td><td>0.551</td><td>0.358</td><td>0.558</td><td>0.305</td><td>0.762</td><td>0.477</td><td>0.629</td><td>0.336</td><td>0.605</td><td>0.373</td><td>0.797</td><td>0.508</td><td>0.621</td><td>0.383</td><td>0.618</td><td>0.328</td></tr><tr><td>720</td><td>0.456</td><td>0.286</td><td>0.467</td><td>0.300</td><td>0.467</td><td>0.302</td><td>0.647</td><td>0.387</td><td>0.586</td><td>0.375</td><td>0.589</td><td>0.328</td><td>0.719</td><td>0.449</td><td>0.640</td><td>0.350</td><td>0.645</td><td>0.394</td><td>0.841</td><td>0.523</td><td>0.626</td><td>0.382</td><td>0.653</td><td>0.355</td></tr><tr><td rowspan=\"4\">Electricity</td><td>96</td><td>0.135</td><td>0.230</td><td>0.142</td><td>0.236</td><td>0.148</td><td>0.240</td><td>0.201</td><td>0.281</td><td>0.195</td><td>0.285</td><td>0.219</td><td>0.314</td><td>0.237</td><td>0.329</td><td>0.168</td><td>0.272</td><td>0.197</td><td>0.282</td><td>0.247</td><td>0.345</td><td>0.193</td><td>0.308</td><td>0.169</td><td>0.273</td></tr><tr><td>192</td><td>0.153</td><td>0.254</td><td>0.158</td><td>0.250</td><td>0.162</td><td>0.253</td><td>0.201</td><td>0.283</td><td>0.199</td><td>0.289</td><td>0.231</td><td>0.322</td><td>0.236</td><td>0.330</td><td>0.184</td><td>0.289</td><td>0.196</td><td>0.285</td><td>0.257</td><td>0.355</td><td>0.201</td><td>0.315</td><td>0.182</td><td>0.286</td></tr><tr><td>336</td><td>0.166</td><td>0.255</td><td>0.172</td><td>0.268</td><td>0.178</td><td>0.269</td><td>0.215</td><td>0.298</td><td>0.215</td><td>0.305</td><td>0.246</td><td>0.337</td><td>0.249</td><td>0.344</td><td>0.198</td><td>0.300</td><td>0.209</td><td>0.301</td><td>0.269</td><td>0.369</td><td>0.214</td><td>0.329</td><td>0.200</td><td>0.304</td></tr><tr><td>720</td><td>0.199</td><td>0.285</td><td>0.207</td><td>0.298</td><td>0.225</td><td>0.317</td><td>0.257</td><td>0.331</td><td>0.256</td><td>0.337</td><td>0.280</td><td>0.363</td><td>0.284</td><td>0.373</td><td>0.220</td><td>0.320</td><td>0.245</td><td>0.333</td><td>0.299</td><td>0.390</td><td>0.246</td><td>0.355</td><td>0.222</td><td>0.321</td></tr><tr><td rowspan=\"4\">ETTh1</td><td>96</td><td>0.352</td><td>0.375</td><td>0.364</td><td>0.387</td><td>0.386</td><td>0.405</td><td>0.386</td><td>0.395</td><td>0.414</td><td>0.419</td><td>0.423</td><td>0.448</td><td>0.479</td><td>0.464</td><td>0.384</td><td>0.402</td><td>0.386</td><td>0.400</td><td>0.654</td><td>0.599</td><td>0.376</td><td>0.419</td><td>0.513</td><td>0.491</td></tr><tr><td>192</td><td>0.412</td><td>0.418</td><td>0.415</td><td>0.416</td><td>0.441</td><td>0.436</td><td>0.437</td><td>0.424</td><td>0.460</td><td>0.445</td><td>0.471</td><td>0.474</td><td>0.525</td><td>0.492</td><td>0.436</td><td>0.429</td><td>0.437</td><td>0.432</td><td>0.719</td><td>0.631</td><td>0.420</td><td>0.448</td><td>0.534</td><td>0.504</td></tr><tr><td>336</td><td>0.479</td><td>0.446</td><td>0.429</td><td>0.421</td><td>0.487</td><td>0.458</td><td>0.479</td><td>0.446</td><td>0.501</td><td>0.466</td><td>0.570</td><td>0.546</td><td>0.565</td><td>0.515</td><td>0.491</td><td>0.469</td><td>0.481</td><td>0.459</td><td>0.778</td><td>0.659</td><td>0.459</td><td>0.465</td><td>0.588</td><td>0.535</td></tr><tr><td>720</td><td>0.478</td><td>0.454</td><td>0.458</td><td>0.453</td><td>0.503</td><td>0.491</td><td>0.481</td><td>0.470</td><td>0.500</td><td>0.488</td><td>0.653</td><td>0.621</td><td>0.594</td><td>0.558</td><td>0.521</td><td>0.500</td><td>0.519</td><td>0.516</td><td>0.836</td><td>0.699</td><td>0.506</td><td>0.507</td><td>0.643</td><td>0.616</td></tr><tr><td rowspan=\"4\">ETTh2</td><td>96</td><td>0.274</td><td>0.328</td><td>0.275</td><td>0.334</td><td>0.297</td><td>0.349</td><td>0.288</td><td>0.338</td><td>0.302</td><td>0.348</td><td>0.745</td><td>0.584</td><td>0.400</td><td>0.440</td><td>0.340</td><td>0.374</td><td>0.333</td><td>0.387</td><td>0.707</td><td>0.621</td><td>0.358</td><td>0.397</td><td>0.476</td><td>0.458</td></tr><tr><td>192</td><td>0.373</td><td>0.379</td><td>0.349</td><td>0.381</td><td>0.380</td><td>0.400</td><td>0.374</td><td>0.390</td><td>0.388</td><td>0.400</td><td>0.877</td><td>0.656</td><td>0.528</td><td>0.509</td><td>0.402</td><td>0.414</td><td>0.477</td><td>0.476</td><td>0.860</td><td>0.689</td><td>0.429</td><td>0.439</td><td>0.512</td><td>0.493</td></tr><tr><td>336</td><td>0.403</td><td>0.408</td><td>0.340</td><td>0.381</td><td>0.428</td><td>0.432</td><td>0.415</td><td>0.426</td><td>0.433</td><td>0.426</td><td>0.433</td><td>1.043</td><td>0.731</td><td>0.643</td><td>0.571</td><td>0.452</td><td>0.452</td><td>0.541</td><td>1.000</td><td>0.744</td><td>0.496</td><td>0.487</td><td>0.496</td><td>0.552</td></tr><tr><td>720</td><td>0.448</td><td>0.434</td><td>0.411</td><td>0.433</td><td>0.427</td><td>0.445</td><td>0.420</td><td>0.440</td><td>0.431</td><td>0.446</td><td>1.104</td><td>0.763</td><td>0.874</td><td>0.679</td><td>0.462</td><td>0.468</td><td>0.831</td><td>0.657</td><td>1.249</td><td>0.838</td><td>0.463</td><td>0.474</td><td>0.562</td><td>0.560</td></tr><tr><td rowspan=\"4\">ETTh1</td><td>96</td><td>0.309</td><td>0.348</td><td>0.317</td><td>0.355</td><td>0.334</td><td>0.368</td><td>0.355</td><td>0.376</td><td>0.329</td><td>0.367</td><td>0.404</td><td>0.426</td><td>0.364</td><td>0.387</td><td>0.338</td><td>0.375</td><td>0.345</td><td>0.372</td><td>0.418</td><td>0.438</td><td>0.379</td><td>0.419</td><td>0.386</td><td>0.398</td></tr><tr><td>192</td><td>0.371</td><td>0.389</td><td>0.357</td><td>0.378</td><td>0.377</td><td>0.391</td><td>0.391</td><td>0.392</td><td>0.367</td><td>0.385</td><td>0.450</td><td>0.451</td><td>0.398</td><td>0.404</td><td>0.374</td><td>0.387</td><td>0.380</td><td>0.389</td><td>0.439</td><td>0.450</td><td>0.426</td><td>0.441</td><td>0.459</td><td>0.444</td></tr><tr><td>336</td><td>0.381</td><td>0.401</td><td>0.379</td><td>0.399</td><td>0.426</td><td>0.420</td><td>0.424</td><td>0.415</td><td>0.399</td><td>0.410</td><td>0.532</td><td>0.515</td><td>0.428</td><td>0.425</td><td>0.410</td><td>0.411</td><td>0.413</td><td>0.413</td><td>0.490</td><td>0.485</td><td>0.445</td><td>0.459</td><td>0.464</td><td></td></tr><tr><td>720</td><td>0.433</td><td>0.423</td><td>0.445</td><td>0.436</td><td>0.491</td><td>0.459</td><td>0.487</td><td>0.450</td><td>0.454</td><td>0.439</td><td>0.666</td><td>0.589</td><td>0.487</td><td>0.461</td><td>0.478</td><td>0.450</td><td>0.474</td><td>0.453</td><td>0.595</td><td>0.550</td><td>0.543</td><td>0.490</td><td>0.585</td><td>0.516</td></tr><tr><td rowspan=\"4\">ETTh2</td><td>96</td><td>0.180</td><td>0.253</td><td>0.175</td><td>0.256</td><td>0.180</td><td>0.264</td><td>0.182</td><td>0.265</td><td>0.175</td><td>0.259</td><td>0.287</td><td>0.366</td><td>0.207</td><td>0.305</td><td>0.187</td><td>0.267</td><td>0.193</td><td>0.292</td><td>0.286</td><td>0.377</td><td>0.203</td><td>0.287</td><td>0.292</td><td>0.287</td></tr><tr><td>192</td><td>0.242</td><td>0.301</td><td>0.239</td><td>0.299</td><td>0.250</td><td>0.309</td><td>0.246</td><td>0.304</td><td>0.241</td><td>0.302</td><td>0.414</td><td>0.492</td><td>0.290</td><td>0.364</td><td>0.249</td><td>0.309</td><td>0.284</td><td>0.362</td><td>0.399</td><td>0.445</td><td>0.269</td><td>0.328</td><td>0.289</td><td>0.339</td></tr><tr><td>336</td><td>0.302</td><td>0.341</td><td>0.287</td><td>0.332</td><td>0.311</td><td>0.348</td><td>0.307</td><td>0.342</td><td>0.305</td><td>0.343</td><td>0.597</td><td>0.542</td><td>0.377</td><td>0.422</td><td>0.321</td><td>0.351</td><td>0.369</td><td>0.427</td><td>0.637</td><td>0.591</td><td>0.325</td><td>0.366</td><td>0.334</td><td>0.361</td></tr><tr><td>720</td><td>0.364</td><td>0.384</td><td>0.371</td><td>0.385</td><td>0.412</td><td>0.417</td><td>0.407</td><td>0.398</td><td>0.402</td><td>0.400</td><td>1.730</td><td>1.042</td><td>0.558</td><td>0.524</td><td>0.408</td><td>0.403</td><td>0.554</td><td>0.522</td><td>0.960</td><td>0.735</td><td>0.421</td><td>0.415</td><td>0.417</td><td>0.413</td></tr></table>\n\n\n# 4.2 QUANTITATIVE RESULTS\n\n\nAcross all seven benchmark datasets, our TimeMachine-TTT model consistently demonstrated superior performance compared to SOTA models. In the Weather dataset, TTT achieved leading performance at longer horizons (336 and 720), with MSEs of 0.246 and 0.339, respectively, outperforming TimeMachine, which recorded MSEs of 0.256 and 0.342. The Traffic dataset, with its high number of channels (862), also saw TTT outperform TimeMachine and iTransformer at both medium (336-step MSE of 0.430 vs. 0.433) and long horizons (720-step MSE of 0.464 vs. 0.467), highlighting the model's ability to handle multivariate time series data.\n\n\nIn the Electricity dataset, TTT showed dominant results across all horizons, achieving an MSE of 0.135, 0.153, 0.166 and 0.199 at horizons 96, 192, 336, and 720 respectively, outperforming TimeMachine and PatchTST. For ETTh1, TTT was highly competitive, with strong short-term results (MSE of 0.352 at horizon 96) and continued dominance at medium-term horizons like 336, with an MSE of 0.412. For ETTh2, TTT beat TimeMachine on horizon 96 (MSE of 0.274), TTT also closed the gap at longer horizons (MSE of 0.448 at horizon 720 compared to 0.411 for TimeMachine).\n\n\nFor the ETTm1 dataset, TTT outperformed TimeMachine at nearly every horizon, recording an MSE of 0.309, 0.381 and 0.431 at horizon 96, 336 and 720 respectively, confirming its effectiveness for long-term forecasting. Similarly, in ETTm2, TTT remained highly competitive at longer horizons, with a lead over TimeMachine at horizon 720 (MSE of 0.362 vs. 0.371). The radar plot in Figure 2 shows the comparison between TTT (ours) and TimeMachine for both MSE and MAE on all datasets.\n\n\n# 5 PREDICTION LENGTH ANALYSIS AND ABLATION STUDY\n\n\n# 5.1 EXPERIMENTAL SETUP WITH ENHANCED ARCHITECTURES\n\n\nTo assess the impact of enhancing the model architecture, we conducted experiments by adding hidden layer architectures before the sequence modeling block in each of the four TTT blocks. The goal was to improve performance by enriching feature extraction through local temporal context. As shown in Figure 3 in Appendix D.\n\n\n![Figure 2: Average MSE and MAE comparison of our model and SOTA baselines with  $L = 720$ . The circle center represents the maximum possible error. Closer to the boundary indicates better performance.](images/0d916bb3a1b959772571d39cd5ba1cadbb5ecffd6d30f330dd1d9c83324d456b.jpg)\n\n\nWe tested the following configurations: (1) Conv 3: 1D Convolution with kernel size 3, (2) Conv 5: 1D Convolution with kernel size 5, (3) Conv Stack 3: two 1D Convolutions with kernel size 3 in cascade, (4) Conv Stack 5: two 1D Convolutions with kernel sizes 5 and 3 in cascade, (5) Inception: an Inception Block combining 1D Convolutions with kernel sizes 5 and 3, followed by concatenation and reduction to the original size and (6) ModernTCN: A modern convolutional block proposed in Donghao & Xue (2024) that uses depthwise and pointwise convolutions with residual connections similar to the structure of the transformer block. For the simpler architectures kernel sizes were limited to 5 to avoid oversmoothing, and original data dimensions were preserved to ensure consistency with the TTT architecture. For ModernTCN we reduced the internal dimensions to 16 (down from the suggested 64) and did not use multiscale due to the exponential increase in GPU memory required which slowed down the training process and did not allow the model to fit in a single A100 GPU. We kept the rest of the parameters of ModernTCN the same as in the original paper.\n\n\nAblation Study Findings Our findings reveal that the introduction of additional hidden layer architectures, including convolutional layers, had varying degrees of impact on performance across different horizons. The best-performing setup was the Conv Stack 5 architecture, which achieved the lowest MSE and MAE at the 96 time horizon, with values of 0.261 and 0.289, respectively, outperforming the TimeMachine model at this horizon. At longer horizons, such as 336 and 720, Conv Stack 5 continued to show competitive performance, with a narrow gap between it and the TimeMachine model. For example, at the 720 horizon, Conv Stack 5 showed an MAE of 0.373, while TimeMachine had an MAE of 0.378.\n\n\nHowever, other architectures, such as Conv 3 and Conv 5, provided only marginal improvements over the baseline TTT architectures (Linear and MLP). While they performed better than Linear and MLP, they did not consistently outperform more complex setups like Conv Stack 3 and 5 across all horizons. This suggests that hidden layer expressiveness can enhance model performance.\n\n\nModernTCN showed competitive results across multiple datasets (see Appendix I), such as ETTh2, where it achieved an MSE of 0.285 at horizon 96, outperforming Conv 3 and Conv 5. However, as with other deep convolutional layers, ModernTCN's increased complexity also led to slower training times compared to simpler setups like Conv 3 and it failed to match Conv Stack 5's performance.\n\n\n# 5.2 EXPERIMENTAL SETUP WITH INCREASED PREDICTION & SEQUENCE LENGTHS\n\n\nFor the second part of our experiments, we extended the sequence and prediction lengths beyond the parameters tested in previous studies. We used the same baseline architectures (MLP and Linear) with the Mamba backbone as in the original TimeMachine paper, but this time also included the best-performing 1D Convolution architecture with kernel size 3.\n\n\nThe purpose of these experiments was to test the model's capacity to handle much longer sequence lengths while maintaining high prediction accuracy. We tested the following sequence and prediction lengths, with  $L = 2880$  and 5760, far exceeding the original length of  $L = 96$ :\n\n\n<table><tr><td>Seq Length</td><td>2880</td><td>2880</td><td>2880</td><td>2880</td><td>5760</td><td>5760</td><td>5760</td><td>5760</td><td>720</td><td>720</td><td>720</td><td>720</td></tr><tr><td>Pred Length</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td></tr></table>\n\n\nTable 2: Testing parameters for sequence and prediction lengths.\n\n\n# 5.3 RESULTS AND STATISTICAL COMPARISONS FOR PROPOSED ARCHITECTURES\n\n\nThe proposed architectures\u2014TTT Linear, TTT MLP, Conv Stack 3, Conv Stack 5, Conv 3, Conv 5, Inception, and TTT with ModernTCN\u2014exhibit varying performance across prediction horizons. TTT Linear performs well at shorter horizons (MSE 0.268, MAE 0.298 at horizon 96) but declines at longer horizons (MSE 0.357 at horizon 336). TTT MLP follows a similar trend with slightly worse performance. Conv 3 and Conv 5 outperform Linear and MLP at shorter horizons (MSE 0.269, MAE 0.297 at horizon 96) but lag behind Conv Stack models at longer horizons. TTT with ModernTCN shows promising results at shorter horizons, such as MSE 0.389, MAE 0.402 on ETTh1, and MSE 0.285, MAE 0.340 on ETTh2 at horizon 96. Although results for Traffic and Electricity datasets are pending, preliminary findings indicate TTT with ModernTCN is competitive, particularly for short-term dependencies (see Table 7 in Appendix I). Conv Stack 5 performs best at shorter horizons (MSE 0.261, MAE 0.289 at horizon 96). Inception provides stable performance across horizons, closely following Conv Stack 3 (MSE 0.361 at horizon 336). At horizon 720, Conv 5 shows a marginal improvement over Conv 3, with an MSE of 0.400 compared to 0.406. The Conv Stack 5 architecture demonstrates the best overall performance among all convolutional models.\n\n\n# 5.4 RESULTS AND STATISTICAL COMPARISONS FOR INCREASED PREDICTION AND SEQUENCE LENGTHS\n\n\nBoth shorter and longer sequence lengths affect model performance differently. Shorter sequence lengths (e.g., 2880) provide better accuracy for shorter prediction horizons, with the TTT model achieving an MSE of 0.332 and MAE of 0.356 at horizon 192, outperforming TimeMachine. Longer sequence lengths (e.g., 5760) result in higher errors, particularly for shorter horizons, but TTT remains more resilient, showing improved performance over TimeMachine. For shorter prediction lengths (96 and 192), TTT consistently yields lower MSE and MAE compared to TimeMachine. As prediction lengths grow to 720, both models experience increasing error rates, but TTT maintains a consistent advantage. For instance, at horizon 720, TTT records an MSE of 0.517 compared to TimeMachine's 0.535. Overall, TTT consistently outperforms TimeMachine across most prediction horizons, particularly for shorter sequences and smaller prediction windows. As the sequence length increases, TTT's ability to manage long-term dependencies becomes increasingly evident, with models like Conv Stack 5 showing stronger performance at longer horizons.\n\n\n# 5.5 EVALUATION\n\n\nThe results of our experiments indicate that the TimeMachine-TTT model outperforms the SOTA models across various scenarios, especially when handling larger sequence and prediction lengths. Several key trends emerged from the analysis:\n\n- - Improved Performance on Larger Datasets: On larger datasets, such as Electricity, Traffic, and Weather, TTT models demonstrated superior performance compared to TimeMachine. For example, at a prediction length of 96, the TTT architecture achieved an MSE of 0.283 compared to TimeMachine's 0.309, reflecting a notable improvement. This emphasizes TTT's ability to effectively handle larger temporal windows.\n\n- - Better Handling of Long-Range Dependencies: TTT-based models, particularly Conv Stack 5 and Conv 3, demonstrated clear advantages in capturing long-range dependencies. As prediction lengths increased, such as at 720, TTT maintained better error rates, with Conv Stack 5 achieving an MAE of 0.373 compared to TimeMachine's 0.378. Although the difference narrows at longer\n\n\n<table><tr><td></td><td colspan=\"2\">Conv stack 5</td><td colspan=\"2\">TimeMachine</td><td colspan=\"2\">Conv 3</td><td colspan=\"2\">Conv 5</td><td colspan=\"2\">Conv stack 3</td><td colspan=\"2\">Inception</td><td colspan=\"2\">Linear</td><td colspan=\"2\">MLP</td></tr><tr><td>horizon</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>96</td><td>0.259</td><td>0.288</td><td>0.262</td><td>0.292</td><td>0.269</td><td>0.297</td><td>0.269</td><td>0.297</td><td>0.272</td><td>0.300</td><td>0.274</td><td>0.302</td><td>0.268</td><td>0.298</td><td>0.271</td><td>0.301</td></tr><tr><td>192</td><td>0.316</td><td>0.327</td><td>0.307</td><td>0.321</td><td>0.318</td><td>0.329</td><td>0.320</td><td>0.331</td><td>0.319</td><td>0.330</td><td>0.321</td><td>0.330</td><td>0.326</td><td>0.336</td><td>0.316</td><td>0.332</td></tr><tr><td>336</td><td>0.344</td><td>0.344</td><td>0.328</td><td>0.339</td><td>0.348</td><td>0.348</td><td>0.347</td><td>0.347</td><td>0.359</td><td>0.358</td><td>0.361</td><td>0.359</td><td>0.357</td><td>0.358</td><td>0.358</td><td>0.357</td></tr><tr><td>720</td><td>0.388</td><td>0.373</td><td>0.386</td><td>0.378</td><td>0.406</td><td>0.389</td><td>0.400</td><td>0.389</td><td>0.399</td><td>0.387</td><td>0.404</td><td>0.390</td><td>0.414</td><td>0.393</td><td>0.394</td><td>0.393</td></tr></table>\n\n\nTable 3: MSE and MAE performance metrics for TimeMachine, TTT blocks with original architectures (MLP & Linear), and TTT block with different convolutional architectures across all prediction horizons.\n\n\n<table><tr><td></td><td colspan=\"2\">TTT</td><td colspan=\"2\">TimeMachine</td></tr><tr><td>Pred. Length</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>96</td><td>0.283</td><td>0.322</td><td>0.309</td><td>0.337</td></tr><tr><td>192</td><td>0.332</td><td>0.356</td><td>0.342</td><td>0.359</td></tr><tr><td>336</td><td>0.402</td><td>0.390</td><td>0.414</td><td>0.394</td></tr><tr><td>720</td><td>0.517</td><td>0.445</td><td>0.535</td><td>0.456</td></tr><tr><td>1440</td><td>0.399</td><td>0.411</td><td>0.419</td><td>0.429</td></tr><tr><td>2880</td><td>0.456</td><td>0.455</td><td>0.485</td><td>0.474</td></tr><tr><td>4320</td><td>0.580</td><td>0.534</td><td>0.564</td><td>0.523</td></tr></table>\n\n\nTable 4: Average MSE and MAE for different prediction lengths and sequence length of 2880 comparing TimeMachine and TTT architectures.\n\n\n<table><tr><td></td><td colspan=\"2\">TTT</td><td colspan=\"2\">TimeMachine</td></tr><tr><td>Seq. Length</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>720</td><td>0.312</td><td>0.336</td><td>0.319</td><td>0.341</td></tr><tr><td>2880</td><td>0.366</td><td>0.384</td><td>0.373</td><td>0.388</td></tr><tr><td>5760</td><td>0.509</td><td>0.442</td><td>0.546</td><td>0.459</td></tr></table>\n\n\nTable 5: Average MSE and MAE for different sequence lengths comparing TimeMachine and Conv stack 5 architectures.\n\n\nhorizons, the TTT architectures remain more robust, particularly in handling extended sequences and predictions.\n\n- - Impact of Hidden Layer Architectures: While stacked convolutional architectures, such as Conv Stack 3 and Conv Stack 5, provided incremental improvements, simpler architectures like Conv 3 and Conv 5 also delivered competitive performance. Conv Stack 5 showed a reduction in MSE compared to TimeMachine, at horizon 96, where it achieved an MSE of 0.261 versus TimeMachine's 0.262. ModernTCN failed to meet the performance of simpler architectures.\n\n- - Effect of Sequence and Prediction Lengths: As the sequence and prediction lengths increased, all models exhibited higher error rates. However, TTT-based architectures, particularly Conv Stack 5 and Conv 3, handled these increases better than TimeMachine. For example, at a sequence length of 5760 and prediction length of 720, TTT recorded lower MSE and MAE values, demonstrating better scalability and adaptability to larger contexts. Moreover, shorter sequence lengths (e.g., 2880) performed better at shorter horizons, while longer sequence lengths showed diminishing returns for short-term predictions.\n\n\n# 6 CONCLUSION AND FUTURE WORK\n\n\nIn this work, we improved the state-of-the-art (SOTA) model for time series forecasting by replacing the Mamba modules in the original TimeMachine model with Test-Time Training (TTT) modules, which leverage linear RNNs to capture long-range dependencies. Extensive experiments demonstrated that the TTT architectures\u2014MLP and Linear\u2014performed well, with MLP slightly outperforming Linear. Exploring alternative architectures, particularly Conv Stack 5 and ModernTCN, significantly improved performance at longer prediction horizons, with ModernTCN showing notable efficiency in capturing short-term dependencies. The most significant gains came from increasing sequence and prediction lengths, where our TTT models consistently matched or outperformed the SOTA model, particularly on larger datasets like Electricity, Traffic, and Weather, emphasizing the model's strength in handling long-range dependencies. While convolutional stacks and ModernTCN showed promise, further improvements could be achieved by refining hidden layer configurations and exploring architectural diversity. We included some potential real world applications of the TTT module in Appendix C along with why we believe it's best suited for LTSF. Overall, this work demonstrates the potential of TTT modules in long-term forecasting, especially when combined with robust convolutional architectures and applied to larger datasets and longer horizons.\n\n\n# REFERENCES\n\n- Md Atik Ahamed and Qiang Cheng. Timemachine: A time series is worth 4 mambas for long-term forecasting, 2024. URL https://arxiv.org/abs/2403.09898.\n\n- Ekin Akyurek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. The surprising effectiveness of test-time training for abstract reasoning, 2024. URL https://arxiv.org/abs/2411.07279.\n\n- Sercan O. Arik, Nathanael C. Yoder, and Tomas Pfister. Self-adaptive forecasting for improved deep learning on non-stationary time-series, 2022. URL https://arxiv.org/abs/2202.02403.\n\n- Jonathan Z. Bakdash, Steve Hutchinson, Erin G. Zaroukian, Laura R. Marusich, Saravanan Thirumuruganathan, Charmaine Sample, Blaine Hoffman, and Gautam Das. Malware in the future? forecasting of analyst detection of cyber events. arXiv preprint arXiv:1707.03243, 2017.\n\n- Muxi Chen, Zhijian Xu, Ailing Zeng, and Qiang Xu. Fraud: Frequency domain augmentation for time series forecasting, 2023a. URL https://arxiv.org/abs/2302.09292.\n\n- Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, and Tomas Pfister. Tsmixer: An all-mlp architecture for time series forecasting, 2023b. URL https://arxiv.org/abs/2303.06053.\n\n- Xupeng Chen, Ran Wang, Amirhossein Khalilian-Gourtani, Leyao Yu, Patricia Dugan, Daniel Friedman, Werner Doyle, Orrin Devinsky, Yao Wang, and Adeen Flinker. A neural speech decoding framework leveraging deep learning and speech synthesis. Nature Machine Intelligence, pp. 1-14, 2024.\n\n- Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, and Rose Yu. Long-term forecasting with tide: Time-series dense encoder, 2024. URL https://arxiv.org/abs/2304.08424.\n\n- John Doe and Jane Smith. Time series forecasting for energy consumption. *Energies*, 15(3):773, 2023. doi: 10.3390/en15030773. URL https://www.mdpi.com/1996-1073/15/3/773.\n\n- John Doe and Jane Smith. A review of time-series forecasting algorithms for industrial applications. *Machines*, 12(6):380, 2024. doi: 10.3390/machines12060380. URL https://www.mdpi.com/2075-1702/12/6/380.\n\n- Luo Donghao and Wang Xue. ModernTCN: A modern pure convolution structure for general time series analysis. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=vpJMJerXHU.\n\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.\n\n- Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://arxiv.org/abs/2312.00752.\n\n- Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces, 2022a. URL https://arxiv.org/abs/2111.00396.\n\n- Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models, 2022b. URL https://arxiv.org/abs/2206.11893.\n\n- Ankit Gupta, Harsh Mehta, and Jonathan Berant. Simplifying and understanding state space models with diagonal linear rnns, 2023. URL https://arxiv.org/abs/2212.00768.\n\n- Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models, 2024. URL https://arxiv.org/abs/2305.18466.\n\n- Haowen Hou and F. Richard Yu. Rwkv-ts: Beyond traditional recurrent neural network for time series tasks, 2024. URL https://arxiv.org/abs/2401.09093.\n\n- Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2021.\n\n- Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=cGDAkQo1C0p.\n\n- Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An investigation on linear mapping, 2023. URL https://arxiv.org/abs/2305.10721.\n\n- Bryan Lim and Stefan Zohren. Time series forecasting with deep learning: A survey. arXiv preprint arXiv:2004.13408, 2020.\n\n- Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiumia Lai, Lingna Ma, and Qiang Xu. Scinet: Time series modeling and forecasting with sample convolution and interaction, 2022a. URL https://arxiv.org/abs/2106.09305.\n\n- Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on Learning Representations, 2022b.\n\n- Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023a.\n\n- Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting, 2023b. URL https://arxiv.org/abs/2205.14415.\n\n- Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting, 2024. URL https://arxiv.org/abs/2310.06625.\n\n- Haowei Ni, Shuchen Meng, Xieming Geng, Panfeng Li, Zhuoying Li, Xupeng Chen, Xiaotong Wang, and Shiyao Zhang. Time series modeling for heart rate prediction: From arima to transformers. arXiv preprint arXiv:2406.12199, 2024.\n\n- Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers, 2023. URL https://arxiv.org/abs/ 2211.14730.\n\n- Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org, 2023.\n\n- Badri N. Patro and Vijay S. Agneeswaran. Simba: Simplified mamba-based architecture for vision and multivariate time series, 2024. URL https://arxiv.org/abs/2403.15360.\n\n- Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts, 2020. URL https://arxiv.org/abs/1909.13231.\n\n- Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. Learning to (learn at test time): Rnns with expressive hidden states, 2024. URL https://arxiv.org/abs/2407.04620.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n\n- Renhao Wang, Yu Sun, Yossi Gandelsman, Xinlei Chen, Alexei A. Efros, and Xiaolong Wang. Test-time training on video streams, 2023. URL https://arxiv.org/abs/2307.05014.\n\n- Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Transformers in time series: A survey, 2023. URL https://arxiv.org/abs/2202.07125.\n\n- Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting, 2022. URL https://arxiv.org/abs/2106.13008.\n\n- Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis, 2023. URL https://arxiv.org/abs/2210.02186.\n\n- Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting?, 2022. URL https://arxiv.org/abs/2205.13504.\n\n- Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=vSVLM2j9eie.\n\n- Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting, 2021. URL https://arxiv.org/abs/2012.07436.\n\n- Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting, 2022. URL https:// arxiv.org/abs/2201.12740.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636562.3277392", "msecs": "327.0", "relativeCreated": "23020.8797454834", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# How Low Can We Go: Trading Memory for Error in Low-Precision Training\n\n# Abstract\nLow-precision arithmetic trains deep learning models using less energy, less memory and less time. However, we pay a price for the savings: lower precision may yield larger round-off error and hence larger prediction error. As applications proliferate, users must choose which precision to use to train a new model, and chip manufacturers must decide which precisions to manufacture. We view these precision choices as a hyperparameter tuning problem, and borrow ideas from meta-learning to learn the tradeoff between memory and error. In this paper, we introduce Pareto Estimation to Pick the Perfect Precision (PEPPP). We use matrix factorization to find non-dominated configurations (the Pareto frontier) with a limited number of network evaluations. For any given memory budget, the precision that minimizes error is a point on this frontier. Practitioners can use the frontier to trade memory for error and choose the best precision for their goals.\n\n\n# 1 INTRODUCTION\n\n\nTraining modern-day neural networks is becoming increasingly expensive as task and model sizes increase. The energy consumption of the corresponding computation has increased alarmingly, and raises doubts about the sustainability of modern training practices (Schwartz et al., 2019). Low-precision training can reduce consumption of both computation (De Sa et al., 2018) and memory (Sohoni et al., 2019a), thus minimizing the cost and energy to train larger models and making deep learning more accessible to resource-limited users.\n\n\nLow-precision training replaces 32-bit or 64-bit floating point numbers with fixed or floating point numbers that allocate fewer bits for the activations, optimizers, and weights. A rich variety of methods appear in recent literature, including bit-centering (De Sa et al., 2018), loss scaling (Micikevicius et al., 2017) and mixed-precision training (Zhou et al., 2016). There is however a fundamental tradeoff: lowering the number of bits of precision increases the quantization error, which may disrupt convergence and increase downstream error (Zhang et al., 2016; Courbariaux et al., 2014; Gupta et al., 2015). The low-precision configuration is thus a hyperparameter to be chosen according to the resource constraints of the practitioner.\n\n\n![(a) error vs memory on CIFAR-10 (Krizhevsky et al., 2009)](images/bf3eed83e98a16a1d63f290ba48a56b1ef09695237b8ca9d02b4c90af7095ce5.jpg)\n\n\n![(b) # activation bits in non-dominated configurations Figure 1: Test error vs memory for ResNet-18 across 99 low-precision floating point configurations. Figure (a) shows the tradeoff on CIFAR-10. (Non-dominated points are blue circles.) Figure (b) shows that the best precision to use varies depending on the memory budget, on 87 image datasets. See Section 4 for experimental details.](images/ca82afa05cb37f487616729bcd307fef9b46547a927f1c9a1d36718af2b3418e.jpg)\n\n\nHow should we choose this hyperparameter? Many believe the highest allowable precision (given a memory budget) generally produces the lowest error model. However, there are typically many ways to use the same amount of memory. As shown in Figure 1(a), some of these configurations produce much lower error than others! Figure 1(b) shows that no one configuration dominates all others. We might consult previous literature to choose a precision; but this approach fails for new applications.\n\n\nOur goal is to efficiently pick the best low-precision configuration under a memory budget. Efficiency is especially important for resource-constrained practitioners, such as individual users or early-stage startups. To promote efficiency, we use a meta-learning (Lemke et al., 2015; Vanschooren, 2018; Hospedales et al., 2020) approach: we train a small number of cheap very-low-precision models on the dataset to choose the perfect precision. The gains from choosing the right low-precision format can offset the cost of this extra training \u2014 but each precision must be chosen carefully to realize the benefits of low-precision training.\n\n\nWe use ideas from multi-objective optimization to characterize the tradeoff between memory and error and identify the Pareto frontier: the set of non-dominated solutions. Users will want to understand the tradeoff between error and memory so they can determine the resources needed to adequately train a model for a given machine learning task. This tradeoff may also influence the design of application-specific low-precision hardware, with profound implications for the future (Hooker, 2020). For example, among all 99 low-precision configurations we tried, we identified some configurations that are Pareto optimal across many different tasks (listed in Appendix D). These results could help hardware manufacturers decide which precision settings are worth manufacturing.\n\n\nComputing the Pareto frontier by training models for all low-precision configurations is expensive and unnecessary. Cloud computing platforms like Google Cloud and Amazon EC2 charge more for machines with the same CPU and double memory:  $25\\%$  more on Google  $\\mathrm{Cloud}^1$  and  $100\\%$  more on Amazon  $\\mathrm{EC2}^2$  as of mid-September 2021. We use techniques from meta-learning to leverage the information from other low-precision training runs on related datasets. This approach allows us to estimate the Pareto frontier without evaluating all of the low-precision configurations.\n\n\nOur system, Pareto Estimation to Pick the Perfect Precision (PEPPP), has two goals. The first goal is to find the Pareto frontiers of a collection of related tasks, and is called meta-training in the meta-learning literature. Meta-training requires a set of measurements, each collected by training and testing a neural network with a given precision on a dataset. This information can be gathered offline with a relatively large resource budget, or by crowdsourcing amongst the academic or open-source community. Still, it is absurdly expensive to exhaustively evaluate all measurements: that is, every possible low-precision configuration on every task. Instead, we study how to choose a subset of the possible measurements to achieve the best estimate. The second goal we call meta-test: using the information learned on previous tasks, how can we transfer that information to a new task to efficiently estimate its Pareto frontier? This goal corresponds to a resource-constrained individual or startup who wants to determine the best low-precision configuration for a new dataset.\n\n\nBoth meta-training and meta-test rely on matrix completion and active learning techniques to avoid exhaustive search: we make a subset of all possible measurements and predict the rest. We then estimate the Pareto frontier to help the user make an informed choice of the best configuration. We consider two sampling schemes: uniform and non-uniform sampling. Uniform sampling is straightforward. Non-uniform sampling estimates the Pareto frontier more efficiently by making fewer or even no high-memory measurements.\n\n\nTo the best of our knowledge, PEPPP is the first to study the error-memory tradeoff in low-precision training and inference without exhaustive search. Some previous works show the benefit of low-precision arithmetic in significant energy reduction at the cost of a small accuracy decrease (Hashemi et al., 2017), and propose hardware-software codesign frameworks to select the desired model (Langroudi et al., 2019). Many other papers focus on low-precision inference only and use it for model compression (Wu et al., 2018; Cai & Vasconcelos, 2020). Another line of work seeks state-of-the-art (SOTA) performance with low-precision training, using carefully designed learning schedules (Sun et al., 2019; 2020). Our work imagines low-precision as a way to reduce training or deployment costs of customized models trained on proprietary datasets (not SOTA models on ML benchmarks). Even so, we show in Section 4 that our work picks promising models for every memory budget, which enables near-SOTA results on CIFAR-10.\n\n\nFigure 2 shows a flowchart of PEPPP. The rest of this paper is organized as follows. Section 2 introduces notation and terminology. Section 3 describes the main ideas we use to actively sample configurations and approximate Pareto frontiers. Section 4 shows experimental results.\n\n\n![Figure 2: The PEPPP workflow. We begin with a collection of (meta-) training datasets and low precision configurations. In the meta-training phase, we sample dataset-configuration pairs to train, and compute the misclassification error. We use matrix factorization to compute a low dimensional embedding of every configuration. In the meta-test phase, our goal is to pick the perfect precision (within our memory budget) for the meta-test dataset. We compute the memory required for each configuration, and we select a subset of fast, informative configurations to evaluate. By regressing the errors of these configurations on the configuration embeddings, we find an embedding for the meta-test dataset, which we use to predict the error of every other configuration (including more expensive ones) and select the best subject to our memory budget.](images/583df5bab9b72a01338cc0e6beef3e4a0014806c0431755d35b706c81e7b074b.jpg)\n\n\n# 2 NOTATION AND TERMINOLOGY\n\n\nLow-precision formats. We use floating point numbers for low-precision training in this paper.\n\n\nAs an example, Figure 3 shows an 8-bit floating point number with 3 exponent bits and 4 mantissa bits. A specific low-precision representation with a certain number of bits for each part is called a low-precision format. We may use different low-precision formats for the weights, activations, and optimizer. In the case of using two formats to train and represent a\n\n\n![Figure 3: An 8-bit floating point number representing  $(-1)^{\\mathrm{sign}}\\cdot 2^{\\mathrm{exponent} - 7}\\cdot 1.b_{3}b_{2}b_{1}b_{0}$](images/33960eaeb3ba45416a9deefa3a5df94f3025b3da72bbfa261173c0dd729733c0.jpg)\n\n\nneural network (as discussed in Section 3.1), these two formats are called Format  $A$  and  $B$ . A specific combination of these two formats is a hyperparameter setting of the neural network, and we call it a low-precision configuration.\n\n\nMath basics. We define  $[n] = \\{1, \\dots, n\\}$  for a positive integer  $n$ . With a Boolean variable  $\\mathcal{X}$ , the indicator function  $\\mathbb{1}(\\mathcal{X})$  equals 1 if  $\\mathcal{X}$  is true, and 0 otherwise. With a scalar variable  $x$ , we use  $x_{+}$  to denote  $\\max \\{x, 0\\} \\subseteq$  and  $\\subset$  denote subset and strict subset, respectively.\n\n\nLinear algebra. We denote vector and matrix variables respectively by lowercase letters  $(a)$  and capital letters  $(A)$ . All vectors are column vectors. The Euclidean norm of a vector  $a \\in \\mathbb{R}^n$  is  $\\|a\\| := \\sqrt{\\sum_{i=1}^{n} a_i^2}$ . To denote a part of a matrix  $A \\in \\mathbb{R}^{n \\times d}$ , we use a colon to denote the varying dimension:  $A_{i,:}$  and  $A_{:,j}$  (or  $a_j$ ) denote the  $i$ th row and  $j$ th column of  $A$ , respectively, and  $A_{ij}$  denotes the  $(i,j)$ -th entry. With an ordered index set  $\\mathcal{S} = \\{s_1, \\ldots, s_k\\}$  where  $s_1 < \\ldots < s_k \\in [d]$ , we denote  $A_{:\\mathcal{S}} := [A_{:,s_1} \\cdots A_{:,s_k}] \\in \\mathbb{R}^{n \\times k}$ . Given two vectors  $x, y \\in \\mathbb{R}^n$ ,  $x \\preceq y$  means  $x_i \\leq y_i$  for each  $i \\in [n]$ . Given a matrix  $A \\in \\mathbb{R}^{n \\times d}$  and a set of observed indices as  $\\Omega \\subseteq [n] \\times [d]$ , the partially observed matrix  $P_{\\Omega}(A) \\in \\mathbb{R}^{n \\times d}$  has entries  $(P_{\\Omega}(A))_{ij} = A_{ij}$  if  $(i,j) \\in \\Omega$ , and 0 otherwise.\n\n\nPareto frontier. Multi-objective optimization simultaneously minimizes  $n$  costs  $\\{c_i\\}_{i \\in [n]}$ . A feasible point  $c^{(1)} = (c_1^{(1)}, \\ldots, c_n^{(1)})$  is Pareto optimal if for any other feasible point  $c^{(2)} = (c_1^{(2)}, \\ldots, c_n^{(2)})$ ,\n\n\n$c^{(2)} \\preceq c^{(1)}$  implies  $c^{(2)} = c^{(1)}$  (Boyd & Vandenberghe, 2004). The set of Pareto optimal points is the Pareto frontier.\n\n\nTasks, datasets, measurements and evaluations. A task carries out a process (classification, regression, image segmentation, etc.) on a dataset. Given a deep learning model and a dataset, the training and testing of the model on the dataset is called a measurement. In our low-precision context, we evaluate a configuration on a dataset to make a measurement.\n\n\nMeta-learning. Meta-learning transfers knowledge from past (meta-training) tasks to better understand the new (meta-test) task. To evaluate the error of meta-training, we use meta-leave-one-out cross-validation (meta-LOOCV): on a collection of tasks, each time we treat one task as meta-test and the others as meta-training, and average the results over all splits.\n\n\nError matrix and memory matrix. Given a neural network, the errors of different low-precision configurations on meta-training datasets form an error matrix, whose  $(i,j)$ -th entry  $E_{ij}$  is the test error of the  $j$ th configuration on the  $i$ th dataset. To compute the error matrix, we split the  $i$ -th dataset into training and test subsets (or use a given split), train the neural network at the  $j$ -th configuration on the training subset, and evaluate the test error on the test subset. The memory required for each measurement forms a memory matrix  $M$ , which has the same shape as the corresponding error matrix. Example error and memory matrices are shown in Figure 4.\n\n\n<table><tr><td colspan=\"5\">low-precision configurations</td></tr><tr><td></td><td>a</td><td>b</td><td>c</td><td>d</td></tr><tr><td>1</td><td>0.85</td><td>0.71</td><td>0.32</td><td>0.28</td></tr><tr><td>2</td><td>0.80</td><td>0.80</td><td>0.80</td><td>0.69</td></tr><tr><td>3</td><td>0.80</td><td>0.68</td><td>0.46</td><td>0.45</td></tr></table>\n\n\n<table><tr><td colspan=\"5\">low-precision configurations</td></tr><tr><td></td><td>a</td><td>b</td><td>c</td><td>d</td></tr><tr><td rowspan=\"3\">1 datasets</td><td>181</td><td>269</td><td>272</td><td>295</td></tr><tr><td>180</td><td>269</td><td>272</td><td>295</td></tr><tr><td>590</td><td>924</td><td>927</td><td>1032</td></tr><tr><td colspan=\"5\">(b) memory matrix M (MB)</td></tr></table>\n\n\nFigure 4: Example error and memory matrices for some datasets and low-precision configurations. Dataset 1: CIFAR-10, 2: CIFAR-100 (fruit and vegetables), 3: ImageNet-stick (Deng et al., 2009). Configuration Format A (exponent bits, mantissa bits), Format B (exponent bits, mantissa bits). a: (3, 1), (6, 7); b: (3, 4), (7, 7); c: (4, 3), (8, 7); d: (5, 3), (6, 7).\n\n\n# 3 METHODOLOGY\n\n\nPEPPP operates in two phases: meta-training and meta-test. First in meta-training, we learn configuration embeddings by training a neural network of a specific architecture at different low-precision configurations on different datasets. As listed in Appendix A, we study image classification tasks and a range of low-precision formats that vary in the number of bits for the exponent and mantissa for the activations, optimizer, and weights. Our hope is to avoid enumerating every possible configuration since exhaustive search is prohibitive in practice. To this end, we make a few measurements and then predict the other test errors by active learning techniques. We also compute the memory matrix to understand the memory consumption of each measurement.\n\n\nThen in meta-test, we assume that we have completed meta-training, and hence know the configuration embeddings. The meta-test goal is to predict the tradeoff between memory usage and test error on a new (meta-test) dataset. This step corresponds to inference in traditional machine learning; it must be quick and cheap to satisfy the needs of resource-constrained practitioners. To select the configuration that has smallest test error and takes less than the memory limit, we measure a few selected (informative and cheap) configurations on the meta-test dataset, and use the information of their test errors to predict the rest. The problem of choosing measurements is known as the \"cold-start\" problem in the literature on recommender systems. Then we use the model built on the meta-training datasets to estimate values for the other measurements on the meta-test dataset.\n\n\n# 3.1 META-TRAINING\n\n\nOn the  $n$  meta-training datasets, we first theoretically compute the memory needed to evaluate each of the  $d$  low-precision configurations to obtain the full memory matrix  $M \\in \\mathbb{R}^{n \\times d}$ . The total memory consists of memory needed for model weights, activations, and the optimizer (gradients, gradient accumulators, etc.) (Sohoni et al., 2019b). Among these three types, activations typically dominate, as shown in Figure 5. Thus using lower precision for activations drastically reduces memory usage. Additionally, empirical studies (Zhou et al., 2016) have shown that adopting higher precision formats for the optimizer can substantially improve the accuracy of the trained model. Since the optimizer usually requires a relatively small memory, it is often the best to use a higher precision format for this component.\n\n\nThus for each low-precision configuration, it is typical to use a lower precision for network weights and activations, and a higher precision for the optimizer, resulting in combinatorially many choices in total. An example of the memory usage in this low-precision scenario is also shown in Figure 5.\n\n\nIn practice, ML tasks are often correlated: for example, meta-training datasets might be subsets of a large dataset like CIFAR-100 or ImageNet. The ranking of different configurations tends to be similar on similar tasks. For example, we computed the test errors of 99 low-precision configurations on 87 datasets (both listed in Appendix A) to form a performance vector in  $\\mathbb{R}^{99}$  for each dataset. We use the Kendall tau correlation to characterize the alignment between the ranking of errors incurred by different configurations on two datasets: the Kendall tau correlation is 1 if the order is the same and -1 if the order is reversed. As shown in Figure 6(a), similar datasets have larger correlations: for example, datasets with indices 38-87 correspond to ImageNet subproblems such as distinguishing types of fruit or boat. Notice also that some dataset pairs have configuration performance rankings that are negatively correlated. The corresponding error matrix\n\n\n![Figure 5: Memory usage under two training paradigms. Both train a ResNet-18 on CIFAR-10 with batch size 32.](images/2a70afb2bf381683e5dc10e6cdfbcc2a69950b0735c8b1bf9ad622b0a2e2e63c.jpg)\n\n\n$E$  concatenates the performance vector for each dataset. It is not low rank, but its singular values decay rapidly, as shown in Figure 6(b). Hence we expect low rank approximation of this matrix from a few measurements to work well: it is not necessary to measure every configuration.\n\n\nIn the uniform sampling scheme, we sample measurements uniformly at random to obtain a partially observed error matrix  $P_{\\Omega}(E)$ . We then estimate the full error matrix  $E$  using a low rank matrix completion method to form estimate  $\\widehat{E}$ . In this paper, we use SOFTIMPUTE (Mazumder et al., 2010; Hastie et al., 2015) (Algorithm 1, Appendix B). Using the estimated error matrix  $\\widehat{E}$  and computed memory matrix  $M$ , we compute the Pareto frontier for each dataset to understand the (estimated) error-memory tradeoff. Section 4 Figure 10 shows an example.\n\n\nIn the non-uniform sampling scheme, we sample measurements with non-uniform probabilities, and use a weighted variant of SOFTIMPUTE, weighting by the inverse sampling probability, to complete the error matrix (Ma & Chen, 2019). Then we estimate\n\n\n![(a) dataset correlation](images/c05558055a70d7882859e1f1181e4f2652c2ddd1a7a9c80a98fee186323204a7.jpg)\n\n\n![(b) singular value decay Figure 6: Kendall tau correlation of test error of all configurations between all pairs of datasets, and singular value decay of corresponding error matrix. Strong correlations allow PEPP to succeed with a few measurements. Details in Appendix A.](images/c53c7c4f029ae09d435248a221efdf675b8eefc9e6f1432e9ede309d3db7c532.jpg)\n\n\nthe Pareto frontier in the same way as above. Non-uniform sampling is useful to reduce the memory needed for measurements. To this end, we construct a probability matrix  $P$  by entry-wise transformation  $P_{ij} = \\sigma (1 / M_{ij})$ , in which the monotonically increasing  $\\sigma : \\mathbb{R} \\rightarrow [0,1]$  maps inverse memory usage to a sampling probability. In this way, we make more low-memory measurements, and thus reduce the total memory usage.\n\n\n# 3.2 META-TEST\n\n\nHaving estimated the embedding of each configuration, our goal is to quickly compute the error-memory tradeoff on the meta-test dataset, avoiding the exhaustive search of all possible measurements.\n\n\nWe first compute the memory usage of each low-precision configuration on the meta-test dataset. With this information and guided by meta-training, we evaluate only a few (cheap but informative) configurations and predict the rest. Users then finally select the non-dominated configuration with highest allowable memory. An example of the process is shown in Figure 7.\n\n\nPEPPP uses Experiment Design with Matrix Factorization (ED-MF) described below to choose informative configurations. With more time, ED-MF evaluates more configurations, improving our estimates. We may also set a hard cutoff on memory: we do not evaluate low-precision configurations exceeding the memory limit. This setting has been studied in different contexts, and is called active learning or sequential decision making.\n\n\nED-MF picks measurements to minimize the variance of the resulting estimate. Specifically, we factorize the true (or estimated) error matrix  $E \\in \\mathbb{R}^{n \\times d}$  (or  $\\widehat{E}$ ) into its best rank- $k$  approximation, and get dataset embeddings  $X \\in \\mathbb{R}^{n \\times k}$  and configuration embeddings  $Y \\in \\mathbb{R}^{d \\times k}$  as  $E \\approx X^{\\top}Y$  (Fusi et al., 2018; Yang et al., 2019). On a meta-test dataset, we denote the error and memory vectors of the configurations as  $e^{\\mathrm{new}} \\in \\mathbb{R}^d$  and  $m^{\\mathrm{new}} \\in \\mathbb{R}^d$ , respectively. We model the error on the new dataset as  $e^{\\mathrm{new}} = Y^{\\top}x^{\\mathrm{new}} + \\epsilon \\in \\mathbb{R}^d$ , where  $x^{\\mathrm{new}} \\in \\mathbb{R}^k$  is the embedding of the new dataset and  $\\epsilon \\in \\mathbb{R}^d$  accounts for the errors from both measurement and low rank decomposition. We estimate the embedding  $x^{\\mathrm{new}}$  from a few measurements (entries) of  $e^{\\mathrm{new}}$  by least squares. Note that this requires at least  $k$  measurements on the meta-test dataset to make meaningful estimations, in which  $k$  is the rank for matrix factorization.\n\n\nIf  $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$ , the variance of the estimator is  $\\left(\\sum_{j\\in S}y_jy_j^\\top\\right)^{-1}$ , in which  $y_{j}$  is the  $j$ th column of  $Y$ . D-optimal experiment design selects measurements on the new dataset by minimizing (a scalarization of) the variance,\n\n\n![](images/8faf4c5a78abbc7492cb768b16d122c0456f5052d78ef06aa1e94989c80cd019.jpg)\n\n\nminimize  $\\log \\operatorname*{det}\\left(\\sum_{j\\in S}y_jy_j^\\top\\right)^{-1}$  subject to  $|S|\\leq l$ $S\\subseteq [d],$  (1)\n\n\nto find  $S$ , the set of indices of configurations to evaluate. The positive integer  $l$  bounds the number of measurements. Given a memory cap  $m_{\\mathrm{max}}$ , we replace the second constraint above by  $S \\subseteq T$ , in which  $T = \\{j \\in [d] \\mid m_j^{\\mathrm{new}} \\leq m_{\\mathrm{max}}\\}$  is the set of feasible configurations. Since Problem 1 is combinatorially hard, we may either relax it to a convex optimization\n\n\nproblem by allowing decision variables to have non-integer values between 0 and 1, or use a greedy method to incrementally choose measurements (initialized by measurements chosen by the column-pivoted QR decomposition (Gu & Eisenstat, 1996; Golub & Van Loan, 2012)). We compare these two approaches in Appendix C.\n\n\nFigure 7: Meta-test on CIFAR-10. After meta-training on all other datasets in Appendix A Table 2, we use ED-MF to choose six informative measurements (orange squares) with a 275MB memory limit for each measurement on CIFAR-10. Then we estimate test errors of other configurations by ED-MF, and restrict our attention to configurations that we estimate to be non-dominated (red x's). Note some of these are in fact dominated, since we plot true (not estimated) test error! Finally we select the estimated non-dominated configuration with highest allowable memory (blue square).\n\n\nIn Section 4.2, we compare ED-MF with competing techniques and show it works the best to estimate the Pareto frontier and select the final configuration.\n\n\n# 4 EXPERIMENTS AND DISCUSSIONS\n\n\nThe code for PEPPP and experiments is in the anonymous GitHub repository at https://github.com/barterer/lp. We use QPyTorch (Zhang et al., 2019) to simulate low-precision formats on standard hardware. The 87 datasets and 99 low-precision configurations in experiments are listed in Appendix A. The datasets consist of natural and medical images from various domains. Apart from CIFAR-10, the datasets include 20 CIFAR-100 partitions from mutually exclusive subsets, based on the superclass labels. They also include 50 subsets of ImageNet (Deng et al., 2009), which contains over 20,000 classes grouped into multiple major hierarchical categories like fungus and amphibian; each of the 50 datasets come from different hierarchies. Finally, we use 16 datasets from the visual domain benchmark (Wallace & Hariharan, 2020) to increase the diversity of domains.\n\n\nThe 99 low-precision configurations use mixed-precision as described in Section 3.1. Format A (for the activations and weights) uses 5 to 9 bits; Format B (for the optimizer) ranges from 14 to 20 bits. For each, these bits may be split arbitrarily between the exponent and mantissa. Across all configurations, we use ResNet-18, ResNet-34 and VGG (Simonyan & Zisserman, 2014) variants (11, 13, 16, 19) with learning rate 0.001, momentum 0.9, weight decay 0.0005 and batch size 32. Each training uses only 10 epochs as an early stopping strategy (Yao et al., 2007) to prevent overfitting. All the measurements take 35 GPU days on NVIDIA\u00ae GeForce\u00ae RTX 3090.\n\n\nDeveloping a method to select the above hyperparameters at the same time as the low-precision format is an important topic for future research, but not our focus here. In Section 4.3 we demonstrate how PEPP can naturally extend to efficiently select both optimization and low-precision hyperparameters.\n\n\nA number of metrics may be used to evaluate the quality of the obtained Pareto frontier in a multi-objective optimization problem (Wu & Azarm, 2001; Li et al., 2014; Audet et al., 2020): the distance between approximated and true frontiers (convergence), the uniformity of the distances between neighboring Pareto optimal points (uniformity), how well the frontier is covered by the approximated points (spread), etc. In our setting, the memory is accurate and the test error is estimated, so we do not have full control of the uniformity and spread of the 2-dimensional frontiers between test error and memory. As illustrated in Figure 8, we evaluate the quality of our estimated Pareto frontiers by the following two metrics:\n\n\nConvergence (Deb et al., 2002) between the sets of true and estimated Pareto optimal points  $\\mathcal{P}$ ,  $\\widehat{\\mathcal{P}}$  is  $\\frac{1}{|\\widehat{\\mathcal{P}}|} \\sum_{v \\in \\widehat{\\mathcal{P}}} \\mathrm{dist}(v, \\mathcal{P})$ , where the distance between point  $v$  and set  $\\mathcal{P}$  is  $\\mathrm{dist}(v, \\mathcal{P}) = \\min \\{\\|v - w\\| : w \\in \\mathcal{P}\\}$ . This is a surrogate for the distance between Pareto frontiers.\n\n\n![(a) convergence](images/c67b113bc487bd92647d5645c647b4cd6296ee97ad11f3dc882fa67db7fab9ae.jpg)\n\n\n![(b) HyperDiff](images/5e94b37ac20e45690800757e6b30ccee3e03578f362cc5e461902584a1c59ccc.jpg)\n\n\n![Figure 8: Illustration of Pareto frontier metrics. (a) Convergence is the average distance from each estimated Pareto optimal point to its closest true point: average  $(d_{1}, d_{2}, d_{3})$ . (b) HyperDiff is the absolute difference in area of feasible regions given by the true and estimated Pareto optimal points: the shaded area between Pareto frontiers.](images/3ebfaa8e4306bb7a51bb5d7bf7dbcc119f354612bb42fc125bdde32719147b2e.jpg)\n\n\nHypervolume difference (HyperDiff) (Zitzler & Thiele, 1998; Coello & Sierra, 1999; Wu & Azarm, 2001) is the absolute difference between the volumes of solution spaces dominated by the true and estimated Pareto frontiers. This metric improves with better convergence, uniformity, and spread. Its computation requires an upper boundary for each resource.\n\n\nWhen computing these metrics, we normalize the memories by proportionally scaling them to between 0 and 1. To evaluate the matrix completion performance, we use the relative error defined as  $\\|\\widehat{v} - v\\| / \\|v\\|$ , in which  $\\widehat{v}$  is the predicted vector and  $v$  is the true vector.\n\n\n# 4.1 META-TRAINING\n\n\nWe study the effectiveness of uniform and non-uniform sampling schemes. For simplicity, we regard all 87 available datasets as meta-training in this section. In SOFTIMPUte, we use  $\\lambda = 0.1$  (chosen by cross-validation from a logarithmic scale); and we choose a rank 5 approximation, which accounts for  $78\\%$  of the variance in the error matrix (as shown in Figure 6(b)).\n\n\nIn the uniform sampling scheme, we investigate how many samples we need for accurate estimates: we sample the error matrix at a number of different ratios, ranging from  $5\\%$  to  $50\\%$ , and complete the error matrix from each set of sampled measurements. In uniform sampling, the sampling ratio is also the percentage of memory we need to make the measurements in parallel, compared to exhaustive search. Hence we show the relationship between Pareto frontier estimation performance and sampling ratio in Figure 9. We can see the estimates are more accurate at larger sampling ratios, but sampling  $20\\%$  of the entries already suffices for good performance.\n\n\nAs a more intuitive example, Figure 10 shows the error-memory tradeoff on CIFAR-10. Compared to the estimated Pareto optimal points at sampling ratio  $5\\%$ , the ones at  $20\\%$  are closer to the true Pareto frontier (better convergence), lie more evenly (better uniformity) and cover the true Pareto frontier better (better spread), as shown by the respective convergence and HyperDiff values.\n\n\nThe non-uniform sampling scheme has a similar trend and is shown in Appendix E.2.\n\n\n# 4.2 META-LEAVE-ONE-OUT CROSS-VALIDATION (META-LOOCV)\n\n\nNow suppose that we have already collected measurements on the meta-training datasets to form a meta-training error matrix  $E$  (or its low rank approximation  $\\widehat{E}$ ). On the meta-test dataset, PEPP estimates the Pareto frontier by the active learning technique ED-MF. We compare ED-MF with a few other active learning techniques: Random selection with matrix factorization (RANDOM-MF),\n\n\n![(a) relative error](images/bf5ce4196bddea2bcb33a6aef3d9f9551f850f0657c707ae92eb767c1ecf54d8.jpg)\n\n\n![(b) convergence](images/c392a2d0c157d4d7eee8e70fbcfecc08bbc7a1477cf044f598e944a351b05a93.jpg)\n\n\n![(c) HyperDiff](images/07c86db75cf0491e2fbc5b4ca2da6277433f9aba22cd15ee15758bb487dcda77.jpg)\n\n\n![(a) sampling ratio  $5\\%$](images/6149c2b954562b7d9e16df93bb6cee488f27471b7cce98997eb272e9464f9adb.jpg)\n\n\n![(b) sampling ratio  $20\\%$](images/935cb5e34aba90c2fa5e3318d947b2f41232c361c7b16501bad4f88dbd8bc4da.jpg)\n\n\n![Figure 9: Pareto frontier estimation in PEPPP metatraining, with uniform sampling of configurations. The violins show the distribution of the performance on individual datasets, and the error bars (blue) show the range. The red error bars show the standard deviation of the error on CIFAR-10 across 100 random samples of the error matrix. Figure (a) shows the matrix completion error for each dataset; Figure (b) and (c) show the performance of the Pareto frontier estimates. Modest sampling ratios (around 0.1) already yield good performance. Figure 10: Error vs memory on CIFAR-10 with true and estimated Pareto frontiers from uniform sampling in PEPPP meta-training. A  $20\\%$  uniform sample of entries yields a better estimate of the Pareto frontier (convergence 0.03 and HyperDiff 0.02) compared to a  $5\\%$  sample (convergence 0.09 and HyperDiff 0.16).](images/51d898005fece4b8e3c2c6f60d56fa0881786c6819c847c441c2e8e5c1153887.jpg)\n\n\nQR decomposition with column pivoting and matrix factorization (QR-MF) and two Bayesian optimization techniques (BO-MF and BO-FULL), to understand whether the strong assumptions in ED-MF (low rank and Gaussian errors) are a hindrance or a help. An introduction to these techniques can be found in Appendix E.1.\n\n\nWe use rank 3 for matrix factorization:  $Y_{\\cdot ,j} \\in \\mathbb{R}^3$  for each  $j \\in [d]$ . In BO, we tune hyperparameters on a logarithmic scale and choose the RBF kernel with length scale 20, white noise with variance 1, and  $\\xi = 0.01$ . Table 1 shows the meta-LOOCV settings for each acquisition technique. We compare the techniques at a range of number of configurations to measure in each meta-LOOCV split,\n\n\nresembling what practitioners do in hyperparameter tuning: evaluate an informative subset and infer the rest. Setting I is the most basic, Setting IV and VI are the most practical. We only show results of Setting I and IV in the main paper, and defer the rest to Appendix E.\n\n\nIn Setting I, we do meta-LOOCV with the full meta-training error matrix in each split and do not cap the memory for meta-test. This means we evaluate every configuration on the meta-training datasets. We can see from Figure 11(a) and 11(b) that:\n\n- - ED-MF stably outperforms under both metrics, especially with fewer measurements.\n\n- - QR-MF overall matches the performance of ED-MF.\n\n- - BO-MF, BO-FULL and RANDOM-MF underperform ED-MF and QR-MF at lower memory usage, but often match their performance with higher memory.\n\n\nPractitioners may have only collected part of the meta-training performance, and desire or are limited by a memory cap when they do meta-test on the new dataset. In Setting IV, we cap the single-configuration memory usage for meta-test at 816MB, the median memory of all possible measurements across configurations and datasets. Additionally, we uniformly sample  $20\\%$  configurations from the meta-training error matrix in each split. In Figure 11(c) and 11(d), we can see similar trends as Setting I, except that QR-MF is slightly worse.\n\n\nUltimately, users would want to select a configuration that both achieves a small error and takes lower memory than the limit. As shown in Figure 2, PEPPP offers users the predicted Pareto frontier and chooses the non-dominated configuration with the highest allowable memory and hence the lowest error. We compare the acquisition techniques with the \"random high-memory\" baseline that randomly chooses a configuration that takes the highest allowable memory: an approach that follows the \"higher memory, lower error\" intuition. Figure 12 shows an example.\n\n\nTable 1: Meta-LOOCV experiment settings\n\n\n<table><tr><td>meta-training error matrix</td><td colspan=\"2\">memory cap on meta-test?</td></tr><tr><td></td><td>no</td><td>yes</td></tr><tr><td>full</td><td>I</td><td>II</td></tr><tr><td>uniformly sampled</td><td>III</td><td>IV</td></tr><tr><td>non-uniformly sampled</td><td>V</td><td>VI</td></tr></table>\n\n\n![(a) Setting I convergence](images/3ee79fd8a74f8ffaaab0159e33cef2fa4eb0643eb7cf6e3f1278f16e01acb89f.jpg)\n\n\n![(c) Setting IV convergence](images/42693977e879fa3cfff4b6b6cc18e8c5c90ddcf236516152caf01451925860f1.jpg)\n\n\n![(d) Setting IV HyperDiff](images/57fddde2422db373e4984d676196eaf74f026d5c6054a2a6afb4a7bf45651ded.jpg)\n\n\n![](images/ffccd97c3a5b3c40a464fa0dc3c3b36cf02b1c9865136fea6faaca803b37bff8.jpg)\n\n\n![(b) Setting I HyperDiff](images/a7566352a50ab9c0d4de4437675fccb810dd0adc609fdf35607a56002c813165.jpg)\n\n\nOverall, among all approaches and across all memory usage, ED-MF outperforms, especially at a smaller number of measurements. Compared to BO techniques, ED-MF also enjoys the benefit of having less hyperparameters to tune. Although techniques like QR-MF and RANDOM-MF are easier to implement, the additional cost of ED-MF is much smaller than the cost of making measurements: neural network training and testing.\n\n\n# 4.3 TUNING OPTIMIZATION HYPERPARAMETERS\n\n\nThe low-precision networks still underfit after 10 epochs of training. This situation is typical: underfitting due to budget constraints is unfortunately common in deep learning. Luckily, meta-learning the best precision will succeed so long as the validation errors are correctly ordered, even if they all overestimate the error of the corresponding fully trained model. Indeed, our validation errors correlate well with the errors achieved after further training: on CIFAR-\n\n\n10, the Kendall tau correlation between ResNet-18 errors at 10 epochs and 100 epochs is 0.73, shown in Figure 19. The lowest error at 100 epochs is  $9.7\\%$ , only approximately  $2\\%$  higher than SOTA (Bungert et al., 2021).\n\n\n![Figure 11: Pareto frontier estimates in meta-LOOCV Setting I and IV (with a  $20\\%$  meta-training sampling ratio and an 816MB meta-test memory cap). Each error bar is the standard error across datasets. The x axis measures the memory usage relative to exhaustively searching the permissible configurations. ED-MF consistently picks the configurations that give the best PF estimates. RANDOM-MF random highest-memory](images/99d0cc17daed4e499011157ccfefefaa99c0dc2adb0a34cf08ce60d374631ec5.jpg)\n\n\n![BO-MF BO-FULL](images/5f5882e4587b8709b044284e0b637e89cd70dabb2b0fbe108dfff43dcde0008a.jpg)\n\n\n![Figure 12: Relative performance with respect to ED-MF in meta-test Setting IV when making 3 measurements (memory usage  $\\sim 10\\%$ ) on 10 ImageNet partitions. ED-MF outperforms in most cases.](images/3175af61d969fa694d59d06b5d5f9aaa5b3ee2397158d982114d93f3b5a58cc1.jpg)\n\n\nPreviously, we have shown that PEPPP can estimate the error-memory tradeoff and select a promising configuration with other hyperparameters fixed. In practice, users may also want to tune hyperparameters like learning rate to achieve the lowest error. In Appendix E.4, we tune learning rate in addition to precision, and show that the methodology can be used in broader settings of hyperparameter tuning.\n\n\n# 4.4 META-LEARNING ACROSS ARCHITECTURES\n\n\nThe meta-learning in previous sections were conducted on ResNet-18. In Appendix E.5, we show that on 10 ImageNet partitions, PEPP is also capable of estimating the error-memory tradeoff of ResNet-34, VGG-11, VGG-13, VGG-16 and VGG-19 competitively. Moreover, the meta-learning across architectures works better than considering each architecture separately.\n\n\n# 5 CONCLUSION\n\n\nThis paper proposes PEPPP, a meta-learning system to select low-precision configurations that leverages training information from related tasks to efficiently pick the perfect precision given a memory budget. Built on low rank matrix completion with active learning, PEPPP estimates the Pareto frontier between memory usage and model performance to find the best low-precision configuration at each memory level. By reducing the cost of hyperparameter tuning in low-precision training, PEPPP allows practitioners to efficiently train accurate models.\n\n\n# REFERENCES\n\n- Charles Audet, Jean Bigeon, Dominique Cartier, S\u00e9bastien Le Digabel, and Ludovic Salomon. Performance indicators in multiobjective optimization. European journal of operational research, 2020.\n\n- Stephen R Becker, Emmanuel J Cand\u00e8s, and Michael C Grant. Templates for convex cone problems with applications to sparse signal recovery. Mathematical programming computation, 3(3):165, 2011.\n\n- C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer, 2006. ISBN 9780387310732.\n\n- Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n\n- Leon Bungert, Tim Roith, Daniel Tenbrinck, and Martin Burger. A bregman learning framework for sparse neural networks. arXiv preprint arXiv:2105.04319, 2021.\n\n- Zhaowei Cai and Nuno Vasconcelos. Rethinking differentiable search for mixed-precision neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2349-2358, 2020.\n\n- Carlos A Coello Coello and Margarita Reyes Sierra. Multiobjective evolutionary algorithms: classifications, analyses, and new innovations. In Evolutionary Computation. CiteSeer, 1999.\n\n- Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks with low precision multiplications. arXiv preprint arXiv:1412.7024, 2014.\n\n- Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher R Aberger, Kunle Olokotun, and Christopher R\u00e9. High-accuracy low-precision training. arXiv preprint arXiv:1803.03383, 2018.\n\n- Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation, 6(2):182-197, 2002.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255. IEEE, 2009.\n\n- Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.\n\n- Nicolo Fusi, Rishit Sheth, and Melih Elibol. Probabilistic matrix factorization for automated machine learning. In Advances in neural information processing systems, pp. 3348-3357, 2018.\n\n- Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU press, 2012.\n\n- Ming Gu and Stanley C Eisenstat. Efficient algorithms for computing a strong rank-revealing qr factorization. SIAM Journal on Scientific Computing, 17(4):848-869, 1996.\n\n- Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International conference on machine learning, pp. 1737-1746. PMLR, 2015.\n\n- David A Harville. Matrix algebra from a statistician's perspective, 1998.\n\n- Soheil Hashemi, Nicholas Anthony, Hokchhay Tann, R Iris Bahar, and Sherief Reda. Understanding the impact of precision quantization on the accuracy and energy of neural networks. In Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017, pp. 1474-1479. IEEE, 2017.\n\n- Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix completion and low-rank svd via fast alternating least squares. The Journal of Machine Learning Research, 16(1):3367-3402, 2015.\n\n- Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020.\n\n- Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439, 2020.\n\n- Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455-492, 1998.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\n- Hamed F Langroudi, Zachariah Carmichael, David Pastuch, and Dhireesha Kudithipudi. Cheetah: Mixed low-precision hardware & software co-design framework for dnns on the edge. arXiv preprint arXiv:1908.02386, 2019.\n\n- Christiane Lemke, Marcin Budka, and Bogdan Gabrys. Metalearning: a survey of trends and technologies. Artificial intelligence review, 44(1):117-130, 2015.\n\n- Miqing Li, Shengxiang Yang, and Xiaohui Liu. Diversity comparison of Pareto front approximations in many-objective optimization. IEEE Transactions on Cybernetics, 44(12):2568-2584, 2014.\n\n- Wei Ma and George H Chen. Missing not at random in matrix completion: The effectiveness of estimating missingness probabilities under a low nuclear norm assumption. In Advances in Neural Information Processing Systems, volume 32, pp. 14900-14909, 2019.\n\n- Vivek Madan, Mohit Singh, Uthaipon Tantipongpipat, and Weijun Xie. Combinatorial algorithms for optimal design. In Conference on Learning Theory, pp. 2210-2258. PMLR, 2019.\n\n- Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. The Journal of Machine Learning Research, 11:2287-2322, 2010.\n\n- Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.\n\n- Jonas Mo\u010dkus. On Bayesian methods for seeking the extremum. In *Optimization techniques IFIP technical conference*, pp. 400-404. Springer, 1975.\n\n- Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. arXiv preprint arXiv:1907.10597, 2019.\n\n- Jack Sherman and Winifred J Morrison. Adjustment of an inverse matrix corresponding to a change in one element of a given matrix. The Annals of Mathematical Statistics, 21(1):124-127, 1950.\n\n- Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\n- Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczy\u0144ski, Jian Zhang, and Christopher R\u00e9. Low-memory neural network training: A technical report. arXiv preprint arXiv:1904.10631, 2019a.\n\n- Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczy\u0144ski, Jian Zhang, and Christopher R\u00e9. Low-memory neural network training: A technical report, 2019b.\n\n- Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi (Viji) Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. Advances in Neural Information Processing Systems, 33, 2020.\n\n- Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018.\n\n- Bram Wallace and Bharath Hariharan. Extending and analyzing self-supervised learning across domains. In European Conference on Computer Vision, pp. 717-734. Springer, 2020.\n\n- Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt Keutzer. Mixed precision quantization of convnets via differentiable neural architecture search. arXiv preprint arXiv:1812.00090, 2018.\n\n- Jin Wu and Shapour Azarm. Metrics for quality assessment of a multiobjective design optimization solution set. J. Mech. Des., 123(1):18-25, 2001.\n\n- Chengrun Yang, Yuji Akimoto, Dae Won Kim, and Madeleine Udell. Oboe: Collaborative filtering for automl model selection. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1173-1183, 2019.\n\n- Chengrun Yang, Jicong Fan, Ziyang Wu, and Madeleine Udell. Automl pipeline selection: Efficiently navigating the combinatorial space. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1446-1456, 2020.\n\n- Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289-315, 2007.\n\n- Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. The zipml framework for training models with end-to-end low precision: The cans, the cannons, and a little bit of deep learning. arXiv preprint arXiv:1611.05402, 2016.\n\n- Tianyi Zhang, Zhiqiu Lin, Guandao Yang, and Christopher De Sa. QPyTorch: A low-precision arithmetic simulation framework, 2019.\n\n- Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.\n\n- Eckart Zitzler and Lothar Thiele. Multiobjective optimization using evolutionary algorithms\u2014a comparative case study. In International conference on parallel problem solving from nature, pp. 292-301. Springer, 1998.\n\n\nAll the tables in this appendix are shown in the last page.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636563.2644715", "msecs": "264.0", "relativeCreated": "23957.61203765869", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Calibration-then-Calculation: A Variance Reduced Metric Framework\n\n# Abstract\nDeep learning has been widely adopted across various fields, but there has been little focus on evaluating the performance of deep learning pipelines. With the increased use of large datasets and complex models, it has become common to run the training process only once and compare the result to previous benchmarks. However, this procedure can lead to imprecise comparisons due to the variance in neural network evaluation metrics. The metric variance comes from the randomness inherent in the training process of deep learning pipelines. Traditional solutions such as running the training process multiple times are usually not feasible in deep learning due to computational limitations. In this paper, we propose a new metric framework, Calibrated Loss, that addresses this issue by reducing the variance in its vanilla counterpart. As a result, the new metric has a higher accuracy to detect effective modeling improvement. Our approach is supported by theoretical justifications and extensive experimental validations in the context of Deep Click-Through Rate Prediction Models and Image Classification Models.\n\n\n# 1 INTRODUCTION\n\n\nThe progress in machine learning is largely influenced by experimental outcomes, particularly in the era of deep learning. Researchers often evaluate the performance of new methods by comparing them to previous benchmark results, in order to demonstrate their superiority. However, it is well known that the performance of deep learning models can vary greatly, even when using the same pipeline (Picard, 2021), where, in this work, we define the pipeline broadly, which includes but is not limited to the selection of feature sets, model architectures, optimization algorithms, initialization schemes, and hyper-parameters. This variability poses a significant challenge to evaluating deep learning methods because it is non-trivial to determine whether the performance gain is a result of the proposed method or simply due to randomness.\n\n\nIn fact, it has been shown that by selecting a fortunate random initialization seed, one can achieve a model performance that is significantly better than average (Picard, 2021). This difference can be substantial enough to be used as a strong argument for publications in selective venues (Picard, 2021).\n\n\nThis issue is exacerbated in industry where the production model performance is hard to get improvement, while there are hundreds of machine learning engineers working on the same model at the same time. The performance gain of a modeling proposal is usually small and within the metric variance, making it difficult to judge the effectiveness of the modeling proposal.\n\n\nTo address this issue, a common approach is to run the training pipeline multiple times and report the average, standard deviation, minimum, and maximum performance scores (Picard, 2021). However, with the rise of large training data and big models, this approach is not always practical due to limited computational resources (Bouthillier & Varouquaux, 2020).\n\n\nIn this work, we take a different approach to solving this problem by designing a new metric that can evaluate proposed deep learning methods more reliably. This new metric has a smaller variance and achieves more accurate comparisons between different deep learning pipelines. We validate our approach in the context of Deep Click-Through Rate Prediction Models and Image Classification Models.\n\n\nOur main contributions are:\n\n- 1) Formulating the deep learning pipeline evaluation problem and proposing to tackle it by designing new metrics.\n\n- 2) Proposing a new metric framework, Calibrated Loss, which can mitigate the above deep learning pipeline evaluation issue.\n\n- 3) Conducting extensive experiments to demonstrate the effectiveness of the proposed metric, using synthetic dataset, ads click dataset and image classification dataset.\n\n- 4) Providing theoretical guarantees under a particular setting of linear regression that the proposed metric has a smaller variance than its vanilla counterpart.\n\n\n# 2 PRELIMINARIES AND PROBLEM SETTING\n\n\nIn this work, we are examining the standard supervised learning setting, where we assume that the training data and test data are randomly drawn from an unknown distribution in an i.i.d. manner, denoted as  $\\mathcal{D}$ .\n\n\nOur goal is to develop a good pipeline that maps from a training distribution to a possibly random model,  $h \\in \\mathcal{H}$ , that generalizes well during the test time. As we mentioned in Introduction, the pipeline includes the whole procedures of training a model, including the selection of model architectures, optimization algorithms, initialization schemes, and hyper-parameters. Model performance is evaluated by a metric,  $e$ , and thus the expected performance of a model  $h$  during the test time is\n\n\n$$\n\n$$\nR _ {e} (h) = \\mathbb {E} _ {\\mathcal {D}} [ e (h (X), Y) | h ]. \\tag {1}\n$$\n\n$$\n\n\nIn practice,  $R_{e}(h)$  is estimated by finite-sample average on the test dataset  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$ . That is,\n\n\n$$\n\n$$\n\\hat {R} _ {e} (h, \\hat {\\mathcal {D}} _ {\\text {t e s t}}) = \\frac {1}{| \\hat {\\mathcal {D}} _ {\\text {t e s t}} |} \\sum_ {(x, y) \\in \\hat {\\mathcal {D}} _ {\\text {t e s t}}} e (h (x), y). \\tag {2}\n$$\n\n$$\n\n\nIt's worth noting that the expected risk,  $R_{e}(h)$ , is a random variable, as  $h$  is random and it depends on a specific model that is produced by the underlying deep learning pipeline. The output model is random due to data randomness from the sample collection and intrinsic randomness during the training process in the deep learning pipeline, such as data order and randomness from stochastic gradient descent. Therefore, a proper evaluation and comparison of different deep learning pipelines should take into account the distribution of  $R_{e}(h)$  (Bouthillier et al., 2021). It's also important to note that the term \"deep learning pipeline\" in this context is general, as we consider different model configurations (e.g. different model hyperparameters) as different \"deep learning pipelines\", even though they may belong to the same model class.\n\n\nTo compare the performance of different deep learning pipelines, we should compare the distribution of  $R_{e}(h,\\mathcal{D}_{\\mathrm{test}})$ . When comparing two pipelines,  $A$  and  $B$ , we say that pipeline  $A$  is better than pipeline  $B$  with respect to metric  $e$  if and only if the probability that pipeline  $A$  produces a better model (i.e. smaller risk), measured by the metric  $e$ , is greater than 0.5. This is represented by the inequality:\n\n\n$$\n\n$$\nP \\left(R _ {e} \\left(h _ {A}\\right) <   R _ {e} \\left(h _ {B}\\right)\\right) > 0. 5, \\tag {3}\n$$\n\n$$\n\n\nwhere  $h_A$  and  $h_B$  are random variables representing the output models produced by pipeline  $A$  and  $B$  respectively.\n\n\nOur objective is to compare the performance of two pipelines,  $A$  and  $B$ , with respect to the metric  $e$  by running the training pipeline only once. Ideally, to estimate  $P(R_{e}(h_{A}) < R_{e}(h_{B}))$ , we could use the Monte Carlo method, but this requires a huge amount of computational resources which is not realistic. In this work, we propose a different approach: we aim to come up with an alternative metric that has roughly the same mean but a smaller variance, i.e., for a new metric  $e_1$ , we would like to have\n\n\n$$\n\n$$\n\\mathbb {E} \\left[ R _ {e _ {1}} (h) \\right] \\approx \\mathbb {E} \\left[ R _ {e} (h) \\right] \\text {a n d} v a r \\left(R _ {e _ {1}} (h)\\right) <   v a r \\left(R _ {e} (h)\\right),\n$$\n\n$$\n\n\nwhere the randomness is from the pipeline producing  $h$ . As a result, the new metric is able to compare the performance of pipelines  $A$  and  $B$  with limited computational resources more accurately.\n\n\nAssuming that pipeline  $A$  is better than  $B$  with respect to the metric  $e$  (i.e. pipeline  $A$  is more likely to produce a better model than pipeline  $B$  in the ground truth if measured by metric  $e$ ), we say that a metric  $e_1$  is better than  $e_2$  with respect to metric  $e$  if and only if the probability that pipeline  $A$  produces a better model than pipeline  $B$  measured by metric  $e_1$  is greater than the probability measured by metric  $e_2$ . This is represented by the inequality:\n\n\n$$\n\n$$\nP \\left(R _ {e _ {1}} \\left(h _ {A}\\right) <   R _ {e _ {1}} \\left(h _ {B}\\right)\\right) > P \\left(R _ {e _ {2}} \\left(h _ {A}\\right) <   R _ {e _ {2}} \\left(h _ {B}\\right)\\right) \\tag {4}\n$$\n\n$$\n\n\nIn other words, using metric  $e_1$  is more likely to accurately detect that pipeline  $A$  is better than pipeline  $B$ , which aligns with the ground truth. Here, we allow for a general form of the risk function, which may not admit the expectation form; i.e.,  $R_{e_1}(h)$  may not necessarily have the form  $\\mathbb{E}_{\\mathcal{D}}[e_1(h(X),Y)]$ .\n\n\nHere, we assume without loss of generality that pipeline  $A$  is better than  $B$  with respect to the metric  $e$ . We define the accuracy of a metric  $\\bar{e}$  with respect to metric  $e$  and pipeline  $A$  and  $B$  as:\n\n\n$$\n\n$$\n\\operatorname {A c c} (\\bar {e}) \\triangleq P \\left(R _ {\\bar {e}} \\left(h _ {A}\\right) <   R _ {\\bar {e}} \\left(h _ {B}\\right)\\right). \\tag {5}\n$$\n\n$$\n\n\nOur goal is to find a metric  $\\bar{e}$  associated with the risk function  $R_{\\bar{e}}$  that has higher accuracy than the original metric  $e$  for a wide range of pipelines  $A$  and  $B$ . In the next section, we will present a new metric framework, Calibrated Loss. In the context of Deep Click-Through Rate Prediction Models, a special instance of Calibrated Loss, Calibrated Log Loss achieves higher accuracy than the vanilla Log Loss. The intuition is that the bias in the function  $h$  is always volatile and carries on a large amount of randomness. Calibrating the bias will usually not change the comparison between two pipelines but can reduce the randomness. In Section 4, we will present a theoretical analysis that justifies this intuition by showing that our new metric has a smaller variance under the linear regression setting. Through extensive experiments in Section 5, we will show that Calibrated Log Loss achieves higher accuracy than Log Loss for a wide range of pipelines, including those with different batch sizes, number of features, model architectures, regularization weights, model sizes, etc.\n\n\n# 3 CALIBRATED LOSS FRAMEWORK\n\n\nCalibrated Log Loss (Binary Classification) In the field of Deep Click-Through Rate Prediction Models, it is common for models to overfit when trained for more than one epoch (Zhou et al., 2018; Zhang et al., 2022). As a result, models are often only trained for a single epoch in practice (Zhang et al., 2022), leaving it uncertain if the model has been fully optimized. This leads to the volatility of the bias term in the final layer of neural networks, creating additional randomness. To address this issue, we propose the following risk function:\n\n\n$$\n\n$$\nR _ {e _ {1}} (h) = \\min _ {c} \\mathbb {E} _ {\\mathcal {D}} [ Y \\log (h ^ {c} (X)) + (1 - Y) \\log (1 - h ^ {c} (X)) | h ],\n$$\n\n$$\n\n\nwhere  $h^c (X) = (1 + e^{-\\log \\mathrm{it}(h(X)) + c})^{-1}$  and  $\\mathrm{logit}(p) = \\log (p / (1 - p))$ .\n\n\nTo execute the aforementioned procedure on a finite-sample test set, we split the test data  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$  into two parts: a test-validation dataset  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and a remaining test dataset  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ . By using the test-validation dataset  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ , we are able to correct the bias term, and then calculate the Log Loss using  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$  with the bias-adjusted predictions. This is referred to as Calibrated Log Loss. The calculation procedure is outlined in Algorithm 1.\n\n\nMathematically speaking, we define bias-adjusted predictions as:  $q_{i} = h^{c}(x_{i})$  for  $x_{i}$  in the test set where  $c$  is the bias-adjusted term we are optimizing.\n\n\nTo optimize  $c$ , the following optimization program is solved, which is the log loss between bias-adjusted predictions  $q_{i}$  and labels  $y_{i}$ :\n\n\n$$\n\n$$\n\\min  _ {c} \\left\\{- \\sum_ {\\substack {(x, y) \\in \\\\ \\hat {\\mathcal {D}} _ {\\text {val - test}}}} (y \\log \\left(h ^ {c} (x)\\right) + (1 - y) \\log \\left(1 - h ^ {c} (x)\\right)) \\right\\}. \\tag{6}\n$$\n\n$$\n\n\nIt can be easily shown that, after optimization, the bias-adjusted predictions  $q_{i}$  are well-calibrated in  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ , meaning that  $\\sum_{i\\in \\hat{\\mathcal{D}}_{\\mathrm{val - test}}}q_i = \\sum_{i\\in \\hat{\\mathcal{D}}_{\\mathrm{val - test}}}y_i$ .\n\n\nLet  $c^*$  be the minimizer of the optimization problem (6). The final risk and metrics are\n\n\n$$\n\n$$\n\\hat{R}_{e_{1}}(h,\\hat{\\mathcal{D}}_{\\text{test}}) = \\frac{1}{|\\hat{\\mathcal{D}}_{\\text{remaining - test}}|}\\sum_{\\substack{(x,y)\\in \\\\ \\hat{\\mathcal{D}}_{\\text{remaining - test}}}}e_{1}(h(x),y),\n$$\n\n$$\n\n\nand\n\n\n$$\n\n$$\ne _ {1} (h (x), y) = y \\log (h ^ {c ^ {*}} (x)) + (1 - y) \\log (1 - h ^ {c ^ {*}} (x)).\n$$\n\n$$\n\n\nExplanations The optimization problem (6) corrects the bias term of original predictions  $h(x)$  using test-validation dataset  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ . The bias-adjusted predictions  $h^{c^*}(x)$  is guaranteed to be well-calibrated in  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$ , hence the name Calibrated Log Loss.\n\n\n# ALGORITHM 1 CALCULATE CALIBRATED LOG LOSS\n\n- 1: Input: Model  $M$ , labeled test data  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$\n\n- 2: Output: Calibrated Log Loss:  $\\dot{R}_{e_1}(h, \\mathcal{D}_{\\mathrm{test}})$ .\n\n- 3: Partition  $\\hat{\\mathcal{D}}_{\\mathrm{test}}$  into  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ .\n\n- 4: Compute model predictions on  $\\hat{D}_{\\mathrm{val - test}}$  and  $\\hat{D}_{\\mathrm{remaining - test}}$ , and obtain the model predictions  $p_i^{\\mathrm{val - test}}$  and  $p_i^{\\mathrm{remaining - test}}$ .\n\n- 5: Solve the optimization problem (6) using  $p_i^{\\mathrm{val - test}}$  and  $y_{i}^{\\mathrm{val - test}}$  and obtain the learned bias term  $c^{*}$ .\n\n- 6: Calculate bias-adjusted predictions  $q_{i}^{\\mathrm{remaining - test}}$  using formula  $q_{i} = h^{c^{*}}(x_{i})$ .\n\n- 7: Calculate the Calibrated Log Loss  $\\hat{R}_{e_1}(h,\\hat{\\mathcal{D}}_{\\mathrm{test}})$  as the Log Loss of  $q_{i}^{\\mathrm{remaining - test}}$  and  $y_{i}^{\\mathrm{remaining - test}}$ .\n\n\nGeneralization to Multiclass Classification Instead of optimizing problem (6) to calculate calibrated predictions, we use \"Temperature Scaling\" method proposed in (Guo et al., 2017a). The rest are exactly the same.\n\n\nGeneralization to Quadratic Loss Calibrated Quadratic Loss is calculated in a similar manner as Calibrated Log Loss, i.e. first perform calibration on  $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and calculate bias-adjusted predictions on  $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ . Here, we define the Quadratic Loss and Calibrated Quadratic Loss:\n\n\n$$\n\n$$\ne (h (x), y) = (y - h (x)) ^ {2}, \\text {a n d}\n$$\n\n$$\n\n\n$$\n\n$$\ne _ {1} (h (x), y) = (y - h (x) - (\\mathbb {E} _ {\\mathcal {D}} [ Y ] - \\mathbb {E} _ {\\mathcal {D}} [ h (X) | h ]) ^ {2}).\n$$\n\n$$\n\n\n# 4 THEORY ON LINEAR REGRESSION\n\n\nIn this section, we provide theoretical justification that our new metric has a smaller variance than its vanilla counterpart under Linear Regression setting, where the randomness only comes from the data randomness. We choose to provide a theoretical guarantee under Linear Regression due to its simplicity. We empirically verify our method's performance under Logistic Regression and Neural Networks in the next section. Note that in Linear Regression, Quadratic Loss is used instead of Log Loss. As a result, in our theory, we compare the variance of Calibrated Quadratic Loss with vanilla Quadratic Loss.\n\n\nTheorem 4.1. Suppose that the features  $X \\in \\mathbb{R}^d$  and the label  $Y$  are distributed jointly Gaussian. We consider linear regression  $h(x) = \\beta^\\top x + \\alpha$ . Let  $\\hat{\\beta}_n$  be the coefficient learned from the training data with sample size  $n$ . Then, we have\n\n\n$$\n\n$$\n\\left(1 + \\frac {1}{n}\\right) \\mathbb {E} [ e _ {1} (h (X), Y) | \\hat {\\beta} _ {n} ] = \\mathbb {E} [ e (h (X), Y) | \\hat {\\beta} _ {n} ],\n$$\n\n$$\n\n\nwhere the expectation is taken over the randomness over both the training and test samples.\n\n\nLet  $\\hat{\\alpha}_n$  be the learned intercept. Note that the original risk and the calibrated risk are\n\n\n$$\n\n$$\n\\begin{array}{l} R _ {e} (h) = \\mathbb {E} \\left[ e (h (X), Y) | \\hat {\\beta} _ {n}, \\hat {\\alpha} _ {n} \\right], \\text {a n d} \\\\ R _ {e _ {1}} (h) = \\mathbb {E} [ e _ {1} (h (X), Y) | \\hat {\\beta} _ {n}, \\hat {\\alpha} _ {n} ] = \\mathbb {E} [ e _ {1} (h (X), Y) | \\hat {\\beta} _ {n} ]. \\\\ \\end{array}\n$$\n\n$$\n\n\nTherefore, Theorem 4.1 implies that\n\n\n$$\n\n$$\n(1 + 1 / n) \\mathbb {E} \\left[ R _ {e _ {1}} (h) \\right] = \\mathbb {E} \\left[ R _ {e} (h) \\right].\n$$\n\n$$\n\n\nFurthermore, to make  $e$  and  $e_1$  comparable, we should scale  $e_1$  to  $(1 + 1/n)e_1$ . We demonstrate that after scaling,  $(1 + 1/n)R_{e_1}(h)$  has a smaller variance than  $R_e(h)$  in the next corollary. In practice, as  $(1 + 1/n)$  is a constant as long as the training sample size is fixed, we can directly compare two pipelines using  $R_{e_1}(h)$ .\n\n\nCorollary 4.2. Suppose that  $h_1(x)$  and  $h_2(x)$  are two different learned linear functions in different feature sets. Then, we have\n\n\n$$\n\n$$\n\\mathbb {E} \\left[ R _ {e} \\left(h _ {1}\\right) \\right] = \\mathbb {E} \\left[ R _ {e} \\left(h _ {2}\\right) \\right] \\Leftrightarrow \\mathbb {E} \\left[ R _ {e _ {1}} \\left(h _ {1}\\right) \\right] = \\mathbb {E} \\left[ R _ {e _ {1}} \\left(h _ {2}\\right) \\right] \\tag {7}\n$$\n\n$$\n\n\nand\n\n\n$$\n\n$$\n\\operatorname {v a r} \\left(\\left(1 + 1 / n\\right) R _ {e _ {1}} (h)\\right) <   \\operatorname {v a r} \\left(R _ {e} (h)\\right)\n$$\n\n$$\n\n\nfor any  $h$  learned from linear regression.\n\n\nCorollary 4.2 indicates that Calibrated Quadratic Loss has a smaller variance than vanilla Quadratic Loss without changing the mean after appropriate scaling. Note that smaller variance and higher accuracy (Inequality 4) are highly correlated under mild conditions, but smaller variance alone does not guarantee higher accuracy. In the next section, we will empirically demonstrate that the new metric has a smaller variance and achieves higher accuracy. All proofs can be found in Appendix A.\n\n\n# 5 EXPERIMENT RESULTS\n\n\n# 5.1 ESTIMATION OF ACCURACY\n\n\nRecall that accuracy of a metric  $\\bar{e}$  is defined as:\n\n\n$$\n\n$$\n\\operatorname {A c c} (\\bar {e}) \\triangleq P \\left(R _ {\\bar {e}} \\left(h _ {A}\\right) <   R _ {\\bar {e}} \\left(h _ {B}\\right)\\right). \\tag {8}\n$$\n\n$$\n\n\nTo get an estimation of  $\\mathrm{Acc}(\\bar{e})$ , we run pipelines  $A$  and  $B$  for  $m$  times, obtaining models  $h_{A_i}$  and  $h_{B_i}$  for  $i \\in [m]$ .  $\\mathrm{Acc}(\\bar{e})$  can be estimated as:\n\n\n$$\n\n$$\n\\widehat {\\operatorname {A c c}} (\\bar {e}) = \\frac {1}{m ^ {2}} \\sum_ {(i, j)} \\mathbb {1} \\left(\\hat {R} _ {\\bar {e}} \\left(h _ {A _ {i}}, \\hat {\\mathcal {D}} _ {\\text {t e s t}}\\right) <   \\hat {R} _ {\\bar {e}} \\left(h _ {B _ {j}}, \\hat {\\mathcal {D}} _ {\\text {t e s t}}\\right)\\right) \\tag {9}\n$$\n\n$$\n\n\n$\\widehat{\\mathrm{Acc}} (\\bar{e})$  is an unbiased estimator of  $\\mathrm{Acc}(\\bar{e})$ , and in the experiments below, we report  $\\widehat{\\mathrm{Acc}} (\\bar{e})$  as our accuracy metric. In all the tables in this section, without loss of generality, we write the tables as pipeline A is better than pipeline B in the sense of  $P(R_{e}(h_{A}) < R_{e}(h_{B})) > 0.5$ .\n\n\n# 5.2 SYNTHETIC DATA\n\n\nIn Appendix B, we consider a linear regression model to give empirical evidence to support our theory. We further consider logistic regression model to demonstrate the effectiveness of Calibrated Log Loss in synthetic data setting. All the details and results can be found in Appendix B.\n\n\n# 5.3 AVAZU CTR PREDICTION DATASET\n\n\nDataset The Avazu CTR Prediction dataset  ${}^{1}$  is a common benchmark dataset for CTR predictions. Due to computational constraints in our experiments, we use the first 10 million samples, shuffle the dataset randomly, and split the whole dataset into  ${80}\\% {\\widehat{D}}_{\\text{train }},2\\% {\\widehat{D}}_{\\text{val - test }}$  and  ${18}\\% {\\widehat{D}}_{\\text{remaining - test }}$  .\n\n\nBase Model We use the xDeepFM model (Lian et al., 2018) open sourced in Shen (2017) as our base model. We primarily conduct experiments using xDeepFM models, including hyperparameter related experiments and feature related experiments. To demonstrate our new metric can also handle comparisons between different model architectures, we also conduct experiments using DCN (Wang et al., 2017), DeepFM (Guo et al., 2017b), FNN (Zhang et al., 2016), and DCNMix (Wang et al., 2021).\n\n\nExperiment Details We consider neural networks with different architectures, different training methods, different hyper-parameters, and different levels of regularization as different pipelines. Such comparisons represent common practices for research and development in both industry and academia. For each pipeline, we train the model 60 times with different initialization seeds and data orders to calculate  $\\widehat{\\mathrm{Acc}} (\\bar{e})$ . Note that we use \"Log Loss\" as our ground truth metric to determine the performance rank of different pipelines. Due to computational constraints, we cannot afford to run the experiments for multiple rounds. Instead, we run the experiments for one round and report accuracy. Note that in the neural network experiments, we do not re-sample the training data each time, as there is intrinsic randomness in the neural network training process. This is the main difference from the Linear Regression and Logistic Regression experiments.\n\n\nPipelines with Different Number of Features In this set of experiments, for pipeline  $A$ , we use all the features available. For pipeline  $B$ , we remove some informative features. We tested the removal of 6 dense features and 1 sparse features respectively.\n\n\nTable 1: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (features)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>Baseline</td><td>remove dense</td><td>81.8%</td><td>88.8%</td></tr><tr><td>Baseline</td><td>remove sparse</td><td>78.6%</td><td>85.9%</td></tr></table>\n\n\nTable 2: Mean and Standard Deviation of Log Loss and Calibrated Log Loss under Neural Networks (features)\n\n\n<table><tr><td>Pipeline</td><td>Log Loss Mean</td><td>Calibrated Log Loss Mean</td><td>Log Loss Std</td><td>Calibrated Log Loss Std</td></tr><tr><td>dense</td><td>0.37408</td><td>0.37403</td><td>0.00047</td><td>0.00038</td></tr><tr><td>sparse</td><td>0.37404</td><td>0.37398</td><td>0.0005</td><td>0.00042</td></tr></table>\n\n\nFrom the result in Table 1, we can clearly see that Calibrated Log Loss has a higher accuracy, indicating its effectiveness when comparing the performance of pipelines with different features.\n\n\nFrom the result in Table 2, we can see that Calibrated Log Loss has a smaller standard deviation (16% - 19% smaller) while the mean of Log Loss and Calibrated Log Loss is almost on par (within 0.02% difference).\n\n\nPipelines with Different Model Architectures In this set of experiments, for pipeline  $A$ , we use one model architecture. For pipeline  $B$ , we use another model architecture. We tested a variety of different model architectures, including DCN (Wang et al., 2017), DeepFM (Guo et al., 2017b), FNN (Zhang et al., 2016), and DCNMix (Wang et al., 2021).\n\n\nTable 3: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (model architectures)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>DCN</td><td>DCNMix</td><td>64.4%</td><td>71.5%</td></tr><tr><td>DeepFM</td><td>DCN</td><td>77.2%</td><td>83.9%</td></tr><tr><td>DeepFM</td><td>FNN</td><td>76.9%</td><td>79.9%</td></tr><tr><td>FNN</td><td>DCNMix</td><td>61.5%</td><td>72.0%</td></tr><tr><td>DeepFM</td><td>DCNMix</td><td>84.8%</td><td>93.4%</td></tr></table>\n\n\nFrom the result in Table 3, we can clearly see that Calibrated Log Loss has higher accuracy, again indicating its effectiveness when comparing the performance of pipelines with different model architectures. In Table 4, we report the mean and standard deviation of Log Loss and Calibrated Log Loss, consistent with previous results.\n\n\nTable 4: Mean and Standard Deviation of Log Loss and Calibrated Log Loss under Neural Networks (model architectures)\n\n\n<table><tr><td>Pipeline</td><td>Log Loss Mean</td><td>Calibrated Log Loss Mean</td><td>Log Loss Std</td><td>Calibrated Log Loss Std</td></tr><tr><td>DCN</td><td>0.38021</td><td>0.38011</td><td>0.00044</td><td>0.00033</td></tr><tr><td>DeepFM</td><td>0.37971</td><td>0.3796</td><td>0.00059</td><td>0.00037</td></tr><tr><td>FNN</td><td>0.38029</td><td>0.38006</td><td>0.00064</td><td>0.0004</td></tr><tr><td>DCNMix</td><td>0.38046</td><td>0.38037</td><td>0.00048</td><td>0.00034</td></tr></table>\n\n\nPipelines with Different Model Hyperparameters In this set of experiments, we compare pipelines with different model hyperparameters, including neural network layer size, Batch Normalization (BN) (Ioffe & Szegedy, 2015), Dropout (Srivastava et al., 2014), and regularization weight.\n\n\nIn the first experiment, we compare a pipeline using a baseline model size with a pipeline using a smaller model size. In the second experiment, we compare a pipeline using Batch Normalization with a pipeline not using Batch Normalization. In the third experiment, we compare a pipeline not using Dropout with a pipeline using Dropout with dropout probability 0.7. In the fourth experiment, we compare a pipeline not using regularization with a pipeline using L2 regularization with regularization weight  $10^{-6}$ .\n\n\nTable 5: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (hyperparameters)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>Baseline Size</td><td>Smaller Size</td><td>69.6%</td><td>73.6%</td></tr><tr><td>BN</td><td>no BN</td><td>80.2%</td><td>89.7%</td></tr><tr><td>no Dropout</td><td>p = 0.7</td><td>95.0%</td><td>99.3%</td></tr><tr><td>no regularization</td><td>10-6</td><td>95.2%</td><td>98.8%</td></tr></table>\n\n\n![(a) Log Loss Plot Figure 1: Batch Normalization Experiment](images/10c97d6bc5fd44ae18ddeda3b3cc0c850b6042abd2d92567fba791b85eab631f.jpg)\n\n\n![(b) Calibrated Log Loss Plot](images/1419d9a784c7f0b9c8b5b475f4d8c9de29ac93f7238bb94c371aff35acb4c744.jpg)\n\n\nFrom figure 1, we can see that by using Calibrated Log Loss, it becomes easier to separate pipeline using Batch Normalization from pipeline without Batch Normalization.\n\n\nFrom the result in Table 5, we can see that Calibrated Log Loss has a higher accuracy regardless of the hyperparameters we are tuning, indicating its effectiveness when comparing the performance of pipelines with different hyperparameters, which is a very common task in Deep Learning. In Appendix B.3 Table 12, we report the mean and standard deviation of Log Loss and Calibrated Log Loss, again consistent with previous results.\n\n\nPipelines with Different Levels of Regularization In this set of experiments, we take a closer look at one hyperparameter we conduct in the previous section: regularization weight. For pipeline  $A$ , we use the baseline model. For pipeline  $B$ , we use different L2 regularization weights.\n\n\nFrom the result in Table 6, we can see that Calibrated Log Loss has a higher accuracy across all different regularization weights, indicating its robustness to different values of regularization weight. As we increase the regularization weight in pipeline  $B$ , the accuracies of both metrics increase. This is because pipeline  $A$  and  $B$  differ more with larger regularization weight, making performance comparison easier.\n\n\nTable 6: Accuracy of Log Loss and Calibrated Log Loss under Neural Networks (regularization weight)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>0</td><td>3*10-7</td><td>63.2%</td><td>69.3%</td></tr><tr><td>0</td><td>5*10-7</td><td>82.2%</td><td>88.2%</td></tr><tr><td>0</td><td>7*10-7</td><td>86.6%</td><td>92.4%</td></tr><tr><td>0</td><td>10-6</td><td>95.2%</td><td>98.8%</td></tr><tr><td>0</td><td>2*10-6</td><td>98.8%</td><td>100.0%</td></tr></table>\n\n\nFrom the result in Appendix B.3 Table 13, we can see that Calibrated Log Loss has a much smaller standard deviation (15% - 40% smaller) than Log Loss while the mean of Log Loss and Calibrated Log Loss is almost on par (within 0.05% difference), again consistent with previous results.\n\n\n# 5.4 CIFAR10\n\n\nDataset We use the default CIFAR10 train dataset as  $\\hat{\\mathcal{D}}_{\\mathrm{train}}$ . We split CIFAR10 test dataset into  $20\\%$ $\\hat{\\mathcal{D}}_{\\mathrm{val - test}}$  and  $80\\%$ $\\hat{\\mathcal{D}}_{\\mathrm{remaining - test}}$ .\n\n\nBase Model We use the ResNet18 model (He et al., 2016) as our base model. We compare ResNet18 with other configurations of ResNet and DenseNet (Huang et al., 2017). We used the open source implementations  ${}^{2}$  to conduct all the experiments.\n\n\nExperiment Details Same as Avazu CTR experiments, for each pipeline, we train the model 60 times with different initialization seeds and data orders to calculate  $\\widehat{\\mathrm{Acc}} (\\bar{e})$ . Note that we use \"Classification Accuracy\" as our ground truth metric to determine the performance rank of different pipelines. We train the model for 200 epochs. We run the experiments for one round and report accuracy. Note that in the neural network experiments, we do not re-sample the training data each time, as there is intrinsic randomness in the neural network training process.\n\n\nCalibration Stage Predicting the image class in CIFAR10 is a Multiclass classification task, which is different from the binary classification task in the CTR prediction context. We use the \"Temperature Scaling\" (Guo et al., 2017a) as our calibration method.\n\n\nExperiment Results From the result in Table 7, we can see that Calibrated Log Loss has a higher\n\n\nTable 7: Accuracy of Log Loss and Calibrated Log Loss (CIFAR10)\n\n\n<table><tr><td>Pipeline A</td><td>Pipeline B</td><td>Log Loss Acc</td><td>Calibrated Log Loss Acc</td></tr><tr><td>resnet18</td><td>resnet101</td><td>5.6%</td><td>74.7%</td></tr><tr><td>resnet18</td><td>resnet152</td><td>8.1%</td><td>87.4%</td></tr><tr><td>resnet18</td><td>DenseNet121</td><td>51.5%</td><td>85.2%</td></tr><tr><td>resnet18</td><td>resnet34</td><td>13.5%</td><td>56.7%</td></tr></table>\n\n\naccuracy, indicating its effectiveness when comparing the performance of pipelines using different image classification models.\n\n\nWe can also see that sometimes the accuracy of Log Loss is below  $50\\%$  (recall that Pipeline A is better than Pipeline B), which indicates that Log Loss and Classification Accuracy will give different results when comparing the performance of pipeline  $A$  and pipeline  $B$  (e.g. pipeline A performs better if using metric Log Loss while pipeline B performs better if using metric Classification Accuracy). This metric inconsistency can be mitigated by Calibrated Log Loss. This phenomenon indicates that some models may have bad calibration (and hence bad Log Loss) while their performances are actually good if measured by accuracy.\n\n\n# 6 RELATED WORK\n\n\nIn recommendation systems, recommending proper items to users is a fundamental task. In order to do accurate recommendations, it is essential to build Click-Through Rate (CTR) prediction models to rank the items and achieve business goals. Prior to Deep Learning Era, traditional machine learning models (Friedman, 2001; Koren et al., 2009; Rendle, 2010; Desrosiers & Karypis, 2010; Canini et al., 2012) like logistic regression, boosted decision trees and factorization machines are commonly used to build CTR models. In the Deep Learning Era, not surprisingly, CTR prediction models have been transitioned to deep models as well (Cheng et al., 2016; Guo et al., 2017b; Covington et al., 2016; Wang et al., 2017; Zhou et al., 2018; Naumov et al., 2019).\n\n\nA typical Deep Click-Through Rate (CTR) prediction model consists of embedding layers and multilayer perceptron (MLP) layers on top of embedding layers. Embedding layers transform raw discrete IDs to low dimensional vectors. Following embedding layers, MLP layers then learn the interactions of different features represented the low dimensional vectors, and finally output the final prediction (i.e. CTR). Currently, Deep Click-Through Rate (CTR) prediction models achieve state-of-the-art performance for CTR tasks, and are commonly used in industries to power various recommendation tasks, including personalized ads recommendations, content recommendations, etc.\n\n\nThere are a number of commonly used metrics (Yi et al., 2013) to evaluate the performance of CTR prediction models. Area Under the ROC Curve (AUC) (Fawcett, 2006; 2004) along with its variants Zhu et al. (2017) and Log Loss are the most common metrics. For example, He et al. (2014); Wang et al. (2017); McMahan et al. (2013) use Log Loss as their core metric, while Zhou et al. (2018); McMahan et al. (2013) use AUC as their core metric. However, AUC has been criticized for not taking into account the predictive probability (Yi et al., 2013). Log Loss, on the other hand, takes predictive probability into account. As a result, in applications where predictions are expected to be calibrated (e.g. Ads Recommendation Systems), Log Loss is preferred.\n\n\n# 7 CONCLUSION AND DISCUSSION\n\n\n# CONCLUSION\n\n\nIn this paper, we have presented a new approach to comparing the performance of different deep learning pipelines. We proposed a new metric framework, Calibrated Loss, which has a higher accuracy and smaller variance than its vanilla counterpart for a wide range of pipelines. Our experiments in section 5 demonstrated the superiority of Calibrated Loss, and we believe this new metric can be used to more effectively and efficiently compare the performance of different pipelines in similar settings. Future work includes expanding this idea to evaluate NLP pipelines, and establish theoretical guarantees under more general settings.\n\n\n# LIMITATIONS\n\n\nOur method sacrifices accuracy when comparing some specific pipelines. For example, if pipeline  $B$  can reliably improve the model calibration in test distribution over pipeline  $A$ , Calibrated Log Loss will not be able to correctly detect the benefits of pipeline  $B$ , while Log Loss is able to. However, for most pipeline comparisons conducted in industry and academia like feature engineering, tuning parameters, etc., Calibrated Log Loss has a huge accuracy boost over Log Loss as we demonstrated in Section 5.\n\n\n# POTENTIAL APPLICATIONS\n\n\nOur method may have applications in AutoML domain. AutoML (Automated Machine Learning) systems are designed to automate the process of selecting, designing, and tuning machine learning models, and a key component of these systems is the selection of the best-performing pipeline (e.g. hyperparameters, model architectures etc.). The new metric can be used as a more accurate way of comparing the performance and selecting the best one. The new metric is in particular useful when performing hyperparameter tuning.\n\n\n# REFERENCES\n\n- Xavier Bouthillier and Gael Varoquaux. Survey of machine-learning experimental methods at NeurIPS2019 and ICLR2020. PhD thesis, Inria Saclay Ile de France, 2020.\n\n- Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, et al. Accounting for variance in machine learning benchmarks. Proceedings of Machine Learning and Systems, 3:747-769, 2021.\n\n- Kevin Canini, Tushar Chandra, Eugene Ie, Jim McFadden, Ken Goldman, Mike Gunter, Jeremiah Harmsen, Kristen LeFevre, Dmitry Lepikhin, Tomas Lloret Llinares, et al. Sibyl: A system for large scale supervised machine learning. Technical Talk, 1:113, 2012.\n\n- Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems, pp. 7-10, 2016.\n\n- Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems, pp. 191-198, 2016.\n\n- Christian Desrosiers and George Karypis. A comprehensive survey of neighborhood-based recommendation methods. Recommender systems handbook, pp. 107-144, 2010.\n\n- Tom Fawcett. Roc graphs: Notes and practical considerations for researchers. Machine learning, 31 (1):1-38, 2004.\n\n- Tom Fawcett. An introduction to roc analysis. Pattern recognition letters, 27(8):861-874, 2006.\n\n- Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pp. 1189-1232, 2001.\n\n- Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pp. 1321-1330. PMLR, 2017a.\n\n- Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-machine based neural network for ctr prediction. arXiv preprint arXiv:1703.04247, 2017b.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising, pp. 1-9, 2014.\n\n- Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708, 2017.\n\n- Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448-456. pmlr, 2015.\n\n- Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30-37, 2009.\n\n- Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 1754-1763, 2018.\n\n- H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1222-1230, 2013.\n\n- Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091, 2019.\n\n- David Picard. Torchmanual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision. CoRR, abs/2109.08203, 2021. URL https://arxiv.org/abs/2109.08203.\n\n- Steffen Rendle. Factorization machines. In 2010 IEEE International conference on data mining, pp. 995-1000. IEEE, 2010.\n\n- Weichen Shen. Deepctr: Easy-to-use, modular and extendible package of deep-learning based ctr models. https://github.com/shenweichen/deepctr, 2017.\n\n- Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.\n\n- Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17, pp. 1-7. 2017.\n\n- Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the Web Conference 2021, pp. 1785-1797, 2021.\n\n- Jeonghee Yi, Ye Chen, Jie Li, Swaraj Sett, and Tak W Yan. Predictive model performance: Offline and online evaluations. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1294-1302, 2013.\n\n- Weinan Zhang, Tianming Du, and Jun Wang. Deep learning over multi-field categorical data. In European conference on information retrieval, pp. 45-57. Springer, 2016.\n\n- Zhao-Yu Zhang, Xiang-Rong Sheng, Yujing Zhang, Biye Jiang, Shuguang Han, Hongbo Deng, and Bo Zheng. Towards understanding the overfitting phenomenon of deep click-through rate prediction models. arXiv preprint arXiv:2209.06053, 2022.\n\n- Guorui Zhou, Xiaogiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1059-1068, 2018.\n\n- Han Zhu, Junqi Jin, Chang Tan, Fei Pan, Yifan Zeng, Han Li, and Kun Gai. Optimized cost per click in taobao display advertising. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 2191-2200, 2017.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636564.5826693", "msecs": "582.0", "relativeCreated": "25275.80976486206", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# An Agnostic Approach to Federated Learning with Class Imbalance\n\n# Abstract\nFederated Learning (FL) has emerged as the tool of choice for training deep models over heterogeneous and decentralized datasets. As a reflection of the experiences from different clients, severe class imbalance issues are observed in real-world FL problems. Moreover, there exists a drastic mismatch between the imbalances from the local and global perspectives, i.e. a local majority class can be the minority of the population. Additionally, the privacy requirement of FL poses an extra challenge, as one should handle class imbalance without identifying the minority class. In this paper we propose a novel agnostic constrained learning formulation to tackle the class imbalance problem in FL, without requiring further information beyond the standard FL objective. A meta algorithm, CLIMB, is designed to solve the target optimization problem, with its convergence property analyzed under certain oracle assumptions. Through an extensive empirical study over various data heterogeneity and class imbalance configurations, we showcase that CLIMB considerably improves the performance in the minority class without compromising the overall accuracy of the classifier, which significantly outperforms previous arts. In fact, we observe the greatest performance boost in the most difficult scenario where every client only holds data from one class.\n\n\n# 1 INTRODUCTION\n\n\nClass imbalance is ubiquitous in real world supervised learning problems. Examples span from medical applications (Lee & Shin, 2020; Roy et al., 2019; Choudhury et al., 2019), fraud detection (Yang et al., 2019; Chan et al., 1999), to consumer based applications (Wang et al., 2021b; Wu et al., 2020; Long et al., 2020). In these scenarios, data belonging to a subset of classes constitute a great proportion of the population while data from the minority classes, generated by uncommon events, are scarce (He & Garcia, 2009). Having a non-uniform number of samples per class deteriorates the performance of the classifier in the minority class (Huang et al., 2016), resulting in low training and testing accuracy. More importantly, unintended consequences of mistreating the minority class can be catastrophic (Van Hulse et al., 2007) if the problem is not handled appropriately.\n\n\nFrom the perspectives of data heterogeneity and privacy, the issue of class imbalance is even more significant in the setting of Federated Learning (FL). Due to the heterogeneity of the local data distributions, there can be a significant mismatch between the local and global imbalance, i.e. the class that is a minority locally can actually be a majority class globally. Moreover, for the purpose of privacy protection in FL, one should tackle the class-imbalance problem in an agnostic manner, i.e. the proposed algorithms should not require the minority class to be identified.\n\n\nIn the centralized training setting, techniques like balanced sampling, loss re-weighting, and gradient tuning have achieved many successes (Johnson & Khoshgoftaar, 2019). However, due to the extra difficulty of handling the class imbalance problem in FL, previous arts that rely on the explicit identification of the minority class do not directly apply. While research along this line is quite limited, a commonly used heuristic is to estimate the portion of a class using the norm of the gradient per class. When used as proxies, these quantities are then utilized to reweight the losses corresponding to different classes (Wang et al., 2021a; Yang et al., 2020). A key drawback of (Wang et al., 2021a) is that a subset of the client's local dataset needs to be shared, which is not ideal for privacy preserving purposes. Moreover, the effectiveness of such approaches degrades when there is a notable mismatch between the local imbalance and the global imbalance (Wang et al., 2021a).\n\n\nIn this work, we target the most difficult yet possibly most interesting setting in FL, where there is a significant mismatch between the local and global imbalance. Concretely, on certain clients, a minority class from a global perspective is in the majority locally. Practitioners often encounters such a mismatch due to the highly heterogeneous data configuration in FL and the principle of locality (Wang et al., 2021a). For example, when the local data are produced by a minority user, the corresponding minority data could occupy a large portion of the local distribution. In this scenario, due to the scarceeness of the minority data from a global perspective, the corresponding underrepresented client will usually experience poor service quality from the trained model, giving rise to potentially severe fairness issue.\n\n\nTo overcome the aforementioned challenges, we propose a constrained learning formulation to handle the class imbalance issue in FL while accounting for both heterogeneity and privacy. In brief, we impose constraints on the standard FL formulation so that the empirical loss on every client should not overly exceed the average empirical loss. Such constraints are shown to force the classifier to account for all classes equally and hence mitigate the detrimental effect of class imbalance, under a type of heterogeneous data configuration that captures the mismatch between the local and global imbalance. The advantages of our formulation are threefold: First, in contrast to previous arts which usually rely on some heuristics to reweight the loss functions for different classes, our approach is principled, yielding a simple optimization interpretation. Second, unlike existing methods, our formulation requires no additional information compared to the original FL formulation and it treats data from different classes agnostically, and hence is less likely to leak the private information of the clients. Third, from our extensive empirical study, our formulation can significantly improve the testing accuracy on the minority class, without compromising the overall performance.\n\n\nContribution. We summarize our contributions as follows.\n\n- - Problem Formulation: To address the challenge of class imbalance in FL with data heterogeneity, we propose a novel constrained FL formulation with explicit enforcement on the similarity between the local empirical losses. Using only information from the standard FL objective, our approach is completely agnostic to class distribution of the client data and its proxies, as opposed to the existing literature (which mostly violate privacy requirements of FL).\n\n- - Meta-Algorithm: We solve our constrained optimization problem via a primal-dual approach. For a fixed dual variable, the corresponding Lagrangian function enjoys a similar structure as the standard FL loss, but with non-uniform weights on the local objectives. Accordingly, we propose an efficient (meta-) algorithm called CLIMB that can use any FL optimization method as a subroutine, with nearly negligible communication overhead. Furthermore, we analyze the convergence properties of CLIMB under certain oracle assumptions.\n\n- - Accuracy Improvement in the minority class: On benchmark datasets, CLIMB with Fed-Avg as base solver achieves significant enhancement of the accuracy in the minority class without compromising the overall accuracy under various data heterogeneity and class imbalance scenarios.\n\n\n# 2 RELATED WORKS\n\n\nClass Imbalance in Centralized Setting. In the centralized learning setting, the number of samples per class is known. We categorize existing solutions that exploit such information as follows.\n\n\nBalanced sampling. In order to balance the data used for gradient calculation, the most common approaches are either to under sample from the majority classes (Liu et al., 2008), or to augment the minority class (Chawla et al., 2002; Guo & Viktor, 2004). Both methods seek to generate an artificial uniform distribution of data (Buda et al., 2018; Pouyanfar et al., 2018).\n\n\nLoss reweighting. Methods in this category focus on re-weighting the loss functions for different classes, with extra emphases on the mistakes made in the minority class (Cui et al., 2019; Ling & Sheng, 2008; Sun et al., 2007; Wang et al., 2016). There also exist works like (Lin et al., 2017) that adjust the scale of the loss sample-wise, without exploiting the global data distribution.\n\n\nGradient tuning. In the context of neural networks, some other works focus on tuning the gradient per class (Anand et al., 1993) showing faster rates of convergence in the class imbalance setting.\n\n\nTo do this, the gradient is reshaped in order to have an equal magnitude when projected to all the gradients per class, while keeping the same norm as the original gradient.\n\n\nClass Imbalance in Federated Learning. Combating with the issue of class imbalance is more challenging in FL since the data composition in such a setting is generally unknown and the minority classes are often difficult to identify Li et al. (2020); Wang et al. (2020b). While research in this direction are quite limited, to the best of our knowledge, we classify them into the following two classes based on whether a proxy of the data composition is explicitly built.\n\n\n(i) A common strategy in existing methods is to explicitly build proxies of the data composition based on some heuristics. These proxies are dynamically adjusted during the learning procedure and allow the aforementioned three techniques, balanced sampling, loss re-weighting and gradient tuning, to be utilized in the FL setting: Wang et al. (2021a) suggests that there exists a proportional relation between the magnitude of the gradients the corresponds to the last layer of the neural network and the sample quantity. Based on such observation, the proposed Ratio-Loss, a class-wise re-weighted version of the standard cross entropy loss. We note that to define the Ratio-Loss, one needs to collect a subset of client data as \"auxiliary data\", which may not be ideal in the setting of FL. On the same vein, Yang et al. (2020) argues that the gradient per class can be used as a proxy to infer the imbalances in the distributions of the clients, and thus clients should be selected according to how uniform the magnitude of gradient per class is. We also notice the work Astraea which introduces extra virtual components called mediators between the FL server and clients (Duan et al., 2019). The mediator is assumed to have access to the local data distributions of the clients so that client rescheduling and data augmentation can be carried out accordingly. Through the prepossessing on the mediator, the gradients communicated to the server are made balanced class-wise. However, when privacy is taken into consideration, methods like Astraea may not be appropriate as in essence it is directly built on the global data distribution.\n\n\n(ii) Without explicitly constructing an estimation of the data composition, there are works that resort to techniques like active learning and reinforcement learning to implicitly learn the data composition as the optimization progresses. Works along this research line usually rely on client selection to mitigate the effects of class imbalances. In (Goetz et al., 2019), to address the class imbalance problem, there will a higher probability to sampling clients that posses the global minority class. Other works, leverage client selection as a multi-armed bandit problem, and design a client selection policy to balance the gradients (Xia et al., 2020). Other arts leverage Q-learning techniques to select clients in order to minimize the overall loss Wang et al. (2020a). However, we believe in the most practical setting of FL, clients that are available per communication round is incoercible and hence these strategies have limited applicability.\n\n\nComparison with Previous Works. Our method significantly differs from the above strategies in the following ways: (1) Our method requires no knowledge of the data composition, nor any proxy of such information. Consequently, our approach is agnostic to the minority class and better preserves clients' privacy. Such a property draws clear distinction between our work and the works listed in class (i) above. (2) As will be more clear in the following section, we only perform clientwise re-weighting as opposed to the class-wise re-weighting schemes used in previous works, which further emphasize the agnostic nature of our approach. (3) In contrast to the methods in class (ii) above, our approach does not require active client selection, enjoying a broader applicability in the most practical and interesting settings.\n\n\n# 3 FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS\n\n\nWe consider the problem of multi-class classification. Let  $x \\in \\mathcal{X} \\subseteq \\mathbb{R}^d$  be the input and use  $y \\in \\mathcal{Y} = \\{1, \\dots, C\\}$  to denote the target label, where  $C$  is the total number of classes. In the context of FL, we assume to have  $N$  clients, each of which possesses its own private data distribution  $p_i(x, y)$ . Here,  $p_i(x, y)$  is a joint probability distribution of the input  $x$  and the output label  $y$ . One can decompose  $p_i(x, y)$  as  $p_i(x, y) = p(x|y)p_i(y)$ , where  $p(x|y)$  is the conditional distribution of the input  $x$  given class  $y$  and  $p_i(y)$  is the marginal distribution of class  $y$  on client  $i$ . We assume that the conditional distribution  $p(x|y)$  is identical on all devices, but the marginal distribution  $p_i(y)$  can vary significantly due to the heterogeneity of the data configuration.\n\n\nLet  $\\mathcal{H} = \\{\\phi : \\mathcal{X} \\times \\Theta \\to \\mathbb{R}^C\\}$  be a family of parameterized predictors with parameter  $\\theta \\in \\Theta \\subseteq \\mathbb{R}^Q$ . Let  $\\ell : \\mathbb{R}^C \\times \\mathcal{Y} \\to \\mathbb{R}_+$  be the loss function, then the local objective function of client  $i$  is defined as\n\n\n$$\n\n$$\nf _ {i} (\\theta) = \\mathbb {E} _ {(x, y) \\sim p _ {i}} [ \\ell (\\phi (x, \\theta), y) ]. \\tag {1}\n$$\n\n$$\n\n\nFor multi-class classification,  $\\ell$  is often chosen to be the cross entropy loss. In the standard formulation of FL, the goal is to minimize the global average of local objectives\n\n\n$$\n\n$$\n\\min  _ {\\theta \\in \\Theta} \\bar {f} (\\theta) := \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (\\theta). \\tag {2}\n$$\n\n$$\n\n\nWhile it is well known that, in the presence of class imbalance, the above vanilla formulation will produce models that perform poorly on the minority data, we consider the following configuration of heterogeneous local class distributions in order to make a quantitative analysis of such a phenomenon for a concrete class imbalance setting. More importantly, such a setting will also motivate our constrained FL formulation. We emphasize that the following setting is just used as a motivating example to showcase our claims, and our results apply generally to any FL setting.\n\n\nMotivating Example. Let  $u$  be the uniform distribution over the classes, i.e. for  $y \\sim u$ ,  $\\operatorname{Pr}(y = c) = \\frac{1}{C}, \\forall c \\in \\mathcal{Y}$  and let  $\\delta_c$  be the Dirac distribution of class  $c$ . We assume for some small but fixed  $\\alpha \\in [0,1]$ , the local class distribution of the client  $i$  is a mixture of the uniform distribution over all classes and the Dirac distribution of a fixed class  $c_i$ , i.e.  $p_i = \\alpha u + (1 - \\alpha)\\delta_{c_i}$ . We use  $N_c$  to denote the number of clients with  $c_i = c$ . Note that in the limit setting when  $\\alpha = 0$ , the aforementioned configuration captures the most heterogeneous setting: clients only have data from a single class.\n\n\nWe consider an extreme case of class imbalance under the above configuration. Without loss of generality, we consider the binary classification problem, i.e.  $C = 2$ , and we assume class 1 to be the minority class with  $N_{1} = 1$ . We define  $g_{i}(\\theta) \\coloneqq \\mathbb{E}_{x \\sim p(x|y = i)}[\\ell(\\phi(x,\\theta),i)]$  as the loss of the predictor  $\\phi(\\cdot,\\theta)$  on the data with  $y = i$ . We can calculate that\n\n\n$$\n\n$$\n\\bar {f} (\\theta) = \\left(\\frac {\\alpha}{2} + \\frac {1 - \\alpha}{N}\\right) g _ {1} (\\theta) + \\left(\\frac {\\alpha}{2} + \\frac {(1 - \\alpha) (N - 1)}{N}\\right) g _ {2} (\\theta). \\tag {3}\n$$\n\n$$\n\n\nClearly, when  $\\alpha$ , the portion of data with uniform label distribution, is small, e.g.  $\\alpha = 0$ , and  $N$ , the total number of clients, is large, the loss of the predictor on class 1 has negligible weight, which often leads to the poor performance on the minority class in the trained classifier as the majority classes dominate the gradient. Moreover, observe that when  $\\alpha$  is small, there is a significant mismatch between the local and global class imbalance: the global minority class 1 is in the majority on the corresponding client locally. Such phenomenon is pertinent to the FL setting due to the data heterogeneity and poses a great challenge for tackling the issue of class imbalance in FL.\n\n\n# 3.1 CONSTRAINED FL FORMULATION\n\n\nIn our work, to address the class imbalance challenge, we propose to minimize the following constrained FL (CFL) formulation\n\n\n$$\n\n$$\nP _ {\\epsilon} ^ {*} = \\min  _ {\\theta \\in \\Theta} \\bar {f} (\\theta) := \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (\\theta) \\tag {CFL}\n$$\n\n$$\n\n\n$$\n\n$$\n\\begin{array}{l} \\text {s . t .} f _ {i} (\\theta) - \\bar {f} (\\theta) \\leq \\epsilon , \\forall i \\in [ 1, \\dots , N ]. \\end{array}\n$$\n\n$$\n\n\nHere,  $\\epsilon$  is a tolerance constant that controls the enforced closeness in the training loss among clients, and we emphasize the dependence of the optimal value  $P_{\\epsilon}^{*}$  on  $\\epsilon$  by encoding it in the subscript. As a motivation, we show that the constraint in our formulation (CFL) can be translated to a constraint on the performance of the minority class, under the above class imbalance setting: Consider the setting where the tolerance constant is very small, i.e.  $\\epsilon$  is close to zero. WLOG, we assume that the first device is the one that has  $c_{i} = 1$ . Recall the definition of  $g_{i}$  above Eq.(3). One can compute that\n\n\n$$\n\n$$\nf _ {1} (\\theta) - \\bar {f} (\\theta) = \\frac {(1 - \\alpha) (N - 1)}{N} \\left(g _ {1} (\\theta) - g _ {2} (\\theta)\\right) \\leq \\epsilon \\Longleftrightarrow g _ {1} (\\theta) - g _ {2} (\\theta) \\leq \\frac {N \\epsilon}{(1 - \\alpha) (N - 1)}.\n$$\n\n$$\n\n\nTherefore, our formulation (CFL) enjoys a clear class-balancing interpretation in the highly heterogeneous setting when  $\\alpha$  is small and  $N$  is large. Note that this is achieved without the identification of the minority class and server collects no additional information compared to the vanilla FL.\n\n\n```\n1: Input: initial model  $\\theta^0$ , a subroutine ClientUpdate, dual step size  $\\eta_{D}$ , maximum round  $T$ ;  \n2: Initialize the dual variables  $\\lambda = [0,\\dots ,0]$ ;  \n3: for  $t = 1,2,\\ldots ,T$  do  \n4: Compute weights:  $\\forall i\\in [N],w_i = 1 + \\lambda_i - \\bar{\\lambda}$  with  $\\bar{\\lambda} = \\frac{1}{N}\\sum_{i = 1}^{N}\\lambda_{i}$ ;  \n5: Primal Update:  $\\theta^{t + 1}\\gets \\text{ClientUpdate} (\\{w_i\\}_{i = 1}^N,\\theta^t)$ ;  \n6: Dual Update:  $\\forall i\\in [N],\\lambda_i\\gets [\\lambda_i + \\eta_D(f_i(\\theta^{t + 1}) - \\bar{f} (\\theta^{t + 1}) - \\epsilon)]_+$ , with  $\\bar{f} = \\frac{1}{N}\\sum_{i = 1}^{N}f_{i}$ ;  \n7: end for  \n8: Output: model  $\\theta^{T + 1}$\n```\n\n\nWhile the above nice interpretation of the proposed constraint may not hold exactly for general class imbalance settings, in spirit, we want the resulting classifier from our algorithm to perform similarly on every class, or in other words to account for the minority class and majority class equally. In the following section, we discuss how to solve the proposed formulation by alternating the primal and dual updates of an equivalent Lagrangian formulation.\n\n\n# 3.2 ALGORITHM CONSTRUCTION\n\n\nIn order to solve problem (CFL), we resort to the method of Lagrange multipliers. By introducing the dual variables  $\\lambda = [\\lambda_1, \\dots, \\lambda_N] \\in \\mathbb{R}_+^N$ , we define the Lagrangian function as,\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) = \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (\\theta) + \\lambda_ {i} \\left(f _ {i} (\\theta) - \\frac {1}{N} \\sum_ {j = 1} ^ {N} f _ {j} (\\theta) - \\epsilon\\right) (4) \\\\ = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\left(1 + \\lambda_ {i} - \\bar {\\lambda}\\right) f _ {i} (\\theta) - \\lambda_ {i} \\epsilon , \\quad \\text {w i t h} \\bar {\\lambda} = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\lambda_ {i}. (5) \\\\ \\end{array}\n$$\n\n$$\n\n\nWith this in hand, we can construct a lower bound of the above constrained optimization problem using the Lagrangian  $\\mathcal{L}(\\theta ,\\lambda)$  as follows:\n\n\n$$\n\n$$\nD _ {\\epsilon} ^ {*} = \\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\min  _ {\\theta \\in \\Theta} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) \\leq \\min  _ {\\theta \\in \\Theta} \\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) = P _ {\\epsilon} ^ {*},\n$$\n\n$$\n\n\nwhere we often refer to  $D_{\\epsilon}^{*}$  as the dual problem. To solve the dual problem, we propose CLIMB, a method described in Algorithm 1, which is discussed in detail as follows.\n\n\nCLIMB proceeds by alternatingly optimizing over the primal variable  $\\theta$  and the dual variable  $\\lambda$ .\n\n\nPrimal Update. For a fixed  $\\lambda$ , the minimization of  $\\mathcal{L}$  with respect to  $\\theta$  is equivalent to a re-weighted version of the standard unconstrained FL objective (2):\n\n\n$$\n\n$$\n\\min  _ {\\theta \\in \\Theta} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) \\Longleftrightarrow \\min  _ {\\theta \\in \\Theta} \\frac {1}{N} \\sum_ {i = 1} ^ {N} (1 + \\lambda_ {i} - \\bar {\\lambda}) f _ {i} (\\theta). \\tag {6}\n$$\n\n$$\n\n\nImportantly, this simple and canonical form allows us to perform the update on  $\\theta$  using any FL solver as the base optimizer. For flexibility, we do not explicitly choose the base optimizer in our algorithm description, but refer to the update on  $\\theta$  as the subroutine ClientUpdate( $\\{w_i\\}_{i=1}^N, \\theta^t) \\to \\theta^{t+1}$ . Such a subroutine takes the non-uniform weights  $\\{w_i\\}_{i=1}^N$  on the local objectives and the current consensus model  $\\theta^t$  as inputs, and returns an updated model  $\\theta^{t+1}$ . Note that every call to ClientUpdate may consists of multiple communication rounds.\n\n\nDual Update. Once we have obtained the new consensus model  $\\theta^{t + 1}$ , we perform dual update on  $\\lambda$  by taking a single dual ascent step of the following equivalent objective (given some fixed  $\\theta$ ),\n\n\n$$\n\n$$\n\\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\mathcal {L} (\\theta , \\boldsymbol {\\lambda}) \\Longleftrightarrow \\max  _ {\\boldsymbol {\\lambda} \\in \\mathbb {R} _ {+} ^ {N}} \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\lambda_ {i} \\left(f _ {i} (\\theta) - \\frac {1}{N} \\sum_ {j = 1} ^ {N} f _ {j} (\\theta) - \\epsilon\\right). \\tag {7}\n$$\n\n$$\n\n\nTo evaluate the gradient of  $\\mathcal{L}$  with respect to  $\\lambda$ , we need to first broadcast the consensus model  $\\theta^{t + 1}$  and then aggregate the function values  $f(\\theta^{t + 1})$ . Since the broadcast model can be used for the next round of primal update, the only overhead of CLIMB compared to the standard FL solver ClientUpdate, is to transmit the functional value, which is negligible.\n\n\n# 3.3 THEORETICAL GUARANTEES\n\n\nBased on the recent progress in constrained learning (Chamon et al., 2021), we show that under the following mild regularity conditions, the duality gap between the dual problem  $D_{\\epsilon}^{*}$  and the primal problem  $P_{\\epsilon}^{*}$  can be controlled by some quantity that describes the capability of the parametric function class  $\\mathcal{H}$ .\n\n\nAssumption 3.1 The loss function  $\\ell$  in the definition of the local objective (1) is  $L$ -Lipschitz, i.e.  $\\| \\ell (x,\\cdot) - \\ell (z,\\cdot)\\| \\leq L\\| x - z\\|$ , and bounded by  $B$ .\n\n\nAssumption 3.2 The conditional distribution  $p(x|y)$  is non-atomic for all  $y \\in \\mathbb{R}^C$ .\n\n\nWhile usually the local data distribution is discrete, we can always augment it by randomly perturbing the data points with white noise, this is often used as data augmentation in vision tasks.\n\n\nAssumption 3.3 There exists a convex hypothesis class  $\\hat{\\mathcal{H}}$  such that  $\\mathcal{H} \\subseteq \\hat{\\mathcal{H}}$ , and there exists a constant  $\\xi > 0$  such that  $\\forall \\hat{\\phi} \\in \\hat{\\mathcal{H}}$ , there exists  $\\theta \\in \\Theta$  such that  $\\sup_{x \\in \\mathcal{X}} \\| \\hat{\\phi}(x) - \\phi(x, \\theta) \\| \\leq \\xi$ .\n\n\nA simple strategy to construct  $\\hat{\\mathcal{H}}$  is to take the convex hull of  $\\mathcal{H}$ . When  $\\mathcal{H}$  is sufficiently rich,  $\\xi$  can be expected to be small. Notice that this bound can be decreased by increasing the richness of the function class  $\\mathcal{H}$ . All the proofs can be found in the Appendix.\n\n\nTheorem 3.1 (Near Zero Duality Gap) Under Assumptions 3.1, 3.2, 3.3, and the Constrained Federated Learning problem is feasible in  $\\hat{\\mathcal{H}}$  with constraint  $\\epsilon - 2L\\xi$ , the Constrained Federated Learning problem has near zero-duality gap,\n\n\n$$\n\n$$\nP _ {\\epsilon} ^ {*} - D _ {\\epsilon} ^ {*} \\leq (2 | \\boldsymbol {\\lambda} _ {\\epsilon - 2 L \\xi} ^ {*} | _ {1} + 1) L \\xi , \\tag {8}\n$$\n\n$$\n\n\nwhere  $\\lambda_{\\epsilon -2L\\xi}^{*}$  is the optimal dual variable associated with the Constrained Federated Learning problem equation CFL with constraints  $\\epsilon -2L\\xi$  over the space of functions  $\\hat{\\mathcal{H}}$ .\n\n\nTheorem 3.1 establishes an upper bound on the duality gap of the Constrained Federated Learning Problem CFL. The gap depends on the Lipschitz constant of the loss function  $L$ , the richness of the function class  $\\xi$ , and the optimal dual variable of a more restrictive problem. Note that we required the Constrained Federated Learning problem to be feasible for constraint  $\\epsilon - 2L\\xi$ . In the case of the cross-entropy loss, as long as  $\\epsilon - 2L\\xi > 0$ , this can be satisfied by a classifier that assigns a uniform label for every sample, as each individual loss will be equal to each other.\n\n\nSince the minimization of the Lagrangian function  $\\mathcal{L}$  is non-convex, to show the convergence of CLIMB, we need an additional oracle assumption as follows.\n\n\nAssumption 3.4 (Approximate Solution) For every dual variable  $\\pmb{\\lambda} \\in \\mathbb{R}_{+}^{N}$ , and precision  $\\delta > 0$  there exists an oracle approximate solution  $\\theta_{\\lambda}$  such that  $\\mathcal{L}(\\theta_{\\lambda}, \\pmb{\\lambda}) \\leq \\min_{\\theta \\in \\mathbb{R}^Q} \\mathcal{L}(\\theta, \\pmb{\\lambda}) + \\delta$ .\n\n\nTheorem 3.2 (Convergence) Define the dual function  $d(\\lambda) = \\min_{\\theta \\in \\Theta} \\mathcal{L}(\\theta, \\lambda)$ . Under Assumptions 3.1 to 3.4, for a fixed tolerance  $r > 0$ , the iterates generated by Algorithm 1, achieve a neighborhood of the dual problem  $D_{\\epsilon}^{*}$  in at most  $T_r = \\mathcal{O}(1 / r)$  steps, i.e.,\n\n\n$$\n\n$$\nd \\left(\\boldsymbol {\\lambda} ^ {T _ {r}}\\right) \\geq D _ {\\epsilon} ^ {*} - \\delta - \\frac {\\eta_ {D}}{2} B ^ {2} - r, \\tag {9}\n$$\n\n$$\n\n\nDue to space limitation, a detailed discussion of the convergence theorem is deferred to the appendix.\n\n\n# 4 EXPERIMENTS\n\n\nIn this section, we evaluate our formulation (CFL) against the competitors on various FL scenarios at the presence of class imbalance. Our results highlight the benefits of our approach especially when the local data distributions are severely heterogeneous and there is a significant mismatch between local and global imbalance. To ensure a fair comparison, we adopt the same base optimizer, Fed-Avg\n\n\n<table><tr><td>Imbalance ratio</td><td>Dataset</td><td>Level of heterogeneity</td><td>Baseline (Eq.(2))</td><td>CLIMB (this work)</td><td>Ratio-Loss</td><td>Focal-Loss</td></tr><tr><td rowspan=\"12\">\u03c1 = 20</td><td rowspan=\"6\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.0532 (0.6754)</td><td>0.2080 (0.6829)</td><td>0.1140 (0.6727)</td><td>0.0450 (0.6445)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.137 (0.7121)</td><td>0.3230 (0.7121)</td><td>0.1790 (0.7037)</td><td>0.0430 (0.6914)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0 (0.5669)</td><td>0.2810 (0.6031)</td><td>0 (0.5746)</td><td>0 (0.6565)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.1279 (0.7098)</td><td>0.3240 (0.7167)</td><td>0.1790 (0.7054)</td><td>0.0552 (0.6905)</td></tr><tr><td rowspan=\"6\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.6889 (0.9375)</td><td>0.8552 (0.9556)</td><td>0.8472 (0.9544)</td><td>0.6186 (0.9278)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.7925 (0.9540)</td><td>0.8748 (0.9616)</td><td>0.8052 (0.9555)</td><td>0.7784 (0.9479)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.3425 (0.8260)</td><td>0.6987 (0.9158)</td><td>0.4134 (0.8484)</td><td>0.1944 (0.7938)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.4720 (0.8596)</td><td>0.7290 (0.9153)</td><td>0.6717 (0.9063)</td><td>0.4602 (0.8654)</td></tr><tr><td rowspan=\"12\">\u03c1 = 10</td><td rowspan=\"6\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.2058 (0.6841)</td><td>0.3629 (0.7041)</td><td>0.2164 (0.6839)</td><td>0.0414 (0.6543)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.1813 (0.7113)</td><td>0.3743 (0.7312)</td><td>0.2657 (0.7083)</td><td>0.1347 (0.6911)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.0492 (0.5933)</td><td>0.2280 (0.6358)</td><td>0.0315 (0.5825)</td><td>0 (0.5548)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.1064 (0.6380)</td><td>0.2734 (0.6696)</td><td>0.0993 (0.6211)</td><td>0.0298 (0.5982)</td></tr><tr><td rowspan=\"6\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.8473 (0.9534)</td><td>0.9305 (0.9584)</td><td>0.8851 (0.9558)</td><td>0.8469 (0.9470)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.8962 (0.9634)</td><td>0.9239 (0.9657)</td><td>0.8798 (0.9615)</td><td>0.8953 (0.9586)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.1</td><td>0.6045 (0.8906)</td><td>0.8084 (0.9356)</td><td>0.6981 (0.9119)</td><td>0.4690 (0.8670)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.7272 (0.9190)</td><td>0.8195 (0.9392)</td><td>0.7663 (0.9320)</td><td>0.6961 (0.9099)</td></tr></table>\n\n\nTable 1: The minority class testing accuracy and the overall testing accuracy (the quantity in the parentheses) after sufficiently many communication rounds. If there are multiple minority classes, we report the worst of them. Here  $N$ , the number of devices, is 500. The base FL solver is Fed-Avg with partial-participation: 100 devices participate in every communication round.\n\n\nin the current experiment<sup>1</sup>, for all formulations: the standard FL objective in Eq.(2), the proposed formulation in Eq.(CFL), Ratio-Loss (Wang et al., 2021a), and Focal-Loss (Lin et al., 2017). We emphasize that the novelty of our work lies in the new constrained FL formulation (CFL) and is orthogonal to the research of how the formulation is solved. In the following, we describe the datasets and models used in our experiments where more details are provided in the Appendix A.\n\n\nDatasets Two benchmark datasets are used in our experiments with the default train/test splits, which are MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al., 2009).\n\n\nHeterogeneity. We generate heterogeneity in the local data distributions according to the strategy from (Karimireddy et al., 2020; Hsu et al., 2019): Let  $\\alpha \\in [0,1]$  be some constant that determines\n\n\nthe level of heterogeneity. For a fixed  $\\alpha$ , we divide the dataset among  $N = 100$  (moderate) or  $N = 500$  (massive) clients as follows: for we allocate to each client a portion of  $\\alpha$  i.i.d. data and the remaining portion of  $(1 - \\alpha)$  by sorting according to label.\n\n\nData Imbalance. We simulate the phenomenon of class imbalance by removing data belong to the minority classes: Observe that the datasets included in our experiments both have 10 perfectly balanced classes. For the minority class(es), we retain only  $1 / \\rho$  portion of the corresponding data. Here,  $\\rho \\geq 1$  the ratio between the numbers of data in the majority class and in the minority class and is termed the imbalance ratio. For example, when there are 3 minority classes with  $\\rho = 10$ ,  $90\\%$  of the data belong to classes 0, 1, 2 (without loss of generality) are manually removed. In our experiments, we consider the setting of 1 or 3 minority classes and we take  $\\rho = 5, 10, 20$ .\n\n\nModels We follow the choice of model architectures in (Acar et al., 2020; McMahan et al., 2017). Specifically, we use a 2 hidden layer fully-connected neural network for MNIST, where the numbers of neurons are (128, 128). For CIFAR10, we use a CNN model consisting of 2 convolutional layers with  $645 \\times 5$  filters followed by 2 fully connected layers with 394 and 192 neurons. We note that higher testing accuracy on the included datasets can be obtained by using models with high capacity, but is orthogonal to our research.\n\n\n# 4.1 RESULTS SUMMARY\n\n\nTo evaluate the effectiveness of an approach against the challenge of class imbalance, one needs to take into consideration both the performance on the minority class(es) and the average performance on all the classes. We report both quantities after sufficient communication rounds (5000 rounds for CIFAR10 and 1000 rounds for MNIST) under various experiment settings in Tables 1 and 2. In every cell of these tables, the quantity above denote the minority class testing accuracy and the quantity in the parentheses is the average performance on all the classes. We also considered the case where there are multiple minority classes and we report the worst accuracy among the minority classes. Our approach outperforms previous arts in all cases, often by a large margin.\n\n\nImbalance Ratio and Number of Minority Classes. The imbalance ratio  $\\rho$  and the number of minority classes are two important quantities to measure the difficulty of a class imbalance problem. Under all the included choices, CLIMB consistently beats previous arts in both minority class testing accuracy and average performance on all the classes. Therefore, we conclude that CLIMB is able to boost the performance on the minority class without compromising the performance on the other classes. This is a rare merit when addressing the class imbalance problem. In fact, methods like Ratio-Lossis able to improve the testing accuracy on the minority class, but it also sacrifices the performance on the rest classes, leading to an inferior average testing accuracy.\n\n\nLevel of Heterogeneity. We test the performance of CLIMB under different levels of heterogeneity and observe that CLIMB outperforms the included methods considerably in all settings. It has the biggest advantage over the competitors in the most heterogeneous setting,  $\\alpha = 0$ . There are situations that existing methods completely fail in the minority class with less than  $10\\%$  accuracy, but CLIMB is still able to correctly classify most of the minority data, e.g. see Table  $2 \\rightarrow \\rho = 5 \\rightarrow \\mathrm{MNIST} \\rightarrow 3$  minority classes  $\\rightarrow \\alpha = 0$ .\n\n\nModerate vs. Large Number of Devices An important goal of FL is to exploit the computational resources of the IoT devices. Hence, the scalability to a large number of devices is a critical property of an FL method. We hence test CLIMB on both moderate ( $N = 100$ , see Table 2) and massive devices ( $N = 500$ , see Table 1) settings. To make things more practical, we instantiate the subroutine ClientUpdate using Fed-Avg with the partial-participation scheme in the massive device setting. Specifically, 100 devices participate model training after every Fed-Avg global communication round. We clearly observe the advantage of CLIMB in all of these setups.\n\n\n# CONCLUSION\n\n\nIn this paper we proposed a novel agnostic constrained learning formulation to tackle the problem of class imbalance in the Federated Learning setting. By introducing constraints in the learning procedure we enforced the performance to be similar in all clients, thus accounting for the class imbalances. In terms of privacy protection, our formulation requires no further information than the standard FL objective to be collected in the server and it never estimates the data composition as opposed to all previous approaches. Moreover, compared with previous arts which are usually heuristic based, our approach is principled as it is purely optimization based and can be efficiently solved via the proposed meta-algorithm CLIMB, yielding major practical benefits. Our extensive empirical study showcases the superiority of proposed constrained formulation over previous arts.\n\n\n<table><tr><td>Imbalance ratio</td><td>Dataset</td><td>Level of heterogeneity</td><td>Baseline (Eq.(2))</td><td>CLIMB (this work)</td><td>Ratio-Loss</td><td>Focal-Loss</td></tr><tr><td rowspan=\"16\">\u03c1 = 10</td><td rowspan=\"8\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0229(0.5734)</td><td>0.5575(0.6076)</td><td>0(0.4836)</td><td>0(0.4205)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.2753(0.7143)</td><td>0.5054(0.7246)</td><td>0.2929(0.6951)</td><td>0.2284(0.6860)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.29880.7348</td><td>0.4689(0.7511)</td><td>0.3825(0.7329)</td><td>0.26180.7249</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0402(0.5534)</td><td>0.2756(0.5598)</td><td>0(0.4678)</td><td>0(0.4527)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.1316(0.6189)</td><td>0.3399(0.6637)</td><td>0.0690(0.615)</td><td>0.0408(0.5976)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.2566(0.6659)</td><td>0.3292(0.6983)</td><td>0.1916(0.6504)</td><td>0.1213(0.6346)</td></tr><tr><td rowspan=\"8\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.3092(0.8630)</td><td>0.9175(0.9341)</td><td>0.4650(0.8630)</td><td>0.3078(0.8556)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.8597(0.9586)</td><td>0.9428(0.9675)</td><td>0.9189(0.9648)</td><td>0.8348(0.9529)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.8750(0.9640)</td><td>0.9377(0.9715)</td><td>0.9283(0.9706)</td><td>0.8882(0.9631)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0153(0.7133)</td><td>0.8029(0.9115)</td><td>0.06310.7222</td><td>0.07170.7401</td></tr><tr><td>\u03b1 = 0.1</td><td>0.7263(0.9189)</td><td>0.8828(0.9522)</td><td>0.7870(0.9341)</td><td>0.6674(0.9069)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.7950(0.9352)</td><td>0.8004(0.9416)</td><td>0.7801(0.9364)</td><td>0.7785(0.9274)</td></tr><tr><td rowspan=\"16\">\u03c1 = 5</td><td rowspan=\"8\">CIFAR10</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.2892(0.6382)</td><td>0.5987(0.6468)</td><td>0.0942(0.5506)</td><td>0.1491(0.5631)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.4101(0.7186)</td><td>0.6075(0.7351)</td><td>0.4011(0.7008)</td><td>0.3558(0.6994)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.5335(0.7536)</td><td>0.6063(0.7556)</td><td>0.5054(0.7427)</td><td>0.4359(0.7380)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0313(0.4742)</td><td>0.3813(0.5639)</td><td>0.0512(0.5108)</td><td>0(0.4514)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.5135(0.6636)</td><td>0.6420(0.6977)</td><td>0.4600(0.6632)</td><td>0.4213(0.6384)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.5251(0.6960)</td><td>0.6328(0.7202)</td><td>0.5751(0.6883)</td><td>0.5311(0.6749)</td></tr><tr><td rowspan=\"8\">MNIST</td><td></td><td colspan=\"4\">1 minority class out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.7245(0.8953)</td><td>0.9154(0.9224)</td><td>0.8020(0.9016)</td><td>0.7429(0.8992)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.9378(0.9666)</td><td>0.9582(0.9693)</td><td>0.9286(0.9651)</td><td>0.9408(0.9624)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.9448(0.9711)</td><td>0.9670(0.9734)</td><td>0.9561(0.9733)</td><td>0.9481(0.9686)</td></tr><tr><td></td><td colspan=\"4\">3 minority classes out of 10 total classes</td></tr><tr><td>\u03b1 = 0.0</td><td>0.0160(0.7722)</td><td>0.8744(0.9137)</td><td>0(0.7370)</td><td>0.0544(0.7826)</td></tr><tr><td>\u03b1 = 0.1</td><td>0.8294(0.9422)</td><td>0.9217(0.9606)</td><td>0.8520(0.9501)</td><td>0.7987(0.9361)</td></tr><tr><td>\u03b1 = 0.2</td><td>0.8557(0.9536)</td><td>0.8818(0.9595)</td><td>0.8730(0.9536)</td><td>0.8321(0.9442)</td></tr></table>\n\n\nTable 2: The minority class testing accuracy and the overall testing accuracy (the quantity in the parentheses) after sufficiently many communication rounds. If there are multiple minority classes, we report the worst of them. Here  $N$ , the number of devices, is 100. The base FL solver is Fed-Avg with full-participation: all devices participate in every communication round.\n\n\n# REFERENCES\n\n- Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Conference on Learning Representations, 2020.\n\n- Rangachari Anand, Kishan G Mehrotra, Chilukuri K Mohan, and Sanjay Ranka. An improved algorithm for neural network classification of imbalanced training sets. IEEE Transactions on Neural Networks, 4(6):962-969, 1993.\n\n- Dimitri Bertsekas. Convex optimization theory. Athena Scientific, 2009.\n\n- Dimitri Bertsekas. Convex optimization algorithms. Athena Scientific, 2015.\n\n- Stephen Boyd and Almir Mutapcic. Subgradient methods.\n\n- Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n\n- Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. Neural Networks, 106:249-259, 2018.\n\n- Luiz Chamon and Alejandro Ribeiro. Probably approximately correct constrained learning. Advances in Neural Information Processing Systems, 33, 2020.\n\n- Luiz FO Chamon, Santiago Paternain, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained learning with non-convex losses. arXiv preprint arXiv:2103.05134, 2021.\n\n- Philip K Chan, Wei Fan, Andreas L Prodromidis, and Salvatore J Stolfo. Distributed data mining in credit card fraud detection. IEEE Intelligent Systems and Their Applications, 14(6):67-74, 1999.\n\n- Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.\n\n- Olivia Choudhury, Yoonyoung Park, Theodoros Salonidis, Aris Gkoulalas-Divanis, Issa Sylla, et al. Predicting adverse drug reactions on distributed health data using federated learning. In AMIA Annual symposium proceedings, volume 2019, pp. 313. American Medical Informatics Association, 2019.\n\n- Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9268-9277, 2019.\n\n- Moming Duan, Duo Liu, Xianzhang Chen, Yujuan Tan, Jinting Ren, Lei Qiao, and Liang Liang. Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications. In 2019 IEEE 37th international conference on computer design (ICCD), pp. 246-254. IEEE, 2019.\n\n- Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, and Anuj Kumar. Active federated learning. arXiv preprint arXiv:1909.12641, 2019.\n\n- Hongyu Guo and Herna L Viktor. Learning from imbalanced data sets with boosting and data generation: the databost-im approach. ACM Sigkdd Explorations Newsletter, 6(1):30-39, 2004.\n\n- Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):1263-1284, 2009.\n\n- Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.\n\n- Chen Huang, Yining Li, Chen Change Loy, and Xiaou Tang. Learning deep representation for imbalanced classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5375-5384, 2016.\n\n- Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal of Big Data, 6(1):1-54, 2019.\n\n- Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.\n\n- Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.\n\n- Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\n- Geun Hyeong Lee and Soo-Yong Shin. Federated learning on clinical benchmark data: Performance assessment. J Med Internet Res, 22(10):e20891, Oct 2020. ISSN 1438-8871. doi: 10.2196/20891. URL http://www.jmir.org/2020/10/e20891/.\n\n- Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data, 2020.\n\n- Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr D\u00f3lar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980-2988, 2017.\n\n- Charles X Ling and Victor S Sheng. Cost-sensitive learning and the class imbalance problem. Encyclopedia of machine learning, 2011:231-235, 2008.\n\n- Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. Exploratory undersampling for class-imbalance learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(2):539-550, 2008.\n\n- Guodong Long, Yue Tan, Jing Jiang, and Chengqi Zhang. Federated Learning for Open Banking, pp. 240-254. Springer International Publishing, Cham, 2020. ISBN 978-3-030-63076-8. doi: 10.1007/978-3-030-63076-8_17. URL https://doi.org/10.1007/978-3-030-63076-8_17.\n\n- Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pp. 1273-1282. PMLR, 20-22 Apr 2017. URL https://proceedings.mlr.press/v54/mcmahan17a.html.\n\n- Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S Kaseb, Kent Gauen, Ryan Dailey, Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-Ching Chen, et al. Dynamic sampling in convolutional neural networks for imbalanced data classification. In 2018 IEEE conference on multimedia information processing and retrieval (MIPR), pp. 112-117. IEEE, 2018.\n\n- Alejandro Ribeiro. Optimal resource allocation in wireless communication and networking. EURASIP Journal on Wireless Communications and Networking, 2012(1):1-19, 2012.\n\n- Abhijit Guha Roy, Shayan Siddiqui, Sebastian P\u00f6lsterl, Nassir Navab, and Christian Wachinger. Brantorrent: A peer-to-peer environment for decentralized federated learning. arXiv preprint arXiv:1905.06731, 2019.\n\n- Yanmin Sun, Mohamed S Kamel, Andrew KC Wong, and Yang Wang. Cost-sensitive boosting for classification of imbalanced data. Pattern recognition, 40(12):3358-3378, 2007.\n\n- Fabio Tardella. A new proof of the lyapunov convexity theorem. SIAM journal on control and optimization, 28(2):478-481, 1990.\n\n- Jason Van Hulse, Taghi M Khoshgoftaar, and Amri Napolitano. Experimental perspectives on learning from imbalanced data. In Proceedings of the 24th international conference on Machine learning, pp. 935-942, 2007.\n\n- Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li. Optimizing federated learning on non-iid data with reinforcement learning. In IEEE INFOCOM 2020-IEEE Conference on Computer Communications, pp. 1698-1707. IEEE, 2020a.\n\n- Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization, 2020b.\n\n- Lixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. Addressing class imbalance in federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 10165-10173, 2021a.\n\n- Shoujin Wang, Wei Liu, Jia Wu, Longbing Cao, Qinxue Meng, and Paul J Kennedy. Training deep neural networks on imbalanced data sets. In 2016 international joint conference on neural networks (IJCNN), pp. 4368-4374. IEEE, 2016.\n\n- Yi Wang, Imane Lahmam Bennani, Xiufeng Liu, Mingyang Sun, and Yao Zhou. Electricity consumer characteristics identification: A federated learning approach. IEEE Transactions on Smart Grid, 12(4):3637-3647, 2021b. doi: 10.1109/TSG.2021.3066577.\n\n- Qiong Wu, Xu Chen, Zhi Zhou, and Junshan Zhang. Fedhome: Cloud-edge based personalized federated learning for in-home health monitoring. IEEE Transactions on Mobile Computing, pp. 1-1, 2020. doi: 10.1109/TMC.2020.3045266.\n\n- Wenchao Xia, Tony QS Quek, Kun Guo, Wanli Wen, Howard H Yang, and Hongbo Zhu. Multiarmed bandit-based client scheduling for federated learning. IEEE Transactions on Wireless Communications, 19(11):7108-7123, 2020.\n\n- Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.\n\n- Miao Yang, Akitanoshou Wong, Hongbin Zhu, Haifeng Wang, and Hua Qian. Federated learning with class imbalance reduction, 2020.\n\n- Wensi Yang, Yuhang Zhang, Kejiang Ye, Li Li, and Cheng-Zhong Xu. Ffd: A federated learning based method for credit card fraud detection. In Keke Chen, Sangeetha Seshadri, and Liang-Jie Zhang (eds.), Big Data - BigData 2019, pp. 18-32, Cham, 2019. Springer International Publishing. ISBN 978-3-030-23551-2.\n\n- Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference on Machine Learning, pp. 7252-7261. PMLR, 2019.\n\nResponse: \\boxed{Accept}", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "TextualGradientDescent prompt for update", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "165", "funcName": "_update_prompt", "created": "1768636564.592121", "msecs": "592.0", "relativeCreated": "25285.261392593384", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "prompt": "Here is the role of the variable you will improve: <ROLE>System prompt for paper acceptance prediction. This prompt sets the context and approach for the AI reviewer.</ROLE>.\n\nThe variable is the text within the following span: <VARIABLE> You are an expert academic reviewer tasked with evaluating research papers. </VARIABLE>\n\nHere is the context and feedback we got for the variable:\n\n<CONTEXT></CONTEXT>\n\nImprove the variable (System prompt for paper acceptance prediction. This prompt sets the context and approach for the AI reviewer.) using the feedback provided in <FEEDBACK> tags.\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.", "message": "TextualGradientDescent prompt for update"}
{"name": "textgrad", "msg": "TextualGradientDescent optimizer response", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "179", "funcName": "step", "created": "1768636577.072014", "msecs": "72.0", "relativeCreated": "37765.15460014343", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "optimizer.response": "<think>\nOkay, let's tackle this. The user wants me to improve the system prompt for a paper acceptance prediction model. The original variable is \"You are an expert academic reviewer tasked with evaluating research papers.\" \n\nFirst, I need to understand the role of this prompt. It's meant to set the context and approach for the AI reviewer. The current prompt is pretty basic. It tells the AI it's an expert reviewer, but maybe it's too vague. \n\nThe feedback isn't provided here, but the context is empty. Hmm, maybe the user expects me to make improvements based on common issues with such prompts. Typically, system prompts should be more detailed. They should specify the criteria for evaluation, like originality, methodology, significance, clarity, and relevance to the field. \n\nAlso, the original prompt doesn't mention the goal of predicting acceptance. The AI needs to know it's not just evaluating but predicting whether the paper would be accepted. So adding that aspect is important. \n\nMaybe include instructions on how to balance different factors. For example, weighing innovation against feasibility. Also, mention considering the journal's scope and audience. \n\nI should structure the prompt to guide the AI through a structured evaluation process. Perhaps start by defining the role, then outline the criteria, and then the goal. \n\nLet me draft something like: \"You are an expert academic reviewer tasked with evaluating research papers to predict their acceptance by a peer-reviewed journal. Assess the paper based on originality, methodology, significance, clarity, and relevance to the field. Prioritize innovation while ensuring feasibility and alignment with the journal's scope. Provide a clear rationale for your prediction, balancing strengths and weaknesses.\" \n\nWait, does that cover all aspects? Maybe include the importance of adhering to submission guidelines and the target audience. Also, ensure the AI considers the paper's contribution to the field and its potential impact. \n\nI think that's a good start. Let me check if the original prompt was too brief and needed more specifics. Yes, adding the prediction aspect and detailed criteria should make the prompt more effective for the AI to follow.\n</think>\n\n<IMPROVED_VARIABLE>You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.</IMPROVED_VARIABLE>", "message": "TextualGradientDescent optimizer response"}
{"name": "textgrad", "msg": "TextualGradientDescent updated text", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py", "filename": "optimizer.py", "module": "optimizer", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "187", "funcName": "step", "created": "1768636577.0721557", "msecs": "72.0", "relativeCreated": "37765.29622077942", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "parameter.value": "You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.", "message": "TextualGradientDescent updated text"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636587.1985354", "msecs": "198.0", "relativeCreated": "47891.67594909668", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Enforcing fairness in private federated learning via the modified method of differential multipliers\n\n# Abstract\nFederated learning with differential privacy, or private federated learning, provides a strategy to train machine learning models while respecting users' privacy. However, differential privacy can disproportionately degrade the performance of the models on under-represented groups, as these parts of the distribution are difficult to learn in the presence of noise. Existing approaches for enforcing fairness in machine learning models have considered the centralized setting, in which the algorithm has access to the users' data. This paper introduces an algorithm to enforce group fairness in private federated learning, where users' data does not leave their devices. First, the paper extends the modified method of differential multipliers to empirical risk minimization with fairness constraints, thus providing an algorithm to enforce fairness in the central setting. Then, this algorithm is extended to the private federated learning setting. The proposed algorithm, FPFL, is tested on a federated version of the Adult dataset and an \"unfair\" version of the FEMNIST dataset. The experiments on these datasets show how private federated learning accentuates unfairness in the trained models, and how FPFL is able to mitigate such unfairness.\n\n\n# 1 INTRODUCTION\n\n\nMachine learning requires data to build models. This data is often private and resides on users' devices. Federated learning (FL) (McMahan et al., 2017) is a strategy where multiple users (or other entities) collaborate in training a model under the coordination of a central server or service provider. In FL, devices share only data statistics (e.g. gradients of the model computed from their data) with the server, and therefore the users' data never leaves their devices.\n\n\nHowever, these statistics can still leak information about the users (see Shokri et al. (2017); Fredrikson et al. (2015) for practical attacks recovering the users' identity and reconstructing the users' face images). To protect against this leakage, differential privacy (DP) can be provided. Differential privacy offers a mathematical guarantee on the maximal amount of information any attacker can obtain about a user after observing the released model. In practice, often (Bhowmick et al., 2018; McMahan et al., 2018a;b; Truex et al., 2019; Granqvist et al., 2020) this guarantee is achieved by adding a certain amount of noise to the individuals' statistics. In this paper, federated learning with differential privacy will be referred to as private federated learning (PFL).\n\n\nThe models trained with PFL are often neural networks, since they work well in many use cases, and turn out to be resistant to the noise added for DP. These models have been successfully trained with PFL for tasks like next word prediction or speaker verification (McMahan et al., 2017; Granqvist et al., 2020). However, they are prone to perpetuating societal biases existing in the data (Caliskan et al., 2017) or to discriminate against certain groups even when the data is balanced (Buolamwini & Gebru, 2018). Moreover, when the training is differentially private, degradation in the performance of these models disproportionately impacts under-represented groups (Bagdasaryan et al., 2019). More specifically, the accuracy of the model on minority groups is deteriorated to a larger extent than the accuracy for the majority groups.\n\n\nIn the realm of FL, there has been some research studying how to achieve that the performance of the model is similar among devices (Hu et al., 2020; Huang et al., 2020; Li et al., 2019; 2021). However,\n\n\nthis notion of fairness falls short in terms of protecting users from under-represented groups falling foul of disproportionate treatment. For example, Castelnovo et al. (2021, Section 3.6) show that a model that performs well on the majority of the users will have a good score on individual fairness metrics, even if all the users suffering from bad performance belong to the same group. Conversely, there is little work proposing solutions to enforce group fairness, i.e. that the performance is similar among users from different groups. Current work in this area is either limited to reducing demographic parity in logistic regression (Du et al., 2021) or to minimizing the largest loss across the different groups with a minimax optimization (Mohri et al., 2019). Concurrently to this paper, Cui et al. (2021) proposed an approach to enforce group fairness in FL for general models and metrics. However, this approach does not consider DP. Moreover, most prior work does not consider the adverse effects that DP has on the models trained with private federated learning, or is restricted to logistic regression models (Abay et al., 2020).\n\n\nOn the other hand, work studying the trade-offs between privacy and group fairness proposes solutions that are either limited to simple models such as linear logistic regression (Ding et al., 2020), require an oracle (Cummings et al., 2019), scale poorly for large hypothesis classes (Jagielski et al., 2019), or only offer privacy protection for the variable determining the group (Tran et al., 2021; Rodr\u00edguez-G\u00e1lvez et al., 2021). Furthermore, the aforementioned work only considers the central learning paradigm, and the techniques are not directly adaptable to FL.\n\n\nFor all the above reasons, in this paper, we propose an algorithm to train neural networks with PFL while enforcing group fairness. We pose the problem as an optimization problem with fairness constraints and extend the modified method of differential multipliers (MMDM) (Platt & Barr, 1987) to solve such a problem with FL and DP. Hence, the resulting algorithm (i) is applicable to any model that can be learned using stochastic gradient descent (SGD) or any of its variants, (ii) can be tailored to enforce the majority of the group fairness metrics, (iii) can consider any number of attributes determining the groups, and (iv) can consider both classification and regression tasks.\n\n\nThe paper is structured as follows: in Section 2, we review the background on differential privacy, private federated learning, the MMDM algorithm, and group fairness; in Section 3, we present our approach for fair private federated learning; in Section 4, we describe our experimental results; and in Section 5, we conclude with a summary and interpretation of our findings.\n\n\n# 2 BACKGROUND\n\n\nIn this section, we review several aspects on the theory of private federated learning, the MMDM algorithm, and group fairness that are necessary to develop and understand the proposed algorithm.\n\n\n# 2.1 FEDERATED LEARNING\n\n\nThe federated learning setting focuses on learning a model that minimizes the expected value of a loss function  $\\ell$  using a dataset  $d = (z_{1},z_{2},\\ldots ,z_{n})\\in \\mathcal{D}$  of  $n$  samples distributed across  $K$  users. This paper will consider models parametrized by a fixed number of parameters  $\\boldsymbol {w}\\in \\mathbb{R}^{p}$  and differentiable loss functions  $\\ell$ , which includes neural networks.\n\n\nSince the distribution of the data samples  $Z$  is unknown and each user  $k$  is the only owner of their private dataset  $d^{k}$ , the goal of FL is to find the parameters  $\\boldsymbol{w}^{\\star}$  that minimize the loss function across the samples of the users. That is, defining  $L(d^{k},\\boldsymbol {w})\\coloneqq \\sum_{\\boldsymbol {z}\\in d^{k}}\\ell (\\boldsymbol {z},\\boldsymbol {w})$ , to solve\n\n\n$$\n\n$$\n\\boldsymbol {w} ^ {\\star} = \\underset {\\boldsymbol {w} \\in \\mathbb {R} ^ {p}} {\\arg \\min } \\frac {1}{n} \\sum_ {k = 1} ^ {K} L \\left(d ^ {k}, \\boldsymbol {w}\\right). \\tag {1}\n$$\n\n$$\n\n\nThis way, the parameters  $\\boldsymbol{w}$  can be learned with an approximation of SGD. McMahan et al. (2017) suggest that the central server iteratively samples  $m$  users; they compute and send back to the server an approximation of the model's gradient  $\\nabla_{\\boldsymbol{w}}L(d^{k},\\boldsymbol{w})$ ; and finally the server updates the parameters as dictated by gradient descent  $\\boldsymbol{w} \\gets \\boldsymbol{w} - \\frac{1}{n}\\sum_{k}\\eta \\nabla_{\\boldsymbol{w}}L(d^{k},\\boldsymbol{w})$ , where  $\\eta$  is the learning rate. If the users send back the exact gradient the algorithm is known as FederatedSGD. A generalisation of this, FederatedAveraging, involves users running several local epochs of mini-batch SGD and sending up the difference. However, the method in this paper considers an optimization with fairness constraints, requiring the communication of extra information in each iteration, and therefore will extend FederatedSGD.\n\n\n# 2.2 PRIVATE FEDERATED LEARNING\n\n\nPrivate federated learning combines federated learning with differential privacy. Differential privacy (Dwork et al., 2006; 2014) provides a mathematical guarantee by restricting the amount of information that a randomized function  $m$  of a dataset  $d$  leaks about any individual. More precisely, it ensures that the probability that the output of the function  $m$  on the dataset  $d$  belongs to any set of outcomes  $\\mathcal{O}$  is close to the probability that the output of the function  $m$  applied to a neighbouring dataset  $d'$  (i.e. equal to  $d$  but where all samples from any individual are removed or replaced) belongs to said set of outcomes. Formally, a randomized function  $m$  is  $(\\epsilon, \\delta)$ -differentially private if\n\n\n$$\n\n$$\n\\mathbb {P} [ \\boldsymbol {m} (d) \\in \\mathcal {O} ] \\leq e ^ {\\epsilon} \\mathbb {P} [ \\boldsymbol {m} (d ^ {\\prime}) \\in \\mathcal {O} ] + \\delta , \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\epsilon$  and  $\\delta$  are parameters that determine the strength of the guarantee. Two useful properties of DP are that it composes (i.e. the combination of DP functions is DP) and that post-processing does not undo its privacy guarantees (Dwork et al., 2014). These properties ensure that the modifications that we will make to FederatedSGD will keep the algorithm differentially private.\n\n\nTypically, the functions of interest are not differentially private. For this reason, they are privatized with \"privacy mechanisms\", which apply a random transformation to the output of the function on the dataset. A standard mechanism is the Gaussian mechanism, which adds Gaussian noise to the function's output, calibrating the noise variance with the privacy parameters  $(\\epsilon, \\delta)$  and the maximum individual's contribution to the function's output measured by the  $\\ell_2$  norm.\n\n\nIn the context of FederatedSGD, the functions to privatize are the gradient estimates sent to the server. The individual's contribution is the gradient computed on their data. This gradient is \"norm-clipped\" (Abadi et al., 2016), i.e. it is scaled down so its  $\\ell_2$  norm is at most a predetermined value  $C$ . Then, Gaussian noise is added to the sum of contributions, which is sent to the server.\n\n\nApplying the privacy mechanism to a sum of contributions is a form of central DP. An alternative is local DP (Dwork et al., 2014), where the privacy mechanism is applied to the statistics before leaving the device. However, models trained with local DP often suffer from low utility (Granqvist et al., 2020), and are thus not considered in this paper. Central DP can involve trust in the server that updates the model, or a trusted third party separated from the model, which is how DP was originally formulated. This paper instead assumes the noisy sum is computed through secure multi-party computation (Goryczka & Xiong, 2015), such as secure aggregation (Bonawitz et al., 2017). This is a cryptographic method that ensures the server only sees the sum of contributions. A crucial limitation of secure aggregation is that it can only compute sums, so the novel method introduced in Section 3 is restricted to working only on sums of individuals' statistics.\n\n\n# 2.3 THE MODIFIED METHOD OF DIFFERENTIAL MULTIPLIERS\n\n\nUltimately, our objective is to find a parametric model that minimizes a differentiable loss function and respects some fairness constraints. Thus, this subsection reviews a constrained differential optimization algorithm, the modified method of differential multipliers (MMDM) (Platt & Barr, 1987).\n\n\nThis algorithm tries to find a solution to the following constrained optimization problem:\n\n\n$$\n\n$$\n\\boldsymbol {w} ^ {\\star} = \\underset {\\boldsymbol {w} \\in \\mathbb {R} ^ {p}} {\\arg \\min } \\phi (\\boldsymbol {w}) \\quad \\text {s . t .} \\boldsymbol {g} (\\boldsymbol {w}) = \\boldsymbol {0}, \\tag {P1}\n$$\n\n$$\n\n\nwhere  $\\phi : \\mathbb{R}^p \\to \\mathbb{R}$  is the function to minimize,  $\\pmb{g}(\\pmb{w}) = (g_1(\\pmb{w}), g_2(\\pmb{w}), \\dots, g_r(\\pmb{w}))$  is the concatenation of  $r$  constraint functions, and  $\\{\\pmb{w} \\in \\mathbb{R}^p : \\pmb{g}(\\pmb{w}) = 0\\}$  is the solution subspace.\n\n\nThe algorithm consists of solving the following set of differential equations resulting from the Lagrangian of (P1) with an additional quadratic penalty  $cg(w)^2$  using gradient descent/ascent. This results in an algorithm that applies these updates iteratively:\n\n\n$$\n\n$$\n\\left\\{ \\begin{array}{l} \\boldsymbol {\\lambda} \\leftarrow \\boldsymbol {\\lambda} + \\gamma \\boldsymbol {g} (\\boldsymbol {w}) \\\\ \\boldsymbol {w} \\leftarrow \\boldsymbol {w} - \\eta \\left(\\nabla_ {\\boldsymbol {w}} \\phi (\\boldsymbol {w}) + \\boldsymbol {\\lambda} ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w}) + c \\boldsymbol {g} (\\boldsymbol {w}) ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w})\\right) \\end{array} , \\right. \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\lambda \\in \\mathbb{R}^r$  is a Lagrange (or dual) multiplier,  $c\\in \\mathbb{R}_+$  is a damping parameter,  $\\eta$  is the learning rate of the model parameters, and  $\\gamma$  is the learning rate of the Lagrange multiplier.\n\n\nIntuitively, these updates gradually fulfill (P1). Updating the parameters  $\\nabla_{\\boldsymbol{w}}\\phi(\\boldsymbol{w})$  and  $\\nabla_{\\boldsymbol{w}}g(\\boldsymbol{w})$  respectively enforce function minimization and constraint's satisfaction. Then, the multiplier  $\\lambda$  and the multiplicative factor  $cg(\\boldsymbol{w})$  control how strongly the constraints' violations are penalized.\n\n\nA desirable property of MMDM is that for small enough learning rates  $\\eta, \\gamma$  and large enough damping parameter  $c$ , there is a region comprising the vicinity of each constrained minimum such that if the parameters' initialization is in that region and the parameters remain bounded, then the algorithm converges to a constrained minimum (Platt & Barr, 1987). Intuitively, the term  $cg(\\boldsymbol{w}) \\nabla_{\\boldsymbol{w}} g(\\boldsymbol{w})$  enforces a quadratic shape on the optimization search space to the neighbourhood of the solution subspace, and this local behavior is stronger the larger the damping parameter  $c$ . Thus, the algorithm converges to a minimum if the parameters' initialization is in this locally quadratic region.\n\n\n# 2.4 GROUP FAIRNESS\n\n\nIn this subsection, we mathematically formalize what group fairness means. To ease the exposition, we describe this notion in the central setting, i.e. where the data is not distributed across users.\n\n\nConsider a dataset  $d = (z_{1}, z_{2}, \\ldots, z_{n})$  of  $n$  samples. The dataset is partitioned into groups from  $\\mathcal{A}$  such that each sample  $z_{i}$  belongs to group  $a_{i}$ . Group fairness considers how differently a model treats the samples belonging to each group. Many fairness metrics can be written in terms of the similarity of the expected value of a function of interest  $f$  of the model evaluated on the general population  $d$  with that on the population of each group  $d_{a} = \\{z_{i} \\in d : a_{i} = a\\}$  (Agarwal et al., 2018; Fioretto et al., 2020). That is, if we consider a supervised learning problem, where  $z_{i} = (x_{i}, y_{i}, a_{i})$  and where the output of the model is an approximation  $\\hat{\\pmb{y}}_{i}$  of  $\\pmb{y}_{i}$ , we say a model is fair if\n\n\n$$\n\n$$\n\\mathbb {E} [ f (\\boldsymbol {X}, \\boldsymbol {Y}, \\hat {\\boldsymbol {Y}}) \\mid A = a ] = \\mathbb {E} [ f (\\boldsymbol {X}, \\boldsymbol {Y}, \\hat {\\boldsymbol {Y}}) ] \\text {f o r a l l} a \\in \\mathcal {A}. \\tag {4}\n$$\n\n$$\n\n\nMost of the group fairness literature focuses on the case of binary classifiers, i.e.  $\\hat{y}_i \\in \\{0,1\\}$ , and on the binary group case, i.e.  $\\mathcal{A} = \\{0,1\\}$ . However, many of the fairness metrics can be extended to general output spaces  $\\mathcal{V}$  and categorical groups  $\\mathcal{A}$ . It is common that the function  $f$  is the indicator function  $\\mathbb{1}$  of some logical relationship between the random variables, thus turning (4) to an equality between probabilities. As an example, we describe two common fairness metrics that will be used later in the paper. For a comprehensive survey of different fairness metrics and their inter-relationships, please refer to Verma & Rubin (2018); Castelnovo et al. (2021).\n\n\nFalse negative rate (FNR) parity (or equal opportunity) (Hardt et al., 2016) This fairness metric is designed for binary classification and binary groups. It was originally defined as equal true positive rate between the groups, which is equivalent to an equal FNR between each group and the overall population. That is, if we let  $f(\\mathbf{X}, Y, \\hat{Y}) = \\mathbb{E}[\\mathbb{1}(\\hat{Y} = 0) \\mid Y = 1]$  then (4) reduces to\n\n\n$$\n\n$$\n\\mathbb {P} [ \\hat {Y} = 0 \\mid Y = 1, A = a ] = \\mathbb {P} [ \\hat {Y} = 0 \\mid Y = 1 ] \\text {f o r a l l} a \\in \\mathcal {A}. \\tag {5}\n$$\n\n$$\n\n\nThis is usually a good metric when the target variable  $Y$  is something positive such as being granted a loan or hired for a job, since we want to minimize the group disparity of misclassification among the individuals deserving such a loan or such a job (Hardt et al., 2016; Castelnovo et al., 2021).\n\n\nAccuracy parity (or overall misclassification rate) (Zafar et al., 2017) This fairness metric is also designed for binary classification and binary groups. Nonetheless, it applies well to general tasks and categorical groups. In this case, if we let  $f(\\boldsymbol{X}, Y, \\hat{Y}) = \\mathbb{1}(\\hat{Y} = Y)$  then (4) reduces to\n\n\n$$\n\n$$\n\\mathbb {P} [ \\hat {Y} = Y \\mid A = a ] = \\mathbb {P} [ \\hat {Y} = Y ] \\text {f o r a l l} a \\in \\mathcal {A}. \\tag {6}\n$$\n\n$$\n\n\nThis is usually a good metric when there is not a clear positive or negative semantic meaning to the target variable, and also when this variable is not binary.\n\n\n# 3 AN ALGORITHM FOR FAIR AND PRIVATE FEDERATED LEARNING (FPFL)\n\n\nIn this section, we describe the proposed algorithm to enforce fairness in PFL of parametric models learned with SGD. First, we describe an adaptation of the MMDM algorithm to enforce fairness in standard central learning. Then, we extend that algorithm to PFL.\n\n\n# 3.1 ADAPTING THE MMDM ALGORITHM TO ENFORCE FAIRNESS\n\n\nConsider a dataset  $d = (z_{1}, z_{2}, \\ldots, z_{n})$  of  $n$  samples. The dataset is partitioned into groups from  $\\mathcal{A}$  such that each sample  $z_{i}$  belongs to group  $a_{i}$ . Then, let us consider also a supervised learning setting where  $z_{i} = (x_{i}, y_{i}, a_{i})$ , the model's output  $\\hat{y}_{i}$  approximates  $y_{i}$ , and the model is parametrized by the parameters  $\\boldsymbol{w} \\in \\mathbb{R}^{p}$ . Finally, assume we have no information about the variables'  $(X, Y, A)$  distribution apart from the available samples.\n\n\nWe concern ourselves with the task of finding the model's parameters  $\\boldsymbol{w}^{\\star}$  that minimize a loss function  $\\ell$  across the data samples and enforce a measure of fairness on the model. That is, we substitute the expected values of the fairness constraints in (4) with empirical averages:\n\n\n$$\n\n$$\n\\boldsymbol {w} ^ {\\star} = \\underset {\\boldsymbol {w} \\in \\mathbb {R} ^ {p}} {\\arg \\min } \\frac {L (d , \\boldsymbol {w})}{n} \\quad \\text {s . t .} \\left| \\frac {F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}} \\right| \\leq \\alpha , \\text {f o r a l l} a \\in \\mathcal {A}, \\tag {P2}\n$$\n\n$$\n\n\nwhere  $L(d, \\boldsymbol{w}) \\coloneqq \\sum_{\\boldsymbol{z} \\in d} \\ell(\\boldsymbol{z}, \\boldsymbol{w}), F(d', \\boldsymbol{w}) \\coloneqq \\sum_{\\boldsymbol{z} \\in d'} f(\\boldsymbol{z}, \\boldsymbol{w}), d'$  is a subset of  $d$  that varies among different fairness metrics,  $n'$  is the number of samples in  $d'$ ,  $d'_a$  is the set of samples in  $d'$  such that  $a_i = a$ ,  $n'_a$  is the number of samples in  $d'_a$ ,  $f$  is the function employed for the fairness metric definition, and  $\\alpha$  is a tolerance threshold. The subset  $d'$  is chosen based on the function  $f$  used for the fairness metric: if the fairness function does not involve any conditional expectation (e.g. accuracy parity), then  $d' = d$ ; if, on the other hand, the subset involves a conditional expectation, then the  $d'$  is the subset of  $d$  where that condition holds, e.g. when the fairness metric is FNR parity  $d' = \\{(x, y, a) \\in d : y = 1\\}$ . We exclude the case where the conditioning in the expectation is  $\\hat{Y}$ , thus excluding predictive parity as a target fairness metric.\n\n\nNote that the constraints are not strict, meaning that there is a tolerance  $\\alpha$  for how much the function  $f$  can vary between certain groups and the overall population. The reason for this choice is twofold. First, it facilitates the training since the solution subspace is larger. Second, it is known that some fairness metrics, such as FNR parity, are incompatible with DP and non-trivial accuracy. However, if the fairness metric is relaxed, fairness, accuracy, and privacy can coexist (Jagielski et al., 2019; Cummings et al., 2019).\n\n\nThis way, we may re-write (P2) in the form of (P1) to solve the problem with MMDM. To do so we let  $\\phi(\\boldsymbol{w}) = L(d, \\boldsymbol{w}) / n$  and  $\\boldsymbol{g}(\\boldsymbol{w}) = (g_0(\\boldsymbol{w}), g_1(\\boldsymbol{w}), \\ldots, g_{|\\mathcal{A}| - 1}(\\boldsymbol{w}))$ , where\n\n\n$$\n\n$$\ng _ {a} (\\boldsymbol {w}) = \\left\\{ \\begin{array}{l l} h _ {a} (\\boldsymbol {w}) & \\text {i f} h _ {a} (\\boldsymbol {w}) \\geq 0 \\\\ 0 & \\text {o t h e r w i s e} \\end{array} \\right. \\quad \\text {a n d} \\quad h _ {a} (\\boldsymbol {w}) := \\left| \\frac {F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}} \\right| - \\alpha . \\tag {7}\n$$\n\n$$\n\n\nTherefore, the parameters are updated according to\n\n\n$$\n\n$$\n\\left\\{ \\begin{array}{l} \\boldsymbol {\\lambda} \\leftarrow \\boldsymbol {\\lambda} + \\gamma \\boldsymbol {g} (\\boldsymbol {w}) \\\\ \\boldsymbol {w} \\leftarrow \\boldsymbol {w} - \\eta \\left(1 / _ {n} \\nabla_ {\\boldsymbol {w}} L (d, \\boldsymbol {w}) + \\boldsymbol {\\lambda} ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w}) + c \\boldsymbol {g} (\\boldsymbol {w}) ^ {T} \\nabla_ {\\boldsymbol {w}} \\boldsymbol {g} (\\boldsymbol {w})\\right) \\end{array} , \\right. \\tag {8}\n$$\n\n$$\n\n\nwhere we note that  $\\nabla_{\\pmb{w}}\\pmb{g}(\\pmb{w}) = (\\nabla_{\\pmb{w}}g_0(\\pmb{w}),\\nabla_{\\pmb{w}}g_1(\\pmb{w}),\\dots,\\nabla_{\\pmb{w}}g_{|\\mathcal{A}| - 1}(\\pmb{w}))$  and\n\n\n$$\n\n$$\n\\nabla_ {\\boldsymbol {w}} g _ {a} (\\boldsymbol {w}) = \\left\\{ \\begin{array}{l l} \\operatorname {s i g n} \\left(\\frac {F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}}\\right) \\left(\\frac {\\nabla_ {\\boldsymbol {w}} F \\left(d ^ {\\prime} , \\boldsymbol {w}\\right)}{n ^ {\\prime}} - \\frac {\\nabla_ {\\boldsymbol {w}} F \\left(d _ {a} ^ {\\prime} , \\boldsymbol {w}\\right)}{n _ {a} ^ {\\prime}}\\right) & \\text {i f} h _ {a} (\\boldsymbol {w}) \\geq 0 \\\\ 0 & \\text {o t h e r w i s e} \\end{array} . \\right. \\tag {9}\n$$\n\n$$\n\n\nNow, the fairness-enforcing problem (P2) can be solved with gradient descent/ ascent or mini-batch stochastic gradient descent/ ascent, where instead of the full dataset  $d$ ,  $d'$ , and  $d_{a}^{\\prime}$ , one considers batches  $b$ ,  $b^{\\prime}$ , and  $b_{a}^{\\prime}$  (or subsets) of that dataset. Moreover, it can be learned with DP adapting the DP-SGD algorithm from (Abadi et al., 2016), where a clipping bound and Gaussian noise is included in both the network parameters' and multipliers' individual updates. Nonetheless, there are a series of caveats of doing so. First, the batch size  $|b^{\\prime}|$  should be large enough to, on average, have enough samples of each group  $a \\in \\mathcal{A}$  so that the difference  $\\left(\\frac{F(b', w)}{|b'|} - \\frac{F(b_{a}', w)}{|b_{a}'|}\\right)$  is well estimated. Second, in many situations of interest such as when we want to enforce FNR parity or accuracy parity, the function  $f$  employed for the fairness metric is not differentiable and thus  $\\nabla_{w} F(d', w)$  does not exist. To solve this issue, we resort to estimate the gradient  $\\nabla_{w} F(d', w)$  using a differentiable estimation of the function aggregate  $F(d', w)$ ; see Appendix A.1 for the details on these estimates in the above situations of interest.\n\n\nWe conclude this section noting the similarities and differences of this work and (Tran et al., 2021). Even though their algorithm is derived from first principles on Lagrangian duality, it is ultimately equivalent to an application of the basic method of differential multipliers (BMDM) to solve a problem equivalent to (P2) when  $\\alpha = 0$ . Nonetheless, the two algorithms differ in three main aspects:\n\n- 1. BMDM vs MMDM: BMDM is equivalent to MMDM when  $c = 0$ , that is, when the effect of making the neighbourhood of the solution subspace quadratic is not present. Moreover, the guarantee of achieving a local minimum that respects the constraints does not hold for  $c = 0$  unless the problem is simple (e.g. quadratic programming).\n\n- 2. How they deal with impossibilities in the goal of achieving perfect fairness together with privacy and accuracy. Tran et al. (2021) include a limit  $\\lambda_{\\mathrm{max}}$  to the Lagrange multiplier to avoid floating point errors and reaching trivial solutions, which in our case is taken care by the tolerance  $\\alpha$ , which, in contrast to  $\\lambda_{\\mathrm{max}}$ , is an interpretable parameter.\n\n- 3. This paper uses the exact expression in (9) for the gradient  $\\nabla_{\\boldsymbol{w}}\\left|\\frac{F(d', \\boldsymbol{w})}{n'} - \\frac{F(d_{a}', \\boldsymbol{w})}{n_{a}'}\\right|$ , while Tran et al. (2021) ignore the sign of the difference and use  $\\left|\\frac{\\nabla_{\\boldsymbol{w}} F(d', \\boldsymbol{w})}{n'} - \\frac{\\nabla_{\\boldsymbol{w}} F(d_{a}', \\boldsymbol{w})}{n_{a}'}\\right|$ .\n\n\nIn the next subsection, we extend the algorithm to PFL, which introduces two new differences with (Tran et al., 2021). Firstly, the privacy guarantees will be provided for the individuals, and not only to the group to which they belong; and secondly, the algorithm will be tailored to FL.\n\n\n# 3.2 EXTENDING THE ALGORITHM TO PRIVATE FEDERATED LEARNING\n\n\nIn the federated learning setting we now consider that the dataset  $d$  is distributed across  $K$  users such that each user maintains a local dataset  $d^{k}$  with  $n^k$  samples. This setting applies to both crossdevice and cross-silo FL, since each sample contains the group information  $a_{i}$ . Nonetheless, as in the central setting, the task is to find the model's parameters  $\\boldsymbol{w}^{\\star}$  that minimize the loss function across the data samples of the users while enforcing a measure of fairness to the model. That is, to solve (P2).\n\n\nTo achieve this goal, we might first combine the ideas from FederatedSGD (McMahan et al., 2017) and the previous section to extend the developed adaptation of MMDM to FL. Our adaptation of MMDM cannot be combined with FederatedAveraging; for more details see Appendix A.2. In order to perform the model updates from (8), the central server requires the following statistics:\n\n\n$$\n\n$$\n\\nabla_ {\\boldsymbol {w}} L (d, \\boldsymbol {w}); F \\left(d ^ {\\prime}, \\boldsymbol {w}\\right); \\left[ F \\left(d _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}; \\nabla_ {\\boldsymbol {w}} F \\left(d ^ {\\prime}, \\boldsymbol {w}\\right); \\left[ \\nabla_ {\\boldsymbol {w}} F \\left(d _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}; n ^ {\\prime}; \\left[ n _ {a} ^ {\\prime} \\right] _ {a \\in \\mathcal {A}}. \\tag {10}\n$$\n\n$$\n\n\nHowever, some of these statistics can be obtained from the others:  $F(d', \\boldsymbol{w}) = \\sum_{a \\in \\mathcal{A}} F(d'_a, \\boldsymbol{w})$ ,  $\\nabla_{\\boldsymbol{w}} F(d', \\boldsymbol{w}) = \\sum_{a \\in \\mathcal{A}} \\nabla_{\\boldsymbol{w}} F(d'_a, \\boldsymbol{w})$ , and  $n' = \\sum_{a \\in \\mathcal{A}} n'_a$ . Moreover, as mentioned in the previous section, one might use a sufficiently large batch  $b$  of the data instead of the full dataset for each update. Therefore, we consider an iterative algorithm where, at each iteration, the central server samples a cohort of  $m$  users  $S$  that report a vector with the sufficient statistics for the update, that is\n\n\n$$\n\n$$\n\\boldsymbol {v} ^ {k} = \\left[ \\nabla_ {\\boldsymbol {w}} L \\left(d ^ {k}, \\boldsymbol {w}\\right), \\left[ F \\left(\\left(d ^ {k}\\right) _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\nabla_ {\\boldsymbol {w}} F \\left(\\left(d ^ {k}\\right) _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\left(n ^ {k}\\right) _ {a} ^ {\\prime} \\right] _ {a \\in \\mathcal {A}} \\right]. \\tag {11}\n$$\n\n$$\n\n\nThis way, if we define the batch  $b$  as the sum of the  $m$  users' local datasets  $b \\coloneqq \\sum_{k \\in S} d^k$  and the batch  $b'$  analogously, then the aggregation of each user's vectors results in\n\n\n$$\n\n$$\n\\boldsymbol {v} = \\sum_ {k \\in S} \\boldsymbol {v} ^ {k} = \\left[ \\nabla_ {\\boldsymbol {w}} L (b, \\boldsymbol {w}), \\left[ F \\left(b _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\nabla_ {\\boldsymbol {w}} F \\left(b _ {a} ^ {\\prime}, \\boldsymbol {w}\\right) \\right] _ {a \\in \\mathcal {A}}, \\left[ \\left| b _ {a} ^ {\\prime} \\right| \\right] _ {a \\in \\mathcal {A}} \\right], \\tag {12}\n$$\n\n$$\n\n\nwhich contains all the sufficient statistics for the parameters' update.\n\n\nFinally, the resulting algorithm, termed Fair PFL or FPFL and described in Algorithm 1, inspired by the ideas from (McMahan et al., 2018b; Truex et al., 2019; Granqvist et al., 2020; Bonawitz et al., 2017) guarantees the users' privacy by (i) making sure the aggregation of the users' sent vectors is done securely with secure aggregation (Bonawitz et al., 2017), and (ii) clipping each users' vectors with a clipping bound  $C$  and employing the Gaussian mechanism to the sum of the clipped vectors (lines 13 and 15). The variance of the Gaussian noise is calculated according to the refined moments accountant privacy analysis from (Wang et al., 2019), taking into account the number of iterations  $T$ , the cohort size  $m$ , the total number of users (or population size)  $K$ , and the privacy parameters  $\\epsilon$  and  $\\delta$ . Note that the post-processing property of DP keeps the algorithm private even if the received noisy vector  $v$  is processed to extract the relevant information for the model's update.\n\n\nBatch size. This algorithm can also be employed using only a fraction of the user's data in each update, i.e. using a batch  $b^{k}$  of their local dataset  $d^{k}$ . Nonetheless, it is desirable (i) to delegate as much computation to the user as possible and (ii) to use as much users' data as possible to have a good approximation of the performance metrics  $F(d_{a}^{\\prime},w)$ , which are needed to enforce fairness.\n\n\n# 4 EXPERIMENTAL RESULTS\n\n\nWe study the performance of the algorithm in two classification tasks. The first task is a binary classification based on some demographic data from the publicly available Adult dataset (Dua & Graff, 2017). The fairness metric considered for this task is FNR parity. The second task is a multi-class classification where there are three different attributes. This task considers accuracy parity as the fairness metric and uses a modification of the publicly available FEMNIST dataset (Caldas et al., 2018), where only digits are considered and the classes are digits written with a black pen on a white sheet, digits written with a blue pen on a white sheet, and digits written with white chalk on a blackboard (see Figure 1).\n\n\n![Figure 1: Samples from the Unfair FEMNIST dataset. Digits written on a white sheet with black pen (top), on a white sheet with blue pen (middle), and on a blackboard with white chalk (bottom).](images/85dfd2b5c6ccd6c20a5a5b9aa63971fd2f1060733ee6962cf97bee00b226c963.jpg)\n\n\nFor the first task, we first compare the performance of\n\n\nthe MMDM algorithm with BMDM, Tran et al. (2021), and vanilla SGD centrally and without privacy. After that, for both tasks, we confirm how FederatedSGD deteriorates the performance of the model for the under-represented classes when clipping and noise (DP) are introduced. Finally, we demonstrate how FPFL can, under the appropriate circumstances, level the performance of the model across the groups without largely decreasing the overall performance of the model.\n\n\nIn all our experiments, the fairness metrics are defined as the maximum difference of the value of a performance measure between the general testing data and each of the groups described by that sensitive attribute. Moreover, the privacy parameters are  $(\\epsilon, \\delta) = (2, 1 / K)$ , where  $K$  is the population size. All experimental details and additional experiments are in Appendix B.\n\n\n# 4.1 RESULTS ON THE ADULT DATASET\n\n\nWe start our experiments comparing the performance and fairness in the central, non-federated, non-private setting. We trained a shallow network (consisting of one hidden layer with 10 hidden units and a ReLU activation function) with these algorithms, where we tried to enforce FNR parity with a tolerance  $\\alpha = 0.02$ . For MMDM, the damping parameter is  $c = 2$ . For Tran et al. (2021), the multiplier threshold is  $\\lambda_{\\mathrm{max}} = 0.05$ . The results after 1,000 iterations are displayed in Table 1, which also shows the gap in other common measures of fairness such as the equalized odds, the demographic parity, or the predictive parity, see e.g. Castelnovo et al. (2021).\n\n\nCompared to Federated SGD, the MMDM algorithm reduces the FNR gap from  $7\\%$  to  $0.5\\%$ , while hardly reducing the accuracy of the model. Similarly, the gap in the equalized odds (EO), which is a stronger fairness notion than the FNR parity, also decreases from around  $7\\%$  to  $3\\%$ . Moreover, the demographic parity (DemP) gap, which considers the probability of predicting one or the other target class, also improves. In terms of predictive parity (PP), which uses the precision as the performance function, the MMDM algorithm did not improve the parity among groups. The BMDM algorithm, which lacks a quadratic term of the fairness function, performs even slightly better in the central scenario. However, the algorithm from Tran et al. (2021), an approximation to BMDM as discussed in Section 3.1, gives only a small improvement. Therefore, we continue our analysis to the federated and private settings only with the BMDM and MMDM algorithms.\n\n\nThe second experiment is to study how federated learning, clipping, and DP decrease the performance for the under-represented groups, thus increasing the fairness gap on the different fairness metrics. For that, we trained the same network with FederatedSGD and versions of this algorithm where only clipping was performed and where both clipping and DP were included. They were trained with a cohort size of  $m = 200$  for  $T = 1,000$  iterations and the model with best training cohort accuracy was selected. The privacy parameters are  $(\\epsilon, \\delta) = (2,5 \\cdot 10^{-5})$ . The results\n\n\nTable 1: Performance in the central (i.e. non-federated) setting on the Adult dataset. The fairness metric optimized is FNR parity. The tolerance for BMDM and MMDM is  $\\alpha = 0.02$ .\n\n\n<table><tr><td>Algorithm</td><td>Accuracy</td><td>FNR gap</td><td>EO gap</td><td>DemP gap</td><td>PP gap</td></tr><tr><td>SGD</td><td>0.857</td><td>0.071</td><td>0.071</td><td>0.113</td><td>0.016</td></tr><tr><td>Tran et al. (2021) without privacy</td><td>0.856</td><td>0.048</td><td>0.048</td><td>0.108</td><td>0.036</td></tr><tr><td>BMDM</td><td>0.857</td><td>0.001</td><td>0.030</td><td>0.094</td><td>0.046</td></tr><tr><td>MMDM (this paper)</td><td>0.856</td><td>0.005</td><td>0.028</td><td>0.091</td><td>0.063</td></tr></table>\n\n\nTable 2: Performance of a neural network on the Adult dataset when trained with FL. The fairness metric considered for BMDM-FL and FPFL is FNR parity and the tolerance is  $\\alpha = 0.02$ .\n\n\n<table><tr><td>Algorithm</td><td>Accuracy</td><td>FNR gap</td><td>EO gap</td><td>DemP gap</td><td>PP gap</td></tr><tr><td>FL</td><td>0.851</td><td>0.121</td><td>0.121</td><td>0.122</td><td>0.037</td></tr><tr><td>BMDM-FL</td><td>0.854</td><td>0.043</td><td>0.043</td><td>0.101</td><td>0.016</td></tr><tr><td>Fair FL</td><td>0.855</td><td>0.036</td><td>0.039</td><td>0.108</td><td>0.033</td></tr><tr><td>FL + Clip</td><td>0.844</td><td>0.169</td><td>0.169</td><td>0.129</td><td>0.051</td></tr><tr><td>BMDM-FL + Clip</td><td>0.851</td><td>0.052</td><td>0.052</td><td>0.105</td><td>0.008</td></tr><tr><td>Fair FL + Clip</td><td>0.853</td><td>0.018</td><td>0.029</td><td>0.090</td><td>0.016</td></tr><tr><td>Private FL</td><td>0.847</td><td>0.148</td><td>0.148</td><td>0.126</td><td>0.041</td></tr><tr><td>BMDM-PFL</td><td>0.850</td><td>0.023</td><td>0.035</td><td>0.097</td><td>0.009</td></tr><tr><td>Fair Private FL</td><td>0.851</td><td>0.001</td><td>0.027</td><td>0.087</td><td>0.042</td></tr></table>\n\n\nare displayed in Table 2. The network becomes less fair under all the metrics considered, compared with the central setting. Moreover, the introduction of clipping largely increases the unfairness of the models reaching more than a  $16\\%$  gap in FNR. The addition of DP requires an increase in cohort size from 200 to 1,000, but does not have a larger effect on the unfairness of the models (see Table 5 in Appendix B.1 for the details). These observations are in line with (Bagdasaryan et al., 2019), where they note that the under-represented groups usually have the higher loss gradients and thus clipping affects them more than the majority groups.\n\n\nAfter that, we repeated the above experiment with FPFL and BMDM with DP. FPFL and BMDM with DP converged to a solution faster than FederatedSGD, and thus the models were trained for only  $T = 250$  iterations. Here the model with the best training cohort accuracy while respecting the fairness condition on the training cohort data was selected. Note that the fairness condition is evaluated with noisy statistics, so a model may be deemed as fair while slightly violating the desired constraints. The results are also included in Table 2 to aid the comparison. We note how, similarly to the central case, the model trained with FPFL manages to enforce the fairness constraints while keeping a similar accuracy. In contrast, BMDM seems to not be able to find network weights that respect the fairness conditions, as expected by the less favorable (\"less convex\") loss surface around these solutions. Moreover, clipping does not seem to affect largely the performance of FPFL since it compensates the gradient loss clipping with the fairness enforcement.\n\n\n# 4.2 RESULTS ON THE UNFAIR FEMNIST DATASET\n\n\nWe start our experiments confirming again the hypothesis and findings from Bagdasaryan et al. (2019) that clipping and DP disproportionately affect under-represented groups. For that, we trained a convolutional network (consisting of 2 convolution layers with kernel of size  $5 \\times 5$ , stride of 2, ReLU activation function, and 32 and 64 filters respectively) with FederatedSGD and versions of this algorithm where only clipping was performed and where both clipping and DP were included. For the FEMNIST experiments, the privacy parameters are  $(\\epsilon, \\delta) = (2, 2.5 \\cdot 10^{-4})$  and the damping parameter is  $c = 20$ . They were trained with a cohort size of  $m = 100$  for  $T = 2,000$  iterations and the model with best training cohort accuracy was selected. The results are displayed in Table 3. Similarly to before, clipping increases the accuracy gap from  $13\\%$  to almost  $17\\%$ . In this case, since the number of users  $K$  is small, the DP noise is large compared to the users' statistics. Therefore, the\n\n\naccuracy drops from more than  $94\\%$  with clipping to  $80.7\\%$  when DP is also used, and the accuracy gap increases to more than  $40\\%$ .\n\n\nThe second experiment tests whether FPFL can remedy the unfairness without decreasing accuracy too much. We trained the same convolutional network, again for  $T = 2,000$  iterations, and selected the model with the best training cohort accuracy that respected the fairness condition on the training cohort. When DP noise is not included, FPFL reduces the accuracy gap with FederatedSGD by around  $9\\%$  while keeping the accuracy within  $1\\%$ . We note how, as before, clipping largely does not affect the ability of FPFL to enforce fairness. However, note that since the data is more non-i.i.d. than before (i.e. there are more differences between the distribution of each user) the models that are deemed fair in the training cohort may not be as fair in the general population, and now we see a larger gap between the desired tolerance  $\\alpha = 0.04$  and the obtained accuracy gap from FPFL without noise (0.047 and 0.053 without and with clipping).\n\n\nTable 3: Performance of a convolutional network on the Unfair FEMNIST when trained with different algorithms: like Table 2, starting with FederatedSGD and ending with FPFL. The fairness metric considered for FPFL is accuracy parity and the tolerance is  $\\alpha = 0.04$ .\n\n\n<table><tr><td>Algo.</td><td>Population</td><td>Accuracy</td><td>Acc. gap</td></tr><tr><td colspan=\"4\">m = 100</td></tr><tr><td>FL</td><td>K</td><td>0.960</td><td>0.134</td></tr><tr><td>FFL</td><td>K</td><td>0.950</td><td>0.047</td></tr><tr><td>FL + Clip</td><td>K</td><td>0.946</td><td>0.166</td></tr><tr><td>FFL + Clip</td><td>K</td><td>0.954</td><td>0.053</td></tr><tr><td>PFL</td><td>K</td><td>0.807</td><td>0.409</td></tr><tr><td>FPFL</td><td>K</td><td>0.093</td><td>0.015</td></tr><tr><td colspan=\"4\">m = 2,000</td></tr><tr><td>PFL</td><td>100K</td><td>0.951</td><td>0.157</td></tr><tr><td>FPFL</td><td>100K</td><td>0.903</td><td>0.074</td></tr><tr><td>PFL</td><td>1,000K</td><td>0.951</td><td>0.153</td></tr><tr><td>FPFL</td><td>1,000K</td><td>0.927</td><td>0.073</td></tr></table>\n\n\nWhen DP is used, the noise is too large for FPFL to function properly and often the sign of the constraints' gradient, see (9), flips. Note that in the estimation of the performance function, i.e.  $F(d_{a}, \\boldsymbol{w}) / n_{a}$ , both the numerator and denominator are obtained from a noisy vector, thus making the estimators more sensitive to noise than the estimators for plain FederatedSGD. This is due to two main factors: (i) the larger the model, the larger the aggregation of the gradients for each weight, and thus more noise needs to be added; and (ii) the smaller the amount of users, the larger the noise that needs to be added.\n\n\nFor this reason, we considered the hypothetical scenario where the population, used for calculating the DP noise, is 100 and 1,000 times larger, which is a conservative assumption for federated learning deployments (Apple, 2017). Then, we repeated the experiment with DP FederatedSGD and FPFL and increased the cohort size to  $m = 2,000$ . In this scenario, DP FederatedSGD maintained an accuracy gap of more than  $15\\%$  while FPFL reduced this gap to less than a half in both cases. Nonetheless, the accuracy still decreases slightly, with a reduction of around  $5\\%$  and  $2\\%$  respectively compared with DP FederatedSGD.\n\n\n# 5 CONCLUSION\n\n\nIn this paper, we studied and proposed a solution to the problem of group fairness in private federated learning. For this purpose, we adapt the modified method of multipliers (MMDM) (Platt & Barr, 1987) to empirical loss minimization with fairness constraints, producing an algorithm for enforcing fairness in central learning. Then, we extend this algorithm to private federated learning.\n\n\nThrough experiments in the Adult (Dua & Graff, 2017) and a modified version of the FEM-NIST (Caldas et al., 2018) datasets, we first confirm previous knowledge that DP disproportionately affects performance on under-represented groups (Bagdasaryan et al., 2019), with the further observation that this is true for many different fairness metrics, and not just for accuracy parity. The proposed FPFL algorithm is able to remedy this unfairness even in the presence of DP.\n\n\nLimitations and future work. The FPFL algorithm is more sensitive to DP noise than other algorithms for PFL. This requires increasing the cohort size or ensuring that enough users take part in training. Still, for the experiments, the number of users required is still lower (more than an order of magnitude) than the usual amount of users available in deployments of federated learning settings (Apple, 2017). In the future, we intend to scale the algorithm to larger models and empirically integrate it with faster training methods like Federated Averaging (McMahan et al., 2017).\n\n\n# REFERENCES\n\n- Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security (CCS), pp. 308-318, 2016.\n\n- Annie Abay, Yi Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube Chuba, and Heiko Ludwig. Mitigating bias in federated learning. arXiv preprint arXiv:2012.02447, 2020.\n\n- Alekh Agarwal, Alina Beygelzimer, Miroslav Dud\u00edk, John Langford, and Hanna Wallach. A reductions approach to fair classification. In International Conference on Machine Learning, pp. 60-69. PMLR, 2018.\n\n- Differential Privacy Team Apple. Learning with privacy at scale. Technical report, Apple, 2017.\n\n- Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate impact on model accuracy. Advances in Neural Information Processing Systems (NeurIPS), 32: 15479-15488, 2019.\n\n- Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan Rogers. Protection against reconstruction and its applications in private federated learning. arXiv preprint arXiv:1812.00984, 2018.\n\n- Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 1175-1191, 2017.\n\n- Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pp. 77-91. PMLR, 2018.\n\n- Sebastian Caldas, Peter Wu, Tian Li, Jakub Kone\u010dny, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018. URL https://arxiv.org/abs/1812.01097.\n\n- Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183-186, 2017.\n\n- Alessandro Castelnovo, Riccardo Crupi, Greta Greco, and Daniele Regoli. The zoo of fairness metrics in machine learning. arXiv preprint arXiv:2106.00467, 2021.\n\n- Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 2921-2926. IEEE, 2017.\n\n- Sen Cui, Weishen Pan, Jian Liang, Changshui Zhang, and Fei Wang. Addressing algorithmic disparity and performance inconsistency in federated learning, 2021.\n\n- Rachel Cummings, Varun Gupta, Dhamma Kimpara, and Jamie Morgenstern. On the compatibility of privacy and fairness. In Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization, pp. 309-315, 2019.\n\n- Jiahao Ding, Xinyue Zhang, Xiaohuan Li, Junyi Wang, Rong Yu, and Miao Pan. Differentially private and fair classification via calibrated functional mechanism. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 622-629, 2020.\n\n- Wei Du, Depeng Xu, Xintao Wu, and Hanghang Tong. Fairness-aware agnostic federated learning. In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM), pp. 181-189. SIAM, 2021.\n\n- Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.\n\n- Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pp. 265-284. Springer, 2006.\n\n- Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9(3-4):211-407, 2014.\n\n- Ferdinando Fioretto, Pascal Van Hentenryck, Terrence WK Mak, Cuong Tran, Federico Baldo, and Michele Lombardi. Lagrangian duality for constrained deep learning. arXiv preprint arXiv:2001.09394, 2020.\n\n- Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pp. 1322-1333, 2015.\n\n- Slawomir Goryczka and Li Xiong. A comprehensive comparison of multiparty secure additions with differential privacy. IEEE Transactions on Dependable and Secure Computing, 2015. doi: 10.1109/TDSC.2015.2484326.\n\n- Filip Granqvist, Matt Seigel, Rogier van Dalen, \u00c1ine Cahill, Stephen Shum, and Matthias Paulik. Improving on-device speaker verification using federated learning with privacy. Interspeech, pp. 4328-4332, 2020.\n\n- Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in neural information processing systems, 29:3315-3323, 2016.\n\n- Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, and Yaoliang Yu. FedMGDA+: Federated learning meets multi-objective optimization. arXiv preprint arXiv:2006.11489, 2020.\n\n- Wei Huang, Tianrui Li, Dexian Wang, Shengdong Du, and Junbo Zhang. Fairness and accuracy in federated learning. arXiv preprint arXiv:2012.10069, 2020.\n\n- Matthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman. Differentially private fair learning. In International Conference on Machine Learning (ICML), pp. 3000-3008. PMLR, 2019.\n\n- Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning. arXiv preprint arXiv:1905.10497, 2019.\n\n- Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In Proceedings of the 38th International Conference on Machine Learning (ICML), pp. 6357-6368. PMLR, 2021.\n\n- Jingcheng Liu and Kunal Talwar. Private selection from private candidates. In Proceedings of the ACM SIGACT Symposium on Theory of Computing, 2019.\n\n- Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag\u00fcera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1273-1282. PMLR, 2017.\n\n- H Brendan McMahan, Galen Andrew, Ulfar Erlingsson, Steve Chien, Ilya Mironov, Nicolas Papernot, and Peter Kairouz. A general approach to adding differential privacy to iterative training procedures. arXiv preprint arXiv:1812.06210, 2018a.\n\n- H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. In International Conference on Learning Representations (ICLR), 2018b.\n\n- Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In International Conference on Machine Learning, pp. 4615-4625. PMLR, 2019.\n\n- John C Platt and Alan H Barr. Constrained differential optimization. In Proceedings of the 1987 International Conference on Neural Information Processing Systems, pp. 612-621, 1987.\n\n- Borja Rodr\u00edguez-G\u00e1lvez, Ragnar Thobaben, and Mikael Skoglund. A variational approach to privacy and fairness. In 2nd Privacy Preserving Artificial Intelligence Workshop (PPAI) of the AAAI Conference on Artificial Intelligence, 2021. URL https://arxiv.org/abs/2006.06332.\n\n- Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 3-18. IEEE, 2017.\n\n- Cuong Tran, Ferdinando Fioretto, and Pascal Van Hentenryck. Differentially private and fair deep learning: A Lagrangian dual approach. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 9932-9939, 2021.\n\n- Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and Yi Zhou. A hybrid approach to privacy-preserving federated learning. In Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security, pp. 1-11, 2019.\n\n- Sahil Verma and Julia Rubin. Fairness definitions explained. In 2018 IEEE/ACM International Workshop on Software Fairness (FairWare), pp. 1-7. IEEE, 2018.\n\n- Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, PEARU Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cirmrnan, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2.\n\n- Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled R\u00e9nyi differential privacy and analytical moments accountant. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1226-1235. PMLR, 2019.\n\n- Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th international conference on world wide web, pp. 1171-1180, 2017.\n\nResponse: Given the content and focus of the paper, here are the key points to consider for prediction:\n\n### Originality\n- **Original Contribution**: The paper presents a novel approach to enforcing group fairness in private federated learning by extending the modified method of differential multipliers (MMDM) to the federated setting.\n- **Methodological Innovation**: By addressing the limitations of existing work in the area, the authors provide a more comprehensive and flexible framework for enforcing fairness in federated learning.\n- **Empirical Validation**: The authors conduct thorough experiments on real-world datasets (Adult and modified FEMNIST) to validate their approach, demonstrating the effectiveness of FPFL in maintaining fairness across under-represented groups even in the presence of privacy mechanisms (DP).\n\n### Methodological Rigor\n- **Mathematical Framework**: The authors provide a detailed derivation of the algorithm, including the adaptation of MMDM to the federated setting and the handling of fairness constraints. The theoretical foundations are sound.\n- **Experiments**: The experiments are well-designed, involving both central and federated settings to validate the approach. They comprehensively analyze the performance and fairness metrics, providing insights into the trade-offs between privacy, fairness, and accuracy.\n\n### Significance of Contribution\n- **Impact on Field**: The paper addresses a critical issue in federated learning: the disproportionate impact of privacy mechanisms on under-represented groups. It proposes a practical solution that could significantly advance the state-of-the-art in federated learning.\n- **Practical Implications**: The findings are likely to influence the design of future federated learning systems, particularly those aiming for fairness and privacy.\n\n### Clarity of Presentation\n- **Clear Structure**: The paper is well-structured, clearly delineating the problem statement, methodology, experimental setup, and results.\n- **Detailed Explanations**: The authors provide extensive explanations for the algorithms and the experimental setup, making the technical details accessible to readers familiar with the relevant fields.\n\n### Alignment with Journal Scope\n- **Relevance to ICLR**: ICLR focuses on advancements in machine learning, especially in areas like representation learning, deep learning, and theoretical foundations. The paper aligns well with these areas.\n- **Specific Fit**: The paper\u2019s focus on private federated learning, which is a cutting-edge topic in the intersection of federated learning and differential privacy, fits well within ICLR\u2019s scope.\n\n### Prediction\n**Prediction**: \\(\\boxed{Accept}\\)\n\n### Rationale\n- **Strengths**:\n  - **Original and Innovative Approach**: The proposed method is novel and fills a significant gap in the current literature.\n  - **Robust Empirical Validation**: The experiments demonstrate the effectiveness of the approach in real-world scenarios.\n  - **Clear and Detailed Explanations**: The paper is well-written and easy to follow, providing a solid foundation for understanding the methodology.\n  - **Practical Impact**: The proposed solution has practical implications for improving fairness in federated learning systems.\n  \n- **Weaknesses**:\n  - **Technical Complexity**: While the paper is technically sound, it may be challenging for non-expert readers to fully grasp the intricacies of the algorithms and proofs.\n  - **Scalability**: The paper does not explicitly discuss scalability issues, which might be a concern for future work.\n  - **Limited Variance in Experiment Design**: Although the experiments are comprehensive, they are conducted on a single dataset and do not explore the variability across different types of models or fairness metrics.\n\nOverall, the paper is strong in terms of originality, methodological rigor, and practical significance. The experimental validation is thorough and the technical details are well-explained. Given these factors, the paper is likely to be accepted by ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636595.252004", "msecs": "252.0", "relativeCreated": "55945.14441490173", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Automated Discovery of Pairwise Interactions from Unstructured Data\n\n# Abstract\nPairwise interactions between perturbations to a system can provide evidence for the causal dependencies of the underlying underlying mechanisms of a system. When observations are low dimensional, hand crafted measurements, detecting interactions amounts to simple statistical tests, but it is not obvious how to detect interactions between perturbations affecting latent variables. We derive two interaction tests that are based on pairwise interventions, and show how these tests can be integrated into an active learning pipeline to efficiently discover pairwise interactions between perturbations. We illustrate the value of these tests in the context of biology, where pairwise perturbation experiments are frequently used to reveal interactions that are not observable from any single perturbation. Our tests can be run on unstructured data, such as the pixels in an image, which enables a more general notion of interaction than typical cell viability experiments, and can be run on cheaper experimental assays. We validate on several synthetic and real biological experiments that our tests are able to identify interacting pairs effectively. We evaluate our approach on a real biological experiment where we knocked out 50 pairs of genes and measured the effect with microscopy images. We show that we are able to recover significantly more known biological interactions than random search and standard active learning baselines.\n\n\n# 1 INTRODUCTION\n\n\nAcross the sciences, measurement of pairwise interaction between perturbations often reveals the existence of underlying mechanisms that single perturbations cannot. For example, quantum entanglement experiments reject classical laws by showing that entangled particles' spins are perfectly anti-correlated once we make a measurement, when classical laws predict independence. In economics, people may be more willing to pay for goods when presented in a bundle than they are to pay for each good in isolation, which reveals complements in the underlying consumer preferences. And in biology\u2014the application area on which we focus\u2014the concept of synthetic lethality (Nijman, 2011) occurs when the simultaneous perturbation of two genes results in cell death while individual perturbations do not, thereby revealing complementary roles in the underlying cellular mechanisms.\n\n\nTo demonstrate a pairwise interaction a scientist has to carefully design three steps of the experiment:\n\n- 1. Measurement: the expert to selects properties of the system to measure to reveal an interaction. E.g., measuring cell viability may reveal synthetic lethality, while the cell's colour will not.\n\n- 2. Hypothesis testing: an interaction is a deviation from what we expect under a null hypothesis that assumes independent effects. The expert needs to predict the outcome under independence and compare this prediction to the actual outcome.\n\n- 3. Selection: there are typically many variables that one could perturb, but only a small fraction will exhibit surprising interactions. The expert selects pairs of perturbations from all possible pairs.\n\n\nMeasuring interactions is further complicated by the fact that measurement and hypothesis testing are interdependent. We can see this in synthetic lethality example, where if we choose to measure the fraction of cells that survived the respective perturbations, then in order to test independence we would need to test whether  $P(\\text{surviv}_a \\cap \\text{surviv}_b) = P(\\text{surviv}_a)P(\\text{surviv}_b)$ . If we had instead measured the fraction of cells that died, we would have needed to test whether  $P(\\text{die}_a \\cup \\text{die}_b) = P(\\text{die}_a) + P(\\text{die}_b) - P(\\text{die}_a)P(\\text{die}_b)$ . The state of the cells in the respective petri dishes is the same in both cases, but correctly testing (in)dependence depends on how the expert chose to measure\n\n\n![Figure 1: The traditional experimental design relies on human experts to choose features of the experimental outcome (e.g., did a cell survive a perturbation?), predict behavior under the null hypothesis to test for interactions, and select interacting pairs from a combinatorial space of perturbations. Our approach automates the selection of perturbants and interaction testing directly from raw signal data (e.g., microscope images of cells).](images/ac130eecd61700bbbe30205cd1b08dc3604b4b404896bdb992668c821df90e45.jpg)\n\n\n![](images/dde1852a2bfe954570085b0712b179f2c00b8c57690072bc2a579b1f5da7f07f.jpg)\n\n\nthat state. These three steps require significant expertise and knowledge to choose and conduct the measurements, which makes scientific discovery hard to automate and scale.\n\n\nModern high throughput screening platforms (Dove, 2007; Baillargeon et al., 2019; Blay et al., 2020; Bock et al., 2022; Fabio et al., 2023; Morris et al., 2023; Messner et al., 2023) enables running large scale perturbation experiments that collect information-rich unstructured measurements. For example, cell painting assays (Bra, 2016; Fay et al., 2023; Chandrasekaran et al., 2023) provide microscopy images of cells, capturing the same information a human expert would measure, but without pre-selecting a specific property like cell viability. This lets us measure all the potential properties of interest at once\u2014albeit as unstructured signal data, such as raw pixel images or sensor measurements, rather than preprocessed properties\u2014but it is not clear how to use these measurements for interaction testing and selection in order to efficiently discover pairwise interactions. This suggests the primary question that this paper seeks to address: if we dispense with prespecified structured measurements, can we automate the discovery of pairwise interactions by designing testing and selection procedures that efficiently detect interactions from unstructured data?\n\n\nTo test for interactions, we show that pairwise perturbations are separable if they provide no additional information beyond what is already known from the single perturbations. This separability property can be tested by comparing density ratios of the single and double perturbations to the control distribution. We also develop a test for the case where pairs of perturbations affect disjoint subsets of the outcome space. For example, if two perturbations affect different organelles in the cell, each of which affect disjoint pixels in an image. This strong form of non-interaction allows us to compose summary statistics from single perturbations to predict the corresponding statistics from double perturbations. In particular, this lets us compose embeddings of single perturbations to predict the embeddings of double perturbations, similar to word-vector analogies Mikolov et al. (2013).\n\n\nWith this notion of an interaction, we address the second task of selection via efficient experimental design. We can search the space of pairwise experiments by selecting pairs of perturbations that are likely to result in large test statistics. In doing so, we reduce the problem of finding interacting pairs of perturbations into an active matrix completion problem. By defining this notion of interaction, we avoid the need to characterize uncertainty over the pixel-level outcomes (or some embedding thereof). Instead we directly model the posterior over an unknown reward matrix defined by the test statistics and at every round select actions striking a balance between exploration and exploitation via information directed sampling (IDS; Russo & Van Roy, 2016; Xu et al., 2022).\n\n\nWe evaluated our approach on both synthetic and real biological experiments. On the synthetic tasks, we found that our tests were able to detect both forms of dependence, validating our theory. On a benchmark consisting of all pairs of gene knockouts for 50 genes in HUVEC cells, we found that our approach using IDS discovers pairs of genes with higher interaction scores significantly faster than baselines, resulting in  $15\\%$  more known biological interactions being discovered and potentially far more novel interactions. The interactions we detected were also complementary to those which would have been discovered using existing cosine similarity-based approaches, and as a result, the two approaches can be combined to get a more detailed estimate of the relationships between genes from\n\n\nperturbation experiments. In summary, we formulate two notions of pairwise interaction\u2014violations of separability and disjointedness\u2014and develop a system for discovering interactions through the following contributions:\n\n- - We show that two perturbations are separable if double perturbations provide no additional information than that which was revealed in single perturbations.\n\n- - Further, we show that two perturbations are disjoint if and only if their densities compose additively. This result also implies sufficient conditions for embeddings of perturbations to be composed additively to predict pairwise perturbations.\n\n- - We demonstrate that the test scores for detecting interactions can be used to efficiently search for pairs of perturbations that interact using active matrix completion algorithms.\n\n\n# 2 RELATED WORK\n\n\nCausal representation learning Our approach to the problem of detecting interactions builds on the modelling assumptions developed in nonlinear independent component analysis (Hyvarinen & Pajunen, 1999; Hyvarinen & Morioka, 2016; Hyvarinen et al., 2019) and causal representation learning (Sch\u00f6lkopf et al., 2021), where we assume we observe some nonlinear mixing function,  $g(\\cdot)$ , of the underlying latent variables. The causal representation learning literature typically focuses on disentangling latent variables, whereas we look for testible implications without disentanglement. Our data generating process assumptions are most similar to the interventional setting (Ahuja et al., 2023; Squires et al., 2023; Buchholz et al., 2023; Varici et al., 2024). If we could successfully disentangle latent variables, then our task would be straightforward, but disentanglement is challenging in practice because it is not possible to validate whether an algorithm has succeeded in disentangling latent variables without access to ground truth. Like us, Jiang & Aragam (2023) also attempt to learn latent dependence properties (they characterize the whole latent DAG, not just marginal dependence) without disentanglement, but they assume a fixed bipartite graph of dependence between latents and observations, which does not apply to the pixel-level observations that we study. Zhang et al. (2023) develop causal representation learning techniques to disentangle latent variables and characterize conditions for extrapolation. Like us, they focus on biological applications, but they rely on stronger polynomial assumptions to achieve disentanglement. Their conditions for extrapolation are complementary to our separability tests: they argue extrapolation is possible when interventions affect non-overlapping latents, while we aim to discover when interventions affect overlapping latents. Finally, our separability test is closely related to, and inspired by Wang et al. (2023), but whereas they assume separability of concepts to manipulate generative models, we aim to test for an analogous notion of independence in experimental data.\n\n\nRepresentation learning for gene knockouts Our experiments build on a number of recent works showing the effectiveness of embedding cells presented into a representation space Sypetkowski et al. (2023); Kraus et al. (2023); Xun et al. (2023). These works show that you can infer that genes code for proteins that form part of complexes by finding embeddings that are highly cosine similar. This works because knocking out proteins on the same pathway will induce the same morphological effect, but this approach is limited to effects that are revealed by single perturbations. There have also been a number of papers that have attempted to learn disentangled representations of cells (e.g. Lotfollahi et al., 2023; Bereket & Karaletsos, 2023; Lopez et al., 2023), mostly from gene expression data.\n\n\nDesign of Gene Knockout Experiments There is growing literature on developing algorithms for the design of gene knockout experiments. Typically the problem is studied in the context of discovering single gene-knockouts (Mehrjou et al., 2021) which result in a particular phenotypic effect of interest. A variety of methods including bandits (Pacchiano et al., 2023) and traditional experimental design (Lyle et al., 2023) approaches have been studied in this context. Further, approaches which aim to learn predictors for the effects of unseen gene knockouts typically operate on RNA-seq data (Huang et al., 2023). RNA-Seq data is far more structured than the image data we consider: the former is a high-dimensional vector of count data where each element corresponds to a particular gene's expression, while the latter is just an unstructured collection of pixel intensities.\n\n\nExperimental design. The task of efficiently designing experiments to gain the most information about a system can be formalized as sequential Bayesian optimal experimental design (BOED; Ryan et al., 2016; Foster, 2021; Rainforth et al., 2023), where the goal to design experiments  $x \\in \\mathcal{X}$  with outcomes  $y \\in \\mathcal{Y}$  governed by a generative process  $y \\sim p(y \\mid \\gamma, x)$  with parameters  $\\gamma$ . The experiments  $(x_1, \\ldots, x_T)$  are performed sequentially, with the objective of maximizing the information gain (Lindley, 1956; Sebastiani & Wynn, 2000). However, applying this framework in\n\n\nhigh-dimensional settings is challenging because estimating information gain becomes difficult. We sidestep this complexity by focusing on the specific task of detecting interactions.\n\n\nStatistical and biological interactions. We further provide a discussion about prior works about classical statistical interactions in regression models and biological interactions in Appendix A.\n\n\n# 3 TESTS FOR PAIRWISE INTERACTIONS\n\n\nIn this section we develop tests for both separable and disjoint interventions. The separability test, Section 3.1, allows us to test whether two perturbations act on disjoint sets of latent variables. Disjointedness, Section 3.2, implies compositional generalization of summary statistics, allowing us to reduce the search space by predicting the outcome of experiments without explicitly running them.\n\n\nSetup We have observations  $X$  in an observation space  $\\mathcal{X}$  of unstructured measurements such as pixels in an image, and a finite set of perturbations  $\\{T_i \\in \\mathcal{T}_i : i \\in [n]\\}$ . Although our theory accommodates generic perturbations, we restrict the discussion to binary perturbations for convenience, i.e.,  $\\mathcal{T}_i := \\{0, 1\\}$  for all  $i \\in [n]$ , and  $T_i = 1$  means perturbation  $i$  is applied. For all  $i, j \\in [n]$ , denote the perturbation indicator as follows:\n\n\n$$\n\n$$\n\\delta_ {0} = \\left\\{T _ {[ n ]} = 0 \\right\\} \\quad \\delta_ {i} = \\left\\{T _ {i} = 1, T _ {[ n ] \\backslash \\{i \\}} = 0 \\right\\} \\quad \\delta_ {i j} = \\left\\{T _ {i} = T _ {j} = 1, T _ {[ n ] \\backslash \\{i, j \\}} = 0 \\right\\}.\n$$\n\n$$\n\n\nIn this section, we assume that for a pair of perturbations  $T_{i}, T_{j}$ , we have access to experimental data from four distributions:  $p(x|\\delta_0)$ ,  $p(x|\\delta_i)$ ,  $p(x|\\delta_j)$ , and  $p(x|\\delta_{ij})$ ; when we discuss the active learning procedures in Section 4, we will assume that we have access to all single perturbations distributions,  $p(x|\\delta_i)$ , and we will adaptively select the pairs,  $i, j$ , on which to collect samples from  $p(x|\\delta_i,\\delta_j)$ . Finally, we assume the existence of a set of latent variables  $\\{Z_1,\\ldots,Z_L\\} \\subseteq \\mathcal{Z}$  in some latent space,  $\\mathcal{Z}$ , that capture all relevant information about the perturbation. We state this precisely as,\n\n\nAssumption 3.1.  $X \\perpeq (T_1, \\ldots, T_n) | Z$ , or equivalently,\n\n\n$$\n\n$$\np (x | T _ {1}, \\dots , T _ {n}) = \\int_ {\\mathcal {Z}} p (x | z) p (z | T _ {1}, \\dots , T _ {n}) \\mathrm {d} z.\n$$\n\n$$\n\n\nThroughout, we assume all distributions have well-defined densities or probability mass functions with respect to some  $\\sigma$ -finite base measure on their corresponding sample spaces. We use the same symbol to denote a distribution and its density. All proofs are deferred to Appendix B.\n\n\n# 3.1 SEPARABILITY TESTING\n\n\nIt would be trivial to verify whether two perturbations intervene on different latents if the underlying latent variables were observed directly. However, our observations are unstructured signal data, which we assume is generated by some deterministic mixing function.\n\n\nAssumption 3.2. There exists a diffeomorphism  $g: \\mathcal{Z} \\to \\mathcal{X}$  such that  $X = g(Z)$ .\n\n\nRemark 3.3. This diffeomorphism assumption between  $\\mathcal{Z}$  and  $\\mathcal{X}$  can be relaxed; our theory and methodology remain valid as long as the change of variable formula holds for the distributions of  $\\mathcal{Z}$  and  $X$ . For example, the distribution of  $X$  may have support on a low-dimensional manifold of  $\\mathcal{X}$ , with the latent space  $\\mathcal{Z}$  having a lower dimension than  $\\mathcal{X}$ . See Krantz & Parks (2008, Lemma 5.1.4) for the generalized change of variable formula for non-bijective transformations.\n\n\nThe key observation that we leverage is that when latent variables are independent, the change of variable formula implies that the density ratio of a perturbed distribution to the original (control) distribution has a simple form that only involves the distribution of the intervened latent variable,\n\n\n$$\n\n$$\n\\frac {p (x | \\delta_ {i})}{p (x | \\delta_ {0})} = \\frac {p _ {Z} (g ^ {- 1} (x) | \\delta_ {i}) \\left| \\det (J (g ^ {- 1} (x))) \\right|}{p _ {Z} (g ^ {- 1} (x) | \\delta_ {0}) \\left| \\det (J (g ^ {- 1} (x))) \\right|} = \\frac {p _ {Z _ {i}} ^ {\\dagger} ([ g ^ {- 1} (x) ] _ {i})}{p _ {Z _ {i}} ([ g ^ {- 1} (x) ] _ {i})}.\n$$\n\n$$\n\n\nHere  $p_{Z_i}^\\dagger$  denotes the perturbed distribution of the latent variable  $Z_i$  targeted by intervention  $i$ ,  $[ \\cdot ]_i$  denotes the projection operator to the subspace of  $Z_i$ . The testable conclusion that we can derive from this observation, is that when two variables are independent, we can predict the density ratio of the double perturbation,  $\\frac{p(x|\\delta_{ij})}{p(x|\\delta_0)}$ , as the product of the density ratios of the respective single perturbations,  $\\frac{p(x|\\delta_i)}{p(x|\\delta_0)}\\frac{p(x|\\delta_j)}{p(x|\\delta_0)}$ . To make this rigorous, we assume that there exists a causal factorization of the latent distribution, where each latent variable is conditionally independent of its non-descendants given its parents. We use  $\\mathrm{Pa}(Y)$  (resp.  $\\mathrm{Ch}(Y)$ ) to denote the parent (resp. children) of a random variable  $Y$ .\n\n\nAssumption 3.4. The latent distribution  $p_Z(z)$  can be factorized into  $L$  latent factors:\n\n\n$$\n\n$$\np _ {Z} (z | T _ {1}, \\dots , T _ {n}) = \\prod_ {l = 1} ^ {L} p _ {Z _ {l}} (z _ {l} | \\operatorname {P a} (z _ {l})),\n$$\n\n$$\n\n\nwhere  $\\forall l\\in [L],\\mathrm{Pa}(Z_l)\\subseteq T_{[n]}\\cup Z_{[L]}$ , and  $\\forall i\\in [n],\\mathrm{Ch}(T_i)\\subseteq Z_{[L]}$ . The random variable  $Z$  here should be interpreted as the concatenation of all latent factors  $\\{Z_1,\\ldots ,Z_L\\}$ .\n\n\nThe observed effects of two non-interacting perturbations are attributed to distinct causal pathways, rather than being confounded by a shared latent factor.\n\n\nDefinition 3.5. Two perturbations  $\\delta_i, \\delta_j$  are separable if  $\\mathrm{Ch}(T_i) \\cap \\mathrm{Ch}(T_j) = \\emptyset$ .\n\n\nWhen two perturbations act separately, the resulting density ratios of the observation distribution will have the predictable interactions that we derived above.\n\n\nTheorem 3.6. Suppose that Assumptions 3.4 and 3.2 hold, and that  $\\delta_i, \\delta_j$  are separable. Then,\n\n\n$$\n\n$$\n\\frac {p (x \\mid \\delta_ {i , j})}{p (x \\mid \\delta_ {0})} = \\frac {p (x \\mid \\delta_ {i})}{p (x \\mid \\delta_ {0})} \\frac {p (x \\mid \\delta_ {j})}{p (x \\mid \\delta_ {0})}. \\tag {1}\n$$\n\n$$\n\n\nThis provides a testable implication of separability, but to test it, we need to derive a real-valued test statistics. By taking logs and rearranging terms, we can rewrite Eq. (1), as testing,\n\n\n$$\n\n$$\n\\log p \\left(x \\mid \\delta_ {i j}\\right) + \\log p \\left(x \\mid \\delta_ {0}\\right) - \\log p \\left(x \\mid \\delta_ {i}\\right) - \\log p \\left(x \\mid \\delta_ {j}\\right) \\stackrel {?} {=} 0 \\tag {2}\n$$\n\n$$\n\n\nIf we instead take expectations of Eq. (2) with respect to  $p(x|\\delta_0)$ , then this amounts to testing if\n\n\n$$\n\n$$\n\\mathrm {D} _ {\\mathrm {K L}} \\left(p _ {0} \\| p _ {i}\\right) + \\mathrm {D} _ {\\mathrm {K L}} \\left(p _ {0} \\| p _ {j}\\right) \\stackrel {?} {=} \\mathrm {D} _ {\\mathrm {K L}} \\left(p _ {0} \\| p _ {i, j}\\right), \\tag {3}\n$$\n\n$$\n\n\nwhere  $p_i = p(x|\\delta_i), p_0 = p(x|\\delta_0)$  and  $\\mathrm{D}_{\\mathrm{KL}}(\\cdot ||\\cdot)$  denotes the Kullback-Leibler (KL) divergence. Note that KL divergence measures the difference between two distributions; in our context, it quantifies the distribution shift resulting from interventions. Eq. (3) reflects the intuition that the influence of two separable perturbations, measured by KL divergence, is additive. We use the KL score,  $|\\mathrm{D}_{\\mathrm{KL}}(p||p_i) + \\mathrm{D}_{\\mathrm{KL}}(p||p_j) - \\mathrm{D}_{\\mathrm{KL}}(p||p_{i,j})|$ , to quantify the violation of the separability of  $\\delta_i, \\delta_j$ , where these KL divergences are estimated using samples.\n\n\nIn practice, we observe that the simple K-nearest neighbor-based (KNN) estimator of the KL divergence (Wang et al., 2009) suffices for low-dimensional problems. However, for high-dimensional data such as images, more dedicated estimation procedures are required (Belghazi et al., 2018; Song & Ermon, 2020; Ghimire et al., 2021). We detail our estimation procedure in Appendix C.\n\n\n# 3.2 DISJOINTEDNESS TESTING\n\n\nOur second testing procedure examines whether two perturbations operate on disjoint domains such that their effects are cumulative. For example, if the morphological changes from knocking out two non-interacting genes can be separated into distinct visual features, such that their measures sum, then they are disjoint. We can define disjointedness of two perturbations formally as,\n\n\nDefinition 3.7. Two perturbations  $\\delta_{i},\\delta_{j}$  are disjoint if\n\n\n$$\n\n$$\np (x \\mid \\delta_ {i j}) - p (x \\mid \\delta_ {0}) = \\left(p (x \\mid \\delta_ {i}) - p (x \\mid \\delta_ {0})\\right) + \\left(p (x \\mid \\delta_ {j}) - p (x \\mid \\delta_ {0})\\right). \\tag {4}\n$$\n\n$$\n\n\nDisjointedness is important, because it implies that we can predict pairwise summary statistics of the distributions from individual perturbations. To see this, let  $h(\\cdot)$  denote any feature map from the observation,  $X$ . If we have disjoint perturbations  $\\delta_{i},\\delta_{j}$ , then,\n\n\n$$\n\n$$\n\\begin{array}{l} \\mathbb {E} [ h (x) | \\delta_ {i, j} ] - \\mathbb {E} [ h (x) | \\delta_ {0} ] = \\int h (x) [ p (x | \\delta_ {i j}) - p (x | \\delta_ {0}) ] \\mathrm {d} x \\\\ = \\int h (x) \\left[ \\left(p (x | \\delta_ {i}) - p (x | \\delta_ {0})\\right) + \\left(p (x | \\delta_ {j}) - p (x | \\delta_ {0})\\right) \\right] d x \\\\ = \\mathbb {E} [ h (x) | \\delta_ {i} ] - \\mathbb {E} [ h (x) | \\delta_ {0} ] + \\mathbb {E} [ h (x) | \\delta_ {j} ] - \\mathbb {E} [ h (x) | \\delta_ {0} ]. \\tag {5} \\\\ \\end{array}\n$$\n\n$$\n\n\nThis implies that we can define average centered embedding vectors,  $\\vec{h}_i\\coloneqq \\mathbb{E}[h(x)|\\delta_i] - \\mathbb{E}[h(x)|\\delta_0]$  and  $\\vec{h}_j$ , and accurately predict  $\\vec{h}_{i,j} = \\vec{h}_i + \\vec{h}_j$  without running the experiments. Notably, growing\n\n\nevidence in the literature (Lotfollahi et al., 2019; Gaudelet et al., 2024) showing that these relationships often hold in real biological experiments (and our experiments support this; see Appendix E.2 for the choice of  $h$  we tested). Disjointedness explains the sufficient conditions for this to hold.\n\n\nWe can test whether Eq. (4) holds, by testing the following null hypothesis:\n\n\n$$\n\n$$\n\\mathrm {H} _ {0}: \\frac {1}{2} p (x | \\delta_ {i j}) + \\frac {1}{2} p (x | \\delta_ {0}) = \\frac {1}{2} p (x | \\delta_ {i}) + \\frac {1}{2} p (x | \\delta_ {j}).\n$$\n\n$$\n\n\nGiven interventional data from  $p(x|\\delta_0), p(x|\\delta_i), p(x|\\delta_j)$  and  $p(x|\\delta_{ij})$ , we can frame this as a standard two-sample test problem. We can test this using the maximal mean discrepancy (MMD) based two-sample test (Gretton et al., 2012), which compares two distributions based on their embeddings in some reproducing kernel Hilbert space (RKHS). The estimated MMD serves as a measure of the extent to which the perturbations violate the disjointedness. Interestingly, MMD test amounts to test Eq. (5) on a \"most discriminative\" feature map  $h$  in the RKHS. We provide a self-contained introduction about MMD and kernel mean embedding of distributions in Appendix E. Disjointness is a constraint on the densities of the observed pixels, but it is implied by assuming an appropriate mixture model on the latent variables. We discuss this in Appendix D.\n\n\n# 4 SELECTING PERTURBATION PAIRS TO EFFICIENTLY DISCOVER INTERACTIONS\n\n\nWith frameworks for testing separability and disjointedness in place, the next question is how to select which experiments to run. The space of possible perturbation pairs is too large to test all combinations, as experiments are costly. For instance, pairwise knockouts of 20,000 genes would require about 200 million experiments (not including replicates). In this section, we explore how experimental design and bandit algorithms can efficiently select perturbation pairs likely to interact.\n\n\nSelection of perturbation pairs as active matrix completion. The testing frameworks discussed in Section 3 prescribe test statistics which can be used to detect pairwise interactions. The test statistics, however, require samples from  $p(x|\\delta_{i,j})$ , which entails running pairwise perturbation experiments. We are thus interested in developing an approach for selecting pairs of perturbations which are likely to have high values for the test statistics and are thus likely to reveal pairwise interactions.\n\n\nThe test statistics for pairs of perturbations can be viewed as an (unknown) symmetric matrix  $\\mathbf{R} \\in \\mathbb{R}^{n \\times n}$ , where each entry  $\\mathbf{R}_{i,j}$  contains the value of the test statistic that we will observe if we run perturbation  $\\delta_{ij}$ .  $\\mathbf{R}$  is unknown a priori but we are allowed to select entries to observe. This can be viewed as an active matrix completion problem (Chakraborty et al., 2013). Active matrix completion is a variant of the standard matrix completion problem (Laurent, 2009) where values of entries of the matrix can be sequentially queried. Framing the problem of selecting perturbation pairs as an active matrix completion allows us to leverage existing efficient algorithms.\n\n\nAdaptive sampling for discovery. In particular, we use the framework of adaptive sampling for discovery (ASD; Xu et al., 2022) which provides a bandit-based approach for active matrix completion. ASD involves using information directed sampling (IDS; Russo & Van Roy, 2016) in a \"discovery\" setting, where an action is only selected once. This is an instantiation of the general sleeping experts setting (Kanade et al., 2009), where the set of available actions shrinks every round.\n\n\nDenote  $\\Delta$  the action space of possible experiment designs, which in this case is the set of perturbations  $\\Delta := \\{\\delta_{ij} : i,j \\in n, i > j\\}$ , where  $n$  is the total number of distinct perturbations. We denote the perturbation pair  $i,j$  selected at step  $k$  as  $a^{(k)} := (i^{(k)}, j^{(k)}) \\in \\Delta$ .  $\\mathcal{D}(\\Delta)$  represents the set of possible (categorical) distributions over  $\\Delta$ . Each time an action  $a^{(k)}$  is selected, the corresponding element of the (unknown) reward matrix  $\\mathbf{R}$  is revealed. In our setting, this reward corresponds to the test\n\n\nstatistic from either Section 3.1 or 3.2 for each perturbation pair. Let  $H_{t} = ((a^{(k)},\\mathbf{R}_{i^{(k)}},j^{(k)}))_{k = 1}^{t - 1}$  denote the history of actions and their corresponding rewards until round  $t$  , and  $\\Delta_t$  the set of remaining actions; i.e. the perturbation pairs that we have not yet tested experimentally. A policy  $\\pi$  is a map from  $H_{t}$  to  $\\mathcal{D}(\\Delta_t)$  . The IDS policy,  $\\pi_{\\mathrm{IDS}}$  maintains a posterior distribution over  $\\mathbf{R}$  given the data observed up to round  $t$  , denoted  $p(\\mathbf{R}\\mid H_t)$\n\n\n# ALGORITHM 1 ASD FOR SELECTING PERTURBATION PAIRS\n\n- 1: Initialize  $H_{1} = \\{\\}$ , batch size  $b$\n\n- 2: for  $t = 1, \\dots, T$  do\n\n- 3: Estimate posterior  $p(\\mathbf{R} \\mid H_t)$\n\n- 4: Compute information ratio  $\\Psi_t$\n\n- 5: Select  $(a_{i})_{i = 1}^{b}$  greedily to maximize  $\\Psi_t$\n\n- 6: Perform experiments and obtain  $(\\mathbf{R}_{a_i})_{i = 1}^b$\n\n- 7: Update  $H_{t + 1} = H_t\\cup \\{(a_i,\\mathbf{R}_{a_i})\\}_{i = 1}^b$\n\n- 8: end for\n\n\nWe can describe the sub-optimality of any action with respect to a set of beliefs by comparing the action's reward to that of the best action that could have been selected at time  $t$ , under the agent's current posterior over the reward matrix. Intuitively, we can evaluate this by sampling a plausible reward matrix from our posterior,  $\\dot{\\mathbf{R}} \\sim (\\mathbf{R}|H_t)$ , and then comparing the reward from action,  $a$ , to the reward an agent would have received from selecting the optimal action,  $a^* = \\arg \\max_{a \\in \\Delta_t} \\dot{\\mathbf{R}}(a)$ ; where  $\\dot{\\mathbf{R}}(a^{(k)}) := \\dot{\\mathbf{R}}_{i^{(k)}, j^{(k)}}$  for  $a^{(k)} = (i^{(k)}, j^{(k)})$ . This is known as the expected instantaneous regret incurred by an action and is defined as,  $\\Delta_t(a) = \\mathbb{E}_{\\dot{\\mathbf{R}} \\sim p(\\mathbf{R}|H_t)}[\\dot{\\mathbf{R}}(a^*) - \\dot{\\mathbf{R}}(a)]$ . Additionally, we define the information gain about the top  $T - t + 1$  remaining actions as follows:\n\n\n$$\n\n$$\ng (a) = M I (a _ {t, 1} ^ {*}, \\dots , a _ {t, T - t + 1} ^ {*}; \\mathbf {R} (a) \\mid H _ {t}, \\delta_ {t} = a).\n$$\n\n$$\n\n\nAlgorithm 1 summarizes the algorithmic procedure. The algorithm operates over a series of  $T$  rounds. In each round, the first step is to estimate the posterior distribution over  $\\mathbf{R}$  given the data observed thus far. Next step involves selecting a batch of perturbations based on the information ratio. The IDS policy at round  $t$  can then be computed by minimizing the information ratio  $\\Psi$ :\n\n\n$$\n\n$$\n\\pi_ {\\mathrm {I D S}} \\in \\arg \\min _ {\\pi \\in \\mathcal {D} (\\Delta_ {t})} \\Psi_ {\\pi , t} := \\frac {(\\Delta_ {t} ^ {\\top} \\pi) ^ {\\lambda}}{g _ {t} ^ {\\top} \\pi}\n$$\n\n$$\n\n\nwhere  $\\lambda$  is a parameter controls the tradeoff between lower instant regret (exploitation) and higher information gain (exploration). Due to the intractability of  $g_{t}(a)$ , we follow Russo & Van Roy (2016); Xu et al. (2022) to approximate  $g_{t}$  with the conditional variance  $v_{t}(a) = \\operatorname{Var}_{t}(\\mathbb{E}[\\mathbf{R}_{a}|a_{1}^{*},a])$ .  $v_{t}(a)$  is a lower bound on the information gain  $g_{t}(a)$  and hence can be used as a surrogate when maximizing  $g_{t}(a)$ . After selecting the perturbation pair  $i,j$ , we compute the pairwise test statistic,  $\\mathbf{R}_{i,j}$  using data from the experimental outcomes. We add these test statistics to our reward matrix, update our posteriors and then continue on to the next round. Linear regret is unavoidable for any bandit algorithm with no structural assumptions on  $\\mathbf{R}$ . We thus assume  $\\mathbf{R}$  is low-rank with a Gaussian prior on the entries, allowing ASD to achieve sublinear regret (Xu et al., 2022). In our motivating biological example the low-rank assumption amounts to assume that there are morphological similarities in the functional response of cells to genetic perturbations (e.g. because knockouts of different genes on the same pathway product the same morphological effect). We validate this empirically in Figure 5 and discuss the assumption further in subsection F.3.\n\n\nBatching. In high-throughput experimental screens, it is possible to run multiple experiments in parallel at the cost of a single experiment. So instead of a single action  $\\delta_t$ , we can select a set of actions  $\\{\\delta_t^1, \\ldots, \\delta_t^b\\}$  where  $b$  is the number of experiments we can run in parallel. However, as there are no efficient algorithms for combinatorial bandits in the discovery setting, we resort to a simple greedy scheme to select batches with ASD. Specifically, instead of picking a single action which minimizes the information ratio, we pick  $b$  actions with the lowest information ratio.\n\n\n# 5 EXPERIMENTS\n\n\nOur experiments aim to address three objectives: (1) verifying that our theoretical claims about interactions are detectable on known synthetic tasks; (2) evaluating the test statistics' ability to recover known biological relationships from real pairwise perturbation experiments; and (3) assessing our active learning pipeline's efficiency in detecting interactions. All experimental details are provided in Appendix F.\n\n\n# 5.1 TESTING ON SYNTHETIC SETTING\n\n\nWe validated the separability test on two synthetic interventional examples: one with 3-dimensional tabular data and one with images  $(3 \\times 128 \\times 128)$ . The disjointedness test was validated on an interventional tabular example. In each case, we generated observations by sampling from a latent distribution  $p_Z$  following a DAG structure, followed by a mapping  $g(\\cdot)$  to the observations. We then estimated the test statistics for each pair of perturbations. Detailed descriptions of the data generation processes, DAG structures, and mappings are provided in Appendix F.1.\n\n\nFig. 2 displays the separability test results for both tabular and image data, showing that our estimated KL scores accurately capture separability: inseparable pairs have large KL scores, while separable pairs have small scores. Fig. 4 shows the disjointedness tests, with the MMD test accurately identifying failures of disjointedness and being relatively insensitive to the choice of kernel.\n\n\n# 5.2 TESTING ON BIOLOGICAL INTERACTIONS\n\n\n![](images/f2255ed83edc51e49d9546a49dfa41c4330c27ec95e899d57bb0042e725ce7d6.jpg)\n\n\n![](images/75840335229e598185017b63aaa48981f045dbda8e73964b03b37ae5e2f88f10.jpg)\n\n\n![](images/3930f49cdd194c2cdd28328aa1547ee1a664f196c112e2246f103b909e30fba7.jpg)\n\n\n![Figure 3: (Left) Pairwise separability scores between different CRISPR guides targeting the two genes TSC2 and MTOR. Missing pairs indicate that data for those combinations were not collected in our experiments. Guides targeting the same gene show high KL scores (red), while guides targeting different genes show low scores (blue). (Right) Random samples of the actual single-cell images used in these experiments. Even trained experts find it very difficult to detect interactions from the image.](images/d0eb3841298d695f0a1e1f2334467f2ae1958d42a2aa5fbd08e3252529d12f18.jpg)\n\n\n![Figure 2: Separability test on synthetic tabular data using the KNN-based KL estimator (left) and NRE-based KL estimator (middle), and on synthetic images (right). Brighter colors indicate stronger interactions. Ground truth interacting pairs, A-B and C-D, are correctly identified in both examples.](images/21a6a383f8d28dd034330add7a74df41bc56ba141eda02c684012dab34c1f5a9.jpg)\n\n\nTo evaluate if our separability test can recover known biological interactions, we ran the following test: In gene knockout experiments, different CRISPR guides can target the same gene, cutting it at different locations but resulting in the same gene knockout. Intuitively, guides targeting the same gene should show high separability scores, while guides targeting different genes should show lower scores, assuming the genes are on distinct pathways. While it's rare for different guides targeting the same gene to be run in a single well, our dataset includes examples for two genes (TSC2 and MTOR). Using single\n\n\n![Figure 4: Disjointedness testing on synthetic example using the MMD-based statistics with a Matern 2.5 kernel (left) and an RBF kernel (right); brighter colors suggest stronger interactions. The ground truth interacting pairs are D-E and F-G, which are correctly identified by the test.](images/cda06cfb339c53043c5cfa237d96024b7c226d3b67ed35a250db93c215faca21.jpg)\n\n\ncell painting images, we tested separability between pairs of guides. The results, shown in Fig. 3, align with expectations: strong interaction scores were observed between guides targeting the same gene (e.g., MTOR guide 3 with MTOR guides 1 and 2), and much weaker scores between MTOR and TSC2 guides. Both MTOR and TSC2 impact multiple cell systems, so some interaction is expected, but the significantly lower scores between genes compared to within the same gene are encouraging.\n\n\nWe then evaluated our testing approaches on a collection of 50 genes with the goal of detecting gene interactions. The dataset was collected by performing CRISPR knockouts on all pairs from a set of 50 selected genes, resulting in 1,225 gene pairs. The targeted genes have a bias towards known gene-gene interactions. We performed in vitro double gene knockout experiments on HUVEC cells using three\n\n\nCRISPR guides per gene. We label perturbations according to the targeted gene, aggregating the effects of the individual CRISPR guides. The experimental protocol and data preprocessing followed the procedures described by Fay et al. (2023) and Sypetkowski et al. (2023), respectively.\n\n\nWe computed the test statistics using 1024-dimensional embeddings of the original cell painting images extracted from a pre-trained masked autoencoder (He et al., 2022; Kraus et al., 2023). Fig. 5 shows the matrices of the test statistics for all perturbation pairs; each entry represents the pairwise interaction scores. Qualitatively, both the disjointedness and separability statistics effectively uncover plausible biological relationships. Many genes in the apoptosis pathway (programmed cell death, e.g., gene 2-3: BAX, BCL2L1) and the proteasome (protein degradation e.g., gene 28-30: PSMA1, PSMB2, PSMD1) show high scores, expected from synthetic lethal (SL) relationships. SL is a complex phenomenon in which simultaneous inactivation of specific gene combinations leads to cell death or extreme sickness, while individual perturbations have little effect. Apoptosis, controlling cell survival, is tightly regulated; disruption of the anti-apoptotic BCL-2 family (BCL2, BCL2L1, MCL1) (Kale et al., 2018) interferes with critical barriers against cell death. Proteasome function is similarly regulated, as cells must maintain a delicate balance of different proteins (Rousseau & Bertolotti, 2018). Apoptosis and proteasome members are commonly found in SL screens in cancer cells due to mutations that compromise cellular buffering capabilities (Li et al., 2020; Ge et al., 2024; Han et al., 2017; Cron et al., 2013; Steckel et al., 2012; Das et al., 2020).\n\n\nAs shown in Fig. 5, the MMD-based statistics reveal strong interaction signals with proteasome components, which regulate global protein levels and interact with multiple essential pathways. While the separability score shows less distinct patterns, it highlights several gene pairs known or expected to interact physically or genetically, such as BCL2L1-MCL1 (genes 3-8) (Shang et al., 2020; Carter et al., 2023), BAX-BCL2L1 (genes 2-3) (Lindqvist & Vaux, 2014), BCL2L1-PSMD1 (genes 3-30) (Craxton et al., 2012), and PSMB2-PSMD1 (genes 29-30) (Voutsadakis, 2017).\n\n\n![Figure 5: Pairwise interaction scores from a selected set of 50 genes using disjointedness test with Matern 2.5 kernel and RBF kernel (left and middle, respectively), and separability test (right); brighter colors suggest stronger interactions. Genes from the same pathway are ordered adjacently. The associated pathways of each selected gene are described in Table 1 of Appendix F.2.1.](images/60e0de2a44af10f5b097a33b253670cc97659d339dcb28ddbf9cb35a5503fde1.jpg)\n\n\n![](images/d4e9ff885dc50529cfa01fc4c960055fb434c0d4fbca9809f112bf94cdd72e42.jpg)\n\n\n![](images/8ed25cb7ecca090dcb1598782c3aeca7ff425879c68a13bd27fa27835ae38c7e.jpg)\n\n\n# 5.3 INTERACTION DISCOVERY WITH ACTIVE MATRIX COMPLETION\n\n\nFinally, we showcase how to use our test statistics to adaptively select the pairwise experiment to run. As introduced in Section 4, the adaptive experimental design is framed as an active matrix completion problem (Algorithm 1). We applied Algorithm 1 on the gene-gene interaction detection example using the MMD-based scores.\n\n\nBaselines. To evaluate the effectiveness of our automated selection method, we compare against selection with random policy, upper confidence bound (UCB), Thompson sampling (TS) and uncertainty sampling (US). To instantiate UCB in the discovery setting, we use the uncertainty from the low-rank matrix posterior in place of the counts used in the standard multi-armed bandit setting and mask the actions once they are selected. Similarly, in US we pick pairs based solely on the uncertainty of the posterior. TS is instantiated in the same way as IDS, but the pairs are selected to minimize only the instant regret. Further details are discussed in Appendix F.\n\n\nEvaluation metrics. We evaluate each approach using three different metrics given a budget of 50 rounds, each with a batch size of 10 resulting in a total of 500 experiments covering  $50\\%$  of all\n\n\n![Figure 6: Empirical results on the active pairwise experiments selection for the gene-gene interaction detection example using MMD based test statistics. The solid lines represent the mean performance, whereas the shaded region represent all the runs (min-max). IDS (ours) outperforms all the baselines significantly in terms of top scoring pairs discovered as well as the regret. In terms of known interactions, IDS still outperforms the baselines by a small margin.](images/6f6e3c5f03ea4efcb374618feb12a5472e9f0d380d766b3cd86a396d87a9b308.jpg)\n\n\n![](images/2d24c9813f57b4cd0b62cfb15162cd518eafb3387bc5d71539ab8453f5da5da3.jpg)\n\n\n![](images/9912ee80f6bdfa2d9cb945e14482df27490dc5e346471e05c72b5e4a961511b9.jpg)\n\n\npossible pairs of the gene set. First, we access the fraction of the pairs with the top 5 percentile of scores recovered by each algorithm, measuring how well they explore high-scoring regions. Next, we evaluate the regret of each algorithm with respect to an optimal policy that always selects the highest-scoring pairs. Finally, we evaluate the number of known biological relations that appear in CORUM (Giurgiu et al., 2019), StringDB (Szklarczyk et al., 2021), Signor (Licata et al., 2020) and hu.MAP (Drew et al., 2021) to see how many each method is able to recover.\n\n\nResults. Fig. 6 (and Fig. 8 in Appendix F.3) show the empirical results. IDS successfully discovers all pairs in the top 5 percentile, while the baselines recover only about half. In terms of regret, IDS outperforms the baselines, with random performing the worst, demonstrating IDS's ability to exploit the low-rank structure of the reward matrix. Regarding known interactions discovered, IDS and TS slightly outperform other methods. After 50 rounds, IDS and TS show a  $12 - 15\\%$  improvement over the baselines in identifying known biological interactions. The significant difference between the performance on the fraction of top pairs and the number of known relations raises questions about the correlation between the prediction errors and the biological interactions.\n\n\n# 6 DISCUSSION\n\n\nThis paper presented a method for efficiently detecting interactions. We present interactivity scores for both separability and disjointedness which both allow us to measure interactions between perturbations, and run experiments only on pairs of perturbations for which we will fail to compositionally generalize. From an active learning perspective, disjointedness is powerful: when we can learn a good posterior over this test statistic, we effectively have a \"confidence score\" for whether two perturbations are likely to compose additively. This allows us to dramatically reduce the number of experiments we run by experimenting only where embeddings are unpredictable. Separability is a more intuitive notion of (in-)dependence in that we directly test for whether perturbations interact in latent space; we aim to explore whether this can be used to recover intervention targets.\n\n\nLimitations While many known interactions scored highly according to our test statistics, the overall correlation between known interactions and this metric was relatively low. It is not clear whether this is the result of a lack of specificity, or whether this is because we are in fact discovering real relationships that are not known to biology. To test this would require additional experimentation with orthogonal assays. Additionally, the results for the separability tests depended on the quality of the KL estimator. We used the SMILE estimator which is somewhat sensitive to the choice of the clipping parameter,  $\\tau$ . In our experiments, we simply used a default of  $\\tau = 5$ , but we need a robust method for choosing this hyperparameter to ensure this test is agnostic to modalities. Finally, the separability score test lacks a rigorous rejection criterion, and a characterization of type I and type II error. While this is less critical when the goal is to use the test statistic for active learning (introduced later in Section 4), a formal hypothesis testing framework is important from a statistical perspective. This issue has been addressed in a follow-up work.\n\n\n# REFERENCES\n\n- Cell painting, a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes. Nature Protocols, 11(9):1757-1774, 2016.\n\n- Constantin Ahlmann-Eltze, Wolfgang Huber, and Simon Anders. Deep learning-based predictions of gene perturbation effects do not yet outperform simple linear methods. *BioRxiv*, pp. 2024\u201309, 2024.\n\n- Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation learning. In International conference on machine learning, pp. 372-407. PMLR, 2023.\n\n- Yevhen Akimov and Tero Aittokallio. Re-defining synthetic lethality by phenotypic profiling for precision oncology. Cell chemical biology, 28(3):246-256, 2021.\n\n- Pierre Baillargeon, Virneliz Fernandez-Vega, Banu Priya Sridharan, Steven Brown, Patrick R Griffin, Hugh Rosen, Benjamin Cravatt, Louis Scampavia, and Timothy P Spicer. The scripps molecular screening center and translational research institute. SLAS DISCOVERY: Advancing Life Sciences R&D, 24(3):386-397, 2019.\n\n- Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International conference on machine learning, 2018.\n\n- Michael Bereket and Theofanis Karaletsos. Modelling cellular perturbations with the sparse additive mechanism shift variational autoencoder. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=DzaCE00jGV.\n\n- Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul A. Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep universal probabilistic programming. J. Mach. Learn. Res., 20:28:1-28:6, 2019. URL http://jmlr.org/papers/v20/18-403.html.\n\n- Vincent Blay, Bhairavi Tolani, Sunita P Ho, and Michelle R Arkin. High-throughput screening: today's biochemical and cell-based approaches. *Drug Discovery Today*, 25(10):1807-1821, 2020.\n\n- Christoph Bock, Paul Datlinger, Florence Chardon, Matthew A Coelho, Matthew B Dong, Keith A Lawson, Tian Lu, Laetitia Maroc, Thomas M Norman, Bicna Song, et al. High-content crisp screening. Nature Reviews Methods Primers, 2(1):1-23, 2022.\n\n- James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n\n- Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Scholkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing, 2023.\n\n- Bing Z. Carter, Po Yee Mak, Wenjing Tao, Edward Ayoub, Lauren B. Ostermann, Xuelin Huang, Sanam Loghavi, Steffen Boettcher, Yuki Nishida, Vivian Ruvolo, Paul E. Hughes, Phuong K. Morrow, Torsten Haferlach, Steven Kornblau, Muharrem Muftuoglu, and Michael Andreeff. Combined inhibition of bcl-2 and mcl-1 overcomes bax deficiency-mediated resistance of tp53-mutant acute myeloid leukemia to individual bh3 mimetics. Blood Cancer Journal, 13(1):57, 2023. doi: 10.1038/s41408-023-00830-w. URL https://doi.org/10.1038/s41408-023-00830-w.\n\n- Shayok Chakraborty, Jiayu Zhou, Vineeth Balasubramanian, Sethuraman Panchanathan, Ian Davidson, and Jieping Ye. Active matrix completion. In 2013 IEEE 13th international conference on data mining, pp. 81-90. IEEE, 2013.\n\n- Srinivas Niranj Chandrasekaran, Jeanelle Ackerman, Eric Alix, D. Michael Ando, John Arevalo, Melissa Bennion, Nicolas Boisseau, Adriana Borowa, Justin D. Boyd, Laurent Brino, Patrick J. Byrne, Hugo Ceulemans, Carolyn Ch'ng, Beth A. Cimini, Djork-Arne Clevert, Nicole Deflaux, John G Doench, Thierry Dorval, Regis Doyonnas, Vincenza Dragone, Ola Engkvist, Patrick W. Falcon, Briana Fritchman, Florian Fuchs, Sakshi Garg, Tamara J. Gilbert, David Glazer, David Gnutt, Amy Goodale, Jeremy Grignard, Judith Guenther, Yu Han, Zahra Hanifehlou, Santosh Hariharan, Desiree Hernandez, Shane R Horman, Gisela Hormel, Michael Huntley, Ilknur Icke, Makiyo Iida, Christina B. Jacob, Steffen Jaensch, Jawahar Khetan, Maria Kost-Alimova, Tomasz Krawiec, Daniel Kuhn, Charles-Hugues Lardeau, Amanda Lembke, Francis Lin, Kevin D. Little, Kenneth R. Lofstrom, Sofia Lotfi, David J. Logan, Yi Luo, Franck Madoux, Paula A. Marin Zapata, Brittany A. Marion, Glynn Martin, Nicola Jane McCarthy, Lewis Mervin, Lisa Miller, Haseeb Mohamed, Tiziana Monteverde, Elizabeth Mouchet, Barbara Nicke, Arnaud Ogier, Anne-Laure Ong, Marc Osterland, Magdalena Otrocka, Pieter J. Peeters, James Pilling, Stefan Prechtl, Chen Qian, Krzysztof Rataj, David E Root, Sylvie K. Sakata, Simon Scrace, Hajime Shimizu, David Simon, Peter Sommer, Craig Spruiell, Iffat Sumia, Susanne E Swalley, Hiroki Terauchi, Amandine Thibaudeau, Amy Unruh, Jelle Van de Waeter, Michiel Van Dyck, Carlo van Staden, Michal Warchol, Erin Weisbart, Am\u00e9lie Weiss, Nicolas Wiest-Daessle, Guy Williams, Shan Yu, Bolek Zapiec, Marek \u017dyla, Shantanu Singh, and Anne E. Carpenter. Jump cell painting dataset: morphological impact of 136,000 chemical and genetic perturbations. bioRxiv, 2023.\n\n- David R Cox. Interaction. International Statistical Review/Revue Internationale de Statistique, pp. 1-24, 1984.\n\n- A Craxton, M Butterworth, N Harper, L Fairall, J Schwabe, A Ciechanover, and G M Cohen. Noxa, a sensor of proteasome integrity, is degraded by 26s proteasomes by an ubiquitin-independent pathway that is blocked by mcl-1. Cell Death Differ, 19(9):1424-1434, Sep 2012. ISSN 1476-5403 (Electronic); 1350-9047 (Print); 1350-9047 (Linking). doi: 10.1038/cdd.2012.16.\n\n- Kyle R Cron, Kaya Zhu, Deepa S Kushwaha, Grace Hsieh, Dmitry Merzon, Jonathan Rameseder, Clark C Chen, Alan D D'Andrea, and David Kozono. Proteasome inhibitors block dna repair and radiosensitize non-small cell lung cancer. PLoS One, 8(9):e73710, 2013. ISSN 1932-6203 (Electronic); 1932-6203 (Linking). doi: 10.1371/journal.pone.0073710.\n\n- Shaoli Das, Xiang Deng, Kevin Camphausen, and Uma Shankavaram. Synthetic lethal drug combinations targeting proteasome and histone deacetylase inhibitors in tp53-mutated cancers. Arch Cancer Biol Ther, 1(2):42-47, 2020.\n\n- Amy Berrington de Gonz\u00e1lez and D. R. Cox. Interpretation of interaction: A review. The Annals of Applied Statistics, 1(2):371-385, 2007.\n\n- Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv. Communications on pure and applied mathematics, 36(2):183-212, 1983.\n\n- Alan Dove. High-throughput screening goes to school. Nature Methods, 4(6):523-532, 2007.\n\n- Kevin Drew, John B Wallingford, and Edward M Marcotte. hu. map 2.0: integration of over 15,000 proteomic experiments builds a global compendium of human multiprotein assemblies. Molecular systems biology, 17(5):e10016, 2021.\n\n- Stossi Fabio, K Singh Pankaj, Safari Kazem, Marini Michela, Labate Demetrio, and A Mancini Michael. High throughput microscopy and single cell phenotypic image-based analysis in toxicology and drug discovery. Biochemical Pharmacology, pp. 115770, 2023.\n\n- Marta M Fay, Oren Kraus, Mason Victors, Lakshmanan Arumugam, Kamal Vuggumudi, John Urbanik, Kyle Hansen, Safiye Celik, Nico Cernek, Ganesh Jagannathan, et al. Rrx3: Phenomics map of biology. bioRxiv, pp. 2023-02, 2023.\n\n- Adam Foster. Variational, Monte Carlo and policy-based approaches to Bayesian experimental design. PhD thesis, University of Oxford, 2021.\n\n- Thomas Gaudelet, Alice Del Vecchio, Eli M Carrami, Juliana Cudini, Chantriolnt-Andreas Kapourani, Caroline Uhler, and Lindsay Edwards. Season combinatorial intervention predictions with salt & peper. arXiv preprint arXiv:2404.16907, 2024.\n\n- Meiyi Ge, Jian Luo, Yi Wu, Guobo Shen, and Xi Kuang. The biological essence of synthetic lethality: Bringing new opportunities for cancer therapy. MedComm - Oncology, 3(1):e70, 2024. doi: https://doi.org/10.1002/mog2.70. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/mog2.70.\n\n- Sandesh Ghimire, Aria Masoomi, and Jennifer Dy. Reliable estimation of kl divergence using a discriminator in reproducing kernel hilbert space. In Advances in Neural Information Processing Systems, 2021.\n\n- Madalina Giurgiu, Julian Reinhard, Barbara Brauner, Irmtraud Dunger-Kaltenbach, Gisela Fobo, Goar Frishman, Corinna Montrone, and Andreas Ruepp. Corum: the comprehensive resource of mammalian protein complexes\u20142019. *Nucleic acids research*, 47(D1):D559\u2013D563, 2019.\n\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, 2014.\n\n- Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723-773, 2012.\n\n- Kyuho Han, Edwin E Jeng, Gaelen T Hess, David W Morgens, Amy Li, and Michael C Bassik. Synergistic drug combinations for cancer identified in a crisp screen for pairwise genetic interactions. Nat Biotechnol, 35(5):463-474, May 2017. ISSN 1546-1696 (Electronic); 1087-0156 (Print); 1087-0156 (Linking). doi: 10.1038/nbt.3834.\n\n- Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr D\u00f3l\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022.\n\n- Joeri Hermans, Volodimir Begy, and Gilles Louppe. Likelihood-free mcmc with amortized approximate ratio estimators. In International conference on machine learning, pp. 4239-4248. PMLR, 2020.\n\n- R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2019.\n\n- Kexin Huang, Romain Lopez, Jan-Christian Hutter, Takamasa Kudo, Antonio Rios, and Aviv Regev. Sequential optimal experimental design of perturbation screens guided by multi-modal priors. bioRxiv, pp. 2023-12, 2023.\n\n- Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ica. Advances in Neural Information Processing Systems, 29, 2016.\n\n- Aapo Hyv\u00e4rinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. Neural Networks, 12(3):429-439, 1999. ISSN 0893-6080. doi: https://doi.org/10.1016/S0893-6080(98)00140-3. URL https://www.sciencedirect.com/science/article/pii/S0893608098001403.\n\n- Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 859-868. PMLR, 2019.\n\n- Yibo Jiang and Bryon Aragam. Learning nonparametric latent causal graphs with unknown interventions. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=S8DFqgmEbe.\n\n- Justin Kale, Elizabeth J Osterlund, and David W Andrews. BCL-2 family proteins: changing partners in the dance towards death. Cell Death & Differentiation, 25(1):65\u201380, January 2018. ISSN 1476-5403. doi: 10.1038/cdd.2017.186. URL https://doi.org/10.1038/cdd.2017.186.\n\n- Varun Kanade, H Brendan McMahan, and Brent Bryan. Sleeping experts and bandits with stochastic action availability and adversarial rewards. In Artificial Intelligence and Statistics, pp. 272-279. PMLR, 2009.\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017. URL https://arxiv.org/abs/1412.6980.\n\n- Yinfei Kong, Daoji Li, Yingying Fan, and Jinchi Lv. Interaction pursuit in high-dimensional multi-response regression via distance correlation. The Annals of Statistics, 45(2):897-922, 2017.\n\n- Steven G Krantz and Harold R Parks. Geometric integration theory. Springer Science & Business Media, 2008.\n\n- Oren Kraus, Kian Kenyon-Dean, Saber Saberian, Maryam Fallah, Peter McLean, Jess Leung, Vasudev Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik, et al. Masked autoencoders are scalable learners of cellular morphology. arXiv preprint arXiv:2309.16064, 2023.\n\n- Monique Laurent. Matrix completion problems. Encyclopedia of Optimization, 3:221-229, 2009.\n\n- Shijie Li, Win Topatana, Sarun Juengpanich, Jiasheng Cao, Jiahao Hu, Bin Zhang, Diana Ma, Xiujun Cai, and Mingyu Chen. Development of synthetic lethality in cancer: molecular and cellular classification. Signal Transduction and Targeted Therapy, 5(1):241, 2020. doi: 10.1038/s41392-020-00358-6. URL https://doi.org/10.1038/s41392-020-00358-6.\n\n- Luana Licata, Prisca Lo Surdo, Marta Iannuccelli, Alessandro Palma, Elisa Micarelli, Livia Perfetto, Daniele Peluso, Alberto Calderone, Luisa Castagnoli, and Gianni Cesareni. Signor 2.0, the signaling network open resource 2.0: 2019 update. Nucleic acids research, 48(D1):D504-D510, 2020.\n\n- Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical Statistics, 27(4):986-1005, 1956.\n\n- Lisa M Lindqvist and David L Vaux. Bcl2 and related prosurvival proteins require bak1 and bax to affect autophagy. Autophagy, 10(8):1474-1475, Aug 2014. ISSN 1554-8635 (Electronic); 1554-8627 (Print); 1554-8627 (Linking). doi: 10.4161/auto.29639.\n\n- Romain Lopez, Natasa Tagasovska, Stephen Ra, Kyunghyun Cho, Jonathan Pritchard, and Aviv Regev. Learning causal representations of single cells via sparse mechanism shift modeling. In Mihaela van der Schaar, Cheng Zhang, and Dominik Janzing (eds.), Proceedings of the Second Conference on Causal Learning and Reasoning, volume 213 of Proceedings of Machine Learning Research, pp. 662-691. PMLR, 11-14 Apr 2023. URL https://proceedings.mlr.press/v213/lopez23a.html.\n\n- Mohammad Lotfollahi, F Alexander Wolf, and Fabian J Theis. scgen predicts single-cell perturbation responses. Nature methods, 16(8):715-721, 2019.\n\n- Mohammad Lotfollahi, Anna Klimovskaia Susmelj, Carlo De Donno, Leon Hetzel, Yuge Ji, Ignacio L Ibarra, Sanjay R Srivatsan, Mohsen Naghipourfar, Riza M Daza, Beth Martin, Jay Shendure, Jose L McFaline-Figueroa, Pierre Boyeau, F Alexander Wolf, Nafissa Yakubova, Stephan Gunnemann, Cole Trapnell, David Lopez-Paz, and Fabian J Theis. Predicting cellular responses to complex perturbations in high-throughput screens. Molecular Systems Biology, 19(6):e11517, 2023. doi: https://doi.org/10.15252/msb.202211517. URL https://www.embopress.org/doi/abs/10.15252/msb.202211517.\n\n- Clare Lyle, Arash Mehrjou, Pascal Notin, Andrew Jesson, Stefan Bauer, Yarin Gal, and Patrick Schwab. Discobax discovery of optimal intervention sets in genomic experiment design. In International Conference on Machine Learning, pp. 23170-23189. PMLR, 2023.\n\n- Arash Mehrjou, Ashkan Soleymani, Andrew Jesson, Pascal Notin, Yarin Gal, Stefan Bauer, and Patrick Schwab. Genedisco: A benchmark for experimental design in drug discovery. In International Conference on Learning Representations, 2021.\n\n- Christoph B Messner, Vadim Demichev, Ziyue Wang, Johannes Hartl, Georg Kustatscher, Michael M\u00fclleder, and Markus Ralser. Mass spectrometry-based high-throughput proteomics and its role in biomedical studies and systems biology. Proteomics, 23(7-8):2200013, 2023.\n\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\n- John A Morris, Jennifer S Sun, and Neville E Sanjana. Next-generation forward genetic screens: uniting high-throughput perturbations with single-cell analysis. Trends in Genetics, 2023.\n\n- Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Scholkopf, et al. Kernel mean embedding of distributions: A review and beyond. Foundations and Trends\u00ae in Machine Learning, 10(1-2):1-141, 2017.\n\n- Sebastian M.B. Nijman. Synthetic lethality: General principles, utility and detection using genetic screens in human cells. FEBS Letters, 585(1):1-6, 2011. ISSN 0014-5793.\n\n- Aldo Pacchiano, Drausin Wulsin, Robert A Barton, and Luis Voloch. Neural design for genetic perturbation experiments. In The Eleventh International Conference on Learning Representations, 2023.\n\n- Du Phan, Neeraj Pradhan, and Martin Jankowiak. Composable effects for flexible and accelerated probabilistic programming in numpyro. arXiv preprint arXiv:1912.11554, 2019.\n\n- Tom Rainforth, Adam Foster, Desi R Ivanova, and Freddie Bickford Smith. Modern bayesian experimental design. arXiv preprint arXiv:2302.14545, 2023.\n\n- Aaditya Ramdas, Sashank Jakkam Reddi, Barnab\u00e1s P\u00f3czos, Aarti Singh, and Larry Wasserman. On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015.\n\n- Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial intelligence and statistics, pp. 814-822. PMLR, 2014.\n\n- Geoffrey Roeder, Luke Metz, and Durk Kingma. On linear identifiability of learned representations. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9030-9039. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/roeder21a.html.\n\n- Adrien Rousseau and Anne Bertolotti. Regulation of proteasome assembly and activity in health and disease. Nature Reviews Molecular Cell Biology, 19(11):697-712, 2018. doi: 10.1038/s41580-018-0040-z. URL https://doi.org/10.1038/s41580-018-0040-z.\n\n- Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of thompson sampling. The Journal of Machine Learning Research, 17(1):2442-2471, 2016.\n\n- Elizabeth G Ryan, Christopher C Drovandi, James M McGree, and Anthony N Pettitt. A review of modern computational algorithms for bayesian optimal design. International Statistical Review, 84(1):128-154, 2016.\n\n- Bernhard Sch\u00f6lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612-634, 2021.\n\n- Paola Sebastiani and Henry P Wynn. Maximum entropy sampling and optimal bayesian experimental design. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 62(1):145-157, 2000.\n\n- Enyuan Shang, Trang T T Nguyen, Chang Shu, Mike-Andrew Westhoff, Georg Karpel-Massler, and Markus D Siegelin. Epigenetic targeting of mcl-1 is synthetically lethal with bcl-xl/bcl-2 inhibition in model systems of glioblastoma. Cancers (Basel), 12(8), Aug 2020. ISSN 2072-6694 (Print); 2072-6694 (Electronic); 2072-6694 (Linking). doi: 10.3390/cancers12082137.\n\n- Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information estimators. In International Conference on Learning Representations, 2020.\n\n- Christian Squires, Aryeh Seigal, Shreya S Bhate, and Caroline Uhler. Linear causal disentanglement via interventions. In International Conference on Machine Learning. PMLR, 2023.\n\nResponse: \\boxed{Accept}\n\n### Rationale for Acceptance Prediction\n\n**Originality:** \nThe paper presents a novel approach to detect pairwise interactions from unstructured data using statistical tests and active learning. It addresses a gap in the current methodologies for high-throughput perturbation experiments by proposing a framework that leverages unstructured data (e.g., images) to infer interactions between perturbations. The novelty lies in combining ideas from causal representation learning, active learning, and unsupervised learning to discover interactions in a way that is scalable and efficient.\n\n**Methodological Rigor:**\n- **Theoretical Framework:** The authors derive tests for separability and disjointedness of perturbations, providing a solid theoretical foundation.\n- **Empirical Validation:** The paper validates the theoretical claims on synthetic data and demonstrates the practical utility on biological data. The use of KL divergence and MMD tests, along with the application of active learning strategies, adds substantial methodological rigor.\n- **Adaptive Experimental Design:** The proposed approach uses adaptive sampling to select pairs of perturbations for testing, leveraging the IDS (Information Directed Sampling) algorithm, which is a sophisticated active learning technique.\n\n**Significance of Contribution:**\n- **Field Impact:** The work has direct relevance to high-throughput screening in genomics and other biological fields, where understanding interactions between perturbations is crucial.\n- **Potential Impact:** By enabling more efficient and effective experimental design, this approach could lead to faster identification of novel biological interactions, which is valuable for advancing fundamental biological research and drug discovery.\n\n**Clarity of Presentation:**\n- **Clear Introduction:** The paper provides a clear and concise introduction that motivates the problem and outlines the contributions.\n- **Detailed Methodology:** The theoretical derivations and experimental setup are well-explained, making the methodology accessible to researchers in the field.\n- **Robust Results:** The results are presented clearly and backed by appropriate figures and tables.\n\n**Alignment with Journal Scope:**\n- **ICLR Scope:** While ICLR primarily focuses on machine learning and artificial intelligence, this work bridges the gap between these areas and computational biology. Given the importance of integrating computational methods with biological data, this paper fits well within the broader scope of ICLR.\n- **Specific Alignment:** The paper integrates concepts from causal inference, active learning, and unsupervised learning, which are topics of interest in ICLR.\n\nOverall, the paper presents a well-thought-out and practically useful approach that advances the state-of-the-art in the field of high-throughput perturbation experiments. It offers a promising solution to a significant challenge in experimental design, and the combination of theory, empirical validation, and practical implementation makes it a strong candidate for acceptance in ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636604.8016212", "msecs": "801.0", "relativeCreated": "65494.76170539856", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning\n\n# Abstract\nModel-agnostic meta-learning (MAML) is a popular method for few-shot learning but assumes that we have access to the meta-training set. In practice, training on the meta-training set may not always be an option due to data privacy concerns, intellectual property issues, or merely lack of computing resources. In this paper, we consider the novel problem of repurposing pretrained MAML checkpoints to solve new few-shot classification tasks. Because of the potential distribution mismatch, the original MAML steps may no longer be optimal. Therefore we propose an alternative meta-testing procedure and combine MAML gradient steps with adversarial training and uncertainty-based stepsize adaptation. Our method outperforms \"vanilla\" MAML on same-domain and cross-domains benchmarks using both SGD and Adam optimizers and shows improved robustness to the choice of base stepsize.\n\n\n# 1 INTRODUCTION\n\n\nDeep learning approaches have shown improvements based on massive datasets and enormous computing resources. Despite their success, it is still challenging to apply state-of-the-art methods in the real world. For example, in semiconductor manufacturing (Nishi & Doering, 2000), collecting each new data point is costly and time consuming because it requires setting up a new manufacturing process accordingly. Moreover, in the case of a \"destructive inspection\", the cost is very high because the wafer must be destroyed for measurement. Therefore, learning from small amounts of data is important for practical purposes.\n\n\nMeta-learning (learning-to-learn) approaches have been proposed for learning under limited data constraints. A meta-learning model optimizes its parameters for the best performance on the distribution of tasks. In particular, few-shot learning (FSL) formulates \"learning from limited data\" as an  $n$ -way  $k$ -shot problem, where  $n$  is the number of classes and  $k$  is the number of labeled samples per class. For each task in FSL, a support set is provided for training, while a query set is provided for evaluation. Ideally, a meta-learning model trained over a set of tasks (meta-training) will generalize well to new tasks (meta-testing).\n\n\nModel-agnostic meta-learning (MAML) (Finn et al., 2017) is a general end-to-end approach for solving few-shot learning tasks. MAML is trained on the meta-training tasks to learn a model initialization (also known as checkpoint) such that a few gradient steps on the support set will yield the best predictions on the query set.\n\n\nHowever, in practice it may not always be possible to retrain or finetune on the meta-training set. This situation may arise when the meta-training data is confidential, subject to restrictive licences, contains private user information, or protected intellectual property such as semiconductor manufacturing know-how. Another reason is that one may not have the computing resources necessary for running large-scale meta-training.\n\n\nIn this paper, we consider the novel problem of repurposing MAML checkpoints to solve new few-shot classification tasks, without the option of (re)training on the meta-training set. Since the meta-testing set (new tasks) may differ in distribution from the meta-training set, the implicit assumption \u2013in end-to-end learning\u2013 of identically distributed tasks may not hold, so there is no reason why the meta-testing gradient steps should match the meta-training. Therefore, we investigate various improvements over the default MAML gradient steps for test time adaptation.\n\n\nConceptually, our approach consists of collecting information while training the model on a new support set and then proposing ways to use this information to improve the adaptation. In this paper, we consider the variance of model parameters during ensemble training as a source of information to use. We propose algorithms that uses this information both to adapt the stepsizes for MAML as well as to generate \"task-specific\" adversarial examples to help robust adaptation to the new task.\n\n\nOur main contributions are the following:\n\n- - We motivate the novel problem of repurposing MAML checkpoints to solve cross-domain few-shot classification tasks, in the case where the meta-training set is inaccessible, and propose a method based on uncertainty-based stepsize adaptation and adversarial data augmentation, which has the particularity that meta-testing differs from meta-training steps.\n\n- - Compared to \"vanilla\" MAML, our method shows improved accuracy and robustness to the choice of base stepsizes on popular cross-domain and same-domain benchmarks, using both the SGD and Adam (Kingma & Ba, 2014) optimizers. Our results also indicate that adversarial training (AT) is helpful in improving the model performance during meta-testing.\n\n\nTo the best of our knowledge, our work is the first few-shot learning method to combine the use of ensemble methods for stepsize computation and generating adversarial examples from the meta-testing support set for improved robustness. Moreover, our empirical observation of improving over the default meta-testing procedure of MAML motivates further research on alternative ways to leverage published model checkpoints.\n\n\n# 2 RELATED WORK\n\n\n# 2.1 META-LEARNING AND MODEL-AGNOSTIC META-LEARNING\n\n\nFSL approaches deal with an extremely small amount of training data and can be classified into three categories. First, metric-based approaches solve few-shot tasks by training a feature extractor that maximizes inter-class similarity and intra-class dissimilarity (Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018). Second, memory-based approaches utilize previous tasks for new tasks with external memories (Santoro et al., 2016; Mishra et al., 2018). Third, optimization-based approaches search for good initial parameters during training and adapt the pretrained model for new tasks (Finn et al., 2017; Lee & Choi, 2018; Grant et al., 2018). We focus on the optimization-based approaches and suggest better training especially for the general family of MAML methods.\n\n\n# 2.2 UNCERTAINTY FOR MODEL TRAINING\n\n\nUncertainty is an important criterion for measuring the robustness of a neural network (NN). Bayesian neural networks (BNN) (Blundell et al., 2015) obtain model uncertainty by placing prior distributions over the weights  $p(\\omega)$ . This uncertainty has been used to adapt the stepsizes during continual learning in Uncertainty-guided Continual BNNs (UCB) (Ebrahimi et al., 2020). For each parameter, UCB scales its stepsize inversely proportional to the uncertainty of the parameter in the BNN to reduce changes in important parameters while allowing less important parameters to be modified faster in favor of learning new tasks. Our approach also decreases the step sizes for \"uncertain\" parameters, but using a different notion of uncertainty, and instead in the context of FSL with a pretrained MAML checkpoint.\n\n\nUnlike BNNs, deep ensembles (Lakshminarayanan et al., 2017) estimate uncertainty of NNs using ensembles over randomly initialized parameters combined with adversarial training (AT). Deep ensembles have been successfully used to boost predictive performance and AT has been used to improve robustness to adversarial examples. Lakshminarayanan et al. (2017) showed that they produce predictive uncertainty estimates comparable in quality to BNNs. Deep ensembles, however, are not directly applicable for FSL tasks, as training randomly initialized parameters from scratch with a limited amount of training data yields poor performance. Our approach is partly inspired from deep ensembles, adapting it to the FSL setting by using instead a parameter perturbations of the MAML checkpoint model rather than a random initialization. We use in particular a multiplicative Gaussian perturbation that rescales the parameters, as the information content of the weights is said to be invariant to their scale (Wen et al., 2018).\n\n\n# 2.3 ADVERSARIAL TRAINING FOR ATTACK AND DEFENSE\n\n\nDeep NNs are sensitive to adversarial attacks (Goodfellow et al., 2015; Madry et al., 2018; Moosavi-Dezfooli et al., 2016). These methods generate an adversarial sample to fool a trained model, where the generated image looks identical to the original one for humans. Goodfellow et al. (2015) proposed the fast gradient sign method (FGSM) which generates adversarial example using sign of input gradient. While stronger attacks have been proposed (such as using projected gradient descent (Madry et al., 2018)), we will focus on the FGSM approach in this paper for simplicity. In FSL, adversarial attack and defense have also been studied. ADversarialMeta-Learner (Yin et al., 2018) utilized one-step adversarial attack for generating adversarial samples during meta-training. There is little consideration, however, for the degradation of accuracy in the original sample in adversarial defense approaches. Xie et al. (2020) reported improvement using generated examples for adversarial attack on the large scale training. Motiian et al. (2017) propose adversarial domain adaptation for FSL. Despite similar keywords, their work is distinct from ours and relies on using generative adversarial networks discriminators to confuse domains, whereas we rely on adversarial examples for improved robustness. To our best knowledge, there is no prior FSL work which uses adversarial examples to enhance the model at test-time.\n\n\n# 2.4 DOMAIN ADAPTATION\n\n\nDomain adaptation methods (DA) (Ben-David et al., 2010) attempt to alleviate the distribution mismatch between source and target domains. Most of the recently proposed DA approaches are based on generative adversarial networks (GANs) (Goodfellow et al., 2014). GAN-based DA approaches require having access to the large amount of unlabeled data from both the source and target datasets (Tzeng et al., 2017; Zhang et al., 2019; Wilson & Cook, 2020). DA methods cannot be directly applied in the FSL scenario due to limited number of target domain samples (shots). Some domain-adaptive FSL (DA-FSL) methods have been proposed in the case where only a very few samples from the target domain are available (Motiian et al., 2017; Zhao et al., 2020).\n\n\nDA and DA-FSL methods cannot be directly applied to our setting because we assume that the meta-training dataset is inaccessible \u2014 mirroring real world situations in which access to the meta-training set is restricted by privacy and confidentiality concerns. Our approach differs from DA and DA-FSL by not requiring access to the meta-training dataset (source domain).\n\n\n# 3 PROPOSED METHOD\n\n\nAt meta-testing time, MAML normally uses the support set to compute fixed gradient steps, which were \"calibrated\" using end-to-end learning during meta-training. That is, the learned initialization is such that a fixed combination of stepsize and loss result in the desired result. However, those stepsizes and losses may be suboptimal on the new task, especially if the new task is out-of-domain with respect to the meta-training tasks.\n\n\nOur method is based on the assumption that the support set can be used to improve the meta-testing procedure itself, beyond merely serving as training examples. We start by leveraging the support set to estimate task-specific uncertainties over the model parameters. Then, we propose two improvements over the \"vanilla\" MAML gradient steps: we scale the gradient steps using layer-wise stepsizes computed from the support set and we train using task-specific adversarial examples.\n\n\n# 3.1 MOTIVATING HYPOTHESES\n\n\nAt meta-testing time, we start by assuming that we can estimate task-specific uncertainties over the model parameters. One possibility, which we adopt in Section 3.2, is to train deep ensembles (Lakshminarayanan et al., 2017) on the support set and use the ensemble to estimate variances over model parameters. Each model learns slightly different parameters, and yields slightly different gradients. We regard the variance over the parameters and input gradients as task-specific uncertainties. Given the uncertainty estimates, we propose two modifications to the original MAML gradient steps.\n\n\nProposal 1 (Task-specific step sizes) Use lower step sizes for model parameters with higher variance.\n\n\nIn our case, the variance could be further amplified if high-variance components were to be moved with large stepsizes. Therefore, we carefully move high-variance components with a lower stepsize with the hope is that we can limit the variance over the model parameters. This approach can be related with the fact that lower step sizes should be taken for SGD when the gradients are very noisy (Dieuleveut et al., 2020).\n\n\nProposal 2 (Task-specific adversarial examples) Use adversarial examples with higher adversarial perturbation on input components with higher variance over the input gradient.\n\n\nThe intuition is that if only slightly perturbed models (from the ensemble) disagree on parts of the input gradient, then it means that they disagree on what to learn and therefore those parts of the input are more vulnerable to attack. Therefore, we propose to use AT with stronger adversarial perturbation in the weak parts of the input, with the hope to incur improved robustness on those parts of the input.\n\n\nWe regard adversarial training as a form of data augmentation or regularization at meta-testing time, which we hope allows the model to overcome the limited size of the support set.\n\n\n# 3.2 UNCERTAINTY-BASED GRADIENT STEPS AT TEST-TIME\n\n\nWe improve over the default MAML gradient steps by implementing the ideas presented in the previous section. The resulting approach is detailed in Algorithm 1 and is a combination of uncertainty-based stepsize adaption (USA, based on Proposal 1) and uncertainty-based fast gradient sign method (UFGSM), adversarial training (AT), and generating additional adversarial examples from deep ensembles (EnAug) which are based on Proposal 2.\n\n\n```\nData: New task support set  $D^{\\mathrm{Spt}} = \\{(x_1,y_1),\\ldots ,(x_{n\\times k},y_{n\\times k})\\}$  with  $n$  ways and  $k$  shots.   \nRequire: Base stepsize  $\\alpha$  , pretrained weights  $\\theta_0$  , Gaussian variance  $\\sigma$  , AT coefficient  $\\epsilon$  , size of ensemble  $M$  , number of gradient steps  $T$  , selection coefficients in  $\\{0,1\\}$  for: base stepsize  $\\lambda_{\\alpha}$  , adversarial loss  $\\lambda_{\\mathrm{AT}}$  , and augmented cross-entropy  $\\lambda_{\\mathrm{Aug}}$  .   \n1  $D^{\\mathrm{Aug}}\\gets \\emptyset ,\\alpha^{\\mathrm{adap}}\\gets \\alpha$    \n2 for  $m = 1$  to  $M$  do   \n3  $\\theta_0^m\\gets \\theta_0(1 + \\mathcal{N}(0,\\sigma))$  \u25b7Initialize deep ensemble   \n4 end   \n5 for  $t = 1$  to  $T$  do   \n6 for  $m = 1$  to  $M$  do   \n7  $\\theta_t^m\\gets \\theta_{t - 1}^m -\\alpha^{\\mathrm{adap}}\\odot \\nabla_{\\theta_{t - 1}^m}[\\mathcal{L}(D^{\\mathrm{Spt}},\\theta_{t - 1}^m) + \\mathcal{L}_{\\mathrm{AT}}(D^{\\mathrm{Spt}},\\theta_{t - 1}^m)]$    \n8  $D^{\\mathrm{Aug}}\\gets D^{\\mathrm{Aug}}\\cup \\{(\\mathcal{A}_{\\theta_{t - 1}^m}(x,y),y)\\mid (x,y)\\in D^{\\mathrm{Spt}}\\}$  \u25b7EnAug   \n9 end   \n10  $\\alpha^{\\mathrm{adap}}\\gets \\mathrm{USA}(\\alpha ,\\theta_t^{1:M})$  \u25b7Algorithm 2   \n11  $D^{\\mathrm{Aug}}\\gets D^{\\mathrm{Aug}}\\cup \\{\\mathrm{UFGSM}(\\theta_0,\\theta_t^{1:M},x,y)\\mid (x,y)\\in D^{\\mathrm{Spt}}\\}$  \u25b7Algorithm 3   \n12 end   \n13  $\\alpha \\gets \\lambda_{\\alpha}\\alpha +(1 - \\lambda_{\\alpha})\\alpha^{\\mathrm{adap}}$    \n14 for  $t = 1$  to  $T$  do   \n15  $\\theta_t\\gets \\theta_{t - 1} - \\alpha \\odot \\nabla_{\\theta_{t - 1}}[\\mathcal{L}(D^{\\mathrm{Spt}},\\theta_{t - 1}) + \\lambda_{\\mathrm{AT}}\\mathcal{L}_{\\mathrm{AT}}(D^{\\mathrm{Spt}},\\theta_{t - 1}) + \\lambda_{\\mathrm{Aug}}\\mathcal{L}(D^{\\mathrm{Aug}},\\theta_{t - 1})]$    \n16 end\n```\n\n\nDenote  $\\mathcal{L}(D,\\theta) = 1 / |D|\\sum_{(x,y)\\in D}l_{\\theta}(x,y)$  the cross-entropy loss for model  $\\theta$  over the labeled dataset  $D = \\{(x,y)\\ldots \\}$ ,  $\\mathcal{A}_{\\theta}(x,y) = x + \\epsilon \\mathrm{sign}(\\nabla_{x}l_{\\theta}(x,y))$  the FGSM adversarial example computed from  $(x,y)$  and  $\\mathcal{L}_{\\mathrm{AT}}(D,\\theta) = 1 / |D|\\sum_{(x,y)\\in D}l_{\\theta}(\\mathcal{A}_{\\theta}(x,y),y)$  the resulting adversarial\n\n\n![(a) Uncertainty of layers](images/96283a6fe48814d04c04895478c16e85cf7aafdf9b4876c98ad155eb2c850080.jpg)\n\n\n![(b) Uncertainty-based stepsize adaption Figure 1: USA converts uncertainty into layer-wise adapted stepsize. Here  $\\alpha = 0.01$ . (a) is standard deviation of trained ensemble models' weights. Each layer has a difference uncertainty. (b) is adapted stepsize by USA. Each layer has a different stepsize based on the uncertainty.](images/0fa625b9ac33a4a7cd8c22b8b9c67cee0ad7150187c0b4c0263b2d8f9264dbcc.jpg)\n\n\ncross-entropy, which we refer to as AT. For easy reference, all notations are summarized in Appendix A.1.\n\n\nStarting from the pretrained MAML checkpoint  $\\theta_0$ , we perturb the model parameters with multiplicative Gaussian noise to create a deep ensemble  $(\\theta_0^m)_{1\\leq m\\leq M}$  (lines 2-4). Then, we repeat the following for  $T$  steps. At time  $t$ , run gradient descent on each model  $\\theta_t^m$  of the ensemble (line 7), where the loss is a combination of the usual cross-entropy  $\\mathcal{L}$  and AT loss  $\\mathcal{L}_{\\mathrm{AT}}$  as in (Lakshminarayanan et al., 2017) but on the support set, and the stepsizes  $\\alpha^{\\mathrm{adap}}$  are updated using USA (details below). Also, we generate adversarial examples using FGSM and UFGSM (details below) and store them into  $D^{\\mathrm{Aug}}$  (lines 8 and 11). Finally, we run  $T$  gradient steps on the original checkpoint, where the loss is a combination of cross-entropy on the support set  $D^{\\mathrm{Spt}}$ , AT loss on the support set, and cross-entropy on the ensemble-augmented support set  $D^{\\mathrm{Aug}}$  (lines 13-16). Note that we recover the original MAML steps for  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 0, \\lambda_{\\alpha} = 1)$ , while we refer to the case  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 1, \\lambda_{\\alpha} = 0)$  as our full method.\n\n\nUSA. We propose uncertainty-based stepsize adaptation (USA), which assigns lower step sizes to layers<sup>1</sup> with higher uncertainty (Algorithm 2) - we call this loosely an \"inverse-relationship\" below. Let  $\\alpha$  denote the default (scalar) stepsize and  $\\theta^{1:M}$  the parameters of the ensemble models, where  $M$  is the size of the ensemble and the number of layers in model is  $L$ . To compute the adapted layer-wise stepsizes  $\\alpha^{\\mathrm{adap}}$ , we compute  $u$ , the parameter-wise standard deviation of the model parameters over the ensemble (line 2), apply an inverse-relationship transformation which flips the max with the min (line 3), average over each layer (line 4) and  $L_{1}$ -normalize the result (line 5). The design choices for USA are explained in Appendix A.2. The resulting step sizes  $\\alpha^{\\mathrm{adap}}$  have an inverse-relationship with the variance of each layer. We give an example of applying USA to the 4-ConvNet architecture on miniImageNet (Figure 1). On the left, we plot the layer-wise standard deviations of the parameters and on the right the corresponding USA step sizes, which follow an inverse relationship with the standard deviations.\n\n\n![Figure 2: Applying UFGSM to 4-ConvNet on miniImageNet with  $\\epsilon = 0.05$ . Starting from the clean image  $x$ , we add the signed gradient  $\\mathrm{sign}(\\nabla_x l_\\theta(x, y))$  after rescaling it by the uncertainty over the input gradient  $u$ , to generate the adversarial example  $x'$  (UFGSM). Note how UFGSM generated a more natural image than FGSM (rightmost,  $u = 1$ ). UFGSM. We also propose uncertainty-based FGSM (UFGSM) to generate adversarial examples, with higher adversarial perturbation on input components with higher variance over the input gradient (Algorithm 3). Starting from an image  $x$  and label  $y$ , we compute the input gradient for each model from the ensemble and calculate the standard deviation  $u$  over the ensemble (line 2). Then, we linearly map  $u$  between 0 and 1 (line 3), compute the FGSM adversarial example for the pretrained model  $\\theta_0$  and rescale it using  $u$ , so that areas of higher variance get more perturbation. We give an example of applying UFGSM to miniImageNet in Figure 2.](images/c9456698fd027d2ea74f43c14b939f7d1ff4e0dcc11a5f0423b44dd9d98016db.jpg)\n\n\n```\n1 Function USA  $(\\alpha, \\theta_t^{1:M})$   \n2  $u = \\operatorname{Std}(\\theta_t^{1:M})$   \n3  $c = \\operatorname{Max}(u) - u + \\operatorname{Min}(u)$   \n4  $\\mu^l \\gets \\text{average of } c \\text{ over each layer } l$   \n5  $\\alpha^{\\text{adap}} = \\alpha \\mu^l / (1/L \\sum_{l=1}^L \\mu^l)$   \n6 return  $\\alpha^{\\text{adap}}$   \n7 end\n```\n\n\n```\nDefine:MinMaxNorm  $(x) = \\frac{x - \\operatorname{Min}(x)}{\\operatorname{Max}(x) - \\operatorname{Min}(x)}$    \n1 Function UFGSM  $(\\theta ,\\theta_t^{1:M},x,y)$    \n2  $u = \\mathrm{Std}(\\{\\nabla_xl_{\\theta '}(x,y)|\\theta '\\in \\theta_t^{1:M}\\})$    \n3  $u = \\mathrm{MinMaxNorm}(u)$    \n4  $x^{\\prime} = x + \\epsilon u\\odot \\mathrm{sign}(\\nabla_{x}l_{\\theta}(x,y))$    \n5 return  $x^{\\prime}$    \n6 end\n```\n\n\n# 4 EXPERIMENTAL EVALUATION\n\n\nWe train MAML on miniImageNet (Vinyals et al., 2016) training split; we then apply our method on the resulting checkpoint. We evaluate our model on the test split of miniImageNet \u2013 for the same-domain setting \u2013 as well as CUB-200-2011 (Welinder et al., 2010), Traffic Sign (Houben et al., 2013) and VGG Flower (Nilsback & Zisserman, 2008) \u2013 for the cross-domain setting. These datasets are denoted as Mini, Birds, Signs and Flowers respectively. A desirable feature for an optimizer is to maintain good performance in a broad range of step sizes (Asi & Duchi, 2019). Therefore, we evaluate our approach not only at the optimal stepsize, but also over a broad range of base step sizes $^{2}$  from  $10^{-4}$  to 1. We evaluate the performance with three metrics: All, Top-1 and Top-40%. If two methods have comparable Top-1 performance, but one has better Top-40% performance, then that method is more robust to the choice of base stepsize. Detailed experimental setup and pretrained model selection are included in Appendix A.3. Our code is available at https://anonymous.4open.science/r/08ef52cf-456a-4e36-a408-04e1ad0bc5a9/ (anonymous project).\n\n\n# 4.1 MAIN RESULTS\n\n\nOur main results are in Table 1. We compare the default MAML steps, which we take as our baseline (denoted as SGD), with our full method (denoted as SGD+All), which consists of combining USA, UFSGM, EnAug, and AT ( $\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 1$ ,  $\\lambda_{\\alpha} = 0$  in Algorithm 1). In terms of absolute performance (Top-1 row), our method outperforms the baseline on cross-domain tasks (Birds,\n\n\nTable 1: Main results. We compare the default MAML steps (SGD) with our method (SGD+All) on same-domain and cross-domain benchmarks.\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"2\">5-way 1-shot</td><td colspan=\"2\">5-way 5-shot</td><td colspan=\"2\">10-way 1-shot</td></tr><tr><td>SGD</td><td>SGD+All</td><td>SGD</td><td>SGD+All</td><td>SGD</td><td>SGD+All</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>37.21\u00b10.22</td><td>36.24\u00b10.22</td><td>38.14\u00b10.39</td><td>43.48\u00b10.38</td><td>21.31\u00b10.37</td><td>22.07\u00b10.4</td></tr><tr><td>Birds</td><td>30.07\u00b10.52</td><td>29.83\u00b10.46</td><td>33.29\u00b10.29</td><td>38.07\u00b10.33</td><td>17.54\u00b10.06</td><td>17.81\u00b10.12</td></tr><tr><td>Flowers</td><td>40.54\u00b10.48</td><td>40.86\u00b10.45</td><td>44.55\u00b10.43</td><td>53.10\u00b10.55</td><td>26.79\u00b10.41</td><td>28.07\u00b10.41</td></tr><tr><td>Signs</td><td>34.19\u00b10.76</td><td>34.76\u00b10.61</td><td>45.39\u00b10.45</td><td>51.02\u00b10.5</td><td>23.77\u00b10.13</td><td>24.88\u00b10.13</td></tr><tr><td>Avg.</td><td>35.50\u00b10.49</td><td>35.42\u00b10.44</td><td>40.34\u00b10.39</td><td>46.42\u00b10.44</td><td>22.35\u00b10.24</td><td>23.21\u00b10.26</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.71\u00b10.81</td><td>46.62\u00b10.58</td><td>62.46\u00b10.68</td><td>62.16\u00b10.66</td><td>31.10\u00b10.66</td><td>31.42\u00b10.82</td></tr><tr><td>Birds</td><td>36.03\u00b10.48</td><td>36.43\u00b10.58</td><td>50.71\u00b10.54</td><td>51.82\u00b10.64</td><td>23.16\u00b10.21</td><td>23.61\u00b10.39</td></tr><tr><td>Flowers</td><td>51.97\u00b10.79</td><td>54.36\u00b11.04</td><td>70.63\u00b10.76</td><td>74.52\u00b10.69</td><td>38.61\u00b10.49</td><td>40.68\u00b10.58</td></tr><tr><td>Signs</td><td>43.44\u00b10.99</td><td>44.10\u00b11.03</td><td>72.52\u00b10.84</td><td>74.13\u00b10.90</td><td>35.10\u00b10.21</td><td>35.11\u00b10.26</td></tr><tr><td>Avg.</td><td>44.54\u00b10.77</td><td>45.37\u00b10.81</td><td>64.08\u00b10.71</td><td>65.66\u00b10.72</td><td>31.99\u00b10.39</td><td>32.70\u00b10.51</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.07\u00b10.52</td><td>45.79\u00b10.44</td><td>55.38\u00b10.46</td><td>60.70\u00b10.91</td><td>28.65\u00b10.64</td><td>30.56\u00b10.77</td></tr><tr><td>Birds</td><td>34.88\u00b10.55</td><td>35.77\u00b10.55</td><td>45.50\u00b10.49</td><td>51.03\u00b10.63</td><td>21.78\u00b10.07</td><td>22.87\u00b10.30</td></tr><tr><td>Flowers</td><td>49.98\u00b10.69</td><td>53.16\u00b10.73</td><td>65.16\u00b10.73</td><td>73.55\u00b10.86</td><td>36.14\u00b10.53</td><td>39.75\u00b10.61</td></tr><tr><td>Traffic</td><td>41.81\u00b11.02</td><td>43.39\u00b11.01</td><td>67.34\u00b10.43</td><td>71.45\u00b10.90</td><td>33.56\u00b10.11</td><td>34.27\u00b10.17</td></tr><tr><td>Avg.</td><td>42.94\u00b10.70</td><td>44.52\u00b10.68</td><td>58.34\u00b10.53</td><td>64.18\u00b10.82</td><td>30.03\u00b10.34</td><td>31.86\u00b10.46</td></tr></table>\n\n\nFlowers, Signs), while the performance is comparable on same-domain tasks (Mini). In terms of robustness to the base stepsize (All and Top-40% rows), our method outperforms the baseline over a large range of stepsizes for 5-way 5-shot and 10-way 1-shot tasks, while for 5-way 1-shot the results are either comparable (All) or better (Top-40%) depending on the metric considered. More results can be found in Appendix A.6.\n\n\n# 4.2 DISCUSSION\n\n\nAblation Study. We perform an ablation study for the 5-way 1-shot case in Table 2 and plot the accuracy at different base step sizes for the Flowers dataset in Figure 3. Comparing SGD to SGD+AT in Table 2, we observe that adversarial training is beneficial over the baseline (Top1 and Top-40%), except for large step sizes with SGD, which is reflected in the All metric and in the dip in performance in Figure 3a. Notice also how the use of AT flattens the accuracy curve near the optimal stepsize. Comparing SGD to SGD+USA, we observe a very small but consistent improvement over the baseline in the vicinity of the optimal stepsize (Top-1 and Top-40%). Comparing SGD+USA to SGD+USA+UFGSM, we observe improvement over a wide range of step sizes, which is reflected in the curves and in All and Top-40% metrics. Comparing SGD+USA+UFGSM+EnAug+AT to SGD+USA+UFGSM+EnAug shows that AT+EnAug is beneficial in terms of absolute performance (Top-1) as well as creating robustness to the choice of stepsize (Top-40%). The drop for the All metric is explained by the dip in the curve for the largest step sizes. Overall, the best absolute performance (Top-1) is always obtained through some use of adversarial training, while using the full method consistently results in increased robustness to the choice of stepsize (best Top-40% performance). Appendix A.5 shows various AT results.\n\n\nUFGSM vs. FGSM. We compare UFGSM to FGSM (Goodfellow et al., 2015) in Table 3. The results show that our uncertainty-based approach consistently outperforms FGSM, which suggests that the uncertainty information extracted from the support set was useful.\n\n\nSGD vs. Adam. Our method can be used with SGD and Adam, however, we have mainly focused on SGD throughout the paper. SGD tends to yield the best results (Top-1 and Top-40%), as shown in Table 4 for the baseline and full method. More results for Adam can be found in Sections A.6 and A.8 of the appendix.\n\n\nBest checkpoint vs. Overfitted checkpoint. We also consider the problem of repurposing an overfitted checkpoint. We find that our method improves both absolute performance and robustness to stepsize. In fact, the gains are more substantial on the overfitted (worse) checkpoint than the best checkpoint (Appendix A.8).\n\n\nTable 2: Ablation Study for 5-way 1-shot classification with SGD optimizer. SGD+AT corresponds to using  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\alpha} = 1, \\lambda_{\\mathrm{Aug}} = 0)$  in Algorithm 1, SGD+USA to  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = \\lambda_{\\alpha} = 0)$ , SGD+USA+UFGSM to  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\alpha} = 0, \\lambda_{\\mathrm{Aug}} = 1)$  and SGD+USA+UFGSM+EnAug+AT to  $(\\lambda_{\\mathrm{AT}} = \\lambda_{\\mathrm{Aug}} = 1, \\lambda_{\\alpha} = 0)$ .\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"5\">5-way 1-shot</td></tr><tr><td>SGD</td><td>SGD+AT</td><td>SGD+USA</td><td>SGD+USA +UFGSM</td><td>SGD+USA +UFGSM +EnAug+AT</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>37.21\u00b10.22</td><td>34.60\u00b10.18</td><td>37.53\u00b10.27</td><td>38.71\u00b10.28</td><td>36.24\u00b10.22</td></tr><tr><td>Birds</td><td>30.07\u00b10.52</td><td>28.81\u00b10.45</td><td>30.39\u00b10.53</td><td>31.07\u00b10.50</td><td>29.83\u00b10.46</td></tr><tr><td>Flowers</td><td>40.54\u00b10.48</td><td>38.80\u00b10.44</td><td>41.09\u00b10.49</td><td>42.33\u00b10.60</td><td>40.86\u00b10.45</td></tr><tr><td>Signs</td><td>34.19\u00b10.76</td><td>33.42\u00b10.59</td><td>35.02\u00b10.82</td><td>36.10\u00b10.48</td><td>34.76\u00b10.61</td></tr><tr><td>Avg.</td><td>35.50\u00b10.49</td><td>33.91\u00b10.41</td><td>36.01\u00b10.36</td><td>37.05\u00b10.27</td><td>35.42\u00b10.44</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.71\u00b10.81</td><td>46.58\u00b10.65</td><td>46.72\u00b10.71</td><td>46.66\u00b10.77</td><td>46.62\u00b10.58</td></tr><tr><td>Birds</td><td>36.03\u00b10.48</td><td>36.40\u00b10.48</td><td>36.08\u00b10.49</td><td>36.02\u00b10.48</td><td>36.43\u00b10.58</td></tr><tr><td>Flowers</td><td>51.97\u00b10.79</td><td>54.55\u00b10.84</td><td>52.01\u00b10.77</td><td>52.30\u00b10.77</td><td>54.36\u00b11.04</td></tr><tr><td>Signs</td><td>43.43\u00b10.99</td><td>43.98\u00b10.98</td><td>43.42\u00b11.06</td><td>43.96\u00b10.64</td><td>44.10\u00b11.03</td></tr><tr><td>Avg.</td><td>44.54\u00b10.77</td><td>45.38\u00b10.74</td><td>44.56\u00b10.44</td><td>44.73\u00b10.41</td><td>45.37\u00b10.81</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.07\u00b10.52</td><td>45.53\u00b10.50</td><td>45.23\u00b10.59</td><td>45.76\u00b10.62</td><td>45.79\u00b10.44</td></tr><tr><td>Birds</td><td>34.88\u00b10.55</td><td>35.57\u00b10.60</td><td>35.17\u00b10.55</td><td>35.30\u00b10.48</td><td>35.77\u00b10.55</td></tr><tr><td>Flowers</td><td>49.98\u00b10.69</td><td>52.92\u00b10.68</td><td>50.39\u00b10.71</td><td>51.07\u00b10.67</td><td>53.16\u00b10.73</td></tr><tr><td>Signs</td><td>41.83\u00b11.02</td><td>43.19\u00b10.96</td><td>42.08\u00b11.06</td><td>43.18\u00b10.73</td><td>43.39\u00b11.01</td></tr><tr><td>Avg.</td><td>42.94\u00b10.70</td><td>44.30\u00b10.68</td><td>43.22\u00b10.43</td><td>43.83\u00b10.39</td><td>44.52\u00b10.68</td></tr></table>\n\n\n![(a) With SGD optimizer](images/3df392ad3021c71ce0e7946c9228b5982feb1bef6db2f2f73375c4d33d688279.jpg)\n\n\n![(b) With Adam optimizer Figure 3: Ablation study for 5-way 1-shot classification on Flowers dataset.](images/891c2152ff15791a2798f7419bd448857b22aa1092a3e328b022664f1e282708.jpg)\n\n\nTable 3: Comparing UFGSM against FGSM (Goodfellow et al., 2015) on 5-way 1-shot tasks. For all metrics and datasets, the proposed uncertainty-based method is better.\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"2\">SGD+USA</td></tr><tr><td>+UFGSM</td><td>+FGSM</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>38.71\u00b10.28</td><td>36.97\u00b10.54</td></tr><tr><td>Birds</td><td>31.07\u00b10.50</td><td>30.05\u00b10.55</td></tr><tr><td>Flowers</td><td>42.33\u00b10.60</td><td>41.17\u00b10.55</td></tr><tr><td>Signs</td><td>36.10\u00b10.48</td><td>34.78\u00b10.50</td></tr><tr><td>Avg.</td><td>37.05\u00b10.27</td><td>35.74\u00b10.27</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.66\u00b10.77</td><td>43.93\u00b10.85</td></tr><tr><td>Birds</td><td>36.02\u00b10.48</td><td>34.52\u00b10.74</td></tr><tr><td>Flowers</td><td>52.30\u00b10.77</td><td>50.63\u00b10.84</td></tr><tr><td>Signs</td><td>43.96\u00b10.64</td><td>42.62\u00b11.00</td></tr><tr><td>Avg.</td><td>44.73\u00b10.41</td><td>42.93\u00b10.47</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.76\u00b10.62</td><td>43.18\u00b10.87</td></tr><tr><td>Birds</td><td>35.30\u00b10.48</td><td>33.88\u00b10.75</td></tr><tr><td>Flowers</td><td>51.07\u00b10.67</td><td>49.47\u00b10.69</td></tr><tr><td>Signs</td><td>43.18\u00b10.73</td><td>41.92\u00b10.90</td></tr><tr><td>Avg.</td><td>43.83\u00b10.39</td><td>42.11\u00b10.46</td></tr></table>\n\n\nTable 4: Comparing SGD vs. Adam for default MAML steps (baseline) and our method (baseline+all) on 5-way 1-shot classification.\n\n\n<table><tr><td rowspan=\"2\">Metric</td><td rowspan=\"2\">Dataset</td><td colspan=\"2\">Baseline</td><td colspan=\"2\">Baseline+All</td></tr><tr><td>SGD</td><td>Adam</td><td>SGD</td><td>Adam</td></tr><tr><td rowspan=\"5\">All</td><td>Mini</td><td>37.21\u00b10.22</td><td>31.87\u00b10.20</td><td>36.24\u00b10.22</td><td>33.57\u00b10.29</td></tr><tr><td>Birds</td><td>30.07\u00b10.52</td><td>28.02\u00b10.39</td><td>29.83\u00b10.46</td><td>29.51\u00b10.46</td></tr><tr><td>Flowers</td><td>40.54\u00b10.48</td><td>40.33\u00b10.57</td><td>40.86\u00b10.45</td><td>44.58\u00b10.70</td></tr><tr><td>Signs</td><td>34.19\u00b10.76</td><td>33.98\u00b10.43</td><td>34.76\u00b10.61</td><td>35.10\u00b10.43</td></tr><tr><td>Avg.</td><td>35.50\u00b10.49</td><td>33.55\u00b10.22</td><td>35.42\u00b10.44</td><td>35.69\u00b10.29</td></tr><tr><td rowspan=\"5\">Top-1</td><td>Mini</td><td>46.71\u00b10.81</td><td>43.08\u00b10.10</td><td>46.62\u00b10.58</td><td>44.72\u00b10.35</td></tr><tr><td>Birds</td><td>36.03\u00b10.48</td><td>34.00\u00b10.63</td><td>36.43\u00b10.58</td><td>35.45\u00b10.67</td></tr><tr><td>Flowers</td><td>51.97\u00b10.79</td><td>51.28\u00b10.47</td><td>54.36\u00b11.04</td><td>56.04\u00b11.14</td></tr><tr><td>Signs</td><td>43.43\u00b10.99</td><td>42.53\u00b10.79</td><td>44.10\u00b11.03</td><td>43.01\u00b10.85</td></tr><tr><td>Avg.</td><td>44.54\u00b10.77</td><td>42.72\u00b10.32</td><td>45.37\u00b10.81</td><td>44.80\u00b10.54</td></tr><tr><td rowspan=\"5\">Top-40%</td><td>Mini</td><td>45.07\u00b10.52</td><td>40.18\u00b10.14</td><td>45.79\u00b10.44</td><td>42.92\u00b10.35</td></tr><tr><td>Birds</td><td>34.88\u00b10.55</td><td>32.38\u00b10.61</td><td>35.77\u00b10.55</td><td>34.54\u00b10.66</td></tr><tr><td>Flowers</td><td>49.98\u00b10.69</td><td>48.18\u00b10.44</td><td>53.16\u00b10.73</td><td>54.08\u00b10.76</td></tr><tr><td>Signs</td><td>41.83\u00b11.02</td><td>40.90\u00b10.70</td><td>43.39\u00b11.01</td><td>42.26\u00b10.77</td></tr><tr><td>Avg.</td><td>42.94\u00b10.70</td><td>40.41\u00b10.29</td><td>44.52\u00b10.68</td><td>43.45\u00b10.40</td></tr></table>\n\n\n**Validating Proposal 1 and 2.** Our method is built on the hypotheses that we should take small step sizes on high uncertainty parameters and add more adversarial perturbation on high uncertainty input gradients. To validate those choices, we have tried taking large step sizes on high uncertainty parameters, and using less adversarial perturbation on high uncertainty input gradients, which resulted in lower accuracy and robustness to the stepsize (Appendix A.7).\n\n\nFreezing most uncertain layers. We observed that the batch normalization (BN) layers have the most uncertainty (Figure 1). In Proposal 1, we argued that the higher uncertainty component amplifies the error when we use bigger stepsize. As suggested by an anonymous reviewer, this motivates an additional experiment that we perform where we \"freeze\" the BN layer during meta-testing (i.e. we manually set the stepsize for the BN scale and shift parameters to zero). We performed the BN layer freezing experiment on  $\\mathrm{SGD + All}$  (see detailed results in Table 10 and Figure 12 in Appendix A.9). It turns out that  $\\mathrm{SGD + All}$  (w/ freezing BN) outperforms  $\\mathrm{SGD + All}$  (w/o freezing BN) on the All metric, with most improvement in the higher stepsize range ( $>0.1$ ). This is consistent with our intuition that updating high-uncertainty layers (such as BN) with large step sizes can be harmful.\n\n\n# 5 CONCLUSION\n\n\nIn this paper we considered the novel problem of repurposing pretrained MAML checkpoints for out-of-domain few-shot learning. Our method uses deep ensembles to estimate model parameter and input gradient uncertainties over the support set, and builds upon the default MAML gradient steps through the addition of uncertainty-based adversarial training and adaptive step sizes. Our experiments over popular few-shot benchmarks show that our method yields increased accuracy and robustness to the choice of base stepsize. More generally, our results motivate the use of adversarial learning as a data augmentation scheme for improving few-shot generalization. In the future, it would be interesting to apply our method to related settings such as domain adaption and transfer learning.\n\n\n# REFERENCES\n\n- Hilal Asi and John C. Duchi. The importance of better models in stochastic optimization. In National Academy of Sciences, 2019.\n\n- Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. In Machine Learning, 2010.\n\n- Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, 2015.\n\n- Aymeric Dieuleveut, Alain Durmus, Francis Bach, et al. Bridging the gap between constant step size stochastic gradient descent and markov chains. In Annals of Statistics. Institute of Mathematical Statistics, 2020.\n\n- Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided continual learning with bayesian neural networks. In International Conference on Learning Representations, 2020.\n\n- Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, 2017.\n\n- Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014.\n\n- Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.\n\n- Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-based meta-learning as hierarchical bayes. In International Conference on Learning Representations, 2018.\n\n- Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of traffic signs in real-world images: The german traffic sign detection benchmark. In International Joint Conference on Neural Networks, 2013.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In arXiv preprint arXiv:1412.6980, 2014.\n\n- Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, 2017.\n\n- Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In International Conference on Machine Learning, 2018.\n\n- Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\n\n- Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive metalearner. In International Conference on Learning Representations, 2018.\n\n- Seyed Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and accurate method to fool deep neural networks. In Computer Vision and Pattern Recognition, 2016.\n\n- Saeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gianfranco Doretto. Few-shot adversarial domain adaptation. In Advances in Neural Information Processing Systems, 2017.\n\n- Maria Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008.\n\n- Yoshio Nishi and Robert Doering. Handbook of semiconductor manufacturing technology. CRC press, 2000.\n\n- Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations, 2017.\n\n- Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International Conference on Machine Learning, 2016.\n\n- Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, 2017.\n\n- Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales. Learning to compare: Relation network for few-shot learning. In Computer Vision and Pattern Recognition, 2018.\n\n- Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Metadata: A dataset of datasets for learning to learn from few examples. In International Conference on Learning Representations, 2020.\n\n- Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and Ming-Hsuan Yang. Cross-domain few-shot classification via learned feature-wise transformation. In International Conference on Learning Representations, 2020.\n\n- Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Conference on Computer Vision and Pattern Recognition, 2017.\n\n- Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, 2016.\n\n- Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010.\n\n- Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudo-independent weight perturbations on mini-batches. In International Conference on Learning Representations, 2018.\n\n- Garrett Wilson and Diane J. Cook. A survey of unsupervised deep domain adaptation. In ACM Transactions on Intelligent Systems and Technology, 2020.\n\n- Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and Quoc V. Le. Adversarial examples improve image recognition. In Computer Vision and Pattern Recognition, 2020.\n\n- Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. Adversarial meta-learning. In arXiv preprint arXiv:1806.03316, 2018.\n\n- Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, 2014.\n\n- Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan. Domain-symmetric networks for adversarial domain adaptation. In Computer Vision and Pattern Recognition, 2019.\n\n- An Zhao, Mingyu Ding, Zhiwu Lu, Tao Xiang, Yulei Niu, Jiechao Guan, Ji Rong Wen, and Ping Luo. Domain-adaptive few-shot learning. In arXiv, 2020.\n\nResponse: \\boxed{Accept}\n\n### Rationale for Prediction\n\n#### Originality\n- **Motivation:** The paper introduces a novel problem of repurposing pretrained MAML checkpoints for out-of-domain few-shot learning, which is a timely and relevant issue given the increasing interest in efficient model adaptation and robustness to distribution shifts. This problem is not trivial and requires innovative solutions.\n- **Approach:** The authors propose a method that combines uncertainty-based stepsize adaptation and adversarial training, which addresses several limitations of vanilla MAML and other existing few-shot learning techniques. This method leverages ensemble methods and adversarial examples, providing a comprehensive solution that is both theoretically grounded and practically effective.\n\n#### Methodological Rigor\n- **Theoretical Framework:** The paper provides a thorough theoretical justification for their approach, including uncertainty-based stepsize adaptation and adversarial training. They clearly define key concepts and transformations, such as uncertainty-based stepsize adaptation and UFGSM, and explain their implementation details.\n- **Experimental Design:** The authors conduct extensive experiments across multiple datasets (miniImageNet, Birds, Signs, Flowers) and provide detailed analysis of the results. They use a broad range of base step sizes and metrics (Top-1, Top-40%), ensuring robust validation of their method\u2019s performance and robustness.\n- **Comprehensive Evaluation:** The paper evaluates the proposed method against a baseline (vanilla MAML) and other variants, demonstrating significant improvements in accuracy and robustness to the choice of base step sizes. This thorough comparison underscores the method's effectiveness.\n\n#### Significance of Contribution\n- **Impact on Field:** The proposed method offers a practical solution for overcoming the challenges of limited data availability in few-shot learning. By leveraging pretrained models and augmenting them with uncertainty-based techniques and adversarial examples, the method enhances the adaptability and robustness of these models.\n- **Broader Impact:** The ability to train models efficiently on small datasets and adapt them to unseen domains has implications for various applications, including medical diagnosis, autonomous systems, and industrial automation. The method could lead to more deployable and reliable AI systems in real-world scenarios.\n\n#### Clarity of Presentation\n- **Clarity of Exposition:** The paper is well-written and structured, making it easy to follow the logical flow of arguments and methodology. The abstract, introduction, and technical sections are coherent and well-organized.\n- **Consistency in Terminology:** The paper uses consistent terminology throughout and avoids jargon unnecessarily, making it accessible to researchers in the field of machine learning and computer vision.\n\n#### Alignment with Journal Scope\n- **Relevance to ICLR:** The topic of few-shot learning and meta-learning aligns closely with the interests of ICLR, which focuses on advances in deep learning and computational learning theory. The proposed method is innovative and addresses a critical challenge in few-shot learning, making it suitable for publication in ICLR.\n\n### Weaknesses\n- **Overfitting Concerns:** While the method shows promise, there is a concern about overfitting, especially since the model is being trained on a small number of support examples. Future work could address this by incorporating techniques like dropout or data augmentation.\n- **Complexity of Implementation:** The method involves multiple steps (ensemble training, uncertainty-based stepsize adaptation, adversarial training, etc.), which might make it harder to implement compared to simpler few-shot learning approaches.\n\nOverall, the paper presents a significant and innovative contribution to the field of few-shot learning, with strong methodological rigor and clear implications for practical applications. Given the novelty and potential impact, the paper is likely to be accepted by ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636611.4977467", "msecs": "497.0", "relativeCreated": "72190.8872127533", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Unified Discrete Diffusion for Simultaneous Vision-Language Generation\n\n# Abstract\nThe recently developed discrete diffusion model performs extraordinarily well in generation tasks, especially in the text-to-image task, showing great potential for modeling multimodal signals. In this paper, we leverage these properties and present a unified multimodal generation model, which can perform text-based, image-based, and even vision-language simultaneous generation using a single model. Specifically, we unify the discrete diffusion process for multimodal signals by proposing a unified Markov transition matrix and a unified objective. Moreover, we design a multimodal mutual attention module to highlight the inter-modal linkages, which is vital for multimodal generation. Extensive experiments indicate that our proposed method can perform comparably to the state-of-the-art solutions in various generation tasks.\n\n\n# 1 INTRODUCTION\n\n\n![Figure 1: Examples of various tasks supported by UniD3. The dark brown portions of the image and description represent the [MASK].](images/cc2b5f09ac9982cc9c03416dfc4b49f4a6f6d2c018ec7e9c0050376f60b90413.jpg)\n\n\nDiffusion models (Ho et al., 2020; Song et al., 2021b) have garnered significant interest on various high quality conditional image generation tasks, such as image super-resolution (Rombach et al., 2022), image inpainting (Lugmayr et al., 2022), image editing (Avrahami et al., 2022), image translation (Saharia et al., 2022a), among others. Concurrently, the Vector Quantized (VQ) models have also achieved rapid advances in image generations, especially on cross-modal tasks, examples include text-to-image (Kim et al., 2022), sketch-to-image (Esser et al., 2021b), image-to-video (Wu et al., 2021).\n\n\nDespite their success, all of these generation tasks are designed for only a single special modality, i.e. either modality translation or modality generation, with the help of a powerful diffusion model. In particular, the former modality translation translates the given conditional signals into the traget domain, while the latter modality generation only works for the unconditional image generation, e.g. image generation on CelebA Karras et al. (2017) or LSUN Yu et al. (2015) datasets. However, none of them consider to learn a join distribtuion for the mixture modality.\n\n\n![Figure 2: The pipeline of UniD3. With an offline model (red background part), the given inputs are represented by discrete token sequence in separate domain. The fusion embedding concatenates the tokens in different modal and embed them to the same space. The unified diffusion (in blue background) will construct the joint distribution of all modalities based on the fused embedding with a fixed unified Markov transition matrix.](images/897c71805f291805d2fee983dc2442c5735f174a67302da7f5091938a67de31c.jpg)\n\n\nHere, we take the use of discrete diffusion model into a new realm - multi-modality generation using a unified vision-language model. In contrast to the modality translation tasks outlined above, our multi-modality generative model does not require any conditional signals given in prior and is capable of simultaneously generating content pairs with the associated multi-modalities. UniD3, our new Unified Discrete Denoising Diffusion model, allows us to construct a joint vision-language probability distribution by mixing discrete image tokens with text tokens, leading to a capability of simultaneously generating cross-domain results. This is achieved by the two-stages framework, illustrated in Fig. 2, (1) An offline model to generate a compact yet expressive discrete representation for both images and texts via discrete VAE (dVAE) and Byte-Pair Encoding (BPE), respectively (the pink part in Fig. 2). (2) A novel unified discrete diffusion model to estimate the joint distribution of such latent visual and language codes (the cyan part in Fig. 2). Once trained, UniD3 can not only inherit the ability to manipulate the provided text or image, but is also able to unify the text and image generation, e.g., unconditional vision-language pairings generation, cross modal manipulation, text guided image completion, and image conditional text caption (Fig. 1 depicts the tasks that our model is capable of handling).\n\n\nBased on the empirical exploration, our model can achieve comparable image quality to the text condition in the unconditional case. In terms of image caption, our model is also comparable to some states of the art.\n\n\nIn summary, our key contributions include the following:\n\n- - We design a specific Markov transition matrix for our unified discrete denoising diffusion model, which leads to a sophisticated control of diffusion process, to estimate the joint distribution of language and image. The purposive design of transfer matrices based on task objectives and data properties is also pioneering for discrete diffusion models.\n\n- - We additionally propose mutual mechanism with fuse embedding to fulfil the objective of multi-modal integration. And we alter a unified objective function to offer the optimization with more concise restrictions.\n\n- - To the best of our knowledge, the UniD3 is the first work to solve both multi-modality generation and modality translation problems that is capable of handling the simultane\n\n\n![Figure 3: Illustration of transformer blocks with mutual attention. A unified transformer is composed of several blocks stacked on top of one another.](images/302a776957349843c03749a62a54d31e52492623a0fbf66b44d4a0ba153f0445.jpg)\n\n\nous unconditional vision-language generation and bi-directional vision-language synthesis tasks with a single model.\n\n\n# 2 PRELIMINARIES\n\n\nVector Quantised Model The Vector-Quantised Variational Auto Encoder (VQ-VAE) (Van Den Oord et al., 2017) learns to embed the high-dimensional data, e.g., image or audio, into a discrete representation. In particular, given a high dimensional data  $\\pmb{x} \\in \\mathbb{R}^{C \\times H \\times W}$ , the encoder  $E$  first converts it to the spatial latent features  $\\pmb{z} = \\{z_{i,j}\\} \\in \\mathbb{R}^{d \\times h \\times w}$ , and then transfer this continuous features into discrete space by looking up the closest features in the codebook  $\\mathcal{Z} = \\{z_k\\} \\in \\mathbb{R}^{K \\times d}$  to obtain the tokens  $\\pmb{z}_q$ :\n\n\n$$\n\n$$\n\\boldsymbol {z} _ {q} = \\operatorname {Q u a n t i s e} (\\boldsymbol {z}) = \\operatorname {Q u a n t i s e} (E (\\boldsymbol {x})) := \\arg \\min  _ {k} \\| z _ {i, j} - z _ {k} \\|, \\tag {1}\n$$\n\n$$\n\n\nwhere the dimensions  $h, w$  of latent feature  $\\mathbf{z}$  are substantially smaller than the original dimensions  $H, W$ . The reconstructions can be obtained through a decoder  $G$ :\n\n\n$$\n\n$$\n\\hat {\\boldsymbol {x}} = G \\left(\\boldsymbol {z} _ {q}\\right) = G (\\text {Q u a n t i s e} (E (\\boldsymbol {x}))). \\tag {2}\n$$\n\n$$\n\n\nRecently, there have been significant advances in learning more compact representation, yet higher-quality reconstruction, such as introducing new losses (Esser et al., 2021a), applying powerful backbone (Yu et al., 2021), using multiple channels representation (Lee et al., 2022) and spatially normalization (Zheng et al., 2022). However, in this paper, we focus on applying the technology in our novel UniD3, rather than exploring a new codebook learning approach.\n\n\nDiscrete Diffusion The discrete diffusion model was originally mentioned in (Sohl-Dickstein et al., 2015), with transitions converging to a binomial distribution. Subsequent work extended this to multinomial diffusion (Hoogeboom et al., 2021; Song et al., 2021a), while Austin et al. (2021) provided more options for transition matrices. Recent works integrated discrete diffusion models with VQ-VAE, allowing for high-quality image synthesis (Gu et al., 2022; Hu et al., 2022).\n\n\nHere we briefly describe the multinomial diffusion with the absorbing state, as employed in VQ-Diffusion (Gu et al., 2022). Besides  $K$  tokens from a discrete VAE, an additional [MASK] token is introduced. The forward process is defined as:\n\n\n$$\n\n$$\nq \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {t - 1}\\right) = \\operatorname {C a t} \\left(\\boldsymbol {x} _ {t}; \\boldsymbol {p} = \\boldsymbol {Q} _ {t} \\boldsymbol {x} _ {t - 1}\\right) = \\boldsymbol {x} _ {t} ^ {T} \\boldsymbol {Q} _ {t} \\boldsymbol {x} _ {t - 1}, \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\pmb{x}$  is a one-hot vector identifying the token index. Here  $[Q_t]_{i,j} = q(x_t = i|x_{t - 1} = j)\\in$ $\\mathbb{R}^{(K + 1)\\times (K + 1)}$  is the Markov transition matrix from  $t - 1$  to  $t$ , which can be expressed as:\n\n\n$$\n\n$$\n\\boldsymbol {Q} _ {[ t - 1 \\rightarrow t ]} = \\left[\\begin{array}{c c c c c}\\alpha_ {t} + \\beta_ {t}&\\beta_ {t}&\\beta_ {t}&\\dots&0\\\\\\beta_ {t}&\\alpha_ {t} + \\beta_ {t}&\\beta_ {t}&\\dots&0\\\\\\beta_ {t}&\\beta_ {t}&\\alpha_ {t} + \\beta_ {t}&\\dots&0\\\\\\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\\\gamma_ {t}&\\gamma_ {t}&\\gamma_ {t}&\\dots&1\\end{array}\\right], \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\alpha_{t} \\in [0,1]$  is the probability of retaining the token, and has a probability of  $\\gamma_{t}$  to be replaced by the [MASK] token, leaving a chance of  $\\beta_{t} = (1 - \\alpha_{t} - \\gamma_{t}) / K$  to be diffused.\n\n\nThe posterior of the diffusion process can be formulated as:\n\n\n$$\n\n$$\nq \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}, \\boldsymbol {x} _ {0}\\right) = \\frac {q \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {t - 1} , \\boldsymbol {x} _ {0}\\right) q \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {0}\\right)}{q \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {0}\\right)} = \\frac {\\left(\\boldsymbol {x} _ {t} ^ {T} \\boldsymbol {Q} _ {t} \\boldsymbol {x} _ {t - 1}\\right) \\left(\\boldsymbol {x} _ {t - 1} ^ {T} \\overline {{\\boldsymbol {Q}}} _ {t - 1} \\boldsymbol {x} _ {0}\\right)}{\\boldsymbol {x} _ {t} ^ {T} \\overline {{\\boldsymbol {Q}}} _ {t} \\boldsymbol {x} _ {0}}, \\tag {5}\n$$\n\n$$\n\n\ngiven  $\\overline{\\mathbf{Q}}_t = \\mathbf{Q}_t\\cdot \\cdot \\cdot \\mathbf{Q}_1$  , which can be calculated in closed form, and\n\n\n$$\n\n$$\nq \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {0}\\right) = \\operatorname {C a t} \\left(\\boldsymbol {x} _ {t}; \\boldsymbol {p} = \\overline {{\\boldsymbol {Q}}} _ {t} \\boldsymbol {x} _ {0}\\right) = \\boldsymbol {x} _ {t} ^ {T} \\overline {{\\boldsymbol {Q}}} _ {t} \\boldsymbol {x} _ {0}. \\tag {6}\n$$\n\n$$\n\n\nIn the reverse process, instead of explicitly predicting the posterior using a denoising neural network, the  $x_0$ -parameterisation might increase the stability and permit fast inference (skipping  $\\Delta t$  steps per iteration). The reverse transition with reparameterisation is given as:\n\n\n$$\n\n$$\np _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}\\right) \\propto \\sum_ {\\tilde {\\boldsymbol {x}} _ {0}} q \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}, \\tilde {\\boldsymbol {x}} _ {0}\\right) \\tilde {p _ {\\theta}} \\left(\\tilde {\\boldsymbol {x}} _ {0} \\mid \\boldsymbol {x} _ {t}\\right), \\tag {7}\n$$\n\n$$\n\n\nin which the neural network predicts the logits of the target data  $q(\\pmb{x}_0)$ .\n\n\n# 3 METHOD\n\n\nOur goal is to adapt the discrete diffusion model to learn the joint distribution of linguistic and visual features concurrently. First, we propose a transition matrix that allows the diffusion model to capture the implicit association between text and images. Second, we present a mutual attention transformer architecture with fuse embedding layer as the denoising function and a unified objective function, which fits our unified diffusion process objective and permits for more precise predictions.\n\n\nOur overall pipeline is illustrated in Fig. 2. Specifically, our solution begins by compressing the figures and texts into discrete token sequences using dVAE and BPE, respectively. A robust diffusion model with unified transition matrix is then constructed to fit the joint distribution across different modalities, which is further empowered by a transformer with a mutual attention mechanism.\n\n\n# 3.1 UNIFIED DIFFUSION PROCESS\n\n\nUnified Transition Matrix Discrete diffusion models with transition matrices can capture global links. The presence of a transition matrix determines the nature of the discrete diffusion model, which also provides us with more choices for token evolution. Thus we may wonder if it is feasible to design transition matrices that capture the global connections between various modalities.\n\n\nThe Markov transition matrix of the discrete diffusion model should satisfy the following requirements: 1. each column in  $Q_{t}$  should sum to one to conserve probability mass; 2. each column in the cumulative-product  $\\overline{Q}_{t}$  should converge to either a known stationary distribution or a learnt prior when  $t$  becomes large.\n\n\nOn the basis of these criteria, we construct a unified transition matrix  $[Q_t]_{i,j} = q(x_t = i|x_{t - 1} = j)$  capable of encapsulating discrete representations among various modalities. The following matrix  $Q_{t}\\in \\mathbb{R}^{(K + K^{*} + 1)\\times (K + K^{*} + 1)}$  illustrates a unified transition process with only text and image modalities:\n\n\n$$\n\n$$\n\\boldsymbol {Q} _ {t} = \\left[ \\begin{array}{c c c c c c c c c c c} \\alpha_ {t} + \\beta_ {t} & \\beta_ {t} & \\beta_ {t} & \\dots & \\beta_ {t} & 0 & 0 & 0 & \\dots & 0 & 0 \\\\ \\beta_ {t} & \\alpha_ {t} + \\beta_ {t} & \\beta_ {t} & \\dots & \\beta_ {t} & 0 & 0 & 0 & \\dots & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \\beta_ {t} & \\beta_ {t} & \\beta_ {t} & \\dots & \\alpha_ {t} + \\beta_ {t} & 0 & 0 & 0 & \\dots & 0 & 0 \\\\ \\hline 0 & 0 & 0 & \\dots & 0 & \\alpha_ {t} + \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\dots & \\beta_ {t} ^ {*} \\\\ 0 & 0 & 0 & \\dots & 0 & \\beta_ {t} ^ {*} & \\alpha_ {t} + \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\dots & \\beta_ {t} ^ {*} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\dots & 0 & \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\beta_ {t} ^ {*} & \\dots & \\alpha_ {t} + \\beta_ {t} ^ {*} \\\\ \\hline \\gamma_ {t} & \\gamma_ {t} & \\gamma_ {t} & \\dots & \\gamma_ {t} & \\gamma_ {t} & \\gamma_ {t} & \\gamma_ {t} & \\dots & \\gamma_ {t} \\\\ \\end{array} \\right],\n$$\n\n$$\n\n\nwhere  $\\alpha_{t} \\in [0,1]$  is the probability to keep this token,  $\\beta_{t}$  and  $\\beta_{t}^{*}$  are the probabilities of a token to be replaced by any other accessible tokens in different modality, and  $\\gamma_{t}$  is the absorbing probability.\n\n\nThe dimension of the matrix  $Q_{t}$  is  $(K + K^{*} + 1) \\times (K + K^{*} + 1)$ , where  $K$  and  $K^{*}$  are the number of states in different modals respectively, e.g.,  $K$  is the size from the codebook in discrete VAE and  $K^{*}$  is the dictionary size of BPE.\n\n\nThe matrix comprises five sections:\n\n- - The final row and column form a section associated with the transition of the absorbing state. Intuitively, if the token is [MASK] at  $t - 1$  step, then the token must be [MASK] at time  $t$ . Conversely, any other modal token has the equal possibility  $\\gamma_{t}$  of being diffused to [MASK].\n\n- - The remainder of the matrix is composed of four quadrants, the first and third of which are fully zeros. Specifically, these two sub-matrices prevent tokens transitioning from one modality to another. The form of the second and fourth quadrants closely resembles that for multinomial diffusion. Here, in addition to some probability of being converted into a [MASK] token, each token also has some chance of transiting to a different state within the same modality, or remaining unaltered.\n\n- - The dimensions of these four quadrants are not identical, which are  $K^{*} \\times K$ ,  $K \\times K$ ,  $K \\times K^{*}$  and  $K^{*} \\times K^{*}$ , respectively.\n\n\nIt is worth noting that  $\\alpha_{t}$  and  $\\gamma_{t}$  are the same in all modalities, whereas  $\\beta_{t}$  varies based on the number of states in different modalities. Mathematically,  $\\beta_{t} = (1 - \\alpha_{t} - \\gamma_{t}) / K$  and  $\\beta_{t}^{*} = (1 - \\alpha_{t} - \\gamma_{t}) / K^{*}$ .\n\n\nThe sum of each column in this transition matrix is one to preserve probability mass, and also all the mass of the stationary distribution falls on the [MASK] token, which satisfies the prerequisite for a discrete diffusion model transition matrix.\n\n\nThe computation of  $\\overline{\\mathbf{Q}}_t\\mathbf{x}_0$ , needed for  $q(\\mathbf{x}_t|\\mathbf{x}_0)$  in Eq. 6, can be efficiently obtained in closed form:\n\n\n$$\n\n$$\n\\overline {{Q}} _ {t} \\boldsymbol {x} _ {0} = \\overline {{\\alpha}} _ {t} \\boldsymbol {x} _ {0} + \\left[ \\overline {{\\gamma}} _ {t} - \\mathbb {1} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} - \\mathbb {1} ^ {*} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} ^ {*} \\right] \\boldsymbol {x} _ {[ \\mathbb {M} ]} + \\mathbb {1} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} + \\mathbb {1} ^ {*} (\\boldsymbol {x} _ {0}) \\overline {{\\beta}} _ {t} ^ {*},\n$$\n\n$$\n\n\nwhere  $\\mathbb{1}(\\pmb{x}_0) = \\left\\{ \\begin{array}{ll} 1 & \\text{if argmax } \\pmb{x}_0 \\in [0, K), \\\\ 0 & \\text{otherwise.} \\end{array} \\right.$ ,  $\\mathbb{1}^*(\\pmb{x}_0) = \\left\\{ \\begin{array}{ll} 1 & \\text{if argmax } \\pmb{x}_0 \\in [K, K + K^*), \\\\ 0 & \\text{otherwise.} \\end{array} \\right.$ ,\n\n\n$$\n\n$$\n\\boldsymbol {x} _ {[ \\mathbb {M} ]} = \\boldsymbol {x} \\leftarrow \\operatorname {a r g m a x} \\boldsymbol {x} = K + K ^ {*} \\text {a n d} \\bar {\\alpha} _ {t}, \\bar {\\beta} _ {t}, \\bar {\\beta} _ {t} ^ {*}, \\bar {\\gamma} _ {t} \\text {a r e t h e c o r r e s p o n d i n g c u m u l a t i v e p r o d u c t .} \\tag {9}\n$$\n\n$$\n\n\nThe detailed proof is provided in Appendix E.\n\n\nUnified Objective For conditional generation tasks such as text-to-image generation, the goal is to maximize  $p(\\pmb{x}|\\pmb{y})$  by finding  $\\pmb{x}$ , where  $\\pmb{y}$  is the given text condition. In our task, it can be approximated that the model try to maximize the joint distribution  $p(\\pmb{x},\\pmb{y})$  simultaneously. In practice, we minimize the Kullback-Leibler divergence between  $q(\\pmb{x}_{t - 1}|\\pmb{x}_t,\\pmb{x}_0)$  and  $p_{\\theta}(\\pmb{x}_{t - 1}|\\pmb{x}_t)$  in both image and text directions, as shown in the following:\n\n\n$$\n\n$$\n\\mathcal {L} _ {0} = - \\mathbb {E} _ {q \\left(\\boldsymbol {x} _ {1} \\mid \\boldsymbol {x} _ {0}\\right)} \\left[ \\log p _ {\\theta} \\left(\\boldsymbol {x} _ {0} ^ {i m g} \\mid \\boldsymbol {x} _ {1}, \\boldsymbol {x} _ {0} ^ {t x t}\\right) + \\log p _ {\\theta} \\left(\\boldsymbol {x} _ {0} ^ {t x t} \\mid \\boldsymbol {x} _ {1}, \\boldsymbol {x} _ {0} ^ {i m g}\\right) \\right], \\tag {10}\n$$\n\n$$\n\n\n$$\n\n$$\n\\mathcal {L} _ {t - 1} = \\mathbb {E} _ {q \\left(\\boldsymbol {x} _ {t} \\mid \\boldsymbol {x} _ {0}\\right)} \\left[ D _ {\\mathrm {K L}} \\left(q \\left(\\boldsymbol {x} _ {t - 1} \\mid \\boldsymbol {x} _ {t}, \\boldsymbol {x} _ {0}\\right) \\| \\left[ p _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {i m g} \\mid \\boldsymbol {x} _ {t}\\right); p _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {t x t} \\mid \\boldsymbol {x} _ {t}\\right) \\right]\\right) \\right], \\tag {11}\n$$\n\n$$\n\n\nwhere  $p_{\\theta}(\\boldsymbol{x}_{t-1})$  is the integration of the logits  $p_{\\theta}(\\boldsymbol{x}_{t-1}^{img})$  and  $p_{\\theta}(\\boldsymbol{x}_{t-1}^{tax})$  from separate modal, and they can be obtained with  $\\boldsymbol{x}_0$ -parameterisation as following:\n\n\n$$\n\n$$\np _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {i m g} \\mid \\boldsymbol {x} _ {t}\\right) \\propto \\sum_ {\\tilde {\\boldsymbol {x}} _ {0} ^ {i m g}} q \\left(\\boldsymbol {x} _ {t - 1} ^ {i m g} \\mid \\boldsymbol {x} _ {t}, \\tilde {\\boldsymbol {x}} _ {0} ^ {i m g}, \\boldsymbol {x} _ {0} ^ {t x t}\\right) \\tilde {p _ {\\theta}} \\left(\\tilde {\\boldsymbol {x}} _ {0} ^ {i m g} \\mid \\boldsymbol {x} _ {t}\\right), \\tag {12}\n$$\n\n$$\n\n\n$$\n\n$$\np _ {\\theta} \\left(\\boldsymbol {x} _ {t - 1} ^ {t x t} \\mid \\boldsymbol {x} _ {t}\\right) \\propto \\sum_ {\\tilde {\\boldsymbol {x}} _ {0} ^ {t x t}} q \\left(\\boldsymbol {x} _ {t - 1} ^ {t x t} \\mid \\boldsymbol {x} _ {t}, \\tilde {\\boldsymbol {x}} _ {0} ^ {t x t}, \\boldsymbol {x} _ {0} ^ {i m g}\\right) \\tilde {p} _ {\\theta} \\left(\\tilde {\\boldsymbol {x}} _ {0} ^ {t x t} \\mid \\boldsymbol {x} _ {t}\\right). \\tag {13}\n$$\n\n$$\n\n\nDue to the  $x_0$ -parameterisation, our model is also capable of achieving fast sampling by increasing the step size. And the last term of the variational lower bound loss  $\\mathcal{L}_T$  is a constant and can be ignored during the training:\n\n\n$$\n\n$$\n\\mathcal {L} _ {T} = D _ {\\mathrm {K L}} \\left(q \\left(\\boldsymbol {x} _ {T} \\mid \\boldsymbol {x} _ {0}\\right) \\| p \\left(\\boldsymbol {x} _ {T}\\right)\\right), \\tag {14}\n$$\n\n$$\n\n\nas the prior  $p(\\boldsymbol{x}_T)$  is fixed with the unified transition matrix:\n\n\n$$\n\n$$\np \\left(\\boldsymbol {x} _ {T}\\right) = \\left[ \\bar {\\beta} _ {T}, \\bar {\\beta} _ {T}, \\dots , \\bar {\\beta} _ {T} ^ {*}, \\bar {\\beta} _ {T} ^ {*}, \\dots , \\bar {\\gamma} _ {T} \\right]. \\tag {15}\n$$\n\n$$\n\n\nThe full expression of the loss function can be found in Appendix C.\n\n\n# 3.2 DENOISING FUNCTION FOR MULTIMODAL\n\n\nMutual Attention As indicated by Eqs. 12 & 13, the neural network is responsible for prediction of the distribution  $\\tilde{p}_{\\theta}(\\tilde{\\boldsymbol{x}}_0|\\boldsymbol{x}_t)$ . However, the input to the neural network covers all modalities, and a simple self-attention mechanism can scarcely highlight the inter-modal linkages. In addition, the distributions predicted by the neural network need to be decoupled according to the various modalities throughout the reparameterisation. In other words, the network predictions should be expressed in terms of different modalities, e.g.,  $\\tilde{p}_{\\theta}(\\tilde{\\boldsymbol{x}}_0^{img}|\\boldsymbol{x}_t)$  and  $\\tilde{p}_{\\theta}(\\tilde{\\boldsymbol{x}}_0^{txt}|\\boldsymbol{x}_t)$ , with interconnections.\n\n\nTherefore, we propose the mutual attention mechanism and construct the unified transformer as shown in Fig. 3. The unified transformer contains several transformer blocks, each of which consists of one self-attention and two parallel mutual attention operations, and one feed-forward module. Each block receives a sequence of mixed-modal tokens as input that traverses a layer of self-attention to capture the inherent connection within the modalities. Afterwards, the hidden tokens are decoupled according to the location of the different modalities and fed to the corresponding mutual attention layers. Mutual attention is a modified version of the cross-attention layer, with the conditional inputs to cross-attention being maintained constant while the inputs to mutual attention are derived from hidden features. Next, the outputs from the various mutual attention are concatenated into one mixed-modal token sequence for transmission to the next block. At the end of the unified transformer, we layer-normalise the sequence of tokens from the blocks, which is then decoupled to different predictive distributions by fully-connected layers.\n\n\nIn the conventional multi-model generative model, conditional modality  $C$  is given and fixed. We have the Cross attention block to inject the conditional information  $C$  into the target modality  $X$ :\n\n\n$$\n\n$$\n\\operatorname {C A} (X, C) = \\operatorname {A t t n} (X, C; W) = \\operatorname {s o f t m a x} \\left(\\frac {Q K ^ {T}}{\\sqrt {2}}\\right) V, \\tag {16}\n$$\n\n$$\n\n\nwhere  $Q = W_{Q}X, K = W_{K}C \\& V = W_{V}C$ , and  $W$  is the learnable parameters. Noted that the cross attention  $CA(X,C)$  could convert to self-attention  $SA(X)$  when  $X = C$ .\n\n\nIn our model, each modality may become a component of what needs to be generated. Therefore, we propose mutual attention to enable tokens of different modalities in a sequence to be conditional on each other, allowing the capture of the relationships between the various modalities. Mathematically, our mutual attention can be expressed as\n\n\n$$\n\n$$\n\\mathrm {M A} (T _ {i}, T _ {j}) = \\operatorname {A t t n} (T _ {i}, T _ {j}; W) = \\operatorname {s o f t m a x} (\\frac {Q _ {i} K _ {j} ^ {T}}{\\sqrt {2}}) V _ {j},\n$$\n\n$$\n\n\nwhere  $Q_{i} = W_{Q}T_{i}, K = W_{K}T_{j} \\& V = W_{V}T_{j}$ , and  $T_{i} \\& T_{j}$  are the tokens in a different modality, e.g. image and text. The Unified block contains a self-attention and several mutual attention in parallel. Given a hidden token vector  $H' = [H_i', H_j']$ , the whole pipeline of Unified block is\n\n\n$$\n\n$$\n\\begin{array}{l} 1. T ^ {\\prime} = S A \\left(H ^ {\\prime}\\right) \\\\ 2. D e c o u p l e: T ^ {\\prime} \\rightarrow T _ {i} ^ {\\prime}, T _ {j} ^ {\\prime} \\\\ 3. \\left\\{ \\begin{array}{l} T _ {i} = \\operatorname {M A} \\left(T _ {i} ^ {\\prime}, T _ {j} ^ {\\prime}\\right) \\\\ T _ {j} = \\operatorname {M A} \\left(T _ {j} ^ {\\prime}, T _ {i} ^ {\\prime}\\right) \\end{array} \\right. \\\\ 4. C o u p l e: T \\leftarrow T _ {i}, T _ {j} \\\\ \\end{array}\n$$\n\n$$\n\n\n![this bird has a white belly, a spotted breast, a short tail, and pointed bill.](images/be3ba2e73ec33a7e8f0c7659132ab3d83c39a3321a6ccb02bc2238cac405112d.jpg)\n\n\n![a white bird with a black crown and a long beak. this bird is all black and has a small yellow eyes and white beak.](images/522ae7abc6e60b30bd016da5a1f6e902067ea2c124a985b3257b0920ddec7cae.jpg)\n\n\n![a clean and tide kitchen with wooden cabinets for use.](images/5ce07f9fef59c0e50194be76fbb3baed0fdcb7fb136572eb3c0ec9c755dedc11.jpg)\n\n\n![a tall giraffe is standing in a forest.](images/865e92291e7a191fb329a9789ce636bce0a2c0ff03f05db3160b52fd080415b3.jpg)\n\n\n![a tall building with a park area.](images/72be18aa80d83d42df4eccb6d7f61a4c8bcde972dadcf62f5180c0b95b49eca7.jpg)\n\n\n![](images/527131bb59ea1574d8f2ab9b790cbbb8f8b2fae64cdb22b79d2355378e8b0dc3.jpg)\n\n\n![this bird has wings that are grey and has a white belly. the small bird is mostly brown and has a light red head. Figure 4: Generated vision-language Pairs from CUB-200 and MSCOCO. Both the image and caption are generated simultaneously. The quality of the created photos and text is comprehensible, and there is a correlation between the descriptions and the visuals.](images/95923822c90433451db30ebe5a577a9108e8f4e50a38eff6f2da664498b46b10.jpg)\n\n\n![this bird has wings that are black and has a yellow and black belly.](images/e057321fedae1534a673629a4c1dc5ce5cc5cfa38484efbbcb7a1f74f551300f.jpg)\n\n\n![colorful flowers in an orange vase on the window.](images/b06ee4ec99a9307d24bc28278d4fe8f9ecf1809cf7ca185efbcc332cfadbb331.jpg)\n\n\n![there is a bench by the trees in the seaside park.](images/c7bbed106ab869a47e6ef9a61541aa80f7844489a65a671a9da0814a9351068f.jpg)\n\n\n![a window on the front of a brick building.](images/ff73cc3f4fd15a16f9cce2924de2ae597fcdb6ac1632b0ce25c4f879291152aa.jpg)\n\n\n![](images/b3096ae11cbdf82c0e3fc7712ceb28743f0a864e9e13248109d4e74336bad61d.jpg)\n\n\nFused Embedding In addition, discrete tokens of various modalities should be embedded in the same space. Considering that two modalities have  $K$  and  $K^{*}$  states respectively, we firstly create an embedding layer with size  $K + K^{*} + 1$ , with the additional state for the [MASK] token. We then create a learnable spatial position encoding for the image modality and a learnable sequence position encoding for the text modality. The final fused embedding is obtained by adding the embedded vectors to the positional encoding of their associated modalities.\n\n\n# 4 EXPERIMENTS\n\n\nThe description of the datasets and model experimental details can be found in Appendix B.\n\n\n# 4.1 QUANTITATIVE RESULTS\n\n\nIn the unconditional scenario, our model is able to sample from all modalities while maintaining the relationship between them. For conditional generation, we conducted experiments on text-based image generation and image-based caption generation, respectively.\n\n\nMetrics We evaluate the multimodal capacity of our model in three distinct areas: Image sampling quality, text sampling quality and the similarity between sampled image and text. Fr\u00e9chet inception distance (FID) (Heusel et al., 2017) and Inception Score (IS) (Salimans et al., 2016) are used to evaluate the realism and variety of the produced images. The BLEU-4, METEOR and SPICE are used to assess the quality of a picture caption. And CLIP (Radford et al., 2021) is used to identify the similarity between visuals and text.\n\n\nImage Quality We quantitatively compare our unconditional and text-conditional results with several state-of-the-art text-to-image and bidirectional conditional generation methods, including GAN-based methods, diffusion-based methods and autoregressive methods. The comparison results are given in Table 1, in which  $\\text{Pair}$  means the unconditional generated vision-language pairings and  $T2I$  represents the text-to-image generation.\n\n\nThe first section of the table solely covers modality translation methods. In the second part, these models are claimed to be endowed with cross modal capabilities. However, such methods are not able to achieve simultaneous vision-language pair generation and necessitate given signals. For a fair\n\n\n![Figure 5: Presentation of the results of cross modal vision and language manipulation and infilling. The dark brown portions of the image and description represent the [MASK], while the strike-through represent the caption manipulation. Image and text compliment each other simultaneously.](images/0a15e5ddbaeffe1921a7d22d22d77a10c46d0440e523142f380ea33d09fe0a93.jpg)\n\n\ncomparison, we exhibit the quantitative generative results based on various modalities individually. We provide more experimental results for text conditional generation in Appendix G.2\n\n\n<table><tr><td rowspan=\"2\"></td><td colspan=\"2\">CUB</td><td colspan=\"2\">MSCOCO</td></tr><tr><td>FID (\u2193)</td><td>IS (\u2191)</td><td>FID (\u2193)</td><td>IS (\u2191)</td></tr><tr><td>DM-GAN (Zhu et al., 2019)</td><td>16.09</td><td>4.75</td><td>32.64</td><td>27.3</td></tr><tr><td>DF-GAN (Tao et al., 2022)</td><td>14.81</td><td>5.1</td><td>21.42</td><td>-</td></tr><tr><td>VQ-Diffusion (Gu et al., 2022)</td><td>11.94</td><td>-</td><td>19.75</td><td>-</td></tr><tr><td>LDM w/o G (Rombach et al., 2022)</td><td>-</td><td>-</td><td>23.31</td><td>20.23</td></tr><tr><td>LAFITE(LF) (Zhou et al., 2022)</td><td>27.53</td><td>4.32</td><td>18.04</td><td>27.2</td></tr><tr><td>Unifying (Huang et al., 2021)</td><td>-</td><td>-</td><td>29.9</td><td>-</td></tr><tr><td>L-Verse (Kim et al., 2022)</td><td>-</td><td>-</td><td>45.8</td><td>-</td></tr><tr><td>OFA (Wang et al., 2022)</td><td>-</td><td>-</td><td>10.5</td><td>31.1</td></tr><tr><td>Ours(Pair)</td><td>17.38</td><td>6.11</td><td>28.63</td><td>24.44</td></tr><tr><td>Ours(T2I)</td><td>16.19</td><td>6.02</td><td>25.11</td><td>21.03</td></tr></table>\n\n\nTable 1: Evaluation of image synthesis on the CUB-200-2011 and MS-COCO dataset. The first section is the text-conditioned results while the model in second part includes multimodal solution.\n\n\n<table><tr><td></td><td>B-4 (\u2191)</td><td>M (\u2191)</td><td>S (\u2191)</td></tr><tr><td>BUTD (Anderson et al., 2018)</td><td>36.3</td><td>27.7</td><td>21.4</td></tr><tr><td>ORT (Herdade et al., 2019)</td><td>38.6</td><td>28.7</td><td>22.6</td></tr><tr><td>AoA (Huang et al., 2019)</td><td>38.9</td><td>29.3</td><td>-</td></tr><tr><td>X-LAN (Pan et al., 2020)</td><td>39.7</td><td>29.5</td><td>23.4</td></tr><tr><td>SimVLM (Wang et al., 2021)</td><td>40.6</td><td>33.7</td><td>25.4</td></tr><tr><td>OSCAR (Li et al., 2020)</td><td>40.5</td><td>29.7</td><td>22.8</td></tr><tr><td>Unifying (Huang et al., 2021)</td><td>37.3</td><td>28.2</td><td>21.9</td></tr><tr><td>L-verse (Kim et al., 2022)</td><td>39.9</td><td>31.4</td><td>23.3</td></tr><tr><td>OFA (Wang et al., 2022)</td><td>41.0</td><td>30.9</td><td>24.2</td></tr><tr><td>Ours(I2T)</td><td>39.6</td><td>29.3</td><td>23.4</td></tr></table>\n\n\nTable 2: Evaluation of image caption on the MSCOCO Karpathy split. The first section is the text-conditioned results while the model in second part includes multimodal solution.\n\n\nText Quality In the case of image captions, we compared several other text description methods, the details are shown in Table 2, where  $I2T$  is the image-based text-generation task. Similarly, the first part of the table has pure image caption solutions and the bi-directional methods are demonstrated in the second part. Some results for image caption tasks are provided in Appendix G.1\n\n\nSimilarity In the multimodal generation phase, all the other methods for comparison need the input of at least one specified modality; instead our method generates both visual and verbal features. The generated vision-language pairings are shown in Fig. 4 while more pair samples are provided in Appendix G.4. In this experiment, we use CLIP scores to evaluate the similarity between our generated visual and verbal modalities, while generated images with given texts were used for other methods. The comparison results are given in Table 3, where a higher correlation between the text/image samples of our model can be found.\n\n\n# 4.2 CROSS MODAL MANIPULATION\n\n\nTo demonstrate the modification potential of our vision-language federation, we present the outcomes of picture alteration with description modification in Fig. 5 and more samples can be found in Appendix G.3. In this experiment, we obscured a portion of the image and modified the text description as well, the tokens in the masked region are merely designated as [MASK] tokens. We anticipate that the model will complete the image based on the amended text, in conjunction with the description based on the visible image region. Depending on the unmasked images, our model may supplement the caption that is derived from a portion of the image. Moreover, based on the amended description, the model may enhance the appearance of the masked component. These findings indicate that our model is capable of modifying text and pictures in both ways, in addition to jointly generating text and images.\n\n\n# 4.3 ABLATION STUDY\n\n\nIn the ablation test, we utilized a rapid sampling scheme, with an interval between each step of 10. First, we evaluated the outcomes without mutual attention, where we replaced all mutual attention in the model structure with causal self-attention. In addition, we evaluated the performance without the unified transition matrix, we employed a matrix similar to Equation 4, with the out-of-bound outputs substituted by the minimal ordinal number of the corresponding modal.\n\n\n<table><tr><td>(\u2191)</td><td>CUB</td><td>COCO</td></tr><tr><td>Eval set</td><td>0.286</td><td>0.306</td></tr><tr><td>VQ-Diff (Gu et al., 2022)</td><td>-</td><td>0.257</td></tr><tr><td>iVQ-Diff (Tang et al., 2022)</td><td>-</td><td>0.304</td></tr><tr><td>Unifying (Huang et al., 2021)</td><td>-</td><td>0.309</td></tr><tr><td>OFA (Wang et al., 2022)</td><td>-</td><td>0.344</td></tr><tr><td>Ours(Pair)</td><td>0.280</td><td>0.301</td></tr><tr><td>Ours(T2I)</td><td>0.293</td><td>0.306</td></tr><tr><td>Ours(I2T)</td><td>0.302</td><td>0.312</td></tr></table>\n\n\nTable 3: The CLIP similarity between the generated captions and images.\n\n\n<table><tr><td>(\u2193)</td><td>FID</td></tr><tr><td>Ours</td><td>17.38</td></tr><tr><td>w/ Fast Sampling \u0394 t = 10</td><td>20.96</td></tr><tr><td>w/o Mutual Attention</td><td>23.19</td></tr><tr><td>w/o Unified Transition Matrix</td><td>32.63</td></tr><tr><td>Ours (T2I)</td><td>16.19</td></tr><tr><td>w/ Fast Sampling \u0394 t = 10</td><td>18.33</td></tr><tr><td>w/o Mutual Attention</td><td>21.65</td></tr><tr><td>w/o Unified Transition Matrix</td><td>28.93</td></tr></table>\n\n\nTable 4: Ablations studies. The experiments are conducted on the CUB-200.\n\n\n# 5 RELATED WORK\n\n\nDenoising Diffusion Model Sohl-Dickstein et al. (2015) initially presented the diffusion model, which consists of a base Gaussian diffusion and a basic binomial diffusion. Ho et al. (2020) recently reigned interest in the extraordinary generating capacity of diffusion models. This is extended in subsequent research to enhance quality with the classifier guidance (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021) and speed of sampling (Song et al., 2021a; Salimans & Ho, 2021). In addition to continuous diffusion models that have received the significant attention, there is also substantial research based on discrete diffusion models (Hoogeboom et al., 2021; Austin et al., 2021) for text generation or image segmentation. With the help of VQ-VAE, Esser et al. (2021a); Hu et al. (2022); Gu et al. (2022); Tang et al. (2022) discrete diffusion models have become capable of generating high quality images.\n\n\nModality Translation Text-to-image generation is one of the key aspects of modality translation tasks. On simpler datasets, traditional GAN-based models (Xu et al., 2018; Zhu et al., 2019) may create high-quality pictures based on text hints, but face difficulties on more complex datasets. As a result of the emergence of transformers employing the attention mechanism (Vaswani et al., 2017), conventional autoregressive models (Van den Oord et al., 2016) have gained more powerful generating capacities as ART (Auto-regressive Transformers) (Ramesh et al., 2021; Esser et al., 2021b; Yu et al., 2022). Other types of modality translation models involve the addition of prospective modalities as criteria for improved generation outcomes (Gafni et al., 2022). There are also some works that focus on addressing bidirectional generation between different modalities (Kim et al., 2022; Huang et al., 2021; Wang et al., 2022); however, these solutions are task-specific, necessitating different input formats and specific models with distinct modalities.\n\n\n# 6 CONCLUSION\n\n\nThis work presents UniD3, a novel unified framework for multi-modality generation tasks. We firstly introduced a unified transition matrix for the diffusion process which permits the states of fused tokens to be corrupted in a stable way. We demonstrated that designing transition matrices based on the objective and data characteristics is beneficial for discrete diffusion models. To capture the connection from the various modalities, we suggested a mutual attention with fuse embedding for noiseless state recovery. Our method is also generic and competent to perform modality translation tasks for the specified modality. The specific designs of transition matrix are intriguing for further exploring, as well as the performance of more modalities is also one of the future directions.\n\n\n# 7 ETHICS\n\n\nIn this work, we propose UniD3, a new method for multimodal multi-task generation based on the discrete denoising diffusion model. All datasets evaluated in our experiments are open source, publicly accessible, and used with permission.\n\n\nSimilar to other generation algorithms, our method has both beneficial and negative societal effects, depending on its application and usage.\n\n- - Positively, UniD3 may explore the limits of deep learning generation through a unique pattern of graphic pair generation, while letting the typical user's imagination run wild and decreasing the entrance barrier for visual content production.\n\n- - On the other hand, UniD3 may be used to make modified photographs of great quality that are difficult to discern from the originals. Ways to fool humans and propagate falsehoods that are malicious. UniD3 can be abused by nefarious users, which may have serious repercussions.\n\n- - In our code distribution. We shall outline the permitted uses of our system and give the corresponding licences.\n\n\nHowever, we observe that the present discriminators used to distinguish generated images are ineffective at identifying images generated by diffusion models. There is still the necessity for more exploration to discern between authentic and fake images.\n\n\n# REFERENCES\n\n- Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6077-6086, 2018.\n\n- Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981-17993, 2021.\n\n- Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18208-18218, 2022.\n\n- Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780-8794, 2021.\n\n- Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis. Advances in Neural Information Processing Systems, 34, 2021a.\n\n- Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873-12883, 2021b.\n\n- Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203.13131, 2022.\n\n- Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696-10706, 2022.\n\n- Simao Herdade, Armin Kappeler, Kofi Boakye, and Joao Soares. Image captioning: Transforming objects into words. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\n- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020.\n\n- Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454-12465, 2021.\n\n- Minghui Hu, Yujie Wang, Tat-Jen Cham, Jianfei Yang, and Ponnuthurai N Suganthan. Global context with discrete diffusion in vector quantised modelling for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11502-11511, 2022.\n\n- Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. Attention on attention for image captioning. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4634-4643, 2019.\n\n- Yupan Huang, Hongwei Xue, Bei Liu, and Yutong Lu. Unifying multimodal transformer for bidirectional image and text generation. In Proceedings of the 29th ACM International Conference on Multimedia, pp. 1138-1147, 2021.\n\n- Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.\n\n- Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, and Kyunghoon Bae. L-verse: Bidirectional generation between image and text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16526-16536, 2022.\n\n- Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, et al. Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages, 2(3):18, 2017.\n\n- Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11523-11532, 2022.\n\n- Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pp. 121-137. Springer, 2020.\n\n- Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr D\u00f3ll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740-755. Springer, 2014.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.\n\n- Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11461-11471, 2022.\n\n- Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pp. 8162-8171. PMLR, 2021.\n\n- Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks for image captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10971-10980, 2020.\n\n- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. PMLR, 2021.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.\n\n- Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video transformer. In VISIGRAPP (5: VISAPP), 2021.\n\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821-8831. PMLR, 2021.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684-10695, 2022.\n\n- Chitwan Sahara, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pp. 1-10, 2022a.\n\n- Chitwan Sahara, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b.\n\n- Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2021.\n\n- Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.\n\n- Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256-2265. PMLR, 2015.\n\n- Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021a.\n\n- Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021b.\n\n- Zhicong Tang, Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen. Improved vector quantized diffusion models. arXiv preprint arXiv:2205.16007, 2022.\n\n- Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A simple and effective baseline for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16515-16525, 2022.\n\n- Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelCNN decoders. Advances in neural information processing systems, 29, 2016.\n\n- Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\n- Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. California Institute of Technology, 2011.\n\n- Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pp. 23318-23340. PMLR, 2022.\n\n- Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In International Conference on Learning Representations, 2021.\n\n- Wansen Wu, Tao Chang, and Xinmeng Li. Visual-and-language navigation: A survey and taxonomy. arXiv preprint arXiv:2108.11544, 2021.\n\n- Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attingan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1316-1324, 2018.\n\n- Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n\n- Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In International Conference on Learning Representations, 2021.\n\n- Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\n\n- Chuanxia Zheng, Long Tung Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35, 2022.\n\n- Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17907-17917, 2022.\n\n- Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5802-5810, 2019.\n\nResponse: Given the complexity and novelty of the proposed model, I would predict \\(\\boxed{\\text{Accept}}\\) for this paper at ICLR. \n\n### Justification:\n**Originality:** \nThe paper introduces a novel unified discrete diffusion model that addresses the challenge of multimodal generation by integrating text and image modalities in a single framework. This is an innovative approach that builds upon existing discrete diffusion models and extends their capabilities to handle both modalities simultaneously.\n\n**Methodological Rigor:**\nThe authors meticulously detail the design of their unified transition matrix and mutual attention mechanism, providing a clear theoretical foundation for their approach. They also provide extensive experimental validation and comparisons against existing methods, demonstrating the effectiveness of their approach.\n\n**Significance of Contribution:**\nThe paper makes significant contributions by presenting a method that can handle both text-based and image-based generation tasks simultaneously, which is a critical advancement in multimodal AI. It opens up possibilities for applications like photo editing, caption generation, and interactive image/text synthesis.\n\n**Clarity of Presentation:**\nThe paper is well-written and structured, making it easy to follow the progression of ideas and methodologies. The inclusion of detailed mathematical derivations and experimental setups enhances the credibility of the claims.\n\n**Alignment with Journal Scope:**\nICLR focuses on advancements in machine learning, particularly those involving deep learning and generative models. The proposed model, which integrates text and image modalities using a unified diffusion process, aligns well with ICLR's scope and mission.\n\n### Weaknesses:\nWhile the paper is strong, there are a few areas where improvements could be made:\n- **Comprehensive Evaluation:** While the experiments cover a wide range of metrics (FID, IS, BLEU, etc.), a more comprehensive evaluation could include additional qualitative analysis, such as human evaluation of the generated visuals and text coherence.\n- **Detailed Analysis:** The paper could benefit from a more thorough discussion of the limitations and potential pitfalls of the model, especially regarding scalability and interpretability.\n- **Code Availability:** Although the authors mention code availability in their ethics section, providing a clear and accessible code repository would strengthen the paper.\n\nOverall, the paper stands out due to its innovative approach and strong experimental results, making it a strong candidate for acceptance at ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636619.1560087", "msecs": "156.0", "relativeCreated": "79849.14922714233", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Optimal Transport for Long-Tailed Recognition with Learnable Cost Matrix\n\n# Abstract\nIt is attracting attention to the long-tailed recognition problem, a burning issue that has become very popular recently. Distinctive from conventional recognition is that it posits that the allocation of the training set is supremely distorted. Predictably, it will pose challenges to the generalisation behaviour of the model. Approaches to these challenges revolve into two groups: firstly, training-aware methods, with the aim of enhancing the generalisability of the model by exploiting its potential in the training period; and secondly, post-hoc correction, liberally coupled with training-aware methods, which is intended to refine the predictions to the extent possible in the post-processing stage, offering the advantages of simplicity and effectiveness. This paper introduces an alternative direction to do the post-hoc correction, which goes beyond the statistical methods. Mathematically, we approach this issue from the perspective of optimal transport (OT), yet, choosing the exact cost matrix when applying OT is challenging and requires expert knowledge of various tasks. To overcome this limitation, we propose to employ linear mapping to learn the cost matrix without necessary configurations adaptively. Testing our methods in practice, along with high efficiency and excellent performance, our method surpasses all previous methods and has the best performance to date.\n\n\n# 1 INTRODUCTION\n\n\nClassification problems in the real world are generally challenged by the long-tailed label distribution, i.e., having a small number of samples for a majority of labels, and a dominant number of samples for a minority of labels (Van Horn & Perona, 2017; Buda et al., 2018; Liu et al., 2019) on the training data set. It is also known as imbalanced recognition, which has been widely studied in the past decades (Cardie & Howe, 1997; Chawla et al., 2002; Qiao & Liu, 2009; Cui et al., 2019). These distribution biases pose a significant challenge to predictive modeling; conceivably, models often suffer from poor generalisation and undesirable estimation bias (Cao et al., 2019; Kang et al., 2019; Zhou et al., 2020).\n\n\nRecently, a renewed interest in the problem of long-tail recognition has emerged following the context of neural networks, as numerous publications in the literature endeavour to resolve the problem albeit in different ways including decouple (Kang et al., 2019), meta-learning (Ren et al., 2020; Wang et al., 2020a; Li et al., 2021), post-hoc correction (Tang et al., 2020; Hong et al., 2020), etc (Liu et al., 2019; Cao et al., 2019; Tang et al., 2020). One of the representative methods of post-hoc correction, Logit Adjustment Menon et al. (2020), provides a statistical correction to the prediction, receiving widespread attention for its simplicity and validity, but the downside is that its optimisation is conducted on individual samples, without any indication that rectified marginal distribution satisfies the desired distribution we are striving for.\n\n\nFiguring out how to solve the above problem, our explicit modeling of the problem mathematically turns into an equational constraint, meanwhile to minimise the difference between refined distribution and the original one, this minimisation is motivated upon the inner-product similarity. A little further, the resulting problem can be linked to OT. Drawing on this linkage, we develop it further by proposing a linear mapping to automatically learn cost matrix, thereby circumventing the requirement for expert knowledge to configure this matrix. In short, we contribute as follows:\n\n\n- We propose an alternative direction based on convex optimisation to do post-hoc correction, which goes beyond previous direction from the statistical view.\n\n- - Imposing marginal distributions to align ideal marginal distributions, we derive an optimisation problem tied to the OT that is solved using Sinkhorn, a highly efficient algorithm. More further, for better learning of the cost matrix, we present a linear mapping enabling very elegant learning with one-layer neural network.\n\n- - The experimental evidence shows the high efficiency of the algorithm, and the best performance achieved on three benchmarks, also supports our claim that addressing the post-hoc problem via OT is helpful and effective.\n\n\n# 2 PRELIMINARIES\n\n\nIn this section, we begin by giving a notational definition, followed by an introduction to the long-tailed recognition problem, and finally, we briefly review the OT, and give a quick recap of Logit Adjustment Menon et al. (2020).\n\n\nNotations: In what follows, for two matrices  $\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{N \\times K}$ , we denote  $\\langle \\mathbf{X}, \\mathbf{Y} \\rangle = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\mathbf{X}_{nk} \\mathbf{Y}_{nk}$  as the Frobenius dot-product.  $\\delta(\\cdot)$  stands for the Dirac function,  $p(\\cdot)$  represents the probability distribution.  $U(\\boldsymbol{r}, \\boldsymbol{c}) = \\{\\boldsymbol{P} \\in \\mathbb{R}_+^{N \\times K} | \\boldsymbol{P} \\mathbf{1}_K = \\boldsymbol{r}, \\boldsymbol{P}^\\intercal \\mathbf{1}_N = \\boldsymbol{c}\\}$ , where  $\\mathbf{1}_N$  and  $\\mathbf{1}_K$  are  $N$ -dimension and  $K$ -dimension vector whose elements are all 1.  $\\boldsymbol{r}$  and  $\\boldsymbol{c}$  refer to the vectors of size  $N$  and  $K$  respectively,  $U(\\boldsymbol{r}, \\boldsymbol{c})$  include all matrices with row and column sums  $\\boldsymbol{r}$  and  $\\boldsymbol{c}$  respectively\n\n\n# 2.1 PROBLEM FORMULATION\n\n\nHaving a collection of training samples  $\\{(x_n^s, y_n^s)\\}_{n=1}^{N_s}$ , validation samples  $\\{(x_n^v, y_n^v)\\}_{n=1}^{N_v}$  and test samples  $\\{(x_n^t, y_n^t)\\}_{n=1}^{N_t}$  for classification with  $K$  labels and input  $\\mathbf{x} \\in \\mathbb{R}^d$ , long-tailed recognition assumes that the class-prior distribution for training data  $p(y^s)$  is different from that for validation data  $p(y^v)$  and test data  $p(y^t)$ . Specifically, long-tailed recognition means the distribution  $p(y^s)$  is highly skewed, some classes have the dominant number of samples, while tailed labels own a very small number of samples. We can use imbalance ratio to measure the skewness in training data set, which can be defined as  $R = \\frac{N_{max}^s}{N_{min}^s}$ , where  $N_{max}^s$  and  $N_{min}^s$  denote the largest and smallest number of samples in the training data set, respectively. In this paper, we assume that the marginal distribution of the test set is known, we consider it as an implicit prior knowledge to be applied. Stepping back, even if we do not know the marginal distribution of the test dataset in advance. There are still ways to estimate the marginal distribution of the test dataset relatively precisely, such as methods in Hendrycks et al. (2018); Azizzadenesheli et al. (2019).\n\n\nObviously, most models trained on imbalanced training data set would suffer from extremely limited generalisation ability. Hence the ultimate goal is to learn a model that minimises the empirical risk, which can be formulated as:\n\n\n$$\n\n$$\n\\mathcal {J} \\left(\\Phi \\left(\\boldsymbol {x} _ {n} ^ {s}\\right), y _ {n} ^ {s}\\right) = \\frac {1}{N _ {s}} \\sum_ {n = 1} ^ {N _ {s}} \\mathcal {L} \\left(\\Phi \\left(\\boldsymbol {x} _ {n} ^ {s}\\right), y _ {n} ^ {s}\\right), \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\Phi (\\pmb{x}_n^s)\\in \\mathbb{R}^K$  denotes logits with associated sample,  $\\Phi (\\cdot):\\mathbb{R}^d\\to \\mathbb{R}^K$  represents the mapping via neural networks,  $\\mathcal{L}$  stands for the loss function, typically cross entropy loss function for classification problem.\n\n\n# 2.2 REMIN DERS ON OPTIMAL TRANSPORT\n\n\nOT is used to calculate the cost of transporting one probability measure to another. We next present a brief introduction to OT with the aim of helping us better view the long-tailed problem from an OT perspective.\n\n\nConsider probability measures  $r$  and  $c$  of the two random variables  $X$  and  $Y$  respectively, when the cost function  $\\mathcal{C}(X,Y): X \\times Y \\to \\mathbb{R}_+$  is defined to denote the cost of transporting  $X$  to  $Y$ , the OT distance can be defined as:\n\n\n$$\n\n$$\nd (r, c) = \\min  _ {\\boldsymbol {\\pi} \\in \\Pi (r, c)} \\int_ {X \\times Y} \\mathcal {C} (\\boldsymbol {x}, \\boldsymbol {y}) \\pi (\\boldsymbol {x}, \\boldsymbol {y}) d \\boldsymbol {x} d \\boldsymbol {y}, \\tag {2}\n$$\n\n$$\n\n\nwhere  $\\Pi(r, c) = \\left\\{\\int_{\\mathcal{Y}} \\pi(\\boldsymbol{x}, \\boldsymbol{y}) d\\boldsymbol{y} = r, \\int_{X} \\pi(\\boldsymbol{x}, \\boldsymbol{y}) d\\boldsymbol{x} = c\\right\\}$  is the joint probability measure with  $r$  and  $c$ . When we extend the above continuous case to the discrete situation, we consider following discrete distributions:\n\n\n$$\n\n$$\n\\boldsymbol {r} = \\sum_ {i = 1} ^ {N} p _ {i} (\\boldsymbol {x} _ {i}) \\delta (\\boldsymbol {x} _ {i}) \\quad \\boldsymbol {c} = \\sum_ {j = 1} ^ {K} p _ {i} (\\boldsymbol {y} _ {j}) \\delta (\\boldsymbol {y} _ {j}) \\tag {3}\n$$\n\n$$\n\n\nwhere  $p_i(\\boldsymbol{x}_i)$  and  $p_i(\\boldsymbol{y}_j)$  represent the probability mass to the sample  $\\boldsymbol{x}_i$  and  $\\boldsymbol{y}_j$  respectively. In this context, OT distance can be expressed as:\n\n\n$$\n\n$$\nd _ {M} (\\boldsymbol {r}, \\boldsymbol {c}) = \\min  _ {\\boldsymbol {P} \\in U (\\boldsymbol {r}, \\boldsymbol {c})} \\langle \\boldsymbol {P}, \\boldsymbol {M} \\rangle . \\tag {4}\n$$\n\n$$\n\n\nwhere  $M$  stands for the cost matrix constructed by  $M_{ij} = \\mathcal{C}(\\pmb{x}_i, \\pmb{y}_j)$ . The goal of OT is to find a transportation matrix  $\\pmb{P}$  that minimizes the distance  $d_M(\\pmb{r}, \\pmb{c})$\n\n\nAs we can see, OT is a distance measure between two probability distributions under some cost matrix (Villani, 2008). However, when we use network simplex or interior point methods to solve the above optimization problem, it often comes at the cost of heavy computational demands. To tackle this issue, OT with entropy constraint is proposed to allow the optimisation at small computational cost in sufficient smoothness (Cuturi, 2013). By adding a Lagrangian multiplier to the entropy constraint, the new formulation can be defined as follows:\n\n\n$$\n\n$$\nd _ {M} ^ {\\lambda} (\\boldsymbol {r}, \\boldsymbol {c}) = \\langle \\boldsymbol {P} ^ {\\lambda}, \\boldsymbol {M} \\rangle \\quad \\text {w h e r e} \\quad \\boldsymbol {P} ^ {\\lambda} = \\underset {\\boldsymbol {P} \\in U (\\boldsymbol {r}, \\boldsymbol {c})} {\\arg \\min } \\langle \\boldsymbol {P}, \\boldsymbol {M} \\rangle - \\lambda h (\\boldsymbol {P}), \\tag {5}\n$$\n\n$$\n\n\nwhere  $\\lambda \\in [0, +\\infty]$ ,  $h(P) = -\\sum_{n=1}^{N}\\sum_{k=1}^{K}P_{nk}\\log P_{nk}, d_M^\\lambda(r,c)$  is also known as dual-Sinkhorn divergence, besides, it can be calculated with matrix scaling algorithms for cheaper computational demand. The following lemma guarantees the convergence and uniqueness of the solution of the algorithm.\n\n\nLemma 1 For  $\\lambda >0$ , the solution  $P^{\\lambda}$  is unique and has the form  $P^{\\lambda} = \\mathbf{diag}(u)K\\mathbf{diag}(v)$ , where  $\\mathbf{u}$  and  $\\mathbf{v}$  are two non-negative vectors uniquely defined up to a multiplicative factor and  $\\mathbf{K} = e^{-M / \\lambda}$  is the element-wise exponential of  $-M / \\lambda$ .\n\n\nThe above lemma states the uniqueness of  $P^{\\lambda}$  (Sinkhorn, 1974), and  $P^{\\lambda}$  can be efficiently computed via Sinkhorn's fixed point iteration  $\\mathbf{u}, \\mathbf{v} \\gets r. / K\\mathbf{v}, c. / K^{\\intercal}\\mathbf{u}$ .\n\n\n# 2.3 A QUICK RECAP OF LOGIT ADJUSTMENT\n\n\nWe give a brief introduction to Logit Adjustment (Menon et al., 2020; Hong et al., 2020). For the model  $\\Phi(\\cdot)$ , it is trained by the standard cross-entropy loss function on imbalanced training data set, and evaluated on test data. In this algorithm, the test logit is adjusted as follows:\n\n\n$$\n\n$$\n\\Phi \\left(\\boldsymbol {x} _ {n} ^ {t}\\right) = \\Phi \\left(\\boldsymbol {x} _ {n} ^ {t}\\right) - \\log p \\left(y ^ {s}\\right) \\tag {6}\n$$\n\n$$\n\n\nThis simple procedure is derived from the Bayes optimal rule. It is apparent that Logit Adjustment involves a post hoc correction on an individual sample, which does not necessarily guarantee that the marginal distribution of the whole dataset matches the desired distribution.\n\n\n# 3 METHODOLOGY\n\n\nThe first part of this section explores post-hoc correction from an OT perspective, proceeds to the automatic learning of the cost matrix via linear mapping. Lastly, we demonstrate how it can be achieved simply with one-layer neural network.\n\n\n# 3.1 POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE\n\n\nSince Logit Adjustment applies adjustment at the individual sample level. It doesn't assure that the marginal distribution of the overall data set fulfils our desired distribution. In this respect, we clearly put the constraint into an equation:\n\n\n$$\n\n$$\n\\boldsymbol {Y} ^ {\\intercal} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\tag {7}\n$$\n\n$$\n\n\nwhere  $\\mathbf{Y} \\in \\mathbb{R}^{N \\times K}$  indicates the refined prediction value in matrix form,  $\\pmb{\\mu}$  represents the expected distribution on the test set. Alternatively, it is desirable to preserve another characteristic of  $\\mathbf{Y}$ , namely, remaining almost as similar to the original prediction as possible. We consider inner-product based similarity to measure the similarity, which is a straightforward yet useful similarity measure. In mathematical terms, this can be expressed as follows:\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y}} {\\operatorname {m a x i m i z e}} \\quad \\langle C (\\hat {\\boldsymbol {Z}}), \\boldsymbol {Y} \\rangle , \\tag {8}\n$$\n\n$$\n\n\nwhere  $\\hat{Z}$  represents the original prediction in matrix form,  $C(\\cdot)$  denotes to perform some transformation to  $\\hat{Z}$ , it can be some simple function, like Logarithmic function  $\\log(z)$ , exponential function  $z^{\\alpha}$ . Here we select  $-\\log(\\cdot)$  as the cost function. This choice was driven by the requirement that the cost matrix must be positive definite, whereas the transformation of the original prediction by  $-\\log(\\cdot)$  satisfies this condition, in addition, as log likelihood represents the local probability density of the associated samples, it can also be used to substitute  $\\hat{Z}$  for the similarity approximation. In brief, the resulting numerical form can be put in formal terms as follows:\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y}} {\\text {m i n i m i z e}} \\quad \\langle - \\log (\\hat {\\boldsymbol {Z}}), \\boldsymbol {Y} \\rangle \\tag {9}\n$$\n\n$$\n\n\n$$\n\n$$\n\\text {s u b j e c t} \\quad \\boldsymbol {Y} ^ {\\mathsf {T}} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\quad \\boldsymbol {Y} \\mathbf {1} _ {K} = \\mathbf {1} _ {N}. \\tag {10}\n$$\n\n$$\n\n\nExtra constraint on  $\\mathbf{Y}$  are imposed simply cos the tuned estimation has to fulfil the basic probabilistic requirement that its sum is one. Comparing Eq. (9-10) with Eq. (4), we can see that if we substitute  $\\mathbf{P}$  with  $\\mathbf{Y}$  and substitute  $\\mathbf{r}$  and  $\\mathbf{c}$  with  $\\mathbf{1}_N$  and  $\\boldsymbol{\\mu}$  respectively, we find that the above optimization problem is actually a special case of OT.\n\n\nIn preliminaries, the entropy regularised OT is introduced (EOT), by adding entropy regularisation to OT, the given equation can be solved efficiently by Sinkhorn algorithm. Specifically, the optimisation problem is:\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y}} {\\text {m i n i m i z e}} \\quad \\left\\langle - \\log (\\hat {\\boldsymbol {Z}}), \\boldsymbol {Y} \\right\\rangle + \\lambda \\boldsymbol {Y} \\log (\\boldsymbol {Y}) \\tag {11}\n$$\n\n$$\n\n\n$$\n\n$$\n\\text {s u b j e c t} \\quad \\boldsymbol {Y} ^ {\\intercal} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\quad \\boldsymbol {Y} \\mathbf {1} _ {K} = \\mathbf {1} _ {N}. \\tag {12}\n$$\n\n$$\n\n\nThe associated algorithmic flow is outlined in detail in Algorithm 1.\n\n\n```txt\nInput: Cost matrix  $M$ , trade-off parameter  $\\lambda$ , max number of iterations  $N_T$ , iteration number  $t$ , error threshold  $\\epsilon$ , current error  $\\delta$ , two probability vectors  $\\mathbf{r}$  and  $\\mathbf{c}$ ,  $|\\cdot|$  denotes the vector norm.\n```\n\n\n```txt\n1. Initialise  $\\pmb{K} = e^{-M / \\lambda},\\pmb{u}_{old} = \\mathbf{1}_N,\\pmb {v} = \\mathbf{1}_K,t = 0;$\n```\n\n\n```txt\n2 while  $t \\leq N_T$  and  $\\delta \\leq \\epsilon$  do\n```\n\n\n```txt\n3  $\\begin{array}{rl} & {\\pmb {u} = \\pmb {r}. / \\pmb {K}\\pmb {v};}\\\\ & {\\pmb {v} = \\pmb {c}. / \\pmb{K}^{\\intercal}\\pmb {u};}\\\\ & {\\sigma = |\\pmb{u}_{old} - \\pmb {u}|;}\\\\ & {\\pmb{u}_{old} = \\pmb {u};}\\\\ & {t = t + 1;} \\end{array}$    \n4   \n5   \n6   \n7\n```\n\n\n```txt\ns end\n```\n\n\n```latex\n9 Output  $\\pmb{Y} = \\mathbf{diag}(\\pmb{u})\\pmb{K}\\mathbf{diag}(\\pmb{v})$\n```\n\n\nAssigning  $\\lambda$  with 1, it is observed that we equate our objective function to the KL divergence, thus illustrating the extensive nature of our approach. DARP (Kim et al., 2020a) has previously applied it to long-tailed semi-supervised classification.\n\n\nRemark We would like to illustrate the non-applicable scenarios of our method. Firstly, our method requires a large number of samples for evaluation. This is because if the batch size is small, we can not guarantee that the desired marginal distribution can be satisfied within the batch. In some online scenarios, the sample-wise correction method is more suitable. In addition, our method assumes that the marginal distribution is already known, and in the paper we assume in particular that it is consistent with a uniform distribution.\n\n\n![Figure 1: Our proposed framework OTLM. The logit  $\\Phi(x)$  is fed into a single-layer feed-forward network to infer  $\\tilde{\\mathbf{Z}}$ . The cost matrix  $M$  is set to  $-\\log \\tilde{\\mathbf{Z}}$ . By solving the optimal transport problem via Sinkhorn algorithm, we can obtain the refined prediction  $\\mathbf{Y}$ .](images/9133a22319add0bd3e8fd0a9710c0179f099b050b88f2c773937afae5d50fe95.jpg)\n\n\n# 3.2 COST FUNCTION LEARNING VIA LINEAR MAPPING\n\n\nSimple functions is likely to be sub-optimal for the real data sets; this suggests the design of a better cost function to better fit the long-tailed recognition problem. However, manually designed cost functions require expert knowledge in different domains. Thus, we propose to use a linear mapping to automatically learn the cost function, which relieves the need to configuration.\n\n\nMore specifically, for predictions  $\\tilde{Z}$  generated by Softmax operation via leveraging linear transformation matrix  $\\mathbf{W}$ ,  $\\mathbf{W} \\in \\mathbb{R}^{K \\times K}$  is learned so that the following objective function is minimised.\n\n\n$$\n\n$$\n\\underset {\\boldsymbol {Y} \\in \\mathbb {R}} {\\text {m i n i m i z e}} \\quad - \\left\\langle \\boldsymbol {Y}, \\log \\tilde {\\boldsymbol {Z}} \\right\\rangle + \\lambda \\left\\langle \\boldsymbol {Y}, \\log \\boldsymbol {Y} \\right\\rangle , \\tag {13}\n$$\n\n$$\n\n\n$$\n\n$$\n\\boldsymbol {Y} \\mathbf {1} _ {K} = \\mathbf {1} _ {N} \\quad \\boldsymbol {Y} ^ {\\intercal} \\mathbf {1} _ {N} = \\boldsymbol {\\mu}, \\quad \\tilde {\\boldsymbol {Z}} _ {n k} = \\frac {\\exp \\left(\\boldsymbol {W} ^ {\\intercal} \\boldsymbol {\\Phi} \\left(\\boldsymbol {x} _ {n}\\right)\\right) _ {k}}{\\sum_ {k} ^ {K} \\exp \\left(\\boldsymbol {W} ^ {\\intercal} \\boldsymbol {\\Phi} \\left(\\boldsymbol {x} _ {n}\\right)\\right) _ {k}}.\n$$\n\n$$\n\n\nThe resulting formula can be illustrated using a simple one-layer network of weight parameter  $W$ , together with an error function in Eq. (13). By normalising each row of  $W$ , each element can be understood as the probability value of confusing two classes, and somehow as a transition matrix between classes. We initialise  $W$  with an identity matrix and use small learning rate to learn  $W$ . One could also absorb the Logit Adjustment term  $-\\log p(y^{s})$  as the fixed bias parameter into the network. Motivated by this description, we can use a general gradient descent algorithm, such as SGD, to optimise the error function. Take into account the implementation in practice, the direct calculation of Eq. (13) can be done using the Sinkhorn iterations (Cuturi, 2013; Frogner et al., 2015; Peyr\u00e9 et al., 2019) in mini-batch training efficiently. Besides, We term the proposed method as OTLM (Optimal Transport via Linear Mapping), Figure 1 illustrates the overview of OTLM.\n\n\nCompute the gradients w.r.t  $W$  With the optimisation in Algorithm 1, it was conducted on the overall data set with a comparatively large number of data. Quite a different scenario now, as minibatch training is more favoured when it comes to neural networks. For this reason, the optimisation workflow in Algorithm 1, as pointed by Peyre et al. (2019); Viehmann (2019), will have a problem of batch stabilisation. To this end, we perform logarithmic transformation to step 3-4 in Algorithm 1.\n\n\n$$\n\n$$\n\\log \\boldsymbol {u} = \\log \\boldsymbol {r} - \\log (\\boldsymbol {M} \\boldsymbol {v}) = \\log \\boldsymbol {r} - \\operatorname {l o g s u m e x p} \\left(\\frac {1}{\\lambda} \\boldsymbol {M} - \\log \\boldsymbol {v}\\right) \\tag {14}\n$$\n\n$$\n\n\n$$\n\n$$\n\\log \\boldsymbol {v} = \\log \\boldsymbol {c} - \\log \\left(\\boldsymbol {M} ^ {\\intercal} \\boldsymbol {u}\\right) = \\log \\boldsymbol {c} - \\operatorname {l o g s u m e x p} \\left(\\frac {1}{\\lambda} \\boldsymbol {M} - \\log \\boldsymbol {u}\\right). \\tag {15}\n$$\n\n$$\n\n\nwith log-sum-exp operation  $\\mathrm{logsumexp}(\\boldsymbol{x}) = \\log (\\sum_{i}\\exp \\boldsymbol{x}_{i})$ . Assuming convergence is achieved in the Sinkhorn's loops, it is not necessary for us manually to compute the derivative of the loss function in Eq. (13) w.r.t  $W$ . As a matter of fact, as long as we make sure that the entire optimisation process is differentiable, modern deep learning libraries can then automatically implement end-to-end derivations, as it allows us to differentiate between the results of Sinkhorn's loops as a mere composition of elementary operations.\n\n\nPerformance degradation caused by imbalanced distributions can be addressed with meta-learning based methods (Ren et al., 2020; Wang et al., 2020a; Li et al., 2021). Recent works have illustrated neu\n\n\nral networks can learn more meaningful representations from the validation data set  $\\{(\\pmb{x}_n^v, y_n^v)\\}_{n=1}^{N_v}$ . Inspired by the methods based on meta learning, Here we also take a validation data set to optimise the parameter  $\\pmb{W}$ , but with the still significant difference in that we require no labelling information, thus avoiding a large expanse of labeling samples.\n\n\n# 4 EXPERIMENTS\n\n\nIn this section, we first conduct experiments comparing our approach versus extant post-hoc correction methods on three data sets, including CIFAR-100-LT (Cao et al., 2019), ImageNet-LT (Liu et al., 2019), and iNaturalist (Van Horn et al., 2018) with varying backbones. Finally, we empirically make a comparison of our algorithm with alternative cutting-edge long-tailed recognition methods. Observe that, for one thing, a plausible coupling of our algorithm to any training-aware long-tailed recognition method exists. As an illustration of the potency and strong generalisation of our approach, we conducted post-hoc correction of the pseudo-predictions of both methods, which are fairly typical for the training stage of all data sets: The first is based on the cross entropy (CE), and the other is the recently proposed  $RIDE$  (Wang et al., 2020b), which is based on multi experts.\n\n\n# 4.1 DATA SETS AND BASELINES\n\n\nThe baseline methods and data set are briefly described here, with implementation details placed in the appendix. In our experiments, we assume that the marginal distribution of the test set is uniform due to the characteristics of the three data sets.\n\n\nBaselines: We compare our methods with (i) post-hoc correction methods including Logit Adjustment (Menon et al., 2020), DARP (Kim et al., 2020a), (ii) state-of-the-art methods including Focal Loss (Lin et al., 2017), LDAM (Cao et al., 2019), BBN (Zhou et al., 2020), Balanced Softmax (Ren et al., 2020), Causal Norm (Tang et al., 2020), LADE (Hong et al., 2020), M2m (Kim et al., 2020b), Decouple (Kang et al., 2019), LFME (Xiang et al., 2020), RIDE (Wang et al., 2020b)\n\n\nLong-tailed data set: We take experiments on three data sets including CIFAR-100-LT, ImageNet-LT, and iNaturalist. We build the imbalanced version of CIFAR-100 by downsampling samples per class following the profile in Liu et al. (2019); Kang et al. (2019) with imbalanced ratio 10, 50, and 100. For all the benchmarks, we evaluate the performance on the test data set using a model trained on the training data set, and report the results using top-1 accuracy.\n\n\n# 4.2 MAIN RESULTS\n\n\nTable 1 illustrates the results on the CIFAR-100-LT data set. For CE-based training-aware methods, it first shows that Logit Adjustment is indeed a simple yet effective method for post-hoc correction. Remarkably, it outperforms the baseline methods by  $2.6\\%$ ,  $4.1\\%$ , and  $4.1\\%$  under the imbalance ratio of 10, 50, and 100 respectively. With a quick look at the results based on the OT approach, the superior results stress the advantages of our approach over the DARP and Logit adjustments. OT algorithm can further improve the performance by  $0.62\\%$ ,  $0.43\\%$ ,  $0.70\\%$ , for OTLM, it outperforms\n\n\nTable 1: Comparison on the top-1 accuracy with post-hoc correction methods on CIFAR-100-LT data set using ResNet-32 backbone, ImageNet-LT and iNaturalist data set using ResNet-50 and ResNeXt-50-32x4d backbones. Better results are marked in bold, the small red font indicates the increase in accuracy, compared to Logit Adjustment and RIDE, '-' means the results are not reported. CE indicates that a cross-entropy loss function is used in the training stage.\n\n\n<table><tr><td>Data set</td><td></td><td colspan=\"2\">CIFAR-100</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Imabalced Ratio</td><td>10</td><td>50</td><td>100</td><td>-</td><td>-</td></tr><tr><td>Baseline (CE)</td><td>59.00</td><td>45.50</td><td>41.00</td><td>43.10</td><td>65.00</td></tr><tr><td>Logit Adjustment+CE</td><td>61.63</td><td>49.62</td><td>45.11</td><td>51.62</td><td>69.44</td></tr><tr><td>DARP+CE</td><td>61.78</td><td>49.52</td><td>45.13</td><td>51.20</td><td>71.18</td></tr><tr><td>OT+CE</td><td>62.25+0.62</td><td>50.05+0.43</td><td>45.81+0.70</td><td>51.87+0.25</td><td>71.20+1.76</td></tr><tr><td>OTLM+CE</td><td>62.18+0.55</td><td>50.35+0.73</td><td>46.10+0.99</td><td>52.35+0.73</td><td>72.30+2.86</td></tr><tr><td>RIDE</td><td>-</td><td>-</td><td>50.20</td><td>57.50</td><td>72.90</td></tr><tr><td>OT+RIDE</td><td>-</td><td>-</td><td>50.82+0.62</td><td>58.45+0.95</td><td>75.85+2.95</td></tr><tr><td>OTLM+RIDE</td><td>-</td><td>-</td><td>51.43+1.23</td><td>58.71+1.21</td><td>76.12+3.22</td></tr></table>\n\n\nLogit Adjustment by  $0.55\\%$ ,  $0.73\\%$ ,  $0.99\\%$  under the three imbalance ratios on CIFAR-100-LT data set. For RIDE-based training-aware methods, there is also a  $0.6\\%$  improvement in the accuracy of our method.\n\n\nTable 1 also provides the results on ImageNet-LT and iNaturalist data sets. The fact again attracts our attention that for CE-based training-aware methods, OT, which is based on convex optimisation, consistently outperform Logit Adjustment by a large margin (1.76%) on iNaturalist data set. The accuracy boosting indicates the great potential of methods based on optimisation for post-hoc correction. Besides, not surprisingly, OTLM can further enhance the prediction accuracy, it outperforms Logit Adjustment by 0.73% and 2.86% on ImageNet-LT and iNaturalist respectively. As for RIDE, the gain in accuracy is also significant, about 3% on iNaturalist.\n\n\n# 4.3 COMPARISON WITH THE STATE-OF-THE-ART METHODS\n\n\nArmed with a demonstration of the validity of OT and OTLM, we switched to a comparison of our performance with existing methods that achieved the most advanced results on three benchmarks. As shown in Table 2, in particular, on ImageNet-LT and iNaturalist data sets, the experimental results are remarkable and impressive, our method outperforms RIDE by  $1.0\\%$  and  $3.0\\%$ , respectively. Since iNaturalist is a very difficult and fine-grained data set consisting of 8,142 categories. These huge performance gains come at a fraction of the cost, as we show in the next subsection.\n\n\nTable 2: Comparison on the top-1 accuracy with state-of-the-arts on CIFAR-100-LT data set using ResNet-32 backbone, on ImageNet-LT and iNaturalist data set using e ResNet-50 and ResNeXt-50-32x4d backbones respectively. - denotes the results are not reported, results underlined are the ones being compared, best results are marked in bold, the small red font denotes performance gain.\n\n\n<table><tr><td>Data Set</td><td colspan=\"3\">CIFAR-100</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Imabalced Ratio</td><td>10</td><td>50</td><td>100</td><td>-</td><td>-</td></tr><tr><td>Cross Entropy (CE)</td><td>59.0</td><td>45.5</td><td>38.3</td><td>43.1</td><td>65.0</td></tr><tr><td>Focal Loss</td><td>55.8</td><td>44.3</td><td>38.4</td><td>-</td><td>-</td></tr><tr><td>CB-Focal</td><td>-</td><td>-</td><td>-</td><td>43.7</td><td>61.1</td></tr><tr><td>OLTR</td><td>-</td><td>-</td><td>-</td><td>41.9</td><td>63.9</td></tr><tr><td>NCM</td><td>-</td><td>-</td><td>-</td><td>47.3</td><td>63.1</td></tr><tr><td>norm</td><td>-</td><td>-</td><td>-</td><td>49.4</td><td>69.3</td></tr><tr><td>cRT</td><td>-</td><td>-</td><td>-</td><td>49.6</td><td>67.6</td></tr><tr><td>LWS</td><td>-</td><td>-</td><td>-</td><td>49.9</td><td>69.5</td></tr><tr><td>LDAM</td><td>-</td><td>-</td><td>-</td><td>-</td><td>64.6</td></tr><tr><td>LDAM+DRW</td><td>58.7</td><td>46.6</td><td>42.0</td><td>-</td><td>68.0</td></tr><tr><td>BBN</td><td>59.1</td><td>47.0</td><td>42.6</td><td>-</td><td>66.3</td></tr><tr><td>Causal Norm</td><td>59.6</td><td>50.3</td><td>44.1</td><td>51.8</td><td>63.9</td></tr><tr><td>M2m</td><td>58.2</td><td>-</td><td>43.5</td><td>-</td><td>-</td></tr><tr><td>LFME</td><td>-</td><td>-</td><td>43.8</td><td>-</td><td>-</td></tr><tr><td>Balanced Softmax</td><td>61.6</td><td>49.9</td><td>45.1</td><td>-</td><td>-</td></tr><tr><td>LADE</td><td>61.7</td><td>50.5</td><td>45.4</td><td>51.9</td><td>70.0</td></tr><tr><td>RIDE (6 experts)</td><td>-</td><td>-</td><td>50.2</td><td>57.5</td><td>72.9</td></tr><tr><td>Logit Adjustment</td><td>61.6</td><td>49.6</td><td>45.1</td><td>51.6</td><td>69.4</td></tr><tr><td>DARP</td><td>61.8</td><td>49.5</td><td>45.1</td><td>51.2</td><td>71.2</td></tr><tr><td>OT+CE</td><td>62.3+0.7</td><td>50.1</td><td>45.8</td><td>51.9</td><td>71.2</td></tr><tr><td>OTLM+CE</td><td>62.2</td><td>50.4+0.8</td><td>46.1</td><td>52.4</td><td>72.3</td></tr><tr><td>OT+RIDE (6 experts)</td><td>-</td><td>-</td><td>50.8+0.6</td><td>58.5+1.0</td><td>75.9+3.0</td></tr><tr><td>OTLM+RIDE (6 experts)</td><td>-</td><td>-</td><td>51.4+1.2</td><td>58.7+1.2</td><td>76.1+3.2</td></tr></table>\n\n\n# 4.4 COMPUTATION COST OF OT AND OTLM\n\n\nAs we have highlighted, the additional computational cost of OT and OTLM is particularly small, compared to that of training-aware methods. In Table 3, exact time of the evaluations on the ImageNet-LT and iNaturalist data sets is provided. Please note, the times here are measured from the start of each method to the final best performance. For the comparison between OT and OTLM, because\n\n\nTable 3: Time for different methods to execute on the ImageNet-LT and iNaturalist data sets. The times here are measured from the start of each method to the final best performance. Their running time are counted in seconds. Coupled with the prior results, an observation can be made that OT can be the best coordinate to trade off performance and efficiency in post-hoc correction methods.\n\n\n<table><tr><td>Data Set</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Logit Adjustment</td><td>0.3</td><td>1.2</td></tr><tr><td>OT</td><td>32.7</td><td>82.5</td></tr><tr><td>OTLM</td><td>134.4</td><td>256.5</td></tr></table>\n\n\nOTLM runs on the GPU, one optimization iteration of OTLM is in fact much faster than OT. Except for OTLM, which was run on an NVidia card (V100), the results come from a 28-core machine (2.20 Ghz Xeon). Firstly, we can observe that all post-hoc correction methods here are not quite time-consuming. For comparison, ImageNet-LT and iNaturalist can be trained on 4 NVidia cards (V100) for extremely long periods of time. Using ResNet-50-32x4d on the iNaturalist training data set, for example, with a batch size of 256, the amount of training time (in seconds) to perform 1 iteration is approximately 850. Also, if one differs the performance of each method, we gather that OT and OTLM provide the best balance of performance and running costs to match.\n\n\n# 4.5 CONVERGENCE BEHAVIOR OF OTLM\n\n\nTo verify that our adopted OTLM algorithm is really efficient. With ResNet-50 as the backbone, we plotted curves depicting the decreasing absolute error  $\\sigma$  with the number of iterations  $t$  on the ImageNet-LT validation data set. Image on the left in Figure 2 gives us a visualisation of the optimisation process. It is found that the absolute error  $\\sigma$  converges at around 60 iterations. In fact, we discovered that 10 or even fewer iterations were enough to yield satisfactory performance.\n\n\n![Figure 2: The left image shows the convergence of OTLM: The absolute error  $\\sigma$  decreases with iterations on the ImageNet-LT validation data set (20k samples), and we used ResNet-50 as the backbone to obtain the original predictions  $Z$ . The right image demonstrates the block in  $W$ , we trained on CIFAR-100 to produce the original predictions with OTLM to minimise the difference between the ideal and actual distributions. For ease of display, the values of the diagonal elements are relatively large and we will replace these values with zeros.](images/1e6fa388a2f2907bb4493cf441d4da8147d0f8579170acb3e35999f1319d3e7c.jpg)\n\n\n![](images/f746a69db0cbd8c10f982d58baa44ba83e8ee5afb892a594b0717dc9a46c636a.jpg)\n\n\n# 4.6 DETAILED ANALYSIS ON DISTRIBUTION MATCHING\n\n\nUpon distribution fitting, preferably the estimation bias is lessened, which implies that the KL divergence distance between the refined prediction and the target distribution is decreased. Table 4 displays the KL distances on the ImageNet-LT and iNaturalist data sets, and we can find that OT possesses a smaller KL distance than Logit Adjustment, which suggests that OT decreases the estimation bias more than Logit Adjustment by a much larger margin.\n\n\n# 4.7 VISUALISATION ON WEIGHT PARAMETER IN OTLM\n\n\nAs a natural consequence, one wonders about the exact structure of this weight matrix  $\\mathbf{W}$ , and no better way to present its results than through visualisation. Towards this end, we applied OTLM\n\n\nTable 4: KL distance between refined probability distribution and target distribution computed on ImageNet-LT and iNaturalist data sets. Smaller distance indicates two distributions are more similar.\n\n\n<table><tr><td>Data Set</td><td>ImageNet-LT</td><td>iNaturalist</td></tr><tr><td>Logit Adjustment</td><td>1.78e-05</td><td>5.81e-06</td></tr><tr><td>DARP</td><td>8.49e-08</td><td>1.07e-08</td></tr><tr><td>OT</td><td>8.62e-10</td><td>4.72e-10</td></tr></table>\n\n\nto refine the original predictions on CIFAR-100 data set with an imbalanced ratio  $R = 100$  and ResNet-32 backbone, and trained a one-layer model until it converged to extract the value of  $W$ , where the shape of is  $100 \\times 100$ . We visualised a sub-block of size  $20 \\times 20$ , the illustration is presented on the right image in Figure 2. The very physical meaning of the elements in  $W$  can be interpreted as the probability of transition between classes. The visualised matrix is very dense, this non-sparsity suggests that the OTLM manages to correct the predictions of the model in the training stage, whilst minimising the distance between the desired and the actual distribution.\n\n\n# 5 RELATED WORK\n\n\nLong-tailed recognition: The great majority of current methods applied to long-tailed recognition can be divided into either training-aware and post-hoc correction methods. The training-aware methods aim to improve the generalisation ability of models trained on imbalanced training data set. Among them, re-sampling and re-weighting are two major methods. Re-sampling controls the class numbers by the means of over-sampling (Chawla et al., 2002) and under-sampling (Barandela et al., 2004; Drummond et al., 2003). The re-weighting approaches achieve balanced learning by giving smaller weights to majority classes and larger weights to minority classes (Tang et al., 2008; Zadrozny et al., 2003; Lin et al., 2017; Lee et al., 2017; Cui et al., 2019; Khan et al., 2019). Other approaches have also been proposed to resolve the imbalanced problem. For example, Cao et al. introduces a theoretically-principled loss function motivated by minimizing the generalisation error bound (Cao et al., 2019), Kang et al. proposes to decouple the learning phases into representation learning and classifier tuning (Kang et al., 2019). Apart from training-aware methods, post-hoc correction also offers us an avenue for long-tailed recognition problem, it can be applied to the normalisation of classifier weights (Kang et al., 2019; Zhang et al., 2019; Kim & Kim, 2020) or logits based on label frequency (Provost, 2000; Collell et al., 2016). Among them, Logit Adjustment (Menon et al., 2020) is a simple yet powerful method to use. Compared with Logit Adjustment, our approach performs the post-hoc correction from the optimisation view.\n\n\nOptimal transport: OT distances is a important family of distances for probability measures (Villani, 2008; Santambrogio, 2015). It has been extensively applied in different fields in machine learning. Courty et al. (Courty et al., 2016) proposes a regularised unsupervised optimal transportation model to perform the domain adaption. Other applications include 3D shape matching (Su et al., 2015), generative model (Arjovsky et al., 2017; Salimans et al., 2018; Bunne et al., 2019; Deshpande et al., 2019; Bhagoji et al., 2019), graph matching (Xu et al., 2019a;b), model designs Kandasamy et al. (2018); Chizat & Bach (2018). To the best of our knowledge, we are the first to perform the post-hoc correction from the OT perspective.\n\n\n# 6 CONCLUSION\n\n\nIn this paper, we present two techniques for post-hoc corrections from an optimisation perspective. Our approach reviews the basic mathematical formulation of the distribution alignment problem and relates the resulting formulation to OT, which we find, moreover, offers greater possibilities and flexibility to solve the problem. In order to avoid manual configuration of the cost matrix, we propose to learn the cost matrix automatically by linear mapping, which leads to further performance improvements. The experimental results confirm the superiority of our proposed approach. I believe our work can open up more possibilities and imagination for post-hoc corrections.\n\n\n# REFERENCES\n\n- Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pp. 214-223. PMLR, 2017.\n\n- Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning for domain adaptation under label shifts. arXiv preprint arXiv:1903.09734, 2019.\n\n- Ricardo Barandela, Rosa M Valdovinos, J Salvador S\u00e1nchez, and Francesc J Ferri. The imbalanced training sample problem: Under or over sampling? In Joint IAPR international workshops on statistical techniques in pattern recognition (SPR) and structural and syntactic pattern recognition (SSPR), pp. 806-814. Springer, 2004.\n\n- Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Lower bounds on adversarial robustness from optimal transport. arXiv preprint arXiv:1909.12272, 2019.\n\n- Paula Branco, Lu\u00eds Torgo, and Rita P Ribeiro. A survey of predictive modeling on imbalanced domains. ACM Computing Surveys (CSUR), 49(2):1-50, 2016.\n\n- Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. *Neural Networks*, 106:249\u2013259, 2018.\n\n- Charlotte Bunne, David Alvarez-Melis, Andreas Krause, and Stefanie Jegelka. Learning generative models across incomparable spaces. In International Conference on Machine Learning, pp. 851-861. PMLR, 2019.\n\n- Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. arXiv preprint arXiv:1906.07413, 2019.\n\n- Claire Cardie and Nicholas Howe. Improving minority class prediction using case-specific feature weights. 1997.\n\n- Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.\n\n- Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.\n\n- Guillem Collell, Drazen Prelec, and Kaustubh Patil. Reviving threshold-moving: a simple plug-in bagging ensemble for binary and multiclass imbalanced data. arXiv preprint arXiv:1606.08698, 2016.\n\n- Nicolas Courty, R\u00e9mi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853-1865, 2016.\n\n- Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9268-9277, 2019.\n\n- Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26:2292-2300, 2013.\n\n- Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David Forsyth, and Alexander G Schwing. Max-sliced wasserstein distance and its use for gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10648-10656, 2019.\n\n- Chris Drummond, Robert C Holte, et al. C4.5, class imbalance, and cost sensitivity: why undersampling beats over-sampling. In Workshop on learning from imbalanced datasets II, volume 11, pp. 1-8. Citeseer, 2003.\n\n- Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso Poggio. Learning with a wasserstein loss. arXiv preprint arXiv:1506.05439, 2015.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise. arXiv preprint arXiv:1802.05300, 2018.\n\n- Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, and Buru Chang. Disentangling label distribution for long-tailed visual recognition. arXiv preprint arXiv:2012.00321, 2020.\n\n- Chen Huang, Yining Li, Chen Change Loy, and Xiaou Tang. Learning deep representation for imbalanced classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5375-5384, 2016.\n\n- Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric Xing. Neural architecture search with bayesian optimisation and optimal transport. arXiv preprint arXiv:1802.07191, 2018.\n\n- Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. arXiv preprint arXiv:1910.09217, 2019.\n\n- Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 103-112, 2019.\n\n- Byungju Kim and Junmo Kim. Adjusting decision boundary for class imbalanced learning. IEEE Access, 8:81674-81685, 2020.\n\n- Jaehyung Kim, Youngbum Hur, Sejun Park, Eunho Yang, Sung Ju Hwang, and Jinwoo Shin. Distribution aligning refinery of pseudo-label for imbalanced semi-supervised learning. arXiv preprint arXiv:2007.08844, 2020a.\n\n- Jaehyung Kim, Jongheon Jeong, and Jinwoo Shin. M2m: Imbalanced classification via major-to-minor translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13896-13905, 2020b.\n\n- Wonji Lee, Chi-Hyuck Jun, and Jong-Seok Lee. Instance categorization by support vector machines to adjust weights in adaboost for imbalanced data classification. Information Sciences, 381:92-103, 2017.\n\n- Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. arXiv preprint arXiv:2103.12579, 2021.\n\n- Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr D\u00f3lar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980-2988, 2017.\n\n- Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2537-2546, 2019.\n\n- Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. arXiv preprint arXiv:2007.07314, 2020.\n\n- Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355-607, 2019.\n\n- Foster Provost. Machine learning from imbalanced data sets 101. In Proceedings of the AAAI'2000 workshop on imbalanced data sets, volume 68, pp. 1-3. AAAI Press, 2000.\n\n- Xingye Qiao and Yufeng Liu. Adaptive weighted learning for unbalanced multicategory classification. Biometrics, 65(1):159-168, 2009.\n\n- Jiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Balanced meta-softmax for long-tailed visual recognition. arXiv preprint arXiv:2007.10740, 2020.\n\n- Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving gans using optimal transport. arXiv preprint arXiv:1803.05573, 2018.\n\n- Filippo Santambrogio. Optimal transport for applied mathematicians. *Birkh\u00e4user*, NY, 55(58-63):94, 2015.\n\n- Richard Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. ii. Proceedings of the American Mathematical Society, 45(2):195-198, 1974.\n\n- Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. arXiv preprint arXiv:2001.07685, 2020.\n\n- Zhengyu Su, Yalin Wang, Rui Shi, Wei Zeng, Jian Sun, Feng Luo, and Xianfeng Gu. Optimal mass transport for shape matching and comparison. IEEE transactions on pattern analysis and machine intelligence, 37(11):2246-2259, 2015.\n\n- Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classification by keeping the good and removing the bad momentum causal effect. arXiv preprint arXiv:2009.12991, 2020.\n\n- Yuchun Tang, Yan-Qing Zhang, Nitesh V Chawla, and Sven Krasser. Svms modeling for highly imbalanced classification. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(1):281-288, 2008.\n\n- Grant Van Horn and Pietro Perona. The devil is in the tails: Fine-grained classification in the wild. arXiv preprint arXiv:1709.01450, 2017.\n\n- Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8769-8778, 2018.\n\n- Thomas Viehmann. Implementation of batched sinkhorn iterations for entropy-regularized wasserstein loss. arXiv preprint arXiv:1907.01729, 2019.\n\n- C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.\n\n- Renzhen Wang, Kaiqin Hu, Yanwen Zhu, Jun Shu, Qian Zhao, and Deyu Meng. Meta feature modulator for long-tailed recognition. arXiv preprint arXiv:2008.03428, 2020a.\n\n- Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella X Yu. Long-tailed recognition by routing diverse distribution-aware experts. arXiv preprint arXiv:2010.01809, 2020b.\n\n- Liuyu Xiang, Guiguang Ding, and Jungong Han. Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification. In European Conference on Computer Vision, pp. 247-263. Springer, 2020.\n\n- Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1492-1500, 2017.\n\n- Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph partitioning and matching. arXiv preprint arXiv:1905.07645, 2019a.\n\n- Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-wasserstein learning for graph matching and node embedding. In International conference on machine learning, pp. 6932-6941. PMLR, 2019b.\n\n- Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate example weighting. In Third IEEE international conference on data mining, pp. 435-442. IEEE, 2003.\n\n- Junjie Zhang, Lingqiao Liu, Peng Wang, and Chunhua Shen. To balance or not to balance: A simple-yet-effective approach for learning with long-tailed distributions. arXiv preprint arXiv:1912.04486, 2019.\n\n- Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9719\u20139728, 2020.\n\nResponse: Given the detailed analysis and contributions of the paper, here is my evaluation:\n\n### Strengths:\n1. **Original Contribution**: The paper introduces a novel approach to post-hoc correction using optimal transport (OT) theory. It proposes an automatic learning mechanism for the cost matrix, which is a significant advancement over previous methods.\n   \n2. **Methodological Rigor**: The authors provide a thorough theoretical foundation by linking the long-tailed recognition problem to OT. They introduce an optimisation problem using Sinkhorn algorithm, ensuring the method is grounded in established mathematical principles.\n\n3. **Empirical Validation**: The paper presents extensive experimental results on three benchmark datasets (CIFAR-100-LT, ImageNet-LT, and iNaturalist). The results demonstrate substantial performance improvements over state-of-the-art methods, especially in terms of reducing estimation bias.\n\n4. **Scalability and Efficiency**: The proposed method shows good scalability and efficiency, as evidenced by its running time on ImageNet-LT and iNaturalist datasets. The authors explicitly highlight that OTLM provides a good balance between performance and efficiency.\n\n### Weaknesses:\n1. **Computational Complexity**: While the authors argue that the computational overhead is manageable, they do not provide detailed comparisons with other methods (like training-aware methods) in terms of total training time. This makes it hard to assess the true trade-off between efficiency and performance.\n\n2. **Model Complexity**: The proposed method uses a one-layer neural network for learning the cost matrix. This might limit the model's ability to capture complex relationships in the data, especially on more challenging datasets.\n\n3. **Limited Scope**: Although the authors claim this is the first work to use OT for post-hoc correction, they don't discuss how this method compares to other OT-based approaches for distribution alignment in other contexts (e.g., domain adaptation, generative models).\n\n4. **Generalizability**: The experiments are conducted on specific datasets and backbones. While encouraging, it would be valuable to see how the method performs on a wider range of datasets and architectures.\n\n### Overall Impact:\nThe paper makes a strong case for using OT for post-hoc correction in long-tailed recognition. The theoretical foundations, experimental validations, and practical considerations all point towards a promising approach. The method's potential for improving generalization in imbalanced settings is significant.\n\n### Prediction:\nGiven the strengths in methodology, empirical validation, and novelty, I would predict this paper to be accepted by ICLR. The paper addresses a critical issue in deep learning and provides a novel and effective solution. However, I recommend the authors to address the computational complexity and model complexity issues in future work to broaden the applicability of their method.\n\n**Prediction:** \\(\\boxed{Accept}\\)", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636629.0409517", "msecs": "40.0", "relativeCreated": "89734.09223556519", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# VEnhancer: Generative Space-Time Enhancement for Video Generation\n\n# Abstract\nWe present \\emph{VEnhancer}, a generative space-time enhancement method that can improve the existing AI-generated videos spatially and temporally through one video diffusion model. Given a generated low-quality video, our approach can increase its spatial and temporal resolution simultaneously with arbitrary up-sampling space and time scales by adding more details in spatial domain and synthesize detailed motion in temporal domain. Furthermore, VEnhancer is able to remove generated spatial artifacts and temporal flickering of generated videos. To achieve this, basing on a pretrained generative video prior, we train a \\textbf{S}pace-\\textbf{T}ime Controller and inject it to the prior as a condition on low-frame-rate and low-resolution videos. To effectively train this ST-Controller, we design \\textit{space-time data augmentation} to create diversified video training pairs as well as \\textit{video-aware conditioning} for realizing different augmentation parameters in both spatial and temporal dimensions. Benefiting from the above designs, VEnhancer can be end-to-end trained to enable multi-function in one single model. Extensive experiments show that VEnhancer surpasses existing state-of-the-art video super-resolution and space-time super-resolution methods in enhancing AI-generated videos. Moreover, VEnhancer is able to greatly improve the performance of open-source state-of-the-art text-to-video methods on video generation benchmark, VBench.\n\n\n# 1 INTRODUCTION\n\n\nWith the advances of text-to-image generation (Rombach et al., 2022; Podell et al., 2023; Chen et al., 2023b; Gao et al., 2024) and large-scale video datasets with text description (Bain et al., 2021), there is fast development of text-to-video generative models (Guo et al., 2023; Chen et al., 2023c; Ho\n\n\net al., 2022; Blattmann et al., 2023b; Wang et al., 2023b;a; Chen et al., 2023a; 2024a; Gupta et al., 2023; Wu et al., 2023b). These developments enable users to generate compelling videos through textual descriptions of the desired content. One common solution (Ho et al., 2022; Blattmann et al., 2023b; Wang et al., 2023b; Blattmann et al., 2023a; Gupta et al., 2023) to obtain high-quality videos is to adopt cascaded pipelines, which stacks several video diffusion models, including text-to-video, temporal super-resolution and spatial super-resolution (S-SR) diffusion. These pipeline significantly reduce computation cost when generating high-resolution and high-frame-rate videos, but also pose several issues. First, using two different models for spatial and temporal enhancement separately is redundant, as they are strongly correlated. Also, the proposed diffusion-based spatial or temporal super-resolution (Blattmann et al., 2023b; Ho et al., 2022; Blattmann et al., 2023a; Wang et al., 2023b; Lin et al., 2024) have a limited flexibility, as they can only handle fixed interpolation ratio (i.e., predicting three frames between two consecutive frames) or fixed upscaling factors (i.e.,  $4 \\times$ ). Second, directly training diffusion models on synthesized video pairs may limit its generalization ability, as these models tailored on classic super-resolution task hallucinate high-frequency details, but cannot semantically improve the visual quality of input videos, such as eliminating distortions, artifacts, or recreating new contents (see LaVie-SR's results in Figure 1).\n\n\nAnother common approach is training another diffusion model to remove artifacts and to refine distorted content in the generated videos (Henschel et al., 2024; Zhang et al., 2023b). One example is I2VGen-XL (Zhang et al., 2023b), which follows the idea of image generation model SDXL (Podell et al., 2023)\u2013 it first upscales videos to higher resolutions using bilinear interpolation, and trains another diffusion refinement model, which will be used for video regeneration through a nois-ingenoising process (Meng et al., 2021). However, this method usually produces over-smoothed videos without realistic texture details (see Figure 1, I2VGen-XL), since the adopted bilinear upsampling cannot generate more spatial details. More importantly, the nois-ingenoising process (i.e., starting from  $t = 600$ ) will substantially change the original video content, which cannot always be acceptable in practical applications.\n\n\nIn short, current generative video enhancement methods face several challenges. First, sequentially applying temporal and spatial super-resolution is redundant, as they are independently trained, but using similar training datasets. Thus, such design is both sub-optimal and inefficient during the inference. Second, existing refinement methods struggle in balancing between video quality and fidelity to the original content. More importantly, they cannot perform effective super-resolution for increasing spatial and temporal details, which limits their practicality. Third, previous generative video enhancement methods lack the flexibility in dealing with different upscaling factors and refinement strengths for spatial or temporal super-resolution and video refinement.\n\n\nTo this end, we propose VEnhancer, a generative space-time enhancement method that supports both spatial and temporal super-resolution with flexible space and time up-sampling scales, as well as has the ability to remove visual artifacts and flickering with good maintenance of video content. It is built upon a pretrained and fixed generative video prior (Zhang et al., 2023b), which supplies the generative ability for video enhancement. To condition the video generation on low-frame-rate and low-resolution videos, we design Space-Time Controller (ST-Controller) for effective conditioning in both spatial and time dimensions. Furthermore, to handle different up-sampling scales and reduce artifacts or flickering with different degrees, we propose a space-time data augmentation algorithm to construct the training data. In particular, at the training stage, we sample different step sizes for skipping frames, downscaling factors, and noise levels to synthesize diversified condition videos. To ensure the proposed ST-Controller be aware of the associated data augmentation applied to each input video, we propose the video-aware conditioning. In particular, for key frame, the condition latent, the embeddings of the associated downscaling factor  $s$  and noise level  $\\sigma$  by noise augmentation are incorporated into ST-Controller through video-aware conditioning.\n\n\nWith these designs, VEnhancer is a single end-to-end trainable network that can handle both spatial and temporal super-resolution, as well as video refinement. Moreover, it also supports arbitrary space and time up-sampling scales, and also supports flexible control on refinement strength and generative strength as user may prefer. Extensive experiments have demonstrated VEnhancer's ability in enhancing generated videos (see Figure 1). In these experiments, it outperforms state-of-the-art real-world and generative video super-resolution methods for spatial super-resolution only. In the space-time super-resolution, VEnhancer also surpasses state-of-the-art methods as well as cascaded spatial and temporal diffusion super-resolution models. At last, on the public video\n\n\ngeneration benchmark, VBenchmark (Huang et al., 2023), VEnhancer can significantly improve the overall performance of existing text-to-video algorithms.\n\n\nOur contributions can be summarized as below:\n\n- 1. We propose VEnhancer, a generative space-time enhancement method that can achieve generative spatial and temporal super-resolution for different upsampling factors, as well as controllable video refinement in one video diffusion model for the first time.\n\n- 2. To achieve the unified generative space-time enhancement, we devise ST-Controller for effective multi-frame condition injection based on a pretrained and fixed generative video prior. Besides, space-time data augmentation and the associated video-aware conditioning are proposed for training ST-Controller in an end-to-end manner.\n\n- 3. VEnhancer surpasses existing state-of-the-art video super-resolution methods and space-time super-resolution methods in enhancing generated videos. Also, it could improve the performance of open-source text-to-video methods on public video generation benchmark.\n\n\n# 2 RELATED WORK\n\n\n# 2.1VIDEO GENERATION\n\n\nRecently, there have been substantial efforts in training large-scale T2V (Wang et al., 2024; Ho et al., 2022; Guo et al., 2023; Chen et al., 2023c; Gupta et al., 2023; Blattmann et al., 2023b; Wang et al., 2023b;a) models on large scale datasets. Some works (Blattmann et al., 2023b; Wang et al., 2023b;a) inflate a pre-trained text-to-image (T2I) model by inserting temporal layers and fine-tuning them or all parameters on video data, or adopts a joint image-video training strategy. In order to achieve high-quality video generation, (Ho et al., 2022; Wang et al., 2023b; Blattmann et al., 2023b) adopts multi-stage pipelines. In particular, cascaded video diffusion models are designed: One T2V base model that is followed by one or more frame interpolation and video super-resolution models. VideoLDM (Blattmann et al., 2023b), LaVie (Wang et al., 2023b), and Upscale-A-Video (Zhou et al., 2023) all develop the video super-resolution model based on  $4 \\times$  sd (StableDiffusion (Rombach et al., 2022)) upscaler, which has an additional downsampled image for conditioning the generation. One drawback of this base model is losing quite a lot generative ability compared with T2I base models. On the contrary, I2VGEN-XL follows SDXL (Podell et al., 2023) and uses noisig-denoising process (Meng et al., 2021) to refine the generated artifacts. However, this strategy could improve stability but cannot increase the space-time resolution. VEnhancer is based on a generative video prior, and could address temporal/spatial super-resolution and refinement in a unified model.\n\n\n# 2.2VIDEO ENHANCEMENT\n\n\nVideo Super-Resolution. Video Super-Resolution (VSR) is proposed to enhance video quality by upsampling low-resolution (LR) frames into high-resolution (HR) ones. Traditional VSR approaches(Cao et al., 2021; Chan et al., 2021; 2022a; Isobe et al., 2020a;b;c; Liang et al., 2024; 2022; Wang et al., 2019; Xue et al., 2019) often rely on fixed degradation models to synthesize training data pairs, which leads to a noticeable performance drop in real-world scenarios. To bridge this gap, recent advances(Chan et al., 2022b; Xie et al., 2023) in VSR have embraced more diversified degradation models to better simulate real-world low-resolution videos. To achieve photo-realistic reconstruction, Upscale-A-Video(Zhou et al., 2023) integrates diffusion prior to produce detailed textures, upgrading VSR performance into next level. Space-Time Super-Resolution. Space Time Video Super-Resolution (STVSR) aims to simultaneously increase the resolutions of video frames in both spatial and temporal dimensions. Deep-learning based approaches(Haris et al., 2020; Kim et al., 2020; Xiang et al., 2020; Chen et al., 2022) have achieved remarkable results on STVSR. STARNet(Haris et al., 2020) increases the spatial resolution and frame rate by leveraging the mutual information between space and time. FISR(Kim et al., 2020) propose a joint framework with a multiscale temporal loss to upscale the spatial-temporal resolution of videos. (Xiang et al., 2020) proposes a one-stage STVSR framework, which incorporates different sub-modules for LR frame features interpolation, temporal information aggregation and HR reconstruction. VideoINR(Chen et al., 2022) utilize the continuous video representation to achieve STVSR at arbitrary spatial resolution and frame rate. Although these methods obtain smooth and high-resolution output videos, but they fail in generating realistic texture details.\n\n\n![Figure 2: The overall framework. It consists of space-time data augmentation for constructing training data, and the associated video-aware conditioning for realizing diversified conditions across frames, as well as ST-Controller for multi-frame condition injection based on generative video prior.](images/cfc7e1100181230ee15b12543b6903001724745994470e9b265521b3f33347bf.jpg)\n\n\n# 3 PRELIMINARIES:VIDEO DIFFUSION MODELS\n\n\nOur method is built on a pretrained video diffusion model (Zhang et al., 2023b), which is developed based on Stable Diffusion 2.1. Given an video  $\\mathbf{x} \\in \\mathbb{R}^{F \\times H \\times W \\times 3}$ , the encoder  $\\mathcal{E}$  first encodes it into latent representation  $\\mathbf{z} = \\mathcal{E}(\\mathbf{x})$  frame-by-frame, where  $\\mathbf{z} \\in \\mathbb{R}^{F \\times H' \\times W' \\times C}$ . Then, the forward diffusion and reverse denoising are conducted in the latent space. In the forward process, the noise is gradually added to the latent vector  $\\mathbf{z}$  in total  $T$  steps. And for each time-step  $t$ , the diffusion process is formulated as follows:\n\n\n$$\n\n$$\n\\mathbf {z} _ {t} = \\alpha_ {t} \\mathbf {z} + \\sigma_ {t} \\boldsymbol {\\epsilon}, \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\epsilon \\in \\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , and  $\\alpha_{t}$ ,  $\\sigma_{t}$  specify the noise schedule in which the corresponding log signal-to-noise-ratio  $(\\log [\\alpha_t^2 /\\sigma_t^2 ])$  decreases monotonically with  $t$ . And at time-step  $T$ ,  $q(\\mathbf{z}_T) = \\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . As for backward pass, a diffusion model is used for iteratively denoising under the guidance of the text prompt  $c_{text}$ . By adopting v-prediction parameterization (Salimans & Ho, 2022), the U-Net denoiser  $f_{\\theta}$  learns to make predictions of  $\\mathbf{v}_t\\equiv \\alpha_t\\pmb {\\epsilon} - \\sigma_t\\mathbf{z}$ . The optimization objective is simply formulated as:\n\n\n$$\n\n$$\n\\mathcal {L} _ {L D M} = \\mathbb {E} _ {\\mathbf {z}, c _ {t e x t}, \\boldsymbol {\\epsilon} \\sim \\mathcal {N} (\\mathbf {0}, \\mathbf {I}), t} \\left[ \\| \\mathbf {v} - f _ {\\theta} (\\mathbf {z} _ {t}, t, c _ {t e x t}) \\| _ {2} ^ {2} \\right]. \\tag {2}\n$$\n\n$$\n\n\nAt the end, the generated videos are obtained through the VAE decoder:  $\\hat{\\mathbf{x}} = \\mathcal{D}(\\mathbf{z})$ .\n\n\n# 4 METHODOLOGY\n\n\nIn this section, we introduce the main components of our method. The overall framework is illustrated in Figure 2. First, we present our architecture design in Section 4.1. Then we elaborate on the proposed space-time data augmentation in Section 4.2. In Section 4.3, we give a detailed description on our designed video-aware conditioning.\n\n\n# 4.1 ARCHITECTURE DESIGN\n\n\nThe architecture is designed based on a pretrained video diffusion model. This video diffusion model is able to generate temporal-coherent content and high-quality texture details through iterative denoising. To upsample and refine a low-frame-rate and low-resolution videos in both spatial and temporal dimensions, the visual information should be incorporated into the video diffusion model carefully in order to obtain high-quality results with good fidelity to the input videos. High-quality generated videos stem from powerful generative models, while fidelity requires the algorithm to preserve the visual information of the input. Inspired by (Zhang et al., 2023a), we keep the pretrained video diffusion model untouched for preserving generative capability, and create a Space-Time Controller (ST-Controller) to obtain effective multi-frame condition injection for generative video enhancement. The architecture is illustrated in Figure 3.\n\n\nThe pretrained video diffusion model follows the design of stacking a sequence of interleaved spatial and temporal layers within the 3D-UNet (Blattmann et al., 2023b) architecture (gray blocks in Figure 3). Specifically, each spatial convolution layer (or attention layer) is followed by a temporal\n\n\nconvolution layer (or attention layer). The spatial layers are the same as those in Stable Diffusion 2.1, including ResBlocks (He et al., 2016), self-attention (Vaswani et al., 2017) layers, and cross-attention layers. The temporal convolution and attention layers are incorporated with their output layers initialized to zero and finetuned with video datasets. Specifically, the temporal convolution is one-dimensional convolution layer with a kernel size of 3, and the temporal attention is one-dimensional attention layer (Wang et al., 2023a). In this 3D-UNet, the video features that aligned by temporal layers in encoder will be skipped to the decoder, in which concatenation operation will be performed to combine skipped features with decoder features.\n\n\nTo build our proposed ST-Controller, we make a copy (both the architectures and weights) of the multi-frame encoder and middle block in 3D-UNet (orange blocks in Fig. 3) as the trainable condition network. This condition network takes low-frame-rate and low-resolution condition latents as well as full frames of noisy latents as inputs. Specifically, the condition latents and the associated augmentation parameters are incorporated into the condition network through our proposed video-aware conditioning. The output multi-scale temporal-coherent video features will be injected into the original 3D-UNet through newly added zero convolutions (yellow blocks in Fig. 3). The output features of the middle block in condition network will be added\n\n\n![Figure 3: The architecture of VEnhancer.](images/38e6f2747369dddd271745c1732923ba878e98f209fb9648d71000892b83e917.jpg)\n\n\nback to the features of the middle block in 3D-UNet. While for output features of encoder blocks in condition network, their features will be added to the skipped video features in 3D-UNet, which are also produced by encoder blocks. The copied condition network, video-aware conditioning, and the newly added zero convolutions are trained simultaneously.\n\n\n# 4.2 SPACE-TIME DATA AUGMENTATION\n\n\nIn this section, we discuss about how to achieve unified space-time super-resolution with arbitrary up-sampling space and time scales, as well as refinement with varying degrees. To this end, we propose a novel data augmentation strategy for both space and time axes. Details are discussed below.\n\n\nTime axis. Given a sequence of high-frame-rate and high-resolution video frames  $I^{1:f} = [I^1, I^2, \\dots, I^f]$  with frame length  $f$ , we use a sliding window across time axis to select frames. The frame sliding window size  $m$  is randomly sampled from a predefined set, ranging from 1 to 8. This corresponds to time scales from  $1 \\times 8 \\times$ . Note that  $1 \\times$  time scale requires no frame interpolation, thus the multi-task problem downgrades to video super-resolution. After the frame skipping, we obtain a sequence of key frames  $I^{1:m:f} = [I^1, I^{1+m}, I^{1+2 \\times m}, \\dots, I^f]$ .\n\n\nSpace axis. Then, we perform spatial downsampling for these obtained key frames. Specifically, the downscaling factor  $s$  is randomly sampled from [1, 8], which represents  $1 \\times \\sim 8 \\times$  space super-resolution. When  $s = 1$ , there is no need to perform spatial super-resolution. All frames in one sequence are downsampled with the same downscaling factor  $s$ . Thus, we arrive at low-frame-rate and low-resolution video frames:  $I_{\\downarrow s}^{1:m:f}$ . In practice, we should upsample them back to the original spatial sizes by bilinear interpolation before being passed to the networks, so we obtain  $I_{\\downarrow s,\\uparrow s}^{1:m:f}$ . Note that each space or time scale corresponds to different difficulty level, and thus the sampling is not uniform. Particularly, we set sampling probabilities of scales  $4 \\times$  and  $8 \\times$  based on a ratio of  $1:2$ , which is determined by their associated scale values.\n\n\nThen, we use the encoder part of a pretrained variational autoencoder (VAE)  $\\mathcal{E}$  to project the input sequence to the latent space frame-wisely:\n\n\n$$\n\n$$\nz _ {s} ^ {1: m: f} = \\left[ \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {1}\\right), \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {1 + m}\\right), \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {1 + 2 \\times m}\\right), \\dots , \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s} ^ {f}\\right) \\right]. \\tag {3}\n$$\n\n$$\n\n\nNoise augmentation in latent space. At this stage, we conduct noise augmentation to noise the latent condition information in varying degrees in order to achieve controllable refinement. This noise\n\n\naugmentation process is the same as the diffusion process equation 1 used in the video diffusion model. Specifically, the condition latent sequence is corrupted by:\n\n\n$$\n\n$$\nz _ {s, t ^ {\\prime}} ^ {1: m: f} = \\alpha_ {t ^ {\\prime}} z _ {s} ^ {1: m: f} + \\sigma_ {t ^ {\\prime}} \\epsilon^ {1: m: f}, \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\alpha_{t'}$ ,  $\\sigma_{t'}$  determine the signal-to-noise-ratio at time-step  $t'$ , and  $t' \\in \\{1, \\dots, T'\\}$ . Note that the pretrained video diffusion model adopts 1,000 steps ( $T = 1000$  in equation 1). While the noise augmentation only needs to corrupt the low-level information,  $T'$  is set to 300 empirically. For more intuitive denotation, we use  $\\sigma$  instead of  $t'$ . Finally, we arrive at  $z_{s,\\sigma}^{1:m:f} = \\mathcal{E}(I_{\\downarrow s, \\uparrow s})_{\\sigma}^{1:m:f}$ .\n\n\nThe whole process of space-time data augmentation is summarized as follows:\n\n\n$$\n\n$$\nI ^ {1: f} \\rightarrow I ^ {1: m: f} \\rightarrow I _ {\\downarrow s} ^ {1: m: f} \\rightarrow I _ {\\downarrow s, \\uparrow s} ^ {1: m: f} \\rightarrow \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s}\\right) _ {\\sigma} ^ {1: m: f} \\rightarrow \\mathcal {E} \\left(I _ {\\downarrow s, \\uparrow s}\\right) _ {\\sigma} ^ {1: m: f}. \\tag {5}\n$$\n\n$$\n\n\n# 4.3 VIDEO-AWARE CONDITIONING\n\n\nBesides data augmentation, the corresponding conditioning mechanism should also be designed in order to influence the model training and avoid averaging performance for different space or time scales and noise augmentation. In practice, the condition latent sequence  $z_{s,\\sigma}^{1:m:f}$ , the corresponding downscaling factor  $s$ , and augmented noises  $\\sigma$  are all considered as for conditioning. Please refer to Figure. 4 for more intuitive demonstration.\n\n\nGiven the synthesized condition latent sequence  $z_{s,\\sigma}^{1:m:f}$ , we use one convolution with zero-initialization  $-\\mathrm{Conv}_{\\mathrm{zero}}$  for connecting it to the condition network. Specifically, we have:\n\n\n$$\n\n$$\nf _ {o u t} ^ {1: f} = \\operatorname {C o n v} \\left(z _ {t} ^ {1: f}\\right), \\tag {6}\n$$\n\n$$\n\n\n$$\n\n$$\nf _ {o u t} ^ {1: m: f} = \\operatorname {C o n v} (z _ {t} ^ {1: m: f}) + \\operatorname {C o n v} _ {\\mathrm {z e r o}} (z _ {s, \\sigma} ^ {1: m: f}), (7)\n$$\n\n$$\n\n\nwhere Conv is the first convolution in the condition network,  $z_{t}^{1:f}$  and  $z_{t}^{1:m:f}$  denote the full frames and key frames of noisy latents at timestep  $t$ , respectively. Note that Conv and Convzero share the same hyperparameter configuration (i.e., kernel size, padding, et.al.), As it is shown, only key-frame features in condition network will be added with the condition features, while others remain unchanged. This strategy enables progressive condition injection as the weights of Convzero grows from zero starting point.\n\n\n![](images/5aaf7915b8f99b325432428416afb94b71918cde7f2ed9e28519c81bebf15ef4.jpg)\n\n\n![Figure 4: Video-aware conditioning. For frame that has condition image as input (key frame), we add the condition latent to the noisy latent after one convolution layer. Besides, the embeddings of noise level  $\\sigma$  and downscaling factor  $s$  are added to the existing  $t$  embedding, which will be broadcast to all spatial layers.](images/d9ed791e28f11031d6c5876e43244328fb8379d29f22c0ab3c4517544011bc84.jpg)\n\n\nFor conditioning regarding downscaling factor  $s$  and noise augmentation  $\\sigma$ , we incorporate them to the existing time embedding in the condition network. Specifically, for timestep  $t$ , sinusoidal encoding (Ho et al., 2020; Rombach et al., 2022; Vaswani et al., 2017) is used to provide the model with a positional encoding for time. Then, one MLP (two linear layers with a SiLU (Elfwing et al., 2018) activation layer in between) is applied. Specifically, we have:\n\n\n$$\n\n$$\nt _ {e m b} = \\mathrm {M L P} _ {t} (\\text {S i n u s o i d a l} (t)), \\quad t _ {e m b} ^ {1: f} = \\text {R e p e a t} (t _ {e m b}, f), \\tag {8}\n$$\n\n$$\n\n\nwhere  $t_{emb}^{1:f}$  is obtained by Repeat  $t_{emb}$  by  $f$  times in the frame axis. This time embedding sequence will be broadcast to all ResBlocks in the condition network for timestep injection.\n\n\nAlso, we elucidate the conditioning for noise augmentation. As mentioned in equation 4, noise augmentation shares the same way as diffusion process, but with much smaller maximum timestep (i.e.,  $T' = 300$ ). Thus, we reuse the encoding and mapping for timestep  $t$  in diffusion process. After this, we add a linear layer with zero initialization (denoted as Linear_zero). To conclude, we have:\n\n\n$$\n\n$$\n\\sigma_ {e m b} = \\operatorname {L i n e a r} _ {\\text {z e r o}, \\sigma} (\\mathrm {M L P} _ {t} (\\text {S i n u s o i d a l} (\\sigma)). \\tag {9}\n$$\n\n$$\n\n\nTo achieve video-aware conditioning, we add  $\\sigma_{emb}$  only to the key frames. We repeat  $\\sigma_{emb}$  by  $k$  times to obtain  $\\sigma_{emb}^{1:k}$ , where  $k$  is the number of key frames. The video-aware controlling is presented as follows:\n\n\n$$\n\n$$\nt _ {e m b} ^ {1: m: f} = t _ {e m b} ^ {1: m: f} + \\sigma_ {e m b} ^ {1: k}, \\tag {10}\n$$\n\n$$\n\n\nwhere the addition operation is performed frame-wisely.\n\n\nRegarding downscaling factor  $s$ , the corresponding encoding, mapping and controlling are similar as above. In particular, we newly introduce one  $\\mathrm{MLP}_{\\mathrm{zero}}$ , in which the output layer is zero-initialized. The video-aware conditioning is performed as:\n\n\n$$\n\n$$\ns _ {e m b} = \\mathrm {M L P} _ {\\text {z e r o}, s} (\\text {S i n u s o i d a l} (s)), \\quad s _ {e m b} ^ {1: k} = \\text {R e p e a t} \\left(s _ {e m b}, k\\right), \\tag {11}\n$$\n\n$$\n\n\n$$\n\n$$\nt _ {e m b} ^ {1: m: f} = t _ {e m b} ^ {1: m: f} + s _ {e m b} ^ {1: k}. \\tag {12}\n$$\n\n$$\n\n\nWith our proposed space-time data augmentation and video-aware conditioning, VEnhancer can be well-trained in an end-to-end manner, and yields great performance in handling diversified conditions for generative enhancement. Here we provide a demonstration in Figure 5. For  $4 \\times$  video superresolution, we modify the input downscaling factor  $s$  to produce different results. It is shown that more texture details are generated as  $s$  grows (from smooth to sharp). This indicates that  $s$  can determine how many details are generated through our proposed video-aware conditioning.\n\n\n![Figure 5: The effectiveness of video-aware conditioning. For video super-resolution  $(4\\times)$ , we modify the input downscaling factor from  $s = 1$  to  $s = 8$ , and more texture details are generated. Zoom in for best view.](images/bbdfbce7c4a22214f43db95b7df6126becf0fbf10147fa3c723ddc4f785166c6.jpg)\n\n\n# 5 EXPERIMENTS\n\n\nDatasets. We collect around 350k high-quality and high-resolution video clips from Panda-70M (Chen et al., 2024b) dataset and the Internet to constitute our training set. We train VEnhancer on resolution  $1280 \\times 720$  with center cropping, and the target FPS is fixed to 24 by frame skipping. Regarding test dataset, we collect generated videos from state-of-the-art text-to-video methods. Practically, we select videos with large motions and diverse contents. This test dataset is denoted as AIGC2023, which is used to evaluate VEnhancer and baselines for video super-resolution and space-time super-resolution tasks. For evaluation on VBench, all generated videos based on the provided prompt suite are considered, resulting in around 5k videos.\n\n\nImplementation Details. The batch size is set to 256. AdamW (Loshchilov & Hutter, 2017) is used as the optimizer, and the learning rate is set to  $10^{-5}$ . During training, we dropout the text prompt with a probability of  $10\\%$ . The training process lasts about four days with 16 NVIDIA A100 GPUs. During inference, we use DPM-Solver (Lu et al., 2022) and perform 15 sampling steps with classifier-free guidance ( $cfg$ ) (Ho & Salimans, 2022).\n\n\nMetrics. Regarding evaluation for video super-resolution and space-time super-resolution on AIGC2023 test dataset, we use both image quality assessment (IQA) and video quality assessment (VQA) metrics. Specifically, MUSIQ (Ke et al., 2021) and DOVER (Wu et al., 2023a) are adopted. Moreover, we refer to video generation benchmark, VBench (Huang et al., 2023), for more comprehensive evaluation. Specifically, we choose Dynamic Degree (i.e., whether it contains large motions), Motion smoothness (i.e., how smooth the video is), and Aesthetic Quality for evaluation. Regarding evaluation for video generation, we consider all 16 evaluation dimensions from VBench.\n\n\n# 5.1 COMPARISON WITH VIDEO SUPER-RESOLUTION METHODS\n\n\nFor video super-resolution, VEnhancer is compared with the state-of-the-art real-world video super-resolution method, RealBasicVSR (Chan et al., 2022b), and the state-of-the-art generative video super-resolution method, LaVie-SR (Wang et al., 2023b) (super-resolution). For more comprehensive comparison, we also include I2VGen-XL' (Zhang et al., 2023b) refinement model as our baseline.\n\n\nTable 1: Quantitative comparison for video super-resolution  $(4\\times)$  on AIGC2023 test dataset. Red and blue indicate the best and second best performance. The top 3 results are marked as gray .\n\n\n<table><tr><td></td><td>DOVER\u2191</td><td>MUSIQ\u2191</td><td>Aesthetic Quality</td><td>Dynamic Degree</td><td>Motion Smoothness</td></tr><tr><td>LaVie-SR Wang et al. (2023b)</td><td>0.8427</td><td>55.8428</td><td>0.6692</td><td>0.525</td><td>0.9710</td></tr><tr><td>I2VGen-XL(refiner) Zhang et al. (2023b)</td><td>0.5603</td><td>25.5988</td><td>0.6439</td><td>0.475</td><td>0.9835</td></tr><tr><td>RealBasicVSR Chan et al. (2021)</td><td>0.8252</td><td>50.5978</td><td>0.6622</td><td>0.550</td><td>0.9729</td></tr><tr><td>Ours</td><td>0.8586</td><td>59.4474</td><td>0.6671</td><td>0.550</td><td>0.9781</td></tr></table>\n\n\n![](images/68e212d6ede20b5fcc60370d46fc8352480f2a346b5fc36ebc312146e7237456.jpg)\n\n\n![](images/d0e3a93b88cf91779d1c20c936b13a1bca7170438abf218fc111cf49ce70a739.jpg)\n\n\n![](images/f69e82847ccd0fd9384f3ed01fc2427d09d42343e96b0ee431d4d0e3dc240e36.jpg)\n\n\n![](images/0107f12fa51c2d5f1539b887d6c69bc67b13e2897908e64bf0c5f99af730c6c6.jpg)\n\n\n![](images/994c105e78bbd064d79ff09a4736d96a9c06bb4c5b7369a9450492e314431d05.jpg)\n\n\n![](images/89d64654194c475ed1b98b5888ddfdbfa4a8209d538bd35a337783f256244af1.jpg)\n\n\n![](images/319a0e197cd5eb6bc76ec742fc077eadd20e66f25a188381a8600828a8c303fc.jpg)\n\n\n![](images/1f88cbf19123aaf61ceee5c3d20b7781adaed67a3973781c5e437f43f38417cb.jpg)\n\n\n![](images/3061d7be88193fe28c8e5d2257c99672679dd3ec03f19022b56e0bce9def7eac.jpg)\n\n\n![](images/bfea63d696a3dd7e0b04f5410095b318f780ec1bf4ee945dc441d8a55460ae1a.jpg)\n\n\n![](images/e6400c8db0e7370145f3c04a1f0c0e4d31ce838c5e31348d0eae1fe9130b29a8.jpg)\n\n\n![](images/a03cdaeee5bbb84cdb98982b806177301f17ffa60e928a5c05671570d168e7d4.jpg)\n\n\n![Figure 6: Visual comparison for video super-resolution  $(4\\times)$  on AIGC2023 test dataset. Input resolution:  $512\\times 312$ ; output resolution:  $2048\\times 1280$ . Prompt: Iron Man flying in the sky.](images/b74b072b25ab349085849feda257baacb69a05aa33f19cbeb63949320838aa51.jpg)\n\n\n![](images/025a0d34e2585e5b86fcfeeffb0519efa261ac8856f6cef60655a04b3d48cdab.jpg)\n\n\n![](images/b2394c6de2ab73a2d02f7cd5f7c782f093127709ccbe687af58b6d549fc21c76.jpg)\n\n\nAs shown in Table 1, VEnhancer outperforms both generative video super-resolution method (LaVie-SR), real-world video super-resolution method (RealBasicVSR), and generative video refinement method (I2VGen-XL, refiner) in most metrics, suggesting its outstanding enhancement ability for videos. Note that LaVie-SR surpasses RealBasicVSR in image/video quality (MUSIQ, DOVER, and Aesthetic Quality), as diffusion-based methods are better at generating sharp details. But LaVie-SR achieves the worst Motion Smoothness, indicating its insufficient capability in balancing video smoothness and video quality. I2VGen-XL's refiner could obtain the highest score in Motion Smoothness, but sacrifices the magnitude of motion significantly (worst Dynamic Degree). Moreover, it achieves very unsatisfactory results in metrics evaluating image and video quality. Because it uses bilinear interpolation for  $\\times 4$  upsampling, which produces very blurry results. Nevertheless, VEnhancer achieves overall best results with good balance among image/video quality, motion smoothness, and motion magnitude.\n\n\nThe visual comparison is presented in Figure. 6. The prompt is \"Iron man flying in the sky\". The input video is already consistent with the prompt, but lacks details on the iron man suit. RealBasicVSR could remove some noises or artifacts of the generated videos as it incorporates complex degradation for model training. However, it fails in generating realistic details but produces over-smoothed results, since its generative ability is limited. On the other hand, the results of LaVie-SR contains more artifacts than input. Without successfully removing artifacts, the generative super-resolution model will enlarge the existing defects. The refiner of I2VGen-XL could achieve successful refinement but produces over-smoothed results. In contrast, VEnhancer could first remove unpleasing artifacts and refine the distorted content (e.g., head region), and then generate faithful details (e.g., helmet and armor) that are consistent with the text prompt.\n\n\n# 5.2 COMPARISON WITH SPACE-TIME SUPER-RESOLUTION METHODS\n\n\nFor space-time super-resolution task, we compare two state-of-the-art space-time super-resolution methods: VideoINR (Chen et al., 2022) and Zooming-Slow-Mo (Xiang et al., 2020) (Zoom for short). We also consider LaVie's cascaded T-SR and S-SR DM-based pipeline: LaVie-FI (frame interpolation) + LaVie-SR (super-resolution) for more thorough comparison.\n\n\nTable 2: Quantitative comparison for space-time super-resolution  $(4\\times)$  on AIGC2023 test dataset. Red and blue indicate the best and second best performance. The top 3 results are marked as gray .\n\n\n<table><tr><td></td><td>DOVER\u2191</td><td>MUSIQ\u2191</td><td>Aesthetic Quality</td><td>Dynamic Degree</td><td>Motion Smoothness</td></tr><tr><td>LaVie-FI + LaVie-SR Wang et al. (2023b)</td><td>0.8159</td><td>53.2128</td><td>0.6566</td><td>0.60</td><td>0.9857</td></tr><tr><td>VideoINR Chen et al. (2022)</td><td>0.7608</td><td>34.1060</td><td>0.6624</td><td>0.60</td><td>0.9933</td></tr><tr><td>Zooming Slow-Mo Xiang et al. (2020)</td><td>0.7328</td><td>33.8470</td><td>0.6624</td><td>0.55</td><td>0.9908</td></tr><tr><td>Ours</td><td>0.8609</td><td>51.2940</td><td>0.6710</td><td>0.60</td><td>0.9937</td></tr></table>\n\n\nAs shown in Table 2, we observe that our method surpasses all baselines in DOVER and Aesthetic Quality, showing its superior capability in generating sharp and realistic video content. Besides, it obtains highest scores in Motion Smoothness and Dynamic Degree, indicating VEnhancer's excellent ability in synthesizing stable temporal details. The cascaded T-SR and S-SR approach (LaVie-FI + LaVie-SR) obtains good scores in DOVER and MUSIQ, demonstrating DM-based methods' advantage in generation. However, its performance in temporal aspect is unsatisfactory due to its inferior capability in temporal refinement. We notice that state-of-the-art space-time super-resolution methods (VideoINR and Zooming Slow-Mo) behave well in Motion Smoothness. As both of them are optimized with reconstruction loss, the produced results are very smooth across frames. At a cost, they perform poorly in metrics regarding quality, such as DOVER and MUSIQ.\n\n\nThe visual comparison is illustrated in Figure. 7. The first and third columns present the low-resolution key frames. Note that the input frames are not consistent especially in the region of guitar strings. The cascaded T-SR and S-SR approach, LaVie-FI + LaVie-SR, can produce very sharp results for all frames (key and predicted ones). However, it generates messy contents which are not semantically aligned with prompt. Moreover, the generated details are changing across time, indicating severe flickering. For reconstruction-based methods (VideoINR and Zoom), the produced results are similar: lacking details and failing in improving the consistency of the original input frames. On the contrary, VEnhancer is not only able to achieve unified space-time super-resolution, but can also improve the temporal consistency of the generated videos by refinement (i.e., guitar strings and raccoon hands).\n\n\n# 5.3 EVALUATION ON IMPROVING VIDEO GENERATION\n\n\nHere we evaluate VEnhancer's ability in improving state-of-the-art T2V methods. The baselines include open-source T2V methods-VideoCrafter-2 (Chen et al., 2024a) (VC-2 for short), Lavie (Wang et al., 2023b), Open-Sora, CogVideoX (Yang et al., 2024), and professional video generation products-Pika and Gen-2. In particular, we enhance T2V results of CogVideoX-5B and VC-2.\n\n\nThe quantitative results are organized in Table 3. Before enhancement, CogVideoX-5B and VC-2 achieve the best and second best in Semantic compared with other baselines, demonstrating their superiority in generating video contents that are highly consistent to the VBenchmark's prompt suite. Regarding Quality, they lag behind Pika. But with VEnhancer, VC-2 and CogVideoX-5B are able to achieve the highest and second highest scores in Quality. Besides, their scores regarding Semantic improves a lot, especially VC-2' (3.31% increase). This indicates that VEnhancer can improve the\n\n\nTable 3: VBenchmark Evaluation Results. This table compares the performance of open-source T2V methods and professional video generation products regarding two aspects (Quality and Semantic). A higher score indicates better performance. Red and blue indicate the best and second best performance. The top 3 results are marked as gray .\n\n\n<table><tr><td></td><td>LaVie\nWang et al. (2023b)</td><td>Open-Sora</td><td>Pika</td><td>Gen-2</td><td>CogVideoX-5B\nYang et al. (2024)</td><td>VC-2\nChen et al. (2024a)</td><td>CogVideoX-5B\n+Venhancer</td><td>VC-2\n+VEnhancer</td></tr><tr><td>Quality</td><td>78.78%</td><td>80.71%</td><td>82.92%</td><td>82.47%</td><td>82.75%</td><td>82.20%</td><td>82.99%</td><td>83.28%</td></tr><tr><td>Semantic</td><td>70.31%</td><td>73.30%</td><td>71.77%</td><td>73.03%</td><td>77.04%</td><td>73.42%</td><td>77.52%</td><td>76.73%</td></tr><tr><td>Overall</td><td>77.08%</td><td>79.23%</td><td>80.69%</td><td>80.58%</td><td>81.61%</td><td>80.44%</td><td>81.90%</td><td>81.97%</td></tr></table>\n\n\nsemantic content and video quality at the same time, showing a powerful generative enhancement ability. More importantly, the advantage of adopting a two-stage pipeline is observed: the first T2V model focuses on generating semantic content and motions with good fidelity to the prompts, while the following enhancement model can improve the semantic in low-level and image quality, as well as temporal consistency. For visual results, please see the video demonstration in supp.\n\n\n# 5.4 ABLATION STUDIES\n\n\nThe Effectiveness of Noise Augmentation. During training, the noise level regarding noise augmentation is randomly sampled within a predefined range. While during inference, one can change the noise level to achieve refinement with different strengths. In general, higher noise corresponds to stronger refinement and regeneration. We present the visual comparison among different noise levels in Figure 8. The first frame of one AI-generated video is presented in the left. It is of low-resolution and lacks details. Also, the original video has very obvious flickering. If we set  $\\sigma = 0$ , VEnhancer will generate unpleasant noises in the background. As there is domain mismatch between the training data and testing data, the enhancement fails in handling unseen and challenging scenarios. Fortunately, we can mitigate this by adding noise in the condition latents for corrupting the noisy and unknown low-level details. As we increase the noise level, the artifacts are gradually vanishing. When  $\\sigma = 250$ , the result is noise-clean, and has abundant semantic details.\n\n\nArbitrary Up-sampling Scales for Spatial Super-Resolution. Here we show that VEnhancer is able to up-sample videos with arbitrary scales. From Figure 9, we observe that VEnhancer could produce satisfactory results on different scales  $(2.5\\times,3\\times,3.5\\times,4\\times,$  and  $4.5\\times)$ , suggesting its flexibility and generalization in adapting to different tasks. In particular, given one frame of the generated video  $(312\\times 512)$ , VEnhancer could improve the generated details when the up-sampling scale grows up. When  $s = 2.5\\sim 3.5$ , the panda's hand is less realistic. But it becomes better when  $s = 4$  or  $s = 4.5$ . It is also noticed that the panda' fur is becoming more realistic as  $s$  grows.\n\n\nArbitrary Up-sampling Scales for Temporal Super-Resolution. In this part, we show VEnhancer is able to achieve arbitrary up-sampling in time axis. Given two low-resolution key frames, we aim to up-sample them to high-resolution ones, and also interpolate several frames (ranging from 2 to 4) between them. As shown in Figure 10, the results are consistent across frames, showing not flicking or distortions. Besides, the spatial quality has also been significantly improved. As shown in the last row,  $5 \\times$  frame interpolation yields smooth frames with generated contents: the shadow in the right leg is changing, showing a very natural transition. This indicates that diffusion-based frame interpolation has great capability in both motion and content generation.\n\n\n# 6 CONCLUSION AND LIMITATION\n\n\nIn this work, we propose a generative space-time enhancement method - VEnhancer for video generation. It can achieve spatial super-resolution, temporal super-resolution and video refinement in one video diffusion model. We base on a pretrained generative video prior and build a Space-Time Controller (ST-Controller) for effective condition injection. Space-time data augmentation and video-aware conditioning are proposed to train ST-Controller in an end-to-end manner. Extensive experiments have demonstrated our superiority over state-of-the-art video super-resolution and space-time super-resolution methods in enhancing AI-generated videos. However, our work has several limitations. First, as it is based on diffusion models, the inference takes more time than one-step methods. Second, it may face challenges in handling AI-generated long videos, since the long-term (over 10s) consistency has not been addressed in this work.\n\n\n# REFERENCES\n\n- Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021.\n\n- Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023a.\n\n- Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22563-22575, 2023b.\n\n- Jiezhang Cao, Yawei Li, Kai Zhang, and Luc Van Gool. Video super-resolution transformer. arXiv preprint arXiv:2106.06847, 2021.\n\n- Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Basicvsr: The search for essential components in video super-resolution and beyond. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4947-4956, 2021.\n\n- Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Basicvsr++: Improving video super-resolution with enhanced propagation and alignment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5972-5981, 2022a.\n\n- Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Investigating tradeoffs in real-world video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5962-5971, 2022b.\n\n- Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023a.\n\n- Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocaster2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024a.\n\n- Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-\u03b1: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023b.\n\n- Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Genton: Delving deep into diffusion transformers for image and video generation. arXiv preprint arXiv:2312.04557, 2023c.\n\n- Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479, 2024b.\n\n- Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vedit Goel, Zhangyang Wang, Humphrey Shi, and Xiaolong Wang. Videoinr: Learning video implicit neural representation for continuous space-time super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2047-2057, 2022.\n\n- Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:3-11, 2018.\n\n- Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024.\n\n- Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023.\n\n- Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos\u00e9 Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023.\n\n- Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita. Space-time-aware multi-resolution video enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2859-2868, 2020.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024.\n\n- Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n\n- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020.\n\n- Jonathan Ho, William Chan, Chitwan Sahara, Jay Whang, Ruiqi Gao, Alexey Gritsanko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.\n\n- Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. arXiv preprint arXiv:2311.17982, 2023.\n\n- Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, and Qi Tian. Video superresolution with recurrent structure-detail network. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XII 16, pp. 645-660. Springer, 2020a.\n\n- Takashi Isobe, Songjiang Li, Xu Jia, Shanxin Yuan, Gregory Slabaugh, Chunjing Xu, Ya-Li Li, Shengjin Wang, and Qi Tian. Video super-resolution with temporal group attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8008-8017, 2020b.\n\n- Takashi Isobe, Fang Zhu, Xu Jia, and Shengjin Wang. Revisiting temporal modeling for video super-resolution. arXiv preprint arXiv:2008.05765, 2020c.\n\n- Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5148-5157, 2021.\n\n- Soo Ye Kim, Jihyong Oh, and Munchurl Kim. Fisr: Deep joint frame interpolation and superresolution with a multi-scale temporal loss. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 11278-11286, 2020.\n\n- Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu Timofte, and Luc V Gool. Recurrent video restoration transformer with guided deformable attention. Advances in Neural Information Processing Systems, 35:378-393, 2022.\n\n- Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Rakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van Gool. Vrt: A video restoration transformer. IEEE Transactions on Image Processing, 2024.\n\n- Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Wanli Ouyang, Yu Qiao, and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior, 2024.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\n- Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022.\n\n- Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.\n\n- Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.\n\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684-10695, 2022.\n\n- Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\n- Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023a.\n\n- Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. Edvr: Video restoration with enhanced deformable convolutional networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 0-0, 2019.\n\n- Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023b.\n\n- Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In International Conference on Computer Vision (ICCV), 2023a.\n\n- Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7623-7633, 2023b.\n\n- Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P Allebach, and Chenliang Xu. Zooming slow-mo: Fast and accurate one-stage space-time video super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3370-3379, 2020.\n\n- Liangbin Xie, Xintao Wang, Shuwei Shi, Jinjin Gu, Chao Dong, and Ying Shan. Mitigating artifacts in real-world video super-resolution models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 2956-2964, 2023.\n\n- Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. Video enhancement with task-oriented flow. International Journal of Computer Vision, 127:1106-1125, 2019.\n\n- Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024.\n\n- Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3836-3847, 2023a.\n\n- Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023b.\n\n- Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-video: Temporal-consistent diffusion model for real-world video super-resolution. arXiv preprint arXiv:2312.06640, 2023.\n\nResponse: \\boxed{Accept}\n\n**Rationale:**\n\nThis paper presents a significant advancement in video generation and enhancement, specifically focusing on improving the spatial and temporal resolution of AI-generated videos. Here\u2019s a detailed breakdown of why I predict it would be accepted:\n\n### Originality:\nThe paper introduces a novel method called VEnhancer, which combines spatial and temporal super-resolution in a single generative model. This is innovative because it addresses the redundancy and limitations of existing methods that typically separate spatial and temporal enhancements into distinct models. Additionally, it introduces a unique approach to handling different up-sampling scales and refinement strengths, which is crucial for flexibility and adaptability in various application scenarios.\n\n### Methodological Rigor:\nThe authors provide a comprehensive explanation of their methodology, including the architecture design, data augmentation strategies, and the video-aware conditioning mechanism. They clearly describe the training process, including the space-time data augmentation and the video-aware conditioning. The experimental setup, including the dataset collection, implementation details, and evaluation metrics, are well-defined, ensuring replicability and credibility.\n\n### Significance of Contribution:\nThe paper makes a strong case for the importance of its contribution. It demonstrates that VEnhancer outperforms state-of-the-art methods in both video super-resolution and space-time super-resolution tasks. The improvement in metrics like DOVER, MUSIQ, Aesthetic Quality, Dynamic Degree, and Motion Smoothness is notable, showcasing its effectiveness in generating high-quality, visually appealing videos. The method\u2019s ability to handle diverse up-sampling scales and refinement strengths further enhances its practical utility.\n\n### Clarity of Presentation:\nThe paper is well-organized and easy to follow. The abstract succinctly captures the essence of the work, and the introduction provides context and motivation for the research. The methodology section is detailed and clear, explaining each component of the VEnhancer architecture and its training process. The experimental section is well-presented, detailing the datasets, metrics, and results comprehensively.\n\n### Alignment with Journal Scope:\nICLR is known for publishing research in machine learning, particularly on generative models and their applications. The paper fits well within this scope, addressing the challenges of generating high-quality, coherent videos from text descriptions. The focus on both spatial and temporal super-resolution, as well as the ability to refine and enhance AI-generated videos, aligns closely with the goals of ICLR.\n\n### Potential Impact on the Field:\nThe proposed method has significant implications for the field of video generation and enhancement. It offers a more efficient and flexible approach compared to existing methods, which can lead to advancements in various applications such as video synthesis, video editing, and video compression. The ability to handle different up-sampling scales and refinement strengths opens up possibilities for future research in adaptive video enhancement techniques.\n\n### Weaknesses:\nWhile the paper is strong overall, there are a few areas that could benefit from further refinement:\n\n1. **Detailed Discussion of Limitations**: While the limitations are briefly mentioned, a more detailed discussion of potential challenges (e.g., computational overhead, scalability for long videos) would strengthen the paper.\n2. **Comparison with Previous Work**: More extensive comparisons with previous state-of-the-art methods, particularly in terms of the specific trade-offs in terms of speed and accuracy, would provide a clearer picture of the method\u2019s strengths and weaknesses.\n3. **Robustness Testing**: Additional robustness tests, such as evaluating the method under different lighting conditions, camera angles, and types of motion, would demonstrate its versatility and reliability.\n\nOverall, despite these minor concerns, the paper presents a compelling and innovative contribution to the field of video generation and enhancement, making it likely to be accepted for publication at ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636636.6731145", "msecs": "673.0", "relativeCreated": "97366.25504493713", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# ChemAgent: Self-updating Memories in Large Language Models Improves Chemical Reasoning\n\n# Abstract\nChemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code ef- effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science.\n\n\n# 1 INTRODUCTION\n\n\nChemical reasoning presents unique challenges in the realm of artificial intelligence, demanding sophisticated reasoning and precise calculations beyond typical reasoning tasks (McQuarrie, 2008; Atkins et al., 2014; Talanquer, 2022; Guo et al., 2023b; Cao et al., 2024). For example, the GPT solution with CoT prompting in Figure 1 contains numerous errors in both the calculation process and the chemical constants used. Even in short reasoning chains, a single error can cascade, reducing answer quality and escalating the probability of additional errors (Liao et al., 2024; Sun et al., 2024b).\n\n\nRecent advancements in large language models (LLMs) have demonstrated capabilities in simpler scientific tasks or chemical scenarios that do not require complex reasoning (Boiko et al., 2023; Atkins et al., 2023; Wang et al., 2024a; Hu & Shu, 2023; Xiao et al., 2024; Darvish et al., 2024; Skreta et al., 2024). However, their application to complex chemical reasoning reveals significant limitations Pei et al. (2024); Li et al. (2024a). LLMs often (1) struggle to effectively utilize domain-specific formulas, (2) exhibit incorrect reasoning steps, and (3) produce errors when combining textual reasoning with Python code for calculations (Zhong et al., 2024b); here, syntax errors may arise, causing the code to fail to compile. As shown in Figure 1 (StructChem output), errors often arise from determining a constant's incorrect form or unit.\n\n\nSome Previous approaches to address these challenges in chemical reasoning tasks have focused on decomposing the reasoning process (Zhong et al., 2023), e.g., formatting the reasoning steps through techniques such as self-reflection (Shinn et al., 2023) and StructChem prompting (Ouyang et al., 2024). However, these methods often rely heavily on human-curated knowledge (Chan et al., 2023), or fixed workflows (Fu et al., 2022; Wang et al., 2023; Zhou et al., 2023). Unlike human learners who utilize a library system to retain and apply previous experiences, these methods lack\n\n\n# PROBLEM (HYDROGEN ATOM ENERGY LEVELS)\n\n\nAn electron in a hydrogen atom transitions from an excited state with the principal quantum number  $n = 4$  to the ground state  $(n = 1)$ . Calculate: a) The energy of the emitted photon in joules (J), b) The wavelength of the emitted photon in nanometers (nm), c) The frequency of the emitted photon in terahertz (THz).\n\n\n![Figure 1: Comparison of problem-solving approaches for a hydrogen atom energy transition problem. The figure illustrates three different methods: (a) shows a standard Chain-of-Thought approach with calculation errors (in steps 3 and 4) in Wang et al. (2024a). (b) demonstrates the StructChem (Ouyang et al., 2024) method with formula generation and step-by-step reasoning but fails due to an incorrect constant and incorrect unit conversion (in steps 1 and 4). (c) presents the ChemAgent solution, featuring task decomposition, memory retrieval from the library, and reasoning, leading to the accurate final answer.](images/c4d1fe7fcb6aff12c6920faf030081d79c1369535709acd6e471b2d6d9509a4f.jpg)\n\n\n![](images/6b9104f5524bf62f8ce5259565a4c01bcf872049ab59a45af48f8131d7943c2f.jpg)\n\n\n![](images/09dbdf7506263c8919abff0077f7f7751005888ea55e84facb58c558ed6a6797.jpg)\n\n\nthe ability to remember and learn from past analogous problems. For instance, humans can abstract and store theorems or solution strategies from previous tasks and utilize this memory for future problem-solving. $^{1}$\n\n\nBuilding on these insights, ChemAgent introduces a dynamic \"LIBRARY\" system that facilitates iterative problem-solving by continuously updating and refining its content based on task decomposition. The LIBRARY in ChemAgent serves as a comprehensive repository where decomposed chemical tasks are stored. These tasks are broken down into sub-tasks, and their solutions are compiled in the library for future reference. As new tasks are encountered, the library is updated with new sub-tasks and corresponding solutions, ensuring its content remains relevant and progressively improves over time. In parallel, inspired by human cognitive mechanisms (Osman, 2004; Kaufman, 2011), our system incorporates three main memory components: Planning Memory for high-level strategies, Execution Memory for specific task solutions, and Knowledge Memory for fundamental chemistry principles. These memories are stored externally, allowing the model to retrieve them efficiently during the problem-solving process. Unlike previous works relevant to LLM's external memory system (Huang et al., 2024a; Zhong et al., 2024a; Zhang et al., 2023; Li et al., 2024b; Guo et al., 2023a; Zhang et al., 2024b), we carefully integrate all these components in a complete agentic framework and allow them to update dynamically.\n\n\nThese memories are stored in a structured tree-based format that allows for efficient retrieval during the problem-solving process. When ChemAgent encounters a new problem, it first decomposes the task into manageable sub-tasks, then leverages the library to retrieve relevant sub-tasks and solutions stored in its memory components. The retrieved information is validated and refined using analogous sub-tasks previously encountered, optimizing both task decomposition and solution generation. The library is dynamically updated by adding new sub-tasks and solutions as they are encountered and validated. This iterative process ensures that the memory is continuously enriched with new strategies and solutions, mimicking human problem-solving improvements through practice. By maintaining an evolving domain knowledge base, ChemAgent enables LLMs to autonomously navigate the problem-solving process, thereby enhancing their performance on similar tasks over time.\n\n\nOur experiments are conducted on four chemical reasoning datasets from SciBench (Wang et al., 2024a) with GPT-3.5, GPT-4 (OpenAI et al., 2024), and open-source models like Llama3\n\n\n![Figure 2: The diagram of our overall framework. It contains (a) library-enhanced reasoning and (b) library construction. (a) illustrates how ChemAgent utilizes the library to address a new task for the test set. And (b) demonstrates the construction of the library over the dev set, including Plan Memory  $\\mathcal{M}_p$  and Execution Memory  $\\mathcal{M}_e$ .](images/11429a3d16f9569a5c20c31d8aeba45b07216b206661ae7319c988910307b8a0.jpg)\n\n\n![](images/bd0c74abbecaceecb224a980485e107b447712119560561bf69ed709d17b7a21.jpg)\n\n\n(Llama Team, 2024). Experimental results indicate that ChemAgent significantly enhances the accuracy, though the degree of improvement achieved by our model varies accordingly due to the varying sizes of these datasets. Compared to the base model (take GPT-4 for example), the application of memory self-improving leads to an average increase in accuracy by  $37\\%$ , with a maximum improvement of  $46\\%$ . In comparison with the current state-of-the-art model, StructChem (Ouyang et al., 2024), our approach achieves an average improvement of  $10\\%$  and a maximum improvement of  $15\\%$  across different datasets. Furthermore, the improvement is more pronounced for stronger base models: GPT-4 exhibits greater enhancements compared to GPT-3.5 when augmented with our framework. Additionally, we analyze the role of our library system and examine the benefits of different memory updates across various stages. This analysis provides insights into how our iterative library updates contributed to performance improvements in sub-task resolution and optimal task decomposition. By continuously enriching the library with structured memory components, ChemAgent ensures that the problem-solving process is progressively refined and optimized, leading to substantial improvements in accuracy.\n\n\n# 2 METHOD\n\n\n# 2.1 PRELIMINARIES\n\n\nAnalogous to how students organize and reference their problem-solving approaches for exams, our motivation for developing a library system in ChemAgent is to enhance LLMs' ability to tackle complex problems by providing structured access to a repository of previous sub-tasks and solutions.\n\n\nThe overall reasoning framework is shown in Figure 2 (a). Formally, in a simplified form, given a complex and open-ended chemical problem  $\\mathcal{P}_S$  as input, our method aims to generate a solution  $\\mathcal{O}_S$ . The problem  $\\mathcal{P}_S$ , which comprises problem descriptions  $\\mathcal{T}_S$  and initial conditions  $\\mathcal{C}_S$ , is firstly decomposed into a series of sub-tasks  $\\mathcal{P}_{S_i}$ . The solution  $\\mathcal{O}_S$  is then synthesized from the subsolutions  $\\mathcal{O}_{S_i}$  of the sub-tasks. Each  $\\mathcal{O}_{S_i}$  includes intermediate steps, such as formulae, reasoning steps, code, and calculations. Then, a final answer  $\\mathcal{A}_S$  is derived from the overall solution  $\\mathcal{O}_S$ . We evaluate the performance via the accuracy of  $\\mathcal{A}_S$  against a ground truth  $\\mathcal{A}_{S_g}$ .\n\n\n# 2.2 COMPOSITION OF THE LIBRARY\n\n\nWe divide library into three main memory components: planning memory, execution memory, and knowledge memory. Figure 3 provides detailed examples. The exact context of memory and corresponding prompts are in Appendix G.\n\n\n- Planning Memory  $(\\mathcal{M}_p)$ : This component stores high-level strategies and methodologies for approaching complex problems.\n\n\n![Figure 3: Given a task  $\\mathcal{P}$ , the relevant memory examples are provided in the library. Specifically, while Execution Memory  $(\\mathcal{M}_e)$  and Plan Memory  $(\\mathcal{M}_p)$  are derived from prior experiences, Knowledge memory  $(\\mathcal{M}_k)$  is generated by LLM based on the problem prompt. The conditions  $\\mathcal{C}$  are not explicitly presented here but are embedded within  $\\mathcal{P}$  and the [GOAL] of  $\\mathcal{M}_e$ . Figure 2 and Figure 4 illustrates this workflow. In the following sections, we explain how each part is constructed and utilized. All prompts used in our framework can be found in Appendix G.](images/d97efd4a2ba11c0158298bbff145ff67856931ddfc2732a22ef5d9872f2ed09b.jpg)\n\n- - Execution Memory  $(\\mathcal{M}_e)$ : This contains highly structured descriptions of specific problem contexts and their corresponding solutions, serving as detailed execution plans.\n\n- - Knowledge Memory  $(\\mathcal{M}_k)$ : This houses fundamental chemistry principles and formulas, acting as a ready reference. This component is generated temporarily during the solution of a specific task and is not intended for permanent retention.\n\n\nThus, ChemAgent enables LLMs to tackle problems proactively, form new memories from these attempts, utilize existing memories to solve complex problems, and continuously refine their solutions. Built upon the library of three types of memories, ChemAgent operates in two main stages:\n\n- 1. Initial Library Construction: Complex problems in the development set are decomposed into structured atomic sub-tasks, which serve as building blocks of the library, consisting of static long-term memory  $(\\mathcal{M}_p$  and  $\\mathcal{M}_e)$ .\n\n- 2. Inference Phase and Dynamic Memory Updating:\n\n- (a) Memory-Enhanced Reasoning: The memory is dynamically enriched and improved during runtime.\n\n- (b) Evaluation & refinement: Each sub-task's solution is evaluated on their veracity and relevance to the main task, which leads to a correction of the solution or a refinement of the overall plan.\n\n\n# 2.3 DECOMPOSITION AS ATOMIC BLOCKS\n\n\nHuman problem-solvers naturally break down complex chemistry problems into smaller, manageable sub-tasks (Zhou et al., 2023). This decomposition enhances the understanding of individual components and their interactions, facilitating the resolution of the original problem in a structured manner (Johnson et al., 2017). These sub-tasks not only improve the reasoning process but also function as atomic building blocks within the execution memory, each paired with a corresponding sub-solution. Key characteristics of this decomposition include:\n\n\nHierarchical Decomposition: Using the strategy stored in Planning Memory, we break down complex chemistry problems into hierarchical sub-tasks. If a sub-task is too difficult to complete, it is further decomposed until it can be executed in a single step by the LLM. We store each decomposed sub-task and its intermediate sub-solutions, enabling direct retrieval of solutions for similar future problems. This hierarchical approach ensures each sub-task acts as an independent atomic block, aiding future problem-solving.\n\n\nStructured Sub-tasks: To maximize the efficiency of decomposition, sub-tasks are structured into four distinct parts: (i) Task Query defining the specific question, (ii) Sub-task Objectives outlining\n\n\nclear goals, (iii) Step-by-step Solution detailing the method to address the sub-task, and (iv) Guidance for Solutions suggesting potential approaches.\n\n\nAtomic Blocks as Memory: Decomposed sub-tasks serve as building blocks for memory. Identifying and recalling similar problems can be challenging, but sub-tasks often share commonalities that make it easier to leverage past experiences. These atomic blocks also function as examples in few-shot prompting, standardizing response formats and bolstering the overall problem-solving process.\n\n\nBuilding on the concept of sub-tasks as fundamental units, we outline the processes involved in our framework, which includes both a dynamic library system and the structured memory inside our library. This section details how we initially construct the library and memory (\u00a72.4), utilize and update them for new tasks (\u00a72.5), and evaluate and refine the generated solutions (\u00a72.6). Specifically, our approach leverages the development set to build a library, which is designed to dynamically self-improve during runtime.\n\n\n# 2.4 LIBRARY CONSTRUCTION\n\n\nThe Library is constructed using the development set. As outlined in \u00a72.3, we leverage sub-tasks decomposed from each problem as the execution memory units. Each execution memory unit  $\\mathcal{U}_i$  is defined as follows, where  $\\mathcal{C}$  represents the conditions of a given problem  $\\mathcal{P}$ , and  $\\mathcal{T}_i$  and  $\\mathcal{O}_i$  denote the sub-task and its corresponding sub-solution, respectively:\n\n\n$$\n\n$$\n\\mathcal {U} _ {i} = \\left(\\mathcal {C}, \\mathcal {T} _ {i}, \\mathcal {O} _ {i}\\right) \\quad \\text {f o r} \\quad i = 1, 2, \\dots , k\n$$\n\n$$\n\n\nGiven a problem  $\\mathcal{P}$  and its corresponding solution  $S$  in the development set, our method begins by identifying and extracting the conditions from  $\\mathcal{P}$ . We then verify these conditions for accuracy to ensure that the subsequent steps operate with precise and correctly parsed data. Based on the identified conditions, we instruct the LLMs to generate detailed\n\n\nsub-tasks. For each identified sub-task  $\\mathcal{T}_i$ , the corresponding sub-solutions  $\\mathcal{O}_i$  are parsed from  $\\mathcal{S}$  and assigned accordingly. Inspired by curriculum learning (Bengio et al., 2009), we then rank the memory units  $\\mathcal{U}$  based on their difficulty. In addition to ranking, we discard any memory units that do not meet a predefined confidence threshold, as evaluated by the LLMs. This ensures that the memory utilized for future problem-solving is both relevant and reliable, enhancing the LLM's ability to tackle increasingly complex chemistry problems. The detailed memory construction process is further described in Algorithm 1.\n\n\n```\nInput: Development set  $\\mathcal{D}$  LLM  $\\mathcal{F}$  prompts  $\\{p_{\\mathrm{split}},p_{\\mathrm{ref}},p_{\\mathrm{rank}}\\}$    \nOutput: Static memory  $\\mathcal{M}$  consisting of units  $\\mathcal{U} =$  {condition, question, solution}   \nfor  $(\\mathcal{P},\\mathcal{S})$  in  $\\mathcal{D}$  do Conditions  $\\mathcal{C}\\gets \\mathcal{F}(p_{\\mathrm{split}}||\\mathcal{P})$  // Verify conditions and refine if necessary  $\\mathcal{C}\\gets \\mathcal{F}(p_{\\mathrm{ref}}||\\mathcal{P}||\\mathcal{C})$  Sub-tasks  $\\mathcal{T}\\gets \\mathcal{F}(p_{\\mathrm{task}}||\\mathcal{P}||\\mathcal{C})$  Sub-solutions  $\\mathcal{O}\\gets \\mathcal{F}(p_{\\mathrm{sol}}||\\mathcal{C}||\\mathcal{P}||\\mathcal{S})$  Assert len  $(\\mathcal{C}) = \\text{len} (\\mathcal{T}) = \\text{len} (\\mathcal{O})$  for i in len(C) do Add  $(\\mathcal{C}_i,\\mathcal{T}_i,\\mathcal{O}_i)$  into U  $\\mathcal{U}\\gets \\mathcal{F}(p_{\\mathrm{rank}}||\\mathcal{U})$    \nreturn U\n```\n\n\n# 2.5 LIBRARY-ENHANCED REASONING\n\n\nDuring testing, we first decompose a given problem into several sub-tasks. For each sub-task, we retrieve related memory units  $\\mathcal{U}_r$  to aid in solving it. Specifically, we compute the similarity between the given sub-task and units stored in the memory. Memory units with similarity above a predefined threshold  $\\theta$  are used to assist the model in determining the answer for the sub-task.\n\n\nFormally, let  $(\\mathcal{C}_j,\\mathcal{T}_j)$  represent a sub-task decomposed from a new problem. We retrieve memory units  $\\mathcal{U}_r$  that satisfy\n\n\n$$\n\n$$\n\\begin{array}{l} \\text {S i m i l a r i t y} (\\mathcal {T} _ {j}, \\mathcal {T} _ {\\mathcal {U} _ {i}}) \\geq \\theta \\quad \\text {f o r} \\quad \\mathcal {U} _ {i} \\in \\mathcal {M} _ {e}. \\end{array}\n$$\n\n$$\n\n\nSpecifically, the similarity between tasks is calculated using Llama3's embeddings:\n\n\n$$\n\n$$\n\\operatorname {S i m i l a r i t y} \\left(T _ {a}, T _ {b}\\right) = \\frac {\\operatorname {E m b e d} \\left(T _ {a}\\right) \\cdot \\operatorname {E m b e d} \\left(T _ {b}\\right)}{\\left| \\left| \\operatorname {E m b e d} \\left(T _ {a}\\right) \\right| \\right| \\times \\left| \\left| \\operatorname {E m b e d} \\left(T _ {b}\\right) \\right| \\right|}.\n$$\n\n$$\n\n\nIn scenarios where the execution memory does not contain similar sub-tasks, we incorporate a self-improving mechanism. Initially, we enrich the memory with the required information by leveraging the internal parametric knowledge of LLMs. The LLM is instructed to identify the topic of the given sub-task (e.g., quantum chemistry or thermodynamics) and generate self-created chemistry\n\n\nproblems related to that topic, adhering to specific guidelines. This forms a kind of \"synthetic\" execution memory. We encourage the LLMs to generate diverse problems and solutions beyond the provided examples, thereby enhancing the robustness and adaptability of the memory system.\n\n\nMoreover, the memory is continuously updated with newly solved sub-tasks and their solutions during runtime. For each newly solved sub-task  $\\mathcal{T}_j$ , sub-answers from previous sub-problems are incorporated into the conditions as  $\\mathcal{C}_j$ , and the execution memory is updated based on solution  $\\mathcal{O}_j$ :\n\n\n$$\n\n$$\n\\mathcal {M} _ {e} = \\mathcal {M} _ {e} \\cup \\left\\{\\left(\\mathcal {C} _ {j}, \\mathcal {T} _ {j}, \\mathcal {O} _ {j}\\right) \\right\\}.\n$$\n\n$$\n\n\nThe plan memory will be updated by a summary of the overall strategy knowledge used,  $\\mathcal{K}_j$  (e.g. formulas, conception, order of resolution):\n\n\n$$\n\n$$\n\\mathcal {M} _ {p} = \\mathcal {M} _ {p} \\cup \\left\\{\\left(\\mathcal {T} _ {j}, \\mathcal {K} _ {j}\\right) \\right\\}.\n$$\n\n$$\n\n\nThis dynamic updating ensures that the memory evolves and improves over time, progressively enhancing the problem-solving capabilities. More details on self-evolution are in \u00a73.3.\n\n\n# 2.6 EVALUATE & REFINE MODULE\n\n\nTo enhance the flexibility and reliability of ChemAgent, we propose an evaluation & refinement module, to correct the planning trajectory and verify the response to each sub-task.\n\n\nAs illustrated in Figure 4, after addressing a specific sub-task  $\\mathcal{P}_i$ , ChemAgent examines the sub-solution  $\\mathcal{O}_i$ . The system evaluates whether the sub-solution conflicts with fundamental knowledge in  $\\mathcal{M}_k$  or contains common errors, such as incorrect units. If discrepancies are identified, a new sub-solution  $\\mathcal{O}'_i$  is generated by refining the original  $\\mathcal{O}_i$  based on the relevant knowledge in  $\\mathcal{M}_k$  and the identified mistakes.\n\n\n![Figure 4: Overall framework of the evaluation & refinement module. ChemAgent continuously modifies the solution or the comprehensive strategy until it either reaches the maximum number of trials or meets the evaluator's criteria.](images/5096752b4b5d514a04291518aa93f434d3446f2ba58cdca1723fceac15a8af9c.jpg)\n\n\nFurthermore, if a sub-task fails due to insufficient conditions or if the evaluation determines that the sub-task's question does not align with the main task (e.g., calculating energy using wavelength instead of calculating wavelength using given energy), the sub-tasks from  $\\mathcal{P}_i$  to  $\\mathcal{P}_n$  will be restructured as  $\\mathcal{P}_i'$  to  $\\mathcal{P}_m'$ , taking into account the main task and all preceding sub-tasks.\n\n\nThe evaluation & refinement module can use a different LLM than the one used for the base reasoning process. For instance, if GPT-3.5 is the base model, GPT-4 can handle evaluation & refinement. The evaluation component judges solutions without modifying them, while the refinement component adjusts solutions based on these evaluations. This separation clarifies error identification, helping humans understand where and why mistakes occur.\n\n\n# 3 EXPERIMENT\n\n\n# 3.1 SETUP\n\n\nWe use four chemistry datasets from SciBench (Wang et al., 2024a), and the detailed distribution of the specific fields covered by each problem in the four datasets is shown in Figure 10. Each dataset is divided into a development set  $(\\mathcal{D}_d)$  and a test set  $(\\mathcal{D}_t)$ , with exact sizes provided in Table 6. According to previous research (Ouyang et al., 2024), SciBench chemical sets are representative of chemical reasoning tasks, addressing a broad range of queries typically encountered in this field.\n\n\nDuring the reasoning stage, we configure the planning memory  $(\\mathcal{M}_p)$  to provide a maximum of two related memory instances (2-shot) for each query, and the execution memory  $(\\mathcal{M}_e)$  to provide up to four related instances (4-shot). However, during the construction of library, only the knowledge memory  $(\\mathcal{M}_k)$  is used, as the standard solutions are already available in the development set  $(\\mathcal{D}_d)$ .\n\n\nTable 1: Performance on the test sets of four datasets: QUAN, CHEMMC, ATKINS, and MATTER. Results are compared with baselines under two different setups: a zero-shot setting with no demonstrations and a few-shot setting with three demonstrations. Accuracy scores are computed using the approximation detailed in Section 4.3. The best results for each setup are highlighted in **bold**, and the second-best results are **underlined**. In some settings, the two modules described in Sections 2.4 (Memory) and 2.6 (Evaluation & refinement) are disabled, helping illustrate each module's impact on the overall performance.\n\n\n<table><tr><td>Models</td><td>CHEMMC</td><td>MATTER</td><td>ATKINS</td><td>QUAN</td><td>Avg.</td></tr><tr><td colspan=\"6\">Baselines, all based on GPT-4 (gpt-4-1106-preview)</td></tr><tr><td>Few-shot + Direct reasoning</td><td>28.21</td><td>14.29</td><td>20.69</td><td>14.71</td><td>19.48</td></tr><tr><td>Few-shot + Python</td><td>38.46</td><td>34.69</td><td>57.01</td><td>44.12</td><td>43.57</td></tr><tr><td>StructChem</td><td>58.97</td><td>30.67</td><td>59.81</td><td>41.18</td><td>47.66</td></tr><tr><td colspan=\"6\">ChemAgent, all based on GPT-4 (gpt-4-1106-preview)</td></tr><tr><td>ChemAgent</td><td>74.36</td><td>48.98</td><td>61.18</td><td>44.12</td><td>57.16</td></tr><tr><td>w/o Evaluation &amp; Refinement</td><td>61.54</td><td>44.89</td><td>57.94</td><td>44.12</td><td>52.12</td></tr><tr><td>w/o Memory, Evaluation &amp; Refinement</td><td>58.97</td><td>38.78</td><td>57.94</td><td>41.18</td><td>49.22</td></tr><tr><td colspan=\"6\">all based on Llama 3.1-7b</td></tr><tr><td>Direct reasoning</td><td>28.20</td><td>10.20</td><td>22.43</td><td>8.82</td><td>17.41</td></tr><tr><td>ChemAgent</td><td>56.41</td><td>12.24</td><td>19.63</td><td>14.71</td><td>25.75</td></tr><tr><td colspan=\"6\">all based on Llama 3.1-70b</td></tr><tr><td>Direct reasoning</td><td>33.33</td><td>30.61</td><td>30.84</td><td>23.53</td><td>29.48</td></tr><tr><td>ChemAgent</td><td>64.10</td><td>32.65</td><td>43.93</td><td>29.41</td><td>42.52</td></tr><tr><td colspan=\"6\">all based on Qwen 2.5-72b</td></tr><tr><td>Direct reasoning</td><td>48.72</td><td>32.65</td><td>57.01</td><td>35.50</td><td>43.47</td></tr><tr><td>ChemAgent</td><td>69.23</td><td>44.90</td><td>56.07</td><td>44.12</td><td>53.58</td></tr></table>\n\n\nWe evaluate the accuracy by comparing their outputs with the correct answers, using a relative tolerance of 0.01.\n\n\nWe consider three baselines in alignment with the evaluation paradigm in SciBench (detailed instructions are provided in Appendix G): (1) Few-shot + Direct reasoning involves directly feeding the problem into the LLM without any additional instructions, the source of the data is StructChem (Ouyang et al., 2024). (2) Few-shot + Python combines few-shot examples with a tool-augmented strategy. Here, we provide six examples for every query, each consisting of a problem, its corresponding solution, and Python code. These examples are taken from  $\\mathcal{D}_d$ . The results are from the original benchmark, SciBench (Wang et al., 2024a). (3) StructChem (Ouyang et al., 2024) employs structured instruction, confidence-based review, and refinement to guide LLMs through precise step-by-step reasoning.\n\n\n# 3.2 RESULTS\n\n\nWe report the performance of all methods regarding accuracy score for each sub-dataset and an average score across the entire dataset. The results are summarized in Tables 1, 4 and 5. Additional results and analysis of experiments conducted on other LLMs can be found in Appendix B.\n\n\nFirstly, ChemAgent consistently outperforms the other baselines across various datasets and settings. Specifically, in terms of the average score, ChemAgent improves by  $9.50\\%$  (47.66 vs. 57.16) over StructChem, which is a 2.93 times increase and by  $37\\%$  (19.48 vs. 57.16) over direct reasoning, which is a 2.93 times increase. Notably, the performance gain varies across different datasets. In the CHEMMC dataset, our method exhibits the largest improvement, with a  $46\\%$  increase (28.21 vs. 74.36) compared to the direct reasoning setup. Secondly, the results also highlight the crucial role of memory in our library. The version of our framework equipped with memory consistently outperforms the version without memory across all cases. Specifically, there is an absolute improvement of  $2.90\\%$  in terms of the average score. This underscores the importance of memory in retaining and leveraging past information to improve the accuracy and reliability of solving chemistry problems. Additionally, the Evaluation & Refinement module plays a significant role when using stronger LLMs. For instance, incorporating Evaluation & Refinement with GPT-4 increases ChemAgent's performance by  $5.04\\%$  compared to ChemAgent with memory alone. In summary, ChemAgent demonstrates superior performance across various LLM backbones, highlighting the critical importance of the library system, memory design, and evaluation modules in\n\n\nenhancing problem-solving capabilities. Appendix C details the specific aspects and reasons for our method's improved performance.\n\n\n# 3.3 SELF-EVOLUTION DURING RUNTIME\n\n\nMoreover, we aim to show that library systems with evolving memory perform better when exposed to an increasing number of problems. Much like humans improve their skills through practice, these systems benefit from continuous exposure to new tasks.\n\n\nWe allow ChemAgent to dynamically update and enrich its library during the test stage to analyze this self-evolution process. Specifically, in iteration  $\\mathcal{I}_i$ , ChemAgent uses all accumulated long-term memory from iterations  $\\mathcal{I}_1$  to  $\\mathcal{I}_{i-1}$  as its library. When solving a new problem  $\\mathcal{P}$ , all related responses and knowledge from that process are added to the library if the solution is correct. This means that in subsequent iterations, the system can leverage the newly acquired information (updated  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$ ) to improve its performance. We employ 2-shot  $\\mathcal{M}_p$  and 4-shot  $\\mathcal{M}_e$  during reasoning but simplify the setup by removing the evaluation and refinement modules. To ensure accuracy and prevent target leakage, the memory derived from problem  $\\mathcal{P}_i$  in iteration  $\\mathcal{I}_j$  is excluded when solving  $\\mathcal{P}_i$  again in  $\\mathcal{I}_k$  for any  $j < i$ .\n\n\n![Figure 5: Self-evolving analysis. We test ChemAgent twice for each iteration, and the difference between the two results serves as the error margin. All the experiments here are done on MATTER dataset.](images/95adae2d9c377ebed9474a236074302b8c0f6913519066c1901f5ddd53844195.jpg)\n\n\nThis iterative process demonstrates that as the memory pool grows with each new example, ChemAgent's problem-solving performance improves. Figure 5 shows that as the number of iterations increases, the agent's performance gradually improves and converges to a score higher than the baseline (44.89%). This improvement indicates that ChemAgent can enhance its performance through a simple correct-or-not evaluation on past solutions instead of human-written high-quality trajectories.\n\n\n# 3.4 COST ANALYSIS\n\n\nOn average, each problem requires 0.012 million tokens without the Evaluation & Refinement module, equivalent to around  $0.09 per example. When this module is applied, the average token consumption per problem increases to approximately 0.023 million, costing about$ 0.1725 per example. Note that the initial library construction is not included in these calculations as it is a one-time setup. Based on the results in Figure 6, the computational time required by our method is reasonable. While the cost and resource consumption are slightly higher than StructChem, the improvements justify the additional expense.\n\n\n# 3.5 ERROR ANALYSIS\n\n\n![Figure 6: Cost Analysis. The size of each bubble corresponds to the average number of inferences for each method, while the  $y$ -axis indicates the average accuracy across the four datasets.](images/351f3762b68fd4c9a055594bc3e46260258d9163560448f7c46f548f90a4b0b8.jpg)\n\n\nWe conduct an analysis of the trajectories for the failed examples and find three types of errors, shown in Figure 7.\n\n\n(1) Lack of Understanding of the Question. We observe that plans often fail when the problem text contains critical hidden information (e.g., in Figure 7, the terms 'reversibly' and 'adiabatically' are key) or includes excessive, redundant details (e.g., the exact conditions such as \"250K\" are irrelevant to the solution). This challenge is understandable, as even human solvers can be misled by such information and err in their approach. This issue is prevalent and independent of whether the task is tackled by our model or directly queried to the LLM, suggesting that these errors may be\n\n\n![Figure 7: Error analysis. This example highlights a typical incorrect solution, which can be attributed to three main types of errors. Specifically, this problem pertains to an adiabatic process.](images/57d0674cc5aa18b36216a513c20f2715a26997102df2926f250970a7197a167e.jpg)\n\n\ninherent limitations of the LLM's capacity. Hence, improving the ability of the foundation model to handle such nuances could significantly enhance performance.\n\n- (2) Inaccurate Reasoning. As indicated before (Ouyang et al., 2024), the planning capabilities of LLMs remain insufficient for handling complex problems. Incorrect planning for the reasoning chains exacerbates the problem-solving process, as subsequent decisions and actions are based on initial problem decompositions. This issue persists until the Evaluation & Refinement module detects an error, which may not happen promptly enough to correct the trajectory.\n\n- (3) Incorrect Memory Selection. While ChemAgent with memory demonstrates superior performance compared to setups without memory, it sometimes invokes misleading information, even when the similarity between the invoked memory and the problem is high. This indicates a need for more sophisticated memory retrieval and utilization strategies.\n\n\nIn Figure 7, the invoked memory and sub-task 1.1 share considerable similarities\u2014both concern entropy change during a compression process. However, the subtle difference is that the current problem involves an adiabatic process, whereas the examples in memory do not. This seemingly minor distinction can lead to significant changes in the problem-solving strategy. Distinguishing between misleading and beneficial memories remains a challenging issue, as invoked memories and problem texts may be semantically similar yet differ in critical aspects.\n\n\n# 4 ABLATION STUDY\n\n\n# 4.1 MEMORY COMPONENT ANALYSIS\n\n\nTo understand why our method performs particularly well and which memory component contributes the most, we conduct an ablation study by independently removing each memory component. We test these different settings on all four sub-datasets using GPT-4 as the base LLM. The results are shown in Table 2. As mentioned in \u00a72.3, the execution memory also serves as a few-shot prompting strategy. Therefore, when we remove  $\\mathcal{M}_e$ , we add two fixed human-written few-shot examples (provided with our code) into each query of each sub-task. These examples are selected from the development set  $(\\mathcal{D}_d)$  corresponding to each dataset to ensure they do not provide misleading information.\n\n\nFirstly, removing any memory component results in an overall performance drop. And notably, the relevant knowledge in LLM itself  $(\\mathcal{M}_k)$  significantly impacts the overall performance of ChemA-gent. Delving into why  $\\mathcal{M}_k$  contributes the most, we find that this might be due to the insufficient  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$  in the Atkins dataset. As shown in Table 6, ATKINS has the lowest ratio of the development set to the test set, while it also has the largest test set. This imbalance may result in a small\n\n\nTable 2: The best performing score is highlighted in bold and second-best is underlined. The number of problems in each textbook weighs the average score. We also provide the results of ChemAgent without any of these memories as a baseline. Evaluation & refinement module is removed.\n\n\n<table><tr><td>Memory component</td><td>CHEMMC</td><td>MATTER</td><td>ATKINS</td><td>QUAN</td><td>Avg.</td></tr><tr><td>None</td><td>58.97</td><td>38.78</td><td>57.94</td><td>41.18</td><td>51.53</td></tr><tr><td>+MpMeMk</td><td>61.54</td><td>44.89</td><td>57.94</td><td>44.12</td><td>53.71</td></tr><tr><td>+MpMe</td><td>66.66</td><td>44.89</td><td>52.34</td><td>41.18</td><td>51.53</td></tr><tr><td>+MeMk</td><td>58.97</td><td>46.94</td><td>61.68</td><td>29.41</td><td>53.27</td></tr><tr><td>+MpMkfew-shot</td><td>74.36</td><td>42.86</td><td>57.94</td><td>32.35</td><td>53.71</td></tr></table>\n\n\nand sometimes irrelevant memory pool for  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$  to search from. We hypothesize that this issue can be mitigated when the model is exposed to more chemical questions over time. However, due to limited computational resources, we cannot scale up to verify this hypothesis in this research.\n\n\nIt is also worth noting that although the  $M_p, M_k, \\text{fewshot}$  setting achieves the same accuracy as the complete memory setting, most of its improvement is evident in the CHEMMC dataset, showing surprisingly higher improvements than others. Since Table 6 and Figure 10 demonstrate that the CHEMMC dataset has a higher ratio of  $\\mathcal{D}_d$  to  $\\mathcal{D}_t$  and a relatively narrow distribution in subfields, and since human experts carefully wrote the few-shot examples, these high-quality, expert-written examples may serve as a superior memory pool in such circumstances. In the next section, we show that better memory leads to better results, which might explain this phenomenon.\n\n\n# 4.2 MEMORY QUALITY INFLUENCE\n\n\nWe investigate the impact of memory quality through a series of experiments. Specifically, we compare the performance of memory generated by GPT-4 against that generated by GPT-3.5 on the MATTER dataset. Additionally, we create a \"hybrid memory\" by mixing memories generated by both GPT-3.5 and GPT-4 to observe its performance. These experiments were conducted without including the relevant knowledge within the LLM itself  $(\\mathcal{M}_k)$ . As mentioned in \u00a72.1, unlike  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$ ,  $\\mathcal{M}_k$\n\n\nTable 3: Results of memory quality analysis, with the best scores highlighting in bold. Here, memory refers to  $\\mathcal{M}_p$  and  $\\mathcal{M}_e$ . The relevant knowledge in LLM itself  $(\\mathcal{M}_k)$  is removed from the framework. Test on MATTER dataset.\n\n\n<table><tr><td></td><td>GPT-3.5</td><td>GPT-4</td></tr><tr><td>GPT-3.5 Memory</td><td>11.22</td><td>36.73</td></tr><tr><td>GPT-4 Memory</td><td>13.26</td><td>44.89</td></tr><tr><td>Hybrid Memory</td><td>10.20</td><td>28.57</td></tr></table>\n\n\nis not preserved in the memory pool shared with other LLMs. Including this type of memory in the ablation study would result in the pollution of provided memory by the  $\\mathcal{M}_k$  generated during use.\n\n\nAs shown in Table 3, our data analysis indicates that memory quality significantly impacts the accuracy of responses from ChemAgent. Whether GPT-3.5 or GPT-4 is used, the memory produced by GPT-4 consistently outperforms that generated by GPT-3.5, showing an  $8\\%$  variation. This underscores the substantial impact of memory quality on the accuracy of generated responses. This observation also explains why the improvements seen with GPT-3.5 are less pronounced compared to those with GPT-4; the lower quality of memory produced by GPT-3.5 is the limiting factor.\n\n\nInterestingly, hybrid memory performs the worst among the three settings. This can be attributed to the simultaneous invocation of different memories for the same question\u2014one generated by GPT-3.5 and the other by GPT-4\u2014which may confuse the LLM, increasing the likelihood of producing irrelevant or incorrect answers. Additionally, invoking an excessive number of memory tracks during resolution may also contribute to this issue. A more detailed analysis regarding the number of memory tracks invoked is provided in Appendix D.\n\n\n# 5 CONCLUSION\n\n\nOur research presents a novel approach to enhancing large language models for solving complex chemical problems through self-exploration and memory formation. This method enables models to construct and utilize a library, significantly improving response accuracy. Experiments using datasets and models like GPT-3.5, GPT-4, and Llama3 demonstrate substantial performance gains, with the ChemAgent architecture achieving up to a  $36\\%$  improvement. The structured library, built through memory decomposition into planning, execution, and knowledge, enhances problem-solving capabilities, which holds promise for generalization to other domains.\n\n\n# REFERENCES\n\n- Peter Atkins, Julio De Paula, and Ronald Friedman. Physical chemistry: quanta, matter, and change. Oxford University Press, USA, 2014.\n\n- Peter Atkins, Julio De Paula, and James Keeler. Atkins' physical chemistry. Oxford university press, 2023.\n\n- Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41-48, 2009.\n\n- Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,16, pp. 17682-17690, 2024.\n\n- Daniil A. Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature, 624:570 - 578, 2023.\n\n- Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. ChemCrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.\n\n- He Cao, Yanjun Shao, Zhiyuan Liu, Zijing Liu, Xiangru Tang, Yuan Yao, and Yu Li. PRESTO: Progressive pretraining enhances synthetic chemistry outcomes. arXiv preprint arXiv:2406.13193, 2024.\n\n- Aaron Chan, Zhiyuan Zeng, Wyatt Lake, Brihi Joshi, Hanjie Chen, and Xiang Ren. KNIFE: Distilling meta-reasoning knowledge with free-text rationales. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML, 2023.\n\n- Kourosh Darvish, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa, Sagnik Som, Miroslav Bogdanovic, Yang Cao, Han Hao, Haoping Xu, Al\u00e1n Aspuru-Guzik, Animesh Garg, and Florian Shkurti. Organa: A robotic assistant for automated chemistry experimentation and characterization. ArXiv, abs/2401.06949, 2024.\n\n- Ernest Davis and Scott Aaronson. Testing GPT-4 with wolfram alpha and code interpreter plug-ins on math and science problems. arXiv preprint arXiv:2308.05713, 2023.\n\n- Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, and Huajun Chen. SciKnowEval: Evaluating multi-level scientific knowledge of large language models. arXiv preprint arXiv:2406.09098, 2024.\n\n- Chrisantha Fernando, Dylan Sunil Banarse, Henrik Michalewski, Simon Osindero, and Tim Roktaschel. Promptbreeder: Self-referential self-improvement via prompt evolution. In *Forty-first International Conference on Machine Learning*, 2023.\n\n- Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations, 2022.\n\n- Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16477-16508, 2023.\n\n- Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, et al. Critic: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, 2023.\n\n- Jing Guo, Nan Li, Jian Qiang Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, and Ming Xu. Empowering working memory for large language model agents. ArXiv, abs/2312.17259, 2023a.\n\n- Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, et al. What can large language models do in chemistry? a comprehensive benchmark on eight tasks. Advances in Neural Information Processing Systems, 36:59662-59688, 2023b.\n\n- Joseph F Hair Jr, Wiliam C Black, Barry J Babin, and Rolph E Anderson. Multivariate data analysis. In Multivariate data analysis, pp. 785-785. Prentice Hall, 2010.\n\n- Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint arXiv:2408.08435, 2024.\n\n- Zhiting Hu and Tianmin Shu. Language models, agent models, and world models: The law for machine reasoning and planning. arXiv preprint arXiv:2312.05230, 2023.\n\n- Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations, 2023.\n\n- Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey. ArXiv, abs/2402.02716, 2024a.\n\n- Yuqing Huang, Rongyang Zhang, Xuesong He, Xuyang Zhi, Hao Wang, Xin Li, Feiyang Xu, Deguang Liu, Huadong Liang, Yi Li, Jian Cui, Zimu Liu, Shijin Wang, Guoping Hu, Guiquan Liu, Qi Liu, Defu Lian, and Enhong Chen. Chemeval: A comprehensive multi-level chemical evaluation for large language models. arXiv preprint arXiv:2409.13989, 2024b.\n\n- Shuyang Jiang, Yuhao Wang, and Yu Wang. Selfevolve: A code evolution framework via large language models. arXiv preprint arXiv:2306.02907, 2023.\n\n- Zhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xiaoran Jin, and Hang Li. Design of chain-of-thought in math problem solving. arXiv preprint arXiv:2309.11054, 2023.\n\n- Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2901-2910, 2017.\n\n- Scott Barry Kaufman. Intelligence and the cognitive unconscious. The Cambridge handbook of intelligence, pp. 442-467, 2011.\n\n- Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In *The Eleventh International Conference on Learning Representations*, 2022.\n\n- Jiawei Li, Yizhe Yang, Yu Bai, Xiaofeng Zhou, Yinghao Li, Huashan Sun, Yuhang Liu, Xingpeng Si, Yuhao Ye, Yixiao Wu, et al. Fundamental capabilities of large language models and their applications in domain scenarios: A survey. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11116-11141, 2024a.\n\n- Kun Li, Xin Jing, and Chengang Jing. Vector storage based long-term memory research on llm. International Journal of Advanced Network, Monitoring and Controls, 2024b.\n\n- Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, et al. MMSci: A multimodal multi-discipline dataset for PhD-level scientific comprehension. arXiv preprint arXiv:2407.04903, 2024c.\n\n- Chang Liao, Yemin Yu, Yu Mei, and Ying Wei. From words to molecules: A survey of large language models in chemistry. arXiv preprint arXiv:2402.01439, 2024.\n\n- AI@Meta Llama Team. The llama 3 herd of models, July 2024. URL https://llama.meta.com/.\n\n- Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521, 2022.\n\n- Donald A McQuarrie. Quantum chemistry. University Science Books, 2008.\n\n- Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu, Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Amir Mohammad Elahi, Maximilian Greiner, Caroline T. Holick, Tanya Gupta, Mehrdad Asgari, Christina Glaubitz, Lea C. Klepsch, Yannik K\u00f6ster, Jakob Meyer, Santiago Miret, Tim Hoffmann, Fabian Alexander Kreth, Michael Ringleb, Nicole Roesner, Ulrich S. Schubert, Leanne M. Stafast, Dinga Wonanke, Michael Pieler, Philippe Schwaller, and Kevin Maik Jablonka. Are large language models superhuman chemists?, 2024. URL https://arxiv.org/abs/2404.01475.\n\n- OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, et al. GPT-4 technical report, 2024.\n\n- Magda Osman. An evaluation of dual-process theories of reasoning. Psychonomic bulletin & review, 11(6):988-1010, 2004.\n\n- Siru Ouyang, Zhuosheng Zhang, Bing Yan, Xuan Liu, Yejin Choi, Jiawei Han, and Lianhui Qin. Structured chemistry reasoning with large language models. In *Forty-first International Conference on Machine Learning*, 2024.\n\n- Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. Is a question decomposition unit all we need? In 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, 2022.\n\n- Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, and Rui Yan. Leveraging biomolecule and natural language through multi-modal learning: A survey. arXiv preprint arXiv:2403.01528, 2024.\n\n- Cheng Qian, Shihao Liang, Yujia Qin, Yining Ye, Xin Cong, Yankai Lin, Yesai Wu, Zhiyuan Liu, and Maosong Sun. Investigate-consolidate-exploit: A general strategy for inter-task agent self-evolution. arXiv preprint arXiv:2401.13996, 2024.\n\n- Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve. arXiv preprint arXiv:2407.18219, 2024.\n\n- Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\n\n- Marta Skreta, Zihan Zhou, Jia Lin Yuan, Kourosh Darvish, Al\u00e1n Aspuru-Guzik, and Animesh Garg. Replan: Robotic replanning with perception and language models. ArXiv, abs/2401.04157, 2024.\n\n- Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. SciEval: A multi-level large language model evaluation benchmark for scientific research. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,17, pp. 19053-19061, 2024a.\n\n- Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. Trustl'm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561, 2024b.\n\n- Vicente Talanquer. The complexity of reasoning about and with chemical representations. JACS Au, 2(12):2658-2669, 2022. doi: 10.1021/jacsau.2c00498.\n\n- David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, et al. SciRIFF: A resource to enhance language model instruction-following over scientific literature. arXiv preprint arXiv:2406.07835, 2024.\n\n- Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. SciBench: Evaluating college-level scientific problem-solving abilities of large language models. In *Forty-first International Conference on Machine Learning*, 2024a.\n\n- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023.\n\n- Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv preprint arXiv:2409.07429, 2024b.\n\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.\n\n- Kangda Wei, Dawn Lawrie, Benjamin Van Durme, Yunmo Chen, and Orion Weller. When do decompositions help for machine reading? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 3599-3606, 2023.\n\n- Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2, 2022.\n\n- Yi Xiao, Xiangxin Zhou, Qiang Liu, and Liang Wang. Bridging text and molecule: A survey on multimodal frameworks for molecule. arXiv preprint arXiv:2403.13830, 2024.\n\n- Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2023.\n\n- Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. arXiv preprint arXiv:2406.04271, 2024.\n\n- Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.\n\n- Yuan Zhang, Chao Wang, Juntong Qi, and Yan Peng. Leave it to large language models! correction and planning with memory integration. Cyborg and Bionic Systems, 5, 2023.\n\n- Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu Wang. Small language models need strong verifiers to self-correct reasoning. arXiv preprint arXiv:2404.17140, 2024a.\n\n- Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model based agents. ArXiv, abs/2404.13501, 2024b.\n\n- Ming Zhong, Siru Ouyang, Minhao Jiang, Vivian Hu, Yizhu Jiao, Xuan Wang, and Jiawei Han. ReactIE: Enhancing chemical reaction extraction with weak supervision. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 12120\u201312130, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023-findings-acl.767. URL https://aclanthology.org/2023-findings-acl.767.\n\n- Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):19724-19731, Mar. 2024a. doi: 10.1609/aaai.v38i17.29946. URL https://ojs.aaai.org/index.php/AAAI/article/view/29946.\n\n- Xianrui Zhong, Yufeng Du, Siru Ouyang, Ming Zhong, Tingfeng Luo, Qirong Ho, Hao Peng, Heng Ji, and Jiawei Han. Actionie: Action extraction from scientific literature with programming languages. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12656-12671, 2024b.\n\n- Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WZH7099tgfm.\n\n- Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532, 2024.\n\nResponse: \\boxed{Accept}\n\n### Rationale for Prediction\n\n**Originality:** The paper introduces a novel framework, ChemAgent, which addresses the limitations of large language models (LLMs) in handling complex chemical reasoning tasks. The framework is innovative in breaking down problems into sub-tasks, storing these in a dynamic library, and using this library to improve LLM performance over time. The idea of a self-updating library is a fresh perspective on how to augment LLMs for specific domain tasks.\n\n**Methodological Rigor:** The methodology is clearly articulated, and the experimental setup is well-defined. The authors provide detailed descriptions of the library construction process, memory management, and the evaluation and refinement module. The use of multiple datasets and a variety of baseline methods to assess performance adds credibility to the claims.\n\n**Significance of Contribution:** The proposed ChemAgent framework shows significant improvements over existing methods on chemical reasoning tasks. The ability to handle complex multi-step calculations and integrate code effectively is particularly noteworthy. The empirical results, especially the substantial performance gains across different datasets, underscore the practical value of the proposed approach.\n\n**Clarity of Presentation:** The paper is well-organized, and the terminology is appropriately used. The figures and tables support the narrative effectively. However, the paper could benefit from clearer explanations of some technical concepts, particularly in the sections on library construction and memory management.\n\n**Alignment with Journal Scope:** ICLR focuses on advances in machine learning and artificial intelligence. The paper\u2019s focus on chemical reasoning tasks fits well within this scope, as it demonstrates the application of advanced AI techniques to solve real-world problems in a specific domain.\n\n### Strengths\n\n1. **Novel Framework**: ChemAgent provides a new approach to improving LLMs for complex tasks, offering a fresh perspective.\n2. **Empirical Validation**: Strong empirical validation across multiple datasets supports the claims.\n3. **Innovative Use of Memory**: The dynamic library system is a unique feature that significantly enhances performance.\n4. **Detailed Methodology**: Clear and thorough description of the framework and its implementation.\n\n### Weaknesses\n\n1. **Clarity of Some Concepts**: Some technical concepts, especially in the library construction and memory management sections, could be more clearly explained.\n2. **Limited Discussion on Limitations**: There is limited discussion on potential limitations or edge cases that the framework might not cover.\n3. **Potential for Further Work**: The authors could discuss avenues for future work, such as expanding the library to include more sub-tasks or exploring different types of memory structures.\n\nOverall, the paper presents a compelling and innovative approach to improving LLMs for chemical reasoning tasks. The novelty, empirical validation, and detailed methodology make it a strong candidate for acceptance.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636642.657083", "msecs": "657.0", "relativeCreated": "103350.22354125977", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Output-Constrained Decision Trees\n\n# Abstract\nWhen there is a correlation between any pair of targets, one needs a prediction method that can handle vector-valued output. In this setting, multi-target learning is particularly important as it is widely used in various applications. This paper introduces new variants of decision trees that can handle not only multi-target output but also the constraints among the targets. We focus on the customization of conventional decision trees by adjusting the splitting criteria to handle the constraints and obtain feasible predictions. We present both an optimization-based exact approach and several heuristics, complete with a discussion on their respective advantages and disadvantages. To support our findings, we conduct a computational study to demonstrate and compare the results of the proposed approaches.\n\n\n# 1 INTRODUCTION\n\n\nDecision trees (DTs) play a fundamental role in machine learning and data science for several key reasons. First, DTs have proven to be accurate predictive models in a wide range of applications. Moreover, the heuristic approaches used for their construction are computationally efficient, resulting in extremely fast training times. Accuracy and speed make them ideal for large datasets or real-time applications. Second, DTs, particularly those that are not too deep, are inherently explainable. By following the branches of such a trained tree, one can easily understand the logic behind the model's predictions. This feature is particularly valuable in sectors where interpretability is critical, such as in healthcare or finance. Lastly, DTs serve as the base learners in powerful ensemble methods. These methods, which include techniques like random forests and gradient boosting, utilize numerous DTs to create robust and accurate predictive models.\n\n\nWhen there is a correlation between any pair of targets, we need a prediction method that can handle vector-valued output. In this setting, multi-target learning is particularly important as it is widely used in fields such as forecasting, monitoring, manufacturing, and recommendation systems. DTs can easily be customized for handling multi-target regression by modifying the splitting criteria to handle vector predictions at each node.\n\n\nMotivation. Despite the wealth of existing literature on DTs, to the best of our knowledge, none consider cases where constraints exist among multiple targets. This is notable, as in applications, decision-makers are aware of these constraints, and data analysts should impose them for feasible predictions. Here is an illustrative example from the field of operations management where machine learning approaches are becoming more prominent in the context of decision-making; see e.g., (Mi\u0161i\u0107 & Perakis, 2019).\n\n\nConsider a dataset of products in a retail store, each with distinct attributes. This dataset includes the inventory amounts of these products across five different warehouses, represented as five-dimensional target vectors (training set). The goal is to predict the inventory amounts for new products across these warehouses (test set).\n\n\nTo achieve this, one can train a DT and obtain the partitions of the existing products represented by the leaf nodes of the trained tree. The prediction for a new product is then simply the average of the inventory amounts (mean vector) within the leaf node that it falls into. This leads to the prediction of five different inventory amounts, i.e., one vector for each warehouse.\n\n\nHowever, it is crucial to consider an additional constraint in our scenario: Due to budget restrictions, a store can only transport a product in a maximum of two out of the five warehouses. This is also evident in the training data set where each target vector has at most two nonzero values. Consequently,\n\n\n![Figure 1: The trade-off between accuracy and feasibility. The red circle in the right plot marks the depth of the tree which returns the lowest mean squared error value for the test set. The corresponding red bar in the left plot shows the corresponding percentage of infeasible predictions. The sample size is 4000.](images/d6bdc19df95d34dbaea9eb56a66a1ba4b1a522ba2c4424d8f9d57ba3b851a5fd.jpg)\n\n\n![](images/2bb04fadddcf6e320cd4122856a38a38c98455662b65df6dea09f31ed66cada8.jpg)\n\n\nif one continues to use the mean vector approach for prediction, it may not only yield inaccurate results but also infeasible predictions because it does not account for this constraint.\n\n\nThis erroneous estimation would also persist with many well-known DT training algorithms, since their estimation at a leaf node is also based on taking the mean or the median of the target vectors within the node. However, one may argue that growing a deeper DT may resolve the infeasibility issue as there will be fewer products at each leaf node. Even if we overlook the loss of interpretability due to the large size of such a tree, it is well-known that deep trees can easily overfit to the training data, potentially leading to poor generalization for new products. For instance, Figure 1 shows our results with one of our datasets from our computational study in Section 3. The bar plot demonstrates that feasible predictions can be achieved with larger tree depths, but this comes with a significant decrease in accuracy as shown in the scatter plot. The scatter plot, on the other hand, shows that while shallow trees offer much better accuracy at the expense of most of their predictions are being infeasible.\n\n\nOutput-Constraints. To obtain accurate and feasible predictions, we propose an optimization-based exact approach. This approach incorporates target constraints into the splitting procedure of common binary tree construction methods. Our approach ensures that the estimated target vector at each node meets the required constraints. However, we observe that the exact approach could have a lengthy computation time, especially for deep trees. To address this, we offer two heuristic approaches. These are based on using existing samples as predictions or relaxing the constraints with a penalty function at each split operation.\n\n\nUp until now, our focus has been on regression problems. It is reasonable to question why we have not considered multi-target classification problems as well. The reason is, if the number of classes for each target and the number of targets are relatively small, we can assign a label to each feasible combination of target values. This approach effectively reduces the problem to a standard single-target classification problem, although possibly with many labels.\n\n\nRelated Work. As machine learning methods improve in accuracy, they may still produce predictions and outcomes that lack physical consistency or feasibility, due to biases or data sparsity (Zhang et al., 2022). To overcome this challenge, hybrid methods that integrate optimization techniques with machine learning models have gained significant attention in recent years due to their ability to leverage the strengths of both approaches. One key aspect of hybrid approaches is their ability to handle constraints from various sources to enhance the feasibility and accuracy of machine learning methods. These constraints can originate from the dataset, user-defined specifications, or domain knowledge (Nanfack et al., 2022). Such machine learning approaches with imposed constraints are developed as practical applications across different domains, such as mechanics (Yan et al., 2022), geomechanics (Chen & Zhang, 2020), and electronics (Lee et al., 2019).\n\n\nTraditional supervised learning methods primarily focus on predicting a single output, using data comprised of instances represented by features and a single response variable. Multi-target regression (also referred to as multi-output regression), is used to predict multiple continuous target variables simultaneously. One of the earliest methods for multi-target regression with decision trees was introduced by De'ath (2001). He extended the univariate recursive partitioning approach from Classification and Regression Trees (CART) (Breiman, 2017) to handle multiple responses, calling this method Multi-Response Trees (MRT). MRTs are built similarly to CART, but the node impurity is redefined as the sum of squared errors across all response variables. Struyf & D\u017eeroski (2006) developed a system for building multi-objective regression trees (MORTs), which include constraints on tree size and accuracy, allowing users to balance interpretability and accuracy by limiting tree size or setting a minimum accuracy threshold. (Adiyeke & Baydo\u011fan, 2022) introduced a novel method combining multi-target regression with unsupervised learning. Specifically, they modified tree construction by incorporating a similarity-based clustering indicator in ensemble methods.\n\n\nDecision trees (DTs) offer some flexibility in enforcing constraints, and many hybrid DT approaches incorporate instance-level constraints to bind a subset of instances in generating meaningfully combined results. For instance, the clustering tree algorithm by Struyf & D\u017eeroski (2007) integrates domain knowledge to impose must-link and cannot-link constraints. DT methods have also been developed to address adversarial examples, which can be considered instance-level constraints (Chen et al., 2019; Calzavara et al., 2019), or attribute-level constraints to assign several features to the predictors (Nanfack et al., 2022). Ben-David (1995) was one of the first to apply attribute-level constraints by imposing monotonicity among the model's predictors, and Cardoso & Sousa (2010) extended this in an ordinal classification setting.\n\n\nOne of the earliest hybrid approaches of imposed constraints was implemented by Lee & Kang (1990), who introduced constraints to deep learning architectures to approximate ordinary and partial differential equations. This approach laid the foundation for the general framework known as physics-informed neural networks, proposed by Raissi et al. (2019), where constraints and boundary conditions are incorporated (Rudin et al., 2022). Similarly, Christopher et al. (2024) integrated constraints and physical principles to generative diffusion processes. To the best of our knowledge, our proposed approach is the first method designed to handle target-level constraints in multi-target regression. This represents a novel contribution, building on the principles of constraint handling and extending them to more complex, interdependent target structures in predictive modeling.\n\n\nContributions. We introduce novel approaches to handle constraints among target variables in DTs. The first method lends itself to an optimization-based exact method that ensures the feasibility of predictions by incorporating target constraints into the splitting procedure of common binary tree construction methods. We also propose two heuristic methods. These methods offer practical solutions to the computation time issue that might be associated with the exact approach, especially for deep trees. We provide the technical details of our proposed methods, exploring their strengths, potential limitations, and the conditions under which they would perform sensibly. To validate our approaches, we provide a reproducible computational study using our publicly available implementation.\n\n\n# 2 OUTPUT-CONSTRAINED DECISION TREES\n\n\nSuppose that we have the training dataset  $\\mathcal{D} = \\{(\\pmb{x}_i, \\pmb{y}_i) : i \\in \\mathcal{I}_{\\mathcal{D}}\\}$  with  $\\pmb{x}_i \\in \\mathbb{R}^p$  and  $\\pmb{y}_i \\in \\mathbb{Y} \\subseteq \\mathbb{R}^k$  denoting the input vector and the target (output) vector for the data point  $i$ , respectively. The set  $\\mathbb{Y}$  shows the feasible set for the target vectors. It is important to observe that the target vector of each sample lies in  $\\mathbb{Y}$ , and hence, they are also feasible.\n\n\nA DT is grown by splitting the dataset into non-overlapping subsets recursively (Breiman et al. 1984). Each subset is represented by a node of the tree. The most common approach of tree construction uses binary splits; that is, at each iteration at most two child nodes, left  $(\\leftarrow)$  and right  $(\\rightarrow)$ , are grown from a parent node. Let  $\\mathcal{N} \\subseteq \\mathcal{D}$  be the subset denoted by the parent node, and  $\\mathcal{I}_{\\mathcal{N}} \\subseteq \\mathcal{I}_{\\mathcal{D}}$  indicate the indices of the samples in that node. To obtain a pair of child nodes, a feature  $j$  and a split value  $v$  is selected to obtain the left and right index sets\n\n\n$$\n\n$$\n\\mathcal {I} _ {\\mathcal {N}} ^ {\\leftarrow} (j, v) = \\{i \\in \\mathcal {I} _ {\\mathcal {N}}: x _ {i j} \\leq v \\}\n$$\n\n$$\n\n\nand\n\n\n$$\n\n$$\n\\overrightarrow {\\mathcal {I} _ {\\mathcal {N}}} (j, v) = \\{i \\in \\mathcal {I} _ {\\mathcal {N}}: x _ {i j} > v \\},\n$$\n\n$$\n\n\nrespectively. Then, the gain by such a split is evaluated by\n\n\n$$\n\n$$\n\\alpha_ {\\mathcal {N}} ^ {\\leftarrow} (j, v) \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}} ^ {\\leftarrow} (j, v)} \\ell (\\boldsymbol {y} _ {\\mathcal {N}}, \\boldsymbol {y} _ {i}) + \\alpha_ {\\mathcal {N}} ^ {\\rightarrow} (j, v) \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}} ^ {\\rightarrow} (j, v)} \\ell (\\boldsymbol {y} _ {\\mathcal {N}}, \\boldsymbol {y} _ {i}), \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\pmb{y}_{\\mathcal{N}} \\in \\mathbb{Y}$  is the prediction of the parent node,  $\\ell: \\mathbb{R}^k \\times \\mathbb{R}^k \\mapsto \\mathbb{R}$  is the loss function,  $\\alpha_{\\mathcal{N}}^{-}(j,v) = \\frac{|\\mathcal{I}_{\\mathcal{N}}^{-}(j,v)|}{|\\mathcal{I}_{\\mathcal{N}}|}$  as well as  $\\alpha_{\\mathcal{N}}^{+}(j,v) = 1 - \\alpha_{\\mathcal{N}}^{-}(j,v)$  are the scaling coefficients representing the proportional weights of the respective subsets. Among all possible splits, the one that achieves the largest gain is used to construct the child nodes. The splitting continues until one of the stopping conditions, such as reaching the maximum depth or the minimum number of samples in a node, is satisfied. Then, the samples in the last parent constitute a leaf node. Considering the feasibility requirements on the predictions, the output vector  $\\pmb{y}_{\\mathcal{N}}$  of such a leaf node should be feasible; i.e., it should belong to the feasible set,  $\\pmb{y}_{\\mathcal{N}} \\in \\mathbb{Y}$ .\n\n\n# 2.1 EXACT APPROACH\n\n\nThe common loss functions (e.g., mean squared error, mean Poisson deviance) used in constructing binary DT regressors are separable. Moreover, the loss in the multi-target case is obtained by summing up individual losses for each target. Consider, for instance, the well-known mean squared loss for which the prediction at a node with the subset  $\\mathcal{N}$  is given by\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {Y}} \\frac {1}{2 | \\mathcal {I} _ {\\mathcal {N}} |} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\| \\boldsymbol {y} - \\boldsymbol {y} _ {i} \\| ^ {2}.\n$$\n\n$$\n\n\nIf there are no restrictions on the output vectors, i.e.,  $\\mathbb{Y} = \\mathbb{R}^k$ , then it is not difficult to observe by simple differentiation that  $\\pmb{y}_{\\mathcal{N}}$  is the mean of the target vectors in  $\\mathcal{N}$  minimizing the subset variance. This is, in fact, the output of a node used by the well-known implementations, like scikit-learn (Pedregosa et al. 2011). The same holds for the mean Poisson deviance loss function. In fact, the following theorem shows that the mean vector is still the optimal choice when the constraints on the output vectors result in a convex feasible set. The proof of this theorem follows from observing that the mean operation is equivalent to taking the convex combination of the feasible target vectors in the node. The details of this proof are given formally in Section A of the supplementary file. There, we also present a counter example showing that the same argument does not apply when the loss function is the mean absolute deviation and its unconstrained minimum, i.e., the median of the target vectors, is selected as the node's output (see Remark B in Section A of the supplementary file).\n\n\nTHEOREM 1 Let the feasible set  $\\mathbb{Y} \\subseteq \\mathbb{R}^k$  be convex. If the loss function is the mean squared error or the mean Poisson deviance, then the mean of the target vectors in a node minimizes the loss.\n\n\nThis theorem implies that a restricted class of feasible sets can be handled with standard DT implementations using certain loss functions. For instance, simple bounds or, more generally, linear constraints on the target vectors lead to convex feasible sets. However, the same conclusion does not hold when the feasible set is not convex. Going back to our illustrative example of inventory amount prediction, suppose for simplicity that each warehouse has the same capacity of  $C$  units. If we use only these bounds as constraints, then any standard implementation returning the mean vector for each node output would not violate this constraint. However, if we also impose the budget restriction \"at most two among five warehouses can be used for transportation,\" then we need auxiliary binary variables to represent this restriction as a constraint, and hence, the feasible set  $\\mathbb{Y}$  becomes non-convex. Formally,\n\n\n$$\n\n$$\n\\mathbb {Y} = \\left\\{\\boldsymbol {y} \\in \\mathbb {R} ^ {5}: \\sum_ {t = 1} ^ {5} z _ {t} \\leq 2, 0 \\leq y _ {t} \\leq C z _ {t}, \\text {a n d} z _ {t} \\in \\{0, 1 \\} \\text {f o r} t = 1, \\dots , 5 \\right\\}, \\tag {2}\n$$\n\n$$\n\n\nwhere the binary variable  $z_{t}$  becomes one if  $y_{t}$  units of the product is transported to warehouse  $t$ . Note that when  $z_{t}$  is zero, then it also forces  $y_{t}$  to become zero implying that product is not transported to the corresponding warehouse.\n\n\nWe can now state the formal prediction model that needs to be solved at each node:\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {Y}} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\ell (\\boldsymbol {y}, \\boldsymbol {y} _ {i}), \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\mathcal{I}_{\\mathcal{N}} \\subseteq \\mathcal{I}_{\\mathcal{D}}$  with  $\\mathcal{N} \\subseteq \\mathcal{D}$ . Dealing with nonconvex sets can pose a challenge when solving large-scale constrained models. Fortunately, the dimension of the problem  $(k)$  is quite small for many applications. In case of a mixed integer linear optimization problem, like our example on warehouse inventories, both free and commercial powerful solvers are available, e.g., COIN-OR (Lougee-Heimer, 2003), GUROBI (Gurobi Optimization, LLC, 2023). However, it is also important to remember that this optimization problem must be solved for each pair of feature-split values at every node. This increases the computation time compared to basic DT implementation that ignores the constraints, especially for deep trees. Next, we will also propose several heuristic approaches to develop faster -albeit potentially less accurate or infeasible- alternatives.\n\n\n# 2.2 HEURISTIC APPROACHES\n\n\nA common method for training machine learning models with restrictions involves relaxing these constraints and adding a term to the loss function. This term serves to penalize predictions that are not feasible. In our setting, this idea leads to the following model:\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} ^ {R} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {R} ^ {k}} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\ell (\\boldsymbol {y}, \\boldsymbol {y} _ {i}) + \\lambda v _ {\\mathbb {Y}} (\\boldsymbol {y}), \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\lambda > 0$  is the predefined penalty coefficient, and  $v_{\\mathbb{Y}}: \\mathbb{R}^k \\mapsto \\mathbb{R}_+$  is the penalty function that assigns a positive value to the infeasible predictions,  $y \\notin \\mathbb{Y}$ . This technique is a reminiscent of the approach known as Lagrangean relaxation. However, it is important to note that Lagrangean relaxation would introduce a separate penalty coefficient, known as Lagrangean multiplier, for each constraint (Bertsekas, 1982). The objective function of (4) shows the trade-off between accuracy and feasibility. For instance, using the feasible region defined in (2) leads to the following splitting problem for the mean-squared loss:\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} ^ {R} = \\arg \\min  _ {\\boldsymbol {y} \\in \\mathbb {R} ^ {5}, \\boldsymbol {z} \\in \\{0, 1 \\} ^ {5}} \\frac {1}{2 | \\mathcal {I} _ {\\mathcal {N}} |} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\| \\boldsymbol {y} - \\boldsymbol {y} _ {i} \\| ^ {2} + \\lambda \\sum_ {t = 1} ^ {5} \\left(\\left(z _ {t} - 2\\right) + \\left(y _ {t} - C z _ {t}\\right)\\right). \\tag {5}\n$$\n\n$$\n\n\nHere,  $\\lambda$  in (4) plays the role of balancing optimality versus feasibility. As  $\\lambda$  increases, the optimization algorithm tends to favor feasible solutions over accurate ones. Conversely, for small values of  $\\lambda$ , accuracy is prioritized. Therefore, one is not guaranteed to obtain a feasible solution with the relaxation approach. Additionally, depending on the structure of the penalty function  $v_{\\mathbb{Y}}$ , problem (4) can be difficult to optimize. Note that tuning  $\\lambda$  requires attention as the optimization problem changes at each node. Attempting to adjust a different penalty coefficient for each node could quickly become a computationally formidable task.\n\n\nOur second heuristic is based on observing that all samples in a node belong to the feasible set. Thus, selecting one of them results in a feasible prediction. Considering the spread of the output values in the subset, a natural candidate could be the medoid as it would be a representative sample within the cluster designated by the node. The medoid of the subset is obtained by solving\n\n\n$$\n\n$$\n\\boldsymbol {y} _ {\\mathcal {N}} ^ {M} = \\arg \\min  _ {\\boldsymbol {y} _ {i ^ {\\prime}} \\in \\mathcal {N}} \\sum_ {i \\in \\mathcal {I} _ {\\mathcal {N}}} \\ell \\left(\\boldsymbol {y} _ {i ^ {\\prime}}, \\boldsymbol {y} _ {i}\\right). \\tag {6}\n$$\n\n$$\n\n\nThe solution to this problem involves evaluating the pairwise loss values and then summing these values for each sample. As a result, the medoid of a subset of samples can be found very quickly. In case there are multiple samples that could be the medoids, one of them can be selected arbitrarily. Clearly, the medoid at each node is a feasible prediction but not necessarily the optimal solution of (3). At this point, we note that a tree can be grown very deep so that each leaf node contains a single sample. While this allows for feasible predictions, such deep trees are susceptible to overfitting and can quickly lose interpretability (see also Figure 1).\n\n\nSimilar to the exact approach of the previous subsection, both heuristic approaches above are applied to every feature and split value at each node. An alternative heuristic would be growing a tree as in basic implementation without considering the constraints. Then, the feasibility of DT's prediction can be guaranteed by solving the exact model (3), the medoid model (6), or the relaxation model (4) with an appropriate choice of the penalty coefficient- at the leaf nodes only. This approach would improve the training time significantly.\n\n\n# 3 COMPUTATIONAL STUDY\n\n\nThis section is dedicated to testing the performance of our exact and heuristic approaches proposed for training output-constrained decision trees (OCDTs). We obtain different implementation variants by using one of the proposed approaches at a decision node for splitting (S) or at a leaf (L) node for prediction. We also use the shorthand notation O, R, M, A for the exact optimization, relaxation, medoid, and average approaches, respectively. For instance, the variant that uses relaxation approach for splitting and the medoid approach for prediction is denoted by S(R):L(M). Likewise, S(A):L(A) simply refers to standard DT implementation in software packages that ignores the constraints. In our study, we have used the scikit-learn implementation only for this variant (Pedregosa et al., 2011).\n\n\nOur objective in this section is two-fold: First, we aim to assess the effectiveness of our proposed approaches on an existing dataset. Second, we aim to evaluate their performance in a controlled setting using an extended version of the same dataset. Our computational experiments can be reproduced by using the dedicated GitHub repo<sup>2</sup>. For all methods compared, we experimented with varying maximum depth values of 3, 6, 9, 12, and 15, while ensuring a minimum of 10 data points to split a node and at least 5 data points in each leaf. The selection of penalty coefficients,  $\\lambda$ , for each variant can be guided by evaluating several values over a reasonable range through cross-validation. Ideally, the chosen coefficient should minimize MSE while ensuring that no infeasible predictions are generated. The results were obtained using a computer equipped with an Intel(R) Core(TM) i7-9750H CPU 2.60GHz processor, featuring 12 CPUs, and 64GB of RAM.\n\n\n# 3.1 SCORE DATASET\n\n\nAs a part of our computational study, we employ the scores dataset available online (Kimmons, 2012). This dataset consists of 1000 records, simulating various personal and socio-economic factors of students at a public school. The dataset features three target variables, representing math, reading, and writing scores. To tailor the data to our research objectives, we have imposed two constraints: First, if the reading score falls below 50, the writing score is set to zero. Second, if the combined reading and writing scores amount to less than 110, the math score is also set to zero. The full description of our mathematical model is available in Section B of the supplementary file.\n\n\nWe conducted a thorough five-fold cross-validation experiment on the dataset using five different tree-depth parameter values, employing a total of nine distinct variants which are trained with the same parameters. These variants are classified into two primary components: split evaluation and leaf predictions. Split evaluation entails the assessment of each candidate for binary splitting based on the prediction generated by a specific method. Conversely, leaf prediction refers to the prediction method employed by each leaf node. For instance, the methodology involving the medoid split evaluation and optimal leaf prediction entails that each split during tree growth is determined by evaluating the total mean squared error (MSE) values of the resulting child nodes using their medoids as predictions. Subsequently, the resulting DT solves an optimization problem specific to the dataset for data falling to each leaf to assign the prediction of that particular node. The penalty coefficient  $\\lambda$  is selected by monitoring the trade-off between feasibility and accuracy through five-fold cross-validation.\n\n\nWe evaluated the performance of each variant based on the MSE value obtained during the cross-validation process. Our analysis focused on comparing the performance of an OCDT variant with different split evaluation and leaf prediction approaches against the baseline DT implementation (scikit-learn). The results are visualized in the left side of Figure 2 illustrating the key metrics concerning the performance of the variants. The bar plot reflects the (scaled) average MSE values with the left axis indicating the corresponding values. On top of each bar, the MSE values are displayed alongside the average number of infeasibilities resulting from the methods' predictions (in parenthesis). The scatter plot shows the average training time of each method with the right axis denoting the corresponding values.\n\n\nIn general, it appears that it is important to assess the performance of each variant individually. We find that the variant utilizing relaxation in the optimization problem in both split evaluation and leaf prediction,  $S(R) : L(R)$  exhibits the worst MSE score. However, the variant employing relaxation\n\n\n![Figure 2: Performance metrics plot (left) and the Friedman test for statistical differences (right) on the scores dataset. The values on each bar represent the average MSE value and the average number of infeasible predictions (in parenthesis) on the test sets. The scatter plot shows the training time for each variant. Note that MSE and computation times are scaled according to the maximum value for the corresponding performance measure obtained by one of the variants.](images/d759bec6808ea51c2bf1bad09f120741cdd224c9bad438dce17f8ba68683bee4.jpg)\n\n\n![](images/a8391e071bd991b506808318d085eac9c0dff9793512688c505bc2e8a32c19b6.jpg)\n\n\nin split evaluation and optimization in leaf prediction,  $S(R) : L(O)$  achieves the best MSE score without violating any constraints. Overall, we observe that the variants solving optimization problems at the leaf nodes for predictions demonstrate the top three performances for this dataset with no constraint violations. This affirms both the efficiency of the proposed variants and their adherence to constraints. We note that the baseline scikit-learn implementation,  $S(A) : L(A)$  violated numerous constraints (averaging 16.4 infeasible predictions), aligning with our earlier findings.\n\n\nThe comparison of the proposed variants for statistical significance is conducted using the Friedman test (Friedman, 1940), and if a significant difference is detected by the Friedman test, it is followed by the Nemenyi test (Nemenyi, 1963). The right side of Figure 2 shows the test results. We observe that the  $p$ -value is practically zero indicating that the null hypothesis of identical variants can be rejected at the 0.95 confidence level. This means that there is a statistically significant difference in the performances of the variants with S (R): L (O) having a notably better performance when compared against the others.\n\n\n# 3.2 EXTENDED DATASETS\n\n\nIn this section, we extended the previous dataset (Kimmons 2012) to mimic the motivating inventory control example that we have given in Section I. To test the proposed approaches in this controlled environment, we imposed other restrictions on the dataset, like each target vector having at most one nonzero value. The details of our data generation process along with the underlying mathematical model is given in Section C of the supplementary file. We call this dataset class.\n\n\nIn accordance with the experiments in Section 3.1, we apply the same methodology with five-fold cross-validation. This is done to compare our nine variants using the same parameters. To leverage the advantage of data generation, we evaluated our proposed variants across nine different datasets, varying in size  $(|\\mathcal{I}_{\\mathcal{D}}|)$  and the number of targets  $(k)$ . These datasets comprised of combinations of 500, 1000, and 2000 samples with total number of target values of 3, 5 and 7.\n\n\nThe MSE score and computation time are depicted on the left side of Figure 3, where the average metrics across all nine datasets are given. On the right side of Figure 3 we see that  $p$ -value is close to 0, which indicates that the methods have statistically different performances at the 0.95 confidence level. We observe that none of the proposed algorithms produce infeasible predictions, unlike the baseline scikit-learn method, S(A):L(A) which, despite yielding the lowest MSE among all approaches, generated the only positive average number of infeasible predictions. This suggests that the scikit-learn method compromised constraints to improve the prediction performance, a trade-off that is clearly not suitable for applications.\n\n\nThe box plot in Figure 4 illustrates the MSE values of all the variants across nine datasets with different sizes in terms of sample-sizes and targets. The proposed variants show comparable MSE scores, with most of the methods achieving scores within a close range, as indicated by the overlapping\n\n\n![Figure 3: Performance metrics plot (left) and the Friedman test for statistical differences (right) on class datasets. The values on each bar of the left plot represent the average MSE value and the average number of infeasible predictions (in parenthesis) on the test sets. The scatter plot shows the training time for each variant. Note that MSE and computation times are scaled according to the maximum value for the corresponding performance measure obtained by one of the variants.](images/ea99b744bf5065b2de52e1733bbc5ecd95b1ec6e9542e885200cbd0489f85569.jpg)\n\n\n![](images/882d8c00bb55aefbb529f955d0916f33c240c7a2651c296cd17f29e92645c454.jpg)\n\n\n![Figure 4: MSE box plot on the class datasets.](images/d63e0ff817c61272c47a7af1313c894f47174d8784d719a1532a0cee31511ae0.jpg)\n\n\ninterquartile ranges. While there is some variation, the results suggest that none of the methods stand out as consistently superior in terms of accuracy.\n\n\nFigure 5 depicts that the training times of the variants vary significantly with those involving optimization and relaxation, such as S (R): L (R) and S (R): L (M), taking notably longer. This observation aligns with the computational complexity expected from optimization-based approaches. Another noteworthy observation is that the methods with relatively short training times, like S (A): L (M), tend to have higher MSE values. This trade-off between model complexity and training time versus prediction accuracy is evident across the board. Furthermore, variants like S (O): L (M) strike a balance, achieving moderate MSE values while maintaining competitive training times, suggesting a promising option for practical use in time-sensitive applications.\n\n\nFigure 6 illustrates a schematic representation of a tree trained on a class dataset. This tree structure is obtained by solving the optimization problem for the split evaluation as well as for the leaf prediction; i.e., the selected variant is  $S(\\mathsf{O}) : \\mathsf{L}(\\mathsf{O})$ . In this figure, the green nodes signify the leaf nodes, while the red nodes represent the decision nodes where the data is split. Additionally, the splits after each decision node are depicted with arrows indicating the left and the right child nodes, accompanied by\n\n\n![Figure 5: Training time box plot on the class datasets.](images/a2a177166d8584ea2899caa573943b423d10617c0985d2e400c464eb417dc84c.jpg)\n\n\n![Figure 6: Visualization of an OCDT model on a class dataset.](images/9713a27d8cdc5a9db062b2c0c34fc7ad87fee56800f83fa43268dc638e9b3f95.jpg)\n\n\ntext specifying the split feature and threshold. It is easy to observe on the tree plot that the OCDT made splits are both accurate and feasible. As we outline in Section C of the supplementary file, the target values are generated in a way that they are dependent on Grade and GradeAvgPrevElec. This is clearly depicted in the tree plot as the majority of splits employed by the OCDT do use those features. Additionally, the second-level splits in the tree plot are dependent on feature Class being one or higher, which aligns with the conditions imposing constraints to assign some targets to zero. This visualization example shows that OCDTs have the potential to be interpretable like their conventional counterparts available in standard DT implementations.\n\n\nOur computational study shows that the proposed approach is highly effective in meeting constraints while delivering strong performance on various datasets. By incorporating optimization in split evaluations and leaf predictions, our methodology offers more accurate and feasible predictions compared to traditional methods, even on complex datasets. These results emphasize the usability and versatility of our approach, particularly in situations where compliance with constraints is essential for success.\n\n\n# 4 CONCLUSION\n\n\nWe have introduced new methods to handle constraints among target variables in decision trees, an area yet unexplored in the existing literature. We have proposed an optimization-based exact approach and several heuristic approaches to ensure feasible predictions and maintaining computational efficiency. Through a computational study with existing and generated datasets, we have demonstrated the effectiveness of the proposed methods in producing feasible predictions, showcasing their potential for applications in multi-target learning.\n\n\nLimitations. The work presented here has a few limitations. First, the optimization-based exact method can be computationally demanding for deep trees, which may impede its application for larger datasets. Second, the heuristic approaches proposed may not always produce accurate or even feasible predictions, limiting their reliability. Third, when the constraints among targets are highly non-convex, solving the prediction problem at each node can prove challenging, potentially affecting the overall performance of the decision tree. Finally, in its current form, it is unclear how these output-constrained decision trees can serve as the base learners for ensemble methods.\n\n\nFuture research. We aim to investigate the integration of our approach into bagging methods like random forests, as trees often act as base learners in these methods. We are also interested in exploring a warm-start method in the exact approach, where an optimization problem needs solving at each node. Given that these nodes share a parent-child relationship, the subsets they create could aid in devising this method for subsequent problems at the child nodes once the parent node is resolved. We are also considering the inclusion of target constraints in optimization-based algorithms for training trees like, for example, optimal classification trees (Bertsimas & Dunn, 2017). Although these algorithms can currently handle smaller problem sizes compared to standard implementations like CART (Breiman et al., 1984), this is a promising research direction. We are also looking into the potential advantages of considering constraints in clustering, since training a decision tree can be seen as a supervised clustering method.\n\n\n# REFERENCES\n\n- Esra Adiyeke and Mustafa G\u00f6k\u00e7e Baydo\u011fan. Semi-supervised extensions of multi-task tree ensembles. Pattern Recognition, 123(1):1-16, 2022. ISSN 0031-3203.\n\n- Arie Ben-David. Monotonicity maintenance in information-theoretic machine learning algorithms. Machine Learning, 19:29-43, 1995.\n\n- D.P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Computer Science and Applied Mathematics: A Series of Monographs and Textbooks. Academic Press, 1982.\n\n- Dimitris Bertsimas and Jack Dunn. Optimal classification trees. Machine Learning, 106(7):1039-1082, 2017.\n\n- Leo Breiman. Classification and Regression Trees. Routledge, New York, 2017.\n\n- Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. Cart. Classification and Regression Trees, 1984.\n\n- Stefano Calzavara, Claudio Lucchese, and Gabriele Tolomei. Adversarial training of gradient-boosted decision trees. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pp. 2429-2432, 2019.\n\n- Jaime S Cardoso and Ricardo Sousa. Classification models with global constraints for ordinal data. In 2010 Ninth International Conference on Machine Learning and Applications, pp. 71-77. IEEE, 2010.\n\n- Hongge Chen, Huan Zhang, Duane Boning, and Cho-Jui Hsieh. Robust decision trees against adversarial examples. In International Conference on Machine Learning, pp. 1122-1131. PMLR, 2019.\n\n- Yuntian Chen and Dongxiao Zhang. Physics-constrained deep learning of geomechanical logs. IEEE Transactions on Geoscience and Remote Sensing, 58(8):5932-5943, 2020.\n\n- Jacob K Christopher, Stephen Baek, and Ferdinando Fioretto. Constrained synthesis with projected diffusion models, 2024. URL https://arxiv.org/abs/2402.03559\n\n- Glenn De'ath. Multivariate regression trees: A new technique for modeling species-environment relationships. Ecology, 83(4):1105-1117, 2001. ISSN 1939-9170.\n\n- Milton Friedman. A comparison of alternative tests of significance for the problem of m rankings. The annals of mathematical statistics, 11(1):86-92, 1940.\n\n- Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. https://www.gurobi.com\n\n- Royce Kimmons. Exam Scores, 2012. http://roycekimmons.com/tools/generated_data/exams. Accessed on May 20, 2024.\n\n- Hoon Lee, Sang Hyun Lee, and Tony QS Quek. Constrained deep learning for wireless resource management. In ICC 2019-2019 IEEE International Conference on Communications (ICC), pp. 1-6. IEEE, 2019.\n\n- Hyuk Lee and In Seok Kang. Neural algorithm for solving differential equations. Journal of Computational Physics, 91(1):110-131, 1990.\n\n- Robin Lougee-Heimer. The common optimization interface for operations research: Promoting open-source software in the operations research community. IBM Journal of Research and Development, 47(1):57-66, 2003.\n\n- Velibor V. Mi\u0161i\u0107 and Georgia Perakis. Data analytics in operations management: A review. Manufacturing & Service Operations Management, 22(1):158-169, 2019.\n\n- G\u00e9rald\u00edn Nanfack, Paul Temple, and Benoit Fr\u00e9nay. Constraint enforcement on decision trees: A survey. ACM Computing Surveys (CSUR), 54(10s):1-36, 2022.\n\n- Peter Bjorn Nemenyi. Distribution-Free Multiple Comparisons. Princeton University, 1963.\n\n- F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.\n\n- Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686-707, 2019.\n\n- Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable machine learning: Fundamental principles and 10 grand challenges. Statistic Surveys, 16:1-85, 2022.\n\n- Jan Struyf and Sa\u0161o D\u017eeroski. Clustering trees with instance level constraints. In Machine Learning: ECML 2007: 18th European Conference on Machine Learning, Warsaw, Poland, September 17-21, 2007. Proceedings 18, pp. 359-370. Springer, 2007.\n\n- Jan Struyf and Sa\u0161o D\u017eeroski. Constraint based induction of multi-objective regression trees. In Knowledge Discovery in Inductive Databases: 4th International Workshop, Porto, 2006.\n\n- Bicheng Yan, Dylan Robert Harp, Bailian Chen, and Rajesh Pawar. A physics-constrained deep learning model for simulating multiphase flow in 3d heterogeneous porous media. *Fuel*, 313: 122693, 2022. ISSN 0016-2361. URL https://www.sciencedirect.com/science/article/pii/S001623612102559X\n\n- Chunjie Zhang, Renguang Zuo, Yihui Xiong, Xinfu Zhao, and Kuidong Zhao. A geologically-constrained deep learning algorithm for recognizing geochemical anomalies. Computers and Geosciences, 162:105100, 2022. ISSN 0098-3004. URL https://www.sciencedirect.com/science/article/pii/S0098300422000620\n\nResponse: Given the detailed content and the nature of the research described, let's weigh the strengths and weaknesses systematically:\n\n### Strengths:\n1. **Novel Approach**: The paper introduces a novel method to handle constraints among target variables in decision trees, which is a gap in the existing literature.\n2. **Theoretical Foundation**: The authors provide theoretical foundations for handling convex and non-convex feasible sets, and they extend these to multi-target regression settings.\n3. **Comprehensive Evaluation**: The computational study includes extensive cross-validation and statistical analyses, providing robust evidence of the methods' effectiveness.\n4. **Practical Relevance**: The proposed methods are demonstrated to deliver both high accuracy and feasibility, which is crucial for practical applications.\n5. **Flexibility**: The paper offers multiple heuristic approaches, balancing computational efficiency with feasibility.\n6. **Scalability Consideration**: While the optimization-based exact method can be computationally demanding for deep trees, the authors acknowledge this limitation and provide practical solutions.\n\n### Weaknesses:\n1. **Computational Complexity**: The optimization-based exact method is computationally intensive and may not scale well for very large datasets or deep trees. This is highlighted as a limitation.\n2. **Heuristic Limitations**: The heuristic methods may not always produce optimal results, especially when the constraints are highly non-convex. This affects their reliability in certain scenarios.\n3. **Ensemble Integration**: The authors mention that their approach is not immediately compatible with ensemble methods like bagging, which is a significant limitation for broader applicability.\n4. **Data Generation**: The synthetic data used in the study is tailored to the motivating example of inventory control. Generalizability to other datasets and real-world applications is limited.\n\n### Impact Potential:\nThe paper addresses a critical gap in the field by providing a theoretically sound and practically effective solution to the problem of handling constraints in multi-target regression. This could significantly impact the field of machine learning and operations management, especially in contexts requiring robust and feasible predictions.\n\n### Prediction:\nGiven the comprehensive evaluation, theoretical grounding, and practical utility demonstrated, the paper presents a strong case for its acceptance. However, the computational limitations and the need for further exploration of ensemble integration are important considerations.\n\n### Final Prediction:\n\\[\n\\boxed{Accept}\n\\]", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636650.574645", "msecs": "574.0", "relativeCreated": "111267.78554916382", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Adversarial Data Robustness via Implicit Neural Representation\n\n# Abstract\nDespite its effectiveness, adversarial training requires that users possess a detailed understanding of training settings. However, many common users lack such expertise, making adversarial training impossible and exposing them to potential threats. We propose ``adversarial data robustness'', allowing the data to resist adversarial perturbations. Then, even if adversaries attack those data, these post-attack data can still ensure downstream models' robustness at users' end. This leads to our new setup, where we store the data as a learnable representation via Implicit Neural Representation (INR). Then, we can train such a representation adversarially to achieve data robustness. This paper analyzes the possible attacks to this setup and proposes a defense strategy. We achieve a comparable robustness level without resorting to model-level adversarial training.\n\n\n# 1 INTRODUCTION\n\n\nAdversarial training, a common defense mechanism against adversarial attacks, operates at the model level by specifically training deep learning models with adversarial examples. However, model-level adversarial training may undermine the desired performance of deep learning models when the robustness requirement is incorporated. Moreover, introducing large-scale base models also makes model-level adversarial training difficult. Therefore, it is essential to investigate alternatives to enhance these models' robustness without resorting to model-level adversarial training.\n\n\nAs adversarial perturbations are directly injected to images (Szegedy et al., 2014; Madry et al., 2018), a straightforward solution is to make those images robust to adversarial attacks, rather than optimizing a model adversarily like existing solutions (Madry et al., 2018; Zhang et al., 2020). This way, even when adversaries corrupt images as usual, ready-to-use models accessible to ordinary users can continue functioning properly. We call this adversarial data robustness, and propose \"training data adversially\" to achieve this goal. For images, one straightforward approach is to consider image pixels as learnable parameters and optimize these learnable parameters adversarily. However, such a straightforward solution significantly undermines image quality since the pixel values are directly manipulated. Therefore, we employ the Implicit Neural Representation (INR) as the cornerstone for our purposes. INRs represent data by optimizing a neural network to continuously map the coordinates to the corresponding data values (Dupont et al., 2021; Mildenhall et al., 2020; Sitzmann et al., 2020; Chen et al., 2022). Once the optimization converges, image pixels are stored as network weights, and the user can query neural networks with the corresponding coordinates to recover the whole image. Moreover, such a network-based representation framework is compatible with classical adversarial optimization for networks, enabling the generalization of existing model-level adversarial defense techniques.\n\n\nWe envision a setup for adversarial data robustness where typical model users or even the model developers are not burdened with extensive modifications to established frameworks. In this setup, images are pre-converted to their respective INRs and then sent to the model users. These users can then invoke straightforward functions encapsulated within an API to obtain the images for subsequent tasks. Since the generation process of representations incorporates adversarial robustness, images obtained from the post-attack representations maintain stable performance for downstream applications. For example, when the modifications to deep learning models are not permitted, or such modifications are costly, the model developers are not capable of training the model adversarially. By adversarily training the data, the adversarial data robustness can guarantee the function\n\n\n![Figure 1: In the traditional setup, images can be manipulated by adversarial perturbations to fool downstream models. In our setup, we represent images as INRs. By applying a defense strategy at the INR creation stage, the data can defend against adversarial perturbations during data creation and transmission. Downstream models' robustness to adversarial attacks can be ensured without adversarial training for models.](images/ccdc9169b35cf42cf86308814d4b8bb68afccdeb26f9918abba700c058271288.jpg)\n\n\nality of the downstream model without altering the model's parameters. This situation has become common, especially when downstream tasks use frozen large models (Guo et al., 2023), as their weights are difficult to modify.\n\n\nWe first explore possible attack types to the INR used as the data representation. Since only INR representation is used during the data transmission, an attack approach is to add adversarial perturbations to the network parameters for data representation. We, therefore, consider two strategies: 1) Attack during creation, which transfers adversarial perturbations from post-attack images to network parameters; and 2) Attack during transmission, which directly injects perturbations into the representation during its transmission. Given that the network parameters of INR are more susceptible to alterations due to external perturbations (Shu & Zhu, 2019), our emphasis is on balancing between attack efficacy and the quality of images generated from post-attack representations. We propose Double Projection Gradient Descent (DPGD) as a method for conducting attacks during data transmission, which can achieve a better balance by implementing gradient constraints on the image during the backpropagation process.\n\n\nWe propose defense-in-creation to defense above attacks within this setup. As Figure 1 depicts, our goal is to achieve adversarial data robustness in the creation process of INR. The INR generated by our method can defend against attacks during creation and attacks during transmission, ensuring the performance of specific downstream applications. To generate robust INR, we solve a min-max optimization problem by incorporating a robustness loss on top of the existing INR training framework. The weights assigned to these two losses can be used to strike a balance between reconstruction quality and robustness. Our research pioneers data robustness exploration, thereby charting a new course towards AI safety. Our major contribution can be concluded as follows:\n\n- - Our adversarial data robustness can guarantee the robustness of deep learning models against adversarial attacks, without necessitating model-level adversarial training.\n\n- - We examine potential attack types to INR used for the data representation. A Double Projection Gradient Descent (DPGD) is proposed to ensure adversarial patterns' invisibility when directly injected into INR parameters used for data representation.\n\n- - We have formulated a defense-in-creation strategy to defense the possible attacks.\n\n\n# 2 RELATED WORK\n\n\nImplicit neural representations. Implicit Neural Representation (INR) is a technique that leverages neural networks to create a mapping between a coordinate and its corresponding signal value. This method offers a continuous and memory-efficient way to model various signals, including 1D audio (Gao et al., 2022), 2D images (Tancik et al., 2020), 3D shapes (Park et al., 2019), 4D light fields (Sitzmann et al., 2021), and 5D radiance fields (Mildenhall et al., 2020). Direct supervision can be used to train accurate INR models for these signals by comparing the network output to ground truth data or indirect means, such as calculating the loss between the output after differentiable operators and a variant of the ground truth signal. This makes INR a powerful tool for solving inverse problems by taking advantage of the well-known forward processes in these problems. INR has found widespread applications in various fields, including computer vision and graphics (Tewari et al., 2022), computational physics (Karniadakis et al., 2021), biomedical engineering (Liu et al.,\n\n\n2022; Zhu et al., 2022), material science (Chen et al., 2020), and fluid mechanics (Raissi et al., 2020; Reyes et al., 2021). As INR is compatible with adversarial training framework, we propose to use INR as cornerstone to achieve data-level robustness.\n\n\nAdversarial attack and defense. Adversarial attacks and defense have emerged as significant research areas in machine learning. To explore the vulnerability of DNNs, various attacks have been proposed (Carlini & Wagner, 2017; Croce & Hein, 2020; Goodfellow et al., 2015; Madry et al., 2018), which can be generally categorized into white-box attacks and black-box attacks (Goodfellow et al., 2015). Most white-box attacks use gradients to obtain the perturbations on the inputs which maximize the loss function, including the Fast Gradient Sign Method (FGSM) (Szegedy et al., 2014), Projection Gradient Descent (PGD) (Madry et al., 2018) and Carlini and Wagner (CW) (Carlini & Wagner, 2017). Black-box attacks involve two methods, where attackers lack access to the victim models' information. Query-based methods approximate perturbations through a large number of queries (Andriushchenko et al., 2020; Cheng et al., 2018; Guo et al., 2019), while transfer-based methods utilize a surrogate model to generate adversarial examples with higher transferability (Liu et al., 2016; Wang & He, 2021). To counter these attacks, researchers have developed various defense mechanisms (Cohen et al., 2019; Li et al., 2021). One common approach is adversarial training, which has shown relative resistance to most existing attacks. The vanilla adversarial training strategy involves incorporating adversarial examples into the training data to create a min-max game during optimization (Madry et al., 2018). Numerous variants of adversarial training algorithms have been developed to improve the performance of adversarial robustness, including early-stopping strategy (Rice et al., 2020), TRADES (Zhang et al., 2019), FAT (Zhang et al., 2020), and CFA (Wei et al., 2023). Other techniques for adversarial defense include defensive distillation (Papernot et al., 2016), gradient regularization (Ross & Doshi-Velez, 2018), gradient masking (Folz et al., 2020), and input denoising (Liao et al., 2018). LINAC (Rusu et al., 2022) proposes to transform images into INR to train downstream networks to improve the robustness. However, these defense methods, which improve robustness via adversially training downstream networks, are not feasible for networks with unmodifiable parameters and could undermine the desired performance. Therefore, we propose a method to achieve adversarial data robustness to solve this problem.\n\n\n# 3 ADVERSARIAL DATA ROBUSTNESS\n\n\n# 3.1 OUR SETUP\n\n\nOur setup is rooted in the reality that many model users often fail to understand the settings associated with adversarial training. Moreover, model-level adversarial training can potentially compromise the intended performance of deep learning models. Therefore, when the adversarial training to models are difficult for common users, we aim to make the data robust to adversarial perturbations. Then, those robust data can ensure robust deep learning models.\n\n\nGiven that adversarial training has been an effective defense mechanism against adversarial attacks, the robust data can only be created via adversarial training. We thus propose storing the image as a network-based representation and enhancing this representation's robustness to potential adversarial perturbations. As depicted in Figure 1, even if this representation is attacked during its creation or transmission, users can still restore its clean state, ensuring robust performance of downstream tasks. We consider Implicit Neural Representation (INR) as the cornerstone of our formulation. For an image  $I$ , its INR is given by a MLP  $f_{\\theta}:\\mathbb{R}^2\\to \\mathbb{R}^3$  that maps the input spatial coordinate  $\\mathbf{p} = (p_x,p_y)\\in \\mathbb{R}^2$  to its corresponding pixel value  $I(\\mathbf{p})\\in \\mathbb{R}^3$  as  $I(\\mathbf{p}) = f_{\\theta}(\\mathbf{p})$ .  $\\theta$  represents the MLP's trainable parameters, which leave access for our adversarial data robustness.\n\n\nGenerally,  $f_{\\theta}$  can only reach its optimum status via iterative optimization. The optimization can be finished by overfitting  $f_{\\theta}$  to the image by minimizing a reconstruction loss between true pixel values  $I(\\mathbf{p}_{m,n})$  and predicted values  $f_{\\theta}(\\mathbf{p}_{m,n})$  as\n\n\n$$\n\n$$\n\\mathcal {L} _ {\\text {r e c o n}} = \\frac {1}{M \\times N} \\sum_ {m, n} \\| f _ {\\theta} (\\mathbf {p} _ {m, n}) - I (\\mathbf {p} _ {m, n}) \\| _ {2} ^ {2}, \\tag {1}\n$$\n\n$$\n\n\nwhere  $(m,n)$  is the index of the corresponding pixel location, and  $(M,N)$  is size of the image. After optimization, the image can be stored or transmitted as network weights  $\\theta$ .\n\n\n![(a)](images/87b24a3fb2a1bc736048938fd74428d83fa4bda0494ad0bed6c0fa00eb565770.jpg)\n\n\n![(b)](images/5c5afad58216684fa43d367672314ca0f7b01834263d7df896b4b98011535ea3.jpg)\n\n\n![(c)](images/1f03163d253a330cacd4fffb90a50a224c6e4bf5e348542c88ebbaa26468e4fa.jpg)\n\n\n![(d) Figure 2: Samples after attack during creation. (a) are the ground truth images. (b) are the results of (a) being attacked by PGD. (c) are the images reconstructed from INRs optimized for (b). (d) are the residuals between (b) and (c). The output results of the classifier are labeled in the top left corner of each image. The images reconstructed from the representations attacked during their creation are almost the same to the images directly attacked by PGD, with a low average MSE value.](images/b336747d02b7c5ca00b335fb5be02abd7406181c238f9bb0c730ca1fc13985de.jpg)\n\n\nOnce such a representation is received by model users, they can obtain the image  $\\hat{I}$  from this representation by querying INR  $f_{\\theta}$  at each pixel coordinate as  $\\hat{I} (\\mathbf{p}_{m,n}) = f_{\\theta}(\\mathbf{p}_{m,n})$ , where  $(m,n)$  is the pixel index on the image. We define  $\\Psi :f_{\\theta}\\mapsto \\hat{I}$  as the mapping from INR to the reconstructed image. The image  $\\hat{I}$  is then fed into the downstream model to obtain the result.\n\n\nIn light of this setup, we examine the potential attack types to INR. Subsequently, we will delve into the associated adversarial training to ensure data robustness.\n\n\n# 3.2 ATTACK TYPES TO INR\n\n\nAn adversarial attack is to perturb the data used for downstream tasks (Szegedy et al., 2014). Usually, it is performed by introducing adversarial perturbations to input data, which is a designed slight alteration to data that can significantly change the output of downstream models. Since the data has been implicitly represented in our setup, the only way to conduct an adversarial attack is to inject adversarial perturbations into the representation. Similar to an adversarial attack directly for images, two criteria should be guaranteed: 1) invisibility: the perturbations are subtle and remain invisible within the content (Laidlaw et al., 2021); and 2) attacking efficacy: the invisible perturbations can still impair the efficacy of downstream applications (Szegedy et al., 2014). On the basis of these criteria, we explore attack types that can introduce adversarial perturbations to the data representation.\n\n\nAttack during creation. The first option is to attack the representation by transferring adversarial perturbations from corrupted images to the respective network parameters of INRs. In this case, adversaries introduce perturbations to images and subsequently strive to impart these perturbations onto the network parameters of the INR during its construction. Consequently, the adversarial perturbations, when recovered from the representation along with the image content, intensify the loss function for downstream tasks (e.g., cross-entropy loss for image classification).\n\n\nFormally, the adversarial perturbation for the  $i$ -th image  $I_{i}$  can be modeled as the solution of the following optimization problem, which is the same as the classical adversarial attacks (Madry et al., 2018):\n\n\n$$\n\n$$\n\\max  _ {\\Delta I _ {i}} \\quad \\mathcal {L} _ {C E} \\left(g _ {\\phi} \\left(I _ {i} + \\Delta I _ {i}\\right), y _ {i}\\right), \\quad \\text {s . t .} \\| \\Delta I _ {i} \\| _ {p} \\leq \\epsilon , \\tag {2}\n$$\n\n$$\n\n\nwhere  $g_{\\phi}$  denotes the neural network for downstream tasks (e.g., classification (Szegedy et al., 2014)),  $\\Delta I_i$  is the adversarial perturbation for the  $i$ -th image,  $y_i$  is the true label of  $I_i$  for classification task, and  $\\mathcal{L}_{CE}(\\cdot)$  denotes the cross entropy loss function. The  $\\| \\cdot \\|_p$  denotes the  $\\ell_p$ -norm, and the constraint limits the size of perturbation with maximum allowable value  $\\epsilon$  to avoid being recognized by humans. Unless otherwise stated, the infinite norm is adopted by setting  $p = \\infty$  in this paper following previous works (Dong et al., 2023).\n\n\nINRs can accurately capture the very fine details of images (Chen et al., 2021; Str\u00fcmpler et al., 2022). Therefore, during the creation of an INR, the INR weights can accurately represent adversarial perturbation  $\\Delta x$ . The reconstructed image from such an INR retains the adversarial information and can deceive the downstream applications. As shown in Figure 2, the images reconstructed from INR under attack during creation resemble the corrupted images with adversarial perturbation, allowing the reconstructed image to fool the downstream applications.\n\n\n![Figure 3: Framework of defense-in-creation. We encode an image into the parameter of its corresponding INR,  $\\theta$ . Left: we calculate the distance between the reconstructed image and the to-be-fitted image to establish a reconstruction loss for high-quality reconstruction. Right: we introduce adversarial perturbation into the parameter  $\\theta$  during INR creation process, and design a robustness loss to ensure downstream models' robustness by comparing its prediction with the groundtruth.](images/1317a0a9c0b6a1410b2351019b63b34f1561739d020434add906d9d69b17d797.jpg)\n\n\nAttack during transmission. The second strategy is to attack the representation directly. This attack typically occurs during the transmission of the data representation. When the adversaries access the representation, they can directly inject adversarial perturbations into the network parameters. The process of conducting an adversarial attack to the INR representation of  $i$ -th image during its transmission can be concluded as follows:\n\n\n$$\n\n$$\n\\max  _ {\\Delta \\theta_ {i}} \\quad \\mathcal {L} _ {C E} \\left(g _ {\\phi} \\left(\\Psi \\left(f _ {\\theta_ {i} + \\Delta \\theta_ {i}}\\right)\\right), y _ {i}\\right), \\quad \\text {s . t .} \\| \\Delta \\theta_ {i} \\| _ {p} \\leq \\delta , \\tag {3}\n$$\n\n$$\n\n\nwhere  $\\Delta \\theta_{i}$  is the adversarial perturbation for the  $i$ -th image INR, and we also use  $\\ell_{\\infty}$ -norm in Equation 3 to limit the size of the perturbation.\n\n\nA simple method to conduct attack during transmission is to manipulate the INR parameters using the traditional gradient-based attack methods, such as Fast Gradient Sign Method (FGSM) (Szegedy et al., 2014), Projected Gradient Descent (PGD) (Madry et al., 2018), and Carlini and Wagner (CW) (Carlini & Wagner, 2017). These attacks are referred to as FGSM for INR, PGD for INR, and CW for INR, respectively. While these methods are initially designed to operate on images, integrating INR into the differentiable pipeline allows us to derive the gradient value of INR parameters by leveraging the loss value of the downstream classifier. The whole process of PGD attack for INR is shown in Algorithm 1. Given an optimized INR, we start with randomly perturbed parameters within the maximum perturbation. In every iteration, we reconstruct the image from the INR and feed it into the classifier to compute the loss. Then, we iteratively apply gradient updates and ensure that the perturbed parameters remain within the allowable range through projection. However, as INR parameters are sensitive to direct manipulations, such a straightforward solution can easily undermine the quality of images obtained from the post-attack representation.\n\n\nTo better balance the attacking efficacy and the image quality, we propose a Double Projection Gradient Descent (DPGD) that aims to directly manipulate the INR parameters while preserving the reconstructed image's quality. The detailed process of DPGD can be found in Algorithm 2. Unlike PGD for INR, we incorporate an additional projection in each iteration. This projection involves projecting the gradient backpropagated to the reconstructed image onto a boundary controlled by the factor  $\\zeta$ . The motivation of this step is to directly limit the difference between the reconstructed image and the original image at the pixel level. For detailed derivation, please refer to Section B in appendix. The projected gradient is then further backpropagated to update the INR parameters. The quality of images derived from the representations remains high by constraining the gradient on the reconstructed image. The factor  $\\zeta$  can control the balance between the attacking efficacy and the image quality.\n\n\n# 3.3 DEFENSE-IN-CREATION\n\n\nTo achieve data-level robustness, we propose a defense-in-creation strategy that generates robust INR during the creation process to defend against adversarial attacks effectively. The perturbations generated in attacks during creation can be eliminated in optimizing phase, and the INR derived using defense-in-creation can further defend against potential attacks during transmission. By employing this approach, we can ensure the performance of downstream networks at a data level. Besides, since the basic function of INR is to store the content of images, we also consider the quality of the reconstructed image in our method.\n\n\n```txt\n1: Let  $\\theta_{adv} = \\theta$\n```\n\n\n```txt\n2: Random start:  $\\theta_0 = \\theta_{adv} + \\Delta \\theta_0, \\theta_0 \\sim \\mathcal{U}(-\\delta, \\delta)$ .\n```\n\n\n```txt\n3: for  $t = 1,2,\\dots ,N$  do\n```\n\n\n```txt\n4: Reconstruct image  $\\hat{I}_{t - 1} = \\Psi (f_{\\theta_{t - 1}})$\n```\n\n\n```txt\n5:  $\\theta_{t} = \\theta_{t - 1} + \\alpha \\cdot \\mathrm{sign}(\\nabla_{\\theta_{t - 1}}\\mathcal{L}_{CE}(g_{\\phi}\\left(\\hat{I}_{t - 1}\\right),y));$\n```\n\n\n```txt\n6: Update  $\\theta_t = \\mathrm{Proj}_{\\|\\theta_t - \\theta\\|_p \\leq \\delta}(\\theta_t)$ .\n```\n\n\n```txt\n7: end for\n```\n\n\n```txt\n8: return  $\\theta_{adv} = \\theta_N$\n```\n\n\n```txt\n1: Reconstruct original image  $\\hat{I}_{org} = \\Psi (f_{\\theta})$\n```\n\n\n```txt\n2: Let  $\\theta_{adv} = \\theta$\n```\n\n\n```txt\n3: Random start:  $\\theta_0 = \\theta_{adv} + \\Delta \\theta_0, \\theta_0 \\sim \\mathcal{U}(-\\delta, \\delta)$ .\n```\n\n\n```txt\n4: for  $t = 1,2,\\dots ,N$  do\n```\n\n\n```txt\n5: Reconstruct image  $\\hat{I}_{t - 1} = \\Psi (f_{\\theta_{t - 1}})$\n```\n\n\n```txt\n6:  $Grad_{I} = \\frac{\\partial\\mathcal{L}_{CE}(g_{\\phi}(\\hat{I}_{t - 1}),y)}{\\partial\\hat{I}_{t - 1}};$\n```\n\n\n```txt\n7: Update  $Grad_{I} = \\mathrm{Proj}\\left\\| \\hat{I}_{t - 1} + Grad_{I} - \\hat{I}_{org}\\right\\|_{p} \\leq \\zeta (Grad_{I})$ ;  $\\triangleright$  Projecting gradient of image pixels\n```\n\n\n```javascript\n8:  $Grad_{\\theta} = \\frac{\\partial\\hat{I}_{t - 1}}{\\partial\\theta_{t - 1}}\\cdot Grad_{I};$\n```\n\n\n```latex\n9:  $\\theta_{t} = \\theta_{t - 1} + \\alpha \\cdot \\mathrm{sign}(Grad_{\\theta})$\n```\n\n\n```txt\n10: Update  $\\theta_{t} = \\mathrm{Proj}_{\\| \\theta_{t} - \\theta \\|_{p}\\leq \\delta}(\\theta_{t})$  Projecting gradient of INR parameters\n```\n\n\n```txt\n11: end for\n```\n\n\n```txt\n12: return  $\\theta_{adv} = \\theta_N$\n```\n\n\nAs shown in Figure 3, defense-in-creation is implemented by designing a robust loss on top of the original reconstruction loss defined in Equation 1. The robustness loss is obtained by adding adversarial perturbations to the INR parameters and calculating the loss after feeding the reconstructed image into the classifier  $g_{\\phi}$ . The whole defense process can be formulated as follows:\n\n\n$$\n\n$$\n\\min  _ {\\theta} \\lambda_ {1} \\mathcal {L} _ {r e c o n} + \\lambda_ {2} \\max  _ {\\Delta \\theta} \\mathcal {L} _ {C E} \\left(g _ {\\phi} \\left(\\Psi \\left(f _ {\\theta + \\Delta \\theta}\\right)\\right), y _ {i}\\right), \\quad \\text {s . t .} \\| \\Delta \\theta \\| _ {p} \\leq \\delta , \\tag {4}\n$$\n\n$$\n\n\nwhere  $\\Delta \\theta$  is the adversarial perturbation added in the training phase. The parameters  $\\lambda_{1}$  and  $\\lambda_{2}$  balance the reconstruction quality and robustness. The detailed solution to the above optimization problem is shown in Algorithm 3. We use the robustness loss generated from Algorithm 1 to enhance the robustness, and employ reconstruction loss to guarantee the image quality. By minimizing the loss obtained from classifier  $g_{\\phi}$ , we can effectively remove the adversarial perturbation embedded in the target image when performing attack during creation. By introducing adversarial samples during the training process of INR, INR can gradually learn to have better robustness against parameter perturbations, thereby defending against attack during transmission.\n\n\n# 4 EXPERIMENTS AND RESULTS\n\n\n# 4.1 EXPERIMENTAL SETTINGS\n\n\nDataset. We follow established settings (Zhang et al., 2020; Jin et al., 2023) on adversarial attack and defense to evaluate our proposed mechanism. Our experiments are conducted on three real-world datasets: CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and SVHN (Netzer et al., 2011). CIFAR-10/100 dataset contains  $60K$  color images with the size\n\n\n```\nInput: Image  $I$  , ground truth label  $y$  , and pre-trained downstream model  $g_{\\phi}$    \nParameter: Number of iterations  $T$  , learning rate  $\\eta$    \nOutput: INR  $f_{\\theta}$  for image  $I$    \n1: Initialize model parameters  $\\theta$    \n2: for  $t = 1$  to  $T$  do   \n3: Generate INR adversarial example  $f_{\\theta_{adv}}$  using Algorithm 1;   \n4: Compute reconstruction loss  $\\mathcal{L}_{recon}$  as Equation 1;   \n5: Compute robustness loss as  $\\mathcal{L}_{CE}(g_{\\phi}(\\Psi (f_{\\theta_{adv}})),y)$  .   \n6: Compute the total loss  $\\mathcal{L}_{total}$  as Equation 4;   \n7: Update parameters using adversarial example:  $\\theta \\gets \\theta -\\eta \\cdot \\nabla_{\\theta}\\mathcal{L}_{total}$    \n8: end for   \n9: return Trained INR parameters  $\\theta$\n```\n\n\nof  $32 \\times 32$ , including  $50K$  training images and  $10K$  test images in 10 and 100 classes, respectively. SVHN is a dataset collected by Google Street View, consisting of  $32 \\times 32$  color images of house numbers, with 73, 257 training images and 26, 032 test images in 10 classes. The classifier is trained using training images. We train INRs for test images to evaluate our attack approaches and defense methods. As suggested in previous work (Wei et al., 2023; Jin et al., 2022), we evaluate the performance of attacks and defenses based on the PreActResNet-18 (He et al., 2016) and WideResNet34-10 (Zagoruyko & Komodakis, 2016) architectures on CIFAR-10/100 and SVHN.\n\n\nImplementation Details. We implement our method using PyTorch. The INR is a 5-hidden layer MLP with 256 channels per layer and ReLU activation functions for all the data. Following Mildenhall et al. (2020), we also use a positional encoding for pixel coordinates with 5 frequencies. For adversarial defense training, 10-step PGD attack for INR is applied, with maximum perturbation size for parameters  $\\delta = 0.0006$ . We use the Adam optimizer with default values and a learning rate of 0.001. A cosine learning rate decay schedule is used with the minimum value of the multiplier 0.0001. We optimize each INR for 1000 iterations. Other hyper-parameters are specified in each experiment. We conduct experiments on FGSM (Szegedy et al., 2014), PGD (Madry et al., 2018), Carlini and Wagner (CW) (Carlini & Wagner, 2017), and AutoAttack (AA) (Croce & Hein, 2020) for attack during creation. The maximum perturbation size  $\\epsilon$  is set to  $8/255$  for FGSM and PGD. PGD is used with steps 100, and step size  $0.2/255$ . We apply FGSM for INR, PGD for INR, CW for INR, and DPGD for attack during transmission. The steps of PGD for INR, and DPGD are set to 100, and maximum perturbation size for INR parameters  $\\delta = 0.0006$ , with step size  $2.5 \\cdot \\delta/$  steps in all experiments. CW attack is applied by optimizing 100-step PGD (Zhou et al., 2022; Huang et al., 2021). All the experiments are performed on NVIDIA Tesla V100 GPUs.\n\n\nBaselines. We found no method specifically for enhancing the robustness at the data level. Therefore, we compare with other three settings for comparison: 1) Normal training: optimizing the INR with only reconstruction loss; 2) Direct pixel manipulation<sup>1</sup>: direct manipulating the pixel values without INR encoding; 3) Model-level adversarial training: adversarial training for model robustness including AT (Madry et al., 2018), TRADES (Zhang et al., 2019), FAT (Zhang et al., 2020), GAIRAT (Zhang et al., 2021), and LAS-AT (Jia et al., 2022). Although the latter method aims to enhance the robustness through training the downstream classifier, we conduct a comparison with them, and the analysis can be found in Section 4.3.\n\n\nEvaluation metrics. We assess all methods' performance by evaluating image quality, attack effectiveness, and defense ability. For image quality, we evaluate the performance by using PSNR, SSIM, and LPIPS (Zhang et al., 2018). Higher PSNR and SSIM value indicates better performance, while lower LPIPS value indicates better performance for both attack and defense. To assess the effectiveness of attacks, we utilize the attack success rate (ASR). In terms of defense, we evaluate the defense ability by considering the classification accuracy of the downstream classifier.\n\n\n# 4.2 EXPERIMENTAL RESULTS FOR DIFFERENT ATTACKS\n\n\nThis section evaluates the performance of the attacks used in our research. Specifically, we assess the attacks during creation and transmission to evaluate their invisibility and attacking efficacy.\n\n\nTable 1: Evaluation of attack during creation with different image-based attack methods. The results are averaged on all examples in CIFAR-10 test dataset.\n\n\n<table><tr><td>Method</td><td>Clean images</td><td>Clean INR</td><td>Method</td><td>FGSM on images</td><td>Creation after FGSM</td><td>PGD on images</td><td>Creation after PGD</td><td>CW on images</td><td>Creation after CW</td><td>AA on images</td><td>Creation after AA</td></tr><tr><td>Accuracy</td><td>94.97%</td><td>94.73%</td><td>ASR</td><td>60.45%</td><td>59.26%</td><td>100.00%</td><td>99.83%</td><td>99.93%</td><td>99.84%</td><td>100.00%</td><td>99.89%</td></tr><tr><td>PSNR</td><td>-</td><td>48.42</td><td>PSNR</td><td>30.14</td><td>31.01</td><td>32.70</td><td>33.70</td><td>32.68</td><td>34.12</td><td>32.71</td><td>33.85</td></tr><tr><td>SSIM</td><td>1.000</td><td>0.998</td><td>SSIM</td><td>0.921</td><td>0.935</td><td>0.956</td><td>0.967</td><td>0.957</td><td>0.970</td><td>0.956</td><td>0.968</td></tr><tr><td>LPIPS</td><td>0.000</td><td>0.002</td><td>LPIPS</td><td>0.146</td><td>0.134</td><td>0.082</td><td>0.071</td><td>0.082</td><td>0.067</td><td>0.082</td><td>0.070</td></tr></table>\n\n\nTable 2: Reconstruction qualities and accuracies of downstream classifiers compared with normal training and direct pixel manipulation. The results are averaged on all examples in each dataset with different classifier architectures. Image quality evaluations are obtained by comparing with original clean images. The classifier architectures used for CIFAR-10/100 and SVHN are PreActResNet-18 and WideResNet34-10, respectively. More results can be found in the appendix.\n\n\n<table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Method</td><td rowspan=\"2\">PSNR</td><td rowspan=\"2\">Accuracy without attacks</td><td colspan=\"3\">Attack during creation</td><td colspan=\"4\">Attack during transmission</td></tr><tr><td>PGD</td><td>CW</td><td>AA</td><td>DPGD</td><td>FGSM</td><td>PGD</td><td>CW</td></tr><tr><td rowspan=\"4\">CIFAR-10</td><td>Normal image</td><td>-</td><td>94.97%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>39.55%</td><td>0.00%</td><td>0.07%</td></tr><tr><td>Pixel manipulation</td><td>33.16</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>-</td><td>99.88%</td><td>33.80%</td><td>27.88%</td></tr><tr><td>Normal training</td><td>48.38</td><td>94.73%</td><td>0.17%</td><td>0.16%</td><td>0.11%</td><td>20.80%</td><td>57.36%</td><td>18.84%</td><td>17.72%</td></tr><tr><td>Defense-in-creation</td><td>42.92</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>99.60%</td><td>100%</td><td>98.68%</td><td>61.36%</td></tr><tr><td rowspan=\"4\">CIFAR-100</td><td>Normal image</td><td>-</td><td>76.92%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>8.44%</td><td>0.01%</td><td>0.00%</td></tr><tr><td>Pixel manipulation</td><td>29.63</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>-</td><td>99.96%</td><td>30.08%</td><td>0.56%</td></tr><tr><td>Normal training</td><td>48.26</td><td>76.44%</td><td>0.16%</td><td>0.12%</td><td>0.11%</td><td>4.32%</td><td>17.68%</td><td>2.16%</td><td>1.76%</td></tr><tr><td>Defense-in-creation</td><td>40.50</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>98.96%</td><td>100%</td><td>98.44%</td><td>74.52%</td></tr><tr><td rowspan=\"4\">SVHN</td><td>Normal image</td><td>-</td><td>95.80%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>41.84%</td><td>1.32%</td><td>0.96%</td></tr><tr><td>Pixel manipulation</td><td>35.78</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>-</td><td>100%</td><td>76.60%</td><td>18.24%</td></tr><tr><td>Normal training</td><td>53.03</td><td>95.80%</td><td>1.28%</td><td>1.20%</td><td>0.18%</td><td>39.04%</td><td>65.96%</td><td>36.24%</td><td>35.76%</td></tr><tr><td>Defense-in-creation</td><td>46.98</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>99.88%</td><td>100%</td><td>99.88%</td><td>84.84%</td></tr></table>\n\n\nAttack during creation. We validate the attack during creation, and the findings are presented in Table 1. We compare the results of this attack on the INR parameters with those obtained by directly inputting the images attacked by FGSM, PGD, CW, and AA into the downstream classifier. The reconstructed images display comparable attack effectiveness and image quality to those manipulated using FGSM, PGD, CW, and AA methods. Due to the excellent representation capabilities of INR, the performance of the attack during creation is determined by attacks applied to the images.\n\n\n![Figure 4: Evaluation of attack during transmission. Left: we present the ASRs and PSNRs across varying gradient control factors in DPGD. Right: we compare the ASR v.s. PSNR curves with varying the maximum perturbation across different methods. The results are averaged on all examples in CIFAR-10 test dataset.](images/e7ac40b9dc77f8b2c90e27b6d44a9a91c41646ee192d80a8299f861a038e41e3.jpg)\n\n\n![](images/c11888adfa90cd788c6725f0e3f1063302e8607a2f85b09c5d4365e72d27bb29.jpg)\n\n\nAttack during transmission. We evaluate the impact of the image gradient control factor  $\\zeta$  on the invisibility and attacking efficacy in the DPGD algorithm. We fix the maximum perturbation at  $\\delta = 0.0006$ , and the ASR and image quality results are shown in Figure 4(left). The results indicate that the factor  $\\zeta$  can regulate the balance between invisibility and attacking efficacy. Specifically, a larger value of  $\\zeta$  indicates higher attacking efficacy, while a smaller value of  $\\zeta$  indicates a closer resemblance to the original image. We further compare our DPGD algorithm with FGSM for INR, PGD for INR, and CW for INR in Figure 4(right). We take  $\\zeta = 0.9$  and 1.3, respectively, and vary the maximum perturbation  $\\delta$  to obtain curves illustrating the relationship between image quality and ASR. At the same level of image quality, our method achieves higher ASR, indicating a higher attacking efficacy. Therefore, our proposed DPGD achieves better performance.\n\n\n# 4.3 EXPERIMENTAL RESULTS FOR DEFENSE\n\n\nIn this section, we present experimental results of our proposed adversarial defense method and compare them with the baselines.\n\n\nTable 3: Accuracies compared with model-level adversarial training. The classifier structures are all WideResNet34-10. The results are averaged on all examples in CIFAR-10 test dataset.\n\n\n<table><tr><td>Defense method</td><td>Defense-in-creation</td><td>AT</td><td>TRADES</td><td>FAT</td><td>GAIRAT</td><td>LAS-AT</td></tr><tr><td>Natural</td><td>94.32%</td><td>85.90%</td><td>85.72%</td><td>87.97%</td><td>86.30%</td><td>86.23%</td></tr><tr><td>PGD</td><td>92.51%</td><td>53.42%</td><td>53.40%</td><td>47.48%</td><td>40.30%</td><td>53.58%</td></tr></table>\n\n\nRobustness and reconstruction quality. We further evaluate our proposed adversarial defense for achieving data robustness. We set  $\\lambda_{1} = 1$ ,  $\\lambda_{2} = 0.0022$ ,  $0.0006$ ,  $0.001$  for CIFAR-10, CIFAR-100, and SVHN, respectively. The results are shown in Table 2. We study the performance of the proposed defense in defending against various attack methods. Our method achieves the highest accuracy and produces high-quality reconstructions across different attacks, datasets, and classifier architectures. While normal training achieves the best reconstruction results, it is vulnerability to all attacks, resulting in low accuracy under various attack scenarios. Although pixel manipulation can defend against attacks during creation, it leads to lower accuracy when encountering attacks during transmission, such as PGD for INR and CW for INR.\n\n\nComparison with different  $\\lambda_{2}$ . To evaluate the balance between reconstruction quality and robustness in our defense-in-creation strategy, we adjust the weight ratio between reconstruction loss  $\\lambda_{1}$  and weight of robustness loss  $\\lambda_{2}$ . We conduct a series of experiments under attacks during transmission by fixing  $\\lambda_{1} = 1$  and varying the weight of robustness loss  $\\lambda_{2}$ . As shown in Figure 5, a smaller value of  $\\lambda_{2}$  leads to higher reconstruction quality, while a larger value of  $\\lambda_{2}$  enhances robustness against attacks.\n\n\n![Figure 5: Balance between reconstruction quality and robustness against attacks during transmission. The results are averaged on all examples in CIFAR-10 test dataset.](images/9a5a470ac00ee922da0bfc9886a7df36417c8bd027da3cbb6792468b9c43da83.jpg)\n\n\nComparison with model-level adversarial training. Our method aims to enhance data-level adversarial robustness, and its motivation differs from traditional adversarial training approaches. As attacks on INRs are different from those on images, we apply PGD for INR in our method. Furthermore, to ensure a fair comparison, we use the predicted labels generated by the downstream classifier instead of the ground truth labels in the creation process of INR. As illustrated in Table 3, our method demonstrates better performance with natural data and stronger robustness compared with model-level adversarial training methods.\n\n\n# 5 CONCLUSION\n\n\nWe demonstrate adversarial data robustness in this paper by utilizing implicit neural representations. Instead of adversially training deep learning models, we focus on adversially training the data. Doing so ensures that the data retains robust performance even if it is subjected to attacks before it reaches the model users. To accomplish this, we initially represent the data implicitly and focus on adversarial training of this representation during its formation. By analyzing the potential attacks, we unveil an adversarial training scheme that bolsters the robustness of implicit data representations against potential adversarial attacks. In our proposed approach, ready-to-use image classifiers exhibit adversarial robustness on par with models that undergo model-level adversarial training.\n\n\nLimitations. Though our method is successful in achieving robustness at the data level, it cannot handle some extreme cases. For example, if malicious users gain access to the reconstructed images from the INR, they can directly apply adversarial perturbations to the image pixels, thereby circumventing attacks on the INR. This problem can be solved through system design by protecting the process from image reconstruction to downstream model input, to ensure that the recovered images are not accessible. Besides, we only consider risks caused by malicious technical designs. However, as security is not solely a technical problem, we still need to collaborate with various parties to consider the legal and social impact. We will further explore the feasibility of our approach in other data types and downstream tasks in the future.\n\n\n# ETHICS STATEMENT\n\n\nOur goal is to ensure the security of deep neural networks. We did not employ crowdsourcing and did not involve human subjects in our experiments. When utilizing existing assets such as code, data, or models, we have properly cited the original creators.\n\n\n# REFERENCES\n\n- Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-efficient black-box adversarial attack via random search. In European Conference on Computer Vision, 2020.\n\n- Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy, 2017.\n\n- Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\n\n- Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics Express, 2020.\n\n- Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vedit Goel, Zhangyang Wang, Humphrey Shi, and Xiaolong Wang. VideoINR: Learning video implicit neural representation for continuous space-time super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, JinFeng Yi, and Cho-Jui Hsieh. Query-efficient hard-label black-box attack: An optimization-based approach. In International Conference on Learning Representations, 2018.\n\n- Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, 2019.\n\n- Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International Conference on Machine Learning, 2020.\n\n- Junhao Dong, Seyed-Mohsen Moosavi-Dezfooli, Jianhuang Lai, and Xiaohua Xie. The enemy of my enemy is my friend: Exploring inverse adversaries for improving adversarial training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Emilien Dupont, Adam Golinski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. COin: Compression with implicit neural representations. In Neural Compression: From Information Theory to Applications ICLRW, 2021.\n\n- Joachim Folz, Sebastian Palacio, Joern Hees, and Andreas Dengel. Adversarial defense based on structure-to-signal autoencoders. In IEEE Winter Conference on Applications of Computer Vision, 2020.\n\n- Ruohan Gao, Zilin Si, Yen-Yu Chang, Samuel Clarke, Jeannette Bohg, Li Fei-Fei, Wenzhen Yuan, and Jiajun Wu. Objectfolder 2.0: A multisensory object dataset for sim2real transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In *Joshua Bengio and Yann LeCun* (eds.), International Conference on Learning Representations, 2015.\n\n- Chuan Guo, Jacob Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Weinberger. Simple black-box adversarial attacks. In International Conference on Machine Learning, 2019.\n\n- Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and Steven Hoi. From images to textual prompts: Zero-shot visual question answering with frozen large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, 2016.\n\n- Hanxun Huang, Yisen Wang, Sarah Erfani, Quanquan Gu, James Bailey, and Xingjun Ma. Exploring architectural ingredients of adversarially robust deep neural networks. Advances in Neural Information Processing Systems, 2021.\n\n- Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. LAS-AT: adversarial training with learnable attack strategy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Gaojie Jin, Xinping Yi, Wei Huang, Sven Schewe, and Xiaowei Huang. Enhancing adversarial training with second-order statistics of weights. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n- Gaojie Jin, Xinping Yi, Dengyu Wu, Ronghui Mu, and Xiaowei Huang. Randomized adversarial training via taylor expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 2021.\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), International Conference on Learning Representations, 2015.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n\n- Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against unseen threat models. In International Conference on Learning Representations, 2021.\n\n- Yanxi Li, Zhaohui Yang, Yunhe Wang, and Chang Xu. Neural architecture dilation for adversarial robustness. Advances in Neural Information Processing Systems, 2021.\n\n- Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against adversarial attacks using high-level representation guided denoiser. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\n- Renhao Liu, Yu Sun, Jiabei Zhu, Lei Tian, and Ulugbek S Kamilov. Recovery of continuous 3d refractive index maps from discrete intensity-only measurements using neural fields. Nature Machine Intelligence, 2022.\n\n- Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In International Conference on Learning Representations, 2016.\n\n- Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017.\n\n- Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\n\n- Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision, 2020.\n\n- Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In Proceedings of the NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.\n\n- Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and Privacy, 2016.\n\n- Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.\n\n- Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 2020.\n\n- Brandon Reyes, Amanda A Howard, Paris Perdikaris, and Alexandre M Tartakovsky. Learning unknown physics of non-newtonian fluids. Physical Review Fluids, 2021.\n\n- Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversially robust deep learning. In International Conference on Machine Learning, 2020.\n\n- Andrew Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.\n\n- Andrei A Rusu, Dan Andrei Calian, Sven Gowal, and Raia Hadsell. Hindering adversarial attacks with implicit neural representations. In International Conference on Machine Learning, 2022.\n\n- Hai Shu and Hongtu Zhu. Sensitivity analysis of deep neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019.\n\n- Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Advances in Neural Information Processing Systems, 2020.\n\n- Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 2021.\n\n- Yannick Str\u00fcmpler, Janis Postels, Ren Yang, Luc Van Gool, and Federico Tombari. Implicit neural representations for image compression. In European Conference on Computer Vision, 2022.\n\n- Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014.\n\n- Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Advances in Neural Information Processing Systems, 2020.\n\n- Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Wang Yifan, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Advances in neural rendering. In Computer Graphics Forum, 2022.\n\n- Xiaosen Wang and Kun He. Enhancing the transferability of adversarial attacks through variance tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\n\n- Zeming Wei, Yifei Wang, Yiwen Guo, and Yisen Wang. CFA: Class-wise calibrated fair adversarial training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n- Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British Machine Vision Conference 2016. British Machine Vision Association, 2016.\n\n- Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning, 2019.\n\n- Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In International Conference on Machine Learning, 2020.\n\n- Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli. Geometry-aware instance-reweighted adversarial training. In International Conference on Learning Representations, 2021.\n\n- Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\n- Dawei Zhou, Nannan Wang, Bo Han, and Tongliang Liu. Modeling adversarial noise for adversarial training. In International Conference on Machine Learning, 2022.\n\n- Hao Zhu, Zhen Liu, You Zhou, Zhan Ma, and Xun Cao. DNF: diffractive neural field for lensless microscopic imaging. Optics Express, 2022.\n\n\n![Figure 6: A visualization of our INR architecture.](images/a5371be97e66712ad169f513b693d85da4c8218a359d50b3728f8df172c743b3.jpg)\n\nResponse: Based on the provided paper, here's a systematic evaluation of the key aspects:\n\n### Originality\n- **Original Contribution**: The paper introduces a novel approach to achieve adversarial robustness at the data level rather than the model level. This is a significant departure from existing adversarial training methods.\n- **Impact**: It opens up a new avenue for ensuring robustness in deep learning systems, particularly for those with unmodifiable parameters, which is a common scenario in practical applications.\n\n### Methodological Rigor\n- **Approach**: The authors propose using Implicit Neural Representations (INRs) to store and transmit images. This is a novel application of INRs in the context of adversarial robustness.\n- **Experimental Setup**: The setup is well-defined, and the experiments cover a variety of attack methods and settings, providing comprehensive validation.\n- **Implementation Details**: The implementation is described clearly, and the choice of hyperparameters and methods (e.g., PGD, DPGD) is justified.\n\n### Significance of Contribution\n- **Problem Addressed**: The paper tackles the issue of adversarial robustness for data that is already in use, addressing the gap between theoretical robustness and practical usability.\n- **Potential Impact**: If successful, this could significantly enhance the security and reliability of deep learning systems in real-world applications, reducing the risk of adversarial attacks.\n\n### Clarity of Presentation\n- **Clarity**: The paper is well-written and structured logically. Each section flows naturally from one to another.\n- **Readability**: The technical details are explained clearly, and the motivations behind various choices are well-articulated.\n- **Relevance**: The terminology and concepts are appropriately defined, and the explanations are clear without unnecessary jargon.\n\n### Alignment with Journal Scope\n- **Relevance**: The paper fits well within the scope of ICLR, which focuses on the intersection of machine learning theory and practice, especially on adversarial defenses and robustness.\n- **Novelty**: The approach is innovative and aligns with current trends in adversarial robustness research.\n\n### Prediction\n**Prediction**: \\boxed{Accept}\n\n### Justification\n- **Strengths**:\n  - Novel approach to adversarial robustness at the data level.\n  - Comprehensive experimental evaluation covering various attack methods and datasets.\n  - Clear and well-documented methodology.\n  - Potential significant impact on practical deployment of deep learning models.\n\n- **Weaknesses**:\n  - Some parts of the paper, particularly the theoretical justifications for certain attack strategies, could benefit from more detailed mathematical derivations.\n  - The paper could benefit from more comparative analysis with existing data-level robustness methods.\n  - The discussion on limitations and future directions could be expanded.\n\nOverall, the paper presents a strong and novel contribution to the field of adversarial robustness, making it highly likely to be accepted for publication at ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636658.582812", "msecs": "582.0", "relativeCreated": "119275.95257759094", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills\n\n# Abstract\nGeneralizable manipulation skills, which can be composed to tackle long-horizon and complex daily chores, are one of the cornerstones of Embodied AI. However, existing benchmarks, mostly composed of a suite of simulatable environments, are insufficient to push cutting-edge research works because they lack object-level topological and geometric variations, are not based on fully dynamic simulation, or are short of native support for multiple types of manipulation tasks. To this end, we present ManiSkill2, the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. ManiSkill2 includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D-input data simulated by fully dynamic engines. It defines a unified interface and evaluation protocol to support a wide range of algorithms (e.g., classic sense-plan-act, RL, IL), visual observations (point cloud, RGBD), and controllers (e.g., action type and parameterization). Moreover, it empowers fast visual input learning algorithms so that a CNN-based policy can collect samples at about 2000 FPS with 1 GPU and 16 processes on a regular workstation. It implements a render server infrastructure to allow sharing rendering resources across all environments, thereby significantly reducing memory usage. We open-source all codes of our benchmark (simulator, environments, and baselines) and host an online challenge open to interdisciplinary researchers.\n\n\n![Figure 1: ManiSkill2 provides a unified, fast, and accessible system that encompasses well-curated manipulation tasks (e.g., stationary/mobile-base, single/dual-arm, rigid/soft-body).](images/f64f737450aaa31ead3f8bb5f4d37ef8fcb587e2b568293b26a918d26499249a.jpg)\n\n\n# 1 INTRODUCTION\n\n\nMastering human-like manipulation skills is a fundamental but challenging problem in Embodied AI, which is at the nexus of vision, learning, and robotics. Remarkably, once humans have learnt to manipulate a category of objects, they are able to manipulate unseen objects (e.g., with different\n\n\nappearances and geometries) of the same category in unseen configurations (e.g., initial poses). We refer such abilities to interact with a great variety of even unseen objects in different configurations as generalizable manipulation skills. Generalizable manipulation skills are one of the cornerstones of Embodied AI, which can be composed to tackle long-horizon and complex daily chores (Ahn et al., 2022; Gu et al., 2022). To foster further interdisciplinary and reproducible research on generalizable manipulation skills, it is crucial to build a versatile and public benchmark that focuses on object-level topological and geometric variations as well as practical manipulation challenges.\n\n\nHowever, most prior benchmarks are insufficient to support and evaluate progress in learning generalizable manipulation skills. In this work, we present ManiSkill2, the next generation of SAPIEN ManiSkill Benchmark (Mu et al., 2021), which extends upon fully simulated dynamics, a large variety of articulated objects, and large-scale demonstrations from the previous version. Moreover, we introduce significant improvements and novel functionalities, as shown below.\n\n\n1) A Unified Benchmark for Generic and Generalizable Manipulation Skills: There does not exist a standard benchmark to measure different algorithms for generic and generalizable manipulation skills. It is largely due to well-known challenges to build realistically simulated environments with diverse assets. Many benchmarks bypass critical challenges as a trade-off, by either adopting abstract grasp (Ehsani et al., 2021; Szot et al., 2021; Srivastava et al., 2022) or including few object-level variations (Zhu et al., 2020; Yu et al., 2020; James et al., 2020). Thus, researchers usually have to make extra efforts to customize environments due to limited functionalities, which in turn makes reproducible comparison difficult. For example, Shridhar et al. (2022) modified 18 tasks in James et al. (2020) to enable few variations in initial states. Besides, some benchmarks are biased towards a single type of manipulation, e.g., 4-DoF manipulation in Yu et al. (2020). To address such pain points, ManiSkill2 includes a total of 20 verified and challenging manipulation task families of multiple types (stationary/mobile-base, single/dual-arm, rigid/soft-body) with over 2000 objects and 4M demonstration frames, to support generic and generalizable manipulation skills. All the tasks are implemented in a unified OpenAI Gym (Brockman et al., 2016) interface with fully-simulated dynamic interaction, supporting multiple observation modes (point cloud, RGBD, privileged state) and multiple controllers. A unified protocol is defined to evaluate a wide range of algorithms (e.g., sense-plan-act, reinforcement and imitation learning) on both seen and unseen assets as well as configurations. In particular, we implement a cloud-based evaluation system to publicly and fairly compare different approaches.\n\n\n2) Real-time Soft-body Environments: When operating in the real world, robots face not only rigid bodies, but many types of soft bodies, such as cloth, water, and soil. Many simulators have supported robotic manipulation with soft body simulation. For example, MuJoCo (Todorov et al., 2012) and Bullet Coumans & Bai (2016-2021) use the finite element method (FEM) to enable the simulation of rope, cloth, and elastic objects. However, FEM-based methods cannot handle large deformation and topological changes, such as scooping flour or cutting dough. Other environments, like SoftGym (Lin et al., 2020) and ThreeDWorld (Gan et al., 2020), are based on Nvidia Flex, which can simulate large deformations, but cannot realistically simulate elasto-plastic material, e.g., clay. PlasticineLab (Huang et al., 2021) deploys the continuum-mechanics-based material point method (MPM), but it lacks the ability to couple with rigid robots, and its simulation and rendering performance have much room for improvement. We have implemented a custom GPU MPM simulator from scratch using Nvidia's Warp (Macklin, 2022) JIT framework and native CUDA for high efficiency and customizability. We further extend Warp's functionality to support more efficient host-device communication. Moreover, we have supported a 2-way dynamics coupling interface that enables any rigid-body simulation framework to interact with the soft bodies, allowing robots and assets in ManiSkill2 to interact with soft-body simulation seamlessly. To our knowledge, ManiSkill2 is the first embodied AI environment to support 2-way coupled rigid-MPM simulation, and also the first to support real-time simulation and rendering of MPM material.\n\n\n3) Multi-controller Support and Conversion of Demonstration Action Spaces: Controllers transform policies' action outputs into motor commands that actuate the robot, which define the action space of a task. Martin-Mart\u00edn et al. (2019); Zhu et al. (2020) show that the choice of action space has considerable effects on exploration, robustness and sim2real transferability of RL policies. For example, task-space controllers are widely used for typical pick-and-place tasks, but might be suboptimal compared to joint-space controllers when collision avoidance (Szot et al., 2021) is required. ManiSkill2 supports a wide variety of controllers, e.g., joint-space controllers for motion planning and task-space controllers for teleoperation. A flexible system is also implemented to\n\n\ncombine different controllers for different robot components. For instance, it is easy to specify a velocity controller for the base, a task-space position controller for the arm, and a joint-space position controller for the gripper. It differs from Zhu et al. (2020), which only supports setting a holistic controller for all the components. Most importantly, ManiSkill2 embraces a unique functionality to convert the action space of demonstrations to a desired one. It enables us to exploit large-scale demonstrations generated by any approach regardless of controllers.\n\n\n4) Fast Visual RL Experiment Support: Visual RL training demands millions of samples from interaction, which makes performance optimization an important aspect in environment design. Isaac Gym (Makoviychuk et al., 2021) implements a fully GPU-based vectorized simulator, but it lacks an efficient renderer. It also suffers from reduced usability (e.g., difficult to add diverse assets) and functionality (e.g., object contacts are inaccessible). EnvPool (Weng et al., 2022) batches environments by a thread pool to minimize synchronization and improve CPU utilization. Yet its environments need to be implemented in  $\\mathbf{C} + +$  , which hinders fast prototyping (e.g., customizing observations and rewards). As a good trade-off between efficiency and customizability, our environments are fully scripted in Python and vectorized by multiple processes. We implement an asynchronous RPC-based render server-client system to optimize throughput and reduce GPU memory usage. We manage to collect samples with an RGBD-input PPO policy at about 2000 FPS  $^1$  with 1 GPU and 16 CPU processors.\n\n\n# 2 BUILDING ENVIRONMENTS FOR GENERALIZABLE MANIPULATION SKILLS\n\n\nBuilding high-quality environments demands cross-disciplinary knowledge and expertise, including physical simulation, rendering, robotics, machine learning, software engineering, etc. Our workflow highlights a verification-driven iterative development process, which is illustrated in Appendix A.1. Different approaches, including task and motion planning (TAMP), model predictive control (MPC) and reinforcement learning (RL), can be used to generate demonstrations according to characteristics and difficulty of tasks, which verify environments as a byproduct.\n\n\n# 2.1 HETEROGENEOUS TASK FAMILIES\n\n\nManiSkill2 embraces a heterogeneous collection of 20 task families. A task family represents a family of task variants that share the same objective but are associated with different assets and initial states. For simplicity, we interchangeably use task short for task family. Distinct types of manipulation tasks are covered: rigid/soft-body, stationary/mobile-base, single/dual-arm. In this section, we briefly describe 4 groups of tasks. More details can be found in Appendix C.\n\n\nSoft-body Manipulation: ManiSkill2 implements 6 soft-body manipulation tasks that require agents to move or deform soft bodies into specified goal states through interaction.\n\n- 1) Fill: filling clay from a bucket into the target beaker;\n\n- 2) Hang: hanging a noodle on the target rod;\n\n- 3) Excavate: scooping up a specific amount of clay and lifting it to a target height;\n\n- 4) Pour: pouring water from a bottle into the target beaker. The final liquid level should match the red line on the beaker.\n\n- 5) Pinch: deforming plasticine from an initial shape into a target shape; target shapes are generated by randomly pinching initial shapes, and are given as RGBD images or point clouds from 4 views.\n\n- 6) Write: writing a target character on clay. Target characters are given as 2D depth maps.\n\n\nA key challenge of these tasks is to reason how actions influence soft bodies, e.g., estimating displacement quantity or deformations, which will be illustrated in Sec 5.2.\n\n\nPrecise Peg-in-hole Assembly: Peg-in-hole assembly is a representative robotic task involving rich contact. We include a curriculum of peg-in-hole assembly tasks that require an agent to place an object into its corresponding slot. Compared to other existing assembly tasks, ours come with two noticeable improvements. First, our tasks target high precision (small clearance) at the level of millimeters, as most day-to-day assembly tasks demand. Second, our tasks emphasize the contact-rich insertion process, instead of solely measuring position or rotation errors.\n\n\n1) PegInsertionSide: a single peg-in-hole assembly task inspired by MetaWorld (Yu et al., 2020). It involves an agent picking up a cuboid-shaped peg and inserting it into a hole with a clearance of\n\n\n3mm on the box. Our task is successful only if half of the peg is inserted, while the counterparts in prior works only require the peg head to approach the surface of the hole.\n\n- 2) PlugCharger: a dual peg-in-hole assembly task inspired by RLBench (James et al., 2020), which involves an agent picking up and plugging a charger into a vertical receptacle. Our assets (the charger and holes on the receptacle) are modeled with realistic sizes, allowing a clearance of  $0.5\\mathrm{mm}$ , while the counterparts in prior works only examine the proximity of the charger to a predefined position without modeling holes at all.\n\n- 3) AssemblingKits: inspired by Transporter Networks (Zeng et al., 2020), this task involves an agent picking up and inserting a shape into its corresponding slot on a board with 5 slots in total. We devise a programmatic way to carve slots on boards given any specified clearance (e.g.,  $0.8\\mathrm{mm}$ ), such that we can generate any number of physically-realistic boards with slots. Note that slots in environments of prior works are visual marks, and there are in fact no holes on boards.\n\n\nWe generate demonstrations for the above tasks through TAMP. This demonstrates that it is feasible to build precise peg-in-hole assembly tasks solvable in simulation environments, without abstractions from prior works.\n\n\nStationary 6-DoF Pick-and-place: 6-DoF pick-and-place is a widely studied topic in robotics. At the core is grasp pose Mahler et al. (2016); Qin et al. (2020); Sundermeyer et al. (2021). In ManiSkill2, we provide a curriculum of pick-and-place tasks, which all require an agent to pick up an object and move it to a goal specified as a 3D position. The diverse topology and geometric variations among objects call for generalizable grasp pose predictions.\n\n- 1) PickCube: picking up a cube and placing it at a specified goal position;\n\n- 2) StackCube: picking up a cube and placing it on top of another cube. Unlike PickCube, the goal placing position is not explicitly given; instead, it needs to be inferred from observations.\n\n- 3) PickSingleYCB: picking and placing an object from YCB (Calli et al., 2015);\n\n- 4) PickSingleEGAD: picking and placing an object from EGAD (Morrison et al., 2020);\n\n- 5) PickClutterYCB: The task is similar to PickSingleYCB, but multiple objects are present in a single scene. The target object is specified by a visible 3D point on its surface.\n\n\nOur pick-and-place tasks are deliberately designed to be challenging. For example, the goal position is randomly selected within a large workspace  $(30 \\times 50 \\times 50 cm^3)$ . It poses challenges to sense-plan-act pipelines that do not take kinematic constraints into consideration when scoring grasp poses, as certain high-quality grasp poses might not be achievable by the robot.\n\n\nMobile/Stationary Manipulation of Articulated Objects: We inherit four mobile manipulation tasks from ManiSkill1 (Mu et al., 2021), which are PushChair, MoveBucket, OpenCabinetDoor, and OpenCabinetDrawer. We also add a stationary manipulation task, TurnFaucet, which uses a stationary arm to turn on faucets of various geometries and topology (details in Appendix C.3).\n\n\nBesides the above tasks, we have one last task, AvoidObstacles, which tests the navigation ability of a stationary arm to avoid a dense collection of obstacles in space while actively sensing the scene.\n\n\n# 2.2 MULTI-CONTROLLER SUPPORT AND CONVERSION OF DEMONSTRATION ACTION SPACES\n\n\nThe selection of controllers determines the action space. ManiSkill2 supports multiple controllers, e.g., joint position, delta joint position, delta end-effector pose, etc. Unless otherwise specified, controllers in ManiSkill2 translate input actions, which are desired configurations (e.g., joint positions or end-effector pose), to joint torques that drive corresponding joint motors to achieve desired actions. For instance, input actions to joint position, delta joint position, and delta end-effector pose controllers are, respectively, desired absolute joint positions, desired joint positions relative to current joint positions, and desired  $\\mathbb{S}\\mathbb{E}(3)$  transformations relative to the current end-effector pose. See Appendix B for a full description of all supported controllers.\n\n\nDemonstrations are generated with one controller, with an associated action space. However, researchers may select an action space that conforms to a task but is different from the original one. Thus, to exploit large-scale demonstrations, it is crucial to convert the original action space to many different target action spaces while reproducing the kinematic and dynamic processes in demonstrations. Let us consider a pair of environments: a source environment with a joint position controller used to generate demonstrations through TAMP, and a target environment with a delta end-effector pose controller for Imitation / Reinforcement Learning applications. The objective is to convert the source action  $a_{\\mathrm{src}}(t)$  at each timestep  $t$  to the target action  $a_{\\mathrm{tgt}}(t)$ . By definition, the\n\n\ntarget action (delta end-effector pose) is  $a_{\\mathrm{tgt}}(t) = \\bar{T}_{\\mathrm{tgt}}(t) \\cdot T_{\\mathrm{tgt}}^{-1}(t)$ , where  $\\bar{T}_{\\mathrm{tgt}}(t)$  and  $T_{\\mathrm{tgt}}(t)$  are respectively desired and current end-effector poses in the target environment. To achieve the same dynamic process in the source environment, we need to match  $\\bar{T}_{\\mathrm{tgt}}(t)$  with  $\\bar{T}_{\\mathrm{src}}(t)$ , where  $\\bar{T}_{\\mathrm{src}}(t)$  is the desired end-effector pose in the source environment.  $\\bar{T}_{\\mathrm{src}}(t)$  can be computed from the desired joint positions  $(a_{\\mathrm{src}}(t)$  in this example) through forward kinematics  $(FK(\\cdot))$ . Thus, we have\n\n\n$$\n\n$$\na _ {\\mathrm {t g t}} (t) = \\bar {T} _ {\\mathrm {t g t}} (t) \\cdot T _ {\\mathrm {t g t}} ^ {- 1} (t) = \\bar {T} _ {\\mathrm {s r c}} (t) \\cdot T _ {\\mathrm {t g t}} ^ {- 1} (t) = F K (\\bar {a} _ {\\mathrm {s r c}} (t)) \\cdot T _ {\\mathrm {t g t}} ^ {- 1} (t)\n$$\n\n$$\n\n\nNote that our method is closed-loop, as we instantiate a target environment to acquire  $T_{\\mathrm{tgt}}^{-1}(t)$ . For comparison, an open-loop method would use  $T_{\\mathrm{src}}^{-1}(t)$  and suffers from accumulated execution errors.\n\n\n# 3 REAL-TIME SOFT BODY SIMULATION AND-RENDERING\n\n\nIn this section, we describe our new physical simulator for soft bodies and their dynamic interactions with the existing rigid-body simulation. Our key contributions include: 1) a highly efficient GPU MPM simulator; 2) an effective 2-way dynamic coupling method to support interactions between our soft-body simulator and any rigid-body simulator, in our case, SAPIEN. These features enable us to create the first real-time MPM-based soft-body manipulation environment with 2-way coupling.\n\n\nRigid-soft Coupling: Our MPM solver is MLS-MPM (Hu et al., 2018) similar to PlasticineLab (Huang et al., 2021), but with a different contact modeling approach, which enables 2-way coupling with external rigid-body simulators. The coupling works by transferring rigid-body poses to the soft-body simulator and transferring soft-body forces to the rigid-body simulator. At the beginning of the simulation, all collision shapes in the external rigid-body simulator are copied to the soft-body simulator. Primitive shapes (box, capsule, etc.) are represented by analytical signed distance functions (SDFs); meshes are converted to SDF volumes, stored as 3D CUDA textures. After each rigid-body simulation step, we copy the poses and velocities of the rigid bodies to the soft-body simulator. During soft-body simulation steps, we evaluate the SDF functions at MPM particle positions and apply penalty forces to the particles, accumulating forces and torques for rigid bodies. In contrast, PlasticineLab evaluates SDF functions on MPM grid nodes, and applies forces to MPM grids. Our simulator supports both methods, but we find applying particle forces produces fewer artifacts such as penetration. After soft-body steps, we read the accumulated forces and torques from the soft-body simulator and apply them to the external rigid-body simulator. This procedure is summarized in Appendix D.1. Despite being a simple 2-way coupling method, it is very flexible, allowing coupling with any rigid-body simulator, so we can introduce anything from a rigid-body simulator into the soft-body environment, including robots and controllers.\n\n\nPerformance Optimization: The performance of our soft-body simulator is optimized in 4 aspects. First, the simulator is implemented in Warp, Nvidia's JIT framework to translate Python code to native C++ and CUDA. Therefore, our simulator enjoys performance comparable to C and CUDA. Second, we have optimized the data transfer (e.g., poses and forces in the 2-way coupling) between CPU and GPU by further extending and optimizing the Warp framework; such data transfers are performance bottlenecks in other JIT frameworks such as Taichi (Hu et al., 2019), on which PlasticineLab is based. Third, our environments have much shorter compilation time and startup time compared to PlasticineLab thanks to the proper use of Warp compiler and caching mechanisms. Finally, since the simulator is designed for visual learning environments, we also implement a fast surface rendering algorithm for MPM particles, detailed in Appendix D.2. Detailed simulation parameters and performance are provided in Appendix D.3.\n\n\n# 4 PARALLELIZING PHYSICAL SIMULATION AND-RENDERING\n\n\nManiSkill2 aims to be a general and user-friendly framework, with low barriers for customization and minimal limitations. Therefore, we choose Python as our scripting language to model environments, and the open-source, highly flexible SAPIEN (Xiang et al., 2020) as our physical simulator. The Python language determines that the only way to effectively parallelize execution is through multi-process, which is integrated in common visual RL training libraries (Raffin et al., 2021; Weng et al., 2021). Under the multi-process environment paradigm, we make engineering efforts to enable our environments to surpass previous frameworks by increased throughput and reduced GPU memory usage.\n\n\n![(a)](images/dd6b2d38ca309fab924cf0752a642f11c3e3dd8df65022c201d0a98c2fd645d7.jpg)\n\n\n![(b) Figure 2: Two pipelines for visual RL sample collection. (a) Sequential pipeline. (b) Our pipeline with asynchronous rendering and render server improves CPU utilization, reduces data transfer, and saves memory.](images/44d55175b2f6f501e6d464ca19b815b37484af073610b2567c71bf5358ebb9b3.jpg)\n\n\nWhat are the Goals in Visual-Learning Environment Optimization? The main goal of performance optimization in a visual-learning environment is to maximize total throughput: the number of samples collected from all (parallel) environments, measured in steps (frames) per second. Another goal is to reduce GPU memory usage. Typical Python-based multi-process environments are wasteful in GPU memory usage, since each process has to maintain a full copy of GPU resources, e.g., meshes and textures. Given a fixed GPU memory budget, less GPU memory spent on rendering resources means more memory can be allocated for neural networks.\n\n\nAsynchronous Rendering and Server-based Renderer: Fig. 2(a) illustrates a typical pipeline (sequential simulation and rendering) to collect samples from multi-process environments. It includes the following stages: (1) do physical simulation on worker processes; (2) take pictures (update renderer GPU states and submit draw calls); (3) wait for GPU render to finish; (4) copy image observations to CPU; (5) compute rewards and get other observations (e.g., robot proprioceptive info); (6) copy images to the main python process and synchronize; (7) copy these images to GPU for policy learning; (8) forward the policy network on GPU; (9) copy output actions from the GPU to the simulation worker processes.\n\n\nWe observe that the CPU is idling during GPU rendering (stage 3), while reward computation (stage 5) often does not rely on rendering results. Thus, we can increase CPU utilization by starting reward computation immediately after stage 2. We refer to this technique as asynchronous rendering.\n\n\nAnother observation is that images are copied from GPU to CPU on each process, passed to the main python process, and uploaded to the GPU again. It would be ideal if we can keep the data on GPU at all times. Our solution is to use a render server, which starts a thread pool on the main Python process and executes rendering requests from simulation worker processes, summarized in figure 2(b). The render server eliminates GPU-CPU-GPU image copies, thereby reducing data transfer time. It allows GPU resources to share across any number of environments, thereby significantly reducing memory usage. It enables communication over network, thereby having the potential to simulate and render on multiple machines. It requires minimal API changes \u2013 the only change to the original code base is to receive images from the render server. It also has additional benefits in software development with Nvidia GPU, which we detail in Appendix E.2.\n\n\nComparison: We compare the throughput of ManiSkill2 with 3 other frameworks that support visual RL: Habitat 2.0 (Szot et al., 2021), RoboSuite 1.3 (Zhu et al., 2020), and Isaac Gym (Makoviychuk et al., 2021). We build a PickCube environment in all simulators. We use similar physical simulation parameters (500Hz simulation frequency and 20Hz control frequency\u00b2), and we use the GPU pipeline for rendering. Images have resolution 128x128. All frameworks are given a computation budget of 16 CPU cores (logical processors) of an Intel i9-9960X CPU with 128G memory and 1 RTX Titan GPU with 24G memory. We test with random actions and also with a randomly initialized nature CNN (Mnih et al., 2015) as policy network. We test all frameworks on 16, 32, 64, 128, 256, 512 parallel environments and report the highest performance. Results are shown in Table 1(a) (more\n\n\n<table><tr><td></td><td>ManiSkill2 Server</td><td>ManiSkill2 Sync</td><td>Habitat 2.0</td><td>RoboSuite 1.3</td><td>Isaac Gym</td></tr><tr><td>Total FPS (rand. action)</td><td>2344\u00b168</td><td>942\u00b119</td><td>1275\u00b110</td><td>924\u00b13</td><td>865\u00b135</td></tr><tr><td>Total FPS (nature CNN)</td><td>2181\u00b175</td><td>931\u00b14</td><td>1224\u00b113</td><td>894\u00b115</td><td>835\u00b15</td></tr><tr><td>Optimal #Envs</td><td>64</td><td>32</td><td>64</td><td>32</td><td>512</td></tr></table>\n\n\n(a)\n\n\n<table><tr><td>#Envs</td><td>ManiSkill2 Server</td><td>Habitat 2.0</td></tr><tr><td>4</td><td>4.9G</td><td>6.4G</td></tr><tr><td>8</td><td>5.1G</td><td>12.9G</td></tr><tr><td>64</td><td>5.8G</td><td>(OOM)</td></tr></table>\n\n\n(b)\n\n\nTable 1: (a) Comparison of sample collection speed (FPS) on PickCube across different frameworks. (b) GPU memory usage on multi-process environments with 74 YCB objects each.\n\n\ndetails in Appendix E.3). An interesting observation is that our environment performs the best when the number of parallel environments exceeds the number of CPU cores. We conjecture that this is because execution on CPU and GPU are naturally efficiently interleaved through OS or driver-level schedulers when the requested computation far exceeds the available resources.\n\n\nAdditionally, we demonstrate the advantage of memory sharing thanks to our render server. We extend the PickClutterYCB environment to include 74 YCB objects per scene, and create the same setting in Habitat 2.0. As shown in Table 1(b), even though we enable GPU texture compression for Habitat and use regular textures for ManiSkill2, the memory usage of Habitat grows linearly as the number of environments increases, while ManiSkill2 requires very little memory to create additional environments since all meshes and textures are shared across environments.\n\n\n# 5 APPLICATIONS\n\n\nIn this section, we show how ManiSkill2 supports widespread applications, including sense-plan-act frameworks and imitation / reinforcement learning. In this section, for tasks that have asset variations, we report results on training objects. Results on held-out objects are presented in Appendix F. Besides, we demonstrate that policies trained in ManiSkill2 have the potential to be directly deployed in the real world.\n\n\n# 5.1 SENSE-PLAN-ACT\n\n\nSense-plan-act (SPA) is a classical framework in robotics. Typically, a SPA method first leverages perception algorithms to build a world model from observations, then plans a sequence of actions through motion planning, and finally execute the plan. However, SPA methods are usually open-loop and limited by motion planning, since perception and planning are independently optimized. In this section, we experiment with two perception algorithms, Contact-GraspNet (Sundermeyer et al., 2021) and Transporter Networks (Zeng et al., 2020).\n\n\nContact-GraspNet for PickSingleYCB: The SPA solution for PickSingleYCB is implemented as follows. First, we use Contact-GraspNet (CGN) to predict potential grasp poses along with confidence scores given the observed partial point cloud. We use the released CGN model pretrained on ACRONYM (Eppner et al., 2021). Next, we start with the predicted grasp pose with the highest score, and try to generate a plan to achieve it by motion planning. If no plan is found, the grasp pose with the next highest score is attempted until a valid plan is found. Then, the agent executes the plan to reach the desired grasp pose and closes its grippers to grasp the object. Finally, another plan is generated and executed to move the gripper to the goal position.\n\n\nFor each of the 74 objects, we conduct 5 trials (different initial states). The task succeeds if the object's center of mass is within  $2.5\\mathrm{cm}$  of the goal. The success rate is  $43.24\\%$ . There are two main failure modes: 1) predicted poses are of low confidence ( $27.03\\%$  of trials), especially for objects (e.g., spoon and fork) that are small and thin; 2) predicted poses are of low grasp quality or unreachable ( $29.73\\%$  of trials), but with high confidence. See Appendix F.1 for examples.\n\n\nTransporter Network for AssemblingKits: We benchmark Transporter Networks (TPN) on our AssemblingKits. The original success metric (pose accuracy) used in TPN is whether the peg is placed within 1cm and 15 degrees of the goal pose. Note that our version requires pieces to actually fit into holes, and thus our success metric is much stricter. We train TPN from scratch with image data sampled from training configurations using two cameras, a base camera and a hand camera. To\n\n\n<table><tr><td>Obs. Mode</td><td>PickCube</td><td>StackCube</td><td>Fill</td><td>Hang</td><td>Excavate</td><td>Pour</td><td>Pinch</td><td>Write</td></tr><tr><td>Point Cloud</td><td>0.22 \u00b1 0.06</td><td>0.04 \u00b1 0.02</td><td>0.45 \u00b1 0.04</td><td>0.35 \u00b1 0.15</td><td>0.08 \u00b1 0.04</td><td>0.02 \u00b1 0.02</td><td>0.00 \u00b1 0.00</td><td>0.00 \u00b1 0.00</td></tr><tr><td>RGBD</td><td>0.01 \u00b1 0.02</td><td>0.00 \u00b1 0.00</td><td>0.62 \u00b1 0.07</td><td>0.20 \u00b1 0.12</td><td>0.21 \u00b1 0.04</td><td>0.00 \u00b1 0.00</td><td>0.00 \u00b1 0.00</td><td>0.00 \u00b1 0.00</td></tr></table>\n\n\nTable 2: Mean and standard deviation of the success rate of behavior cloning on rigid-body and soft-body tasks. For rigid-body assembly tasks not shown in the table, success rate is 0.\n\n\n<table><tr><td>Obs. Mode</td><td>PickCube</td><td>StackCube</td><td>PickSingleYCB</td><td>PegInsSide</td><td>PlugCharger</td><td>AssemblingKits</td><td>TurnFaucet</td><td>AvoidObstacles</td></tr><tr><td>Point Cloud</td><td>0.94 \u00b1 0.03</td><td>0.95 \u00b1 0.02</td><td>0.51 \u00b1 0.05</td><td>0.01 \u00b1 0.01</td><td>0.01 \u00b1 0.02</td><td>0.00 \u00b1 0.00</td><td>0.04 \u00b1 0.03</td><td>0.00 \u00b1 0.00</td></tr><tr><td>RGBD</td><td>0.91 \u00b1 0.05</td><td>0.87 \u00b1 0.04</td><td>0.18 \u00b1 0.07</td><td>0.01 \u00b1 0.01</td><td>0.01 \u00b1 0.01</td><td>0.00 \u00b1 0.00</td><td>0.03 \u00b1 0.03</td><td>0.00 \u00b1 0.00</td></tr></table>\n\n\nTable 3: Mean and standard deviation of success rates of DAPG+PPO on rigid-body tasks. Training budget is 25M time steps.\n\n\naddress our high-precision success criterion, we increase the number of bins for rotation prediction from 36 in the original work to 144. During evaluation, we employ motion planning to move the gripper to the predicted pick position, grasp the peg, then generate another plan to move the peg to the predicted goal pose and drop it into the hole. The success rate over 100 trials is  $18\\%$ , and the pose accuracy is  $96\\%$ . See Appendix F.2 for more details.\n\n\n# 5.2 IMITATION & REINFORCEMENT LEARNING WITH DEMONSTRATIONS\n\n\nFor the following experiments, unless specified otherwise, we use the delta end-effector pose controller for rigid-body environments and the delta joint position controller for soft-body environments, and we translate demonstrations accordingly using the approach in Sec.2.2. Visual observations are captured from a base camera and a hand camera. For RGBD-based agents, we use IMPALA (Espeholt et al., 2018) as the visual backbone. For point cloud-based agents, we use PointNet (Qi et al., 2017) as the visual backbone. In addition, we transform input point clouds into the end-effector frame, and for environments where goal positions are given (PickCube and PickSingleYCB), we randomly sample 50 green points around the goal position to serve as visual cues and concatenate them to the input point cloud. We run 5 trials for each experiment and report the mean and standard deviation of success rates. Further details are presented in Appendix F.3.\n\n\nImitation Learning: We benchmark imitation learning (IL) with behavior cloning (BC) on our rigid and soft-body tasks. All models are trained for 50k gradient steps with batch size 256 and evaluated on test configurations.\n\n\nResults are shown in Table 2. For rigid-body tasks, we observe low or zero success rates. This suggests that BC is insufficient to tackle many crucial challenges from our benchmark, such as precise control in assembly and large asset variations. For soft-body tasks, we observe that it is difficult for BC agents to precisely estimate action influence on soft body properties (e.g. displacement quantity or deformation). Specifically, agents perform poorly on Excavate and Pour, as Excavate is successful only if a specified amount of clay is scooped up and Pour is successful when the final liquid level accurately matches a target line. On the other hand, for Fill and Hang, which do not have such precision requirements (for Fill, the task is successful as long as the amount of particles in the beaker is larger than a threshold), the success rates are much higher. In addition, we observe that BC agents cannot well utilize target shapes to guide precise soft body deformation, as they are never successful on Pinch or Write. See Appendix F.5 for further analysis.\n\n\nRL with Demonstrations: We benchmark demonstration-based online reinforcement learning by augmenting Proximal Policy Gradient (PPO) (Schulman et al., 2017) objective with the demonstration objective from Demonstration-Augmented Policy Gradient (DAPG) (Rajeswaran et al., 2017). Our implementation is similar to Jia et al. (2022), and further details are presented in Appendix F.3. We train all agents from scratch for 25 million time steps. Due to limited computation resources, we only report results on rigid-body environments.\n\n\nResults are shown in Table 3. We observe that for pick-and-place tasks, point cloud-based agents perform significantly better than RGBD-based agents. Notably, on PickSingleYCB, the success rate is even higher than Contact-GraspNet with motion planning. This demonstrates the potential\n\n\n![Simulation Figure 3: Left: Simulation and real world setup for PickCube. Right: Motion planning execution results for Pinch in simulation and in the real world: (a) initial state; (b) letter \"M\"; (c) letter \"S\".](images/7a07dbb345a60a7cd55c39228a734b046eb1a726de9e8a856e922795455ad67a.jpg)\n\n\n![Real](images/c7c36b92349b7fcb1c50eb60066566380aa6891289bd19adc17eb6f2ce3c2d47.jpg)\n\n\n![(a)](images/0ee92fed74707b8816abf16c98a113ab2867391fc389cc3e1ce569d446117c87.jpg)\n\n\n![(b)](images/e0101b5681f62e463b89a22041706d65691901961d9fdbe112aa9b70c4d0531e.jpg)\n\n\n![(c)](images/957abea9fabebd9f7d8d612939e6f85b9cd05a8ed7386dc3b7ffb129e88ebbce.jpg)\n\n\nof obtaining a single agent capable of performing general pick-and-place across diverse object geometries through online training. We also further examine factors that influence point cloud-based manipulation learning in Appendix F.6. However, for all other tasks, notably the assembly tasks that require high precision, the success rates are near zero. In Appendix F.7, we show that if we increase the clearance of the assembly tasks and decrease their difficulty, agents can achieve much higher performance. This suggests that existing RL algorithms might have been insufficient yet to perform highly precise controls, and our benchmark poses meaningful challenges for the community.\n\n\nIn addition, we examine the influence of controllers for agent learning, and we perform ablation experiments on PickSingleYCB using point cloud-based agents. When we replace the delta end-effector pose controller in the above experiments with the delta joint position controller, the success rate falls to  $0.22 \\pm 0.18$ . The profound impact of controllers demonstrates the necessity and significance of our multi-controller conversion system.\n\n\n# 5.3 SIM2REAL\n\n\nManiSkill2 features fully-simulated dynamic interaction for rigid-body and soft-body. Thus, policies trained in ManiSkill2 have the potential to be directly deployed in the real world.\n\n\nPickCube: We train a visual RL policy on PickCube and evaluate its transferability to the real world. The setup is shown in Fig. 3-L, which consists of an Intel RealSense D415 depth sensor, a 7DoF ROKAE xMate3Pro robot arm, a Robotiq 2F-140 2-finger gripper, and the target cube. We first acquire the intrinsic parameters for the real depth sensor and its pose relative to the robot base. We then build a simulation environment aligned with the real environment. We train a point cloud-based policy for 10M time steps, where the policy input consists of robot proprioceptive information (joint position and end-effector pose) and uncolored point cloud backprojected from the depth map. The success rate in simulation is  $91.0\\%$ . We finally directly evaluate this policy in the real world 50 times with different initial states, where we obtain  $60.0\\%$  success rate. We conjecture that the performance drop is caused by the domain gap in depth maps, as we only used Gaussian noise and random pixel dropout as data augmentation during simulation policy training.\n\n\nPinch: We further evaluate the fidelity of our soft-body simulation by executing the same action sequence generated by motion planning in simulation and in the real world and comparing the final plasticine shapes. Results are shown in Fig. 3-R, which demonstrates that our 2-way coupled rigid-MPM simulation is able to reasonably simulate plasticine's deformation caused by multiple grasps.\n\n\n# 6 CONCLUSION\n\n\nTo summarize, ManiSkill2 is a unified and accessible benchmark for generic and generalizable manipulation skills, providing 20 manipulation task families, over 2000 objects, and over 4M demonstration frames. It features highly efficient rigid-body simulation and rendering system for sample collection to train RL agents, real-time MPM-based soft-body environments, and versatile multi-controller conversion support. We have demonstrated its applications in benchmarking sense-plan-act and imitation / reinforcement learning algorithms, and we show that learned policies on ManiSkill2 have the potential to transfer to the real world.\n\n\n# REFERENCES\n\n- Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.\n\n- Kent Beck, Mike Beedle, Arie Van Bennekum, Alistair Cockburn, Ward Cunningham, Martin Fowler, James Grenning, Jim Highsmith, Andrew Hunt, Ron Jeffries, et al. Manifesto for agile software development, 2001.\n\n- Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n\n- Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 international conference on advanced robotics (ICAR), pp. 510-517. IEEE, 2015.\n\n- Hilko Cords and Oliver G. Staadt. Instant liquids: Fast screen-space surface generation for particle-based liquids. In Poster proceedings of ACM Siggraph/Eurographics Symposium on Computer Animation, 2008.\n\n- Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016-2021.\n\n- Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: A framework for visual object manipulation. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4495-4504, 2021.\n\n- Clemens Eppner, Arsalan Mousavian, and Dieter Fox. Acronym: A large-scale grasp dataset based on simulation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 6222-6227. IEEE, 2021.\n\n- Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pp. 1407-1416. PMLR, 2018.\n\n- Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, et al. Threadworld: A platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020.\n\n- Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel LK Yamins, James J DiCarlo, Josh McDermott, Antonio Torralba, et al. The threadworld transport challenge: A visually guided task-and-motion planning benchmark for physically realistic embodied ai. arXiv preprint arXiv:2103.14025, 2021.\n\n- Jiayuan Gu, Devendra Singh Chaplot, Hao Su, and Jitendra Malik. Multi-skill mobile manipulation for object rearrangement. arXiv preprint arXiv:2209.02778, 2022.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Yuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin Zhu, Andre Pradhana, and Chenfanfu Jiang. A moving least squares material point method with displacement discontinuity and two-way rigid body coupling. ACM Transactions on Graphics (TOG), 37(4):150, 2018.\n\n- Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, and Fredo Durand. Taichi: a language for high-performance computation on spatially sparse data structures. ACM Transactions on Graphics (TOG), 38(6):1-16, 2019.\n\n- Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B Tenenbaum, and Chuang Gan. Plasticinelab: A soft-body manipulation benchmark with differentiable physics. arXiv preprint arXiv:2104.03311, 2021.\n\n- Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019-3026, 2020.\n\n- Zhiwei Jia, Xuanlin Li, Zhan Ling, Shuang Liu, Yiran Wu, and Hao Su. Improving policy optimization with generalist-specialist learning. In International Conference on Machine Learning, pp. 10104-10119. PMLR, 2022.\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\n\n- Chengshu Li, Cem Gokmen, Gabrael Levine, Roberto Martin-Martin, Sanjana Srivastava, Chen Wang, Josiah Wong, Ruohan Zhang, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In 6th Annual Conference on Robot Learning.\n\n- Xingyu Lin, Yufei Wang, Jake Olkin, and David Held. Softgym: Benchmarking deep reinforcement learning for deformable object manipulation. In Conference on Robot Learning, 2020.\n\n- Minghua Liu, Xuanlin Li, Zhan Ling, Yangyan Li, and Hao Su. Frame mining: a free lunch for learning robotic manipulation from 3d point clouds. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=d-JYso87y6s.\n\n- Miles Macklin. Warp: A high-performance python framework forgpu simulation and graphics. https://github.com/nvidia/warp, March 2022. NVIDIA GPU Technology Conference (GTC).\n\n- Jeffrey Mahler, Florian T Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry, Kai Kohlhoff, Torsten Kr\u00f6ger, James Kuffner, and Ken Goldberg. Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards. In IEEE International Conference on Robotics and Automation (ICRA), pp. 1957\u20131964. IEEE, 2016.\n\n- Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performancegpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021.\n\n- Roberto Martin-Martin, Michelle A Lee, Rachel Gardner, Silvio Savarese, Jeannette Bohg, and Animesh Garg. Variable impedance control in end-effector space: An action space for reinforcement learning in contact-rich tasks. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1010-1017. IEEE, 2019.\n\n- Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529-533, 2015.\n\n- Douglas Morrison, Peter Corke, and J\u00fcrgen Leitner. Egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation. IEEE Robotics and Automation Letters, 5 (3):4368-4375, 2020.\n\n- Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Cathera Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\n- Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652-660, 2017.\n\n- Yuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu, and Hao Su. S4g: Amodal single-view single-shot se (3) grasp detection in cluttered scenes. In Conference on robot learning, pp. 53-65. PMLR, 2020.\n\n- Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1-8, 2021. URL http://jmlr.org/papers/v22/20-1364.html.\n\n- Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.\n\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\n- Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for robotic manipulation. arXiv preprint arXiv:2209.05451, 2022.\n\n- Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00edn-Mart\u00edn, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on Robot Learning, pp. 477-490. PMLR, 2022.\n\n- Martin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, and Dieter Fox. Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 13438-13444. IEEE, 2021.\n\n- Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. Habitat 2.0: Training home assistants to rearrange their habitat. Advances in Neural Information Processing Systems, 34:251-266, 2021.\n\n- Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.\n\n- Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. arXiv preprint arXiv:2107.14171, 2021.\n\n- Jiayi Weng, Min Lin, Shengyi Huang, Bo Liu, Denys Makoviichuk, Viktor Makoviychuk, Zichen Liu, Yufan Song, Ting Luo, Yukun Jiang, et al. Envpool: A highly parallel reinforcement learning environment execution engine. arXiv preprint arXiv:2206.10558, 2022.\n\n- Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11097-11107, 2020.\n\n- Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 1094-1100. PMLR, 2020.\n\n- Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and Johnny Lee. Transporter networks: Rearranging the visual world for robotic manipulation. Conference on Robot Learning (CoRL), 2020.\n\n- Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Martin-Martin. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.\n\nResponse: Given the comprehensive and ambitious nature of the paper, presenting a unified benchmark for manipulative skills across diverse environments, I predict:\n\n\\[\n\\boxed{\\text{Accept}}\n\\]\n\n### Justification:\n\n#### Originality:\nThe paper introduces ManiSkill2, which is a significant advancement in the field of robotic manipulation benchmarks. It addresses several shortcomings of existing benchmarks by incorporating a broader range of manipulation tasks, including soft-body manipulation, multi-controller support, and real-time soft-body simulation. This novelty stands out as a substantial contribution to the field.\n\n#### Methodological Rigor:\nThe authors provide a thorough methodology for building and validating the benchmark. They describe the rigorous approach to generating demonstrations for various tasks using task and motion planning (TAMP) and model predictive control (MPC). The paper details the implementation of a cloud-based evaluation system, which is a valuable feature for reproducibility and fairness in benchmark comparisons. The discussion on the challenges and solutions for soft-body simulation and real-time rendering is also well-presented, showcasing technical depth and feasibility.\n\n#### Significance of Contribution:\nThe proposed benchmark addresses critical gaps in the literature by providing a versatile, large-scale, and realistic environment for testing and training algorithms on manipulative skills. The inclusion of heterogeneous task families, real-time soft-body simulation, and multi-controller support make ManiSkill2 a comprehensive and practical resource for researchers and developers in the field of embodied AI and robotics.\n\n#### Clarity of Presentation:\nThe paper is well-organized and clearly structured. It begins with an introduction to the problem and motivation, followed by detailed descriptions of the benchmarks, methodologies, and experimental results. The supplementary materials and figures provide additional context and support the claims made in the main text. The code and datasets are openly available, enhancing the reproducibility of the work.\n\n#### Alignment with Journal Scope:\nICLR, while primarily focused on machine learning, accepts papers that bridge the gap between ML and robotics. The paper's focus on manipulation skills and the integration of ML techniques (such as imitation learning and reinforcement learning) aligns well with ICLR\u2019s scope. The paper also mentions the potential impact of the benchmark on the field, indicating a clear contribution to advancing research in both robotics and AI.\n\n### Weaknesses:\nWhile the paper presents a strong case for the utility of ManiSkill2, some areas could benefit from further elaboration:\n1. **Detailed Performance Comparisons:** While some results are provided, a more comprehensive comparison across different algorithms and setups would strengthen the argument for the benchmark's effectiveness.\n2. **Real-world Deployment Insights:** Although the paper discusses transferability to the real world, more concrete examples or results demonstrating successful deployment in real-world scenarios would enhance the impact.\n3. **Future Work Directions:** Mentioning future directions for improving the benchmark or expanding its capabilities could provide a roadmap for future research.\n\nOverall, despite these minor suggestions, ManiSkill2 is a compelling and impactful contribution to the field, making it likely to be accepted by ICLR.", "message": "LLMCall function forward"}
{"name": "textgrad", "msg": "LLMCall function forward", "args": "()", "levelname": "INFO", "levelno": "20", "pathname": "/n/fs/vision-mix/jl0796/LLaMA-Factory-AutoReviewer/textgrad_experiments/.venv/lib/python3.11/site-packages/textgrad/autograd/llm_ops.py", "filename": "llm_ops.py", "module": "llm_ops", "exc_info": "None", "exc_text": "None", "stack_info": "None", "lineno": "69", "funcName": "forward", "created": "1768636664.938273", "msecs": "938.0", "relativeCreated": "125631.41345977783", "thread": "139964048283456", "threadName": "MainThread", "processName": "MainProcess", "process": "3548717", "text": "System:You are an expert academic reviewer tasked with evaluating research papers to predict their likelihood of acceptance by a peer-reviewed journal. Assess the paper based on originality, methodological rigor, significance of contribution, clarity of presentation, and alignment with the journal's scope. Prioritize balancing innovation with feasibility, and consider the paper's potential impact on the field. Provide a clear rationale for your prediction, weighing strengths and weaknesses systematically.\nQuery: I am giving you a paper. I want to predict its acceptance outcome at ICLR.\n - Your answer will either be: \\boxed{Accept} or \\boxed{Reject}\n - Note: ICLR generally has a ~30% acceptance rate\n\n# Support-set bottlenecks for video-text representation learning\n\n# Abstract\nThe dominant paradigm for learning video-text representations \u2013 noise contrastive learning \u2013 increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semantically-related \u2013 for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample\u2019s caption must be reconstructed as a weighted combination of a support set of visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX, ActivityNet, and MSVD for video-to-text and text-to-video retrieval.\n\n\n# 1 INTRODUCTION\n\n\nNoise contrastive learning (Gutmann & Hyvarinen, 2010) is emerging as one of the best approaches to learn data representations both for supervised (Khosla et al., 2020) and unsupervised regimes (Chen et al., 2020c). The idea is to learn a representation that discriminates any two data samples while being invariant to certain data transformations. For example, one might learn a representation that identifies a specific image up to arbitrary rotations (Misra & van der Maaten, 2020). In a multi-modal setting, the transformations can separate different modalities, for example, by extracting the audio and visual signals from a video. The resulting noise contrastive representation associates audio and visual signals that come from the same source video, differentiating others (Patrick et al., 2020).\n\n\nThe noise contrastive approach is motivated by the fact that the transformations that are applied to the data samples leave their 'meaning' unchanged. For example, rotating an image does not change the fact that it contains a cat or not (Gidaris et al., 2018). However, in most cases, we expect to find many data samples that share the same content without being necessarily related by simple transformations (e.g. think of any two images of cats). Existing noise contrastive formulations are unaware of these relationships and still try to assign different representations to these samples (Wu et al., 2018), despite the fact that they are semantically equivalent. If the representation is learned for a downstream task such as semantic video retrieval, this might degrade performance.\n\n\nThis suggest that there might be other learning signals that could complement and improve pure contrastive formulations. In this paper, we explore this idea in the case of learning from two modalities: videos and text, in the form of video transcripts or captions. Given a state-of-the-art contrastive formulation that learns from these two modalities, we investigate complementary pretext objectives to improve it. First, we consider the (instance) captioning task, namely mapping a video to the corresponding text, casting this as a conditional stochastic text generation problem. We show that this brings only a modest benefit.\n\n\nWe observe that the captioning task is highly sample-specific, as the goal is to produce a caption which describes a specific video and not any other video, and thus it suffers from the same disadvantages (discouraging concept sharing among samples) as contrastive learning. Thus, we propose\n\n\n![Fig. 1: Cross-modal discrimination and cross-captioning. Our model learns from two complementary losses: (a) Cross-modal contrastive learning learns strong joint video-text embeddings, but every other sample is considered a negative, pushing away even semantically related captions (orange arrows). (b) We introduce a generative task of cross-captioning, which alleviates this by learning to reconstruct a sample's text representation as a weighted combination of a support-set, composed of video representations from other samples.](images/1c91305c049a8f80771f0f0bd39786287895c94293b9e8ccc07105316bd5134f.jpg)\n\n\n![](images/436d25e4814ed30d5e1584b0af72f4f6435be0d2e99d0442443d1af69e98320d.jpg)\n\n\nto address this issue by switching to a different text generation task. The idea is to modify the text generator to take as input a learnable mixture of a support-set of videos, which we call cross-instance captioning. The mixture weights are generated by comparing the learned video representations to captions' representations in an online way over the batch. The limited set of support samples acts as a bottleneck that encourages extraction of shared semantics. In this manner, the embeddings can associate videos that share similar captions even if the contrastive loss tries to push them apart.\n\n\nWe show that, when the captioning task is added in this manner, it brings a sensible improvement to already very strong video representation learning results, further improving our own state-of-the-art baseline by a significant margin.\n\n\n# 2 RELATED WORKS\n\n\nLearning data representations from unlabelled data has been a long standing goal of machine learning. These approaches are called \"self-supervised learning\" because the learning signals, termed pretext tasks, are obtained from the data itself. In the image and video domain, pretext tasks include colorization (Zhang et al., 2016), rotation (Gidaris et al., 2018), or clustering (Asano et al., 2020a;b; Caron et al., 2018; Ji et al., 2018), while in the natural language domain, masked language modeling (Devlin et al., 2019), and next word prediction (Mikolov et al., 2013; Pennington et al., 2014) are extremely popular. These pretext tasks can be broadly classified into two classes: generative and discriminative.\n\n\nDiscriminative approaches learn representations by differentiating input samples, using objectives such as the contrastive loss (Gutmann & Hyvarinen, 2010; Hadsell et al., 2006). Discriminative approaches have proven to be particularly successful for image (Chen et al., 2020c; He et al., 2020; Misra & van der Maaten, 2020; Wu et al., 2018) and video (Han et al., 2019; Morgado et al., 2020; Patrick et al., 2020) representation learning. Generative approaches, on the other hand, try to reconstruct its input. GANs (Donahue & Simonyan, 2019; Goodfellow et al., 2014; Radford et al., 2015), autoencoders (Hinton & Salakhutdinov, 2006) and sequence-to-sequence models (Sutskever et al., 2014) are popular generative models. In this work, we show the importance of combining both discriminative and generative objectives to learn effective video-text representations.\n\n\nThe success of representation learning has also been due to advances in model architectures, such as the Transformer (Vaswani et al., 2017). BERT (Devlin et al., 2019) demonstrated that a transformer architecture pretrained on large-scale textual data can learn transferable text representations that can be fine-tuned on a variety of downstream tasks. Subsequent works (Clark et al., 2020; Lewis et al., 2020a;b; Radford et al., 2019; Raffel et al., 2019), have improved upon the transformer architecture or training objective to learn even better representations. Inspired by the success of transformers in the NLP domain, several works have leveraged transformers to learn transferable image (Chen et al., 2020a; Desai & Johnson, 2020; Sariyildiz et al., 2020) or multi-modal image-text representations (Chen et al., 2019; Li et al., 2020a; 2019; Lu et al., 2019; Su et al., 2019; Tan & Bansal, 2019). In this work, we leverage the transformer architecture to better encode and represent text and video.\n\n\n![Fig. 2: (a) Our cross-modal framework with the discriminative (contrastive) objective and the generative objective. The model learns to associate video-text pairs in a common embedding space with text and video encoders (top). Meanwhile, the text must also be reconstructed as a weighted combination of video embeddings from a support-set (bottom), selected via attention, which enforces representation sharing between different samples. (b) Weights matrices (attention maps) used in each cross-captioning objective (see section 3.1.2).](images/78d794b3160c7f4719768a52eeb8a56695dbc530ed87535b536fb26df095e183.jpg)\n\n\nLarge-scale training data has enabled the more effective pretraining of image (Sun et al., 2017; Yalniz et al., 2019), video (Ghadiyaram et al., 2019; Thomee et al., 2016) and textual representations (Raffel et al., 2019). The release of the HowTo100M dataset (Miech et al., 2019), a large-scale instructional video dataset, has spurred significant interest in leveraging large-scale pretraining to improve video-text representations for tasks such as video question-answering (Lei et al., 2018), text-video retrieval (Liu et al., 2019) and video captioning (Zhou et al., 2018b) on smaller datasets such as YouCookII (Zhou et al., 2018a), MSVD (Venugopalan et al., 2015a), MSR-VTT (Xu et al., 2016), LSMDC (Rohrbach et al., 2017), DiDeMo (Hendricks et al., 2018) and ActivityNet (Krishna et al., 2017). Although semantically rich and diverse, instructional videos from the web are super noisy and therefore a few approaches have been proposed to combat this. A few works (Luo et al., 2020; Sun et al., 2019a;b; Zhu & Yang, 2020) extend the BERT model to accept both visual and textual tokens to learn high-level semantic video-text representations. Other works have leveraged the contrastive loss (Miech et al., 2020) and show that using the raw audio (Alayrac et al., 2020; Rouditchenko et al., 2020) and other modalities (Gabeur et al., 2020) can be used to better align and improve video-text representations. While all these approaches rely on a contrastive objective, VidTranslate (Korbar et al., 2020) shows that a generative objective can also be used to learn joint video-text representations. In contrast to Korbar et al. (2020), we show that combining contrastive and generative objectives to pre-train video-text representations on large-scale data such as HowTo100M is very effective. The generative objective serves as regularizer to mitigate the strictness of the instance discrimination task of the constrastive objective, showing benefits similar to approaches such as clustering (Caron et al., 2020; Li et al., 2020b) and feature mixing (Kalantidis et al., 2020) which have been applied in the image domain.\n\n\n# 3 METHOD\n\n\nWe consider the problem of learning multimodal representations from a corpus  $\\mathcal{C}$  of video-text pairs  $(v, t)$ , where  $v$  is a video and  $t$  is its corresponding text (caption or transcription). Our goal is to learn a pair of representation maps  $c_v = \\Psi(v)$  and  $c_t = \\Phi(t)$ , with outputs in a  $d$ -dimensional embedding space  $c_v, c_t \\in \\mathbb{R}^d$ , where semantically similar instances are close to each other.\n\n\n# 3.1 OBJECTIVE FOR LEARNING MULTIMODAL REPRESENTATIONS\n\n\nWe consider two learning objectives, also illustrated in Figure 1. The first is the contrastive objective, pushing embeddings  $c_t$  and  $c_v$  to be close if text  $t$  and video  $v$  come from the same sample and push-\n\n\ning them apart otherwise. This assumes that every sample is its own class and does not benefit from modelling similarities across instances. The second objective is generative captioning. In its most basic variant, it maximizes the probability of generating the text  $t$  given the corresponding video  $v$ . However, we suggest that variants that explicitly promote concept sharing between instances will result in better downstream performance, in tasks such as video retrieval. These variants, illustrated in Figure 2, have in common that the caption  $t$  is reconstructed from a learned weighted combination over other videos  $\\hat{v}$ . This is a form of attention (Bahdanau et al., 2014) which encourages the network to learn about which videos share similar semantics, compensating for the contrastive loss and grouping them implicitly.\n\n\nIn the following, we denote with  $\\mathcal{B} \\subset \\mathcal{C}$  a batch of multi-modal samples, i.e. a finite collection of video-text pairs  $(t,v) \\in \\mathcal{C}$ . For simplicity, we denote the batch as  $\\mathcal{B} = \\{(t^i,v^i)\\}_{i=1}^B$ .\n\n\n# 3.1.1 CONTRASTIVE OBJECTIVE\n\n\nTo define the contrastive objective, let  $s(a,b) = \\frac{a^\\top b}{\\|a\\|\\|b\\|}$  be the similarity measure between vectors  $a$  and  $b$ . Following Faghri et al. (2018), we adopt the hinge-based triplet ranking loss with hard negative mining:\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {c o n t r a s t}} = \\frac {1}{B} \\sum_ {i = 1} ^ {B} \\left[ \\max  _ {j} \\left[ \\alpha - s \\left(c _ {t} ^ {i}, c _ {v} ^ {i}\\right) + s \\left(c _ {t} ^ {i}, c _ {v} ^ {j}\\right) \\right] _ {+} + \\max  _ {j} \\left[ \\alpha - s \\left(c _ {t} ^ {i}, c _ {v} ^ {i}\\right) + s \\left(c _ {t} ^ {j}, c _ {v} ^ {i}\\right) \\right] _ {+} \\right], \\tag {1}\n$$\n\n$$\n\n\nwhere  $\\alpha$  is the correlation margin between positive and negative pairs and  $[\\cdot]_{+} = \\max\\{0, \\cdot\\}$  is the hinge function. In our experiments, we set  $\\alpha = 0.2$ .\n\n\n# 3.1.2 CROSS-CAPTIONING OBJECTIVES\n\n\nIn the conventional captioning, the decoder seeks to optimize the negative log-likelihood of a text sequence  $t$  given its corresponding video  $v$ :\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {c a p t i o n}} = - \\frac {1}{B} \\sum_ {i = 1} ^ {B} \\log p \\left(t ^ {i} \\mid e _ {v} ^ {i}\\right). \\tag {2}\n$$\n\n$$\n\n\nHere, the log-likelihood is obtained via auto-regressive decoding (Vaswani et al., 2017) from an intermediate video embedding  $e_v^i = \\Phi'(v^i)$ . For the cross-captioning objective, we modify this loss to condition the generation process on a weighted average of the embeddings of the other videos in the batch, which we call the support-set. The weights themselves, which can be interpreted as a batch-wise attention, are obtained as a softmax distribution with temperature  $T$  over batch indices based on the video embeddings, as follows:\n\n\n$$\n\n$$\n\\mathcal {L} ^ {\\text {c r o s s - c a p t i o n i n g}} = - \\frac {1}{B} \\sum_ {i = 1} ^ {B} \\log p \\left(t ^ {i} \\mid \\bar {e} _ {v} ^ {i}\\right), \\quad \\bar {e} _ {v} ^ {i} = \\sum_ {j \\in \\mathcal {S} _ {i}} \\frac {\\exp \\left\\langle c _ {t} ^ {i} , c _ {v} ^ {j} \\right\\rangle / T}{\\sum_ {k \\in \\mathcal {S} _ {i}} \\exp \\left\\langle c _ {t} ^ {i} , c _ {v} ^ {k} \\right\\rangle / T} \\cdot e _ {v} ^ {j}. \\tag {3}\n$$\n\n$$\n\n\nBy default, the summation in the softmax is conducted over a support set  $S_{i}$  containing all indices except  $i$ . In the experiments, we consider the following attention types for reconstruction. Identity captioning  $(S_{i} = \\{i\\})$  generates the caption from the corresponding video and reduces to the standard captioning objective, eq. (2). Full support  $(S_{i} = \\{1,\\dots ,B\\})$  considers all videos as possible candidates for captioning. Hybrid captioning sets the weights in eq. (3) as the average of the weights for identity captioning and full support. Cross-captioning  $(S_{i} = \\{j\\neq i\\})$  considers all but the video that one wishes to caption. This variant forces the network to extract all information required for captioning from other videos in the batch. Figure 2 compares graphically these attention mechanisms.\n\n\nConsidering both discriminative and generative objectives for learning multimodal representations, our full objective is  $\\mathcal{L} = \\mathcal{L}^{\\mathrm{contrast}} + \\lambda \\mathcal{L}^{\\mathrm{cross - captioning}}$ , where  $\\lambda$  balances two objectives. We set  $\\lambda = 10$  to ensure similar magnitudes for both losses in our experiments. In the training phase, we use Adam (Kingma & Ba, 2015) to minimize our loss. At inference time, we directly use  $\\Phi(t)$  and  $\\Psi(v)$  to encode video and text representations for retrieval.\n\n\n# 3.2 MODEL ARCHITECTURE\n\n\nWe now discuss the details of the encoders and decoder components in our architecture, illustrated in fig. 2. For the text decoder  $p(t|e_v)$  in eq. (2) and (3), we use a pre-trained T-5 decoder (Raffel et al., 2019).\n\n\nFor the video representation  $c_{v} = \\Psi (v) = \\Psi^{\\prime \\prime}(\\Psi^{\\prime}(v))$ , we use a video encoder  $e_v = \\Psi '(v)$  followed by a multi-layer transformer pooling head  $c_{v} = \\Psi^{\\prime \\prime}(e_{v})$ . The encoder  $\\Psi^{\\prime}(v)$  concatenates the output of pretrained ResNet-152 (He et al., 2016) and R(2+1)D-34 (Tran et al., 2018) networks applied to individual video frames, resulting in a code  $e_v = [e_{v1}\\dots e_{vM}]$  where  $M$  is the maximum duration of a video clip. For the pooling head  $c_{v} = \\Psi^{\\prime \\prime}(e_{v})$ , we consider a transformer architecture to attend to important context and summarize it into a fixed-length representation  $c_{v}$ . For this, we follow MMT (Gabeur et al., 2020), but with two important differences. First, while MMT uses 7 expert features that results in  $7\\times$  the sequence length, we only use a transformer to attend to early-fused motion and appearance features as the video representation, thus significantly reducing the sequence length and computational cost. Second, instead of stacking 6 transformer layers to encode the visual stream as in MMT, we only use a shallow two-layer transformer architecture with additional pre-encoders, further increasing model efficiency. As temporal 1D-convolutional neural networks (CNNs) (LeCun et al., 1998) were shown to effectively capture temporal dependencies in videos (Dong et al., 2019), we integrate CNNs into our transformer pooling heads to better capture video temporal signals. In more detail, we compute  $c_{v} = \\Psi^{\\prime \\prime}(e_{v})$  by chaining two transformer layers, each of the type:\n\n\n$$\n\n$$\n\\psi (e) = \\mathrm {B N} (\\mathrm {F F N} \\left(e _ {\\text {a t t n}}\\right) + e _ {\\text {a t t n}}), \\quad e _ {\\text {a t t n}} = \\mathrm {B N} (\\mathrm {M H A} (f (e)) + f (e)). \\tag {4}\n$$\n\n$$\n\n\nHere  $f$  is a pre-encoder that refines the video representation; we found empirically that a 1D CNN works well for this purpose. Then, we apply multi-head self-attention (MHA) (Vaswani et al., 2017) followed by a feed-forward network (FNN) with batch normalization (BN) (Ioffe & Szegedy, 2015). The architecture maps the input sequence  $e_v$  to a new 'contextualized' sequence of representation vectors; we take the first one as  $c_v$ .\n\n\nThe text representation decomposes in the same way as  $c_{t} = \\Phi(t) = \\Phi''(\\Phi'(t))$ . The text encoder  $e_{t} = \\Phi'(t)$  uses a pretrained T-5 network resulting in a code  $e_{t} = [e_{t1} \\cdots e_{tN}]$ , where  $N$  is the maximum length of a sentence. The pooling head  $c_{t} = \\Phi''(e_{t})$  follows the same design as the video case, but  $f$  is set to a recurrent neural network (RNN) instead of a CNN. Please refer to the appendix for details.\n\n\nIn practice, for computational reasons, we use eq. (3) to finetune the parameters of all networks except the video encoder  $\\Psi'(v)$ , which is fixed.\n\n\n# 4 EXPERIMENTS\n\n\nWe validate empirically the ability of our method to learn better representations for the downstream tasks of text-to-video and video-to-text retrieval. First, in sec. 4.2 we ablate various model components on the MSR-VTT dataset. Then, in sec. 4.3 we show that our best model significantly outperforms state-of-the-art retrieval systems on three datasets, MSR-VTT, ActivityNet and VATEX. Finally, in sec. 4.4 we analyse qualitatively the effect of the attention mechanism used during training.\n\n\n# 4.1 EXPERIMENTAL SETUP\n\n\nDatasets. HowTo100M (Miech et al., 2019) is a large-scale instructional video collection of 1.2 million YouTube videos, along with automatic speech recognition transcripts. We use this dataset for our pre-training experiments. MSR-VTT (Xu et al., 2016) contains 10000 videos, where each video is annotated with 20 descriptions. We report results on the 1k-A split (9000 training, 1000 testing) as in Liu et al. (2019). VATEX (Wang et al., 2019) is a multilingual (Chinese and English) video-text dataset with 34,911 videos. We use the official training split with 25991 videos and report on the validation split as in HGR (Chen et al., 2020b). The ActivityNet Caption (Krishna et al., 2017) dataset consists of densely annotated temporal segments of 20K YouTube videos. We use the 10K training split to train from scratch/ finetune the model and report the performance on the 5K 'val1' split. The MSVD (Chen & Dolan, 2011) dataset consists of 80K English descriptions for\n\n\nTable 1: Model Architecture and Training Details Ablation. Text  $\\rightarrow$  Video retrieval performance on MSR-VTT. Recall@1, 5, and Median Recall are shown. (a) Video Encoder. Stronger features and combination improves performance.\n\n\n<table><tr><td>Feature source</td><td>R@1\u2191</td><td>R@5\u2191</td><td>MdR \u2193</td></tr><tr><td>R-152</td><td>20.8</td><td>46.2</td><td>6.0</td></tr><tr><td>R(2+1)D-34</td><td>23.7</td><td>53.2</td><td>4.0</td></tr><tr><td>R(2+1)D-34 +R-152</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n(c) Text Encoder. Stronger encoding of text improves retrieval.\n\n\n<table><tr><td>Text Encoder</td><td>R@1 \u2191</td><td>R@5 \u2191</td><td>MdR \u2193</td></tr><tr><td>W2V (GloVe)</td><td>22.1</td><td>49.8</td><td>6.0</td></tr><tr><td>T5-Small</td><td>24.5</td><td>51.2</td><td>3.0</td></tr><tr><td>T5-Base</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n(b) Feature Aggregation. Learning temporal attention yields strong gains over pooling.\n\n\n<table><tr><td>Temporal reduction</td><td>R@1 \u2191</td><td>R@5 \u2191</td><td>MdR \u2193</td></tr><tr><td>Max</td><td>21.8</td><td>49.5</td><td>8.0</td></tr><tr><td>Mean</td><td>22.5</td><td>51.3</td><td>6.0</td></tr><tr><td>Multi-Head Attn</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n(d) Text Decoder. Stronger decoding of text improves retrieval.\n\n\n<table><tr><td>Text Encoder</td><td>Text Decoder</td><td>R@1 \u2191</td><td>R@5 \u2191</td><td>MdR \u2193</td></tr><tr><td>T5-Base</td><td>T5-Small</td><td>26.2</td><td>54.2</td><td>3.0</td></tr><tr><td>T5-Base</td><td>T5-Base</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n(e) Contrastive Loss. Inter-modal Triplet loss yields the best performance.\n\n\n<table><tr><td>Contrastive</td><td>R@1 \u2191</td><td>R@5 \u2191</td><td>MdR \u2193</td></tr><tr><td>InfoNCE (inter+intra)</td><td>10.7</td><td>28.5</td><td>15.0</td></tr><tr><td>InfoNCE (inter)</td><td>10.8</td><td>29.0</td><td>14.5</td></tr><tr><td>Triplet (inter+intra)</td><td>26.8</td><td>56.2</td><td>3.0</td></tr><tr><td>Triplet (inter)</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n(f) Support-set Size. Retrieval degrades when reconstructing from too small and too large sets.\n\n\n<table><tr><td rowspan=\"2\">Size</td><td colspan=\"7\">Batch-size</td><td colspan=\"2\">Memory bank</td></tr><tr><td>8</td><td>16</td><td>32</td><td>64</td><td>128</td><td>256</td><td>512</td><td>2k</td><td>8k</td></tr><tr><td>R@1/5</td><td>18.5/45.6</td><td>20.7/49.9</td><td>25.2/54.6</td><td>27.2/55.2</td><td>28.0/56.1</td><td>26.9/55.0</td><td>25.3/53.5</td><td>26.8/54.7</td><td>26.2/52.7</td></tr></table>\n\n\n1970 videos from YouTube, with each video associated with around 40 sentences each. We use the standard split of 1200, 100, and 670 videos for training, validation, and testing (Liu et al., 2019; Venugopalan et al., 2015b; Xu et al., 2015).\n\n\nEvaluation Metrics. To measure the text-to-video and video-to-text retrieval performance, we choose Recall at K (R@K) and Median Rank (MedR), which are common metrics in information retrieval.\n\n\n# 4.2 ABLATIONS\n\n\nIn Tab. 1, we first only ablate the cross-modal retrieval part of our network architecture, while the generative objectives are analysed in Tab. 2.\n\n\nVideo Encoder. In Tab. 1a, we show the effect of the choice of visual input features. We find that for text-to-video retrieval at Recall at 1 and 5 ( $R@1$ ,  $R@5$ ), features obtained from a video R(2+1)D-34 ResNet achieve  $2.9\\%$  and  $7.0\\%$  higher performance compared to only image-frame based features from a ResNet-152. A further  $3.5\\%$  and  $2.0\\%$  can be gained by concatenating both features, yielding the strongest  $MdR$  of  $3.0\\%$ .\n\n\nFeature Aggregation. While the features from both video and image-based visual encoders have reduced spatial extent after a fully-connected layer, the temporal dimension can be reduced in various ways. In Tab. 1b, we find that our multi-head, parameter-\n\n\nized attention reduction yields strong gains over the mean- or max-pooling baselines of over  $4\\%$  for\n\n\nTable 2: Effect of learning objectives. Text  $\\rightarrow$  Video retrieval on MSR-VTT.\n\n\n<table><tr><td></td><td>R@1\u2191</td><td>R@5\u2191</td><td>MdR\u2193</td></tr><tr><td>None</td><td>25.9</td><td>53.0</td><td>4.0</td></tr><tr><td>Identity</td><td>26.4</td><td>51.9</td><td>4.0</td></tr><tr><td>Full</td><td>25.8</td><td>53.9</td><td>3.0</td></tr><tr><td>Hybrid</td><td>26.0</td><td>54.8</td><td>3.0</td></tr><tr><td>Cross</td><td>27.2</td><td>55.2</td><td>3.0</td></tr></table>\n\n\n$R@1$ . This shows that learning attention over the temporal dimension of fixed feature sets can give strong gains even without fine-tuning the encoder.\n\n\nText Encoder. In Tab. 1c, we find decent gains of  $2.7\\%$  and  $0.4\\%$  for  $\\mathbf{R}@\\mathbf{1},\\mathbf{5}$  for using T5-base, instead of T5-small. We do not use the T-5-Large model, as in Korbar et al. (2020), due to the prohibitively large relative model size increase of  $+220\\%$ .\n\n\nText Decoder. In Tab. 1d, we find that using a larger text decoder gives a  $1\\%$  increase in performance when using the cross-captioning objective.\n\n\nContrastive Loss. To validate the choice of a triplet loss in eq. (1), in Tab. 1e, we compare the results of the InfoNCE contrastive loss (Oord et al., 2018) with a triplet loss, with both the intra and inter-intra modality variants. We find that InfoNCE (Oord et al., 2018) loss does not work well in our case, likely due to the difficulty in tuning this loss to have the right combination of temperature and batch-size.\n\n\nSupport-Set Size. Lastly, in Tab. 1f, we show the effect of the size of the support set used for cross-instance captioning. We find that our reconstruction loss indeed acts as a bottleneck, with both smaller and very large sizes degrading the performance.\n\n\nCaptioning Objective. In Tab. 2, we show the effect of the different variants of our learning objective eq. (3). First, we find that the naive addition of a reconstruction objective (\"Identity\") does not improve the contrastive-only baseline (\"None\") much. Considering reconstruction from other videos improves the performance more. In particular, the \"Hybrid\" variant, which combines \"Identity\" and \"Full\" (sec. 3.1.2) improves Recall at 1 and 5 from  $25.9\\%$  and  $53.0\\%$  to  $26.0\\%$  and  $54.8\\%$ , respectively. However, the best result by far  $(27.2 / 55.2\\%)$  is obtained forcing captions to be reconstructed only from other videos, via our cross-instance attention mechanism (\"Cross\"). This variant cannot use information contained in a video to generate the corresponding caption and thus entirely relies on the model to discover meaningful relationship between different videos. This newly-proposed scheme seems to have the most beneficial effect for semantic retrieval.\n\n\nTable 3: Retrieval performance on the MSR-VTT dataset. Models in the second group are additionally pretrained on HowTo100M.\n\n\n<table><tr><td></td><td colspan=\"4\">Text\u2192Video</td><td colspan=\"4\">Video\u2192Text</td></tr><tr><td></td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td></tr><tr><td>Random Baseline</td><td>0.1</td><td>0.5</td><td>1.0</td><td>500.0</td><td>0.1</td><td>0.5</td><td>1.0</td><td>500.0</td></tr><tr><td>JSFusion (Yu et al., 2018)</td><td>10.2</td><td>31.2</td><td>43.2</td><td>13.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HT100M (Miech et al., 2019)</td><td>12.1</td><td>35.0</td><td>48.0</td><td>12.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>JPoSE (Wray et al., 2019)</td><td>14.3</td><td>38.1</td><td>53.0</td><td>9.0</td><td>16.4</td><td>41.3</td><td>54.4</td><td>8.7</td></tr><tr><td>CE (Liu et al., 2019)</td><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td><td>20.6</td><td>50.3</td><td>64.0</td><td>5.3</td></tr><tr><td>MMT (Gabeur et al., 2020)</td><td>24.6</td><td>54.0</td><td>67.1</td><td>4.0</td><td>24.4</td><td>56.0</td><td>67.8</td><td>4.0</td></tr><tr><td>Ours</td><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td><td>26.6</td><td>55.1</td><td>67.5</td><td>3.0</td></tr><tr><td>VidTranslate (Korbar et al., 2020)</td><td>14.7</td><td>-</td><td>52.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HT100M (Miech et al., 2019)</td><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td><td>16.8</td><td>41.7</td><td>55.1</td><td>8.0</td></tr><tr><td>NoiseEstimation (Amrani et al., 2020)</td><td>17.4</td><td>41.6</td><td>53.6</td><td>8.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>UniVL (Luo et al., 2020)</td><td>21.2</td><td>49.6</td><td>63.1</td><td>6.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>AVLnet (Rouditchenko et al., 2020)</td><td>27.1</td><td>55.6</td><td>66.6</td><td>4.0</td><td>28.5</td><td>54.6</td><td>65.2</td><td>4.0</td></tr><tr><td>MMT (Gabeur et al., 2020)</td><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td><td>27.0</td><td>57.5</td><td>69.7</td><td>3.7</td></tr><tr><td>Ours-pretrained</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td><td>28.5</td><td>58.6</td><td>71.6</td><td>3.0</td></tr></table>\n\n\n# 4.3 COMPARISON TO STATE-OF-THE-ART\n\n\nIn this section, we compare the results of our method to other recent text-to-video and video-to-text retrieval approaches on various datasets. In Tab. 3 to 5, we show the results of our model applied to text-to-video and video-to-text retrieval on MSR-VTT, VATEX, ActivityNet and MSVD with and without pre-training on HowTo100M. Without pre-training, our method outperforms all others in all metrics and datasets. In particular, for the VATEX dataset, our retrieval performance at recall at 1 and 5 is  $45.9\\%$  and  $82.4\\%$ , exceeding recent state-of-the-art methods (Chen et al., 2020b) by a margin of  $9\\%$ . For ActivityNet, our model outperforms MMT by a margin of  $4\\%$  at recall at 1. With\n\n\nTable 4: Retrieval performance on the VATEX dataset\n\n\n<table><tr><td></td><td colspan=\"4\">Text \u2192 Video</td><td colspan=\"4\">Video \u2192 Text</td></tr><tr><td></td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td></tr><tr><td>Random Baseline</td><td>0.2</td><td>0.7</td><td>1.05</td><td>2000.5</td><td>0.02</td><td>0.1</td><td>1.02</td><td>2100.5</td></tr><tr><td>VSE (Kiros et al., 2014)</td><td>28.0</td><td>64.3</td><td>76.9</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VSE++ (Faghri et al., 2018)</td><td>33.7</td><td>70.1</td><td>81.0</td><td>2.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Dual (Dong et al., 2019)</td><td>31.1</td><td>67.4</td><td>78.9</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HGR (Chen et al., 2020b)</td><td>35.1</td><td>73.5</td><td>83.5</td><td>2.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Ours</td><td>44.6</td><td>81.8</td><td>89.5</td><td>1.0</td><td>58.1</td><td>83.8</td><td>90.9</td><td>1.0</td></tr><tr><td>Ours-pretrained</td><td>45.9</td><td>82.4</td><td>90.4</td><td>1.0</td><td>61.2</td><td>85.2</td><td>91.8</td><td>1.0</td></tr></table>\n\n\nTable 5: Retrieval performance on ActivityNet\n\n\n<table><tr><td></td><td colspan=\"4\">Text \u2192 Video</td><td colspan=\"4\">Video \u2192 Text</td></tr><tr><td></td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@50\u2191</td><td>MdR\u2193</td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@50\u2191</td><td>MdR\u2193</td></tr><tr><td>Random Baseline</td><td>0.02</td><td>0.1</td><td>1.02</td><td>2458</td><td>0.02</td><td>0.1</td><td>1.02</td><td>2458</td></tr><tr><td>FSE(Zhang et al., 2018)</td><td>18.2</td><td>44.8</td><td>89.1</td><td>7.0</td><td>16.7</td><td>43.1</td><td>88.4</td><td>7.0</td></tr><tr><td>CE (Liu et al., 2019)</td><td>18.2</td><td>47.7</td><td>91.4</td><td>6.0</td><td>17.7</td><td>46.6</td><td>90.9</td><td>6.0</td></tr><tr><td>HSE (Zhang et al., 2018)</td><td>20.5</td><td>49.3</td><td>-</td><td>-</td><td>18.7</td><td>48.1</td><td>-</td><td>-</td></tr><tr><td>MMT (Gabeur et al., 2020)</td><td>22.7</td><td>54.2</td><td>93.2</td><td>5.0</td><td>22.9</td><td>54.8</td><td>93.1</td><td>4.3</td></tr><tr><td>Ours</td><td>26.8</td><td>58.1</td><td>93.5</td><td>3.0</td><td>25.5</td><td>57.3</td><td>93.5</td><td>3.0</td></tr><tr><td>MMT-pretrained (Gabeur et al., 2020)</td><td>28.7</td><td>61.4</td><td>94.5</td><td>3.3</td><td>28.9</td><td>61.1</td><td>94.3</td><td>4.0</td></tr><tr><td>Ours-pretrained</td><td>29.2</td><td>61.6</td><td>94.7</td><td>3.0</td><td>28.7</td><td>60.8</td><td>94.8</td><td>2.0</td></tr></table>\n\n\nTable 6: Retrieval performance on the MSVD dataset\n\n\n<table><tr><td></td><td colspan=\"4\">Text \u2192 Video</td><td colspan=\"4\">Video \u2192 Text</td></tr><tr><td></td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td><td>R@1\u2191</td><td>R@5\u2191</td><td>R@10\u2191</td><td>MdR\u2193</td></tr><tr><td>VSE (Kiros et al., 2014)</td><td>12.3</td><td>30.1</td><td>42.3</td><td>14.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VSE++ (Faghri et al., 2018)</td><td>15.4</td><td>39.6</td><td>53.0</td><td>9.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Multi. Cues (Mithun et al., 2018)</td><td>20.3</td><td>47.8</td><td>61.1</td><td>6.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CE (Liu et al., 2019)</td><td>19.8</td><td>49.0</td><td>63.8</td><td>6.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Ours</td><td>23.0</td><td>52.8</td><td>65.8</td><td>5.0</td><td>27.3</td><td>50.7</td><td>60.8</td><td>5.0</td></tr><tr><td>Ours-pretrained</td><td>28.4</td><td>60.0</td><td>72.9</td><td>4.0</td><td>34.7</td><td>59.9</td><td>70.0</td><td>3.0</td></tr></table>\n\n\npre-training on HowTo100M, our performance further increases across the board. Notably, unlike MMT which uses 7 features, our model uses only 2 features and achieves state-of-the-art in most metrics.\n\n\n# 4.4 ANALYSIS\n\n\nIn order to better understand the effect of our learning objective, we visualize the soft attention of our best-performing cross-instance reconstruction model in fig. 3. As we can see in the top-left square, which shows the pairwise attention between all pairs of videos in the batch, it is highly focused, with the model mostly attending one or two other instances in the batch.\n\n\nFor the first video's caption reconstruction (second row), we find that the model solely attends to another musical\n\n\n![Fig. 3: Support-set attention map. Attention scores of all pairs in a batch (top-left square) and a subset of rows/columns (other squares) on VTT.](images/c7efd7d29452a2cce015646dea8cd2bfd03ff80d4fd0e4e9a9bd38ec0e784c96.jpg)\n\n\nperformance video that is in the batch, ignoring the others. For the second video (third row), the model focuses on another sample that shows the sea but differs in most other aspects since there are no semantically-equivalent clips in the batch. The third video shares a similar scenario. These examples show that the bottleneck is effective at forcing the model to avoid memorising the videocaption association of each clip in isolation, and attempt to match other clips more broadly, since an exact (or very close) match is not guaranteed.\n\n\n# 5 CONCLUSION\n\n\nIn this work, we studied classic contrastive learning methods such as the triplet loss to learn video-text representations for cross-model retrieval. We suggested that the contrastive approach might pull apart videos and captions even when they are semantically equivalent, which can hinder downstream retrieval performance. To mitigate this effect, we propose to consider a captioning pretext task as an additional learning objective. In particular, we show that cross-instance captioning can encourage the representation to pull together videos that share a similar caption, and are thus likely to be equivalent for retrieval. Leveraging these ideas, our model achieves state-of-the-art performance on the text-to-video and video-to-text retrieval tasks, on three datasets.\n\n\nWhile we demonstrated these ideas in the specific case of text-to-video retrieval, they can in principle generalize to any setting that utilizes a contrastive loss, including self-supervised learning, provided that it is possible to learn reasonable conditional generators of a modality or data stream given another.\n\n\n# REFERENCES\n\n- Jean-Baptiste Alayrac, A. Recasens, Ros\u00e1lia G. Schneider, R. Arandjelovi\u0107, Jason Ramapuram, J. Fauw, Lucas Smaira, S. Dieleman, and Andrew Zisserman. Self-supervised multimodal versatile networks. *ArXiv*, abs/2006.16228, 2020.\n\n- Humam Alwassel, Bruno Korbar, Dhruv Mahajan, Lorenzo Torresani, Bernard Ghanem, and Du Tran. Self-supervised learning by cross-modal audio-video clustering. In NeurIPS, 2020.\n\n- Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein. Noise estimation using density estimation for self-supervised multimodal learning. arXiv preprint arXiv:2003.03186, 2020.\n\n- Yuki M. Asano, Mandela Patrick, Christian Rupprecht, and Andrea Vedaldi. Labelling unlabelled videos from scratch with multi-modal self-supervision, 2020a.\n\n- Yuki M Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020b.\n\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n\n- M. Caron, I. Misra, J. Mairal, Priya Goyal, P. Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.\n\n- Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, 2018.\n\n- Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299-6308, 2017.\n\n- David L Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp. 190-200. Association for Computational Linguistics, 2011.\n\n- Mark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from pixels. In ICML, 2020a.\n\n- Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained video-text retrieval with hierarchical graph reasoning. 2020b.\n\n- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020c.\n\n- Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019.\n\n- Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training text encoders as discriminators rather than generators, 2020.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pp. 248-255, 2009.\n\n- Michael Denkowski and Alon Lavie. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the ninth workshop on statistical machine translation, pp. 376-380, 2014.\n\n- Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. arXiv preprint arXiv:2006.06666, 2020.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In ACL, 2019.\n\n- Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In NeurIPS, 2019.\n\n- Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang, and Xun Wang. Dual encoding for zero-example video retrieval. In CVPR, 2019.\n\n- Haodong Duan, Yue Zhao, Yuanjun Xiong, Wentao Liu, and Dahua Lin. Omni-sourced webly-supervised learning for video recognition. In ECCV, 2020.\n\n- Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. In BMVC, 2018.\n\n- Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, 2020.\n\n- Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Large-scale weakly-supervised pre-training for video action recognition. In CVPR, 2019.\n\n- Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. *ICLR*, 2018.\n\n- Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.\n\n- Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS, 2010.\n\n- Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In CVPR, 2006.\n\n- Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In ICCV Workshops, 2019.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning, 2020.\n\n- Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with temporal language. In Empirical Methods in Natural Language Processing (EMNLP), 2018.\n\n- Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 2006.\n\n- Jingyi Hou, Xinxiao Wu, Wentian Zhao, Jiebo Luo, and Yunde Jia. Joint syntax representation learning and visual cue translation for video captioning. In ICCV, 2019.\n\n- Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n\n- Xu Ji, Jo\u00e3o F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classification and segmentation, 2018.\n\n- Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. In NeurIPS, 2020.\n\n- Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning, 2020.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\n- Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.\n\n- Bruno Korbar, Fabio Petroni, Rohit Girdhar, and Lorenzo Torresani. Video understanding as machine translation, 2020.\n\n- Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In CVPR, 2017.\n\n- H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: a large video database for human motion recognition. In ICCV, 2011.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\n- Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa: Localized, compositional video question answering. In EMNLP, pp. 1369-1379, 2018.\n\n- Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer. Pre-training via paraphrasing, 2020a.\n\n- Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL, 2020b.\n\n- Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In AAAI, 2020a.\n\n- Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven C. H. Hoi. Prototypical contrastive learning of unsupervised representations, 2020b.\n\n- Lianian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language, 2019.\n\n- Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei. Jointly localizing and describing events for dense video captioning. In CVPR, 2018.\n\n- Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004.\n\n- Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In BMVC, 2019.\n\n- Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019.\n\n- Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. Univl: A unified video and language pre-training model for multimodal understanding and generation, 2020.\n\n- Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, 2019.\n\n- Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In CVPR, 2020.\n\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\n- Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In CVPR, 2020.\n\n- Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze, and Amit K Roy-Chowdhury. Learning joint embedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, pp. 19-27, 2018.\n\n- Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-visual instance discrimination with cross-modal agreement. arXiv preprint arXiv:2004.12943, 2020.\n\n- Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n\n- Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.\n\n- Mandela Patrick, Yuki M. Asano, Polina Kuznetsova, Ruth Fong, Jo\u00e3o F. Henriques, Geoffrey Zweig, and Andrea Vedaldi. Multi-modal self-supervision from generalized data transformations, 2020.\n\n- Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532-1543, 2014.\n\n- AJ Piergiovanni, Anelia Angelova, and Michael S. Ryoo. Evolving losses for unsupervised video representation learning. In CVPR, 2020.\n\n- Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d residual networks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5533-5541, 2017.\n\n- Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n\n- Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Chris Pal, Hugo Larochelle, Aaron Courville, and Bernt Schiele. Movie description. International Journal of Computer Vision, 2017.\n\n- Andrew Rouditchenko, Angie Boggust, David Harwath, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Rogerio Feris, Brian Kingsbury, Michael Picheny, Antonio Torralba, et al. Avlnet: Learning audio-visual language representations from instructional videos. arXiv preprint arXiv:2006.09199, 2020.\n\n- Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning visual representations with caption annotations, 2020.\n\n- Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human action classes from videos in the wild. In *CRCV-TR-12-01*, 2012.\n\n- Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations, 2019.\n\n- Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. 2017.\n\n- Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning video representations using contrastive bidirectional transformer, 2019a.\n\n- Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In ICCV, 2019b.\n\n- Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NeurIPS, pp. 3104-3112, 2014.\n\n- Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In EMNLP, 2019.\n\n- Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64-73, 2016.\n\n- Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In CVPR, 2018.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIps, 2017.\n\n- Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4566-4575, 2015.\n\n- Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. Sequence to sequence - video to text. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015a.\n\n- Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. Sequence to sequence - video to text. In ICCV, 2015b.\n\n- Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaou Tang, and Luc Van Gool. Temporal segment networks for action recognition in videos. IEEE transactions on pattern analysis and machine intelligence, 41(11):2740-2755, 2018.\n\n- Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4581-4591, 2019.\n\n- Michael Wray, Diane Larlus, Gabriela Csurka, and Dima Damen. Fine-grained action retrieval through multiple parts-of-speech embeddings. In ICCV, 2019.\n\n- Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In CVPR, 2018.\n\n- Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 305\u2013321, 2018.\n\n- Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A large video description dataset for bridging video and language. In CVPR, 2016.\n\n- Ran Xu, Caiming Xiong, Wei Chen, and Jason J Corso. Jointly modeling deep video and compositional text to bridge vision and language in a unified framework. Citeseer, 2015.\n\n- I. Zeki Yalniz, Herv\u00e9 J\u00e9gou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised learning for image classification, 2019.\n\n- Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In ECCV, 2018.\n\n- Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In ECCV, pp. 374-390, 2018.\n\n- Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\n\n- Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, and Zheng-Jun Zha. Object relational graph with teacher-recommended learning for video captioning. In CVPR, 2020.\n\n- Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional videos. 2018a.\n\n- Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end dense video captioning with masked transformer. In CVPR, 2018b.\n\n- Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In CVPR, 2020.\n\nResponse: \\boxed{Accept}\n\n### Rationale:\n\n**Originality:** \nThe paper introduces a novel approach to multimodal representation learning by incorporating a captioning pretext task alongside a contrastive learning objective. It proposes leveraging a generative model to push related samples together, thereby mitigating the strictness of the contrastive loss. This originality lies in combining generative and discriminative objectives in a novel way to improve multimodal representation learning.\n\n**Methodological Rigor:**\n- The authors provide detailed experimental setup and methodology, including the use of various model components like text and video encoders, decoders, and attention mechanisms.\n- They carefully analyze different model components and learning objectives through ablation studies, demonstrating the effectiveness of their approach.\n- The paper includes comprehensive evaluations on multiple datasets (MSR-VTT, VATEX, ActivityNet, MSVD) showing superior performance compared to state-of-the-art methods.\n\n**Significance of Contribution:**\n- The proposed method significantly improves existing state-of-the-art results on text-to-video and video-to-text retrieval tasks.\n- By encouraging the model to share semantics across samples, the method addresses a critical limitation of purely contrastive learning.\n- The use of a support-set bottleneck ensures that the model avoids overfitting to individual samples, leading to more generalizable representations.\n\n**Clarity of Presentation:**\n- The paper is well-written and organized, providing clear explanations of the methodology, results, and contributions.\n- The technical details are presented in a structured manner, making it easy to follow and verify.\n\n**Alignment with Journal Scope:**\n- The paper falls squarely within the scope of the ICLR conference, which focuses on the latest advancements in machine learning theory and applications.\n- The contributions are novel, theoretically grounded, and experimentally validated, aligning well with the goals of ICLR.\n\nOverall, the paper presents a substantial and innovative contribution to the field of multimodal representation learning, with clear benefits demonstrated through extensive experimentation. The authors effectively balance theoretical rigor with practical applicability, making it likely to have a significant impact on the field.", "message": "LLMCall function forward"}
