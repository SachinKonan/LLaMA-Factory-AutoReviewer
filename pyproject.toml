[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "llamafactory"
requires-python = "==3.10.*"
dynamic = [
    "version",
    "dependencies",
    "optional-dependencies",
    "scripts",
    "authors",
    "description",
    "readme",
    "license",
    "keywords",
    "classifiers"
]

[tool.ruff]
target-version = "py310"
line-length = 119
indent-width = 4

[tool.ruff.lint]
ignore = [
    "C408", # collection
    "C901", # complex
    "E501", # line too long
    "E731", # lambda function
    "E741", # ambiguous var name
    "D100", # no doc public module
    "D101", # no doc public class
    "D102", # no doc public method
    "D103", # no doc public function
    "D104", # no doc public package
    "D105", # no doc magic method
    "D107", # no doc __init__
]
extend-select = [
    "C",      # complexity
    "E",      # error
    "F",      # pyflakes
    "I",      # isort
    "W",      # warning
    "UP",     # pyupgrade
    "D",      # pydocstyle
    "PT009",  # pytest assert
    "RUF022", # sort __all__
]

[tool.ruff.lint.isort]
lines-after-imports = 2
known-first-party = ["llamafactory"]
known-third-party = [
    "accelerate",
    "datasets",
    "gradio",
    "numpy",
    "peft",
    "torch",
    "transformers",
    "trl",
]

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
docstring-code-format = true
skip-magic-trailing-comma = false
line-ending = "auto"

[tool.uv]
conflicts = [
    [
        { extra = "torch-npu" },
        { extra = "aqlm" },
    ],
    [
        { extra = "torch-npu" },
        { extra = "vllm" },
    ],
]

[[tool.uv.index]]
name = "torch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[tool.uv.sources]
torch = { index = "torch-cu128" }
torchvision = { index = "torch-cu128" }
torchaudio = { index = "torch-cu128" }
flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.0.post2/flash_attn-2.8.0.post2+cu12torch2.7cxx11abiFALSE-cp310-cp310-linux_x86_64.whl" }

# Extra build hooks for flash-attn
[tool.uv.extra-build-dependencies]
flash-attn = [{ requirement = "torch", match-runtime = true }]

# Skip CUDA build for flash-attn (use cached wheel)
[tool.uv.extra-build-variables]
flash-attn = { FLASH_ATTENTION_SKIP_CUDA_BUILD = "TRUE" }
